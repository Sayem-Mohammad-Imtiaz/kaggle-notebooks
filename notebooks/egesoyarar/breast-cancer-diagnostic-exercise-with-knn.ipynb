{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T11:20:19.491703Z","iopub.execute_input":"2021-05-31T11:20:19.492025Z","iopub.status.idle":"2021-05-31T11:20:19.503832Z","shell.execute_reply.started":"2021-05-31T11:20:19.491994Z","shell.execute_reply":"2021-05-31T11:20:19.502816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Introduction**","metadata":{}},{"cell_type":"markdown","source":"I will import required libraries.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\n\n#warning library\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.510892Z","iopub.execute_input":"2021-05-31T11:20:19.511231Z","iopub.status.idle":"2021-05-31T11:20:19.516598Z","shell.execute_reply.started":"2021-05-31T11:20:19.511196Z","shell.execute_reply":"2021-05-31T11:20:19.515797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will implement dataset to notebook.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.534884Z","iopub.execute_input":"2021-05-31T11:20:19.535235Z","iopub.status.idle":"2021-05-31T11:20:19.545421Z","shell.execute_reply.started":"2021-05-31T11:20:19.5352Z","shell.execute_reply":"2021-05-31T11:20:19.544629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.56117Z","iopub.execute_input":"2021-05-31T11:20:19.561635Z","iopub.status.idle":"2021-05-31T11:20:19.590563Z","shell.execute_reply.started":"2021-05-31T11:20:19.561601Z","shell.execute_reply":"2021-05-31T11:20:19.589866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, \"id\" and \"Unnamed\" columns are not related with our work. So they should be dropped.","metadata":{}},{"cell_type":"code","source":"df.drop([\"id\",\"Unnamed: 32\"], inplace = True, axis = 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.59211Z","iopub.execute_input":"2021-05-31T11:20:19.592704Z","iopub.status.idle":"2021-05-31T11:20:19.625456Z","shell.execute_reply.started":"2021-05-31T11:20:19.592657Z","shell.execute_reply":"2021-05-31T11:20:19.624735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Very first column \"diagnosis\" has confusing name, so I will rename.","metadata":{}},{"cell_type":"code","source":"df = df.rename(columns = {\"diagnosis\":\"target\"})","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.626658Z","iopub.execute_input":"2021-05-31T11:20:19.627046Z","iopub.status.idle":"2021-05-31T11:20:19.630952Z","shell.execute_reply.started":"2021-05-31T11:20:19.627015Z","shell.execute_reply":"2021-05-31T11:20:19.630232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize how much bening or melignant cell that dataset has.","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"target\"])\nprint(df.target.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.631938Z","iopub.execute_input":"2021-05-31T11:20:19.632326Z","iopub.status.idle":"2021-05-31T11:20:19.740263Z","shell.execute_reply.started":"2021-05-31T11:20:19.632295Z","shell.execute_reply":"2021-05-31T11:20:19.739409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can convert our \"target\" column to binary form, so we can easily apply algorithm.","metadata":{}},{"cell_type":"code","source":"df[\"target\"] = [1 if i.strip() == \"M\" else 0 for i in df.target] #Strip function can delete unwanted space char (\" \") from data.","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.741399Z","iopub.execute_input":"2021-05-31T11:20:19.741808Z","iopub.status.idle":"2021-05-31T11:20:19.746223Z","shell.execute_reply.started":"2021-05-31T11:20:19.741775Z","shell.execute_reply":"2021-05-31T11:20:19.745478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Data shape: \", df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.747225Z","iopub.execute_input":"2021-05-31T11:20:19.747647Z","iopub.status.idle":"2021-05-31T11:20:19.762183Z","shell.execute_reply.started":"2021-05-31T11:20:19.747617Z","shell.execute_reply":"2021-05-31T11:20:19.760774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() #shortcut for observing null data","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.763863Z","iopub.execute_input":"2021-05-31T11:20:19.764236Z","iopub.status.idle":"2021-05-31T11:20:19.785573Z","shell.execute_reply.started":"2021-05-31T11:20:19.764186Z","shell.execute_reply":"2021-05-31T11:20:19.784386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.786953Z","iopub.execute_input":"2021-05-31T11:20:19.787424Z","iopub.status.idle":"2021-05-31T11:20:19.881275Z","shell.execute_reply.started":"2021-05-31T11:20:19.78739Z","shell.execute_reply":"2021-05-31T11:20:19.88041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"markdown","source":"Let's take a close look at relations between features.","metadata":{}},{"cell_type":"code","source":"corr_matrix = df.corr()\ncorr_matrix","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.882408Z","iopub.execute_input":"2021-05-31T11:20:19.882666Z","iopub.status.idle":"2021-05-31T11:20:19.924354Z","shell.execute_reply.started":"2021-05-31T11:20:19.882638Z","shell.execute_reply":"2021-05-31T11:20:19.923381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can gain more insight about correlation thanks to better visualization.","metadata":{}},{"cell_type":"code","source":"sns.clustermap(corr_matrix, annot = True, fmt = \".2f\", figsize = (20,20)) # annot stands for showing numeric values in graph\nplt.title(\"Correlation Between Features\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:19.92678Z","iopub.execute_input":"2021-05-31T11:20:19.92708Z","iopub.status.idle":"2021-05-31T11:20:24.406798Z","shell.execute_reply.started":"2021-05-31T11:20:19.927047Z","shell.execute_reply":"2021-05-31T11:20:24.405802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to see only over .75 threshold of correlation.","metadata":{}},{"cell_type":"code","source":"threshold = 0.75\nfilter = np.abs(corr_matrix[\"target\"]) > threshold  #relation between target and the others\nfilter","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:24.408381Z","iopub.execute_input":"2021-05-31T11:20:24.408672Z","iopub.status.idle":"2021-05-31T11:20:24.416718Z","shell.execute_reply.started":"2021-05-31T11:20:24.408643Z","shell.execute_reply":"2021-05-31T11:20:24.415817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_features = corr_matrix.columns[filter].tolist()\nsns.clustermap(df[corr_features].corr(), annot = True, fmt =\".2f\")\nplt.title(\"Correlation Between Features w/ Corr Threshold 0.75\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:24.418516Z","iopub.execute_input":"2021-05-31T11:20:24.41894Z","iopub.status.idle":"2021-05-31T11:20:24.933598Z","shell.execute_reply.started":"2021-05-31T11:20:24.41889Z","shell.execute_reply":"2021-05-31T11:20:24.932593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df[corr_features], diag_kind = \"kde\", markers = \"o\", hue = \"target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:24.93499Z","iopub.execute_input":"2021-05-31T11:20:24.935383Z","iopub.status.idle":"2021-05-31T11:20:28.975245Z","shell.execute_reply.started":"2021-05-31T11:20:24.935338Z","shell.execute_reply":"2021-05-31T11:20:28.974269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If there is a skewness, we should handle it, change them to Gaussian. So, we have skewness in some graphs such as third orange one, we will use outlier detection to fix.","metadata":{}},{"cell_type":"markdown","source":"# **Outlier Detection**","metadata":{}},{"cell_type":"markdown","source":"I will separate features and target to x and y from DataFrame.","metadata":{}},{"cell_type":"code","source":"y = df[\"target\"]\nx = df.drop([\"target\"], axis = 1)\ncolumn_names = x.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:28.976847Z","iopub.execute_input":"2021-05-31T11:20:28.977598Z","iopub.status.idle":"2021-05-31T11:20:28.983711Z","shell.execute_reply.started":"2021-05-31T11:20:28.977544Z","shell.execute_reply":"2021-05-31T11:20:28.983048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use Local Outlier Factor method of Density Based Outlie Detection System.","metadata":{}},{"cell_type":"code","source":"clf = LocalOutlierFactor()\ny_outlier_pred = clf.fit_predict(x)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:28.984606Z","iopub.execute_input":"2021-05-31T11:20:28.984961Z","iopub.status.idle":"2021-05-31T11:20:29.011733Z","shell.execute_reply.started":"2021-05-31T11:20:28.984924Z","shell.execute_reply":"2021-05-31T11:20:29.010669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In outlier score calculation, minus signed score means that point is outlier, otherwise it is inlier.","metadata":{}},{"cell_type":"code","source":"outlier_score = pd.DataFrame()\noutlier_score[\"score\"] = clf.negative_outlier_factor_","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.013195Z","iopub.execute_input":"2021-05-31T11:20:29.013773Z","iopub.status.idle":"2021-05-31T11:20:29.020651Z","shell.execute_reply.started":"2021-05-31T11:20:29.013722Z","shell.execute_reply":"2021-05-31T11:20:29.019484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For better understanding, I am going to visualize data in 2D graph with outlier scores. Outlier scores will be shown as red circle. However, we notice that some circles are wide, even some points appear close. The reason is in other features, they have close relations. Moreover I will use filter for detecting outliers thanks to -2.2 threshold.","metadata":{}},{"cell_type":"code","source":"threshold = -2.2\nfilter = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filter].index.tolist()\n\nplt.figure(figsize=(10,5))\nplt.scatter(x.iloc[outlier_index,3], x.iloc[outlier_index,5], color = \"blue\", s = 50, label = \"Outliers\")\nplt.scatter(x.iloc[:,3], x.iloc[:,5], color = \"k\", s = 3, label = \"Data Points\")\n\nradius = (clf.negative_outlier_factor_.max() - clf.negative_outlier_factor_)/(clf.negative_outlier_factor_.max() - clf.negative_outlier_factor_.min())\noutlier_score[\"radius\"] = radius\nplt.scatter(x.iloc[:,3], x.iloc[:,5], s = 1000*radius, edgecolor = \"r\", facecolors = \"none\", label = \"Outlier Score\")\nplt.legend()\nplt.show","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.022177Z","iopub.execute_input":"2021-05-31T11:20:29.022781Z","iopub.status.idle":"2021-05-31T11:20:29.270715Z","shell.execute_reply.started":"2021-05-31T11:20:29.022729Z","shell.execute_reply":"2021-05-31T11:20:29.269962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it is time to dropping outliers.","metadata":{}},{"cell_type":"code","source":"x = x.drop(outlier_index)\ny = y.drop(outlier_index).values","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.271687Z","iopub.execute_input":"2021-05-31T11:20:29.272083Z","iopub.status.idle":"2021-05-31T11:20:29.276803Z","shell.execute_reply.started":"2021-05-31T11:20:29.272035Z","shell.execute_reply":"2021-05-31T11:20:29.275982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train-Test Split**","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.279181Z","iopub.execute_input":"2021-05-31T11:20:29.279429Z","iopub.status.idle":"2021-05-31T11:20:29.292774Z","shell.execute_reply.started":"2021-05-31T11:20:29.279404Z","shell.execute_reply":"2021-05-31T11:20:29.291351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Standardization**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test) # We have already scaled according to x_train, so we dont need to scale again for x_test!","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.296317Z","iopub.execute_input":"2021-05-31T11:20:29.296623Z","iopub.status.idle":"2021-05-31T11:20:29.311733Z","shell.execute_reply.started":"2021-05-31T11:20:29.296592Z","shell.execute_reply":"2021-05-31T11:20:29.310701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_df = pd.DataFrame(x_train, columns = column_names)\nx_train_df[\"target\"] = y_train","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.313187Z","iopub.execute_input":"2021-05-31T11:20:29.313611Z","iopub.status.idle":"2021-05-31T11:20:29.32496Z","shell.execute_reply.started":"2021-05-31T11:20:29.313563Z","shell.execute_reply":"2021-05-31T11:20:29.324071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Box plot supplies a detailed informations about how features have effects on being benign or melignant, where are the outliers and so on. ","metadata":{}},{"cell_type":"code","source":"df_melted = pd.melt ( x_train_df, id_vars = \"target\",\n             var_name = \"features\",\n             value_name = \"value\")\nplt.figure(figsize=(10,5))\nsns.boxplot(x = \"features\", y = \"value\", hue = \"target\" , data = df_melted)\nplt.xticks(rotation = 90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:29.326649Z","iopub.execute_input":"2021-05-31T11:20:29.32709Z","iopub.status.idle":"2021-05-31T11:20:30.478191Z","shell.execute_reply.started":"2021-05-31T11:20:29.327041Z","shell.execute_reply":"2021-05-31T11:20:30.477028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **KNN**","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nscore = knn.score(x_test, y_test)\nprint(\"Score: \", score)\nprint(\"CM: \", cm)\nprint(\"Basic KNN Acc: \", acc)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:30.479505Z","iopub.execute_input":"2021-05-31T11:20:30.479866Z","iopub.status.idle":"2021-05-31T11:20:30.522315Z","shell.execute_reply.started":"2021-05-31T11:20:30.479823Z","shell.execute_reply":"2021-05-31T11:20:30.521012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"optimumKValue method finds best K value for KNN.","metadata":{}},{"cell_type":"code","source":"def optimumKValue(x_train,x_test,y_train,y_test):\n    \n    k_range = list(range(1,31))\n    weight_options = [\"uniform\",\"distance\"]\n    param_grid = dict(n_neighbors = k_range, weights = weight_options)\n    \n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn, param_grid, cv = 10, scoring = \"accuracy\")\n    grid.fit(x_train, y_train)\n    \n    print(\"Best training score: {} with parameters: {}\".format(grid.best_score_, grid.best_params_))\n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train, y_train)\n    \n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n    \n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n    \n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n    print(\"Test Score: {}, Train Score: {}\".format(acc_test, acc_train))\n    print()\n    print(\"CM Test: \", cm_test)\n    print(\"CM Train: \", cm_train)\n    \n    return grid\n\ngrid = optimumKValue(x_train,x_test,y_train,y_test)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:30.524119Z","iopub.execute_input":"2021-05-31T11:20:30.524555Z","iopub.status.idle":"2021-05-31T11:20:33.195806Z","shell.execute_reply.started":"2021-05-31T11:20:30.524505Z","shell.execute_reply":"2021-05-31T11:20:33.194753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our train and test scores are so close to one and train score is higher than test score, that situation can be explained as overfitting. ","metadata":{}},{"cell_type":"markdown","source":"# **PCA**","metadata":{}},{"cell_type":"markdown","source":"PCA (Princible Component Analysis), helps to reduce the dimensionality of dataset. (reduction of features)","metadata":{}},{"cell_type":"markdown","source":"Standardization","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:33.197568Z","iopub.execute_input":"2021-05-31T11:20:33.198312Z","iopub.status.idle":"2021-05-31T11:20:33.21055Z","shell.execute_reply.started":"2021-05-31T11:20:33.198259Z","shell.execute_reply":"2021-05-31T11:20:33.20945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implementation of PCA","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 2) #n_components, desired number of components \npca.fit(x_scaled)\nx_reduced_pca = pca.transform(x_scaled)\npca_data = pd.DataFrame(x_reduced_pca, columns = [\"p1\",\"p2\"])\npca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\", y = \"p2\", hue = \"target\", data = pca_data)\nplt.title(\"PCA: p1 vs p2\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:33.212447Z","iopub.execute_input":"2021-05-31T11:20:33.213219Z","iopub.status.idle":"2021-05-31T11:20:33.529835Z","shell.execute_reply.started":"2021-05-31T11:20:33.213166Z","shell.execute_reply":"2021-05-31T11:20:33.528807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train-test split of x_reduced_pca","metadata":{}},{"cell_type":"code","source":"x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_reduced_pca, y, test_size = 0.33, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:33.53096Z","iopub.execute_input":"2021-05-31T11:20:33.531262Z","iopub.status.idle":"2021-05-31T11:20:33.535821Z","shell.execute_reply.started":"2021-05-31T11:20:33.531235Z","shell.execute_reply":"2021-05-31T11:20:33.535206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the best K value ","metadata":{}},{"cell_type":"code","source":"grid_pca = optimumKValue(x_train_pca, x_test_pca, y_train_pca, y_test_pca)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:33.53681Z","iopub.execute_input":"2021-05-31T11:20:33.537189Z","iopub.status.idle":"2021-05-31T11:20:35.120387Z","shell.execute_reply.started":"2021-05-31T11:20:33.537157Z","shell.execute_reply":"2021-05-31T11:20:35.119256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize each grid's class as a map and training points' classes. Map helps us to understand which training points is classified incorrectly.","metadata":{}},{"cell_type":"code","source":"#visualize\ncmap_light = ListedColormap([\"orange\",\"cornflowerblue\"])\ncmap_bold = ListedColormap([\"darkorange\",\"darkblue\"])\n\nh = 0.05 #step size in the mesh\nx = x_reduced_pca\nx_min, x_max = x[:,0].min() - 1, x[:,0].max() + 1\ny_min, y_max = x[:,1].min() - 1, x[:,1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nz = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n#plot results\nz = z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, z, cmap = cmap_light)\n\n#plot training points\nplt.scatter(x[:,0], x[:,1], c = y, cmap = cmap_bold, edgecolor = \"k\", s = 20)\nplt.xlim(xx.min(), xx.max()) #axis' size\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights '%s')\" % (len(np.unique(y)), grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:35.121614Z","iopub.execute_input":"2021-05-31T11:20:35.121895Z","iopub.status.idle":"2021-05-31T11:20:40.089397Z","shell.execute_reply.started":"2021-05-31T11:20:35.121864Z","shell.execute_reply":"2021-05-31T11:20:40.088351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **NCA**","metadata":{}},{"cell_type":"markdown","source":"Neighborhood Components Analysis is a supervised learning algorithm and similar with KNN.","metadata":{}},{"cell_type":"code","source":"nca = NeighborhoodComponentsAnalysis(n_components = 2, random_state = 42)\nnca.fit(x_scaled, y)\nx_reduced_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(x_reduced_nca, columns = [\"p1\",\"p2\"])\nnca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\", y = \"p2\", hue = \"target\", data = nca_data)\nplt.title(\"NCA: p1 vs p2\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:40.090744Z","iopub.execute_input":"2021-05-31T11:20:40.091126Z","iopub.status.idle":"2021-05-31T11:20:41.218322Z","shell.execute_reply.started":"2021-05-31T11:20:40.091082Z","shell.execute_reply":"2021-05-31T11:20:41.217594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train-test split","metadata":{}},{"cell_type":"code","source":"x_train_nca, x_test_nca, y_train_nca, y_test_nca = train_test_split(x_reduced_nca, y, test_size = 0.33, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:41.219242Z","iopub.execute_input":"2021-05-31T11:20:41.219615Z","iopub.status.idle":"2021-05-31T11:20:41.224201Z","shell.execute_reply.started":"2021-05-31T11:20:41.219587Z","shell.execute_reply":"2021-05-31T11:20:41.22351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_nca = optimumKValue(x_train_nca, x_test_nca, y_train_nca, y_test_nca)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:41.225068Z","iopub.execute_input":"2021-05-31T11:20:41.225418Z","iopub.status.idle":"2021-05-31T11:20:42.753015Z","shell.execute_reply.started":"2021-05-31T11:20:41.225391Z","shell.execute_reply":"2021-05-31T11:20:42.752341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize\ncmap_light = ListedColormap([\"orange\",\"cornflowerblue\"])\ncmap_bold = ListedColormap([\"darkorange\",\"darkblue\"])\n\nh = 0.1 #step size in the mesh\nx = x_reduced_nca\nx_min, x_max = x[:,0].min() - 1, x[:,0].max() + 1\ny_min, y_max = x[:,1].min() - 1, x[:,1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nz = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n#plot results\nz = z.reshape(xx.shape)\nplt.figure(figsize=(15,10))\nplt.pcolormesh(xx, yy, z, cmap = cmap_light)\n\n#plot training points\nplt.scatter(x[:,0], x[:,1], c = y, cmap = cmap_bold, edgecolor = \"k\", s = 20)\nplt.xlim(xx.min(), xx.max()) #axis' size\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights '%s')\" % (len(np.unique(y)), grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T11:20:42.753894Z","iopub.execute_input":"2021-05-31T11:20:42.754262Z","iopub.status.idle":"2021-05-31T11:23:17.870215Z","shell.execute_reply.started":"2021-05-31T11:20:42.754233Z","shell.execute_reply":"2021-05-31T11:23:17.86937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a result; I get approximately 98% accuracy with 4 mistakes out of test 183 samples.","metadata":{}}]}