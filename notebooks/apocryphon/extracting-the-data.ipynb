{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json         #json file reading\nimport multiprocessing as mp\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Installing packages**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#ensure that the internet option is on (Kaggle) to install textract package\n!pip install textract --upgrade\n!pip install wordcloud\n!pip install PIL\nimport textract\nfrom wordcloud import WordCloud, STOPWORDS\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Defining Functions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkForDuplicates(List):\n    ''' Check for any duplicates in a list'''\n    if len(List) == len(set(List)):\n        return False\n    else:\n        return True\n    \n    \ndef read_pdf(file):\n    '''Reads \"/kaggle/input/CORD-19-research-challenge/2020-03-13/COVID.DATA.LIC.AGMT.pdf\" and makes it a bit more readable '''\n    import textract\n    text = str(textract.process(file))\n    text = text.lstrip('b')\n    text = text.strip(\"'\")\n    text = text.split('\\\\n')\n\n    pdf_str = ''\n    for t in text:\n        if t != '':\n            if t[0].isupper():\n                pdf_str = pdf_str + t + '\\n'\n            else:\n                pdf_str = pdf_str + t    \n            \n    return pdf_str\n\n\ndef load_json_files(directory_name):\n    '''Loads json files into list'''\n    articles = []\n    \n    articlenames = os.listdir(directory_name)\n\n    for articlename in tqdm(articlenames):\n        articlename = directory_name + articlename\n        article = json.load(open(articlename, 'rb'))\n        articles.append(article)\n    \n    return articles\n\n\n\ndef article_title_list(article,source):\n    '''Create a list of a paper's ID, title and source''' \n\n    row = [article[\"paper_id\"],article[\"metadata\"][\"title\"],source]\n    return  row\n\n\n\n\ndef article_author_list(article):\n    '''Create a list of a paper's ID, authors and source'''\n    authors = []\n\n    for idx in range(len(article[\"metadata\"][\"authors\"])):\n        author = [article[\"paper_id\"],article[\"metadata\"][\"authors\"][idx][\"first\"], article[\"metadata\"][\"authors\"][idx][\"last\"]]\n        authors.append(author)\n    \n    return authors\n\n\n\n\n\ndef article_body_list(article):\n    '''Create a list of a paper's ID and body'''\n    body = []\n    for idx in range(len(article[\"body_text\"])):\n        bod = [article[\"paper_id\"],article[\"body_text\"][idx][\"section\"],article[\"body_text\"][idx][\"text\"]]\n        body.append(bod)\n    \n    return body\n\n\n\n\n\ndef article_abstract_list(article):\n    '''Create a list of a paper's ID, and abstracts'''\n    abstracts = []\n    for idx in range(len(article[\"abstract\"])):\n        abstract = [article[\"paper_id\"], article[\"abstract\"][idx][\"text\"]]\n        abstracts.append(abstract)\n    \n    return abstracts\n\n\ndef article_text(article):\n    '''Create a list of a paper's ID, and abstracts'''\n    text = ''\n    for idx in range(len(article[\"abstract\"])):\n        text = text + '\\n\\n' + article[\"abstract\"][idx][\"text\"]\n        \n    for idx in range(len(article[\"body_text\"])):\n        text = text + '\\n\\n' + article[\"body_text\"][idx][\"section\"] + '\\n\\n' + article[\"body_text\"][idx][\"text\"]\n    \n    text = [article[\"paper_id\"],text]\n    \n    \n    return text\n\n\n\n\ndef create_df(articles,source):\n    '''Creates dataframes for Kaggle Covid-19'''\n    \n    titles_list = []\n    authors_list = []\n    text = []\n     \n\n    \n    for article in tqdm(articles):\n        \n        '''Create a list of lists containing a paper's ID, title and source'''\n        titles_list.append(article_title_list(article,source))\n\n        '''Create a list of lists containing a paper's ID, authors and source'''    \n        authors_list.extend([*[item for item in article_author_list(article)]])           \n \n\n        '''Create a list of lists containing a paper's abstracts and body'''    \n        text.append(article_text(article)) \n        \n    text = pd.DataFrame(text,columns = [\"Paper_Id\",'Text'])   \n    title_df = pd.DataFrame(titles_list,columns = [\"Paper_Id\",\"Title\",'Source'])\n    author_df = pd.DataFrame(authors_list,columns = [\"Paper_Id\",\"First_Name\",\"Last_Name\"])\n\n        \n    return title_df,author_df,text\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Difining directory names**"},{"metadata":{"trusted":true},"cell_type":"code","source":"noncomm_use_subset_Dir = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/'\nbiorxiv_medrxiv_Dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'\ncomm_use_subset_Dir = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'\ncustom_license_Dir = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/'\n\nnoncomm_pmc_Dir = '/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pmc_json/'\nbiorxiv_pmc_Dir = '/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pmc_json/'\ncomm_pmc_Dir = '/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pmc_json/'\ncustom_pmc_Dir = '/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pmc_json/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for any duplicates**"},{"metadata":{"trusted":true},"cell_type":"code","source":"listOfFileNames = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        listOfFileNames.append(os.path.join(dirname, filename))\n\ncheckForDuplicates(listOfFileNames)\n\ndel listOfFileNames","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Previewing Files**"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/CORD-19-research-challenge/metadata.readme\",'r') as f:\n    file = f.read()\n    print(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(read_pdf(\"/kaggle/input/CORD-19-research-challenge/COVID.DATA.LIC.AGMT.pdf\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\nmeta_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/CORD-19-research-challenge/json_schema.txt\",'r') as f:\n    file = f.read()\n    print(file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating DataFrames**"},{"metadata":{},"cell_type":"markdown","source":"These look at the /pdf_json/ files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create stopword list:\n\nstopwords = set(STOPWORDS)\nstopwords.update([\"et al\",'et', 'al',\"addition\", \"respectively\", \"found\", \"although\",'present',\n                  'identified','Thu','Finally','either','suggesting','include',\"well\", \n                  \"including\", \"associated\", \"method\", \"result\",'used','doi','display',\n                  'https','copyright', 'holder','org','author','available','made','peer',\n                  'reviewed','without','permission','license','rights','reserverd','Furthermore'\n                  'using','preprint','allowed','following','may','thus','funder','International',\n                 'granted','compared','will','one','two','use','different','likely','Discussion',\n                 'medRexiv','Introduction','Moreover','known','funder','granted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles = load_json_files(comm_use_subset_Dir)\ncomm_subset_title,comm_subset_author,comm_subset_text = create_df(articles,'comm_use_subset')\n\ncomm_subset_title.to_csv('comm_subset_title.csv',index = False)\ncomm_subset_author.to_csv('comm_subset_author.csv',index = False)\ncomm_subset_text.to_csv('comm_subset_text.csv',index = False)\n\n\nbody_text = \" \".join(text for text in comm_subset_text.Text)\n\n\n#del comm_subset_title\n#del comm_subset_author\n#del comm_subset_text\n\n\nprint (\"There are {} words in the bodies of the articles.\".format(len(body_text)))\n\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords,max_words=200, background_color=\"white\").generate(body_text)\n\n# Display the generated image:\n# the matplotlib way:\nprint('World Cloud for Bodies of articles')\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate Wordclouds\n'''\narticles = load_json_files(biorxiv_medrxiv_Dir)\nbio_title,bio_author,bio_text = create_df(articles,'biorxiv_medrxiv')\n\nbio_title.to_csv('biorxiv_medrxiv_title.csv',index = False)\nbio_author.to_csv('biorxiv_medrxiv_author.csv',index = False)\nbio_text.to_csv('biorxiv_medrxiv_text.csv',index = False)\n\n\nbody_text = \" \".join(review for review in bio_text.Text)\n\n\ndel bio_title\ndel bio_author\ndel bio_text\n\n\nprint (\"There are {} words in the abstracts of articles.\".format(len(body_text)))\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords,max_words=200, background_color=\"white\").generate(body_text)\n\n# Display the generated image:\n# the matplotlib way:\nprint('World Cloud for Bodies of articles')\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n# Save the image:\n#wordcloud.to_file(\"wordcloud_biorxiv.png\")\n###############################################################\n\narticles = load_json_files(noncomm_use_subset_Dir)\nnoncomm_subset_title,noncomm_subset_author,noncomm_subset_text= create_df(articles,'noncomm_use_subset')\n  \n\nnoncomm_subset_title.to_csv('noncomm_subset_title.csv',index = False)\nnoncomm_subset_author.to_csv('noncomm_subset_author.csv',index = False)\nnoncomm_subset_text.to_csv('noncomm_subset_text.csv',index = False)\n\n\nbody_text = \" \".join(review for review in noncomm_subset_text.Text)\n\n\ndel noncomm_subset_title\ndel noncomm_subset_author\ndel noncomm_subset_text\n\n\nprint (\"There are {} words in the combination of all review.\".format(len(body_text)))\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords,max_words=200, background_color=\"white\").generate(body_text)\n\n# Display the generated image:\nprint('World Cloud for Bodies of articles')\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n# Save the image:\n#wordcloud.to_file(\"wordcloud_noncom.png\")\n\n##########################################################################\n\narticles = load_json_files(custom_license_Dir)\ncustom_license_title,custom_license_author,custom_license_text= create_df(articles,'custom_license')\n    \n\ncustom_license_title.to_csv('custom_license_title.csv',index = False)\ncustom_license_author.to_csv('custom_license_author.csv',index = False)\ncustom_license_text.to_csv('custom_license_text.csv',index = False)\n\n\nbody_text = \" \".join(review for review in custom_license_text.Text)\n\n\ndel custom_license_title\ndel custom_license_author\ndel custom_license_text\n\nprint (\"There are {} words in the combination of all review.\".format(len(body_text)))\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords,max_words=200, background_color=\"white\").generate(body_text)\n\n# Display the generated image:\nprint('World Cloud for Bodies of articles')\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n# Save the image:\n#wordcloud.to_file(\"wordcloud_custom.png\")\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word clouds while interesting are not particularly usefull. So let's look at searching for a word or phrase"},{"metadata":{"trusted":true},"cell_type":"code","source":"comm_subset_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(word,df_text):\n    from nltk.tokenize import word_tokenize\n    papers = []\n    \n    for idx in tqdm(range(len(df_text))):\n        if word in set(word_tokenize(df_text.loc[idx,'Text'])):\n            papers.append(df_text.loc[idx,'Paper_Id'])\n            \n    return papers\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"looking_for = 'pregnancy'\n\npapers = search(looking_for,comm_subset_text)\n\nprint('Numbers of papers containing the word {0} : {1}'.format(looking_for,len(papers)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bag of words\n\ndef bow(article):\n    '''Creates a bag of words from a string'''\n    #import packages \n    from collections import Counter\n    from nltk.corpus import stopwords\n    from nltk.stem import WordNetLemmatizer\n    from nltk.tokenize import word_tokenize\n\n    #tokenize and convert to lowercase\n    tokens = word_tokenize(article.lower()) \n    #remove punctuation\n    words = [word for word in tokens if word.isalpha()]  \n    #define stopwords\n    stop_words = stopwords.words('english') \n    stop_words = set(stopwords.words('english')) \n    #remove stopwords\n    words = [w for w in words if not w in stop_words]\n    # Lemmatize all tokens into a new list\n    lemmatized = [WordNetLemmatizer().lemmatize(t) for t in words ] \n    bow = Counter(lemmatized)\n    return bow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_and_stem(text):\n    import re\n    from nltk.stem.snowball import SnowballStemmer\n    import nltk\n    \n    # Tokenize by sentence, then by word\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    tokens = [word for word in tokens if word.isalpha()] \n    # Filter out raw tokens to remove noise\n    filtered_tokens = [token for token in tokens if re.search(r'[a-zA-Z]', token)]\n    \n    # Stem the filtered_tokens\n    stemmer = SnowballStemmer(\"english\")\n    stems = [stemmer.stem(word) for word in filtered_tokens]\n    \n    return stems\n\n\n\n# Import TfidfVectorizer to create TF-IDF vectors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n\n\nstemmer = SnowballStemmer(\"english\")\ntokenized_stop_words = [stemmer.stem(word) for word in nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))]\n\n# Instantiate TfidfVectorizer object with stopwords and tokenizer\n# parameters for efficient processing of text\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.2, stop_words=tokenized_stop_words,\n                                 use_idf=True, tokenizer=tokenize_and_stem,\n                                 ngram_range=(1,3))\n\ntfidf_matrix = tfidf_vectorizer.fit_transform([x for x in comm_subset_text['Text']])\n\nprint(tfidf_matrix.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import k-means to perform clusters\nfrom sklearn.cluster import KMeans\n\n# Create a KMeans object with 5 clusters and save as km\nkm = KMeans(n_clusters=5)\n\n# Fit the k-means object with tfidf_matrix\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()\n\n# Create a column cluster to denote the generated cluster for each movie\ntfidf_matrix[\"cluster\"] = clusters\n\n# Display number of films per cluster (clusters from 0 to 4)\ntfidf_matrix['cluster'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Calculate similarity distance\n\n\n# Import cosine_similarity to calculate similarity of movie plots\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate the similarity distance\nsimilarity_distance = 1 - cosine_similarity(tfidf_matrix)\n\n\n##Create merging and plot dendrogram\n\n# Import matplotlib.pyplot for plotting graphs\nimport matplotlib.pyplot as plt\n\n# Configure matplotlib to display the output inline\n%matplotlib inline\n\n# Import modules necessary to plot dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n\n# Create mergings matrix \nmergings = linkage(similarity_distance, method='complete')\n\n# Plot the dendrogram, using title as label column\ndendrogram_ = dendrogram(mergings,\n               labels=[x for x in movies_df[\"title\"]],\n               leaf_rotation=90,\n               leaf_font_size=16,\n\n)\n\n# Adjust the plot\nfig = plt.gcf()\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\nfig.set_size_inches(108, 21)\n\n# Show the plotted dendrogram\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sum_of_squared_distances = []\nK = range(1,15)\nfor k in tqdm(K):\n    km = KMeans(n_clusters=k)\n    km = km.fit(tfidf_matrix)\n    Sum_of_squared_distances.append(km.inertia_)\n    \n    \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}