{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About DATA\n\nA manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT3VfYG2ZJfvlRVtaora8MkuAnTSr-iiPq52Q&usqp=CAU\" width=500 arl=\"img\"><br><br>\n\n\n\nThere are only 16% customers in the dataset get churned. That is labeled by the column `Attrition_Flag`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_churn = pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\ncard_churn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop Uneccessary Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_churn.drop([\"CLIENTNUM\",\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\",\n                    \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_churn.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Change categorical features to numeric\n\n`Attrition Flag`, `Gender`, `Education_Level`, `Marital_Status`, `Income_Category`, `Card_Category`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.Attrition_Flag.unique()\n# Output: ['Existing Customer', 'Attrited Customer']\n\n# df.Education_Level.unique()\n# Output: ['High School', 'Graduate', 'Uneducated', 'Unknown', 'College',\n#        'Post-Graduate', 'Doctorate']\n\n# df.Income_Category.unique()\n# Output: ['$60K - $80K', 'Less than $40K', '$80K - $120K', '$40K - $60K',\n#        '$120K +', 'Unknown']\n\n# df.Card_Category.unique()\n# Output: ['Blue', 'Gold', 'Silver', 'Platinum']\n\ncard_churn.Attrition_Flag = card_churn.Attrition_Flag.map({\"Attrited Customer\": 1,\"Existing Customer\": 0})\ncard_churn.Gender = card_churn.Gender.map({\"M\":1, \"F\":0})\ncard_churn.Education_Level = card_churn.Education_Level.map({'High School':2, 'Graduate':4, 'Uneducated':1, 'Unknown':0, 'College':3,\n       'Post-Graduate':5, 'Doctorate':6})\ncard_churn.Marital_Status = card_churn.Marital_Status.map({\"Unknown\":0, \"Single\":1, \"Married\":2, \"Divorced\":3})\ncard_churn.Income_Category = card_churn.Income_Category.map({'$60K - $80K':3, 'Less than $40K':1, '$80K - $120K':4, '$40K - $60K':2,\n       '$120K +':5, 'Unknown':0})\ncard_churn.Card_Category = card_churn.Card_Category.map({'Blue':0, 'Gold':1, 'Silver':2, 'Platinum':3})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choosing feature to fit into the model\n\nBy call `corr()` method, we get a `pd.DataFrame()` object where entries are the correlations of the features. I choosing features which are more related to `Attrition_Flag` to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_feats = list(card_churn.corr().index[abs(card_churn.corr()[\"Attrition_Flag\"]) >= 0.1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_feats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spliting Data into train, test set\n\nNote that there are only 16% of custumer get churned, so we should carefully in split training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_churn = card_churn[tr_feats]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### My splitting IDEA\nMy Idea is to split the data where `Attrition_Flag == 0` into 6 parts. So I construct `6 batches` having `16% churned` and `13% not`. And giving 6 batches into the model.\n\nSo that the features of `not getting churned` will not dominate the `churned`. But there is another problem that those `16% churned` will get trained 6 times more than the other. \n\nTherefore, I will normalize all the features to `zero mean`, `1 stds`. And then before fitting data into the model, I'll add to each features a random variable `0.1 std`. So it wont change much the features but reduce the bias of `16% churned`."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_churn.Attrition_Flag.sum()/card_churn.Attrition_Flag.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the features\nfor feat in tr_feats[1:]:\n    card_churn[feat] = (card_churn[feat] - card_churn[feat].mean())/ card_churn[feat].var()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tf.random.normal(\n#     shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None\n# )\n\nchurned = card_churn[card_churn.Attrition_Flag == 1]\nnchurned = card_churn[card_churn.Attrition_Flag==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churned[tr_feats[1:]].agg(lambda x: x+np.random.normal(0,0.1,x.shape))\n# churned[tf_feat[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = int(1./6 * nchurned.shape[0])\ntr_ratio = 0.9","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick evaluate the Idea by KNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# My IDEA\ndef take_patial(churned, nchurned, n=6):\n    l = int(nchurned.shape[0]//n)\n    trainsets = []\n    for i in range(1,n):\n        trainsets.append(pd.concat([churned,nchurned[(i-1)*l:i*l]]))\n    trainsets.append(pd.concat([churned,nchurned[(n-1)*l:]]))\n    return trainsets\n\ndef train_test(p, df):\n    s = df.shape[0]\n    train = df[:int(p*s)]\n    val = df[int(p*s):]\n    return train, val\n\ndef fit(trainset, model, tr_feats=tr_feats):\n    x, y = trainset[tr_feats[1:]], trainset[tr_feats[0]]\n    noise = np.random.normal(0, 0.1, x.iloc[0].shape[0])\n    totrain = x + noise\n    model.fit(totrain, y)\n    print(model.score(totrain,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_churned, val_churned = train_test(tr_ratio, churned)\ntr_nchurned, val_nchurned = train_test(tr_ratio, nchurned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainsets = take_patial(tr_churned, tr_nchurned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainsets[0].Attrition_Flag.sum()/trainsets[0].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbor = KNeighborsClassifier(n_neighbors=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for trainset in trainsets:\n    fit(trainset,neighbor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score(val, model, tr_feats=tr_feats):\n    x, y = val[tr_feats[1:]], val[tr_feats[0]]\n    print(model.score(x,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(val_nchurned, neighbor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score(pd.concat([val_churned,val_nchurned]),neighbor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normal way"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nxtrain, xval, ytrain, yval = train_test_split(card_churn[tr_feats[1:]], card_churn.Attrition_Flag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbor2 = KNeighborsClassifier(n_neighbors=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbor2.fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbor2.score(xval, yval)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Turn out I found that I haven't split train, val randomly\nAlso in taking partion is not randomly.\n\nAnw, It just an Idea I come up in my mine, if someone read my notebook and feel that it make sense at some points. PLs tell me, I'll be very appriciate !!!"},{"metadata":{},"cell_type":"markdown","source":"## Trying with Deep Learning"},{"metadata":{},"cell_type":"markdown","source":"### Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(16, input_shape=(9,)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(xtrain, ytrain,\n                    batch_size=128,\n                    validation_data=(xval,yval),\n                    epochs=10,\n                    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot(history, metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_'+metric])\n    plt.title('model '+metric)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(history, 'acc')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}