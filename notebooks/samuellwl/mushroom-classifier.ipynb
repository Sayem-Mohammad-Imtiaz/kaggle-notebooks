{"cells":[{"metadata":{"trusted":true,"_uuid":"82edb2c7bf8462c308d0314b28fa963e7070080a"},"cell_type":"markdown","source":"# **Let's classify mushrooms!**"},{"metadata":{"_uuid":"57037c43eec504adfea3dca1ff57ea978fb4a39d"},"cell_type":"markdown","source":"![wow](http://2jl7cd1oy3fddj8w23cb3brh-wpengine.netdna-ssl.com/wp-content/uploads/2017/02/poisonousmushrooms.inpost.jpg)\n\n\nHave you ever encountered a wild mushroom and wondered if it was safe for consumption? How do you even determine if it's edible? Do you look at its size? Or colour? Or smell?\n\n\nIn this kernel, we try to predict if a mushroom is poisonous or edible using 4 models: **Logistic Regression**, **Adaboosted Decision Trees**, **Random Forest** and **Support Vector Machine**."},{"metadata":{"_uuid":"ff7f5fb9192090b64ccdd0477ccb17ca4b2e1927"},"cell_type":"markdown","source":"## **Import libraries and dataset**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# =============================================================================\n# Import libraries and dataset\n# =============================================================================\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\ndata = pd.read_csv(\"../input/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cccfb4c2b0b41f44607e1aeb0d462f3332c3dc05"},"cell_type":"markdown","source":"## **Data background and exploration**"},{"metadata":{"_uuid":"53dcc271a81c14e70f2783813e7edf553588c5dd"},"cell_type":"markdown","source":"Let's first try to explore and get an idea of what the data is like. The following code shows that there are  **8124 observations** and **23 variables** in the dataset: "},{"metadata":{"trusted":true,"_uuid":"680a6857e1d4431d93de4e09b9ebedccbd2b8de6"},"cell_type":"code","source":"np.shape(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40799981cd6557fcc41c7baead29f6ef8345d8bd"},"cell_type":"markdown","source":"It seems that all of the variables are **categorical** in nature:"},{"metadata":{"trusted":true,"_uuid":"3a1f9077c44af9aae58d183990f1b4456fa7430f"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4caac8641f5001774259c797dbba1555fbfd0b1"},"cell_type":"markdown","source":"Taking a look at the first 5 observations of the dataset:"},{"metadata":{"trusted":true,"_uuid":"9b223435ab337d5d632fdd5be9138ff91526669c"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"600ce964904ee0d1354f572af41c1d59da54d2c5"},"cell_type":"markdown","source":"Our objective is to predict the variable `class` using the other 22 variables, where \"p\" stands for poisonous and \"e\" stands for edible. Here are what the letters stand for if you are interested:\n1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n4. bruises: bruises=t,no=f\n5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n6. gill-attachment: attached=a,descending=d,free=f,notched=n\n7. gill-spacing: close=c,crowded=w,distant=d\n8. gill-size: broad=b,narrow=n\n9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n10. stalk-shape: enlarging=e,tapering=t\n11. stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n16. veil-type: partial=p,universal=u\n17. veil-color: brown=n,orange=o,white=w,yellow=y\n18. ring-number: none=n,one=o,two=t\n19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n21. population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n22. habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d"},{"metadata":{"_uuid":"01239d884bc7ff64a7b0c70d0f4eb6685cdeebf5"},"cell_type":"markdown","source":"We remove NA values and recode the response variable: e to edible, p to poisonous "},{"metadata":{"trusted":true,"_uuid":"097a3f0a72cbe6db25dd44a7293d3d1e3c8a6112"},"cell_type":"code","source":"# Remove NA values\ndata = data.dropna()\n\n# Recode response variable\ndata.loc[data.iloc[:,0]=='e','class'] = 'edible'\ndata.loc[data.iloc[:,0]=='p','class'] = 'poisonous'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5199b1d9946c0564926d0a734916de845001be81"},"cell_type":"markdown","source":"We may plot barplots for each variable grouped by its response value (poisonous or edible) in order to get an idea of variables that are most significant:\n"},{"metadata":{"trusted":true,"_uuid":"c4eb74330f67174b90bf3be31171c27d575c6994"},"cell_type":"code","source":"sns.set(style=\"darkgrid\")\nnames = data.columns\n\n# Plot each variable's proportion by level, according to their class (poisonous or edible)\nfor k in range(4):\n    fig, axe = plt.subplots(2, 3, figsize=(20, 25))\n    for i in range(1+k*(6),7+k*(6)): \n        if i == 23:\n            break\n        prop_df = (data.iloc[:,i].groupby(data.iloc[:,0]).value_counts(normalize=True).rename('proportion').reset_index())\n        if i-k*(6)<4:\n            sns.barplot(hue=prop_df.iloc[:,1], x=prop_df.iloc[:,0], y=prop_df.iloc[:,2], data=prop_df, ax=axe[0][i-k*(6)-1]).set_title(names[i])\n        else:\n            sns.barplot(hue=prop_df.iloc[:,1], x=prop_df.iloc[:,0], y=prop_df.iloc[:,2], data=prop_df, ax=axe[1][i-k*(6)-3-1]).set_title(names[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6620d05b0e3bcf5fc6cee0bb79ecc076acfceec0"},"cell_type":"markdown","source":"From the barplots, we can make some interesting observations:\n* Edible mushrooms tend to have bruises, have either no smell or smell like almonds and anise, have a broad gill size and have a pendant ring-type\n* Poisonous mushrooms tend to have spore print of colours white or chocolate\n* The veil-type of a mushroom is probably not useful in determining if it is poisonous or edible\n\nWe can make such observations from barplots by quickly glancing through and looking out for levels of a variable that are highly present in one class and absent in the other class."},{"metadata":{"_uuid":"477b1bfb8fb1d173e5b9994bd0c45ab271b021ec"},"cell_type":"markdown","source":"## **Data Pre-processing**"},{"metadata":{"_uuid":"6df8e3729bd225ed9e525e7c60883debe0fe606b"},"cell_type":"markdown","source":"We separate the predictor variables and response variable into `x` and `y`. `x` and `y` were then split into 70%-30% training set and test sets. Whenever possible, we set `random_state = 10` to ensure the code's reproducibility. We convert the categorical variables into dummy variables. We also check for variables' levels to ensure that each level of a variable appears in both the training and test sets."},{"metadata":{"trusted":true,"_uuid":"388309d9a6751a5f80bb3301f436a20326709aa9"},"cell_type":"code","source":"# =============================================================================\n# Data Pre-processing\n# =============================================================================\n# Separate into predictor variables and response variable\nx = (data.iloc[:,1:])\ny = (data.iloc[:,0])\n\n# Obtain train and test sets, set seed to 10\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)\n\n# Checking for variables' levels\ntrain_level = x_train.describe().iloc[1,:]\ntest_level = x_test.describe().iloc[1,:]\n\n# Negate since we want the columns that are different\ntruth = ~(train_level == test_level)\n\ncol = x_train.columns[truth][0]\nprint(x_train.loc[:,col].value_counts())\nprint(x_test.loc[:,col].value_counts())\n\n# Convert categorical variables into dummy variables of type int\nx_traindummy = pd.get_dummies(x_train, drop_first=True, dtype=int)\nx_testdummy = pd.get_dummies(x_test, drop_first=True, dtype=int)\n\n# Encode response variable to 0 and 1\nEncoder_y = LabelEncoder()\ny_trainencoded = Encoder_y.fit_transform(y_train)\ny_testencoded = Encoder_y.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba6699238c47a533088aa4f97869e618384608f4"},"cell_type":"markdown","source":"We use `pandas.DataFrame.align` since some levels of some factors do not appear in both the training set and test set. More specifically, the `cap-surface` variable has 4 levels (y, s, f, g) but only 3 (y, s, f)  appear in the test set. This will create problems when we convert to dummy variables since a column is created for each level of a variable which means that the test set will have 1 less column."},{"metadata":{"trusted":true,"_uuid":"5fd9485129f27531493f1e58a119d89ae3898ba7"},"cell_type":"code","source":"# Align both train and test sets to ensure columns are the same (Since test-set's cap-surface variable has only 3 levels while train-set has 4)\nx_finaltrain, x_finaltest = x_traindummy.align(x_testdummy, fill_value=np.int32(0), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e4fe38815aa23a1d485863f050b4087801cf09a"},"cell_type":"code","source":"data.iloc[:,0].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b028d99bb109412d51deae18bf9daa4fb1b60587"},"cell_type":"markdown","source":"The classes are mostly balanced which is a good thing, since many problems may arise from imbalanced data. Next, we train models and use them for predictions."},{"metadata":{"trusted":true,"_uuid":"b3241975ca5e5871ecac03c8d613b1cbe1a59f82"},"cell_type":"markdown","source":"For each model, we tune some hyperparameter using 10-fold cross validation. In each iteration, the training set (`k -1` folds) is standardised and its mean and standard deviation is applied to the test set (`k-th` fold). By using sklearn's built-in `pipeline`, this process can be greatly simplified. Also, we need to convert the data type in our training and test sets from `int` to `float`. Otherwise, `sklearn.preprocessing.StandardScaler` would produce an error."},{"metadata":{"trusted":true,"_uuid":"e6ac031e97588bfc8528d8f059db1aa3608ae118"},"cell_type":"code","source":"# Convert data type from int to float otherwise there would be DataConversionWarning from StandardScaler\nx_finaltrain = x_finaltrain.astype(float)\nx_finaltest = x_finaltest.astype(float)\n\n# Cross-validation to determine parameter for regularisation\n# Split into 10 folds\nkf = KFold(n_splits=10, shuffle=True, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed650c878b3b4a98af3291943f04155d86239bfe"},"cell_type":"markdown","source":"## **Logistic Regression**\n\nWe apply Logistic Regression with L2-norm as the penalty. This would shrink the coefficients close to 0 and help to prevent overfitting and reduce variance.. I initially used L1-norm as penalty but I do not think it is appropriate since each column is a **level** of a variable, **not** a variable. It does not make sense to select only levels of a variable and not a variable itself for model fitting. \n\nWe cross-validate 20 values of `C` to find the most appropriate value for regularization strength. Large values of `C` correspond to weak regularization while small values of `C` correspond to strong regularization. "},{"metadata":{"trusted":true,"_uuid":"0046b6c36c2ea9fe5a126833a21b2d4b4bb08dde"},"cell_type":"code","source":"# =============================================================================\n# Logistic Regression with Ridge\n# =============================================================================\n# Range of parameters to test\nc_logreg = np.linspace(0,2,21)\nc_logreg = c_logreg[1:] # We don't want 0 since 1/0 will produce an error\n\n# Does a grid search over all parameter values and refits entire dataset using best parameters \nparameterslogreg = {'clf__C':c_logreg}\npipelogreg = Pipeline([('scale', StandardScaler()), ('clf', LogisticRegression(random_state=10, penalty='l2', solver='liblinear'))])\nlogreg = GridSearchCV(pipelogreg, parameterslogreg, cv=kf)\nlogreg.fit(x_finaltrain, y_trainencoded)\n\nprint(\"The confusion matrix is\")\nprint(confusion_matrix(y_testencoded, logreg.predict(x_finaltest)))\nprint(\"Logistic regression with L2 norm accuracy is\", logreg.score(x_finaltest, y_testencoded))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ef435fd85997b8c3b03ccea15d9d6a158cd23a4"},"cell_type":"markdown","source":"## **Adaboost Decision Tree**\n\nBoosting refers to training several weak learners and combining them to form a strong learner by reducing **bias**. Here, we tune 3 hyperparameters. `depth` refers to the depth of each weak learner, with `depth = 1` being a decision stump. `num_est` refers to the number of weak learners to train and `rate` refers to the learning rate which determines the contribution of each weak learner. \n\nSince there is more than 1 hyperparameter, `sklearn.model_selection.GridSearchCV` performs a grid search by trying all possible combinations of hyperparameters. Since there are 3 options for each hyperparameter, it will evaluate 9 potential models during cross-validation. Naturally, if we increase the number of parameters to test, the number of models to be evaluated also increases which leads to longer training time. Thus, I tried to select possible values for each parameter over a wide range of values."},{"metadata":{"trusted":true,"_uuid":"1e5cdfa6f60cea8cdfd621efb43858dfb9c05a5c"},"cell_type":"code","source":"# =============================================================================\n# Adaboost classification tree\n# =============================================================================\n# Range of parameters to test\ndepth = [1,3,5]\nnum_est = [50, 100, 150]\nrate = [0.0001, 0.01, 1]\n\n# Does a grid search over all parameter values and refits entire dataset using best parameters \nparameterstree = {'clf__base_estimator__max_depth':depth, 'clf__n_estimators':num_est, 'clf__learning_rate':rate}\nDTC = DecisionTreeClassifier(random_state=10)\nABC = AdaBoostClassifier(base_estimator = DTC, random_state=10)\npipeada = Pipeline([('scale', StandardScaler()), ('clf', ABC)]) \nadatree = GridSearchCV(pipeada, parameterstree, cv=kf)\nadatree.fit(x_finaltrain, y_trainencoded) \n\nprint(\"The confusion matrix is\")\nprint(confusion_matrix(y_testencoded, adatree.predict(x_finaltest)))\nprint(\"Adaboosted classification tree accuracy is\", adatree.score(x_finaltest, y_testencoded))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de35abe203f0b826b0dfad90cd3b8a8932ed2ff2"},"cell_type":"markdown","source":"## **Random Forest**\nRandom forests build on the idea of bagging, which stands for bootstrap aggregating. In contrast to boosting, bagging works by training several strong learners and averaging their predictions to reduce **variance**. Similarly, `depth` refers to the depth of each tree, with stronger learners usually having greater depth than weak learners. `trees` refers to the number of strong learners to train. When growing a tree, each time a split in the tree is considered, a **random** selection of `m` variables is chosen as potential splits where `m < p` and `p` is the full set of variables. This helps to de-correlate each tree to reduce variance when averaging and gives *Random* forest its name.\n\n`m` can be chosen to be anything and here, we try $m =\\sqrt{p}$ and  $m =$ log<sub>2</sub>($p$). Once again, `GridSearchCV` does the heavy lifting for us."},{"metadata":{"trusted":true,"_uuid":"e1f3d436b31e60b8805222e44b0139664e93709b"},"cell_type":"code","source":"# =============================================================================\n# Random Forest\n# =============================================================================\n# Range of parameters to test\ndepth = np.linspace(1,10,4)\ntrees = np.linspace(100,400,4)\ntrees = trees.astype(np.int64)\nm = ['sqrt','log2']\n\n# Does a grid search over all parameter values and refits entire dataset using best parameters \nparametersforest = {'clf__n_estimators':trees, 'clf__max_depth':depth, 'clf__max_features':m}\npipeforest = Pipeline([('scale', StandardScaler()), ('clf', RandomForestClassifier(random_state=10))]) \nrforest = GridSearchCV(pipeforest, parametersforest, cv=kf)\nrforest.fit(x_finaltrain, y_trainencoded)\n\nprint(\"The confusion matrix is\")\nprint(confusion_matrix(y_testencoded, rforest.predict(x_finaltest)))\nprint(\"Random forest accuracy is\", rforest.score(x_finaltest, y_testencoded))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae39fd19231c9f55b2925643458d3f002378d403"},"cell_type":"markdown","source":"## **Support Vector Machine**\nSVMs deal with the concept of margins. When presented with data (assume binary classification), the model attempts to find a hyperplane to separate the two classes. In 2 dimensions, a hyperplane is a line. If it succeeds, the data is linearly separable and the model will try to find the best line such that data points from both classes are furthest from each other. In other words, it **maximizes** the **margin**. Usually, data contains nosie and is not perfectly linearly separable. No straight line would be able to separate it. Support vector *classifiers* overcome this by allowing **soft** margins: margins that allow some points to be incorrectly classified. The amount of misclassification allowed is the hyperparameter `C` to be tuned. Small values of `C` correspond to less misclassification and stronger regularization. \n\nSometimes, a linear boundary will not work regardless of the value of `C` because the data is non-linear. In this case, support vector *machines* are used and they overcome this by using kernel methods. The idea is to transform the data into higher dimensions and finding a hyperplane in that dimension. This may sound computationally expensive but thanks to kernel methods, only the inner products of data transformed into that dimensional space are needed.\n\nPreviously, we have seen the accuracy of logistic regression on the data. Logistic regression has a linear decision boundary i.e it will not perform well on data with non-linear boundary. This suggests that the data is linearly separable and thus I chose `kernel = 'linear'` for this model. Cross-validation is also performed over 11 values of `C`."},{"metadata":{"trusted":true,"_uuid":"7b109a1b2f94f95939714eff6cd919e326570b7a"},"cell_type":"code","source":"# =============================================================================\n# SVM\n# =============================================================================\n# Range of parameters to test\nc_svc = np.linspace(-5, 5, 11)\nc_svc = [10**i for i in c_svc]\n\n# Does a grid search over all parameter values and refits entire dataset using best parameters \nparameterssvc = {'clf__C':c_svc}\npipesvc = Pipeline([('scale', StandardScaler()), ('clf', SVC(random_state=10, kernel='linear'))]) \nsvc = GridSearchCV(pipesvc, parameterssvc, cv=kf)\nsvc.fit(x_finaltrain, y_trainencoded)\n\nprint(\"The confusion matrix is\")\nprint(confusion_matrix(y_testencoded, svc.predict(x_finaltest)))\nprint(\"Support vector classifier with linear kernel accuracy is\", svc.score(x_finaltest, y_testencoded))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d84fa9bfcf177907bd77ea91515e34464bfefc8"},"cell_type":"markdown","source":"## **Conclusion**\nIt appears that all 4 models performed perfectly on the test set. It is likely that this data set is very nicely separated. This kernel also illustrates that simpler models such as logistic regression may perform as well as more complicated models such as SVMs. It it thus advisable to try simpler models first as they are easily understood, more interpretable and potentially have shorter training times before moving on to more advanced models such as neural networks or SVMs."},{"metadata":{"trusted":true,"_uuid":"1fab4b118beda8c890a2dfa0cad8e26f41b89ede"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}