{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Install missing packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport unicodedata as ud\nimport fasttext\nimport re\nimport scispacy\nimport spacy\nimport requests\n\nfrom typing import Dict\nfrom itertools import chain\nfrom tqdm.notebook import tqdm\nfrom string import punctuation\nfrom html import unescape\nfrom ftfy import fix_text\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import PorterStemmer\nfrom Levenshtein import distance\n\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThe purpose of the notebook is to clean the CORD-19 dataset in such a way that it can be utilized by any English-based pre-trained model. The work here is the result of [Evidence Prime](https://evidenceprime.com/) team. We work on the project called [LASER](https://evidenceprime.com/laser/), which aims to track scientific literature and synthesize the knowledge about the given topic. We share our experience & knowledge in the notebook, hoping that it will inspire the community to fight COVID-19."},{"metadata":{},"cell_type":"markdown","source":"## Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\n    filepath_or_buffer='/kaggle/input/CORD-19-research-challenge/metadata.csv', \n    encoding='utf-8',\n    delimiter=',',\n)\n\ndf.fillna({'title': '', 'abstract': ''}, inplace=True)\nprint(f'Number of studies: {len(df)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Clean data"},{"metadata":{},"cell_type":"markdown","source":"### Remove empty records\nSome records might miss titles and abstracts, but they have full texts. Such records would be discarded anyway in the first stage of screening, hence I remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_mask = (df['title'].str.strip() == '') & (df['abstract'].str.strip() == '')\nprint(f'Number of empty studies: {sum(empty_mask)}')\n\ndf = df.loc[~empty_mask]\nprint(f'Number of studies: {len(df)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unescape HTML chars\n- I perform \"double\" unescaping to transform `&amp;lt;` into `<`\n- I want to get rid of `&nbsp;` first to combine `text` column properly from `title` & `abstract`. Otherwise, I might not detect all empty strings. I need such detection in the `text` column concatenation algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'] = df['title'].progress_apply(\n    lambda title: unescape(unescape(title))\n)\n\ndf['abstract'] = df['abstract'].progress_apply(\n    lambda abstract: unescape(unescape(abstract))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove non-english studies\nI decided to remove non-English studies because pre-trained models are mostly language-specific. Although TF-IDF algorithm can be considered an exception, take in mind that the tokenization algorithm still depends on the language. Empty text is mapped into `en` language using the FastText model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = fasttext.load_model('/kaggle/input/language-identification/lid.176.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title_lang'] = df['title'].progress_apply(\n    lambda title: model.predict(title.lower(), k=1)[0][0].split('__label__')[1]\n)\n\ndf['title_lang'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['abstract_lang'] = df['abstract'].progress_apply(\n    lambda abstract: model.predict(abstract.lower(), k=1)[0][0].split('__label__')[1]\n)\n\ndf['abstract_lang'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some titles and abstracts have different languages. I decided to keep a study if at least one of them contains English text."},{"metadata":{"trusted":true},"cell_type":"code","source":"title_en_mask = df['title_lang'] == 'en'\ndf.loc[~title_en_mask, 'title'] = ''\nprint(f'Number of non-english titles: {sum(~title_en_mask)}')\n\nabstract_en_mask = df['abstract_lang'] == 'en'\ndf.loc[~abstract_en_mask, 'abstract'] = ''\nprint(f'Number of non-english abstracts: {sum(~abstract_en_mask)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Concat `title` and `abstract` columns into `text` column\nSome papers (e.g. [SWIFT-Review: a text-mining workbench for systematic review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877757/)) treat the columns separately.  I decided to process further a single-column `text` for code clarity. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def append_dot(text: str) -> str:\n    return text if re.search(r'[.?!]$', text) or text == '' else text + '.'\n\ndf['title'] = df['title'].str.strip().progress_apply(append_dot)\ndf['abstract'] = df['abstract'].str.strip().progress_apply(append_dot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some abstracts start with `Abstract`. I decided to remove such a prefix."},{"metadata":{"trusted":true},"cell_type":"code","source":"abstract_mask = df['abstract'].str.startswith('Abstract')\nprint(f\"There are {sum(abstract_mask)} abstracts that start with word `abstract`.\")\n\ndf.loc[abstract_mask, 'abstract'] = (df\n    .loc[abstract_mask, 'abstract']\n    .progress_apply(lambda text: text[8:])\n    .str.strip()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's create the `text` column and drop non-English texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = (df['title'] + ' ' + df['abstract']).str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_mask = df['text'] == ''\nprint(f'Number of empty studies: {sum(empty_mask)}')\n\ndf = df.loc[~empty_mask]\nprint(f'Number of studies: {len(df)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Find HTML tags\nThe text contains HTML tags that are useful for people because they support text with formatting information. As far as I know, current language models don't utilize such knowledge, thus I just extract text from tags."},{"metadata":{"trusted":true},"cell_type":"code","source":"html_mask = df['text'].progress_apply(\n    lambda text: bool(BeautifulSoup(text, \"html.parser\").find())\n)\n\nprint(f'Found {sum(html_mask)} studies with HTML tags.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove HTML tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].progress_apply(\n    lambda text: BeautifulSoup(text, \"html.parser\").get_text()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find hyperlinks\nThe text contains hyperlinks that bring no information to pre-trained language models. In a worst-case scenario such hyperlinks might introduce out-of-distribution errors to pre-trained model, thus I decided to remove them from the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperlink_pattern = re.compile('http://\\S+|https://\\S+')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperlink_mask = df['text'].progress_apply(\n    lambda text: bool(re.search(hyperlink_pattern, text))\n)\n\nprint(f'Found {sum(hyperlink_mask)} studies with hyperlinks.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove hyperlinks"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].progress_apply(\n    lambda text: re.sub(hyperlink_pattern, '', text)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize encoding\nStudies are crawled / downloaded from multiple databases and likely processed by many tools like EndNote or Covidence. As the result contains a lot of [mojibakes](https://en.wikipedia.org/wiki/Mojibake). I used a `ftfy` package to fix some of them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].progress_apply(lambda text: fix_text(text, normalization='NFKC'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I suspect that it doesn't fix all the cases (this shows my experience), therefore I decided to fix manually some of them. I found decoding tables online to help with this task. I crawl & use only the first one.\n* https://www.i18nqa.com/debug/utf8-debug.html\n* http://string-functions.com/encodingindex.aspx"},{"metadata":{"trusted":true},"cell_type":"code","source":"response = requests.get('https://www.i18nqa.com/debug/utf8-debug.html')\nresponse.encoding = 'UTF-8'  # By default is ISO-8859-1 (aka. Latin1) -> See RFC2854\nif response.status_code != 200:\n    print(\"Couldn't download a webpage source.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soup = BeautifulSoup(\n    markup=response.text, \n    features='html.parser'\n)\n\ntable = soup.find(\"table\", attrs={\"id\": \"dbg\"})\n\nmapping = []\nfor row in table.findAll(\"tr\"):\n    cells = row.findAll(\"td\", attrs={\"class\": \"ch\"})\n    if cells:\n        if cells[1].text != '' and cells[0].text != '':\n            mapping.append((cells[1].text, cells[0].text))\n        \n        if cells[3].text != '' and cells[2].text != '':\n            mapping.append((cells[3].text, cells[2].text))\n            \ndf_mapping = pd.DataFrame(mapping, columns=['key', 'value'])\ndf_mapping.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also don't trust the table fully. Sometimes I found in the text [Angstrom unit](https://en.wikipedia.org/wiki/Angstrom) that according to the table it should be mapped into `Š`. Therefore I drop the keys with single char after striping the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mapping = df_mapping[df_mapping['key'].str.strip().apply(len) > 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is it a valid mapping?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mapping_agg = df_mapping.groupby('key')['value'].apply(list)\ndf_mapping_agg[df_mapping_agg.apply(len) > 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes! Let's make an encoding mapper then!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mapping['key'] = df_mapping['key'].apply(lambda key: ud.normalize('NFKC', key))\ndf_mapping['value'] = df_mapping['value'].apply(lambda value: ud.normalize('NFKC', value))\n\nencoding_mapping = dict(zip(df_mapping['key'], df_mapping['value']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find encoding table issues"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_pattern = '|'.join(encoding_mapping.keys())\nencoding_mask = df['text'].str.contains(encoding_pattern)\n\nprint(f'Found {sum(encoding_mask)} studies with encoding issues.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fix encoding table issues"},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_multiple(text: str, mapping: Dict[str, str]) -> str:\n    for key in mapping:\n        text = text.replace(key, mapping[key])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].progress_apply(lambda text: replace_multiple(text, encoding_mapping))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find non-unicode chars\nI couldn't identify all broken chars. Let's investigate non-unicode chars and fix them."},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_nonunicode_char(char):\n    try:\n        ud.name(char)\n    except ValueError:\n        return True\n    \n    return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nonunicode_chars = df['text'].progress_apply(\n    lambda text: [char for char in text if is_nonunicode_char(char)]\n)\n\nnonunicode_chars_counts = pd.Series(chain(*nonunicode_chars)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nonunicode_chars_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Investigate non-unicode chars"},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_colwidth', -1):    \n    char_mask = df['text'].str.contains('')\n    display(df.loc[char_mask, 'text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replace non-unicode chars"},{"metadata":{"trusted":true},"cell_type":"code","source":"unicode_mapping = {\n    '': '-',\n    '': '',\n    '': '=',\n    '': '=',\n    '': ''\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].progress_apply(lambda text: replace_multiple(text, unicode_mapping))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find non-ascii chars\nThe Unicode standard is too rich for people to use. As the result, people use different chars for the same thing (e.g. `α`, `ɑ`). I believe that deep learning models can learn such meaning, but char simplification makes it easier for sure. This way we could also identify some encoding issues that couldn't be fixed previously."},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_nonascii_char(char: str) -> bool:\n    try:\n        char.encode('ascii')\n    except ValueError:\n        return True\n    \n    return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nonascii_df(df: pd.DataFrame) -> pd.DataFrame:\n    nonascii_chars = df['text'].progress_apply(\n        lambda text: [\n            (char, ud.category(char), ord(char)) \n            for char in text \n            if is_nonascii_char(char)\n        ]\n    )\n\n    df_nonascii_chars = pd.DataFrame(\n        data=chain(*nonascii_chars), \n        columns=['char', 'category', 'ord']\n    )\n    \n    return (df_nonascii_chars\n        .groupby('category')['char']\n        .apply(set)\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I decided to first fix some studies with Chinese chars from `Lo` Unicode category."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[10993, 'text'] = \"\"\"Traditional usages, botany, phytochemistry, pharmacology and toxicology of Polygonum multiflorum Thunb.: A review. Ethnopharmacological relevance Polygonum multiflorum Thunb., which is known as Heshouwu in China. It is traditionally valued and reported for hair-blacking, liver and kidney-tonifying and anti-aging effects as well as low toxicity. The aim of this review is to provide comprehensive information on the botany, traditional uses, phytochemistry, pharmacological research and toxicology of Polygonum multiflorum, based on the scientific literature. Moreover, trends and perspectives for future investigation of this plant are discussed. It will build up a new foundation for further study on Polygonum multiflorum. Materials and methods A systematic review of the literature on Polygonum multiflorum was performed using several resources, including classic books on Chinese herbal medicine and various scientific databases, such as PubMed, SciFinder, the Web of Science, Science Direct, China Knowledge Resource Integrated (CNKI). Results Polygonum multiflorum is widely distributed throughout the world and has been used as a traditional medicine for centuries in China. The ethnomedical uses of Polygonum multiflorum have been recorded in many provinces of China and Japan for nine species of adulterants in six families. More than 100 chemical compounds have been isolated from this plant, and the major components have been determined to be stilbenes, quinones, flavonoids and others. Crude extracts and pure compounds of this plant are used as effective agents in pre-clinical and clinical practice due to their anti-aging, anti-hyperlipidaemia, anti-cancer and anti-inflammatory effects and to promote immunomodulation, neuroprotection, and the curing of other diseases. However, these extracts can also lead to hepatotoxicity, nephrotoxicity and embryonic toxicity. Pharmacokinetic studies have demonstrated that the main components of Polygonum multiflorum, such as 2,3,5,4′-tetrahydroxystilbene-2-O-β-d-glucopyranoside and emodin are distributed among many organs and tissues. Conclusion Therapeutic potential of Polygonum multiflorum has been demonstrated in the conditions like Alzheimer׳s disease, Parkinson׳s disease, hyperlipidaemia, inflammation and cancer, which is attributed to the presence of various stilbenes, quinones, flavonoids, phospholipids and other compounds in the drug. On the other hand, the adverse effects (hepatotoxicity, nephrotoxicity, and embryonic toxicity) of this plant were caused by the quinones, such as emodin and rhein. Thus more pharmacological and toxicological mechanisms on main active compounds are necessary to be explored, especially the combined anthraquinones (Emodin-8-O-β-d-glucopyranoside, Physcion-8-O-β-d-glucopyranoside, etc.) and the variety of stilbenes.\"\"\"\ndf.loc[14871, 'text'] = \"\"\"YouTube as source of information on 2019 novel coronavirus outbreak: A cross sectional study of English and Mandarin content. Background The current 2019 novel coronavirus outbreak is rapidly evolving. YouTube has been recognized as a popular source of information in previous disease outbreaks. We analyzed the content on YouTube about n-CoV in English and Mandarin languages. Methods YouTube was searched using the terms '2019 novel coronavirus', 'Wuhan virus' on 1st and 2nd February 2020. First 50 videos in each group were analyzed. Videos in other languages, duplicate videos, those without an audio and duration >15 min were excluded .72 videos in English and 42 in Mandarin were reviewed. 2 reviewers classified the videos as useful, misleading or news based on pre specified criterion. Inter-observer agreement was evaluated with kappa coefficient. Modified DISCERN index for reliability and medical information and content index (MICI) score were used for content analysis. Results These videos attracted cumulative 21,288,856 views. 67% of English and 50% Mandarin videos had useful information. The viewership of misleading Mandarin videos was higher than the useful ones. WHO accounted for only 4% of useful videos. Mean DISCERN score for reliability was 3.12/5 and 3.25/5 for English and Mandarin videos respectively. Mean cumulative MICI score of useful videos was low (6.71/25 for English and 6.28/25 for Mandarin). Conclusions YouTube viewership during 2019 n-CoV outbreak is higher than previous outbreaks. The medical content of videos is suboptimal International health agencies are underrepresented. Given its popularity, YouTube should be considered as important platform for information dissemination.\"\"\"\ndf.loc[15931, 'text'] = \"\"\"Traditional Chinese medicine herbal extracts of Cibotium barometz, Gentiana scabra, Dioscorea batatas, Cassia tora, and Taxillus chinensis inhibit SARS-CoV replication. Development of anti-severe acute respiratory syndrome associated coronavirus (SARS-CoV) agents is pivotal to prevent the reemergence of the life-threatening disease, SARS. In this study, more than 200 extracts from Chinese medicinal herbs were evaluated for anti-SARS-CoV activities using a cell-based assay that measured SARS-CoV-induced cytopathogenic effect (CPE) in vitro on Vero E6 cells. Six herbal extracts, one each from Gentianae Radix (lóng dǎn; the dried rhizome of Gentiana scabra), Dioscoreae Rhizoma (shān yào; the tuber of Dioscorea batatas), Cassiae Semen (jué míng zǐ; the dried seed of Cassia tora) and Loranthi Ramus (sāng jì shēng; the dried stem, with leaf of Taxillus chinensis) (designated as GSH, DBM, CTH and TCH, respectively), and two from Rhizoma Cibotii (gǒu jǐ; the dried rhizome of Cibotium barometz) (designated as CBE and CBM), were found to be potent inhibitors of SARS-CoV at concentrations between 25 and 200μg/ml. The concentrations of the six extracts needed to inhibit 50% of Vero E6 cell proliferation (CC50) and 50% of viral replication (EC50) were determined. The resulting selective index values (SI=CC50/EC50) of the most effective extracts CBE, GSH, DBM, CTH and TCH were>59.4,> 57.5,> 62.1,> 59.4, and>92.9, respectively. Among these extracts, CBM and DBM also showed significant inhibition of SARS-CoV 3CL protease activity with IC50 values of 39μg/ml and 44μg/ml, respectively. Our findings suggest that these six herbal extracts may have potential as candidates for future development of anti-SARS therapeutics.\"\"\"\ndf.loc[15932, 'text'] = \"\"\"Antiviral Decoction of Isatidis Radix (bǎn lán gēn) Inhibited Influenza Virus Adsorption on MDCK Cells by Cytoprotective Activity. The aim of this study is to elucidate how the Isatidis Radix (bǎn lán gēn) tonic, as an aqueous mixture of hundreds of compositions, interrupts the infection of influenza viruses to their host cells. The efficacy of the tonic was evaluated and expressed as cell proliferation rate and plaque reduction rate in Madin-Darby Canine Kidney (MDCK) cells, against 3 strains of influenza A and B viruses. This boiling water (at 100°C) extract of Isatidis Radix (RIE) showed antiviral activity against influenza virus A and B. The concentration for 50% inhibition of influenza virus A replication (IC50) in MDCK cell was 12.6mg/mL with a therapeutic index >8. When cells were incubated with RIE prior to virus adsorption, the numbers of viable cell were at least doubled compared to the numbers of virus control, RIE incubation after virus adsorption and RIE incubation with virus prior to adsorption, in both influenza virus A and B. Moreover, much less virus particles were spotted by scanning electron microscope (SEM) in the RIE pre-treated cells than the cells without RIE treatment. These results indicate the antiviral activity of RIE is mainly attributed to its host cell protection effect but not actions on virus or post-virus-adsorption interruption. Cell, but not virus, is more likely to be the action target of RIE.\"\"\"\ndf.loc[20310, 'text'] = \"\"\"Antiviral Decoction of Isatidis Radix (bǎn lán gēn) Inhibited Influenza Virus Adsorption on MDCK Cells by Cytoprotective Activity. The aim of this study is to elucidate how the Isatidis Radix (bǎn lán gēn) tonic, as an aqueous mixture of hundreds of compositions, interrupts the infection of influenza viruses to their host cells. The efficacy of the tonic was evaluated and expressed as cell proliferation rate and plaque reduction rate in Madin-Darby Canine Kidney (MDCK) cells, against 3 strains of influenza A and B viruses. This boiling water (at 100°C) extract of Isatidis Radix (RIE) showed antiviral activity against influenza virus A and B. The concentration for 50% inhibition of influenza virus A replication (IC(50)) in MDCK cell was 12.6 mg/mL with a therapeutic index >8. When cells were incubated with RIE prior to virus adsorption, the numbers of viable cell were at least doubled compared to the numbers of virus control, RIE incubation after virus adsorption and RIE incubation with virus prior to adsorption, in both influenza virus A and B. Moreover, much less virus particles were spotted by scanning electron microscope (SEM) in the RIE pre-treated cells than the cells without RIE treatment. These results indicate the antiviral activity of RIE is mainly attributed to its host cell protection effect but not actions on virus or post-virus-adsorption interruption. Cell, but not virus, is more likely to be the action target of RIE.\"\"\"\ndf.loc[20309, 'text'] = \"\"\"Traditional Chinese medicine herbal extracts of Cibotium barometz, Gentiana scabra, Dioscorea batatas, Cassia tora, and Taxillus chinensis inhibit SARS-CoV replication. Development of anti-severe acute respiratory syndrome associated coronavirus (SARS-CoV) agents is pivotal to prevent the reemergence of the life-threatening disease, SARS. In this study, more than 200 extracts from Chinese medicinal herbs were evaluated for anti-SARS-CoV activities using a cell-based assay that measured SARS-CoV-induced cytopathogenic effect (CPE) in vitro on Vero E6 cells. Six herbal extracts, one each from Gentianae Radix (lóng dǎn; the dried rhizome of Gentiana scabra), Dioscoreae Rhizoma (shān yào; the tuber of Dioscorea batatas), Cassiae Semen (jué míng zǐ; the dried seed of Cassia tora) and Loranthi Ramus (sāng jì shēng; the dried stem, with leaf of Taxillus chinensis) (designated as GSH, DBM, CTH and TCH, respectively), and two from Rhizoma Cibotii (gǒu jǐ; the dried rhizome of Cibotium barometz) (designated as CBE and CBM), were found to be potent inhibitors of SARS-CoV at concentrations between 25 and 200 μg/ml. The concentrations of the six extracts needed to inhibit 50% of Vero E6 cell proliferation (CC(50)) and 50% of viral replication (EC(50)) were determined. The resulting selective index values (SI = CC(50)/EC(50)) of the most effective extracts CBE, GSH, DBM, CTH and TCH were > 59.4, > 57.5, > 62.1, > 59.4, and > 92.9, respectively. Among these extracts, CBM and DBM also showed significant inhibition of SARS-CoV 3CL protease activity with IC(50) values of 39 μg/ml and 44 μg/ml, respectively. Our findings suggest that these six herbal extracts may have potential as candidates for future development of anti-SARS therapeutics. Abbreviations SARS, severe acute respiratory syndrome CoV, coronavirus CPE, cytopathogenic effect TCM, traditional Chinese medicine.\"\"\"\ndf.loc[25814, 'text'] = \"\"\"Suffering a Loss Is Good Fortune: Myth or Reality? We sometimes decide to take an offered option that results in apparent loss (e.g., unpaid overtime). Mainstream decision theory does not predict or explain this as a choice we want to make, whereas such a choice has long been described and highly regarded by the traditional Chinese dogma - suffering a loss is good fortune. To explore what makes the dogma work, we developed a celebrity anecdote‐based scale to measure \"Chikui\" (suffering a loss) likelihood and found that:(i) people with higher scores on the Chikui Likelihood Scale (CLS) were more likely to report higher scores on subjective well‐being and the Socioeconomic Index for the present and (ii) the current Socioeconomic Index could be positively predicted not only by current CLS scores but also by retrospective CLS scores recalled for the past, and the predictive effect was enhanced with increasing time intervals. Our findings suggest that \"suffering a loss is good fortune\" is not a myth but a certain reality. © 2017 The Authors Journal of Behavioral Decision Making Published by John Wiley & Sons Ltd.\"\"\"\ndf.loc[43373, 'text'] = \"\"\"Performance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT. Background Despite its high sensitivity in diagnosing COVID-19 in a screening population, chest CT appearances of COVID 19 pneumonia are thought to be non-specific. Purpose To assess the performance of United States (U.S.) and Chinese radiologists in differentiating COVID-19 from viral pneumonia on chest CT. Methods A total of 219 patients with both positive COVID-19 by RT-PCR and abnormal chest CT findings were retrospectively identified from 7 Chinese hospitals in Hunan Providence, China from January 6 to February 20, 2020. A total of 205 patients with positive Respiratory Pathogen Panel for viral pneumonia and CT findings consistent with or highly suspicious for pneumonia by original radiology interpretation within 7 days of each other were identified from Rhode Island Hospital in Providence, RI. Three Chinese radiologists blindly reviewed all chest CTs (n=424) to differentiate COVID-19 from viral pneumonia. A sample of 58 age-matched cases was randomly selected and evaluated by 4 U.S. radiologists in a similar fashion. Different CT features were recorded and compared between the two groups. Results For all chest CTs, three Chinese radiologists correctly differentiated COVID-19 from non-COVID-19 pneumonia 83% (350/424), 80% (338/424), and 60% (255/424) of the time, respectively. The seven radiologists had sensitivities of 80%, 67%, 97%, 93%, 83%, 73% and 70% and specificities of 100%, 93%, 7%, 100%, 93%, 93%, 100%. Compared to non-COVID-19 pneumonia, COVID-19 pneumonia was more likely to have a peripheral distribution (80% vs. 57%, p<0.001), ground-glass opacity (91% vs. 68%, p<0.001), fine reticular opacity (56% vs. 22%, p<0.001), and vascular thickening (59% vs. 22%, p<0.001), but less likely to have a central+peripheral distribution (14.% vs. 35%, p<0.001), pleural effusion (4.1 vs. 39%, p<0.001) and lymphadenopathy (2.7% vs. 10.2%, p<0.001). Conclusion Radiologists in China and the United States distinguished COVID-19 from viral pneumonia on chest CT with high specificity but moderate sensitivity. A translation of this abstract in Farsi is available in the supplement.\"\"\"\ndf.loc[43519, 'text'] = \"\"\"Expert consensus on Pulmonary Function Testing during the epidemic of Corona Virus Disease 2019. Corona virus disease 2019 (COVID-19) is mainly transmitted by respiratory droplets and close contact. Pulmonary function testing procedures have been associated with an increasing risk of COVID-19 transmission among patients/subjects and medical staffs. Effective prevention and control strategies must be compulsorily implemented to prevent nosocomial infection. This recommendation is intended to be followed by healthcare workers (HCWs) of pulmonary function testing laboratory when COVID-19 is in epidemic. Based on the features of pulmonary function testing, precaution principles and strategies are developed in three aspects of management for HCWs, operating procedure, environment and equipment. Indications of pulmonary function testing should be followed strictly. It is strongly recommended to suspend the test for the confirmed or suspected cases of COVID-19 during the contagious stage, and to postpone the test for other patients if it is not imperative. Medical personnel should mandatorily adhere to the standard stratification of precaution measures. Patients/Subjects should be isolated in a separate area for testing. Disposable in-line filters must be used during pulmonary function testing. Cleaning and disinfection procedures for environment and equipment in pulmonary function testing laboratory should be paid more attention.\"\"\"\ndf.loc[43523, 'text'] = \"\"\"Cause analysis and treatment strategies of \"recurrence\" with novel coronavirus pneumonia (covid-19) patients after discharge from hospital. With a large number of COVID-19 patients discharging from hospital, some had showed re-fever and positive nucleic acid test after discharge from hospital. This might be due to the biological characteristics of 2019-nCoV, and might also be related to the basic disease, clinical status, glucocorticoid using, sample sampling, processing and detecting of patients, and some even related to the re-infection or secondary bacterial virus infection. Therefore, we suggest that in view of this phenomenon, further stratified management of discharge from hospital should be carried out on the basis of guidelines, especially for patients with advanced age, underlying diseases or severe or critical pneumonia. For those patients who can\\'t completely deoxygenate for a long time after hospitalization, individualized treatment methods and different discharge evaluation criteria should be adopted to ensure the complete cure of patients and prevent recurrencing after discharge from hospital.\"\"\"\ndf.loc[43798, 'text'] = \"\"\"Analysis of clinical features of 153 patients with novel coronavirus pneumonia in Chongqing. Objective To analyze the clinical data of 153 patients with novel coronavirus pneumonia (COVID-19) in chongqing ,and provide reference and thinking for the diagnosis and treatment. Methods Analyze the clinical data, laboratory examination and chest imaging characteristics of 153 COVID-19 patients in Chongqing Public Health Medical Center from January 26 to February 5, 2020. According to the relevant diagnostic criteria ,patients were divided into non-severe group(n=132) and severe group(n=21),and analyze the correlation between serum index changes and disease severity. Results Combined with diabetes and chronic respiratory diseases, the severity of the disease was statistically significant ( χ 2 =11.04 and 6.94, P <0.05). No symptoms were found in patients with mild illness ( χ 2 =4.09, P <0.05) .The proportion of fever and muscle soreness in the severe group was higher than that in the non-severe group ( χ 2 =4.40 and 22.67, P <0.05).Among the concomitant symptoms, the proportion of cough and shortness of breath in the severe group was higher than that in the non-severe group ( χ 2 =8.46 and 4.80, P <0.05).C-reactive protein and d-dimer were higher in the severe group than in the non-severe group ( t =43.44 and 37.13, P <0.05), and the number of CD 3 + T lymphocyte cells, CD 4 + T lymphocyte cells and CD 8 + T lymphocyte cells in the severe group was lower than that in the non-severe group (Z=27.25, 20.60 and 17.36, P <0.05).Compared with the non-severe group, both lungs and the right lung lower lobe were more susceptible to involved ( χ 2 =6.95 and 20.39, P <0.05) . Conclusion Severity of COVID-19 was associated with underlying disease, symptoms, site of involvement, C-reactive protein, d-dimer, CD 3 + T lymphocyte count, CD 4 + T lymphocyte count, and CD 8 + T lymphocyte count.\"\"\"\ndf.loc[43896, 'text'] = \"\"\"First case of severe childhood novel coronavirus pneumonia in China. Summary One patient with a complaint of \"intermittent diarrhea, vomiting for 6 days, fever with shortness of breath for half a day\" was referred to the Department of Critical Medicine, Wuhan Children\\'s Hospital, and was diagnosed with neonatal severe coronavirus pneumonia (NCP). Relevant databases such as China Knowledge Net, Weipu, Wanfang and other related databases were searched with the keywords of \"new coronavirus pneumonia\", \"children\", and \"critical severity\" as of February 8, 2020. This case is the first child with severe NCP in China. It started with gastrointestinal symptoms, early respiratory symptoms were not obvious, and rapidly progressed to acute respiratory distress syndrome, septic shock, and acute renal failure. The patient was negative for nucleic acid test of 2019 new-type coronavirus (2019-nCoV) for 2 consecutive consecutive throat swabs. For severe suspected cases, it is recommended to take samples of the lower respiratory tract or repeat samples of the upper respiratory tract for testing. Continuous blood purification technology can be applied to the treatment of children with severe NCP as early as possible.\"\"\"\ndf.loc[46814, 'text'] = \"\"\"Some Chinese folk prescriptions for wind-cold type common cold. Although self-limiting, the common cold (gǎn mào) is highly prevalent. There are no effective antivirals to cure the common cold and few effective measures to prevent it, However, for thousands years, Chinese people have treated the common cold with natural herbs, According to the traditional Chinese medicine (TCM) theory (zhōng yī lǐ lùn), the common cold is considered as an exterior syndrome, which can be further divided into the wind-cold type (fēng hán xíng), the wind-heat type (fēng rè xíng), and the summer heat dampness type (shǔ rè xíng). Since the most common type of common cold caught in winter and spring is the wind-cold type, the article introduced some Chinese folk prescriptions for the wind-cold type common cold with normal and weak physique, respectively. For thousands of years, Chinese folk prescriptions for the common cold, as complementary and alternative medicine (CAM; bǔ chōng yǔ tì dài yī xué), have been proven to be effective, convenient, cheap, and most importantly, safe. The Chinese folk prescriptions (zhōng guó mín jiān chǔ fāng) for the wind-cold type common cold are quite suitable for general practitioners or patients with the wind-cold type common cold, to treat the disease. Of course, their pharmacological features and mechanisms of action need to be further studied.\"\"\"\ndf.loc[45438, 'text'] = \"\"\"Anti-inflammatory and Antimicrobial Effects of Heat-clearing Chinese Herbs: A Current Review. ABSTRACT Inflammation is a normal immune response; but if the body's regulation of inflammation is dysfunctional, then it will have an adverse effect on the body. Although use of modern drugs for inflammation has a relieving effect, it is still unsatisfactory. Moreover, the emergence of drug-resistant strains and even new kinds of microorganisms is causing significant morbidity and mortality. Recently, more attention has been focused on herbal medicine to treat various diseases because of the ability of the herbs to affect multiple target signaling pathways and their multiple mechanisms of action. Thus, a large number of studies have reported on the anti-inflammatory and antimicrobial effects of the traditional Chinese herbs. Literature survey was performed by conducting systematic electronic search in PubMed, Science Direct, Google Scholar, and in books. This review has listed 11 heat-clearing Chinese herbs (HCCHs) including Scutellaria baicalensis (Huáng Qín), Coptis chinensis (Huáng Lián), Flos Lonicerae (Jīn Yín Hūa), Forsythia suspensa (Lián Qiào), Isatidis Folium (Dà Qīn Yè), Radix Isatidis (Bǎn Lán Gēn), Viola yedoensis (Zǐ Huā Dì Dīn), Pulsatilla Radix (Bái Tóu Wēn), Andrographis paniculata (Chuān Xīn Lián), Houttuynia cordata (Yú Xīng Cǎo), and Patrinia Herba (Bài Jiàn Cǎo), which have anti-inflammatory and antimicrobial effects, and has described their effects through different mechanisms of action and multiple targets. Their ability to affect multiple target signaling pathways and their potential mechanisms of action contributing to their anti-inflammatory and antimicrobial activity may be related to their action of removing heat and counteracting toxicity. Further studies are needed on the collection of HCCHs to know the detailed mechanism of action of herbs in this group for the assessment of effective drug.\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nonascii_chars = get_nonascii_df(df)\n\nwith pd.option_context('display.max_colwidth', -1):\n    display(df_nonascii_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Investigate non-ascii chars"},{"metadata":{"trusted":true},"cell_type":"code","source":"with pd.option_context('display.max_colwidth', -1):    \n    char_mask = df['text'].str.contains('≓')\n    display(df.loc[char_mask, 'text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replace non-ascii chars"},{"metadata":{"trusted":true},"cell_type":"code","source":"ascii_mapping = {\n    # Cf\n    '\\xad': '',\n    '\\u200b': '',   \n        \n    # Ll    \n    'ĺ': 'l', \n    'ç': 'c',\n    'χ': 'x', \n    'ǧ': 'g', \n    'е': 'e', \n    'ń': 'n', \n    'è': 'e',\n    'ē': 'e', \n    'ӧ': 'o', \n    'ò': 'o', \n    'ü': 'u', \n    'ș': 's', \n    'ț': 't', \n    'ñ': 'n', \n    'ū': 'u', \n    'ú': 'u', \n    'ã': 'a', \n    'â': 'a', \n    'ê': 'e', \n    'í': 'i', \n    'ć': 'c', \n    'ο': 'o', \n    'ő': 'o', \n    'ō': 'o', \n    'ς': 'c', \n    'ī': 'i', \n    'ë': 'e', \n    'ǐ': 'i', \n    'ϊ': 'i', \n    'ā': 'a', \n    'ù': 'u', \n    'ǔ': 'u', \n    'é': 'e', \n    'ý': 'y', \n    'ï': 'i', \n    'ǒ': 'o', \n    'ó': 'o', \n    'ǎ': 'a', \n    'î': 'i', \n    'œ': 'oe', \n    'ł': 'l',\n    'ă': 'a', \n    'ţ': 't', \n    'ı': 'i', \n    'ś': 's', \n    'ö': 'o', \n    'à': 'a', \n    'á': 'a', \n    'ì': 'i', \n    'ş': 's', \n    'ä': 'a', \n    'æ': 'ae', \n    'ô': 'o',\n    # Exceptions\n    'ĸ': 'κ',\n    'к': 'κ',\n    'ɛ': 'ε',\n    'ɑ': 'α',\n    \n    # Lm\n    'ʹ': \"'\", \n    'ˈ': \"'\",\n    \n    # Lu\n    'С': 'C', \n    'Á': 'A', \n    'Η': 'H', \n    'Ó': 'O', \n    'Æ': 'AE', \n    'Χ': 'X', \n    'Ö': 'O', \n    'Ş': 'S', \n    'Ț': 'T', \n    'Ι': 'I', \n    'Α': 'A', \n    'À': 'A', \n    'Ç': 'C', \n    'Ä': 'A', \n    'Μ': 'M', \n    'Ü': 'U', \n    'Λ': '^', \n    'Ý': 'Y', \n    'Î': 'I', \n    'É': 'E', \n    'Ï': 'I', \n    'Œ': 'OE', \n    'È': 'E', \n    'Ν': 'N', \n    'İ': 'I', \n    'Ș': 'S',\n    # Exceptions\n    'Σ': '∑',\n    'Ф': 'Φ',\n    \n    # Mn\n    '́': '\"', \n    '̄': '', \n    '̇': '', \n    '̈': '', \n    '̊': '', \n    '̱': '', \n    '̶': '-',\n    \n    # Pd\n    '―': '-',\n    '–': '-', \n    '‒': '-', \n    '‐': '-', \n    '—': '-', \n    \n    # Pf\n    '»': '',\n    \n    # Pf\n    '«': '',\n     \n    # Po\n    '′': \"'\",\n    '׳': \"'\",\n    '·': '.',\n    '§': '',\n    '†': '',\n    '⁎': '*',\n    '‡': '',\n    '¶': '',\n    '•': '-',\n    \n    # Sk    \n    '˂': '<',\n                                                                                                                              \n    # Sm\n    '⪢': '>=',\n    '∶': ':',\n    '⧹': '\\\\', \n    '⋍': '≈', \n    '±': '+/-', \n    '≥': '>=', \n    '⋯': '-', \n    '∕': '/', \n    '≦': '<=', \n    '∝': '∞', \n    '∣': '|', \n    '⩾': '>=', \n    '≠': '!=', \n    '∙': '*', \n    '≤': '<=', \n    '−': '-', \n    '×': '*', \n    '≪': '<<', \n    '⁄': '/', \n    '⩽': '<=', \n    '≫': '>>', \n    '∗': '*', \n    '⋅': '', \n    '∖': '\\\\', \n    '≧': '>=',\n    # Exceptions\n    '∆': 'Δ',\n\n    # So\n    '▾': ' ',\n    '□': '',\n    '●': '-',\n    '␣': ' ',\n    '▀': '-',\n    '☐': 'r',\n    '▸': '-',\n    '☆': '',\n    '®': '',   \n    '©': '',\n    '≓': '\"',\n    # Exceptions:\n    '⇌': '↔',\n    '△': 'Δ',\n    '⍺': 'α',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].progress_apply(lambda text: replace_multiple(text, ascii_mapping))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look again for non-ascii characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nonascii_chars = get_nonascii_df(df)\n\nwith pd.option_context('display.max_colwidth', -1):\n    display(df_nonascii_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Simplify text\nSimple models like TF-IDF work best if the work upon lemmatized stemmed text with no stop words [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law). Such a simplified text form will be also useful for duplicate detection."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_sci_sm')\nstemmer = PorterStemmer()\n\npunctation_mapping = dict(zip(\n    punctuation, [''] * len(punctuation)\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simplify_text(text: str) -> str:\n    # Remove stop words & puncation & lemmatize & stem\n    tokens = [\n        stemmer.stem(token.lemma_.lower())\n        for token in nlp(text) \n        if not (token.is_stop or token.is_punct)\n    ]\n\n    # Prepare text \n    text = ' '.join(tokens)    \n    \n    # Remove all punctation chars \n    text = replace_multiple(text, punctation_mapping)\n    \n    # Remove multiple spaces & strip text\n    text = re.sub(' +', ' ', text)\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_simplified'] = df['text'].progress_apply(simplify_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove duplicates\nStudies are downloaded from multiple databases. As a result, a great deal of \"duplicates\" exist. Duplicates are not easy to detect, because often there is no 1-1 relation between them. The text might contain typos or additional information. We would need fuzzy matching to detect duplicates then!\n\nIn the literature, there is a concept of studification and duplicate detection. What is the difference between those? Every study has its own life i.e. it evolves over time. Some journals have length requirements, thus they have to be truncated (e.g. some endpoints need to be removed). In addition, population size might get bigger and bigger or intermediate results are published. No one would wait 50 years to publish just one study. Furthermore, some authors might be appended or dropped. This is what we call studification. Taking all this into consideration we can define that two studies are duplicates if two studies are the same and were published the same year. What does mean the word 'same'? Well, we don't know until we go deeper into the article. We would need to extract information from the publication. What should we do then?\n\nIn the metadata we don't have correct publication years, thus we just need to rely on plain text. I decided on \"safe\" fuzzy matching with Levensthein distance. I sorted the list to avoid $n^2$ complexity by looking only on adjacent texts. I need to be careful not to drop a very important study, thus I aim for low threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted = df.sort_values(by='text_simplified').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Levensthein distances"},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_text = ''\n\nfor idx, text in tqdm(df_sorted['text_simplified'].iteritems(), total=len(df_sorted)):\n    if prev_text == '' and (text == '' or text != ''):\n        dist = 999\n    else:\n        dist = distance(prev_text, text)\n    \n    df_sorted.loc[idx, 'distance'] = dist\n    prev_text = text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find distance threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted.loc[df_sorted['distance'] <= 2000, 'distance'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted.loc[df_sorted['distance'] <= 100, 'distance'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted.loc[df_sorted['distance'] <= 20, 'distance'].hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of studies: {len(df_sorted)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted = df_sorted.loc[df_sorted['distance'] > 3]\nprint(f'Number of studies after duplicates removal: {len(df_sorted)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save results"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sorted.to_csv('/kaggle/working/cord_metadata_cleaned.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}