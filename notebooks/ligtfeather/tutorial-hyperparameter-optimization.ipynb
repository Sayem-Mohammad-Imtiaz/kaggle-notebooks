{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Optimization\n> Hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters are learned. The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. \nHyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance"},{"metadata":{},"cell_type":"markdown","source":"For each method, we'll how to search for the optimal structure of a random forest classifer. Random forests are an ensemble model comprised of a collection of decision trees.\nHyperparameters to keep in mind:\n* How many estimators (ie. decision trees) should be utilized?\n* What should be the maximum allowable depth for each decision tree?\n* What criterion to pick to measure the quality of a split?"},{"metadata":{},"cell_type":"markdown","source":"# Please upvote the kernel if you found it insightful! "},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Dataset and define features and target"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv(\"../input/mobile-price-classification/train.csv\")\nX = df.drop(\"price_range\", axis = 1).values #Features\ny = df.price_range.values #target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid Search with Random Forest Classifier\n> Grid search is essentially an optimization algorithm which lets you select the best parameters for your optimization problem from a list of parameter options that you provide, hence automating the 'trial-and-error' method. It is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"classifier = ensemble.RandomForestClassifier(n_jobs=-1) #n_jobs = -1 means using all processors.\nparam_grid = {\n    \"n_estimators\": [100, 200, 300, 400], \n    \"max_depth\": [1, 3, 7, 5],\n    \"criterion\": [\"gini\", \"entropy\"],\n}\n\nmodel = model_selection.GridSearchCV(\n    estimator=classifier,\n    param_grid=param_grid, #Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries\n    scoring=\"accuracy\", #A single str to evaluate the predictions on the test set.\n    n_jobs=1, #Number of jobs to run in parallel\n    cv=5, #Determines the cross-validation splitting strategy. If the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n)\n\nmodel.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(model.best_score_)\nprint(model.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, this is an exhaustive sampling of the hyperparameter space and can be quite inefficient."},{"metadata":{},"cell_type":"markdown","source":"# Random Search with Random Forest Classifier\n> Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. Random search differs from grid search in that we longer provide a discrete set of values to explore for each hyperparameter; rather, we provide a statistical distribution for each hyperparameter from which values may be randomly sampled. \nWe can also define how many iterations we'd like to build when searching for the optimal model."},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = ensemble.RandomForestClassifier(n_jobs=-1)\nparam_grid = {\n        \"n_estimators\": np.arange(100, 1500, 100),\n        \"max_depth\": np.arange(1, 20),\n        \"criterion\": [\"gini\", \"entropy\"],\n    }\n# Random search is not as expensive as grid search\nmodel = model_selection.RandomizedSearchCV(\n    estimator=classifier,\n    param_distributions=param_grid,\n    n_iter=10, #Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n    scoring=\"accuracy\",\n    n_jobs=1,\n    cv=5,\n)\nmodel.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(model.best_score_)    \nprint(model.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, this search method works best under the assumption that not all hyperparameters are equally important. "},{"metadata":{},"cell_type":"markdown","source":"# Bayesian Optimization with Gaussian Process \n> Bayesian optimization is a global optimization method for noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization, aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). \n\n> Gaussian Processes (GPs) provide a rich and flexible class of non-parametric statistical models over function spaces with domains that can be continuous, discrete, mixed, or even hierarchical in nature. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from functools import partial\n# Sequential model-based optimization in Python\nfrom skopt import space # Initialize a search space from given specifications.\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The idea is to approximate the function using a Gaussian process. In other words the function values are assumed to follow a multivariate gaussian. The covariance of the function values are given by a GP kernel between the parameters. Then a smart choice to choose the next parameter to evaluate can be made by the acquisition function over the Gaussian prior which is much quicker to evaluate."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Function to minimize. Should take a single list of parameters and return the objective value.\ndef optimize(params, param_names, x, y):\n    params = dict(zip(param_names, params)) # Create a dictonary of parameter names and values to feed into the model.\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.KFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n\n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = metrics.accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n\n    return -1.0 * np.mean(accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initialize a search space of max_depth, n_estimators, criterion and max_features\nparam_space = [\n    space.Integer(3, 15, name=\"max_depth\"),\n    space.Integer(100, 600, name=\"n_estimators\"),\n    space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n    space.Real(0.01, 1, prior = \"uniform\", name=\"max_features\")\n]\n\nparam_names = [\n    \"max_depth\",\n    \"n_estimators\",\n    \"criterion\",\n    \"max_features\"\n]\n\noptimization_function = partial(\n    optimize,\n    param_names=param_names,\n    x=X,\n    y=y\n)\n\nresult = gp_minimize(\n    optimization_function,  # Function to minimize. Should take a single list of parameters and return the objective value.\n    dimensions=param_space, # List of search space dimensions.\n    n_calls=15, # Number of calls to func\n    n_random_starts=10, # Number of evaluations of func with random points\n    verbose=10 # Control the verbosity. It is advised to set the verbosity to True for long optimization runs\n)\n\nprint(dict(zip(param_names, result.x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run."},{"metadata":{},"cell_type":"markdown","source":"# Hyperopt\n> Hyperopt is a way to search through an hyperparameter space. For example, it can use the Tree-structured Parzen Estimator (TPE) algorithm, which explore intelligently the search space while narrowing down to the estimated best parameters.This is an oriented random search, in contrast with a Grid Search where hyperparameters are pre-established with fixed steps increase. Random Search for Hyper-Parameter Optimization (such as what Hyperopt do) has proven to be an effective search technique."},{"metadata":{"trusted":false},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Optimization is finding the input value or set of values to an objective function that yields the lowest output value, called a “loss”. \ndef optimize(params, x, y):\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.KFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n\n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = metrics.accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n\n    return -1.0 * np.mean(accuracies)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The way to use hyperopt is to describe:\n\n* the objective function to minimize\n* the space over which to search\n* the database in which to store all the point evaluations of the search\n* the search algorithm to use"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''There is also a few quantized versions of those functions, which rounds the generated values at each step of “q”:\n    ∙ hp.quniform(label, low, high, q)\n    ∙ hp.qloguniform(label, low, high, q) '''\n\nparam_space = {\n    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 3, 15, 1)),\n    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 100, 600, 1)),\n    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n    \"max_features\": hp.uniform(\"max_features\", 0.01, 1)\n}\n\n\noptimization_function = partial(\n    optimize,\n    x=X,\n    y=y\n)\n\ntrials = Trials() # It would be nice to see exactly what is happening inside the hyperopt black box. The Trials object allows us to do just that.\n\nresult = fmin(\n    fn=optimization_function,\n    space=param_space,\n    algo=tpe.suggest,\n    max_evals=15,\n    trials=trials,\n)\n\nprint(result)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}