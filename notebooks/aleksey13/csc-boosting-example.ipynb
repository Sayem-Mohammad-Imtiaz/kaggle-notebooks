{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import ensemble\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nimport sklearn as skl\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom catboost import CatBoostClassifier, Pool\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nprint(\"scikit-learn\",skl.__version__)\nprint(\"lightgbm\",lgb.__version__)\nprint(\"xgboost\",xgb.__version__)\nprint(\"catboost\",cb.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils\n\nКак пользоваться пайплайнами:\n\n1. [Preprocessing data in scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html)\n2. [Building Scikit-Learn Pipelines With Pandas DataFrames](https://ramhiser.com/post/2018-04-16-building-scikit-learn-pipeline-with-pandas-dataframe/)\n3. [Building End to End predictive models with sklearn pipelines](https://www.kaggle.com/gautham11/building-predictive-models-with-sklearn-pipelines)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn_pandas import DataFrameMapper\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelBinarizer, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.utils import column_or_1d\nfrom sklearn.utils.validation import check_is_fitted\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.pipeline import FeatureUnion, make_union\nfrom sklearn_pandas import DataFrameMapper, gen_features\n\n# https://www.kaggle.com/gautham11/building-predictive-models-with-sklearn-pipelines\nclass TypeSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, dtype):\n        self.dtype = dtype\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.select_dtypes(include=[self.dtype])\n\nclass StringIndexer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.apply(lambda s: s.cat.codes.replace(\n            {-1: len(s.cat.categories)}\n        ))\n\nclass CustomImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='mean', filler='NA'):\n        self.strategy = strategy\n        self.fill = filler\n\n    def fit(self, X, y=None):\n        if self.strategy in ['mean', 'median']:\n            if not all([dtype in [np.number, np.int] for dtype in X.dtypes]):\n                raise ValueError('dtypes mismatch np.number dtype is required for ' + self.strategy)\n        if self.strategy == 'mean':\n            self.fill = X.mean()\n        elif self.strategy == 'median':\n            self.fill = X.median()\n        elif self.strategy == 'mode':\n            self.fill = X.mode().iloc[0]\n        elif self.strategy == 'fill':\n            if type(self.fill) is list and type(X) is pd.DataFrame:\n                self.fill = dict([(cname, v) for cname, v in zip(X.columns, self.fill)])\n        return self\n\n    def transform(self, X, y=None):\n        if self.fill is None:\n            self.fill = 'NA'\n        return X.fillna(self.fill)\n    \ndef CustomMapper(result_column='mapped_col', value_map={}, default=np.nan):\n    def mapper(X, result_column, value_map, default):\n        def colmapper(col):\n            return col.apply(lambda x: value_map.get(x, default))\n        mapped_col = X.apply(colmapper).values\n        mapped_col_names = [result_column + '_' + str(i) for i in range(mapped_col.shape[1])]\n        return pd.DataFrame(mapped_col, columns=[mapped_col_names])\n    return FunctionTransformer(\n        mapper,\n        validate=False,\n        kw_args={'result_column': result_column, 'value_map': value_map, 'default': default}\n    )\n\n\nclass SafeLabelEncoder(LabelEncoder):\n    \n    @staticmethod\n    def _get_unseen():\n        return 99999\n    \n    def fit_transform(self, y):\n        f\n\n    def transform(self, y):\n        check_is_fitted(self, 'classes_')\n        y = column_or_1d(y, warn=True)\n        classes = np.unique(y)\n        # Check not too many:\n        unseen = self._get_unseen()\n        if len(classes) >= unseen:\n            raise ValueError('Too many factor levels in feature. Max is %i' % unseen)\n        e = np.array([\n                         np.searchsorted(self.classes_, x) if x in self.classes_ else unseen\n                         for x in y\n                         ])\n\n        return e","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Читаем данные"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hotel-booking-demand/hotel_bookings.csv')\nprint(df['is_canceled'].value_counts() / len(df))\n\npd.set_option('display.max_columns', 50)\n# df[df['is_canceled'] == 1]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['reservation_status'].value_counts()\n# tdf = pd.get_dummies(df['reservation_status'])\n# tdf['is_canceled'] = df['is_canceled']\n# tdf.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hotel-booking-demand/hotel_bookings.csv',parse_dates=['reservation_status_date'])\n# possible target leak\ndf = df.drop(columns=['reservation_status','reservation_status_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['global_week'] = df['arrival_date_week_number'] + (df['arrival_date_year'] - 2015) * 53\nval_df = df[df['global_week'] > 121]\nlearn_df = df[df['global_week'] <= 121]\nprint(f\"Learn {len(learn_df)*100/len(df)}%, Val {len(val_df)*100/len(df)}%\")\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_X_y(df):\n    return df.drop('is_canceled', axis=1), df['is_canceled']\n\nval_X,val_y = to_X_y(val_df)\nlearn_X,learn_y = to_X_y(learn_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Обзор данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check na values \nprint(\"Nan in each columns\")\nfor i,line in enumerate(str(learn_df.isna().sum()).split('\\n')):\n    print(i,'\\t',line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_cols = learn_X.columns[learn_X.isna().sum() > 0]\nprint(\"NA columns:\",na_cols)\nprint(\"NA before\",learn_X.isna().sum().sum())\nfor col in na_cols:\n    learn_X[col].fillna(learn_X[col].mode()[0], inplace=True)\nprint(\"NA after\",learn_X.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select_dtypes :( doesn't work\n\ncat_columns = [x for i,x in enumerate(learn_X.columns) if learn_X[x].dtype == np.object]\ncat_columns_idx = [i for i,x in enumerate(learn_X.columns) if learn_X[x].dtype == np.object]\nnum_columns = [x for i,x in enumerate(learn_X.columns) if learn_X[x].dtype in ['int','float']]\nprint(\"Cat columns:\",cat_columns)\nprint(\"Number columns:\",num_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LabelEncoder can't handle unknown labels.\n# For example after e.fit(['a','b']), e.transform(['a','b','c']) return error.\nclass LabelEncoderUnknownHack(LabelEncoder):\n    __UNKNOWN_CLS = 999999\n    def transform(self, X):\n        class_to_i = {c:i for i,c in enumerate(self.classes_)}\n        vf = np.vectorize(lambda x: class_to_i[x] if x in class_to_i else self.__UNKNOWN_CLS)\n        return vf(X)\n    \n### Preprocessing pipline ###\n# Numerical features\nnum_data_pipeline = DataFrameMapper(\n        [(num_columns, [\n                CustomImputer(strategy='median'),\n                StandardScaler()\n            ], {'alias': 'num_data'})],\n    input_df=True ,df_out=True)\n\n# Categorical one hot\n# Apply for every cat feature\ncat_data_pipeline = DataFrameMapper(\n    [([column], [\n        CustomImputer(strategy='mode'),\n        OneHotEncoder(handle_unknown='ignore')\n    ], {'alias': 'cat_'+column}) for column in cat_columns],\n    input_df=True ,df_out=True)\n\n# Categorical labeling\ncat_data_pipeline_labeling = DataFrameMapper(\n    [([column], [\n        CustomImputer(strategy='mode'),\n        LabelEncoderUnknownHack()\n    ], {'alias': 'cat_'+column}) for column in cat_columns],\n    input_df=True ,df_out=True)\n\n# Pipeline 1\nfeatures_pipeline = make_union(num_data_pipeline, cat_data_pipeline)\n# Pipeline 2\nfeatures_pipeline_labeling = make_union(num_data_pipeline, cat_data_pipeline_labeling)\n\n### Function for testing ###\ndef test_model(model_handler,params,n_split=5):\n    aucs = []\n    kf = KFold(n_splits = n_split, shuffle = True, random_state = 2)\n    for i,(train_index, test_index) in enumerate(kf.split(learn_X)):\n        print(f\"Fold {i+1}/{n_split} ...\")\n        X_train, X_test = learn_X.iloc[train_index], learn_X.iloc[test_index]\n        y_train, y_test = learn_y.iloc[train_index], learn_y.iloc[test_index]\n        y_pred_proba = model_handler(X_train,y_train,X_test,y_test,params)\n        fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n        auc = roc_auc_score(y_test, y_pred_proba)\n        aucs.append(auc)\n        plt.plot(fpr,tpr,label=f\"Fold {i+1}, auc={str(auc)}\")\n    \n    plt.legend(loc=4)\n    plt.show()\n    m = np.mean(aucs)\n    s = np.std(aucs)\n    print(\"AUC: %.4f±%.4f\" % (m,s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scikit-learn\n\n[Параметры GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scikit_learn_handler(X_train,y_train,X_test,y_test,params):    \n    \n    features_pipeline.fit(X_train)\n\n    X = features_pipeline.transform(X_train)\n    X2 = features_pipeline.transform(X_test)\n\n    model = ensemble.GradientBoostingClassifier(**params)\n    model.fit(X, y_train)\n    return model.predict_proba(X2)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n        'n_estimators': 10, \n        'max_depth': 3, \n        'learning_rate': 0.05\n    }\n    \ntest_model(scikit_learn_handler,params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n        'n_estimators': 10, \n        'max_depth': 3, \n        'learning_rate': 0.05,\n        'subsample': 0.5\n    }\n    \ntest_model(scikit_learn_handler,params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\n\n* [Параметры XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier)\n* [Замечания по тюнигу параметров](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def xg_boost_handler(X_train,y_train,X_test,y_test,params):\n    \n    features_pipeline.fit(X_train)\n    \n    X = features_pipeline.transform(X_train)\n    X2 = features_pipeline.transform(X_test)    \n    \n    model = xgb.XGBClassifier(**params)\n    model.fit(X, y_train)\n    return model.predict_proba(X2)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 10, \n    'max_depth': 3, \n    'learning_rate': 0.05, \n    'tree_method': 'exact'\n}\n\ntest_model(xg_boost_handler,params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 10, \n    'max_depth': 3, \n    'learning_rate': 0.05, \n    'tree_method': 'hist'\n}\n\ntest_model(xg_boost_handler,params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 10, \n    'max_depth': 3, \n    'learning_rate': 0.05, \n    'tree_method': 'hist',\n    'n_jobs': 4\n}\n\ntest_model(xg_boost_handler,params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LigtGBM\n\n* [Общее описание параметров LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm-lgbmclassifier)\n* [Более детально описание](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\n* [Туториал по тюнингу](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lightgbm_handler(X_train,y_train,X_test,y_test,params):\n    \n    features_pipeline.fit(X_train)\n    \n    X = features_pipeline.transform(X_train)\n    X2 = features_pipeline.transform(X_test) \n    \n    model = lgb.LGBMClassifier(**params)\n    model.fit(X, y_train)\n    return model.predict_proba(X2)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 10, \n    'max_depth': 3, \n    'learning_rate': 0.05,\n}\ntest_model(lightgbm_handler, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 100, \n    'max_depth': 3, \n    'learning_rate': 0.05,\n}\ntest_model(lightgbm_handler, params)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"########## !!!!! IN PGROGRESS  !!!! ##########\n# import warnings\n# from sklearn.exceptions import DataConversionWarning\n# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n# TODO fix label encoding\n\ndef lightgbm_handler_cat(X_train,y_train,X_test,y_test,params):\n    \n    features_pipeline_labeling.fit(X_train)\n    \n    X = features_pipeline_labeling.transform(X_train)\n    X2 = features_pipeline_labeling.transform(X_test) \n    \n    model = lgb.LGBMClassifier(**params)\n    model.fit(X, y_train, categorical_feature=cat_columns_idx)\n    return model.predict_proba(X2)[:,1]\n\n# %%time\nparams = {\n    'n_estimators': 100, \n    'max_depth': 3, \n    'learning_rate': 0.05\n}\ntest_model(lightgbm_handler_cat, params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost\n\n* [Параметры CatBoostClassifier](https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html)\n* [Туториал по тюнингу](https://catboost.ai/docs/concepts/parameter-tuning.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def catboost_handler(X_train,y_train,X_test,y_test,params):\n    features_pipeline.fit(X_train)\n    \n    X = features_pipeline.transform(X_train)\n    X2 = features_pipeline.transform(X_test) \n\n    model = CatBoostClassifier(**params)\n\n    model.fit(X,y_train,silent=True)\n    return model.predict_proba(X2)[::,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 100, \n    'max_depth': 3, \n    'learning_rate': 0.05\n}\n    \ntest_model(catboost_handler,params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def catboost_handler_cat(X_train,y_train,X_test,y_test,params):\n    train_dataset= cb.Pool(X_train, y_train, cat_features=cat_columns)\n    eval_dataset = Pool(X_test, y_test, cat_features=cat_columns)\n\n    model = CatBoostClassifier(**params)\n\n    model.fit(train_dataset,eval_set=eval_dataset,silent=True)\n    return model.predict_proba(X_test)[::,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n    'n_estimators': 100, \n    'max_depth': 3, \n    'learning_rate': 0.05\n}\n    \ntest_model(catboost_handler_cat,params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learning curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\n\ndef experiment_lgbm(params,sample_size = 1000):\n    X_train,y_train,X_test,y_test = learn_X[:sample_size],learn_y[:sample_size],learn_X[sample_size:],learn_y[sample_size:],\n\n    features_pipeline.fit(X_train)\n\n    X = features_pipeline.transform(X_train)\n    X2 = features_pipeline.transform(X_test) \n\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X, y_train, eval_metric='binary', eval_set=[(X,y_train),(X2,y_test)],eval_names=['train','test'],verbose=False)\n    ax1 = lgb.plot_metric(model, metric='binary_logloss')\n    plt.show()\n    print(\"Evaluation results:\")\n    for title in model.evals_result_.keys():\n        r = model.evals_result_[title]\n        for metric in r.keys():\n            r_m = r[metric]\n            print(\"Type:\",title,\"\\tmetric:\",metric,\"\\tmin: %.5f, max:%.5f\" % (np.min(r_m), np.max(r_m)))\n    return model,features_pipeline\n\ndef calc_validation(model,f_pipeline):\n    X = f_pipeline.transform(val_X)\n    y_pred_proba = model.predict_proba(X)[:,1]\n    loss = log_loss(val_y, y_pred_proba)\n    print(\"Validation LogLoss: %.4f\" % loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators': 100, \n    'max_depth': 3, \n    'learning_rate': 0.05,\n    'objective': 'binary',\n    'metric': ['binary_logloss']\n}\nm,p = experiment_lgbm(params,sample_size = 1000)\ncalc_validation(m,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators': 15, \n    'max_depth': 3, \n    'learning_rate': 0.05,\n    'objective': 'binary',\n    'metric': ['binary_logloss']\n}\nm,p = experiment_lgbm(params,sample_size = 1000)\ncalc_validation(m,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def experiment_lgbm_full_learn(params):\n    features_pipeline.fit(learn_X)\n    X = features_pipeline.transform(learn_X)\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X, learn_y,verbose=False)\n    y_pred_proba = model.predict_proba(X)[:,1]\n    loss = log_loss(learn_y, y_pred_proba)\n    print(\"Train LogLoss: %.4f\" % loss)\n\n    return model,features_pipeline\n\nm,p = experiment_lgbm_full_learn(params)\ncalc_validation(m,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}