{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Employee Turnover / Employee Churn Using Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"## <p>For performing Employee Churn using machine learning we are going to use dataset from <a herf = \"https://www.kaggle.com/giripujar/hr-analytics\">kaggle</a>.Below is the brief description about data.</p>\n\n## Data Description:\n\n### This data is consist of 10 variables(columns) and 15,000 observation(rows).Each row represent a employee which has revelent values for each 10 variabales.More detail about columns is mentioned below:\n\n* <b>satisfaction_level (Numerical):</b> This column represents employee satisfaction level between 0 to 1 where 0 represents least and 1 represents high.\n* <b>last_evaluation (Numerical):</b> This column represents how much time since last evaluation of employee.</br>\n* <b>number_project (Numerical):</b> This column reprsents how much projects employee has done so far.</br>\n* <b>average_montly_hours (Numerical):</b> This column represents average monthly hours employee has spend in organization.</br>\n* <b>time_spend_company (Numerical):</b> This column represents how many years employee has spend in organization</br>\n* <b>Work_accident (Numerical):</b></b> This column represents while working whether they have any work accident or not where 1 means yes they have work accident and 0 means No they have not work accident.</br>\n* <b>promotion_last_5years (Numerical):</b> This column represents whether employee got any promotion or not in last 5 years where 0 means No they got no promotion in last 5 years and 1 means Yes they got promotion in last 5 years.</br>\n* <b>Department (Nominal Categorical):</b> This column represents in which department employee is/was working for example sales,IT,accounting etc. there are 10 total category or department exist in the column.</br>\n* <b>Salary (Ordinal Categorical):</b> This column repersents salary of employee whether it is hight,low or medium.</br>\n* <b>left (Numerical):</b> This column represents whether employee has left the organization or not where 0 means No employee has not left the organization and 1 means Yes employee has left the organization.</br>"},{"metadata":{},"cell_type":"markdown","source":"## Now we will try to get some basic understanding of data by looking at each column"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imporing required libraries for project\n\nimport pandas as pd # for data manipluation\nimport numpy as np  # for data calculations and statistical measurement\nimport matplotlib.pyplot as plt #for ineractive visualization charts\nimport seaborn as sns #for ineractive visualization charts\n%matplotlib inline \nimport warnings  # For warnings\nwarnings.filterwarnings(\"ignore\") # To ignore unwanted warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading Dataset and loading into variable\n\nemployee_churn = pd.read_csv(\"../input/hr-analytics/HR_comma_sep.csv\")\n          \nemployee_churn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# structer Of data\nemployee_churn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking data type and null values for each column\nemployee_churn.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_churn.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_churn.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Above output suggest us that there no null values in our dataset and data types are also correct for all columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing summary for numerical columns\nemployee_churn.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see average statisfaction level of employees is 0.6128,average number of project done by eomployees is approximately 4,average monthly hours spend by employees is 201.0503 hours and on an average time spend in company by employees is approximately 4 years."},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing catergorical columns unique values\nprint(\"Department:\\n\",employee_churn['Department'].value_counts().to_string())\nprint(\"\\nSalary:\\n\",employee_churn['salary'].value_counts().to_string())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Out all 10 departments most number of employees are in sales where least  number of employees are in managment and salary columns suggest that from given data most of employees salary is low."},{"metadata":{},"cell_type":"markdown","source":"\n## Now we will try to explore data by grouping different column with our target column 'left'  and categorical column in order to generate some insight"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing number of churn and non-churn employees\nprint( \"Employee Distribution:\\n\",employee_churn['left'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing Employee Distribution\nsns.catplot(data=employee_churn,x='left',kind=\"count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Above result tell us that 3571 employees has left and 11428 employees has stayed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing all numeric variable and comparing them with our target variable\nprint('Left Vs all numeric variable:\\n\\n',employee_churn.groupby('left').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Form the above numbers we interpret that:\n\n* The people who have low statisfaction level are leaving the company more and people with high staisfaction level are staying.\n* The people who have less salary are leaving the company more and people with more salary are staying.\n* The people who have low promotion rate are leaving the company more and people with high promotion rate are staying.\n* The people who have worked more are leaving the company more and people who have worked less are staying.\n### This all points makes sense with the reality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing all features with respect to different departments.\nprint('Department Vs all numeric variable:\\n\\n',employee_churn.groupby('Department').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing all columns in accordance with Salary column\nprint('Salary Vs all numeric variable:\\n\\n',employee_churn.groupby('salary').mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"## Ploting histogram of all continuous features "},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_churn[\"satisfaction_level\"].hist(bins=10, figsize=(5,5))\nplt.xlabel(\"satisfaction_level\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_churn[\"last_evaluation\"].hist(bins=10, figsize=(5,5))\nplt.xlabel(\"last_evaluation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_churn[\"average_montly_hours\"].hist(bins=10, figsize=(5,5))\nplt.xlabel(\"average_montly_hours\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see the data distribution of the continoues features in the above plots and can say that our data is skewed."},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization of all categorical features with respect to feature \"left\" that is we are checking that how the data is distributed if we compare it with the people who are leaving the company"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ploting all feature with respect to feature \"left\"\n\ncolumns=['number_project','time_spend_company','Work_accident', 'promotion_last_5years','Department','salary']\nfig=plt.subplots(figsize=(20,30))\nfor i, j in enumerate(columns):\n    plt.subplot(4, 2, i+1)\n    plt.subplots_adjust(hspace = 1.0)\n    sns.countplot(x=j,data = employee_churn, hue='left')\n    plt.xticks(rotation=90)\n    plt.title(\"No. of employee\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The above plot shows us few important observations\n\n*   Department as people say that sales people do often change their job frequently, also technical, support, IT can be considered as technology oriented people also tends to change there job most of the time.\n and salary are the 2 most important features which affect in \nemployee turnover. Thus these 2 feature can be play vital role in predicting the employee churn.\n*   Employee engagement is another critical factor to influence the employee to leave the company. Employees with 3-5 projects are less likely to leave the company. The employee with less and more number of projects are likely to leave.\n*   Promotion becomes the reason to leave or stay in the company as our data provides evidence to support our statement, people who had promoted in last 5 years stays in the company and the others tends to leave.\n*   Work accident is also important factor as we can see that people with work accident stays in the company."},{"metadata":{},"cell_type":"markdown","source":"# Handling Categorical Features\n\n###### There is a need of handling categorical features as any machine learning model can only accepts numerical values and cannot take any objects. So there is a need to prepair our data into numerical form to feed into ML model.\n\n*   Department fearure will be handeled by One-Hot-Encoding also known as dummy variables as it is nominal categorical variable\n*   Salary feature being the ordinal categorical variable and needs to handeled by Ordinal Encoding which is also known as Lable Encoding. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will convert Department fearure into numerical from by performing One-Hot-Encoding that is Dummy Variable.\nemployee_churn = pd.get_dummies(employee_churn, columns=['Department'])\nemployee_churn.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employee_churn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will convert salary freature to numerical data by just adding a feature in the data frame and mapping salary to it.\n# Where we will use 1 as high, 2 as medium, 3 as low in salary feature.\nsalary_map = {'low':3,\n              'medium':2,\n              'high':1\n              }\n        \nemployee_churn['salary_numeric'] = employee_churn.salary.map(salary_map)\n\nemployee_churn['salary_numeric'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We have tried Ordinal Encode technique from sklearn.preprocessing also but it does not work on 1D array hence we have to use mapping to convert the salary feature in to numeric form which also considered for ordinal encoading."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the coloumn names.\nemployee_churn.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping salary column as it is of no use as of now for modeling.\nemployee_churn = employee_churn.drop(['salary'], axis=1)\nemployee_churn.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking correlation between 'left' feature with respect to other features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_corr = employee_churn.corr()\ndata_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Our findings show us that there are certain features which are negatively correalted with the 'left' feature. Hence we have to eleminate those features as they are of no use for ML models as they will affect the results. \n\n###### Features that are positivly associated with feature 'left' are: 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Department_accounting', 'Department_hr', 'Department_sales', 'Department_support', 'Department_technical', 'salary_numeric'\n\nHence we will use this features to predict which employees are leaving our company."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X1 as the input dataframe and y1 as output feature for our model.\ncols_corr=['last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company',\n           'Department_accounting', 'Department_hr', 'Department_sales', \n           'Department_support', 'Department_technical', 'salary_numeric'] \nX1=employee_churn[cols_corr]\ny1=employee_churn['left']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Model Implementaion\n #### Implementing Logistic Regression to predict employee churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling and importing the libraries to spilt the data into test and train\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=0)\n\n#Calling and importing the libraries to fit the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X1_train, y1_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking accuracy on our test and train datasets\nfrom sklearn.metrics import accuracy_score\nprint('Logistic regression train accuracy: {:.3f}'.format(accuracy_score(y1_train, logreg.predict(X1_train))))\nprint('Logistic regression test accuracy: {:.3f}'.format(accuracy_score(y1_test, logreg.predict(X1_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Our baseline accuracy was 0.771 and we just got 0.745 in our model, so now we need to do some changes. May be we have choose wrong features manually."},{"metadata":{},"cell_type":"markdown","source":"\n###### To take care of eleminating these features and selecting the ones which really affect our output there is a process called Automatic feature selection."},{"metadata":{},"cell_type":"markdown","source":"# Automatic Feature Selection\n\n###### We will be using one of the most frequently used technique that is called Recursive Feature Elimination (RFE). It works recursively by removing variables and building a model on the variables which are left in this process. It also uses the model accuracy to find which variables or combination of variables contribute the most to predicting the target attribute.\n\n###### When we have checked the correation in the above table and we fould that there are only 10 features positively associated with the feature 'left'. Hence we will ask our model to find 10 best features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X2 as the input dataframe and y2 as output feature for our REF model.\nemployee_churn_vars=employee_churn.columns.values.tolist()\ny2=['left']\nX2=[i for i in employee_churn_vars if i not in y2]\n\nprint('y variable being our output variable \\n', y2)\nprint('X variable being our input dataframe \\n', X2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Here we will again take 10 features out of 18 features available in our X, the way we choose 10 feature manually and check is there any difference in our prediction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing and calling Recursive Feature Elimination and Logistic Regression\nfrom sklearn.feature_selection import RFE \nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nrfe = RFE(lr, 10)\nrfe = rfe.fit(employee_churn[X2], employee_churn[y2])\nprint(rfe.support_)\nprint(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### We can see that RFE chose the 10 variables for us, which are marked True in the support_ array and marked with a choice “1” in the ranking_array. They are:\n\n###### 'satisfaction_level', 'last_evaluation', 'number_project' 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'Department_RandD', 'Department_hr', 'Department_management', 'salary_numeric'\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X as the input dataframe and y as output feature for our REF model.\ncols_RFE=['satisfaction_level', 'last_evaluation', 'number_project', 'time_spend_company', \n          'Work_accident', 'promotion_last_5years', 'Department_RandD', \n          'Department_hr', 'Department_management', 'salary_numeric'] \nX = employee_churn[cols_RFE]\ny = employee_churn['left']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Logistic Regression ML model on feature selected by REF model to find employee churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling and importing the libraries to spilt the data into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n#Calling and importing the libraries to fit the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking accuracy on our test and train datasets\nfrom sklearn.metrics import accuracy_score\nprint('Logistic regression Train accuracy: {:.3f}'.format(accuracy_score(y_train, logreg.predict(X_train))))\nprint('Logistic regression Test accuracy: {:.3f}'.format(accuracy_score(y_test, logreg.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Here we get more accuracy then our baseline accuracy, that is .804 which is actually more then the our pervious accuracy performed by the same logistic regression classifier using the featuers selected manually by checking the correlation table. So from here we will move ahead with the features selected by our REF model to work on other models."},{"metadata":{},"cell_type":"markdown","source":"## Evaluating our Logistic Regression model by Precision and Recall and building Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing Precision and Recall and f1-score for Logistic Regression model\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, logreg.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Confusion Matrix for Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Confusion Matrix \nlogreg_y_pred = logreg.predict(X_test)\nlogreg_cm = metrics.confusion_matrix(logreg_y_pred, y_test, [1,0])\nsns.heatmap(logreg_cm, annot=True, fmt='.2f',xticklabels = [\"Left\", \"Stayed\"] , yticklabels = [\"Left\", \"Stayed\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Logistic Regression')\nplt.savefig('logistic_regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Random Forest Classifier ML model on feature selected by REF model to find employee churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling and importing the libraries to fit the Random Forest Classifier model\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking accuracy on our test and train datasets\nfrom sklearn.metrics import accuracy_score\nprint('Random Forest Classifier Train accuracy: {:.3f}'.format(accuracy_score(y_train, rf.predict(X_train))))\nprint('Random Forest Classifier Test accuracy: {:.3f}'.format(accuracy_score(y_test, rf.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating our Random Forest Classifier model by Precision and Recall and building Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing Precision and Recall and f1-score\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, rf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Confusion Matrix for Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nforest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\nsns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"Left\", \"Stayed\"] , yticklabels = [\"Left\", \"Stayed\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Random Forest')\nplt.savefig('random_forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Support Vector Machine ML model on feature selected by REF model to find employee churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling and importing the libraries to fit the Support Vector Machine model\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking accuracy on our test and train datasets\nfrom sklearn.metrics import accuracy_score\nprint('Support Vector Machine Train accuracy: {:.3f}'.format(accuracy_score(y_train, svc.predict(X_train))))\nprint('Support Vector Machine Test accuracy: {:.3f}'.format(accuracy_score(y_test, svc.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating our Support Vector Machine model by Precision and Recall and building Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, svc.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Confusion Matrix for Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_y_pred = svc.predict(X_test)\nsvc_cm = metrics.confusion_matrix(svc_y_pred, y_test, [1,0])\nsns.heatmap(svc_cm, annot=True, fmt='.2f',xticklabels = [\"Left\", \"Stayed\"] , yticklabels = [\"Left\", \"Stayed\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Support Vector Machine')\nplt.savefig('support_vector_machine')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Implementing Gradient Boosting Classifier ML model on feature selected by REF model to find employee churn"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling and importing the libraries to fit the Gradient Boosting Classifier model\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking accuracy on our test and train datasets\nfrom sklearn.metrics import accuracy_score\nprint('Gradient Boosting Classifier Train accuracy: {:.3f}'.format(accuracy_score(y_train, gb.predict(X_train))))\nprint('Gradient Boosting Classifier Test accuracy: {:.3f}'.format(accuracy_score(y_test, gb.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating our Gradient Boosting Classifier model by Precision and Recall and building Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, gb.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Confusion Matrix for Gradient Boosting Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_y_pred = gb.predict(X_test)\ngb_cm = metrics.confusion_matrix(gb_y_pred, y_test, [1,0])\nsns.heatmap(gb_cm, annot=True, fmt='.2f',xticklabels = [\"Left\", \"Stayed\"] , yticklabels = [\"Left\", \"Stayed\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Gradient Boosting Classifier')\nplt.savefig('Gradient Boosting Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Neural Network for Classification Problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calling and importing the libraries to fit the Sequential Neural Network model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting our model on our data\nmodel= Sequential()\nmodel.add(Dense(1,input_dim=10,activation='sigmoid'))\nmodel.compile(Adam(lr=0.5),'binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Sequential Neural Network Test accuracy: {:.3f}'.format(accuracy_score(y_test, model.predict_classes(X_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_y_pred = model.predict_classes(X_test)\nmodel_cm = metrics.confusion_matrix(model_y_pred, y_test)\nsns.heatmap(model_cm, annot=True, fmt='.2f',xticklabels = [\"Stayed\", \"Left\"] , yticklabels = [\"Stayed\", \"Left\"] )\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Sequential Neural Network')\nplt.savefig('Sequential Neural Network')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Selection\n\n###### As mentioned in our Project Proposal that we will perform 4 classifical model those are Logistic Regression, Random Forest, SVM (Support Vector Machine), and Gradient Boosting Classifier. So we have applied all those models. Apart from that we know that if we are doing classification problem then just accuracy is not just the parameter to evaluate our model there are certain other things we need to keep in our mind when we are working on classification model.\n\n###### That are precision, recall, confusion matrix, True Positive, False Positive, True Negative, False Negative.\n\n###### Now lets talk about our problem that is employee churn so here we have to more focus on True Positive and Flase Positive. To understand this in our problem True Positive are those employees who are leaving the company and our ML model predicts them correctly that they are leaving the company and Flase Positive are those employees who are going to leave the company but our model predicts that they will stay in our company. So we need to keep in mind before selecting any ML model from the above 4 models that which model has more precision that is which model has less false positive.\n\n###### By examining all the 4 model we realized that Random Forest wins the race in with high precision and accuracy. That with the score of 0.98 for precision for employees who are going to leave and test accuracy with 0.988.\n\n###### Hence we will select Ranfom Forest as the best model for the employee churn problem."},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation\n#### Cross-validation is one of the most important techniques for generalizing our model or restricting it from getting overfitting on our dataset.\n\n#### We will use Random Forset Classifer as our best model for Cross Validation.\n\n#### We are using 10 k-fold Cross-Validation to train our Random Forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = RandomForestClassifier()\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The average accuracy remains very close to the Random Forest model accuracy; hence, we can conclude that the model generalizes well.\n\n#### Our average accuracy is remaining very close to the Random Forest model accuracy; hence, we can conclude that the model generalizing well.\n\n#### We have also beat our baseline average accuracy which was 0.977."},{"metadata":{},"cell_type":"markdown","source":"## The ROC Curve\n\n#### ROC and AUC curve will help us to evaluate which classification model is better to work it and will help us to understand in which model and at which threshold our model gives us the best recall or true positive rate with the less False positive rate.\n\n#### We will not consider SVC in ROC curve as ROC curve analysis does not use accuracy or error rate. An ROC curve plots sensitivity (y axis) versus 1-specificity (x axis) and SVC gives the probability of 0 or 1 in the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\n\ngb_roc_auc = roc_auc_score(y_test, gb.predict(X_test))\ngb_fpr, gb_tpr, gb_thresholds = roc_curve(y_test, gb.predict_proba(X_test)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\nplt.plot(gb_fpr, gb_tpr, label='Gradient Boosting (area = %0.2f)' % gb_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('ROC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Random Forest Classifier is the best model for this specific problem as Random Forest is far away from the dotted line which represents the ROC curve of a purely random classifier. A good classifer stays as far away from that line as possible that towards the top-left corner."},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance for Random Forest Model\n\n#### We will perform feature importance as by applying feature importance we can understand that which feature is affecting employee churn the most. It can help HR manager to understand why employee is going to churn and can focus on keeping that employee and try to improve that area of their company."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Appling Feature Importance\nfeature_labels = np.array(['satisfaction_level', 'last_evaluation', 'number_project', 'time_spend_company', \n                           'Work_accident', 'promotion_last_5years', 'Department_RandD', \n                           'Department_hr', 'Department_management', 'salary_numeric'])\nimportance = rf.feature_importances_\nfeature_indexes_by_importance = importance.argsort()\nfor index in feature_indexes_by_importance:\n    print('{}-{:.2f}%'.format(feature_labels[index], (importance[index] *100.0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above results shows the importance of each feature in ascending order 'promotion_last_5years' being the least important and satisfaction_level being the most important feature to understand reason of employee churn."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}