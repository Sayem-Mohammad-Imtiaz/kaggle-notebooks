{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TOPIC MODELING + LDA (latent dirichlet allocation)\n\nWe used only papers after 2020: (1) reduce the computation time; (2) most papers directly describe about COVID-19 are available after 2020. In order to increase speed and model accuracy, we included abstracts (papers without abstracts were excluded) as our main corpus, followed"},{"metadata":{},"cell_type":"markdown","source":"## 1. Load library and prepare data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#load library\nimport os\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim import corpora, models\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\nimport datetime\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"meta = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nprint(meta.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### first filter by meta file. select only papers after 2020\nmeta[\"publish_time\"] = pd.to_datetime(meta[\"publish_time\"])\nmeta[\"publish_year\"] = (pd.DatetimeIndex(meta['publish_time']).year)\nmeta[\"publish_month\"] = (pd.DatetimeIndex(meta['publish_time']).month)\nmeta = meta[meta[\"publish_year\"] == 2020]\nprint(meta.shape[0], \" papers are available after 2020 Jan 1.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count how many has abstract\ncount = 0\nindex = []\nfor i in range(len(meta)):\n    #print(i)\n    if type(meta.iloc[i, 8])== float:\n        count += 1\n    else:\n        index.append(i)\n\nprint(len(index), \" papers have abstract available.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##extract the abstract to pandas \ndocuments = meta.iloc[index, 8]\ndocuments=documents.reset_index()\ndocuments.drop(\"index\", inplace = True, axis = 1)\n\n##create pandas data frame with all abstracts, use as input corpus\ndocuments[\"index\"] = documents.index.values\ndocuments.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Processing\n\nThis section will go through simple text processing, tokenization, remove stop words, lemmatization, and stemming."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(400)\nstemmer = SnowballStemmer(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##lemmatize and stemming\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\n# Tokenize and lemmatize\ndef preprocess(text):\n    result=[]\n    for token in gensim.utils.simple_preprocess(text) :\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            # TODO: Apply lemmatize_stemming on the token, then add to the results list\n            result.append(lemmatize_stemming(token))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## use example to check the preprocessing step\n\ndocument_num = 1000  ##randomly pick one abstract\ndoc_sample = documents[documents[\"index\"] == document_num].values[0][0]\n\nprint(\"Original document: \")\nwords = []\nfor word in doc_sample.split(' '):\n    words.append(word)\nprint(words)\nprint(\"\\n\\nTokenized and lemmatized document: \")\nprint(preprocess(doc_sample))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##preprocess all abstracts\nprocessed_docs = documents['abstract'].map(preprocess)\nprocessed_docs[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Bag of words on the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"##create dictionary based on the preprocessed_documents\ndictionary = gensim.corpora.Dictionary(processed_docs)\n\n##check the dictionary\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 5:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## remove extreme words (very common and very rare)\ndictionary.filter_extremes(no_below=15, no_above=0.1)\n\n##create bag-of-word model for each documents\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check the bow_corpus\nbow_doc_1000 = bow_corpus[document_num]\n\nfor i in range(len(bow_doc_1000)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1000[i][0], \n                                                     dictionary[bow_doc_1000[i][0]], \n                                                     bow_doc_1000[i][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 TF-IDF\n\nCreate the TF-IDF and use it as input for LDA also."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create tf-idf from bow_corpus\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\n#preview the corpus_tfidf\nfrom pprint import pprint\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Run LDA with bow_corpus\n\nHere, we use the LDA model from gensim. I arbitary choose 5 topics. This can be changed based on domain knowledge."},{"metadata":{"trusted":true},"cell_type":"code","source":"now = datetime.datetime.now()\nprint (\"start model building at \",now.strftime(\"%Y-%m-%d %H:%M:%S\"))\nlda_model = gensim.models.LdaMulticore(bow_corpus, \n                                       num_topics=5, \n                                       id2word = dictionary, \n                                       passes = 50, \n                                       workers=4) \n\nnow = datetime.datetime.now()\nprint ('Model training finished at ',now.strftime(\"%Y-%m-%d %H:%M:%S\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##print out the key words of five topics\nfor idx, topic in lda_model.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the key words selected above, we can somehow summarized the five major topics as below:\n1. immunology\n2. hubei social, individual quarantin\n3. healthcare, recommendation\n4. genomic sequence\n5. symptoms (fever, chest image) + admision"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Run LDA + TF-IDF corpus\n\nHere, we use the TF-IDF corpus as our input and compare the topics with what we obtained above."},{"metadata":{"trusted":true},"cell_type":"code","source":"now = datetime.datetime.now()\nprint (\"start model building at \",now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n                                             num_topics=5, \n                                             id2word = dictionary, \n                                             passes = 50, \n                                             workers=4)\nnow = datetime.datetime.now()\nprint ('Model training finished at ',now.strftime(\"%Y-%m-%d %H:%M:%S\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check the key words of five topics\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print(\"Topic: {} Word: {}\".format(idx, topic))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the keywords above, we can summarize the five topics as:\n1. healthcare and research\n2. disease co-morbidities\n3. Drug and genomic sequencing, biomedical\n4. Disease spread\n5. Fever, chest image, symptoms\n  \nWe can see, the topics selected out by LDA+TF_DF are not exactly the same as above but very similar."},{"metadata":{},"cell_type":"markdown","source":"## 5. Apply model to get all abstracts' topic\n\nWith the model we trained above (LDA + bow_copus, and LDA + TF-IDF_corpus), we applied all our abstracts into them and save the probability of each topic to data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"documents_lda_topics = pd.DataFrame(columns = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"])\ndocuments_lda_tfidf_topics = pd.DataFrame(columns = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"])\nfor i in range(len(bow_corpus)):\n    if i % 500 ==0:\n        print(i)\n    documents_lda_topics.loc[i] = [0] * 5\n    documents_lda_tfidf_topics.loc[i] = [0] * 5\n    \n    output = lda_model.get_document_topics(bow_corpus[i])\n    for j in range(len(output)):\n        a = output[j][0]\n        b = output[j][1]\n        documents_lda_topics.iloc[i,a] = b\n    \n    output_tfidf = lda_model_tfidf.get_document_topics(bow_corpus[i])\n    for k in range(len(output_tfidf)):\n        a = output_tfidf[k][0]\n        b = output_tfidf[k][1]\n        documents_lda_tfidf_topics.iloc[i, a] = b\n        \nprint(\"Data processing finished\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pick the final topic for each abstract based on max-probability\nfor i in range(5):\n    documents_lda_topics.iloc[:, i] = documents_lda_topics.iloc[:, i].astype('float64', copy=False)\n    \ndocuments_lda_topics[\"final_topic\"] =documents_lda_topics.iloc[:, :5].idxmax(axis=1)\n\nfor i in range(5):\n    documents_lda_tfidf_topics.iloc[:, i] = documents_lda_tfidf_topics.iloc[:, i].astype('float64', copy=False)\n\ndocuments_lda_tfidf_topics[\"final_topic\"] =documents_lda_tfidf_topics.iloc[:, :5].idxmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##preview the dataframe for both models\nprint(\"LDA + bow_corpus: topic probability:\")\ndocuments_lda_topics.head(3)\nprint(\"LDA + TF-IDF_corpus: topic probability:\")\ndocuments_lda_tfidf_topics.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Abstracts' topic visualization\n\nIn this section, we used PCA-2D, PCA-3D, and T-SNE to visualize how the topics are distributed in all abstracts. Only the LDA+bow_corpus model will be visualized here."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3)\npca_result = pca.fit_transform(documents_lda_topics.iloc[:, :5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## with 3 components, variance explained\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##create dataframe with projected vectors from PCA\npca_df = pd.DataFrame()\npca_df['pca-one'] = pca_result[:,0]\npca_df['pca-two'] = pca_result[:,1] \npca_df[\"pca-three\"] = pca_result[:, 2]\npca_df[\"topic\"] = documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.1 PCA-2D"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"pca-one\", y=\"pca-two\",\n    hue= documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}),\n    data=pca_df,\n    legend=\"full\",\n    alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 PCA-3D"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_df[\"pca-one\"], \n    ys=pca_df[\"pca-two\"], \n    zs=pca_df[\"pca-three\"], \n    cmap='tab10',\n    c = documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"})\n)\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 T-SNE-2D"},{"metadata":{"trusted":true},"cell_type":"code","source":"##first run TSNE\nimport time\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(documents_lda_topics.iloc[:, :5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##create dataframe with TSNE results\ntsne_df = pd.DataFrame()\ntsne_df['tsne-2d-one'] = tsne_results[:,0]\ntsne_df['tsne-2d-two'] = tsne_results[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue=documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}),\n    #palette=sns.color_palette(\"hls\", 10),\n    data=tsne_df,\n    legend=\"full\",\n    alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}