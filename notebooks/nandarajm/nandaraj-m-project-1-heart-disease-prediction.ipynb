{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T01:39:44.745588Z","iopub.execute_input":"2021-09-18T01:39:44.746173Z","iopub.status.idle":"2021-09-18T01:39:44.845633Z","shell.execute_reply.started":"2021-09-18T01:39:44.746066Z","shell.execute_reply":"2021-09-18T01:39:44.844792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Heart disease prediction**\n\nThe medical industry and relies on factors such as the physical examination, symptoms and signs of the patient.Factors that influence heart disease are body cholesterol levels, smoking habit and obesity, family history of illnesses, blood pressure, and work environment. \n\nMachine learning algorithms play an essential and precise role in the prediction of heart disease.\nHeart disease is seen as the worldâ€™s deadliest disease of human life. In particular, in this type of disease, the heart is not able to push the required amount of blood to the remaining organs of the human body to perform regular functions.","metadata":{}},{"cell_type":"markdown","source":"Now we have to import the necessary librarys as show below ","metadata":{}},{"cell_type":"code","source":"#importing python libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsb.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:44.847334Z","iopub.execute_input":"2021-09-18T01:39:44.847594Z","iopub.status.idle":"2021-09-18T01:39:45.538175Z","shell.execute_reply.started":"2021-09-18T01:39:44.847562Z","shell.execute_reply":"2021-09-18T01:39:45.537451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After importing the python libraies we will be importing the data set using pandas  ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-uci/heart.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.539222Z","iopub.execute_input":"2021-09-18T01:39:45.539488Z","iopub.status.idle":"2021-09-18T01:39:45.555387Z","shell.execute_reply.started":"2021-09-18T01:39:45.539457Z","shell.execute_reply":"2021-09-18T01:39:45.554711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To view the first 5 rows of the data set:","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.557059Z","iopub.execute_input":"2021-09-18T01:39:45.557385Z","iopub.status.idle":"2021-09-18T01:39:45.585496Z","shell.execute_reply.started":"2021-09-18T01:39:45.55735Z","shell.execute_reply":"2021-09-18T01:39:45.584708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get the columns names and is data types:","metadata":{}},{"cell_type":"code","source":"print(df.info())","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.586904Z","iopub.execute_input":"2021-09-18T01:39:45.587208Z","iopub.status.idle":"2021-09-18T01:39:45.603387Z","shell.execute_reply.started":"2021-09-18T01:39:45.587173Z","shell.execute_reply":"2021-09-18T01:39:45.602444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA Exploratory Data analysis**\nFor this data set we have to create a model and train it for logistic regression.So before  we train the model for logistic regression we have to first explore the data sets which means that we have to first analyze the data to see what it is going to work with.\nEDA helps us find answers to some important questions such as: What question (s) are you trying to solve? How can you add, change, or remove features to get the most out of your data?What kind of data do we have and how do we handle the different types? Where are the outliers and why should you care? What is missing in the data and how do you deal with it? ","metadata":{}},{"cell_type":"code","source":"pd.set_option(\"display.float\", \"{:.2f}\".format)\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.605067Z","iopub.execute_input":"2021-09-18T01:39:45.605373Z","iopub.status.idle":"2021-09-18T01:39:45.653847Z","shell.execute_reply.started":"2021-09-18T01:39:45.605338Z","shell.execute_reply":"2021-09-18T01:39:45.653076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.value_counts().plot(kind=\"bar\", color=[\"green\", \"lightblue\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.654871Z","iopub.execute_input":"2021-09-18T01:39:45.655092Z","iopub.status.idle":"2021-09-18T01:39:45.873721Z","shell.execute_reply.started":"2021-09-18T01:39:45.655061Z","shell.execute_reply":"2021-09-18T01:39:45.873069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here by ploting the target values we can obsrerve that 165 people with heart disease and 138 people without heart disease, so our problem is balanced.","metadata":{}},{"cell_type":"code","source":"# Checking for missing values\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.874922Z","iopub.execute_input":"2021-09-18T01:39:45.875185Z","iopub.status.idle":"2021-09-18T01:39:45.883311Z","shell.execute_reply.started":"2021-09-18T01:39:45.87516Z","shell.execute_reply":"2021-09-18T01:39:45.882445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After checking for the null values we can observe that the dataset has no null vaules  ","metadata":{}},{"cell_type":"code","source":"categorical_val = []\ncontinous_val = []\nfor column in df.columns:\n    print('*************************************************************************')\n    print(f\"{column} : {df[column].unique()}\")\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.886875Z","iopub.execute_input":"2021-09-18T01:39:45.887443Z","iopub.status.idle":"2021-09-18T01:39:45.907204Z","shell.execute_reply.started":"2021-09-18T01:39:45.887391Z","shell.execute_reply":"2021-09-18T01:39:45.905663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 15))\n\nfor i, column in enumerate(categorical_val, 1):\n    plt.subplot(3, 3, i)\n    df[df[\"target\"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)\n    df[df[\"target\"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:45.908381Z","iopub.execute_input":"2021-09-18T01:39:45.90925Z","iopub.status.idle":"2021-09-18T01:39:49.570234Z","shell.execute_reply.started":"2021-09-18T01:39:45.909214Z","shell.execute_reply":"2021-09-18T01:39:49.569523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations from the above plot:**\n\n**cp {Chest pain}**: People with cp 1, 2, 3 are more likely to have heart disease than people with cp 0.\n\n**restecg {resting EKG results}**: People with a value of 1 (reporting an abnormal heart rhythm, which can range from mild symptoms to severe problems) are more likely to have heart disease.\n\n**exang {exercise-induced angina}**: people with a value of 0 (No ==> angina induced by exercise) have more heart disease than people with a value of 1 (Yes ==> angina induced by exercise)\n\n**slope {the slope of the ST segment of peak exercise}**: People with a slope value of 2 (Downslopins: signs of an unhealthy heart) are more likely to have heart disease than people with a slope value of 2 slope is 0 (Upsloping: best heart rate with exercise) or 1 (Flatsloping: minimal change (typical healthy heart)).\n\n**ca {number of major vessels (0-3) stained by fluoroscopy}**: the more blood movement the better, so people with ca equal to 0 are more likely to have heart disease.\n\n**thal {thalium stress result}**: People with a thal value of 2 (defect corrected: once was a defect but ok now) are more likely to have heart disease.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 15))\n\nfor i, column in enumerate(continous_val, 1):\n    plt.subplot(3, 2, i)\n    df[df[\"target\"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)\n    df[df[\"target\"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:49.571161Z","iopub.execute_input":"2021-09-18T01:39:49.571429Z","iopub.status.idle":"2021-09-18T01:39:52.045059Z","shell.execute_reply.started":"2021-09-18T01:39:49.571394Z","shell.execute_reply":"2021-09-18T01:39:52.044126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations from the above plot:**\n\n* **trestbps**: resting blood pressure anything above 130-140 is generally of concern\n\n* **chol:** greater than 200 is of concern.\n \n* **thalach:** People with a maximum of over 140 are more likely to have heart disease.\n \n* **the old peak of exercise**-induced ST depression vs. rest looks at heart stress during exercise an unhealthy heart will stress more.","metadata":{}},{"cell_type":"code","source":"# Create another figure\nplt.figure(figsize=(10, 8))\n\n# Scatter with postivie examples\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c=\"red\")\n\n# Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c=\"blue\")\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:52.046568Z","iopub.execute_input":"2021-09-18T01:39:52.046866Z","iopub.status.idle":"2021-09-18T01:39:52.402583Z","shell.execute_reply.started":"2021-09-18T01:39:52.046827Z","shell.execute_reply":"2021-09-18T01:39:52.401932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here in the above scatter plot we can observe the age groups prone to maximum heart diseases","metadata":{}},{"cell_type":"markdown","source":"**Correlation Matrix**\nwe will be finding correlation matrix through the heat maps. ","metadata":{}},{"cell_type":"code","source":"\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 15))\nax = sb.heatmap(corr_matrix,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt=\".2f\",\n                 cmap=\"YlGnBu\");\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:52.403868Z","iopub.execute_input":"2021-09-18T01:39:52.40413Z","iopub.status.idle":"2021-09-18T01:39:53.680682Z","shell.execute_reply.started":"2021-09-18T01:39:52.404082Z","shell.execute_reply":"2021-09-18T01:39:53.679979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8), \n                                                   title=\"Correlation with target\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:53.681903Z","iopub.execute_input":"2021-09-18T01:39:53.682253Z","iopub.status.idle":"2021-09-18T01:39:54.001441Z","shell.execute_reply.started":"2021-09-18T01:39:53.682216Z","shell.execute_reply":"2021-09-18T01:39:54.000768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**The above shows the plot shows the correlation with the target**\n\n* fbs and chol are the least correlated with the target variable.\n* All other variables have a significant correlation with the target variable.","metadata":{}},{"cell_type":"markdown","source":"**DATA proccessing**\n\nAfter exploring the dataset, we can observe that we need to convert some categorical variables to dummy variables and scale all values before training the machine learning models.","metadata":{}},{"cell_type":"code","source":"categorical_val.remove('target')\ndataset = pd.get_dummies(df, columns = categorical_val)\n\nfrom sklearn.preprocessing import StandardScaler\n\ns_sc = StandardScaler()\ncol_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:54.002774Z","iopub.execute_input":"2021-09-18T01:39:54.003019Z","iopub.status.idle":"2021-09-18T01:39:54.114455Z","shell.execute_reply.started":"2021-09-18T01:39:54.002987Z","shell.execute_reply":"2021-09-18T01:39:54.11373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying the Logistic Regression**\nNow finally we can apply finally train machine learing model using the logistic regression algorithm to predict the heart disease","metadata":{}},{"cell_type":"markdown","source":"First we have to import the accuracy score, confusion matrix, classification report from sklean function.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n*********************************************\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"-------------------------------------------------------------------------\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n***********************************************************\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"-------------------------------------------------------------------------\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"-------------------------------------------------------------------------\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:54.115632Z","iopub.execute_input":"2021-09-18T01:39:54.116636Z","iopub.status.idle":"2021-09-18T01:39:54.152296Z","shell.execute_reply.started":"2021-09-18T01:39:54.116592Z","shell.execute_reply":"2021-09-18T01:39:54.151614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Spliting the data sets for training and testing**\n\nNow we will be splitting the data 70% training and 30% testing:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = dataset.drop('target', axis=1)\ny = dataset.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:54.153446Z","iopub.execute_input":"2021-09-18T01:39:54.153774Z","iopub.status.idle":"2021-09-18T01:39:54.173389Z","shell.execute_reply.started":"2021-09-18T01:39:54.153737Z","shell.execute_reply":"2021-09-18T01:39:54.172688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we will train the machine learning model and print the classification report of our logistic regression model:**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:54.174869Z","iopub.execute_input":"2021-09-18T01:39:54.175173Z","iopub.status.idle":"2021-09-18T01:39:54.265604Z","shell.execute_reply.started":"2021-09-18T01:39:54.175133Z","shell.execute_reply":"2021-09-18T01:39:54.264668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","metadata":{"execution":{"iopub.status.busy":"2021-09-18T01:39:54.267155Z","iopub.execute_input":"2021-09-18T01:39:54.267444Z","iopub.status.idle":"2021-09-18T01:39:54.283048Z","shell.execute_reply.started":"2021-09-18T01:39:54.267407Z","shell.execute_reply":"2021-09-18T01:39:54.282219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see the model performs very well of the test set as it is giving almost the same accuracy in the test set as in the training set. We could see applying the logistic regression gives 86.81% of the total accuracy.**\n\nEven Random forest algorithm can be applied to the same data set which also gives almost the same accuracy around 78-79% percentage.","metadata":{}}]}