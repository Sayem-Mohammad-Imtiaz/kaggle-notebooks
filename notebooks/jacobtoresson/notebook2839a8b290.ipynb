{"cells":[{"metadata":{"id":"9vn_yEp2s0gA"},"cell_type":"markdown","source":"# Our project uses 3 models\n1. Speech_model: takes a sound and returns a word \n2. CNN_model (Xception): Takes a pictures and returns a vector representation of the image \n3. RNN_model: Takes a vector representation of an image and a sequence and returns a caption\n\n"},{"metadata":{"id":"n_aXtRVJve3E"},"cell_type":"markdown","source":"# Sound to text model\n\nFor this part of the project we used this notebook as inspiration: https://github.com/douglas125/SpeechCmdRecognition\n\nCite: \n@ARTICLE{2018arXiv180808929C,\n   author = {{Coimbra de Andrade}, D. and {Leo}, S. and {Loesener Da Silva Viana}, M. and \n\t{Bernkopf}, C.},\n    title = \"{A neural attention model for speech command recognition}\",\n  journal = {ArXiv e-prints},\narchivePrefix = \"arXiv\",\n   eprint = {1808.08929},\n keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},\n     year = 2018,\n    month = aug,\n   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180808929C},\n  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}"},{"metadata":{"id":"WiQsopqrvzaq"},"cell_type":"markdown","source":"2## Download and import the files needed from the github project"},{"metadata":{"id":"QrGdD71Cv388","trusted":true},"cell_type":"code","source":"!pip install {\"../input/kapree/kapre-0.1.7-py3-none-any.whl\"}\n\n# This file is used to download audio from various datasets and converts them audio into numpy \n!wget -q https://raw.githubusercontent.com/douglas125/SpeechCmdRecognition/master/SpeechDownloader.py\n\n# A generator for reading and serving audio files\n!wget -q https://raw.githubusercontent.com/douglas125/SpeechCmdRecognition/master/SpeechGenerator.py\n\n# Utility functions for audio files\n!wget -q https://raw.githubusercontent.com/douglas125/SpeechCmdRecognition/master/audioUtils.py\n\n# The actual speech model\n!wget -q https://raw.githubusercontent.com/douglas125/SpeechCmdRecognition/master/SpeechModels.py\n\nimport SpeechDownloader\nimport SpeechGenerator\nimport SpeechModels\nimport audioUtils","execution_count":null,"outputs":[]},{"metadata":{"id":"sC2mUNMBz8UM"},"cell_type":"markdown","source":"## Install the needed requirements"},{"metadata":{"id":"NkekE5S2z7YA","outputId":"6caa495f-4a8e-46fd-bf5a-d9ce10afd417","trusted":true},"cell_type":"code","source":"!wget pip install tensorflow>=2\n!wget pip install kapre==0.2\n!wget pip install pandas>=0.25\n!wget pip install librosa>=0.8\n!wget pip install tqdm\n!wget pip install matplotlib","execution_count":null,"outputs":[]},{"metadata":{"id":"x59wQPvg0zjq"},"cell_type":"markdown","source":"## Import other libraries that we need "},{"metadata":{"id":"mtNstkfB01Xt","trusted":true},"cell_type":"code","source":"import matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt  \nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"id":"aBfuDbsR1OC5"},"cell_type":"markdown","source":"## Download and prepare Google Speech Dataset"},{"metadata":{"id":"belhdAno1Ssm","outputId":"6413318a-0322-4d46-f0d4-85801caae040","trusted":true},"cell_type":"code","source":"# this indicates that we want to use a pretrained model\nversion = 2\n\n# specify the amount of commands that should be recognized by the model\ngscInfo, nCategs = SpeechDownloader.PrepareGoogleSpeechCmd(version=version, task='35word')\n\n# gscInfo: path to trainig, test and validation files \n# nCategs: the amount of classes\nprint(\"gscInfo: \", gscInfo)\nprint(\"nCategs: \", nCategs)\n\nwith open('gscInfo','wb') as f: \n  pickle.dump(gscInfo, f)\n\nwith open('nCategs','wb') as f: \n  pickle.dump(nCategs, f)","execution_count":null,"outputs":[]},{"metadata":{"id":"DlBcd3Ggaanp"},"cell_type":"markdown","source":"### Pickle dump the previous datastructures so that we dont have to download them again "},{"metadata":{"id":"HnANK2TKagdj","outputId":"f6a3d4e0-b4db-4be5-c8c5-aee6f7f48e24","trusted":true},"cell_type":"code","source":"with open('gscInfo','rb') as f: gscInfo = pickle.load(f)\n\nwith open('nCategs','rb') as f: nCategs = pickle.load(f)\n\n# test if it worked \n#print(numpy.array_equal(gscInfo,gscInfo2))\n#print(numpy.array_equal(nCategs,nCategs2))","execution_count":null,"outputs":[]},{"metadata":{"id":"0Y6vXLdw3jsK"},"cell_type":"markdown","source":"## Explanation of how the pre-trained model we use was trained\n\nIt was trained using ..."},{"metadata":{"id":"VtzB4b402s4U"},"cell_type":"markdown","source":"## Build the test and validation generator (dont need train since we use a pre-trained model)"},{"metadata":{"id":"1zs008Gu2zxA","outputId":"974fe895-54ac-4062-e842-cd7ff164f660","trusted":true},"cell_type":"code","source":"# to handla that the number of samples in validation may not be multiple of batch_size\nshuffle = True  \n\n# Create the speech generators, one for validation and one for testing\nvalidation_speech_generator   = SpeechGenerator.SpeechGen(gscInfo['val']['files'], gscInfo['val']['labels'], shuffle=shuffle)\ntest_speech_generator = SpeechGenerator.SpeechGen(gscInfo['test']['files'], gscInfo['test']['labels'], shuffle=False, batch_size=len(gscInfo['test']['files']))\n\nprint(\"validation generator: \", validation_speech_generator)\nprint(\"test generator: \", test_speech_generator)\n\nprint(\"Length of validation generator: \", validation_speech_generator.__len__())\nprint(\"Length of test generator: \", test_speech_generator.__len__())","execution_count":null,"outputs":[]},{"metadata":{"id":"vC89CWFe6cES"},"cell_type":"markdown","source":"## Look at validation data"},{"metadata":{"id":"FOYN4p9T6e0Z","outputId":"d313bdd6-7b49-46d5-9f1d-e3a1209e7da9","trusted":true},"cell_type":"code","source":"X, Y = validation_speech_generator.__getitem__(5)\nprint(\"Features: \", X)\nprint(\"Labels: \", Y)","execution_count":null,"outputs":[]},{"metadata":{"id":"JHUtF9SZ7ryC"},"cell_type":"markdown","source":"## Create the speech model and load the pre-trained weights"},{"metadata":{"id":"Bidws5Qf7u0L","trusted":true},"cell_type":"code","source":"# varför är input length None? Kan det vara vad som helst då? \nspeech_model = SpeechModels.AttRNNSpeechModel(nCategs, inputLength = None)\n\n# vad gör denna?? trodde vi hade tränat redan?\nspeech_model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['sparse_categorical_accuracy'])\n\n# gissar att det är här vi laddar in vikterna som de tränat fram \n# ÄNDRA LÄNKEN SEN NÄR ALLT ÄR NEDLADDAT \nspeech_model.load_weights('../input/pre-trained-speech-model/model-attRNN_pre_trained.h5')","execution_count":null,"outputs":[]},{"metadata":{"id":"mXym7SKK8iXj"},"cell_type":"markdown","source":"## Test if the model works by looking at the test data\n'unknown': 0,\n'silence': 0,\n'_unknown_': 0,\n'_silence_': 0,\n'_background_noise_': 0,\n'yes': 2,\n'no': 3,\n'up': 4,\n'down': 5,\n'left': 6,\n'right': 7,\n'on': 8,\n'off': 9,\n'stop': 10,\n'go': 11,\n'zero': 12,\n'one': 13,\n'two': 14,\n'three': 15,\n'four': 16,\n'five': 17,\n'six': 18,\n'seven': 19,\n'eight': 20,\n'nine': 1,\n'backward': 21,\n'bed': 22,\n'bird': 23,\n'cat': 24,\n'dog': 25,\n'follow': 26,\n'forward': 27,\n'happy': 28,\n'house': 29,\n'learn': 30,\n'marvin': 31,\n'sheila': 32,\n'tree': 33,\n'visual': 34,\n'wow': 35}\n\n['nine', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n           'zero', 'one', 'two', 'three', 'four', 'five', 'six', \n           'seven',  'eight', 'backward', 'bed', 'bird', 'cat', 'dog',\n           'follow', 'forward', 'happy', 'house', 'learn', 'marvin', 'sheila', 'tree',\n           'visual', 'wow']\n\n\n"},{"metadata":{"id":"irHwOf0G8lDz","trusted":true},"cell_type":"code","source":"X_test, Y_test = test_speech_generator.__getitem__(0)","execution_count":null,"outputs":[]},{"metadata":{"id":"Fk3CLEOvJ5JJ","outputId":"0a56fa46-ecf5-49d9-ee1c-b53f6a35310f","trusted":true},"cell_type":"code","source":"predictions = np.argmax(speech_model.predict(X_test, verbose = 1), 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Hhw72Om76LWJ","outputId":"a1ca2f7e-c6ba-4944-bebb-fc5dc222f8ec","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nclasses = ['unknown', 'nine', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n           'zero', 'one', 'two', 'three', 'four', 'five', 'six', \n           'seven',  'eight', 'backward', 'bed', 'bird', 'cat', 'dog',\n           'follow', 'forward', 'happy', 'house', 'learn', 'marvin', 'sheila', 'tree',\n           'visual', 'wow']\n\ncm = confusion_matrix(Y_test, predictions)\naudioUtils.plot_confusion_matrix(cm, classes, normalize = False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"T-pZzAO8J4Gu"},"cell_type":"markdown","source":"# Use the code from Lab 2 to create the model that generates captions "},{"metadata":{"id":"yq9Tl4vQKePz"},"cell_type":"markdown","source":"## Import Everything "},{"metadata":{"id":"58c_EPCDKAJQ","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n# regex\nimport re\n\n# merge paths\nimport os\n\nimport numpy as np \n\n# create datasets\nimport pandas as pd \n\n# to get file names in a folder \nfrom glob import glob\n\n# The Image module provides a class with the same name which is used to represent a PIL image. The module also provides a number of factory functions, including functions to load images from files, and to create new images.\nfrom PIL import Image\n\n# to save a model as a file \nfrom pickle import load, dump\n\n# to print a model \nfrom keras.utils import plot_model\n\n# one-hot encoding \nfrom keras.utils import to_categorical\n\n# It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).\nfrom keras.layers.merge import add\n\n# Loads a model saved via model.save(), Model is the actual model \nfrom keras.models import Model, load_model\n\n# Dense: fully connected layer that perform classification on the features extracted by the convolutional layers and down-sampled by the pooling layers in the CNN and that are used in the RNN. \n# \n# LSTM: RNN may suffer from the vanishing gradient problem - LSTM solves this problem, LSTM knows what to store and what to throw away. \n# Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n# Embedding: Use to create our own word embeddings using a tokenizer \n# Input: is used to instantiate a Keras tensor.\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, Input\n\n# Pads sequences to the same length.\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"id":"WNnKHW-gKlAw"},"cell_type":"markdown","source":"## Define global variables"},{"metadata":{"id":"QcmVzJZPKogb","trusted":true},"cell_type":"code","source":"start = \"<start>\"\nend = \"<end>\"\npathToImageFolder = '../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\npathToImageCaptionCSV = '../input/flickr-image-dataset/flickr30k_images/results.csv'\n\n# training params \npictures_used_when_training = 100\n\n# In terms of artificial neural networks, an epoch refers to one cycle through the full training dataset\nepochs = 10\n\n# dimentions of the pictures\nxdim = 299\nydim = 299","execution_count":null,"outputs":[]},{"metadata":{"id":"vqJKv9AYKqyt"},"cell_type":"markdown","source":"## Define functions used when preprocessing data\n\nWe use the pretrained CNN model \"Xception\": https://keras.io/api/applications/xception/\n\nQuote from webbsite: \nOptionally loads weights pre-trained on ImageNet. \nthe default input image size for this model is 299x299."},{"metadata":{"id":"8Jrupf2fK7-3","trusted":true},"cell_type":"code","source":"# returns a list with the name of all vectors and a dictionary with the name as key and caption as value\ndef fetchData(): \n    \n    # read the captions to a panda datastructure \n    captions = pd.read_csv(pathToImageCaptionCSV, delimiter='|')\n    captions.columns = ['imageName', 'captionNumber', 'caption']\n\n    # stores all the file names in a list \n    all_img_name_vector = []\n    \n    # stores all the captions in a dictionary with filenames as key \n    all_captions = {}\n    \n    # parse the panda and fill the structures above \n    for index, row in captions.head(n=pictures_used_when_training).iterrows():\n        caption = start + \" \" + row[2].replace(\".\", \"\").strip() + \" \" + end\n        im_ID = row[0]\n        all_img_name_vector.append(im_ID)\n        \n        if im_ID not in all_captions:\n            all_captions[im_ID] = []\n        all_captions[im_ID].append(caption)\n    \n    all_img_name_vector = list(set(all_img_name_vector))\n    return all_img_name_vector, all_captions\n\n# creates the feature dictionary (feature representation of every picture)\ndef createFeatures(img_name_vector, CNN_model):\n    \n    # create a features.p file if it does not exist\n    if not os.path.exists('features.p'):\n        \n        # load the pretrained Xception model \n        # the pooling layers downsomples the image data extracted by the convolutional layers to reduce the dimentionality of the feaature map inorder to decrease processing time \n        # include_top false to excklude the top layer \n        \n        # dictionary used to store the features images \n        features = {}       \n        \n        # create the feature of each picture (represent the picture as a vector) using the CNN\n        for im_ID in img_name_vector:       \n            image = loadPicture(im_ID)    \n            \n            \n            feature = CNN_model.predict(image)\n            features[im_ID] = feature\n\n        # store the feature representation fo every picture in the file feature.p\n        dump(features, open(\"../input/long-training-overnight/results/features.p\",\"wb\"))  \n\n    features = load(open(\"features.p\", \"rb\"))\n   \n    # only return the images that are in the name vector \n    features = {im_ID:features[im_ID] for im_ID in img_name_vector}\n    \n    return features\n\n# transform picture to standard dimention  (299x299)\ndef loadPicture(im_ID):\n    imagePath = pathToImageFolder + im_ID\n    image = Image.open(imagePath).resize((xdim,ydim))\n    image = np.expand_dims(image, axis=0) \n    return normalize(image) \n\n# normalising the values to -1 to 1 \ndef normalize(image):\n    return (image/127.5) - 1.0\n\n# create and return tokenizer \n# This class allows to vectorize a text corpus, by turning each text into a sequence of integers \n\ndef extractTokens(all_captions, num_words):\n    tkz = tf.keras.preprocessing.text.Tokenizer(num_words = num_words, oov_token = \"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n    # Updates internal vocabulary based on a list of texts.\n    tkz.fit_on_texts(all_captions)\n    dump(tkz, open('tokenizer.p', 'wb'))\n    return tkz \n\n# flatten dictionary\ndef flatten(dictionary):\n    flat = list()\n    for item in dictionary.keys():\n        [flat.append(d) for d in dictionary[item]]\n    return flat","execution_count":null,"outputs":[]},{"metadata":{"id":"ONqYGRD6LAjL"},"cell_type":"markdown","source":"## Preprocess data"},{"metadata":{"id":"hdGnOhs1LEGn","outputId":"ce508105-8497-4b2a-df62-99b976956b8d","trusted":true},"cell_type":"code","source":"all_img_name, all_captions = fetchData()\n\n# feature representation of the images\nCNN_model = tf.keras.applications.Xception(pooling = 'avg', include_top = False)\n\n# The output of our CNN is a 2048 feature vector\nimage = loadPicture(\"10002456.jpg\")    \nprint(CNN_model.predict(image).shape)\n\nfeatures = createFeatures(all_img_name, CNN_model)\n\n# tokenizer\nflat_all_captions = flatten(all_captions)\ntkz = extractTokens(flat_all_captions, 5000)\nvocabSize = len(tkz.word_index) + 1\n\n# max length used for the caption length\ncaption_max_length = max(len(t.split()) for t in flat_all_captions)","execution_count":null,"outputs":[]},{"metadata":{"id":"-21UwSAILIP_"},"cell_type":"markdown","source":"## Define functions used when building and training"},{"metadata":{"id":"oTeXiv4iLM2A","trusted":true},"cell_type":"code","source":"# Build the RNN \ndef buildRNN(vocabSize, caption_max_length):\n\n    # specify the shape (the number of features that each input sample has) \n    # 2048 because that if the shape that Imagenet uses \n    # Output from Imagenets is the input for the RNN model \n    inputs1 = Input(shape=(2048,))\n    \n    # the dropout layer randomly sets input units to 0 with a frequency of 50%\n    # to avoid overfitting \n    fe1 = Dropout(0.5)(inputs1)\n    \n    # create the fully connected layer with 256 neurons and use the relu as the activation function\n    fe2 = Dense(256, activation='relu')(fe1)\n    \n    # Create the LSTM layer\n    # shape of the caption input \n    inputs2 = Input(shape=(caption_max_length,))\n    se1 = Embedding(vocabSize, 256, mask_zero=True)(inputs2)\n    # drop 50% of all neurons \n    se2 = Dropout(0.5)(se1)\n    # the layer for the picture \n    se3 = LSTM(256)(se2)\n    \n    # Merg the two models \n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocabSize, activation='softmax')(decoder2)\n    \n    # tie it together [image, seq] [word]\n    RNN_model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n    RNN_model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return RNN_model\n\n# generator is used so that we dont have to give all data to the model at once (that will exhaust the model)\ndef data_generator(all_captions, features, tkz, caption_max_length):\n    \n    while True:\n        for im_ID, captions in all_captions.items():   \n            \n            # vector representation of the picture \n            feature = features[im_ID][0]\n            \n            # create the sequence \n            input_image, input_sequence, output_word = createSequences(tkz, caption_max_length, captions, feature)\n            \n            yield ([input_image, input_sequence], output_word)\n            \ndef createSequences(tkz, caption_max_length, captions, feature):\n    X1 = []\n    X2 = []\n    y = []\n    \n    for caption in captions:\n        # transform text to vector using the tokenizor\n        seq = tkz.texts_to_sequences([caption])[0]\n        \n        # loop every word in the sequence \n        for i in range(1, len(seq)):\n            \n            # all the words in the caption until letter i\n            inputSequence = seq[:i]\n            \n            # pad so that it always is the same length\n            inputSequence = pad_sequences([inputSequence], maxlen = caption_max_length)[0]\n\n            # the new word \n            outputSequence = seq[i]\n            \n            # go from words to indexes \n            outputSequence = to_categorical([outputSequence], num_classes=vocabSize)[0]\n            \n            # append to inputs and outputs \n            X1.append(feature)\n            X2.append(inputSequence)\n            y.append(outputSequence)\n    \n    X1 = np.array(X1)\n    X2 = np.array(X2)\n    y = np.array(y)\n    return X1, X2 , y","execution_count":null,"outputs":[]},{"metadata":{"id":"RIDw_41kLQd5"},"cell_type":"markdown","source":"## Build and train the model"},{"metadata":{"id":"rm5jKW0wLTnQ","trusted":true,"collapsed":true},"cell_type":"code","source":"model = buildRNN(vocabSize, caption_max_length)\nsteps_per_epoch = len(all_captions)\n\nfor i in range(epochs):\n    filename = \"model_\" + str(i) + \".h5\"\n\n    gen = data_generator(all_captions, features, tkz, caption_max_length)\n    model.fit_generator(gen, epochs = epochs, steps_per_epoch = steps_per_epoch, verbose = 1)\n    \n    model.save(filename)","execution_count":null,"outputs":[]},{"metadata":{"id":"9P8OetjeLXfB"},"cell_type":"markdown","source":"## Load pre-saved model (so we dont have to train it every time)"},{"metadata":{"id":"MSlsDFCWLhyD","trusted":true},"cell_type":"code","source":"tkz = load(open(\"../input/long-training-overnight/results/tokenizer.p\",\"rb\"))\n\nCNN_model = tf.keras.applications.Xception(pooling = 'avg', include_top = False)\nRNN_model = load_model('../input/long-training-overnight/results/model_8.h5')\n\ntkz = load(open(\"../input/long-training-overnight/results/tokenizer.p\",\"rb\"))\nRNN_model = load_model('../input/long-training-overnight/results/model_8.h5')\n\n# ./model_8.h5\n# ./tokenizer.p\n","execution_count":null,"outputs":[]},{"metadata":{"id":"rzCTsZ2CLmDn"},"cell_type":"markdown","source":"## Define functions used when testing the model"},{"metadata":{"id":"MV85I7sALs1S","trusted":true},"cell_type":"code","source":"# takes an index and returns a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word    \n    return None\n    \ndef generate_caption(model, tkz, img_feature, caption_max_length):    \n    # first word is always the start symbol \n    caption = start\n    \n    # loop until we get \"<end>\" or reached the max length\n    for i in range(caption_max_length):\n        \n        # convert the text to a vector using the tokenizer\n        seq = tkz.texts_to_sequences([caption])[0]\n        \n        # transform to the same length \n        seq = pad_sequences([seq], maxlen = caption_max_length)\n        \n        # gives the probability of each word given the picture and the previous sequence\n        predicted_id_list = model.predict([img_feature, seq], verbose=0)\n        \n        # gives the index of the word with the highest probability\n        pred_id = np.argmax(predicted_id_list)\n        \n        # gives the word of the above index \n        word = word_for_id(pred_id, tkz)\n        \n        # bug-fix when a tokenizer that does not match the captions have been used \n        if word is None:\n            break\n            \n        caption += ' ' + word\n        \n        # end if the word is end or it it wants to write start again \n        if word == end or word == start:\n            break\n            \n    return caption\n\ndef test_model(CNN_model, RNN_model, tkz, im_ID):        \n    \n    # create the vector representation of the image \n    image = loadPicture(im_ID)\n    feature = CNN_model.predict(image)\n    \n    # generate caption\n    print(generate_caption(RNN_model, tkz, feature, caption_max_length))\n    \n    # display image \n    img = Image.open(pathToImageFolder + im_ID)\n    plt.imshow(img)\n\ndef test_model2(CNN_model, RNN_model, tkz, im_ID):        \n    \n    # create the vector representation of the image \n    image = loadPicture(im_ID)\n    feature = CNN_model.predict(image)\n    \n    # generate caption\n    caption = generate_caption(RNN_model, tkz, feature, caption_max_length)\n\n    return caption","execution_count":null,"outputs":[]},{"metadata":{"id":"TZgApc4LLxTN"},"cell_type":"markdown","source":"## Test the RNN model"},{"metadata":{"id":"TECVNs38L-Qa","outputId":"44eb126a-5f1a-46a6-c488-0fff45692e93","trusted":true},"cell_type":"code","source":"test_model(CNN_model, RNN_model, tkz, '1355703632.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(CNN_model, RNN_model, tkz, '1032122270.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(CNN_model, RNN_model, tkz, '1009434119.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(CNN_model, RNN_model, tkz, '1714937792.jpg')","execution_count":null,"outputs":[]},{"metadata":{"id":"P1Z9L7HIGSUK"},"cell_type":"markdown","source":"# Combine the models to find pictures "},{"metadata":{"id":"-D6TiB8JGTKs","trusted":true},"cell_type":"code","source":"# use the speech model to predict the classes of all test sound \nword_predictions= np.argmax(speech_model.predict(X_test, verbose = 1), 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To quickly be able to go fram an index to a word\nclasses = ['nine', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go',\n           'zero', 'one', 'two', 'three', 'four', 'five', 'six', \n           'seven',  'eight', 'backward', 'bed', 'bird', 'cat', 'dog',\n           'follow', 'forward', 'happy', 'house', 'learn', 'marvin', 'sheila', 'tree',\n           'visual', 'wow']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now use the CNN and RNN model to create a caption for each picture and return the picture if the \n# caption contains the word we choose \n\n# Lets say we are looking for a picture for sound 5 in the test data;\nsound_index = 900\nprint(word_predictions[sound_index])\npredicted_word = classes[word_predictions[sound_index]]\nreal_word = classes[Y_test[sound_index]]\n\nprint(\"We are looking for the sound with index\", sound_index, \". We classified this sound as the word:\", predicted_word,\"(the real class for this word is:\",real_word,\")\")\n\npictures_Wanted = 3\npictures_looked_at = 0\nfound_images = 0\nfound_images_ID = []\nfound_pictures = 0\n\nfor img_name in all_img_name:\n    pictures_looked_at +=1\n    caption = test_model2(CNN_model, RNN_model, tkz, img_name) \n    \n    print(\"\\n\\n\\nLooking at picture: \", pictures_looked_at)\n    \n    if predicted_word in caption:\n        print(\"found an image!\")\n        found_pictures += 1\n        found_images_ID.append(img_name)\n    \n    if found_pictures == pictures_Wanted:\n        break\n\nif found_pictures == 0:\n    print(\"\\n\\nCould not find a picture with the word\")\nelse:\n    print(\"\\n\\nWe found these pictures for sound : \",sound_index, \"(\",predicted_word, \")\",found_images_ID)\n    print(\"One of them looks like this; \")\n    # display image \n    img = Image.open(pathToImageFolder + found_images_ID[found_pictures-1])\n    plt.imshow(img)\n    \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}