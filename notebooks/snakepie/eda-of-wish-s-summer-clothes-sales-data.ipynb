{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nmain_csv = '/kaggle/input/summer-products-and-sales-in-ecommerce-wish/summer-products-with-rating-and-performance_2020-08.csv'\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(main_csv)\n\nfrom collections import Counter\n\nCounter(df['badge_fast_shipping'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# * Plotting histograms for several stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfig, axs = plt.subplots(3, 1, figsize=(10,10))\n\nfig.suptitle(\"KDE and histogram of the several variables\", fontsize=20)\nplt.xlabel('Bins of values', color='white', fontsize=14)\nplt.ylabel('Density', color='white', fontsize=14)\n\nsns.set_style('whitegrid')\n\nhist_columns = ['price', 'retail_price', 'shipping_option_price']\n\nN = 100\n\nfor i in range(len(hist_columns)):\n    name = hist_columns[i]\n    arr = df[name]\n    \n    ax = axs[i]\n    \n    ax.set_title(name)\n    sns.distplot(arr, kde=True, color='g', ax=ax).set(xlim=(-3,25))\n    plt.plot()\n    \n    ax.set_xlim([0,30])\n    \nfig.tight_layout(pad = 3.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['units_sold'], kde=True, color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.PairGrid(data=df, vars = ['price', 'retail_price'], hue='units_sold', height=4)\ng.map(plt.scatter)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='price', y='retail_price', data=df, kind='kde', xlim=(0,20), ylim=(-5,35), height=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price_drop'] = df['retail_price'] - df['price']\nplt.scatter(df['price_drop'], df['units_sold'], alpha=.3)\nplt.xlabel('price drop')\nplt.ylabel('units sold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### ONLY NUMERIC VALUES:\nnumeric_cols = df.describe().columns\ndf_numeric = df[numeric_cols]\ndf_numeric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 100\ndf_sorted_units = df_numeric.sort_values(by='units_sold', ascending=False)\ntop_n = df_sorted_units.head(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Max-min Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf_new = scaler.fit_transform(df_sorted_units)\nnumeric_cols = df.describe().columns\ndf_sorted_units = pd.DataFrame(df_new, columns=numeric_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quantile Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_quantiles = [top_n]\nfor i in range(1, len(df)//n):\n    n_quan = df_sorted_units.iloc[ (len(df)- n*(i+1)) : -n*i]\n    n_quantiles.append(n_quan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qms, qss = [],[]\nqms_r, qss_r = [], []\nfor quan in n_quantiles:\n    quan_mean = quan.describe().loc['mean'][['price', 'retail_price']]\n    quan_std = quan.describe().loc['std'][['price', 'retail_price']]\n    \n    qms.append(quan_mean['price'])\n    qss.append(quan_std['price'])\n    \n    qms_r.append(quan_mean['retail_price'])\n    qss_r.append(quan_mean['retail_price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Price & retail price doesnt seem to make much of a difference between the top rankers and the lowest rankers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean of mean discounted price across 15 quantiles:', np.mean(qms), '\\n Standard deviation of the mean discounted price:', np.std(qms))\nprint('\\n')\nprint('Mean of mean retailed price across 15 quantiles:', np.mean(qms_r), '\\n Standard deviation of the mean retailed price:', np.std(qms_r))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_cols = df.describe().columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply the previous test to figure out what differentiates the quantiles\n- Such features have high stdev between means of the quantiles\n\nSmall note:\n- For np functions like np.mean ad np.std, axis=0 -> operation by column, and axis=1 -> operation by row"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n #take only numeric cols\n\ndef feature_search(df, search_feature, n, with_qr=True):\n    '''\n    Search for features in the given DataFrame that differentiates quantiles based on some value\n    '''\n    features = df.describe().columns\n    print(features)\n    sorted_arr = df.sort_values(by=search_feature)[features].to_numpy()\n    # Turn this into numpy\n    # Consider making them into arrays, then make column as features and the row as the quantile means\n    # We're interested in the stdev of the quartile means\n    # 1) Find feature mean for each quartile\n    \n    fq_means = np.array([None]*len(features)) #placeholder for feature means on each quartile\n    \n    while sorted_arr.shape[0] > n:\n        quan = sorted_arr[:n, :] #take the first n\n        sorted_arr = np.delete(sorted_arr, slice(n), 0) #remove the bottom n from the sorted arr (free up some mems)\n        quan_range = np.max(quan, axis=0) - np.min(quan, axis=0)\n        if with_qr:\n            quan_feature_means = np.mean(quan, axis=0)*quan_range #yields 1D array of features' means in this quantile MULTIPLIED by the range of the quantile\n        else:\n            quan_feature_means = np.mean(quan, axis=0)\n        fq_means = np.vstack((fq_means, quan_feature_means)) # stack it on previous\n        \n    fq_means = np.delete(fq_means, 0, 0).astype('float32') # delete the first one, and convert to float32 for numpy ops\n    \n    # 2) Get np.nanstd(, 0) (by column)\n    #features_stdevs = np.std(fq_means, axis=0)\n    features_std = np.nanstd(fq_means, axis=0)\n    features_mean = np.nanmean(fq_means, axis=0)\n    std_to_mean = features_std / features_mean\n    return std_to_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_search(df_sorted_units, 'units_sold', 10, with_qr=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### WITHOUT QUAN RANGE\nstd_mean_ratio = feature_search(df_sorted_units, 'units_sold', 10, with_qr=False)\nfv_units_sold = {numeric_cols[i]: std_mean_ratio[i] for i in range(len(numeric_cols))}\nprint('Below is the measure of sensitivity (stdev to mean) for numeric features for quantiles created for the units_sold column: \\n', fv_units_sold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows a general variability of features across 10 quantiles of products split / ranked via the units sold.\n\nIt seems like discounted price exhibits the least amount of variability across units_sold quantiles ==> price doesn't separate the top sold products against other undersellers\n\nRatings are also mostly invariable with units_sold ==> doesn't separate top sellers with others\n\nMerchant's rating also doesn't matter that much.\n\nMost differentiating feature seems to be:\n- Merchant getting badge for fast shipping\n- Rating counts (maybe that the more rating the person sees, the more trustable it is)\n    - What about the actual rating? Well, the mean rating across all products don't really vary that much (3.82 mean with 0.52 stdev)\n- Express shipping\n\nHOWEVER, some of these features are binary data, so it's easy to get variability => not trustable\n"},{"metadata":{},"cell_type":"markdown","source":"Augmenting the variability to consider the range of the dataset\n\nThis is done by **multiplying the feature range in a quantile to its variability ratio**\n\n*This is equivalent to multiplying each data piece in the quantile by the range of the feature of that quartile, diminishing features with little range because it would be easy to get high variability with just a small change in the stdev.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### WITH QUAN RANGE\nstd_mean_ratio_quan = feature_search(df_sorted_units, 'units_sold', 10, with_qr=True)\nfv_units_sold_quan = {numeric_cols[i]: std_mean_ratio_quan[i] for i in range(len(numeric_cols))}\nprint('Below is the measure of sensitivity (stdev to mean) for numeric features for quantiles created for the units_sold column: \\n', fv_units_sold_quan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top 5 most sale quantile-sensitive features (i.e products' features that varies the most with products' sale):"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5 = sorted(fv_units_sold_quan, key=fv_units_sold_quan.get, reverse=True)[:10]\n\nfor feature in top_5:\n    print(feature, fv_units_sold_quan[feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature ratio between the two feature variabilities, that with and without multiplying the quantile range (to see how much the quantile range mattered for each feature):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def range_matter_func(fv_units_sold_quan, fv_units_sold):\n    range_matter = {}\n\n    for name in fv_units_sold:\n        range_matter[name] = fv_units_sold_quan[name] / fv_units_sold[name]\n\n    del range_matter['has_urgency_banner']\n\n    sort_by_rm = sorted(range_matter, key=range_matter.get, reverse=True)\n    top10_by_rm = sort_by_rm[:10]\n    top10_by_rm\n\n    for feature in top10_by_rm:\n        print(feature,\"| range_matter: \", range_matter[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"range_matter_func(fv_units_sold_quan, fv_units_sold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion:\n- Range does have quite an effect on the variability of the data."},{"metadata":{},"cell_type":"markdown","source":"# Some more chart analysis plotting units_sold against all other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in numeric_cols:\n    plt.scatter(df_sorted_units['units_sold'], df_sorted_units[feature], alpha=0.03)\n    plt.ylabel(feature)\n    plt.xlabel('units_sold')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Inventory total is more for the least unit sold ==> they couldn't sell all which makes sense\n- A familiar pattern: \n    - ***Price, retail_price converges for top sellers*** ==> we see market price here, because top sellers don't compete about price, while ***bottom sellers place price that might be not beneficial to them.*** (THIS IS EXACTLY LIKE A DEMAND CURVE)\n    - The amount of countries shipped to: surprisingly, **top sellers hone in on some countries**, while bottom sellers tend to ship literally everywhere.\n        - It would be nice to view which countries are these referring to, but dataset doesn't have it so..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}