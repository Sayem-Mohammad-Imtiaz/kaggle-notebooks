{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.impute import KNNImputer\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv'\n\ndf = pd.read_csv(path)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,7))\nplt.title('Missing values', fontweight='bold')\nax = sns.heatmap(df.isna().sum().to_frame(), annot=True, fmt='d', cmap='turbo')\nax.set_xlabel('Ammount missing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='DEATH_EVENT',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_cols = df.select_dtypes('float')\nplt.figure(figsize=(19,12))\n\nn = 1\nfor i in float_cols:\n    plt.subplot(2,2,n)\n    sns.histplot(x=i, hue='DEATH_EVENT', edgecolor='black', alpha=0.6,\n                multiple='stack', data=df)\n    sns.despine()\n    plt.title(f'Histplot of DEATH EVENT by {i}')\n    n += 1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"float_cols = df.drop('DEATH_EVENT',axis=1).select_dtypes('int')\nplt.figure(figsize=(15,17))\n\nn = 1\nfor i in float_cols:\n    plt.subplot(7,2,n)\n    sns.kdeplot(x=i, hue='DEATH_EVENT', data=df)\n    sns.despine()\n    plt.title(f'Histplot of DEATH EVENT by {i}')\n    n += 1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(19,7))\nsns.heatmap(df.corr(), annot=True, cmap='Accent')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mms = MinMaxScaler()\nx = df.drop('DEATH_EVENT', axis=1)\ny = df['DEATH_EVENT']\nX_train, X_test, y_train, y_test = train_test_split(x,y,train_size=0.75,random_state=42)\nX_train, X_test = mms.fit_transform(X_train), mms.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare configuration for cross validation test harness\nseed = 42\n\nmodels = []\nmodels.append(('LR', LogisticRegression(max_iter=300)))\nmodels.append(('DTC', DecisionTreeClassifier(criterion='entropy', random_state=42)))\nmodels.append(('KNC', KNeighborsClassifier()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('XGB', XGBClassifier(use_label_encoder=False, objective=\"binary:logistic\",\n                       learning_rate=1.0e-3,\n                       n_estimators=800, \n                        n_jobs=4,\n                       eval_metric='error')))\nmodels.append(('LightGbm', lgbm.LGBMClassifier(\n    is_unbalance=True,\n    #categorical_feature=obj_cols_idx,\n    seed=42,\n    boosting_type='goss',\n    device_type= 'cpu',#'gpu',\n    learning_rate=1.0e-3,\n    max_depth=4,\n    n_estimators=800,\n    n_jobs=4,\n    num_leaves=31,\n    reg_alpha=0.0,\n    reg_lambda=0.0\n)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(f'name: {name}, mean_cv_result: {cv_results.mean()}, std_cv_result: {cv_results.std()}')\n    \n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC_clf = RandomForestClassifier(n_estimators=1000)\nRFC_clf.fit(X_train, y_train)\nRFC_preds = RFC_clf.predict(X_test)\nprint(f'ROC_AUC_score = {roc_auc_score(RFC_preds, y_test)}')\nprint(f'Accuracy score = {accuracy_score(RFC_preds, y_test)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, RFC_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, RFC_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_auc(actual, predict):\n    \n    fpr, tpr, threshold = roc_curve(actual, predict)\n    plt.plot(fpr, tpr, color='b')\n    #plt.plot()\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.plot([0.0, 1.0], [0.0, 1.0], color='r')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC AUC = {:.3f}'.format(roc_auc_score(actual, predict)))\n\nplot_roc_auc(y_test, RFC_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['DEATH_EVENT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inverse of Null Accuracy\nprint('Inverse of Null Accuracy: ',96/(96+203))\nprint('Null Accuracy: ',203/(96+203))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Models\n\n# Scale our data in pipeline, then split\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nrf_pipeline = Pipeline(steps = [\n    ('scale', MinMaxScaler()),\n    ('RF', RandomForestClassifier(random_state=42))])\n\nsvm_pipeline = Pipeline(steps=[\n    ('scale', MinMaxScaler()),\n    ('SVM', SVC(random_state=42))])\n\nlogreg_pipeline = Pipeline(steps=[\n    ('scale', MinMaxScaler()),\n    ('LR', LogisticRegression(random_state=42))])\n\nrf_cv = cross_val_score(rf_pipeline, X_train, y_train,cv=10, scoring='f1')\nsvm_cv = cross_val_score(svm_pipeline, X_train, y_train, cv=10, scoring='f1')\nlogreg_cv = cross_val_score(logreg_pipeline, X_train, y_train, cv=10, scoring='f1')\n\nprint('Mean f1 scores: ')\nprint('Random Forest mean: ', rf_cv.mean())\nprint('SVM mean: ', svm_cv.mean())\nprint('Logistic Regression mean: ', logreg_cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%timeit\n\nrf_pipeline.fit(X_train, y_train)\nsvm_pipeline.fit(X_train, y_train)\nlogreg_pipeline.fit(X_train, y_train)\n\nrf_pred = rf_pipeline.predict(X_test)\nsvm_pred = svm_pipeline.predict(X_test)\nlogreg_pred = logreg_pipeline.predict(X_test)\n\nrf_cm  = confusion_matrix(y_test,rf_pred )\nsvm_cm = confusion_matrix(y_test,svm_pred)\nlogreg_cm  = confusion_matrix(y_test,logreg_pred )\n\nrf_f1  = f1_score(y_test,rf_pred)\nsvm_f1 = f1_score(y_test,svm_pred)\nlogreg_f1  = f1_score(y_test,logreg_pred)\n\nprint('Mean f1 scores:')\n\nprint('RF mean :',rf_f1)\nprint('SVM mean :',svm_f1)\nprint('LR mean :',logreg_f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pretty good accuracy, but poor recall!\n# Unscaled and not upsampled test\n\nfrom sklearn.model_selection import GridSearchCV\n\nn_estimators =[64,100,128,200]\nmax_features = [2,3,5,7]\nbootstrap = [True,False]\n\nparam_grid = {'n_estimators':n_estimators,\n             'max_features':max_features,\n             'bootstrap':bootstrap}\n\nrfc = RandomForestClassifier()\ngrid = GridSearchCV(rfc,param_grid)\ngrid.fit(X_train,y_train)\ngrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's use those params now\n\nrfc = RandomForestClassifier(**grid.best_params_)\n\nrfc.fit(X_train,y_train)\n\nrfc_tuned_pred = rfc.predict(X_test)\n\nprint(classification_report(y_test,rfc_tuned_pred))\n\nprint('Accuracy Score: ',accuracy_score(y_test,rfc_tuned_pred))\nprint('F1 Score: ',f1_score(y_test,rfc_tuned_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, rfc_tuned_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_auc(y_test, rfc_tuned_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rfc.predict(x)\nsubmission = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nsubmission['Prediction'] = predictions\nsubmission\nsubmission.to_csv(\"submission_RFC_21032021.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}