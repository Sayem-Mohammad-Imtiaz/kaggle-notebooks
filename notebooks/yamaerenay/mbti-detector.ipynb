{"cells":[{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#import modules\nimport numpy as np\nimport torch\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler as Sampler\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchtext\nfrom gensim.models import FastText\n!pip install fse\nfrom fse.models import Average\nfrom fse import IndexedList\nfrom torch.utils.data import Dataset\nfrom torchtext.data import Field\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport re\ndf = pd.read_csv(\"/kaggle/input/mbti-type/mbti_1.csv\")\n\n#split all sentences\nnew_features, new_labels = [], []\nfor index in tqdm(range(len(df[\"posts\"].values))):\n    person = df[\"posts\"].iloc[index]\n    for sent in person.split(\"|||\"):\n        new_features.append(sent)\n        new_labels.append(df[\"type\"].iloc[index])\n        \ndf1 = pd.DataFrame({\"feature\": new_features, \"label\": new_labels})\n\n#lowercase\ndf1.feature = df1.feature.str.lower()\n\n#delete urls\nurl_pattern = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\nnew_features_url = re.sub(url_pattern, \"\", \"|||\".join(df1.feature))\n\ndf1.feature = new_features_url.split(\"|||\")\n\nnew_features_url2, new_labels_url2 = [], []\nfor index in tqdm(range(len(df1.feature))):\n    if len(df1.feature.iloc[index]) > 1:\n        new_features_url2.append(df1.feature.iloc[index])\n        new_labels_url2.append(df1.label.iloc[index])\n        \ndf2 = pd.DataFrame({\"feature\": new_features_url2, \"label\": new_labels_url2})\n\n#map label values to independent binary values\ndef map_label(df, charlist = [\"E\", \"N\", \"T\", \"P\"]):\n    labels = []\n    map_char = lambda label: [(charlist[i] == label[i])*1 for i in range(len(charlist))]\n    for label in df[\"label\"].values: \n        labels.append(map_char(label))\n    dfx = pd.DataFrame(labels, columns = charlist)\n    return pd.concat([df.drop(\"label\", 1), dfx], axis = 1) \n\ndf3 = map_label(df2)\n\n#tokenize and vectorize sentences\ntext_field = Field(\n    tokenize='basic_english', \n    lower=True\n)\nlabel_field = Field(sequential=False, use_vocab=False)\npreprocessed_text = df3['feature'].apply(lambda x: text_field.preprocess(x))\n\nsentences = preprocessed_text.tolist()\nft = FastText(sentences, min_count=1, size=100)\nvectorizer = Average(ft)\nvectorizer.train(IndexedList(sentences))\n\ntmps = [(df3[\"feature\"].iloc[i].split(\" \"), i) for i in tqdm(range(len(df3.index)))]\n\nx = vectorizer.infer(tmps)\ny = df3.drop(\"feature\", 1).values\n\n#x,y -> dataset\nclass CustomDataset(Dataset):\n    def __init__(self, x, y, shuffle=True):\n        super().__init__()\n        if(shuffle): n = np.random.permutation(len(x))\n        else: n=range(len(x))\n        x = torch.from_numpy(x[n])\n        y = torch.from_numpy(y[n])\n        self.data = list(zip(x, y))\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n#whole dataset -> dataset in batches\nclass CustomDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b, self.device)\n    def __len__(self):\n        return len(self.dl)\n    def __getitem__(self, idx):\n        return self.dl[idx]\n\n#persist data/model in gpu memory\ndef get_default_device():\n    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndef to_device(data, device):\n    if(isinstance(data, (list, tuple))):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n#data -> train, validation\ndef split_indices(n, val_pct):\n    every = np.random.permutation(n)\n    return every[int(n*val_pct):], every[:int(n*val_pct)]    \n\n#initial hyperparameters\nTEST_PCT = 0.2\nVAL_PCT = 0.1\nDEVICE = get_default_device()\nBATCH_SIZE = 20\n\n#datasets\ndataset = CustomDataset(x, y)\ntest_dataset = dataset[:int(len(dataset) * TEST_PCT)]\ndataset = dataset[int(len(dataset) * TEST_PCT):]\n\n#dataloaders\ntrain_indices, val_indices = split_indices(len(dataset), VAL_PCT)\ntrain_dl = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=Sampler(train_indices))\nval_dl = DataLoader(dataset, batch_size = BATCH_SIZE, sampler = Sampler(val_indices))\ntrain_dl = CustomDataLoader(dl=train_dl, device=DEVICE)\nval_dl = CustomDataLoader(dl=val_dl, device=DEVICE)\n\n#detector model\nclass Detector(nn.Module):\n    def __init__(self, act_fn, end_act_fn, *layers):\n        super().__init__()\n        self.module_list = nn.ModuleList()\n        for i in range(len(layers) - 1):\n            self.module_list.append(nn.Linear(layers[i], layers[i+1]))\n        self.act_fn = act_fn\n        self.end_act_fn = end_act_fn\n        \n    def forward(self, xb):\n        for i in range(len(self.module_list)):\n            xb = self.module_list[i](xb)\n            if(i==len(self.module_list)-1):\n                xb = self.end_act_fn(xb)\n            else:\n                xb = self.act_fn(xb)\n        return xb\n    \ndef loss_batch(model, xb, yb, loss_fn, opt_fn=None, metric_fn=None, verbose=True, n_class=4):\n    pb = list(model(xb).reshape(n_class, -1))\n    yb = list(yb.reshape(n_class, -1)*1.0)\n    loss = 0\n    for i in range(len(pb)):\n        p, y = pb[i], yb[i]\n        loss += LOSS_FN(p, y)\n    if opt_fn is not None:\n        opt_fn.zero_grad()\n        loss.backward()\n        opt_fn.step()\n    num = len(xb)\n    metric=None\n    metric_string = \"\"\n    if metric_fn is not None:\n        metrics = []\n        for i in range(len(pb)):\n            p, y = pb[i], yb[i]\n            p = predict(p)\n            metrics.append(metric_fn(p, y))\n        metric = sum(metrics)/n_class\n        metric_string = f\", Metric: {metric:.4f}\"\n    string_to_format = f\"Loss: {loss:.4f}\"+metric_string\n    if(verbose):\n        print(string_to_format)\n    return loss.item(), num, metric\n    \ndef evaluate(model, val_dl, loss_fn, metric_fn=None, verbose=True):\n    with torch.no_grad():\n        losses, nums, metrics = zip(*[loss_batch(model=model, xb=xb, yb=yb, loss_fn=loss_fn, metric_fn=metric_fn, verbose=False) for xb, yb in val_dl])\n        total = np.sum(nums)\n        loss = np.sum(np.multiply(losses, nums))/total\n        metric=None\n        metric_string = \"\"\n        if metric_fn is not None:\n            metric = np.sum(np.multiply(metrics, nums))/total\n            metric_string = f\", Metric: {metric:.4f}\"\n        string_to_format = f\"Loss: {loss:.4f}\" + metric_string\n        if(verbose):\n            print(string_to_format)\n        return loss, total, metric\n    \ndef fit(model, epochs, train_dl, val_dl, loss_fn, opt_fn=None, metric_fn=None, verbose=True):\n    losses, metrics = [], []\n    for epoch in tqdm(range(epochs)):\n        for i, (xb, yb) in enumerate(tqdm(train_dl)):\n            loss_batch(model=model, xb=xb, yb=yb, loss_fn=loss_fn, opt_fn=opt_fn, metric_fn=metric_fn, verbose=False)\n        loss, _, metric = evaluate(model=model, val_dl=val_dl, loss_fn=loss_fn, metric_fn=metric_fn, verbose=False)\n        metric_string = \"\"\n        if metric is not None:\n            metric_string = f\", Metric: {metric:.4f}\"\n        string_to_format = f\"Epoch: [{epoch+1}/{epochs}], Loss: {loss:.4f}\"+metric_string\n        if(verbose):\n            print(string_to_format)\n        losses.append(loss)\n        metrics.append(metric)\n    return losses, metrics\n\n#metric=accuracy\npredict = lambda pb: torch.round(pb)\ndef accuracy(pb, yb):\n    return torch.sum(pb==yb).item()/len(pb)\n\n#build model\nLAYERS = [100, 1000, 400, 4]\nACT_FN = torch.tanh\nEND_ACT_FN = torch.sigmoid\nMODEL = to_device(Detector(ACT_FN, END_ACT_FN, *LAYERS), device=DEVICE)\n\n#last hyperparameters\nLR = 5e-2\nOPT_FN = torch.optim.SGD(list(MODEL.parameters()), lr=LR)\nLOSS_FN = nn.BCELoss()\nMETRIC_FN = accuracy\nEPOCHS = 50\nVERBOSE = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(model=MODEL, epochs=10, train_dl=train_dl, val_dl=val_dl, loss_fn=LOSS_FN, metric_fn=METRIC_FN, opt_fn=OPT_FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}