{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Standard python import\nimport math, datetime, os \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualisation \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as mn\n\n# Stats\nfrom scipy import stats\n\n# ML\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV, cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier, plot_importance\n\nimport lime\nimport shap\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC, ClassPredictionError, PrecisionRecallCurve\nfrom yellowbrick.features import FeatureImportances\nfrom yellowbrick.model_selection import LearningCurve, ValidationCurve\n\n\n# Setting parameters for plotting \nplt.rcParams['figure.figsize'] = 8,6\nplt.rcParams['image.cmap'] = 'viridis'\nplt.style.use('ggplot')\n%config InlineBackend.figure_format = 'png'\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import csv\nemployee = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nemployee.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding attrition to binary variable \nemployee['Attrition'] = np.where(employee.Attrition=='Yes',1,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the types of variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspecting the types of variables in the dataset\nemployee.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving the categorical variables\ncategorical = employee.select_dtypes(include='object')\nprint('There are {} categorical variables'.format(len(categorical.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving the numerical variables\nnumerical = employee.select_dtypes(include=['int64','float64'])\nprint('There are {} numerical variables'.format(len(numerical.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the categorical variables\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing the numerical variables \nnumerical.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Continuous variables (13)**: Age, DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, TotalWorkingYears, TrainingTimesLastYear, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager\n\n* **Constant variable (2)**: StandardHours, EmployeeCount\n\n* **Discrete variables (9)**: DistanceFromHome, Education, EnvironmentSatisfaction, JobInvolvement, JobLevel, JobSatisfaction, PerformanceRating, RelationshipSatisfaction, WorkLifeBalance, \n\n* **Binary variables (1)**: StockOptionLevel, Attrition (target)\n\n* **ID variable (1)**: EmployeeNumber"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding the values in discrete variables\nfor var in [\"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \n            \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"PerformanceRating\", \n            \"RelationshipSatisfaction\", 'TrainingTimesLastYear']:\n    print(var, 'values: ', employee[var].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the types of problem within the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values\nemployee.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values within the dataset which is great! Let's move on explore the outliers present."},{"metadata":{},"cell_type":"markdown","source":"### Outliers"},{"metadata":{},"cell_type":"markdown","source":"#### Outliers in continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"non_cont = ['Attrition', 'BusinessTravel', 'Department', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'NumCompaniesWorked', 'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', \n            'RelationshipSatisfaction', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager',\n           'DistanceFromHome', 'EmployeeCount', 'EmployeeNumber', 'StandardHours', 'StockOptionLevel']\ncontinuous = [var for var in numerical.columns if var not in non_cont]\ncontinuous","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create boxplot to visualise the outliers in the continous variables\nfor var in continuous:\n    plt.figure(figsize=(10,4), dpi=300)\n    plt.subplot(1,2,1)\n    fig = employee.boxplot(column=var)\n    fig.set_title('')\n    fig.set_ylabel(var)\n    \n    plt.subplot(1,2,2)\n    fig = employee[var].hist(bins=20)\n    fig.set_ylabel('Number of employees')\n    fig.set_xlabel(var)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have identified several variables that seems to contain outliers. We have also identifed a few variables are not normally distributed. \n\n**Outliers Present**: MonthlyIncome, NumCompaniesWorked, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager\n\n**Not Normally Distributed**: MonthlyIncome, NumCompaniesWorked, \nTotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager"},{"metadata":{},"cell_type":"markdown","source":"#### Outliers in discrete variables"},{"metadata":{},"cell_type":"markdown","source":"To find the outliers in the discrete variables, we will have to calculate the overall percentage of employees in each value that a discrete variables can take. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outliers in discrete variables\ndiscrete = []\nfor var in employee.columns:\n    if len(employee[var].unique()) <20:\n        discrete.append(var)\n        \ndiscrete = [var for var in discrete if var not in ['StandardHours', 'EmployeeCount', 'StockOptionLevel', 'EmployeeCount', 'EmployeeNumber']]\ndiscrete","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in discrete:\n    print(employee[var].value_counts()/np.float(len(employee)))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, we classify any values of a variable that consist of less that 1% to be an outlier. \n\n**Outliers**: YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager"},{"metadata":{},"cell_type":"markdown","source":"#### Number of labels: Cardinality"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical.columns:\n    print(var, 'contains', len(employee[var].unique()), 'labels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the categorical data consist of only low number of labels. In this case, we do not need to be concern with high cardinality (i.e containing a lot of labels)."},{"metadata":{},"cell_type":"markdown","source":"## Further EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# BusinessTravel\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(x='Attrition', data=employee, palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='Gender', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BusinessTravel\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='BusinessTravel', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Department\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='Department', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EducationField\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='EducationField', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='Gender', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # JobRole\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(25,6), dpi=300)\n    _ = sns.countplot(x='JobRole', data=employee, palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MaritalStatus\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='MaritalStatus', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OverTime\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    _ = sns.countplot(y='OverTime', data=employee, hue='Attrition', palette='viridis',\n                     saturation=1,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop features with constant values and redundant features\nemployee = employee.drop(['StandardHours','Over18','EmployeeCount', 'EmployeeNumber'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking dataframe\nemployee.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperating into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(employee, employee.Attrition, test_size=0.2, random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check shape\nemployee.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers in numerical variables "},{"metadata":{},"cell_type":"markdown","source":"In order to handle both outliers and non-normally distributed variables, we can use a Decision Tree to help us discretised the variables. The Decision Tree can help us find the optimal number of buckets accordingly. \n\nMore can be refered to this article: https://towardsdatascience.com/discretisation-using-decision-trees-21910483fa4b"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tree_binariser(var):\n    score_ls = []\n\n    for tree_depth in [1,2,3,4]:\n        # Calling the model\n        tree_model = DecisionTreeRegressor(max_depth=tree_depth)\n\n        # Train the model with 3 fold CV\n        scores = cross_val_score(tree_model, X_train[var].to_frame(), y_train, cv=3, scoring='neg_mean_squared_error')\n        score_ls.append(np.mean(scores))\n\n    # Finding the depth with the smallest MSE\n    depth = [1,2,3,4][np.argmax(score_ls)]\n    #print(score_ls, np.argmax(score_ls), depth)\n\n    # Transform the continous variable with the tree\n    tree_model = DecisionTreeRegressor(max_depth=depth)\n    tree_model.fit(X_train[var].to_frame(), X_train.Attrition)\n    X_train[var] = tree_model.predict(X_train[var].to_frame())\n    X_test[var] = tree_model.predict(X_test[var].to_frame())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the continuous variables\nfor var in continuous:\n    tree_binariser(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[continuous].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the number of bins in each continuous variables\nfor var in continuous:\n    print(var, len(X_train[var].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising LabelEncoder()\nle = LabelEncoder()\n\n# Retrieving categorical columns\ncategorical = employee.select_dtypes(include='object')\ncategorical = categorical.columns\n\nfor var in categorical:\n    X_train[var] = le.fit_transform(X_train[var])\n    X_test[var] = le.fit_transform(X_test[var])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables for all categorical features\n\ncat = [\"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \n            \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"PerformanceRating\", \n            \"RelationshipSatisfaction\", 'TrainingTimesLastYear', \"BusinessTravel\",\n        \"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\", \"OverTime\", 'WorkLifeBalance',\n      'StockOptionLevel', 'NumCompaniesWorked']\n\nfor var in cat:\n    X_train[var] = X_train[var].astype('object')\n    X_test[var] = X_test[var].astype('object')\n    \nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop attrition\nX_train = X_train.drop('Attrition', axis=1)\nX_test = X_test.drop('Attrition', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialise StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we have fitted our training set, we can use it accordingly in algorithms that requires normalized dataset to perform better."},{"metadata":{},"cell_type":"markdown","source":"## Building Machine Learning Models"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1st model - Logistic Regression \nlogr = LogisticRegression()\nlogr.fit(sc.transform(X_train), y_train)\nlogr.score(sc.transform(X_test), y_test), cross_val_score(logr, sc.transform(X_test), y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting confusion matrix for logr\nwith sns.plotting_context('paper'):\n    fig, ax = plt.subplots(figsize=(8,8), dpi=300)\n    cm_viz = ConfusionMatrix(logr, cmap=False, percent=False)\n    cm_viz.fit(sc.transform(X_train), y_train)\n    cm_viz.score(sc.transform(X_test), y_test)\n    cm_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report for logr\nprint(classification_report(y_test, logr.predict(sc.transform(X_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2nd model - Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf.score(X_test, y_test), cross_val_score(rf, X_test, y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting confusion matrix for rf\nwith sns.plotting_context('paper'):\n    fig, ax = plt.subplots(figsize=(8,8), dpi=300)\n    cm_viz = ConfusionMatrix(rf, cmap=False, percent=False)\n    cm_viz.fit(X_train, y_train)\n    cm_viz.score(X_test, y_test)\n    cm_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report for logr\nprint(classification_report(y_test, rf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\n\nxgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\nxgb.score(X_test, y_test), cross_val_score(xgb, X_test, y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting confusion matrix for xgb\nwith sns.plotting_context('paper'):\n    fig, ax = plt.subplots(figsize=(8,8), dpi=300)\n    cm_viz = ConfusionMatrix(xgb, cmap=False, percent=False)\n    cm_viz.fit(X_train, y_train)\n    cm_viz.score(X_test, y_test)\n    cm_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report for logr\nprint(classification_report(y_test, xgb.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting ROC curve for logr, rf, xgb\n\nwith sns.plotting_context('notebook'):\n    fig, (ax, ax2, ax3) = plt.subplots(ncols=3, figsize=(15,8), dpi=300)\n    roc_viz = ROCAUC(logr, ax=ax, micro=False)\n    roc_viz.score(X_test, y_test)\n    roc_viz.finalize()\n    roc_viz2 = ROCAUC(rf, ax=ax2, micro=False)\n    roc_viz2.score(X_test, y_test)\n    roc_viz2.finalize()\n    roc_viz3 = ROCAUC(xgb, ax=ax3, micro=False)\n    roc_viz3.score(X_test, y_test)\n    roc_viz3.finalize()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMOTE"},{"metadata":{},"cell_type":"markdown","source":"Since our target is imbalanced, we attempt to oversample and see if our models performs better."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new, y_train_new = sm.fit_sample(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logr = LogisticRegression()\nlogr.fit(sc.transform(X_train_new), y_train_new)\nlogr.score(sc.transform(X_test), y_test), cross_val_score(logr, sc.transform(X_test), y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, logr.predict(sc.transform(X_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2nd model - Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_train_new, y_train_new)\nrf.score(X_test, y_test), cross_val_score(rf, X_test, y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, rf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\n\nxgb.fit(X_train_new, y_train_new, eval_set=[(X_test, y_test)], verbose=False)\nxgb.score(X_test, y_test), cross_val_score(xgb, X_test, y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, xgb.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observed that Logistic Regression remains the champion model with both the highest accuracy and highest recall. Let's try to tuned the model and see if we can achieved better performance."},{"metadata":{},"cell_type":"markdown","source":"## Tuning: Regularization and Hyperparameters\nBelow are some of the hyperparameters that can be optimized for both the Logistic Regression to get better results.\n\n**penalty** - Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is only supported by the ‘saga’ solver. If ‘none’ (not supported by the liblinear solver), no regularization is applied.\n\n**C** - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n\n**solver** - Algorithm to use in the optimization problem.\n\nLet's setup our search grid for Logistic Regression!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Hyperparameter turning of logr\nparam_grid = {\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l1', 'l2'],\n    'C' : [100, 10, 1.0, 0.1, 0.01]\n}\n\n# Instantiate the grid search\nlogr_g = GridSearchCV(logr, param_grid=param_grid, n_jobs=-1, verbose=0, cv=5, error_score=0)\nlogr_g.fit(sc.transform(X_train), y_train)\n# Summarizing results\nprint(\"Best: %f using %s\" % (logr_g.best_score_, logr_g.get_params()))\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logr_g.score(sc.transform(X_test), y_test), cross_val_score(logr_g, sc.transform(X_test), y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance remains approximately the same but fare slightly worse than the untuned model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"logr_g = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using untuned model's parameter\nlogr_g.fit(sc.transform(X_train_new), y_train_new)\nlogr_g.score(sc.transform(X_test), y_test), cross_val_score(logr_g, sc.transform(X_test), y_test, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, logr_g.predict(sc.transform(X_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.plotting_context('paper'):\n    fig, ax = plt.subplots(figsize=(8,8), dpi=300)\n    cm_viz = ConfusionMatrix(logr_g, cmap=False, percent=False)\n    cm_viz.fit(sc.transform(X_train), y_train)\n    cm_viz.score(sc.transform(X_test), y_test)\n    cm_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Interpretation"},{"metadata":{},"cell_type":"markdown","source":"So now considering that Logistic Regression has performed the best out of the 3 models, but one thing is lacking. What variables is the best in predicting employee attrition? Let's explore the model further!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising js\nshap.initjs()\n\n# Create a tree explainer and understanding the values we have \nshap_ex = shap.LinearExplainer(logr_g, X_test)\nvals = shap_ex.shap_values(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at feature importance \nshap.summary_plot(vals, X_test, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on summary plot generated, we identified that OverTime, YearsSinceLastPromotion, JobLevel, EnvironmentSatisfaction are the top 5 more features in predicting the outcome of employee attrition. However, the summary plot on shows the average impact, let's dive deeper and look at how the values of these variables affect the outcome as a whole."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a summary plot to see how the value of the features help us in predicting the patients\n\nwith sns.plotting_context('talk'):\n    fig, ax = plt.subplots(figsize=(10,6), dpi=300)\n    shap.summary_plot(vals, X_test, alpha=.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the summary plot generated, we identified a few key points that results in employee attrition.\n\n1. **OverTime** - Both OverTime_0 and OverTime_1 indicates the same thing, which is whether an employee does overtime. In this case as reflected by the plot, employees who overtime more, is more associated with employee attrition. This could mean that employees that do no have a proper worklife balance, or spend more time couped up at work are likely to be more unhappy hence, leaving the company. \n\n\n2. **YearsSinceLastPromotion** - We observed that employees that have not been promoted in a long time (higher up the range within the dataset) is found to be strongly associated with employee attrition. This could means that employees that stayed in a company for a long period of time and yet passed on for promotion could result in employee feeling neglected by the management, which possibly result in them leaving the company. \n\n\n3. **JobLevel_1** - Employees who are associated the lower job level such as JobLevel=1, are also more likely to result in employee attrition. This could indicate the employees are still new in their career, possibly in entry level jobs and hence, would pursue other jobs if the current one is not suitable or they are looking for a change. \n\n\n4. **EnvironmentSatisfaction_1** - Employees who indicated 'low' in their satisfaction in their work environment are also associated with employee attrition. This can be explained by the fact that having an environment that is not suitable or to their liking is not a good way to keep an employee. This could result in employee attrition as employees are looking for a change in their job environment. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a force plot to explain the first 100 samples\nshap.force_plot(shap_ex.expected_value, vals[:100], X_test.iloc[:100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To better explain why each individal employee is classified into the respective classes (0,1), let's use the individual force plot and LIME to better understand!\n\nLet's consider taking the no.50 employee of the test set for explaination!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieving employee's 50 details\nX_test.iloc[[50]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting using the logr_g\nlogr_g.predict(sc.transform(X_test.iloc[[50]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explaining why no.49 is classified as no employee attrition.\nshap.force_plot(shap_ex.expected_value, vals[50,:], X_test.iloc[50,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dataframe to a matrix \nlogr_g.fit(X_test.as_matrix(), y_test.as_matrix())\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(\n    X_test.values,\n    feature_names=X_test.columns,\n    class_names=[0,1]\n)\n\n# Taking row 50 and intepreting the prediction\npos = 50\nexp = explainer.explain_instance(X_test.iloc[pos].values, \n                                 logr_g.predict_proba)\n_ = exp.show_in_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From both SHAP and LIME we saw that employee 49 is indeed classifed as no employee attrition. Based on SHAP, we understood the main reason why the classification occured was due to the lower number of the years in the current role and time with the current manager. This could indicate that the employee is still relatively new to the job and hence there is no employee attrition."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}