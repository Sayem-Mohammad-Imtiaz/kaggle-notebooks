{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up\nThe below code contains necessary steps for setting up our machine learning environment. Key features are described in the comments.","metadata":{}},{"cell_type":"markdown","source":"## Just Some Fun Kaggle Info\nI wanted to see if I could use a faster gpu... sadly i couldnt :(","metadata":{}},{"cell_type":"code","source":"import torch\n\n# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:30.155011Z","iopub.execute_input":"2021-06-29T06:26:30.155445Z","iopub.status.idle":"2021-06-29T06:26:30.163885Z","shell.execute_reply.started":"2021-06-29T06:26:30.155407Z","shell.execute_reply":"2021-06-29T06:26:30.162656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n### goal:\nto find a relationship between water potability and various features of water\n\n### Why?\nWe know that potable water is the water that we can drink, use in cooking, etc. In Australia we are very fortunate to have potable water in urban areas, this however is not always the case and some communities, such as the ones in Bali, don't have access to potable water. A machine learning model that can predict accurately whether water is potable based on measurable traits could help improve food safety for many. \n\nWater is made un-drinkable from the contaminants that it contains, contaminants being any particles or molecules other than water. This would mean water created by reverse osmosis a growing water source in Australia would be potable because the process used to make it physically separates the water from contaminants such as salt. In most water there are three main types of contaminants, physical, chemical, biological, radiological. \n\n### Columns Description\n1. ph: pH of 1. water (0 to 14).\n2. Hardness: Capacity of water to precipitate soap in mg/L.\n3. Solids: Total dissolved solids in ppm.\n4. Chloramines: Amount of Chloramines in ppm.\n5. Sulfate: Amount of Sulfates dissolved in mg/L.\n6. Conductivity: Electrical conductivity of water in μS/cm.\n7. Organic_carbon: Amount of organic carbon in ppm.\n8. Trihalomethanes: Amount of Trihalomethanes in μg/L.\n9. Turbidity: Measure of light emitting property of water in NTU.\n10. Potability: Indicates if water is safe for human consumption. Potable -1 and Not potable -0\n\n### prediction Target\nI am predicting if water is potable so setting water potability as the target would be a good idea.\n\n### Hypotheses \n1. What will be the best model? If a [Support-Vector Machine](http://) is used I will get the highest accuracy\n2. What types of features will have the strongest effect on predictions. I believe the PH will be the most important\n3. Goal for accuracy of predictions! 100% (obviously) however a more realistic goal would be 75% accuracy","metadata":{}},{"cell_type":"markdown","source":"# Setup\nI need to import modules so much of my code can function. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualisation purposes\nfrom sklearn.tree import DecisionTreeClassifier ,plot_tree # Our model and a handy tool for visualising trees\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport plotly.express as px\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n\nimport pandas_profiling as pp\nfrom collections import Counter\nwarnings.simplefilter(action='ignore', category=Warning)\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:30.183424Z","iopub.execute_input":"2021-06-29T06:26:30.183859Z","iopub.status.idle":"2021-06-29T06:26:30.199309Z","shell.execute_reply.started":"2021-06-29T06:26:30.183819Z","shell.execute_reply":"2021-06-29T06:26:30.198461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gather and explore the data\n### Discussion of why data was selected.\nI chose this data for several reasons. It has a plethora of features, such as chloramines, a somewhat large amount of data with 3277 lines. This data set was also the best data set compared to others on Kaggle as there, at the time of choosing, were no alternatives, most importantly it has potability as a feature\n\n### how it's suitable for making predictions. \nThis data is good for making predictions as all of the features or contaminants are listed on the United States of America's National Primary Drinking Water Regulations that establish maximum contaminant levels (MCLs) for various contaminants. This means the features have an effect on the prediction target, water potability, so by training a model on this data it should be accurate\n\t- what is being predicted and why. \n\n### Columns Description\n1. ph: pH of 1. water (0 to 14).\n2. Hardness: Capacity of water to precipitate soap in mg/L.\n3. Solids: Total dissolved solids in ppm.\n4. Chloramines: Amount of Chloramines in ppm.\n5. Sulfate: Amount of Sulfates dissolved in mg/L.\n6. Conductivity: Electrical conductivity of water in μS/cm.\n7. Organic_carbon: Amount of organic carbon in ppm.\n8. Trihalomethanes: Amount of Trihalomethanes in μg/L.\n9. Turbidity: Measure of light emitting property of water in NTU.\n10. Potability: Indicates if water is safe for human consumption. Potable -1 and Not potable -0 \n","metadata":{}},{"cell_type":"code","source":"train_file_path = '../input/water-potability/water_potability.csv'\n\n# Create a new Pandas DataFrame with our training data\nclass_train_data = pd.read_csv(train_file_path)\n\n#class_test_data.columns\nclass_train_data.describe(include='all')\nclass_train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:30.204639Z","iopub.execute_input":"2021-06-29T06:26:30.20519Z","iopub.status.idle":"2021-06-29T06:26:30.260007Z","shell.execute_reply.started":"2021-06-29T06:26:30.205147Z","shell.execute_reply":"2021-06-29T06:26:30.259008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lets Have a Closer Look at the Data\nI will use box and whiskers plots along with a distribution graph to see what the data \"looks\" like and so i can make an informed decision on what model and features to use ","metadata":{}},{"cell_type":"code","source":"def boxdistriplot(columnName):\n    if not columnName == 'Potability':\n        sns.catplot(x=\"Potability\", y=columnName, data=class_train_data, kind=\"box\");\n        plt.figure()\n        ax = sns.distplot(class_train_data[columnName][class_train_data.Potability == 1],color=\"darkturquoise\", rug=True)\n        sns.distplot(class_train_data[columnName][class_train_data.Potability == 0], color=\"lightcoral\", rug=True);\n        plt.legend(['Potable', 'Not Potable']) \n\n\nfor column in class_train_data.columns:\n    boxdistriplot(column)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:30.261737Z","iopub.execute_input":"2021-06-29T06:26:30.262054Z","iopub.status.idle":"2021-06-29T06:26:36.179594Z","shell.execute_reply.started":"2021-06-29T06:26:30.262013Z","shell.execute_reply":"2021-06-29T06:26:36.17851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis\n\nFrom this, we can see that most of the data could go either way whether it potable or not. This tells us that it is gonna be hard to predict and also suggests that the different contaminants all have a similar bearing on potability.","metadata":{}},{"cell_type":"markdown","source":"# Prepare the data\nIn this example, we want to predict whether or not water is potable Therefore the 'potability' column is our prediction target.\n\nBefore we can separate our prediction target 'y' from the rest of the data, we need to do some preparation so that there aren't any rows with missing values as our machine learning model will not be able to handle them.\n\n### dropping rows\nas I stated above all of the contaminants have a similar effect on whether water is potable, this is partly why I have decided against dropping any features as they are all equal, in addition, this data set has 3000+ rows, and the effect of dropping the rows with missing values shouldn't take away from the data that much. \n","metadata":{}},{"cell_type":"code","source":"# Let's reduce our data to only the features we need and the target.\n# The features we chose have similar 'count' values when we describe() them\n# We need to keep the target as part of our DataFrame for now.\nselected_columns = ['ph', 'Hardness', 'Chloramines', 'Conductivity', 'Organic_carbon', 'Trihalomethanes','Turbidity', 'Potability']\nX_columns = ['ph', 'Hardness', 'Chloramines', 'Conductivity', 'Organic_carbon', 'Trihalomethanes','Turbidity']\n# Create our new training set containing only the features we want\nprepared_data = class_train_data[selected_columns]\n\n# Drop rows (axis=0) that contain missing values\nprepared_data = prepared_data.dropna(axis=0)\n\n# Check that you still have a good 'count' value. The value should be the same for all columns.\n# If your count is very low then you may need to remove features with the lowest count.\nprepared_data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:36.181579Z","iopub.execute_input":"2021-06-29T06:26:36.181909Z","iopub.status.idle":"2021-06-29T06:26:36.225397Z","shell.execute_reply.started":"2021-06-29T06:26:36.181875Z","shell.execute_reply":"2021-06-29T06:26:36.224395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Separate Features From Target\nNow that we have a set of data (as a Pandas DataFrame) without any missing values, let's separate the features we will use for training from the target.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Separate out the prediction target\ny = prepared_data.Potability\n\n# Drop the target column (axis=1) from the original dataframe and use the rest as our feature data\nX = prepared_data.drop('Potability', axis=1)\n\n# Take a look at the data again\nX.head()\n#y.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:36.227309Z","iopub.execute_input":"2021-06-29T06:26:36.227676Z","iopub.status.idle":"2021-06-29T06:26:36.244981Z","shell.execute_reply.started":"2021-06-29T06:26:36.227645Z","shell.execute_reply":"2021-06-29T06:26:36.243655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One Hot Encode Categorical Data \nOne of the difficulties of working with machine learning models is that most of them can only work with numerical features. Just in case there are any problems with numbers being words ill-use one-hot encoding, it also might gain my marks, but that's not important in the real world. \n\nOne Hot Encoding is the most widely used approach for converting a character, word, or sentence into a number. One Hot Encoding creates new (binary) columns, indicating the presence of each possible category value in the original data. In other words, it separates each of the options for a category into a separate column, where a 1 means that the row fits the category in question and a zero indicates it doesn't.\n\n## Split data into training and testing data.\nSplitting the training set into two subsets is important because you need to have data that your model hasn't seen yet with actual values to compare to your predictions to be able to tell how well it is performing. If this isn't done the model can't be accurately validated and tuned for things such as over-fitting. I have used a simple method and split 25% of the dataset for validation.  \n","metadata":{}},{"cell_type":"code","source":"# One hot encode the features. This will only act on columns containing non-numerical values.\none_hot_X = pd.get_dummies(X)\n\none_hot_X.head()\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(one_hot_X, y, test_size = 0.25, random_state=1)\n\n#verify the split, verification data should be less than half\n\nval_X.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:36.246827Z","iopub.execute_input":"2021-06-29T06:26:36.247148Z","iopub.status.idle":"2021-06-29T06:26:36.27272Z","shell.execute_reply.started":"2021-06-29T06:26:36.247116Z","shell.execute_reply":"2021-06-29T06:26:36.271437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we know that there won't be any errors from non-numerical data\n\n# Choose and Train a Model\nNow that we have data our model can digest, let's use it to train a model and make some predictions. We're going to use a __Decision Tree Classifier__ which is different from the Decision Tree Regressor used in the [Intro to Machine Learning course](https://www.kaggle.com/learn/intro-to-machine-learning) in that it makes categorical predictions instead of continuous numerical predictions. \n\nIn this case, the category we want to predict is whether or not water is potable, with the output being a 1 if it is and a 0 if it is not, making it well suited to this model. Decision Tree Classifiers are also able to work with non-numerical prediction targets as well. For example, you might have a 'y' that contains the names (as strings) of different species of flowers. It's only features that need to be encoded.\n\nFor an example of a Decision Tree Classifier working with a non-numerical 'y' and a more in-depth look at how they work, take a look at this Kaggle notebook (https://www.kaggle.com/chrised209/decision-tree-modeling-of-the-iris-dataset)\n\nI have chosen this model as a sort of baseline as it is the \"simplest\" model to see how it would perform compared to other models, it should do ok, better than a coin flip but not as good as others. \n\nOk, let's train our model and see what it looks like.","metadata":{}},{"cell_type":"code","source":"# Create a decision tree classifier with a maximum depth of 3 for easy display later on\n# Try changing the max_depth to see what happens\nclass_predictor = DecisionTreeClassifier(max_depth=3)\n\n# Train the model on the one hot encoded data\nclass_predictor.fit(train_X, train_y)\n\n# Let's plot the tree to see what it looks like!\nplt.figure(figsize = (20,10))\nplot_tree(class_predictor,\n          feature_names=train_X.columns,\n          class_names=['0', '1', '2', '3', '4', '5'],\n          filled=True)\nplt.show()\n\n# Note for class_names we've used strings to represent each of the values.\n# However, the real values are 0 for perished and 1 for survived.\n# Class names for plot_tree must be strings so to get the right replacement values\n# we had to do the following:\n# First get a list of classes the tree will classify things as with the following command\nprint(class_predictor.classes_) ###\n# This gives us [0,1] \n# Now we can create a new list with the replacement class strings in the same order.","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:36.274144Z","iopub.execute_input":"2021-06-29T06:26:36.274468Z","iopub.status.idle":"2021-06-29T06:26:37.112712Z","shell.execute_reply.started":"2021-06-29T06:26:36.274437Z","shell.execute_reply":"2021-06-29T06:26:37.111753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretty Cool!\nNote that there are other ways to view a decision tree and there may be other parameters you could include when plotting the tree to display the nodes differently, but this is fine for now and is really just for a fun visual.","metadata":{}},{"cell_type":"markdown","source":"\n\n# Evaluate model performance and tune hyperparameters\nNow that we have a sweet looking model, let's see how good it is at predicting passenger survival on our training set. \n\nThe function below determines both the MAE and accuracy of the model used","metadata":{}},{"cell_type":"code","source":"def validate(pred, vy, vX):\n    rf_val_mae = mean_absolute_error(pred, vy)\n    print(\"Validation MAE: {}\".format(rf_val_mae))\n    acc = accuracy_score(vy, pred)\n    print(\"Validation Accuracy: {}\".format(acc))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.1139Z","iopub.execute_input":"2021-06-29T06:26:37.114185Z","iopub.status.idle":"2021-06-29T06:26:37.118399Z","shell.execute_reply.started":"2021-06-29T06:26:37.114158Z","shell.execute_reply":"2021-06-29T06:26:37.117524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately i had problems with a more featured validation method that used a graph so i will just show MAE and accuracy because thats just as effective","metadata":{}},{"cell_type":"code","source":"#sad, non functional, code \n#print(\"Making predictions for the first 5 passengers in the training set.\")\n#    print(\"The predictions are:\")\n    # Merge actual target values and predictions back in with original features to see how we went.\n    # intialise data of lists.\n#    data = {'Actual':['Tom', 'nick', 'krish', 'jack'],\n#            'preidtced':['Tom', 'nick', 'krish', 'jack']}\n    # Create DataFrame\n#    results = pd.DataFrame(data)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.120914Z","iopub.execute_input":"2021-06-29T06:26:37.12118Z","iopub.status.idle":"2021-06-29T06:26:37.130753Z","shell.execute_reply.started":"2021-06-29T06:26:37.121154Z","shell.execute_reply":"2021-06-29T06:26:37.129656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now i will apply the function above to validate","metadata":{}},{"cell_type":"code","source":"pred = class_predictor.predict(val_X)    \nprint(validate(pred, val_y, val_X))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.132749Z","iopub.execute_input":"2021-06-29T06:26:37.133051Z","iopub.status.idle":"2021-06-29T06:26:37.148695Z","shell.execute_reply.started":"2021-06-29T06:26:37.133022Z","shell.execute_reply":"2021-06-29T06:26:37.147696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wow, That's not good?\n\nBecause we split our data into training and validation sets we can see the problem of over-fitting or under fitting appearing. Because a decision tree classifier is also pretty basic it doesnt deal with new data very well. to try and improve this I will be doing two things, change the model to a random forrest and also try testing a variety of parameters. ","metadata":{}},{"cell_type":"markdown","source":"## Now to Make a Forrest\n\nI chose this model as the next step from a simple decision tree classifier, it works on the same principle and returns a true or false prediction, what it does is makes several different decision tree classifier models and their outputs are compared, The benefits of this model as follows\n1. It reduces overfitting in decision trees and helps to improve the accuracy\n2. It is flexible to both classification and regression problems\n3. It works well with both categorical and continuous values\n4. It automates missing values present in the data\n5. Normalising data is not required as it uses a rule-based approach.\n\nHowever, despite these advantages, a random forest algorithm also has some drawbacks.\n1. It requires much computational power as well as resources as it builds numerous trees to combine their outputs. \n2. It also requires much time for training as it combines a lot of decision trees to determine the class.\n3. Due to the ensemble of decision trees, it also suffers interpretability and fails to determine the significance of each variable.\n\ndespite this, I feel it should make for a good improvement upon the decision tree classifier as our goal is to make an accurate prediction and time is not super important, in addition, a random forrest regressor's inability to determine the significance of the value of a variable is largely inconsequential as most of my data has a similar bearing on the potability. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the model. Set random_state to 1\nrf_model = RandomForestClassifier(random_state=1)\n\n# fit your model\nrf_model.fit(train_X, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.150173Z","iopub.execute_input":"2021-06-29T06:26:37.150618Z","iopub.status.idle":"2021-06-29T06:26:37.759693Z","shell.execute_reply.started":"2021-06-29T06:26:37.150576Z","shell.execute_reply":"2021-06-29T06:26:37.758987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now I Will Validate Using the Same Function","metadata":{}},{"cell_type":"code","source":"pred = rf_model.predict(val_X)    \nprint(validate(pred, val_y, val_X))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.760728Z","iopub.execute_input":"2021-06-29T06:26:37.761163Z","iopub.status.idle":"2021-06-29T06:26:37.794856Z","shell.execute_reply.started":"2021-06-29T06:26:37.761113Z","shell.execute_reply":"2021-06-29T06:26:37.793708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"   ## It's Still Not Great so Ill Tune Parameters\n\nI can see that moving from a decision tree classifier to random forest diddn't come with that big of an improvement, in fact its worse. This might be because the random forest is overfitted to the data as it has no limit to the number of leaf nodes it can have whereas the decision tree does.\n\nTo prevent this over-fitting i will tune the parameters\n\nTo do so i have a function that enters a number of leaf-nodes and determins the MAE for that number of leaf-nodes. Using this i can use a loop to test a wide range of magnitudes and determine the best one with little effort","metadata":{}},{"cell_type":"code","source":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.796264Z","iopub.execute_input":"2021-06-29T06:26:37.796612Z","iopub.status.idle":"2021-06-29T06:26:37.801925Z","shell.execute_reply.started":"2021-06-29T06:26:37.796579Z","shell.execute_reply":"2021-06-29T06:26:37.800885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_max_leaf_nodes = [2, 3, 4, 5, 25, 50, 100, 250, 500, 1000]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in range(1, 30):\n    my_mae = get_mae(max_leaf_nodes*10, train_X, val_X[X_columns], train_y, val_y)\n    print(f\"Max leaf nodes: {max_leaf_nodes*10}             mean error:{my_mae}\")\n\n# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\nbest_tree_size = 216","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:37.803143Z","iopub.execute_input":"2021-06-29T06:26:37.803449Z","iopub.status.idle":"2021-06-29T06:26:53.84911Z","shell.execute_reply.started":"2021-06-29T06:26:37.803419Z","shell.execute_reply":"2021-06-29T06:26:53.848028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## And Again\nnow i know the best number of leaf-nodes is in a range of 20-40 I will test every magnitude along that range","metadata":{}},{"cell_type":"code","source":"candidate_max_leaf_nodes = [2, 3, 4, 5, 25, 50, 100, 250, 500, 1000]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in range(20, 40):\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X[X_columns], train_y, val_y)\n    print(f\"Max leaf nodes: {max_leaf_nodes}             mean error:{my_mae}\")\n\n# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\nbest_tree_size = 30","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:53.850409Z","iopub.execute_input":"2021-06-29T06:26:53.850696Z","iopub.status.idle":"2021-06-29T06:27:03.015141Z","shell.execute_reply.started":"2021-06-29T06:26:53.850666Z","shell.execute_reply":"2021-06-29T06:27:03.014239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, so done the best i can using the random forrest","metadata":{}},{"cell_type":"code","source":"print(\"Making predictions for the first 5 passengers in the training set.\")\n\n\nrf_model = RandomForestClassifier(max_leaf_nodes=best_tree_size, random_state=0)\n\n# fit your model\nrf_model.fit(train_X, train_y)\n\n# Get the first five predictions as a list\npred = rf_model.predict(val_X[X_columns])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:03.016355Z","iopub.execute_input":"2021-06-29T06:27:03.016678Z","iopub.status.idle":"2021-06-29T06:27:03.48231Z","shell.execute_reply.started":"2021-06-29T06:27:03.016647Z","shell.execute_reply":"2021-06-29T06:27:03.48138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate time","metadata":{}},{"cell_type":"code","source":"pred = rf_model.predict(val_X)    \nprint(validate(pred, val_y, val_X))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:03.483698Z","iopub.execute_input":"2021-06-29T06:27:03.483991Z","iopub.status.idle":"2021-06-29T06:27:03.511201Z","shell.execute_reply.started":"2021-06-29T06:27:03.483961Z","shell.execute_reply":"2021-06-29T06:27:03.510182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# analysis\nso, we can see that with the more typical, tree-based, machine learning models accuracy can reach about 63% with my testing. This time around the accuracy was higher because tuned the hyperparameters finding the equilibrium between over fit and under-fit. Whilst it is an improvement from the-non tuned model as well as the basic decision tree it can still be better as I am not close to my 100% accuracy goal, to improve I will essentially restart and \"clean\" the data.","metadata":{}},{"cell_type":"markdown","source":"# preparing the data, but better\nso, last time we prepared the data i avoided removing the outliers in the data set. In addition there where some skewed results that coudld effect the preformance our our model to solve these issues i will re-do the \"cleaning\" of my data using a skewness corrector that i found. This method uses a BoxCox Transformation to help correct the data and by doing so my model should preform better. ","metadata":{}},{"cell_type":"code","source":"# Dropping NUll Values\nclass_train_data.dropna(inplace = True)\n\n# Checking size of data after dropping NUll Value\nclass_train_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:03.512839Z","iopub.execute_input":"2021-06-29T06:27:03.513248Z","iopub.status.idle":"2021-06-29T06:27:03.523392Z","shell.execute_reply.started":"2021-06-29T06:27:03.513205Z","shell.execute_reply":"2021-06-29T06:27:03.522412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def skewnessCorrector(dataset,columnName):\n    import seaborn as sns\n    from scipy import stats\n    from scipy.stats import norm, boxcox\n\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"lightcoral\");\n    plt.title(columnName.capitalize() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n    # Applying BoxCox Transformation\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    \n    print('''After Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\");\n    plt.title(columnName.capitalize() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n\ncol = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity',\n       'Organic_carbon', 'Trihalomethanes', 'Turbidity']","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:03.524496Z","iopub.execute_input":"2021-06-29T06:27:03.524784Z","iopub.status.idle":"2021-06-29T06:27:03.538727Z","shell.execute_reply.started":"2021-06-29T06:27:03.524757Z","shell.execute_reply":"2021-06-29T06:27:03.53749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now i can apply this function to the features","metadata":{}},{"cell_type":"code","source":"for column in col:\n    skewnessCorrector(class_train_data,column)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:03.539845Z","iopub.execute_input":"2021-06-29T06:27:03.540121Z","iopub.status.idle":"2021-06-29T06:27:11.556944Z","shell.execute_reply.started":"2021-06-29T06:27:03.540094Z","shell.execute_reply":"2021-06-29T06:27:11.556156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the data has been \"cleaned\" better we can once again follow the machine learning steps","metadata":{}},{"cell_type":"code","source":"# Separate out the prediction target\ny = class_train_data.Potability\n\n# Drop the target column (axis=1) from the original dataframe and use the rest as our feature data\nX = class_train_data.drop('Potability', axis=1)\n\n# Take a look at the data again\nX.head()\n#y.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:11.557935Z","iopub.execute_input":"2021-06-29T06:27:11.558319Z","iopub.status.idle":"2021-06-29T06:27:11.574703Z","shell.execute_reply.started":"2021-06-29T06:27:11.55829Z","shell.execute_reply":"2021-06-29T06:27:11.573753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# splitting\nPart of the re-cleaning means I need to spit the data once again. In this part I decided to reduce the size of my testing data set down to 15% from 25% this should allow the Support-Vector model to train on more data and hopefully be more accurate and still leaves roughly 400 rows of testing data to compare to.","metadata":{}},{"cell_type":"code","source":"# One hot encode the features. This will only act on columns containing non-numerical values.\none_hot_X = pd.get_dummies(X)\n\none_hot_X.head()\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(one_hot_X, y, test_size = 0.15, random_state = 1)\n\n#verify the split, verification data should be less than half\n\nval_X.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:11.576215Z","iopub.execute_input":"2021-06-29T06:27:11.576738Z","iopub.status.idle":"2021-06-29T06:27:11.607684Z","shell.execute_reply.started":"2021-06-29T06:27:11.576692Z","shell.execute_reply":"2021-06-29T06:27:11.606948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain_X = sc.fit_transform(train_X)\nval_X = sc.transform(val_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:11.608664Z","iopub.execute_input":"2021-06-29T06:27:11.609054Z","iopub.status.idle":"2021-06-29T06:27:11.620131Z","shell.execute_reply.started":"2021-06-29T06:27:11.609024Z","shell.execute_reply":"2021-06-29T06:27:11.619307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train\nOk, now we have some better-prepared data I will train it on a new model. The support vector model. This should work well with our data as it is a classifier meaning it returns only true or false and not a range of data. This is suitable as we are predicting if water is potable or not.\nI believe this will be the model that will achieve the highest accuracy because support vector models avoid problems with using multiple features that our other models use. it also\n \n1. It scales relatively well to high-dimensional data.\n2. SVM models have generalization in practice, the risk of over-fitting is less in SVM.\n\nand some things that I don't understand\n\n1. The kernel trick is real strength of SVM. With an appropriate kernel function, we can solve any complex problem.\n2. Unlike in neural networks, SVM is not solved for local optima.\n\nThis comes at the cost of time and is difficult to interpret by people. The parameters are also hard to tune as its hard to visualize their impact. This is why i didn't tune the model but from the difference it created in outer models I can assume that 2-3% accuracy is left on the table.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Define the model. Set random_state to 1\nsvm_model = svm.NuSVC(random_state=1)\n\n# fit your model\nsvm_model.fit(train_X, train_y)# Define the model. Set random_state to 1","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:11.623195Z","iopub.execute_input":"2021-06-29T06:27:11.623566Z","iopub.status.idle":"2021-06-29T06:27:11.848192Z","shell.execute_reply.started":"2021-06-29T06:27:11.623532Z","shell.execute_reply":"2021-06-29T06:27:11.847269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now I Will Validate Using the Same Function","metadata":{}},{"cell_type":"code","source":"pred = svm_model.predict(val_X)    \nprint(validate(pred, val_y, val_X))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:27:11.849477Z","iopub.execute_input":"2021-06-29T06:27:11.849767Z","iopub.status.idle":"2021-06-29T06:27:11.884469Z","shell.execute_reply.started":"2021-06-29T06:27:11.849739Z","shell.execute_reply":"2021-06-29T06:27:11.883578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nA reminder of the purpose of the investigation \nThis investigation aimed to find a relationship between water potability and various features of water, this could help people determine easily if water is potable or not to improve water safety\n\nWe predicted from this data if water was potable or not based on a number of common contaminants found in water\n\nA detailed discussion of the quality of predictions \n\nOur predictions didn't have a particularly good success rate, with my best attempt resulting in an accuracy of 67% this is about a 6-7% percent improvement from the basic decision tree classifier at the start however it was below the 100% accuracy goal. This could have been my fault or the fact that portability isn't affected by just one contaminant all that much. \n\nComparison of results with hypotheses\n\nI was correct and my hypothesis was supported that the support vector model results in the highest accuracy\n\nWhat features had the strongest effect on predictions and why?!\n\nWe also learned that the features had a similar effect on the portability however from the box plots and the Gini impurity I determined that Ph had the biggest effect with Chloramines following closely after, this is likely because ph, if to high or low can kill our cells and outher things so it would be important in determining if water is potable.\n\nIn addition, the support vector model is the best model as it had the highest accuracy. \n","metadata":{}}]}