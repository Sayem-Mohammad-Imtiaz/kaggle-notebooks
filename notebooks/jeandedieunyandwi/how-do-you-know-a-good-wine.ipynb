{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Good Wine With ML Algorithms"},{"metadata":{},"cell_type":"markdown","source":"Welcome to this notebook of predicting the best wine quality using provided dataset. In this notebook, we will have the following parts:\n\n* Introduction\n* Exploratory Data Analysis\n* Data Pre-processing\n* Creating a Models\n* Improving Model Performance\n* Conclusion\n* Credits & Aspirations\n\nLet us get started."},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIf you thought to work on exploring more on Red Wine Quality, it is probably that you don't have any problem having a wine(Same for me, I may not be right on this saying). Having that in mind, let us start to see what makes good wine. Each wine is rated as good or bad on scale of 0 to 10. Good wine start from 7 to 10 whereas bad wine is less than 7 based on label of quality. \n\nLet us start by importing relevant libraries. I will import all libraries to use all over the notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, let us import the data that we are going to be working with. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"wine_data=pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see the variables that we are going to be working with. "},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Rows,columns:\" + str(wine_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has 1599 rows and 12 columns. Let us quickly use 3 methods to explore our data, which are info, describe, and head. "},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nIn this section, we will check if our data is ready for further processing, perform visualization with seaborn, and clean it (where necessary). \n\nLet us start by checking the missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data.isnull().sum()\n#Same as wine_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we are pretty sure our data is okay, we can start understanding the data by using the power of visualizations."},{"metadata":{"trusted":true},"cell_type":"code","source":"#From this plot, we see that most wine types are in 5 and 6 category, which means most are considered bad. \nplt.figure(figsize = (11,6))\nsns.countplot(data=wine_data, x='quality')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us see the relationship between the label \"quality\" and other variables.\nplt.figure(figsize = (11,6))\nsns.heatmap(wine_data.corr(), \n            xticklabels=wine_data.corr().columns, \n            yticklabels=wine_data.corr().columns, \n            annot=True, \n            cmap=sns.diverging_palette(220,20,\n            as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we see that the most 3 features which are very linked to the quality of the wine are alcohol, citric acid, and sulphates."},{"metadata":{"trusted":true},"cell_type":"code","source":"#PH has a correlation of -0.058. Now plotting it is obvious that wine quality doesn't depend on pH. \n#PH is Power of Hydrogen which is a scale used to specify how acidic or basic a water-based solution is\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='pH')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alcohol is the most correlated feature with the quality of the wine, hence a reason why here good wine has high alcohol\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='alcohol')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Citric acid also correlates with the quality of the wine, hence a reason why here good wine has high citric acid\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='citric acid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another feature to explore is sulphates. \nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='sulphates')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Also, we can see that the quality of the wine does not depend on the residual sugar\n#Alcohol is the most correlated feature with the quality of the wine, hence a reason why here good wine has high alcohol\nplt.figure(figsize = (11,6))\nsns.barplot(data=wine_data, x='quality',y='residual sugar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"We've see that the wine quality is rated from 0 to 10. I will make a column of good quality, where wine from 0 to 6 or less than 7 will be labelled 0, and else 1. [1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_data['good quality']=[1 if x>=7 else 0 for x in wine_data['quality']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize = (11,6))\nsns.countplot(data=wine_data, x='good quality')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The exact values of wine quality. 0 stands for bad wine, whereas 1 is for good wine.\nwine_data['good quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us drop quality off the dataset\n\nwine_data=wine_data.drop('quality',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let us seperate the dataset as response variable and label or target variable\nX = wine_data.drop('good quality', axis = 1)\ny = wine_data['good quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Standard scaling to get optimized result\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a Model"},{"metadata":{},"cell_type":"markdown","source":"I wiss compare three Machine learning algotithms namely SVC(Support Vector Classifier), Random Forest Classifier,and Logistic Regression. \n\n# With Support Vector Classifier **(SVC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=SVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_svc=model1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SVC Model achieved 86% accuracy. "},{"metadata":{},"cell_type":"markdown","source":"# With Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = RandomForestClassifier(n_estimators=200)\nmodel2.fit(X_train, y_train)\npred_rfc = model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random Forest Classifier Model achieved 88% accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nprint(confusion_matrix(y_test, pred_rfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lrmodel=LogisticRegression()\nlrmodel.fit(X_train,y_train)\nlogpred=lrmodel.predict(X_test)\nprint(classification_report(y_test, logpred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking on the classification report, the model achieved 86% of accuracy."},{"metadata":{},"cell_type":"markdown","source":"# Improving Model Performance[2]"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will make the use two powerful options for improving model performance, which are Grid search and Cross Validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding best parameters for our SVC model\nparam = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\ngrid_svc = GridSearchCV(model1, param_grid=param, scoring='accuracy', cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_svc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_svc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's run SVC again with the best parameters.\nmodel_svc2 = SVC(C = 1.2, gamma =  0.9, kernel= 'rbf')\nmodel_svc2.fit(X_train, y_train)\npred_svc2 = model_svc2.predict(X_test)\nprint(classification_report(y_test, pred_svc2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen on above report, the model with the best parameters achieved 90% accuracy from 86%."},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation Score for random forest and SVC"},{"metadata":{},"cell_type":"markdown","source":"Now, let us attempt to improve the model performance by using cross validation.\n\n> Cross Validation is used to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data. The motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset.  (Full definition by Research Gate)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_forest_val = cross_val_score(estimator = model2, X = X_train, y = y_train, cv = 10)\nrand_forest_val.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we now see, the model accuracy achieved with cross validating the model is 91% from 88%"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_val = cross_val_score(estimator = model1, X = X_train, y = y_train, cv = 10)\nsvc_val.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, as seen above, by assessing the model to unseen data or test set (Cross validation), The SVC model accuracy increased from 86% to 89% "},{"metadata":{"trusted":true},"cell_type":"code","source":"log_val = cross_val_score(estimator = lrmodel, X = X_train, y = y_train, cv = 10)\nlog_val.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By doing the same thing on the Logistic model, the accuracy increase 2%, which is good improvement. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Jumping on the start of our work, the goal was to predict the good quality of the wine given the data of what makes the good wine.\nWe performed analysis to have insights on various quality measures such as alcohol, citric acid, etc... The data set was orginally clean, so we didn't have to spend much time doing cleaning. \n\nWe also compared three different Machine Learning Algorith which are Support Vector Classifier (SVC), Random Forest Classifier, and Logistic Regression. Random Forest outlined other algorithms in making good predictions. \nThere are many other algorithms that can be applied here, our focus was to use some of them.  \n\n\nFinally, we opted to use two techniques (which are GridSearch for finding best parameters, and Cross Validation) for improving accuracy on each of the 3 Machine learning algorithms. The model accuracy for each one was improved a number of percentages(Example is for Random Forest Classifer, accuracy went from 88% to 91%). "},{"metadata":{},"cell_type":"markdown","source":"# Thank you for reading. If you made it to this point, help me know your observations(can be a comment, correcting mistake, or additional input) so that anyone reading this can get the full understanding of algorithms used in this notebook. \n\n# * Happy modeling & predicting!*"},{"metadata":{},"cell_type":"markdown","source":"# Credits & Inspirations\n\n\n[1] Blog, Terence Shin, TDS. Available [Here](https://towardsdatascience.com/predicting-wine-quality-with-several-classification-techniques-179038ea6434)\n\n[2] Kaggle Notebook, Prediction of quality of Wine, Vishar Kumar. Available [Here](https://www.kaggle.com/vishalyo990/prediction-of-quality-of-wine)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}