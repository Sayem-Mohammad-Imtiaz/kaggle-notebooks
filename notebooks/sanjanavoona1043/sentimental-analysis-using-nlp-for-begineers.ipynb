{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Definition","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Given tweets about six US airlines, the task is to predict whether a tweet contains positive, negative, or neutral sentiment about the airline. This is a typical supervised learning task where given a text string, we have to categorize the text string into predefined categories.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Solution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To solve this problem, we will follow the typical machine learning pipeline. We will first import the required libraries and the dataset. We will then do exploratory data analysis to see if we can find any trends in the dataset. Next, we will perform text preprocessing to convert textual data to numeric data that can be used by a machine learning algorithm. Finally, we will use machine learning algorithms to train and test our sentiment analysis models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing the Required Libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The first step as always is to import the required libraries:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport nltk \nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_source_url = r\"C:\\Users\\ASUS\\Desktop\\ai and ml\\data\\Tweets.csv\"\nairline_tweets = pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets.airline_sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's explore the dataset a bit to see if we can find any trends. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_size = plt.rcParams[\"figure.figsize\"] \nprint(plot_size[0]) \nprint(plot_size[1])\n\nplot_size[0] = 8\nplot_size[1] = 6\nplt.rcParams[\"figure.figsize\"] = plot_size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets.airline.value_counts().plot(kind='pie', autopct='%1.0f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the output, you can see the percentage of public tweets for each airline. United Airline has the highest number of tweets i.e. 26%, followed by US Airways (20%).\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's now see the distribution of sentiments across all the tweets. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets.airline_sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"green\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output, you can see that the majority of the tweets are negative (63%), followed by neutral tweets (21%), and then the positive tweets (16%).\n\nNext, let's see the distribution of sentiment for each individual airline,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_sentiment = airline_tweets.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack()\nairline_sentiment.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is evident from the output that for almost all the airlines, the majority of the tweets are negative, followed by neutral and positive tweets. Virgin America is probably the only airline where the ratio of the three sentiments is somewhat similar.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"let's use the Seaborn library to view the average confidence level for the tweets belonging to three sentiment categories. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.barplot(x='airline_sentiment', y='airline_sentiment_confidence' , data=airline_tweets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output, you can see that the confidence level for negative tweets is higher compared to positive and neutral tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sub_sentiment(Airline):\n    pdf = airline_tweets[airline_tweets['airline']==Airline]\n    count = pdf['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    color=sns.color_palette(\"husl\", 10)\n    plt.bar(Index,count,width=0.5,color=color)\n    plt.xticks(Index,['Negative','Neutral','Positive'])\n    plt.title('Sentiment Summary of' + \" \" + Airline)\n\nairline_name = airline_tweets['airline'].unique()\nplt.figure(1,figsize=(12,12))\nfor i in range(6):\n    plt.subplot(3,2,i+1)\n    plot_sub_sentiment(airline_name[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#counting the total number of negative reasons\nairline_tweets.negativereason.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting all the negative reasons \ncolor=sns.color_palette(\"husl\", 10)\npd.Series(airline_tweets[\"negativereason\"]).value_counts().plot(kind = \"bar\",\n                        color=color,figsize=(8,6),title = \"Total Negative Reasons\")\nplt.xlabel('Negative Reasons', fontsize=10)\nplt.ylabel('No. of Tweets', fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Cloud for the negative Tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets=airline_tweets [airline_tweets ['airline_sentiment']=='negative']\nwords = ' '.join(airline_tweets ['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can find that the Tweets with negative moods are frequently involved some words like cancelled, flight ,customer or hour. People might guess that customer tends to complain when they are waiting for the delayed flights.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Wordcloud for positive reasons","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The code for getting positive sentiments is completely same with the one for negative sentiments. Just replace negative with positive in the first line. Easy, right!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets=airline_tweets [airline_tweets ['airline_sentiment']=='positive']\nwords = ' '.join(airline_tweets ['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate highest frequency words in positive tweets\ndef freq(str): \n  \n    # break the string into list of words  \n    str = str.split()          \n    str2 = [] \n  \n    # loop till string values present in list str \n    for i in str:              \n  \n        # checking for the duplicacy \n        if i not in str2: \n  \n            # insert value in str2 \n            str2.append(i)  \n              \n    for i in range(0, len(str2)): \n        if(str.count(str2[i])>50): \n            print('Frequency of', str2[i], 'is :', str.count(str2[i]))\n        \nprint(freq(cleaned_word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"air_senti=pd.crosstab(airline_tweets.airline, airline_tweets.airline_sentiment)\nair_senti","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent=air_senti.apply(lambda a: a / a.sum() * 100, axis=1)\npercent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index = airline_tweets[\"airline\"],columns = airline_tweets[\"airline_sentiment\"]).plot(kind='bar',\n                figsize=(10, 6),alpha=0.5,rot=0,stacked=True,title=\"Airline Sentiment\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**American,US Airways , United have more negative tweets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets['tweet_created'] = pd.to_datetime(airline_tweets['tweet_created'])\nairline_tweets[\"date_created\"] = airline_tweets[\"tweet_created\"].dt.date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets[\"date_created\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = airline_tweets.groupby(['date_created','airline'])\ndf = df.airline_sentiment.value_counts()\ndf.unstack()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Enough of the exploratory data analysis, our next step is to perform some preprocessing on the data and then convert the numeric data into text data as shown below.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Tweets contain many slang words and punctuation marks. We need to clean our tweets before they can be used for training the machine learning model. However, before cleaning the tweets, let's divide our dataset into feature and label sets.\n\nOur feature set will consist of tweets only. If we look at our dataset, the 11th column contains the tweet text. Note that the index of the column will be 10 since pandas columns follow zero-based indexing scheme where the first column is called 0th column. Our label set will consist of the sentiment of the tweet that we have to predict. The sentiment of the tweet is in the second column (index 1). To create a feature and a label set, we can use the iloc method off the pandas data frame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = airline_tweets.iloc[:, 10].values\nlabels = airline_tweets.iloc[:, 1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we divide the data into features and training set, we can preprocess data in order to clean it. To do so, we will use regular expressions. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_features = []\n\nfor sentence in range(0, len(features)):\n    # Remove all the special characters\n    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n\n    # remove all single characters\n    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n\n    # Remove single characters from the start\n    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n\n    # Substituting multiple spaces with single space\n    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n\n    # Removing prefixed 'b'\n    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n\n    # Converting to Lowercase\n    processed_feature = processed_feature.lower()\n\n    processed_features.append(processed_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Representing Text in Numeric Form","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To make statistical algorithms work with text, we first have to convert text to numbers. To do so, three main approaches exist i.e. Bag of Words, TF-IDF and Word2Vec. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the bag of words approach, each word has the same weight. The idea behind the TF-IDF approach is that the words that occur less in all the documents and more in individual document contribute more towards classification.\n\nTF-IDF is a combination of two terms. Term frequency and Inverse Document frequency. They can be calculated as:","execution_count":null},{"metadata":{},"cell_type":"raw","source":"TF  = (Frequency of a word in the document)/(Total words in the document)\n\nIDF = Log((Total number of docs)/(Number of docs containing the word))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\nprocessed_features = vectorizer.fit_transform(processed_features).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code above, we define that the max_features should be 2500, which means that it only uses the 2500 most frequently occurring words to create a bag of words feature vector. Words that occur less frequently are not very useful for classification.\n\nSimilarly, max_df specifies that only use those words that occur in a maximum of 80% of the documents. Words that occur in all documents are too common and are not very useful for classification. Similarly, min-df is set to 7 which shows that include words that occur in at least 7 documents.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dividing Data into Training and Test Sets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"n the previous section, we converted the data into the numeric form. As the last step before we train our algorithms, we need to divide our data into training and testing sets. The training set will be used to train the algorithm while the test set will be used to evaluate the performance of the machine learning model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Once data is split into training and test set, machine learning algorithms can be used to learn from the training data. \nThe sklearn.ensemble module contains the RandomForestClassifier class that can be used to train the machine learning model using the random forest algorithm. To do so, we need to call the fit method on the RandomForestClassifier class and pass it our training features and labels, as parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\ntext_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Predictions and Evaluating the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The last step is to make predictions on the model. To do so, we need to call the predict method on the object of the RandomForestClassifier class that we used for training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = text_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To evaluate the performance of the machine learning models, we can use classification metrics such as a confusion metrix, F1 measure, accuracy, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint(accuracy_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN ALGO","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\ntext_classifier2 = KNeighborsClassifier(n_neighbors = 5)#no of neighbors is hpyer parameter\ntext_classifier2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2 = text_classifier2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions2))\nprint(classification_report(y_test,predictions2))\nprint(accuracy_score(y_test, predictions2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel =LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions3 = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions3))\nprint(classification_report(y_test,predictions3))\nprint(accuracy_score(y_test, predictions3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Algorithim","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmodel3= DecisionTreeClassifier(criterion=\"gini\")\n#here we are facing the problem of overfitting\n#train the model\nmodel3.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions4 = model3.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions4))\nprint(classification_report(y_test,predictions4))\nprint(accuracy_score(y_test, predictions4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Please upvote me if you found valuable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}