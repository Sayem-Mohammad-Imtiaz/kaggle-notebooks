{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Set Information:\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at [Web Link]\n\nSeparating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n\n## Attribute Information:\n\n1. ID number\n2. Diagnosis (M = malignant, B = benign) \n3. 3-32 Ten real-valued features are computed for each cell nucleus:\n\n    * radius (mean of distances from center to points on the perimeter)\n    * texture (standard deviation of gray-scale values)\n    * perimeter\n    * area\n    * smoothness (local variation in radius lengths)\n    * compactness (perimeter^2 / area - 1.0)\n    * concavity (severity of concave portions of the contour)\n    * concave points (number of concave portions of the contour)\n    * symmetry\n    * fractal dimension (\"coastline approximation\" - 1)\n\n![](https://static.packt-cdn.com/products/9781783980284/graphics/3a298fcc-54fb-42c2-a212-52823e709e30.png)"},{"metadata":{},"cell_type":"markdown","source":"# 1. import library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# manipulation data\nimport pandas as pd\nimport numpy as np\n\n#visualiation data\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport matplotlib\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\nmatplotlib.rcParams['figure.figsize'] =[8,8]\nmatplotlib.rcParams.update({'font.size': 15})\nmatplotlib.rcParams['font.family'] = 'sans-serif'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. load and analys data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### like we see all our feautres are numirical values exept the target value ***diagnosis*** (M = malignant, B = benign)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### we had 569 Rows and 33 columns (small data )"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### from the first look in our data description we can see that :\n\n1. B = benign is the most frequent value in our target columns\n2. Unnamed: 32 columns is an empty column "},{"metadata":{},"cell_type":"markdown","source":"## A) finding missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values=train.isnull().sum()\npercent_missing = train.isnull().sum()/train.shape[0]*100\n\nvalue = {\n    'missing_values ':missing_values,\n    'percent_missing %':percent_missing\n}\nframe=pd.DataFrame(value)\nframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### like we c our data is clean exept the last columns that is empty so we gonna drop it "},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop('Unnamed: 32',axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. feautres selection "},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the id columns\ntrain=train.drop('id',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformation of type of the target value to numerical \nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain.diagnosis = le.fit_transform(train.diagnosis)\ntrain.diagnosis\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Diagnosis \n\n1. M = malignant ==> 1\n2. B = benign    ==> 0 "},{"metadata":{},"cell_type":"markdown","source":"## A) correlation map"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr().style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### this feautres had a corralation valus < 0.07 with the target columns \nfractal_dimension_mean / texture_se / smoothness_se / symmetry_se / fractal_dimension_se"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop this columns \ntrain=train.drop(['fractal_dimension_mean','texture_se','smoothness_se','symmetry_se','fractal_dimension_se'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection\n\nplt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\n\nx = train.drop('diagnosis',axis=1)\ny = train.diagnosis\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.title('the most 15 important feature are')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['texture_mean','smoothness_mean','compactness_mean','symmetry_mean','perimeter_se','compactness_se','concavity_se','concave points_se','smoothness_worst','symmetry_worst','fractal_dimension_worst'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. data vizualisation"},{"metadata":{},"cell_type":"markdown","source":"## Plotting many distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.PairGrid(x)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A) diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=25,7 \nsns.set_style(\"darkgrid\")\nax = sns.countplot(x=train.diagnosis , palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"diagnosis malignant = 1 / benign = 0 \", fontsize = 10 )\nplt.ylabel(\"count\", fontsize = 10)\nplt.title('Number of diagnosis ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B) concave points_worst"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['concave points_worst'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(train, x='concave points_worst')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(train, x=\"concave points_worst\", kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(train, x=\"concave points_worst\", col=\"diagnosis\", multiple=\"dodge\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B) concavity_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['concavity_mean'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are some outliers. Lets remove them (0.3- 0.5)"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier=train[train['concavity_mean']>=0.25]\noutlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['concavity_mean']<0.25]\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['concavity_mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"concavity_mean\",\n                col=\"diagnosis\",\n                data=train, kind=\"box\",\n                height=4, aspect=.7);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3) perimeter_worst"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['perimeter_worst'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are 2 outliers. Lets remove them >120"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier=train[train['perimeter_worst']>=165]\noutlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['perimeter_worst']<165]\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['perimeter_worst'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) machine learning application"},{"metadata":{},"cell_type":"markdown","source":"## A) split data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train.drop('diagnosis',axis=1)\ny=train.diagnosis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B) Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a) Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making Confusion Matrix and calculating accuracy score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmodel = LogisticRegression()\n\n#Fit the model\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nmylist = []\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n# accuracy score\nacc_logreg = accuracy_score(y_test, y_pred)\n\nmylist.append(acc_logreg)\nprint(cm)\nprint(acc_logreg,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the optimum number of neighbors \n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlist1 = []\nfor neighbors in range(1,5):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(1,5)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the K Nearest Neighbor Classifier on the Training set\n\nclassifier = KNeighborsClassifier(n_neighbors=3)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_knn = accuracy_score(y_test, y_pred)\nmylist.append(acc_knn)\nprint(cm)\nprint(acc_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c) Support Vector Machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor c in [0.5,0.6,0.7,0.8,0.9,1.0]:\n    classifier = SVC(C = c, random_state=0, kernel = 'rbf')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.5,0.6,0.7,0.8,0.9,1.0], list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Support Vector Classifier on the Training set\n\nfrom sklearn.svm import SVC\nclassifier = SVC(C = 0.9, random_state=0, kernel = 'rbf')\nclassifier.fit(x_train, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_svc = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_svc,'%')\nmylist.append(acc_svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d) DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the optimum number of max_leaf_nodes\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor leaves in range(2,15):\n    classifier = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(2,15)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Decision Tree Classifier on the Training set\n\nclassifier = DecisionTreeClassifier(max_leaf_nodes = 5, random_state=0, criterion='entropy')\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_decisiontree = accuracy_score(y_test, y_pred)\nprint(cm)\nprint(acc_decisiontree)\nmylist.append(acc_decisiontree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### e) RANDOM FOREST CLASSIFCATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the optimum number of n_estimators\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the RandomForest Classifier on the Training set\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 15, criterion='entropy', random_state=0)\nclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set results\n\ny_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_randomforest = accuracy_score(y_test, y_pred)\nmylist.append(acc_randomforest)\nprint(cm)\nprint(acc_randomforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### f) ANN (neural network )"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nimport tensorflow as tf\n\n# Initialising the ANN\n\nann = tf.keras.models.Sequential()\n\n# Adding the input layer and the first hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the second hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the third hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the fourth hidden layer\n\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\n\n# Adding the output layer\n\nann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n\n# Compiling the ANN\n\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n\n# Training the ANN on the training set\n\nann.fit(x_train, y_train, batch_size = 16, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set results\n\ny_pred = ann.predict(x_test)\ny_pred = (y_pred > 0.9)\nnp.set_printoptions()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix, calculating accuracy_score \n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# confusion matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix\")\nprint(cm)\nprint()\n\n# accuracy\nac_ann = accuracy_score(y_test,y_pred)\nprint(\"Accuracy\")\nprint(ac_ann)\nmylist.append(ac_ann)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### g) xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 15, max_depth=12, subsample=0.7)\nclassifier.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_xgboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_xgboost)\nprint(cm)\nprint(ac_xgboost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### h) catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_catboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_catboost)\nprint(cm)\nprint(ac_catboost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'ANN',   \n              'Decision Tree','xgboost','catboost'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, ac_ann, acc_decisiontree,ac_xgboost,ac_catboost\n              ]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=15,6 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=models.Model, y=models.Score, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}