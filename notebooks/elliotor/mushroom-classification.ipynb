{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics \nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\npd.set_option('display.max_columns', None)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nprint('Importing Finished')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading in the data\ndf = pd.read_csv(\"../input/mushroom-classification/mushrooms.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning column names\ncleaned_cols = [col.replace(\"-\", \"_\") for col in df.columns]\ndf = pd.DataFrame(data = df.values,\n                 columns = cleaned_cols,\n                 index = df.index)\n\nX = df.iloc[:,1:]\ny = df.iloc[:,0]\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes\n\n#All columns are categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()\n\n#Binomial outcome (edible or poisonous), relatively low cardinality of features\n#Some features are ordinal and others are not","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping veil_type as it is uninformative (all same value)\ndf = df.drop(columns = 'veil_type')\nX = X.drop(columns = 'veil_type')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of values within each feature\nfor col in df.columns:\n    print(df[col].value_counts())\n    \n#We see that there is a ? for stalk root, indicating it is missing data\n#Most features are well-disbursed but many (cap-shape, cap-surface, cap-color, stalk-color-above-ring, veil-color) have some values \n#that appear very infrequently (< 20 / 8124)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features with rare values\nrare_valued_cols = [col for col in df.columns if df[col].value_counts().min() <= 20]\nprint(rare_valued_cols)\n\n#These rare values could present issues in training a model as they could effectively act as noise in the data. \n#If problematic, we may need to treat them as outliers and remove.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,30))\nfor i in range(len(X.columns)):\n    fig.add_subplot(7,3,i+1)\n    col = X.columns[i]\n    a = df.groupby(col).apply(lambda df: df['class'] == 'p').reset_index()\n    a = a.groupby(col)['class'].agg({'sum', 'count'}).reset_index().rename(columns = {'count':'total', 'sum':'total_poisonous'})\n    a['percent_poisonous'] = round(a['total_poisonous'] *100 / a['total'], 2)\n#     plt.title('Percent of Edible Mushrooms by Subcategory in {}'.format(df.columns[1]))\n    ax = sns.barplot(x = a[col], y = a['percent_poisonous'], color = 'b')\n    ax2 = ax.twinx()\n    sns.lineplot(x = a[col], y = a['total'], ax = ax2, color = 'r')\n    \nplt.show()\n\n#Some variables are clearly more predictive of poisonous/edible such as odor, stalk_color_below_ring, spore_print_color whereas\n#others are poor predictors as they split close to 50% (stalk_shape, cap_color, stalk_surface_above_ring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#The next step to exploring the data is understanding the relationships between the features.\n#Because all of the data is categorical, we need another measure or method to determine similarity outside of correlation.\n#Clustering can highlight distinguishing features of the mushrooms as well as those that permeate across many types.  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting data into train, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,train_size=0.8, random_state = 0)\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size = 0.25,train_size =0.75, random_state = 0)\nprint(\"\"\"Train dataset size: {},\nValidation dataset size: {},\nTest dataset size: {}\"\"\".format(X_train.shape, X_val.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming target labels\ny_le = LabelEncoder()\ny_train = pd.Series(y_le.fit_transform(y_train), index = X_train.index)\ny_val = pd.Series(y_le.transform(y_val), index = X_val.index)\ny_test = pd.Series(y_le.transform(y_test), index = X_test.index)\n\ny_le.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kmodes Clustering\n\nfrom kmodes.kmodes import KModes\n\n#performing clustering on training and validation sets\nX_val_train = pd.concat([X_val, X_train])\n\n#imputing missing values (which are '?') and encoding the data\nclustering_imputer = SimpleImputer(missing_values = '?', strategy = 'most_frequent')\nclustering_ord_enc = OrdinalEncoder()\n\nclustering_transformer = Pipeline(steps = [\n    ('imputer', clustering_imputer),\n    ('ord_enc', clustering_ord_enc)\n])\n\nkmodes_df = pd.DataFrame(data = clustering_transformer.fit_transform(X_val_train), columns = X.columns)\n\n#clustering mushrooms into 2 to 10 clusters\nk_list = [x+2 for x in range(9)]\n\nkm = {}\nkm_clusters = {}\n\nfor k in k_list:\n    km[k] = KModes(n_clusters = k, n_init = 10, verbose = 0)\n    km_clusters[k] = km[k].fit_predict(kmodes_df)\n\nkm_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Graphing the clustering cost for each k\nkm_cost = [km[k].cost_ for k in k_list]\nsns.lineplot(k_list, km_cost, color = 'b')\n\n#There are \"elbows\" at k=4,7,9 but for interpretability, we'll choose 4 clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The 4 clusters' centroids, or modes\nfinal_clusters = pd.DataFrame(data = clustering_ord_enc.inverse_transform(pd.DataFrame(km[4].cluster_centroids_)), columns = X.columns, index = [[0,1,2,3]] )\nfinal_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Graphing the features of the mushrooms by what cluster they fall into\ndf_val_train = X_val_train\ndf_val_train['cluster'] = pd.Series(km_clusters[4], index = df_val_train.index)\n\nfig = plt.figure(figsize=(16,26))\nfor i in range(len(X.columns)):\n    fig.add_subplot(7,3,i+1)\n    col = X.columns[i]\n    sns.countplot(x = col, data = df_val_train, hue = 'cluster')\n    \nplt.show()\n\n#Nearly all of cluster 2 mushrooms have gill color b (b stands for buff) and gill color b is exclusively comprised of cluster 2 mushrooms. \n#This indicates that a gill color of buff is a defining characteristic of cluster 2 mushrooms. \n#Looking at the graphs, we can see that gill_color, odor, spore_print_color among others are defining characteristics for some of the clusters\n#whereas cap_shape, for example, is not a differentiating feature between the clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Graphing clusters by their class\ny_val_train = pd.Series(pd.concat([y_val, y_train]))\ndf_val_train['class'] = y_val_train\nsns.countplot('cluster', data = df_val_train, hue = 'class')\n\n#Cluster 0 and 3 are predominantly of edible mushrooms and clusters 1 and 2 are made up of nearly all poisonous mushrooms.\n#Cluster 0 seems to be a catch-all category as evidenced by the greater number of mushrooms in its cluster, its diversity of \n#mushroom class, and its diversity of features as seen in the graphs above\n\n#This graph indicates that mushrooms of similar features are likely of the same class. In other words, at least some of these \n#features, likely the defining ones mentioned above, have the potential to help us in classifying mushrooms with high accuracy.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Logistic Regression**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing pipeline for logistic regression\n\n#categorizing features as ordinal or nominal\n#features with 2 unique values in the training set are categorized as ordinal (and ring_number), all others are nominal\nord_cols = [col for col in X.columns if X[col].nunique() == 2]\nnom_cols = [col for col in X.columns if col not in ord_cols]\nnom_cols.remove('ring_number')\n\n#imputer for any columns with missing values (only stalk_root)\nimputer = SimpleImputer(missing_values = '?', strategy = 'most_frequent')\n\n#onehot encoder for nominal variables\nencoder_nom = OneHotEncoder(handle_unknown = 'ignore')\n\n#Nominal variables should in general be one-hot encoded as there is no known or implied ordering between the values\n#Logistic regression with a penalty is adept at handling one-hot encoded variables and the penalty will account for the collinearity \n#deriving from one-hot encoding\n\n#ordinal encoder for columns with inferred ordering (ring_number)\nencoder_ord1 = OrdinalEncoder(categories=[['n', 'o', 't']])\n\n#n, o, and t stand for none, one, and two, respectively. This is a natural ordering that we know so therefore it was hardcoded in the encoder\n\n#ordinal encoder for all other ordinal variables\nencoder_ord2 = OrdinalEncoder()\n\nnom_transformer = Pipeline(steps = [\n    ('imputer', imputer),\n    ('onehot', encoder_nom)\n])\n\nord_transformer = Pipeline(steps = [\n    ('imputer', imputer),\n    ('ord_enc', encoder_ord2)\n])\n\npreprocessor_log = ColumnTransformer(\n    transformers = [\n        ('nom', nom_transformer, nom_cols),\n        ('ord1', encoder_ord1, ['ring_number']),\n        ('ord2', ord_transformer, ord_cols)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ord_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building logistic model with penalty\npenalty = ['l1','l2']\nC = [.01, 0.1, 1, 5, 10, 100]\n\nlogistic = {}\nfor p in penalty:\n    for c in C:\n        logistic[(c,p)] = LogisticRegression(C=c, penalty=p, solver='liblinear', random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for key in logistic.keys():\n#     print(key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bundling pipeline (preprocessing and modeling) for logistic regression with penalty\nbundled_pipeline_log = {}\nfor key in logistic.keys():\n    bundled_pipeline_log[key] = Pipeline(steps = [\n        ('preprocessing', preprocessor_log),\n        ('model', logistic[key])\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit, predict, and calculate loss for the logistic models of different parameters\npred_log = {}\npred_proba_log = {}\naccuracy_log = {}\n\nfor key in bundled_pipeline_log.keys():\n    bundled_pipeline_log[key].fit(X_train, y_train)\n\n    pred_log[key] = bundled_pipeline_log[key].predict(X_val)\n    pred_proba_log[key] = bundled_pipeline_log[key].predict_proba(X_val)\n\n    accuracy_log[key] = bundled_pipeline_log[key].score(X_val, y_val)\n\naccuracy_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing logistic models' accuracies on validation set as a heatmap\naccuracy_log_series = pd.Series(list(accuracy_log.values()),\n                  index=pd.MultiIndex.from_tuples(accuracy_log.keys()))\naccuracy_log_df = accuracy_log_series.unstack()\nsns.heatmap(accuracy_log_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Displaying feature importance through coefficients of logistic regression\npd.set_option('display.max_rows', 150)\n\n#choosing one of the many perfectly accurate logistic models\nbest_log = bundled_pipeline_log[(1, 'l1')]\nnom_cols_expanded = list(best_log['preprocessing'].transformers_[0][1]['onehot'].get_feature_names(nom_cols))\n\npipeline_cols_log = nom_cols_expanded + ['ring_number'] + ord_cols\ncoef_data_log = best_log['model'].coef_.reshape((108,1))\ncoef_df_log = pd.DataFrame(data = coef_data_log, index = pipeline_cols_log).rename(columns = {0: 'coef_value'})\n\n#filtering out zero-valued coefficients\ncoef_df_log.loc[coef_df_log.coef_value != 0].coef_value.sort_values(ascending=True)\n\n#Note: Negative indicates a prediction of edible and positive for poisonous. \n#We can use coefficient magnitudes as a proxy for feature importance. \n#Odor, spore_print_color, gill_size, and population are some of the most impactful features (if that characteristic of the mushroom\n# is present).  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ordinal column labels\nfor i in range(len(ord_cols)):\n    print('Ordering of levels within {} column: {}'.format(ord_cols[i], best_log['preprocessing'].transformers_[2][1][1].categories_[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree / Random Forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing Decision Tree / Random Forest Classifier\npreprocessor_dt = ColumnTransformer(\n    transformers = [\n        ('ord1', encoder_ord1, ['ring_number']),\n        ('ord2', ord_transformer, ord_cols + nom_cols)\n    ])\n\n#Using the same preprocessor for DTs and RFs. Unlike logistic regression, these tree-based models perform better with ordinal encoding,\n#even when the variable is nominal. This is because one-hot encoding variables in these models generally devalues the splits of these\n#variables and leads to sparse trees","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating DTs and RFs with various hyperparameter values\nmax_depth = [1, 5, 10, 20]\nmax_features = [.1, .25, .5, .75, \"auto\"]\nn_estimators = [10, 25, 50]\n\ndt = {}\nrf = {}\nfor d in max_depth:\n    for f in max_features:\n        dt[(f,d)] = DecisionTreeClassifier(max_depth = d, max_features = f, random_state=0)\n\nfor d in max_depth:\n    for f in max_features:\n        for e in n_estimators:\n            rf[(f,d,e)] = RandomForestClassifier(n_estimators = e, max_depth = d, max_features = f, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bundling pipeline (preprocessing and modeling) for logistic regression with penalty\nbundled_pipeline_dt = {}\nfor key in dt.keys():\n    bundled_pipeline_dt[key] = Pipeline(steps = [\n        ('preprocessing', preprocessor_dt),\n        ('model', dt[key])\n    ])\n    \nbundled_pipeline_rf = {}\nfor key in rf.keys():\n    bundled_pipeline_rf[key] = Pipeline(steps = [\n        ('preprocessing', preprocessor_dt),\n        ('model', rf[key])\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit, predict, and calculate loss for DT model\npred_dt = {}\npred_proba_dt = {}\naccuracy_dt = {}\n\nfor key in bundled_pipeline_dt.keys():\n    bundled_pipeline_dt[key].fit(X_train, y_train)\n\n    pred_dt[key] = bundled_pipeline_dt[key].predict(X_val)\n    pred_proba_dt[key] = bundled_pipeline_dt[key].predict_proba(X_val)\n\n    accuracy_dt[key] = bundled_pipeline_dt[key].score(X_val, y_val)\n\naccuracy_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit, predict, and calculate loss for RF model\npred_rf = {}\npred_proba_rf = {}\naccuracy_rf = {}\n\nfor key in bundled_pipeline_rf.keys():\n    bundled_pipeline_rf[key].fit(X_train, y_train)\n\n    pred_rf[key] = bundled_pipeline_rf[key].predict(X_val)\n    pred_proba_rf[key] = bundled_pipeline_rf[key].predict_proba(X_val)\n\n    accuracy_rf[key] = bundled_pipeline_rf[key].score(X_val, y_val)\n\naccuracy_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Displaying feature importance through coefficients of DTs\naccuracy_dt_series = pd.Series(list(accuracy_dt.values()),\n                  index=pd.MultiIndex.from_tuples(accuracy_dt.keys()))\naccuracy_dt_df = accuracy_dt_series.unstack()\nsns.heatmap(accuracy_dt_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Displaying feature importance through coefficients of RFs\naccuracy_rf_series = pd.Series(list(accuracy_rf.values()),\n                  index=pd.MultiIndex.from_tuples(accuracy_rf.keys()))\naccuracy_rf_df = accuracy_rf_series.unstack()\nsns.heatmap(accuracy_rf_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing feature importance in the DT\n\n#choosing one of the many perfectly accurate DTs\nbest_dt = bundled_pipeline_dt[(0.5, 20)]\n\n#features are in the order they were preprocessed in the pipeline\ndt_features = ['ring_number'] + ord_cols + nom_cols\n\nfor i,f in enumerate(best_dt['model'].feature_importances_):\n    print('{} has feature importance score: {}'.format(dt_features[i], round(f, 2)))\n\nsns.barplot(best_dt['model'].feature_importances_, dt_features, color = 'b')\n\n#Note: Changing order of columns with set random seed affects importance levels. \n#Therefore, random forest using a random forest with many trees is likely a better indication of feature importances.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing feature importance in the RF\n\n#choosing one of the many perfectly accurate RFs\nbest_rf = bundled_pipeline_rf[(0.5, 20, 25)]\n\nrf_features = ['ring_number'] + ord_cols + nom_cols\n\nfor i,f in enumerate(best_rf['model'].feature_importances_):\n    print('{} has feature importance score: {}'.format(rf_features[i], round(f, 2)))\n    \nsns.barplot(best_rf['model'].feature_importances_, rf_features, color = 'b')\n\n#Gill_color, spore_print_color, odor, gill_size, and population are the most important features of the RF. This is mostly in line\n#with what was observed in the DT and logistic regression model. Gill_color was not as emphasized in the logistic regression model\n#as it is in the DT and RF, potentially because of the differences in preprocessing, the well-disbursed nature and high cardinality \n#of gill_color that would favor tree-based methods, and/or due to underlying similarities between features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting chosen models with training and validation data and then predicting on test data\n\n#Fitting\nbest_log.fit(X_val_train, y_val_train)\nbest_dt.fit(X_val_train, y_val_train)\nbest_rf.fit(X_val_train, y_val_train)\n\n#Predicting\nlog_test_score = best_log.score(X_test, y_test)\ndt_test_score = best_dt.score(X_test, y_test)\nrf_test_score = best_rf.score(X_test, y_test)\n\nprint(\"\"\"\nLogistic regression accuracy on test set: {},\nPruned decision tree accuracy on test set: {},\nRandom Forest accuracy on test set: {},\n\"\"\".format(dt_test_score, pruned_dt_test_score, rf_test_score))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}