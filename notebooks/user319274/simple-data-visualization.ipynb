{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv',dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n#print(data.head(5))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data cleaning by \n1. https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool\n2. https://www.kaggle.com/qingliu67/data-preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nall_json = glob.glob('../input/CORD-19-research-challenge/**/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nwith open(all_json[0]) as file:\n    article1 = json.load(file)\n    #print(json.dumps(article1, indent = 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.title = content['metadata']['title']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if content.get(\"abstract\") != None:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            # Extend Here\n            #\n            #\n    def __repr__(self):\n        return f'{self.paper_id}: {self.title}: {self.abstract[:200]}... {self.body_text[:200]}...'\n \nfirst_row = FileReader(all_json[0])\n#print(first_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = {'paper_id': [], 'title': [], 'abstract': [], 'body_text': []}\nfor ind, entry in enumerate(all_json):\n    if ind % (len(all_json) // 10) == 0:\n        print(f'Processing index: {ind} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['title'].append(content.title)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'title', 'abstract', 'body_text'])\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndf_title = df_covid.loc[:, [\"title\"]].dropna()\ndf_title[\"title\"] = df_title['title'].apply(lambda x: x.lower())\ndf_title[\"title\"] = df_title['title'].apply(lambda x: x.strip())\ndf_title[\"title\"] = df_title['title'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_title[\"title\"] = df_title['title'].apply(lambda x: re.sub(' +',' ',x))\ntitles = ' '.join(df_title[\"title\"])\n\n#print(titles[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt\n\ntitles_word_list = titles.split()\n\ntitles_word_count = Counter(titles_word_list).most_common()\n\nnew_titles_word_count = [i for i in titles_word_count if i[0] not in ['of', 'and', 'in', 'the', 'a', \n                                                                      'for', 'with', 'to', 'from', 'by', 'on',\n                                                                     'an', 'at', 'as', 'is', 'are']]\n\nlabels, values = zip(*new_titles_word_count)\n\nindexes = np.arange(len(labels[:100]))\nwidth = 1\n\nplt.figure(num=None, figsize=(16, 14), dpi=80, facecolor='w', edgecolor='k')\n\nplt.bar(indexes, values[:100], width)\nplt.xticks(indexes + width * 0.5, labels[:100], fontsize=10, rotation='vertical')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport squarify\n\ncmap = matplotlib.cm.Blues\nnorm = matplotlib.colors.Normalize(vmin=min(values[:100]), vmax=max(values[:100]))\ncolors = [cmap(norm(value)) for value in values[:100]]\n\nplt.figure(num=None, figsize=(16, 14), dpi=80, facecolor='w', edgecolor='k')\n\nsquarify.plot(label=labels[:100],sizes=values[:100], alpha=.8, color = colors)\nplt.title(\"Title Word Count\",fontsize=18,fontweight=\"bold\")\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict2_ = {'paper_id': [], 'title': [], 'abstract': [], 'body_text': [], 'publish_time': []}\nfor ind, entry in enumerate(all_json):\n    if ind % (len(all_json) // 10) == 0:\n        print(f'Processing index: {ind} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = data.loc[data['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    \n    dict2_['paper_id'].append(content.paper_id)\n    dict2_['title'].append(content.title)\n    dict2_['abstract'].append(content.abstract)\n    dict2_['body_text'].append(content.body_text)\n    dict2_['publish_time'].append(meta_data['publish_time'].values[0])\ndf_covid2 = pd.DataFrame(dict2_, columns=['paper_id', 'title', 'abstract', 'body_text', 'publish_time'])\ndf_covid2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid2_subset = df_covid2.dropna()\ndf_covid2_subset['publish_year'] = df_covid2_subset['publish_time'].apply(lambda x: x[:4])\ndf_covid2_subset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid2_subset['title_word_count'] = df_covid2_subset['title'].apply(lambda x: len(x.strip().split()))\ndf_covid2_subset['abstract_word_count'] = df_covid2_subset['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid2_subset['body_word_count'] = df_covid2_subset['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid2_subset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid2_subset['title_COVID-19_count'] = df_covid2_subset['title'].apply(lambda x: len([i for i, e in enumerate(x.strip().split()) if e == 'coronavirus']))\ndf_covid2_subset['abstract_COVID-19_count'] = df_covid2_subset['abstract'].apply(lambda x: len([i for i, e in enumerate(x.strip().split()) if e == 'coronavirus']))\ndf_covid2_subset['body_COVID-19_count'] = df_covid2_subset['body_text'].apply(lambda x: len([i for i, e in enumerate(x.strip().split()) if e == 'coronavirus']))\ndf_covid2_subset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_sum_pvt = pd.pivot_table(df_covid2_subset, values=['title_COVID-19_count', 'abstract_COVID-19_count', 'body_COVID-19_count'], \n                              index=['publish_year'], aggfunc=np.sum, fill_value=0)\n\n\nword_sum_pvt.plot(figsize=(16,14))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LDA learned from \n(1)https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n(2)https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.feature_extraction.text import CountVectorizer\n\n#count_vectorizer = CountVectorizer(stop_words='english')\n#count_data = count_vectorizer.fit_transform(df_covid2_subset['body_text'][:50])\n\nimport gensim, spacy, logging, warnings\nimport gensim.corpora as corpora\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim.models import CoherenceModel\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(df_covid2_subset['body_text'][:100]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define functions for stopwords, bigrams, trigrams and lemmatization\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n\nprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation as LDA\n\nnumber_topics = 5\nnumber_words = 10\n\nlda = LDA(n_components=number_topics, n_jobs=-1)\nlda.fit(count_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_ind, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_ind)\n        print(\" \".join([words[i]\n            for i in topic.argsort()[:-n_top_words -1:-1]]))\n\nprint_topics(lda, count_vectorizer, number_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyLDAvis import sklearn as sklearn_lda\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nLDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\npyLDAvis.display(LDAvis_prepared)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}