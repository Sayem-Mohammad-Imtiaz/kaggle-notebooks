{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# plot feature importance using built-in function\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. if you liked my work, please upvote this kernel since it will keep me motivated. On the other hand please share your pozitif or negatif comments and ideas about kernel with me.\n# 2.Our Goals: Increase scores and some different metrics with resampling  and gridsearch methods.\n    \n"},{"metadata":{},"cell_type":"markdown","source":"# Content\n# 1. [Data Visualization](#visualizations)\n\n# 2. [Build Model](#model)\n*      [Model conclusion ](#confusion)\n\n# 3. [Feature Engineering](#feature_engineering)\n\n*      [Feature Extraction ](#feature_extraction)\n*      [Create Different datasets ](#different_data)\n*      [Dimension reduction the Datasets ](#Dim_reduction)\n\n# 4. [Under Sampling Process](#under_samp)\n*      [Enn--- Tomeklinks--- Nearmiss ](#techniques)\n*      [GridSearch](#GridSearch)\n*      [F1 Scores After Under Sampling ](#F1_scores)\n\n\n# 5. [Over Sampling](#over_sampling)\n*      [F1 Scores After Over Sampling ](#over_sampling_f1)\n\n# 6. [Roc Curve and models conclusion](#roc_curve)\n\n*      [Conclusions ](#Conclusions)\n"},{"metadata":{},"cell_type":"markdown","source":"1. ID: ID of each client\n1. LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n1. SEX: Gender (1=male, 2=female)\n1. EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n1. MARRIAGE: Marital status (1=married, 2=single, 3=others)\n1. AGE: Age in years\n1. PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, 8=payment delay for eight months, 9=payment delay for nine months and above)\n1. PAY_2: Repayment status in August, 2005 (scale same as above)\n1. PAY_3: Repayment status in July, 2005 (scale same as above)\n1. PAY_4: Repayment status in June, 2005 (scale same as above)\n1. PAY_5: Repayment status in May, 2005 (scale same as above)\n1. PAY_6: Repayment status in April, 2005 (scale same as above)\n1. BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n1. BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n1. BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n1. BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n1. BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n1. BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n1. PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n1. PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n1. PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n1. PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n1. PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n1. PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n1. default.payment.next.month: Default payment (1=yes, 0=no)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns={\"default.payment.next.month\": \"target\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we know that there is no any missing value. Great!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"id number is not necessary for us..."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=[\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"visualizations\"></a>\n# Data Visualizations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x=\"target\", data=data, palette=\"bwr\")\nplt.xticks(np.arange(2),('no', \"yes\"))\nplt.xlabel(\"Default Payent\" , color=\"red\", alpha=0.7, size=22)\nplt.show()\n\nprint(data[\"target\"].value_counts())\nprint((100*6636)/30000,\"percantage of customers will default\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x=\"SEX\", data=data, palette=\"bwr\")\nplt.xticks(np.arange(2), ('Male', 'Female'))\nplt.xlabel(\"Sex\" , color=\"red\", alpha=0.7, size=22)\nplt.show()\nnumberofsex=data[\"SEX\"].value_counts()\n\nprint(\"Male count: {} \\nFamele count: {}\".format(numberofsex[1], numberofsex[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupby1 = data.groupby(\"SEX\")[\"LIMIT_BAL\"].mean()\nprint(groupby1)\n\nplt.figure(figsize=(12,6))\nplt.style.use(\"seaborn\")\ngroupby1.plot.bar(color=\"#10CCBC\")\nplt.xticks(np.arange(2), ('Male', 'Female'), color=\"#CC1057\")\nplt.xlabel(\"Sex\" , color=\"red\", alpha=0.7, size=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Age values are between {} and {}.\".format(data[\"AGE\"].min(), data[\"AGE\"].max()))\nlimit_mean=data.groupby('AGE')['LIMIT_BAL'].mean()\nprint(limit_mean)\nlimit_mean=limit_mean.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.style.use(\"ggplot\")\nplt.grid(True)\nplt.plot(np.arange(21,77,1),limit_mean, color=\"purple\", alpha=0.7)\nplt.title(\"Average credit limit and Age\", color=\"red\", size=22)\nplt.xlabel(\"Age\", color=\"black\", alpha=0.8, size=16)\nplt.xlim(20,80)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets we look at any relationship between age and repayment."},{"metadata":{"trusted":true},"cell_type":"code","source":"groupby_2=data.groupby('AGE')['target'].mean()\n\nplt.figure(figsize=(16,8))\nplt.style.use(\"ggplot\")\nplt.grid(True)\nplt.plot(np.arange(21,77,1),groupby_2, color=\"black\", alpha=0.7)\nplt.hlines(0.25,xmin=20, xmax=80, color=\"red\", alpha=0.4, linewidth=3)\nplt.title(\"repayment_value and Age\", color=\"red\", size=22)\nplt.xlabel(\"Age\", color=\"black\", alpha=0.8, size=16)\nplt.annotate('Critical line',xy=(120, 200), xycoords='figure points', xytext=(80, 160),textcoords='offset points',\n            arrowprops=dict(facecolor='red', shrink=0.05, alpha=0.6))\nplt.xlim(20,80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"EDUCATION\"].value_counts(dropna=False, ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x=\"EDUCATION\", data=data, palette=\"dark\")\nplt.xticks(np.arange(7), (\"graduate school\" if i==1 else \"university\" if i==2 else \"high school\" if i==3 else \n                          \"other1\" if i==4 else \"other2\" if i==5 else \"other3\" if i==6 else \"other\" for i in range(7)), color=\"black\")\nplt.xlabel(\"Education\", color=\"red\", size=22, alpha=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"target\"].value_counts())\n#(Yes = 1, No = 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupby_3=data.groupby('EDUCATION')['target'].mean()\ngroupby_3=pd.DataFrame(groupby_3)\ngroupby_3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Swarmplot takes more time... lets use that only 4000 sample for this visualization"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data2=data.iloc[:4000,:] #because its take a lot of time.\nsns.set(style=\"dark\")\nplt.figure(figsize=(14,10))\nsns.swarmplot(x =\"EDUCATION\", y = \"LIMIT_BAL\", hue=\"SEX\",  \n              data = data2, color=\"green\")\nplt.ylabel(\"Education levels\", color=\"red\")\nplt.ylabel(\"Credit Limits \", color=\"red\")\nplt.legend()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x=\"SEX\", data=data, palette=\"dark\")\nplt.xticks(np.arange(2), ('Married', 'single'))\nplt.xlabel(\"Marriage or Not\" , color=\"red\", alpha=0.7, size=22)\nplt.show()\nmarriage=data[\"MARRIAGE\"].value_counts()\nprint(\"Marriage count: {} \\nSingle count: {}\".format(marriage[1], marriage[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nsns.heatmap(data.corr(),annot=True,fmt='.2f',color='red',cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nX = data.iloc[:,:23]\ny = data.iloc[:,23]\nprint(X.columns)\ny=pd.DataFrame(y)\n\nclf = RandomForestClassifier()\nclf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"importance = pd.DataFrame({'Feature': X.columns, 'Feature importance': clf.feature_importances_})\nimportance = importance.sort_values(by='Feature importance',ascending=False)\n#print(importance)\n\nplt.figure(figsize = (12,8))\nplt.title('Features importance for RandomForestClassifier',fontsize=18, color=\"red\")\nplt.bar(x='Feature',height='Feature importance',data=importance, alpha = 0.8, edgecolor=\"black\")\nplt.xticks(rotation=80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model\"></a>\n\n# Build models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we controlled disturbitions and we can see the numbers are so close each other. Nice!"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data.drop(columns=['target'],axis=1)\ny=data.iloc[:,23]\n\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=35)\nprint(\"X_train:\" , len(X_train))\nprint(\"X_test:\", len(X_test))\nprint(\"y_train\", len(y_train))\nprint(\"y_test\", len(y_test))\n\n\n#y_train ve y_test deki 0 ve 1 değerleri.\nprint(\"percantage of target=0 :%\",round(len(data[data[\"target\"]==0])/len(data),3))\nprint(\"percantage of target=0 :%\",round(len(y_train[y_train==0]) / len(y_train),3))\nprint(\"percantage of target=0 :%\",round(len(y_test[y_test==0]) / len(y_test),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Are you realize any mistakes about logistic regression ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\nacc_log_train=log_reg.score(X_train,y_train)*100\nacc_log_test=log_reg.score(X_test,y_test)*100\ny_pred_log = log_reg.predict(X_test)\n\nprint('Training accuracy : % {}'.format(acc_log_train))\nprint('Test accuracy : % {}'.format(acc_log_test))\n\nscore_log=accuracy_score(y_pred_log,y_test)*100\nprint(\"Logistic Regression SCORE:\",score_log)\n\nprint(\"Log_Reg confusion matrix\")\ncm_log = confusion_matrix(y_test, y_pred_log)\nprint(cm_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nacc_rfc_train = rfc.score(X_train, y_train)*100\nacc_rfc_test = rfc.score(X_test, y_test)*100\ny_pred_rfc = rfc.predict(X_test)\n\nprint(\"Training accuracy : % {}\".format(acc_rfc_train))\nprint(\"Test Accuracy: % {}\".format(acc_rfc_test))\n\nscore_rfc=accuracy_score(y_pred_rfc,y_test)*100\nprint(\"RandomForestClassifier SCORE:{:.3f}\".format(score_rfc))\n\nprint(\"RandomForestClassifier Confusion matrix\")\ncm_rfc = confusion_matrix(y_test, y_pred_rfc)\nprint(cm_rfc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\n\nxgb.fit(X_train, y_train)\nacc_xgb_train = xgb.score(X_train, y_train)*100\nacc_xgb_test = xgb.score(X_test, y_test)*100\ny_pred_xgb =xgb.predict(X_test)\n\nprint(\"Training accuracy : % {}\".format(acc_xgb_train))\nprint(\"Test Accuracy: % {}\".format(acc_xgb_test))\n\nscore_xgb=accuracy_score(y_pred_xgb,y_test)*100\nprint(\"XgboostClassifier SCORE:\",score_xgb)\n\nprint(\"Xgb Confusion matrix\")\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\nprint(cm_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmodel_list=[log_reg, rfc, xgb]\n\nall_cross_scores=[]\n\n\nfor i in model_list:\n    scores=cross_val_score(i, X_train, y_train, cv=5, n_jobs=-1)\n    print(str(i),\", scores: \", scores)\n    all_cross_scores.append(scores)  \n    print(\"Mean cross validation score of {} models : {:.2f}\".format(i, scores.mean() ))\n    print(\"*\"* 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"log_cross_score = all_cross_scores[0]\nrfc_cross_score = all_cross_scores[1]\nxgb_cross_score = all_cross_scores[2]\n\nplt.style.use(\"ggplot\")\nfig, axs = plt.subplots(1, 3, figsize=(18,4))\n\naxs[0].plot(log_cross_score, linewidth=3, color=\"black\", marker='o')\naxs[0].hlines(log_cross_score.mean(),xmin=0, xmax=4, color=\"red\", alpha=0.4, linewidth=2)\naxs[0].set_ylabel(\"score\", color=\"red\", size=24)\naxs[0].set_title(\"Logistic Regression\", color=\"#FF69B4\")\naxs[0].grid(True)\n\naxs[1].plot(rfc_cross_score, linewidth=3, color=\"black\", marker='o')\naxs[1].hlines(rfc_cross_score.mean(),xmin=0, xmax=4, color=\"red\", alpha=0.4, linewidth=2)\naxs[1].set_xlabel(\"iterations\",  color=\"red\", size=24)\naxs[1].set_title(\"RandomForest\", color=\"#FF69B4\")\naxs[1].grid(True)\n\naxs[2].plot(xgb_cross_score, linewidth=3, color=\"black\", marker='o')\naxs[2].set_title(\"XGBoost\", color=\"#FF69B4\")\naxs[2].hlines(xgb_cross_score.mean(),xmin=0, xmax=4, color=\"red\", alpha=0.4, linewidth=2)\naxs[2].grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"confusion\"></a>\n\n# Actually the accuracy score is not our aim. We find out customers who in default. as a conclusion we use confusion matrix to have a better perspective."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nfig, axs = plt.subplots(1, 3, figsize=(24,10))\n\nplot_confusion_matrix(log_reg, X_test, y_test,cmap=plt.cm.Blues, ax=axs[0],display_labels=(\"not default\", \"default\"))\naxs[0].set_title(\"Logistic Regression\",fontsize=18, color=\"#CC1057\")\naxs[0].grid(False)\n\n\nplot_confusion_matrix(rfc, X_test, y_test,cmap=plt.cm.PuBuGn, ax=axs[1],display_labels=(\"not default\", \"default\"))\naxs[1].set_title(\"RandomForestClassifier\" ,fontsize=18, color=\"#10ABCC\")\naxs[1].grid(False)\n\nplot_confusion_matrix(xgb, X_test, y_test,cmap=plt.cm.magma, ax=axs[2],display_labels=(\"not default\", \"default\"))\naxs[2].set_title(\"XGBoost Classifier\",fontsize=18, color=\"#96CC10\")\naxs[2].grid(False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_cross_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_score={\"names\":[\"Log_reg\",\"RandomFC\",\"XGBoost\"], \"scores\":[score_log,score_rfc,score_xgb]}\ndata_score = pd.DataFrame(data_score)\n\nplt.figure(figsize=(12,8))\nplt.style.use(\"ggplot\")\nplt.bar(x='names',height='scores',data=data_score, color=\"#dc7633\")\nplt.yticks(np.arange(0,110,10), rotation=45)\nplt.title(\"MODEL SCORES\")\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"feature_engineering\"></a>\n# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"for accuracy or discrimination the most important part is feature engineering. We have to use math and statistical skill in there."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns = {'PAY_0': 'PAY_1'}, inplace = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Payment delays.\n1. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\ndefault payment (Yes = 1, No = 0)\n\n#-2: Tüketim yok; -1: Tam olarak ödendi; 0: Döner kredi kullanımı; 1 = bir aylık ödeme gecikmesi; 2 = iki aylık ödeme gecikmesi; . . .; 8 = sekiz aylık ödeme gecikmesi; 9 = dokuz ay ve üzeri ödeme gecikmesi."},{"metadata":{"trusted":true},"cell_type":"code","source":"payment_data=['PAY_1','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\ndata[payment_data]\nprint(data[\"PAY_1\"].value_counts())\nprint( len(data[(data[\"PAY_1\"]>=1) & (data[\"target\"]==1)]) )   #nice for new feature\nprint( len(data[(data[\"PAY_1\"]>=1) & (data[\"target\"]==0)]) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. 0 and 2 values is significant for understanding payment flow. But 0 is not important for us because of that every people can payment regular for a month in 6 month."},{"metadata":{"trusted":true},"cell_type":"code","source":"def investing_pay_1():\n    for j in range(1,6):\n        print(\"Payment_\"+str(j))\n        for i in range(-2,8,1):\n            pay_contains = len(data[(data[\"PAY_\"+str(j)]==i) & (data[\"target\"]==1)])\n            print(str(i)+\"_quantity: \", pay_contains)\n            \ninvesting_pay_1()\ndata[\"payment_1_eng\"]=[1 if i<0 else 0 if i==0 else 2 for i in data[\"PAY_1\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aykırı değerleri boxplotla görselleştirebiliriz aynı zamanda çeşitli formülasyonlar ile bu outlierları tespit edebiliriz."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['AGE'])\nprint(\"Skewness(çarpıklık): %{:.3f}\".format(data['AGE'].skew()))\nprint(\"Kurtosis(basıklık): %{:.3f}\".format(data['AGE'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"ggplot\")\nfig, axs = plt.subplots(2,3, figsize=(16,10))\n\naxs[0,0].boxplot(data[\"AGE\"])\naxs[0,0].set_title(\"Age outliers\", color=\"#14B0B3\", size=20)\naxs[0,0].set_ylabel(\"Age\", color=\"#36EA15\", size = 15)\n\naxs[0,1].boxplot(data[\"LIMIT_BAL\"])\naxs[0,1].set_title(\"Limit Amount\", color=\"#14B0B3\", size=20)\naxs[0,1].set_ylabel(\"credit_limit\", color=\"#258912\", size = 15)\n\naxs[0,2].boxplot(data[\"PAY_1\"])\naxs[0,2].set_title(\"PAY 1\", color=\"#14B0B3\", size=20)\naxs[0,2].set_ylabel(\"Pay_1 Amount\", color=\"#16560A\", size = 15)\n\naxs[1,0].boxplot(data[\"PAY_AMT1\"])\naxs[1,0].set_title(\"PAY Amount 1\", color=\"#14B0B3\", size=20)\naxs[1,0].set_ylabel(\"Pay_1 Amount\", color=\"#36EA15\", size = 15)\n\naxs[1,1].boxplot(data[\"PAY_AMT2\"])\naxs[1,1].set_title(\"PAY Amount 2\", color=\"#14B0B3\", size=20)\naxs[1,1].set_ylabel(\"Pay_1 Amount\", color=\"#258912\", size = 15)\n\naxs[1,2].boxplot(data[\"BILL_AMT1\"])\naxs[1,2].set_title(\"Bill Amount 1\", color=\"#14B0B3\", size=20)\naxs[1,2].set_ylabel(\"Pay_1 Amount\", color=\"#16560A\", size = 15)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ımmm its looks like normal distribution but ı think some outliers on the right side.\n2. at the same time the graph skewed distribution to the right"},{"metadata":{},"cell_type":"markdown","source":"# outlier önce tespit edip ardından bu değerlerin target yüzdelerine bakalım\n\n# Now lets catch outliers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_columns=[\"AGE\",\"LIMIT_BAL\", \"PAY_1\", \"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\", \"PAY_AMT1\", \"PAY_AMT2\",\"BILL_AMT1\",\"BILL_AMT2\"]\nage_outlier = []\nlimitbal_outlier = []\npay1_outlier = []\npay2_outlier = []\npay3_outlier = []\npay4_outlier = []\npay5_outlier = []\npay6_outlier = []\npayamt1_outlier = []\npayamt2_outlier = []\nbillamt_outlier = []\nbillamt2_outlier = []\n\noutliers_array=[age_outlier,limitbal_outlier,pay1_outlier,pay2_outlier,pay3_outlier, pay4_outlier, pay5_outlier, pay6_outlier,payamt1_outlier,payamt2_outlier,billamt_outlier,billamt2_outlier]\n\ndef outlier_detect(data):\n    for i, column in enumerate(outlier_columns):\n        Q1 = np.percentile(data[column], 25)\n        Q3 = np.percentile(data[column], 75)\n        IQR = Q3- Q1\n        outlier_step = IQR*1.5\n\n        outlier_values = data[(data[column]<Q1-outlier_step) | (data[column] >Q3+outlier_step)]\n        print(\"*****\", column, \"******\")\n        #print(outlier_values)\n        \n        outlier_values_and_1 = data[((data[column]<Q1-outlier_step) | (data[column] >Q3+outlier_step)) & (data[\"target\"]==1)]\n        print(\"1 ratio is: %\",((len(outlier_values_and_1)/len(outlier_values))*100))\n        \n        outliers_array[i].append(outlier_values_and_1.index)\n\noutlier_detect = outlier_detect(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"feature_extraction\"></a>\n# feature extraction"},{"metadata":{},"cell_type":"markdown","source":"1. about payment one"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"dangeroues_cust\"]=0\n\nfor i in pay1_outlier:\n    data[\"dangeroues_cust\"].iloc[i]=1\n\nprint(data[\"dangeroues_cust\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. about education"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"education_level\"]=[0 if i == 0 or i==4 or i==5 else 1 if i ==6 or i==1 else 2  for i in data[\"EDUCATION\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.about limit bal."},{"metadata":{"trusted":true},"cell_type":"code","source":"limit_mean = data[\"LIMIT_BAL\"].mean()\nlimit_std = data[\"LIMIT_BAL\"].std()\nprint(\"Credit limit mean: {:.2f} and standart deviation: {:.2f}\".format(limit_mean, limit_std))\n\nQ1= data[\"LIMIT_BAL\"].quantile(.20) \nQ2= data[\"LIMIT_BAL\"].quantile(.40) \nQ3= data[\"LIMIT_BAL\"].quantile(.60) \nQ4= data[\"LIMIT_BAL\"].quantile(.80) \ndata[\"limit_class\"] = [0 if i<Q1 else 1 if i>=Q1 and i<Q2 else 2 if i>=Q2 and i<Q3 else 4 if i>=Q3 and i<Q4 else 5 for i in data[\"LIMIT_BAL\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.pay amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"dangeroues_cust2\"]=0\n\nfor i in payamt2_outlier:\n    data[\"dangeroues_cust2\"].iloc[i]=1\n\nprint(data[\"dangeroues_cust2\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. repay_cust"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"total_payment\"]=data[\"PAY_AMT1\"]+data[\"PAY_AMT2\"]+data[\"PAY_AMT3\"]+data[\"PAY_AMT4\"]+data[\"PAY_AMT5\"]+data[\"PAY_AMT6\"]\n\ndata[\"repay_cust\"]=1\nrepay_cust = data[(data[\"total_payment\"]>data[\"LIMIT_BAL\"]) & (data[\"target\"]==0)].index\nfor i in repay_cust:\n    data[\"repay_cust\"].iloc[i]=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. pay grades means"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"pays_means\"]=0\ndef mean_grades(data, column):\n    for i in range(1,7):\n        data[column] += data[\"PAY_\"+str(i)]\n    data[column]=data[column]/i\n\nmean_grades(data, \"pays_means\")\n\n\npay_mean = data[\"pays_means\"].mean()\npay_std = data[\"pays_means\"].std()\npay_min = data[\"pays_means\"].min()\npay_max = data[\"pays_means\"].max()\nprint(\"for grades:\\n pay mean:{:.2f}\\n pay standart dev.:{:.2f}\\n pay min: {}\\n pay max: {}\".format(pay_mean,pay_std,pay_min, pay_max))\ndata[\"pays_means\"]=[0 if i<(pay_mean-pay_std) else 2 if i>(pay_mean+pay_std) else 1 for i in data[\"pays_means\"] ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Age Classes "},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1= data[\"AGE\"].quantile(.20) \nQ2= data[\"AGE\"].quantile(.40) \nQ3= data[\"AGE\"].quantile(.60) \nQ4= data[\"AGE\"].quantile(.80) \ndata[\"AGE_class\"] = [0 if i<Q1 else 1 if i>=Q1 and i<Q2 else 2 if i>=Q2 and i<Q3 else 4 if i>=Q3 and i<Q4 else 5 for i in data[\"AGE\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"different_data\"></a>\n# Creating Different Dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2=pd.DataFrame(data.iloc[:,23:])\nplt.figure(figsize=(18,10))\nsns.heatmap(data2.corr(),annot=True,fmt='.2f',color='red',cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data2 = pd.concat([data[[\"MARRIAGE\",\"PAY_1\"]], data2],axis=1)\ndata2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dont forget feature scaling for logistic regression !!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_2=data2.drop(columns=['target'],axis=1)\ny_2=data2.iloc[:,2]\n\n#scaled = (x - xmin) / (xmax - xmin)  for logistic regression\nX_2[\"total_payment\"] = (X_2[\"total_payment\"]- X_2[\"total_payment\"].min()) /  (X_2[\"total_payment\"].max()- X_2[\"total_payment\"].min()) \nX_train_2,X_test_2,y_train_2,y_test_2=train_test_split(X_2,y_2,test_size=0.35,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(X_train_2, y_train_2)\ny_pred_xgb =xgb.predict(X_test_2)\nscore_xgb_2=accuracy_score(y_pred_xgb,y_test_2)*100\nprint(\"Xgboost SCORE:{:.3f}\".format(score_xgb_2))\n\nrfc.fit(X_train_2, y_train_2)\ny_pred_rfc = rfc.predict(X_test_2)\nscore_rfc_2=accuracy_score(y_pred_rfc,y_test_2)*100\nprint(\"RandomForestClassifier SCORE:{:.3f}\".format(score_rfc_2))\n\nlog_reg = LogisticRegression(n_jobs=-1)\nlog_reg.fit(X_train_2, y_train_2)\ny_pred_log_2 = log_reg.predict(X_test_2)\nscore_log_2=accuracy_score(y_pred_log_2,y_test_2)*100\nprint(\"Logistic Regression SCORE:{:.3f}\".format(score_log_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_3=data.drop(columns=['AGE','target','PAY_6', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6','BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'],axis=1)\ny=data.iloc[:,23]\n\n\ndata3 = pd.concat([X_3, y], axis=1)\nX_train_3,X_test_3,y_train_3,y_test_3=train_test_split(X_3,y,test_size=0.35,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data3 Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(X_train_3, y_train_3)\ny_pred_xgb_ =xgb.predict(X_test_3)\nscore_xgb_3=accuracy_score(y_pred_xgb_,y_test_3)*100\nprint(\"XgboostClassifier SCORE:{:.3f}\".format(score_xgb_3))\nprint(\"Xgb Confusion matrix\")\ncm_xgb = confusion_matrix(y_test_3, y_pred_xgb_)\nprint(cm_xgb)\n\nrfc.fit(X_train_3, y_train_3)\ny_pred_rfc = rfc.predict(X_test_3)\nscore_rfc_3=accuracy_score(y_pred_rfc,y_test_3)*100\nprint(\"RandomForestClassifier SCORE:{:.3f}\".format(score_rfc_3))\nprint(\"RandomForestClassifier matrix\")\ncm_rfc = confusion_matrix(y_test_3, y_pred_rfc)\nprint(cm_rfc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Dim_reduction\"></a>\n# Dimension Deduction"},{"metadata":{},"cell_type":"markdown","source":"# for the more information about this part you should go to [StatQuest](https://www.youtube.com/user/joshstarmer) youtube channel!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.patches as mpatches\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_original = data.loc[data[\"target\"]==0].drop([\"target\"], axis=1)\nx_original2 = data.loc[data[\"target\"]==1].drop([\"target\"], axis=1)\n\nx_data = data2.loc[data2[\"target\"]==0].drop([\"target\"], axis=1)\nx_data2 = data2.loc[data2[\"target\"]==1].drop([\"target\"], axis=1)\n\nx_data3 = data3.loc[data3[\"target\"]==0].drop([\"target\"], axis=1)\nx_data_3 = data3.loc[data3[\"target\"]==1].drop([\"target\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA implementation for original data\nt1=time.time()\npca_reduc_X1 = PCA(n_components=2, random_state=0).fit_transform(x_original.values)\npca_reduc_y1 = PCA(n_components=2, random_state=0).fit_transform(x_original2.values)\nt2=time.time()\nprint(\"PCA time: {:.3f}\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA implementation for data2\nt1=time.time()\npca_reduc_X = PCA(n_components=2, random_state=0).fit_transform(x_data.values)\npca_reduc_y = PCA(n_components=2, random_state=0).fit_transform(x_data2.values)\nt2=time.time()\nprint(\"PCA time: {:.3f}\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA implementation for data3\nt1=time.time()\npca_reduc_X3 = PCA(n_components=2, random_state=0).fit_transform(x_data3.values)\npca_reduc_y3 = PCA(n_components=2, random_state=0).fit_transform(x_data_3.values)\nt2=time.time()\nprint(\"PCA time: {:.3f}\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TSNE implementation for data 3\nt1=time.time()\ntsne_reduc_X = TSNE(n_components=2, random_state=2).fit_transform(x_data3.values)\ntsne_reduc_y = TSNE(n_components=2, random_state=2).fit_transform(x_data_3.values)\nt2=time.time()\nprint(\"T-SNE time: {:.3f}\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(28,16))\nfig.suptitle(\"Dimension Reduction with PCA and T-SNE\", size=20)\n\naxs[0,0].scatter(pca_reduc_X1[:,0], pca_reduc_X1[:,1], color=\"#FAAC58\", label=(\"target=0\"), linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[0,0].scatter(pca_reduc_y1[:,0], pca_reduc_y1[:,1], color=\"#210B61\", label=(\"target=1\"), linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[0,0].set_title(\"Original Data Scatter with *PCA*\", color=\"#FF0040\", size=20)\naxs[0,0].legend()\n\naxs[0,1].scatter(pca_reduc_X[:,0], pca_reduc_X[:,1], color=\"#FAAC58\", label=(\"target=0\"), linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[0,1].scatter(pca_reduc_y[:,0], pca_reduc_y[:,1], color=\"#210B61\", label=(\"target=1\"), linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[0,1].set_title(\"Created Data2 Scatter with *PCA*\", color=\"#FF0040\", size=20)\naxs[0,1].legend()\n\naxs[1,0].scatter(pca_reduc_X3[:,0], pca_reduc_X3[:,1], color=\"#FAAC58\", label=(\"target=0\"), linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[1,0].scatter(pca_reduc_y3[:,0], pca_reduc_y3[:,1], color=\"#210B61\", label=(\"target=1\"), linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[1,0].set_title(\"Created Data3 Scatter with *PCA*\", color=\"#FF0040\", size=20)\naxs[1,0].legend()\n\naxs[1,1].scatter(tsne_reduc_X[:,0], tsne_reduc_X[:,1], color=\"#F4E80E\", label=(\"target=0\"), linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[1,1].scatter(tsne_reduc_y[:,0], tsne_reduc_y[:,1], color=\"#F1140D\", label=(\"target=1\"), linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[1,1].set_title(\"Created Data3 Scatter with *T-SNE*\", color=\"#FF0040\", size=20)\naxs[1,1].legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I make different data with different features. After that I observe their distributions. I determine data3(with PCA) is more processable. \n\n# Does this way make sense?\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"under_samp\"></a>\n# Undersampling process"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.under_sampling import NearMiss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enn= EditedNearestNeighbours(sampling_strategy=\"majority\", n_neighbors=2)\nTomekLinks = TomekLinks(sampling_strategy=\"not minority\")\nNearMiss = NearMiss(version=2)\n\n\n\ndef under_sampling_function(X_train_3, y_train_3, model, removed_array):\n\n    x_resampling, y_resampling = model.fit_resample(X_train_3, y_train_3)\n    removed_array = model.sample_indices_\n    print(\"*\"*25)\n    print(\"Models Resampling score with\", str(model))\n\n    xgb = XGBClassifier()\n    xgb.fit(x_resampling, y_resampling)\n    y_pred_xgb =xgb.predict(X_test_3)\n    score_xgb=accuracy_score(y_pred_xgb,y_test_3)*100\n    print(\"XGBoost Classifier SCORE:{:.3f}\".format(score_xgb))\n    cm_xgb = confusion_matrix(y_test_3, y_pred_xgb)\n    print(cm_xgb)\n    \n\n    rfc = RandomForestClassifier()\n    rfc.fit(x_resampling, y_resampling)\n    y_pred_rfc = rfc.predict(X_test_3)\n    score_rfc=accuracy_score(y_pred_rfc,y_test_3)*100\n    print(\"RandomForestClassifier SCORE:{:.3f}\".format(score_rfc))\n    cm_rfc = confusion_matrix(y_test_3, y_pred_rfc)\n    print(cm_rfc,\"\\n\")\n    \n\n    \n    return removed_array, x_resampling, y_resampling, y_pred_xgb, y_pred_rfc\n\n\nremoved_data_enn = []\nremoved_data_tmk = []\nremoved_data_NearM = []\n\n#bir liste oluşturalım ve fonksiyon parametresi olarak ekleyelim\nedited_nearest_neighbours = under_sampling_function(X_train_3, y_train_3, enn, removed_data_enn)\ntomeklinks_undersampling = under_sampling_function(X_train_3, y_train_3, TomekLinks, removed_data_tmk)\nnearmiss_undersampling = under_sampling_function(X_train_3, y_train_3, NearMiss, removed_data_NearM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"edited_nearest_neighbours_x = pd.DataFrame(edited_nearest_neighbours[1])\nedited_nearest_neighbours_y = pd.DataFrame(edited_nearest_neighbours[2])\n\ntomeklinks_undersampling_x = pd.DataFrame(tomeklinks_undersampling[1])\ntomeklinks_undersampling_y = pd.DataFrame(tomeklinks_undersampling[2])\n\nnearmiss_undersampling_x = pd.DataFrame(nearmiss_undersampling[1])\nnearmiss_undersampling_y = pd.DataFrame(nearmiss_undersampling[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_enn = pd.concat([edited_nearest_neighbours_x,edited_nearest_neighbours_y], axis=1)\ndata_tomek = pd.concat([tomeklinks_undersampling_x,tomeklinks_undersampling_y], axis=1)\ndata_nearmiss = pd.concat([nearmiss_undersampling_x,nearmiss_undersampling_y], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removed data for different under_sampling tecknique\nprint(\"number of removed data with EditedNearestNeighbours :{}\".format(data_enn.shape[0]))\nprint(\"number of removed data with TomekLinks :{}\".format(data_tomek.shape[0]))\nprint(\"number of removed data with Nearmiss(version2) :{}\".format(data_nearmiss.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"techniques\"></a>\n\n# 7. UnderSampling visualization"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x_0 = data3.loc[data[\"target\"]==0].drop([\"target\"], axis=1)\nx_1 = data3.loc[data[\"target\"]==1].drop([\"target\"], axis=1)\npca_reduc_X = PCA(n_components=2, random_state=0).fit_transform(x_0.values)\npca_reduc_y = PCA(n_components=2, random_state=0).fit_transform(x_1.values)\n\nx_0 = data_enn.loc[data_enn[\"target\"]==0].drop([\"target\"], axis=1)\nx_1 = data_enn.loc[data_enn[\"target\"]==1].drop([\"target\"], axis=1)\npca_reduc_X_enn =PCA(n_components=2, random_state=0).fit_transform(x_0.values)\npca_reduc_y_enn = PCA(n_components=2, random_state=0).fit_transform(x_1.values)\n\nx_0 = data_tomek.loc[data_tomek[\"target\"]==0].drop([\"target\"], axis=1)\nx_1 = data_tomek.loc[data_tomek[\"target\"]==1].drop([\"target\"], axis=1)\npca_reduc_X_tomek = PCA(n_components=2, random_state=0).fit_transform(x_0.values)\npca_reduc_y_tomek = PCA(n_components=2, random_state=0).fit_transform(x_1.values)\n\nx_0 = data_nearmiss.loc[data_nearmiss[\"target\"]==0].drop([\"target\"], axis=1)\nx_1 = data_nearmiss.loc[data_nearmiss[\"target\"]==1].drop([\"target\"], axis=1)\npca_reduc_X_nearmiss = PCA(n_components=2, random_state=0).fit_transform(x_0.values)\npca_reduc_y_nearmiss = PCA(n_components=2, random_state=0).fit_transform(x_1.values)\n\n\n\nfig, axs = plt.subplots(2, 2, figsize=(24,18))\n\naxs[0,0].scatter(pca_reduc_X[:,0], pca_reduc_X[:,1], color=\"#FAAC58\", label=0, linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[0,0].scatter(pca_reduc_y[:,0], pca_reduc_y[:,1], color=\"#210B61\", label=1, linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[0,0].set_title(\"Original Data Distribution\", color=\"#FF0040\", size=20)\naxs[0,0].set_xlabel(\"number of data: {}\".format(len(pca_reduc_X) +len(pca_reduc_y)))\naxs[0,0].spines['top'].set_visible(False)\naxs[0,0].spines['right'].set_visible(False)\naxs[0,0].get_xaxis().tick_bottom()\naxs[0,0].get_yaxis().tick_left()\n\n\naxs[0,1].scatter(pca_reduc_X_enn[:,0], pca_reduc_X_enn[:,1], color=\"#FAAC58\", label=0, linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[0,1].scatter(pca_reduc_y_enn[:,0], pca_reduc_y_enn[:,1], color=\"#210B61\", label=1, linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[0,1].set_title(\"Edited Nearest Neighbours\", color=\"#FF0040\", size=20)\naxs[0,1].set_xlabel(\"number of data: {}\".format(len(pca_reduc_X_enn)+len(pca_reduc_y_enn)))\n    \naxs[1,0].scatter(pca_reduc_X_tomek[:,0], pca_reduc_X_tomek[:,1], color=\"#FAAC58\", label=0, linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[1,0].scatter(pca_reduc_y_tomek[:,0], pca_reduc_y_tomek[:,1], color=\"#210B61\", label=1, linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[1,0].set_title(\"TomekLinks\", color=\"#FF0040\", size=20)\naxs[1,0].set_xlabel(\"number of data: {}\".format(len(pca_reduc_X_tomek)+len(pca_reduc_y_tomek)))\n\naxs[1,1].scatter(pca_reduc_X_nearmiss[:,0], pca_reduc_X_nearmiss[:,1], color=\"#FAAC58\", label=0, linewidth=0.5, alpha=0.7, edgecolors=\"black\")\naxs[1,1].scatter(pca_reduc_y_nearmiss[:,0], pca_reduc_y_nearmiss[:,1], color=\"#210B61\", label=1, linewidth=1, alpha=0.8, edgecolors=\"#E0F8E0\")\naxs[1,1].set_title(\"Nearmiss\", color=\"#FF0040\", size=20)\naxs[1,1].set_xlabel(\"number of data: {}\".format(len(pca_reduc_X_nearmiss)+len(pca_reduc_y_nearmiss)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"GridSearch\"></a>\n# GridSearch "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#randomforest grid_search\ndef rfc_gridsearch(Model, X_Train, y_train):\n    \n    param_grid_rfc = {\"n_estimators\":[100, 120, 140],\n                     \"criterion\": [\"gini\",\"entropy\"],\n                     \"min_samples_leaf\":list(range(3,5,1))}\n\n    grid_search = GridSearchCV(estimator=Model,\n                              param_grid = param_grid_rfc,\n                              scoring=\"accuracy\")\n\n    grid_search.fit(X_Train,y_train) \n    rfc_best_score=grid_search.best_score_\n    best_parameters=grid_search.best_params_\n                      \n    return Model, best_parameters\n\nrfc_gridSearch_def = rfc_gridsearch(rfc, edited_nearest_neighbours[1], edited_nearest_neighbours[2])\nprint(\"Random Forest Best Parameters: {}\".format(rfc_gridSearch_def[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(criterion='entropy', min_samples_leaf=4, n_estimators=140)\nrfc.fit(edited_nearest_neighbours[1], edited_nearest_neighbours[2])\ny_pred_rfc = rfc.predict(X_test_3)\n\nscore_rfc_enn_grid=accuracy_score(y_pred_rfc,y_test_3)*100\nprint(\"RandomForestClassifier SCORE:{:.3f}\".format(score_rfc_enn_grid))\n\ncm_rfc = confusion_matrix(y_test_3, y_pred_rfc)\nprint(cm_rfc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb grid search\ndef xgb_gridSearch(Model, X_train, y_train):\n    param_grid_xgb = { \"gamma\": [0.1, 0.5, 1, 2],\n                       \"learning_rate\": [0.1, 1, 5],\n                       \"max_depth\": [1, 5, 10]}\n\n    grid_search = GridSearchCV(estimator=Model,\n                              param_grid = param_grid_xgb,\n                              scoring=\"accuracy\")\n\n    grid_search.fit(X_train,y_train) \n\n    xgb_best_score=grid_search.best_score_\n    best_parameters=grid_search.best_params_\n    \n    return Model, best_parameters\n\nxgb_gridSearch_def_1 = xgb_gridSearch(xgb,  edited_nearest_neighbours[1], edited_nearest_neighbours[2])\nprint(\"XGBoost Best Parameters for ENN: {}\".format(xgb_gridSearch_def_1[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(gamma= 0.1 ,learning_rate=0.1, max_depth=5)\nxgb.fit(edited_nearest_neighbours[1], edited_nearest_neighbours[2])\ny_pred_xgb =xgb.predict(X_test_3)\n\nscore_xgb_enn_grid=accuracy_score(y_pred_xgb,y_test_3)*100\nprint(\"XGboost SCORE:{:.3f}\".format(score_xgb_enn_grid))\n\ncm_xgb = confusion_matrix(y_test_3, y_pred_xgb)\nprint(cm_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# xgb gridsearch did not change accuracy score effectively but it made precision and recall values more useful for our aim."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"F1_scores\"></a>\n# lets look at the F1 scores"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmodel_list=[rfc, xgb]\n\nall_cross_scores=[]\n\n\nfor i in model_list:\n    scores=cross_val_score(i, edited_nearest_neighbours[1], edited_nearest_neighbours[2], cv=5, n_jobs=-1)\n    print(str(i),\", scores: \", scores)\n    all_cross_scores.append(scores)  \n    print(\"Mean cross validation score of {} models : {:.2f}\".format(i, scores.mean() ))\n    print(\"*\"* 25)\n    \n\n    \nxgb_cross_score = all_cross_scores[0]\nrfc_cross_score = all_cross_scores[1]\n\n\nplt.style.use(\"ggplot\")\nfig, axs = plt.subplots(1, 2, figsize=(24,8))\n\n\naxs[0].plot(rfc_cross_score, linewidth=3, color=\"black\", marker='o')\naxs[0].hlines(rfc_cross_score.mean(),xmin=0, xmax=4, color=\"red\", alpha=0.4, linewidth=2)\naxs[0].set_xlabel(\"iterations\",  color=\"red\", size=24)\naxs[0].set_title(\"RandomForest\", color=\"#FF69B4\")\naxs[0].grid(True)\n\naxs[1].plot(xgb_cross_score, linewidth=3, color=\"black\", marker='o')\naxs[1].set_title(\"XGBoost\", color=\"#FF69B4\")\naxs[1].hlines(xgb_cross_score.mean(),xmin=0, xmax=4, color=\"red\", alpha=0.4, linewidth=2)\naxs[1].grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"over_sampling\"></a>\n# Over Sampling Process"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import KMeansSMOTE\nfrom imblearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE(sampling_strategy=0.3)\nadsyn = ADASYN(n_neighbors=3)\n\n\n\ndef over_sampling_function(X_train, y_train, model):\n    \n    x_resampling, y_resampling = model.fit_resample(X_train_3, y_train_3)\n    \n    print(\"*\"*25)\n    print(\"Models Resampling score with\", str(model))\n\n    xgb = XGBClassifier()\n    xgb.fit(x_resampling, y_resampling)\n    y_pred_xgb_over =xgb.predict(X_test_3)\n    score_xgb=accuracy_score(y_pred_xgb_over,y_test_3)*100\n    print(\"XGBoost Classifier SCORE:{:.3f}\".format(score_xgb))\n    cm_xgb = confusion_matrix(y_test_3, y_pred_xgb_over)\n    print(cm_xgb)\n\n    rfc = RandomForestClassifier()\n    rfc.fit(x_resampling, y_resampling)\n    y_pred_rfc_over = rfc.predict(X_test_3)\n    score_rfc=accuracy_score(y_pred_rfc_over,y_test_3)*100\n    print(\"RandomForestClassifier SCORE:{:.3f}\".format(score_rfc))\n    cm_rfc = confusion_matrix(y_test_3, y_pred_rfc_over)\n    print(cm_rfc,\"\\n\")\n    \n    return y_pred_xgb_over, y_pred_rfc_over, x_resampling,y_resampling\n\n\n\nsmote_function=over_sampling_function(X_train_3, y_train_3, smote) \nadasyn_function=over_sampling_function(X_train_3, y_train_3, adsyn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"over_sampling_f1\"></a>\n# F1 scores  of Over Sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score_xgb_2  = classification_report(y_test_3, smote_function[0], digits=3)\nprint(\"*\"*20,\"XGBoost scores with SMOTE\",\"*\"*20,\"\\n\",f1_score_xgb_2)\n\nf1_score_xgb  = classification_report(y_test_3, adasyn_function[0], digits=3)\nprint(\"*\"*20,\"XGBoost scores with SMOTE\",\"*\"*20,\"\\n\",f1_score_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# we have some datasets and teckniques. recall precions we başarı yüzdelerini dikkate aldığımızda bu modelleri ve şu teknikleri seçtik."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"xgb = XGBClassifier(gamma= 0.1, learning_rate=0.1, max_depth=5)\nxgb.fit(edited_nearest_neighbours[1], edited_nearest_neighbours[2])\nxgb_pred_enn = xgb.predict(X_test_3)\nscore_xgb_enn=accuracy_score(xgb_pred_enn,y_test_3)*100\n\n\nrfc = RandomForestClassifier(criterion='entropy', min_samples_leaf=4, n_estimators=140)\nrfc.fit(edited_nearest_neighbours[1], edited_nearest_neighbours[2])\nrfc_pred_enn = rfc.predict(X_test_3)\nscore_rfc_enn=accuracy_score(rfc_pred_enn,y_test_3)*100\n\n\nxgb_2 = XGBClassifier() \nxgb_2.fit(nearmiss_undersampling[1], nearmiss_undersampling[2])  \n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(28,10))\n\nplot_confusion_matrix(xgb, X_test_3, y_test_3 ,cmap=plt.cm.copper, ax=axs[0],display_labels=(\"not default\", \"default\"))\naxs[0].set_title(\"XGBoost/ EditedNearestNeighbours\",fontsize=22, color=\"#FF0000\")\naxs[0].grid(False)\n\n\nplot_confusion_matrix(rfc, X_test_3, y_test_3 ,cmap=plt.cm.hot, ax=axs[1],display_labels=(\"not default\", \"default\"))\naxs[1].set_title(\"RandomFC/ EditedNearestNeighbours\",fontsize=22, color=\"#FF0000\")\naxs[1].grid(False)\n\nplot_confusion_matrix(xgb_2, X_test_3, y_test_3 ,cmap=plt.cm.magma, ax=axs[2],display_labels=(\"not default\", \"default\"))\naxs[2].set_title(\"XGBoost Classifier / NearMiss\",fontsize=22, color=\"#FF0000\")\naxs[2].grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top 3 classification_report"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score_xgb_enn  = classification_report(y_test_3, y_pred_xgb, digits=3)\nprint(\"*\"*20,\"XGB scores with EditedNearestNeighbours\",\"*\"*20,\"\\n\",f1_score_xgb_enn)\n\nf1_score_rfc_enn  = classification_report(y_test_3, y_pred_rfc, digits=3)\nprint(\"*\"*20,\"RandomForest scores with EditedNearestNeighbours\",\"*\"*20,\"\\n\",f1_score_rfc_enn)\n\nf1_score_rfc_nearM  = classification_report(y_test_3, nearmiss_undersampling[3], digits=3)\nprint(\"*\"*20,\"RandomForest scores with NearMiss\",\"*\"*20,\"\\n\",f1_score_rfc_nearM)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"roc_curve\"></a>\n# Roc Curve"},{"metadata":{},"cell_type":"markdown","source":"xgb = XGBClassifier(gamma= 0.1, learning_rate=0.1, max_depth=10)\n\nxgb.fit(edited_nearest_neighbours[1], edited_nearest_neighbours[2])\n\nrfc = RandomForestClassifier(criterion='gini', min_samples_leaf=3, n_estimators=120)\n\nrfc.fit(edited_nearest_neighbours[1], edited_nearest_neighbours[2])\n\nxgb_2 = XGBClassifier(gamma= 1, learning_rate=0.1, max_depth=5) \n\nxgb_2.fit(nearmiss_undersampling[1], nearmiss_undersampling[2])  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_auc_score\nimport sklearn.metrics as metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"probs = xgb.predict_proba(X_test_3)\npreds = probs[:,1]\nxgb_fpr, xgb_tpr, lof_threshold_xgb = roc_curve(y_test_3, preds)\nroc_auc_xgb = metrics.auc(xgb_fpr, xgb_tpr)\n\nprobs = rfc.predict_proba(X_test_3)\npreds = probs[:,1]\nrfc_fpr, rfc_tpr, lof_threshold_rfc = roc_curve(y_test_3, preds)\nroc_auc_rfc=roc_auc_score(y_test_3, preds)\n\nprobs = xgb_2.predict_proba(X_test_3)\npreds = probs[:,1]\nxgb_fpr_2, xgb_tpr_2, lof_threshold_xgb = roc_curve(y_test_3, preds)\nroc_auc_xgb_2 = metrics.auc(xgb_fpr_2, xgb_tpr_2)\n\n\ndef graph_roc_curve_multiple(xgb_fpr, xgb_tpr,rfc_fpr, rfc_tpr):\n    \n    plt.figure(figsize=(16,12))\n    plt.title(\"ROC Curve \\n TOP 3 Classifiers using X_Train3\", fontsize=18)\n    \n    plt.plot(xgb_fpr, xgb_tpr, color=\"red\",    marker=\".\", markersize=3, markeredgecolor =\"white\", markeredgewidth=.1, label=\"AUC XGboost C. with ENN:{:.4f}\".format(roc_auc_xgb))\n    plt.plot(rfc_fpr, rfc_tpr, color=\"purple\", marker=\".\", markersize=3, markeredgecolor =\"white\", markeredgewidth=.1, label=\"AUC RandomFC with ENN:{:.4f}\".format(roc_auc_rfc))\n    plt.plot(xgb_fpr_2, xgb_tpr_2, color=\"yellow\", marker=\"o\", markersize=3, markeredgecolor =\"green\", markeredgewidth=.1, label=\"AUC XGboost C. with Nearmiss:{:.4f}\".format(roc_auc_xgb_2))\n    \n    plt.plot([0,1], [0,1], \"k--\")\n    plt.axis([-0.01, 1, 0, 1.01])\n    #eksenler ile çizim konumlarını belirler\n    plt.xlabel(\"False Positive Rate\", fontsize=16)\n    plt.ylabel(\"True Positive Rate\", fontsize=16)\n\n    plt.legend()\n    plt.show()\n    \ngraph_roc_curve_multiple(xgb_fpr, xgb_tpr,rfc_fpr, rfc_tpr)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Conclusions\"></a>\n# Datasets about Conclusions"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {'X_train':   [round(score_rfc,3), round(score_xgb,3)],\n          'X_train_2': [round(score_rfc_2,3), round(score_xgb_2,3)],\n          'X_train_3':  [round(score_rfc_3,3), round(score_xgb_3,3)],\n          \"Scores with ENN (over_sampling)\":[round(score_rfc_enn,3), round(score_xgb_enn,3)],\n           \"Scores with ENN and GridSearchng)\":[round(score_rfc_enn_grid,3), round(score_xgb_enn_grid,3)]}\n\npd.DataFrame(index=['Random Forest Classifier', 'XGboost Classifier'],\n             data = scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# so the dataset is the shows that why we use X_train_3 and ENN. little Bam ?\n# but the answer is not just about Accuracy scores lets look over precision Scores"},{"metadata":{},"cell_type":"markdown","source":"1. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives.\n2. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_precision_score  = precision_score(y_pred_xgb_, y_test_3)\nxgb_enn_precision  = precision_score(xgb_pred_enn, y_test_3)\nprint(\"Xgboost Classifier precision score without enn technique :{:.3f} \".format(xgb_precision_score))\nprint(\"Xgboost Classifier precision score with enn technique :{:.3f} \".format(xgb_enn_precision))\n\nprint(\"*\"*50)\nrfc_precision_score  = precision_score(y_pred_rfc, y_test_3)\nrfc_enn_precision  = precision_score(rfc_pred_enn, y_test_3)\nprint(\"Random Forest Classifier precision score without enn technique :{:.5f} \".format(rfc_precision_score))\nprint(\"Random Forest Classifier  precision score with enn technique :{:.5f} \".format(rfc_enn_precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# X_train_3 Precision Scores.  (I Think this dataset is Double Bam!!!) Thanks the  [statquest with Josh Starmer](https://www.youtube.com/user/joshstarmer) for Great informations."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {\"Precision Score of without ENN\":[round(rfc_precision_score,4), round(xgb_precision_score,4)],\n         \"Precision Score with ENN\":[round(rfc_enn_precision,4), round(xgb_enn_precision,4)]}\n\npd.DataFrame(index=['Random Forest Classifier', 'XGboost Classifier'],\n             data = scores)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}