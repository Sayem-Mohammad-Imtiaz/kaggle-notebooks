{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install mlrose","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import mlrose\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nimport time\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import ensemble\nfrom sklearn import svm\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport math\nimport matplotlib.ticker as plticker\nimport matplotlib.patches as mpatches\nimport matplotlib.lines as mlines\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics \nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\nprint(\"Finished installing modules.\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# EDIT HERE!!\ninput_source = '/kaggle/input'\ndata_frames = []\nfor dirname, _, filenames in os.walk(input_source):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        data_frames.append(pd.read_csv(file_path))\nbig_frame = pd.concat(data_frames, ignore_index=True)\nbig_frame.info()\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df = pd.DataFrame(columns=[\"model\", \"recall\", \"accuracy\", \"precision\"])\nbig_frame = big_frame.drop(columns=[\"tourney_id\",\"tourney_name\", \"draw_size\", \"tourney_date\", \"match_num\", \"winner_id\", \"winner_seed\", \"tourney_level\", \"winner_ioc\",\\\n                                    \"winner_rank_points\", \"winner_entry\",\"winner_name\", \"loser_id\", \"loser_seed\", \"loser_rank_points\", \"loser_entry\", \"loser_ioc\",\\\n                                    \"score\", \"round\", \"loser_name\", \"minutes\",\"surface\", \"winner_hand\", \"winner_ht\", \"winner_age\", \"winner_rank\", \"loser_hand\", \"loser_ht\",\\\n                                    \"loser_age\", \"loser_rank\" ])\nbig_frame = big_frame.dropna()\nbig_frame.info()\n#big_frame[\"surface\"].value_counts()\n# filtered_df = big_frame[(big_frame.surface == \"Hard\")] # | (big_frame.surface == \"Clay\" )] #\"Clay\", \"Grass\", \"Carpet\"])]\nfiltered_df = big_frame[(big_frame.best_of == 3)]\nbig_frame.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"filtered_df[\"best_of\"].value_counts()\nw\nw_ace        44451 non-null float64\nw_df         44451 non-null float64\nw_svpt       44451 non-null float64\nw_1stIn      44451 non-null float64\nw_1stWon     44451 non-null float64\nw_2ndWon     44451 non-null float64\nw_SvGms      44451 non-null float64\nw_bpSaved    44451 non-null float64\nw_bpFaced    44451 non-null float64\nl_ace        44451 non-null float64\nl_df         44451 non-null float64\nl_svpt       44451 non-null float64\nl_1stIn      44451 non-null float64\nl_1stWon     44451 non-null float64\nl_2ndWon     44451 non-null float64\nl_SvGms      44451 non-null float64\nl_bpSaved    44451 non-null float64\nl_bpFaced    44451 non-null float64\n\"\"\"\nlost_df = filtered_df[['l_ace', 'l_df', 'l_svpt','l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms', 'l_bpSaved', 'l_bpFaced']]\nwon_df = filtered_df[['w_ace', 'w_df', 'w_svpt', 'w_1stIn', 'w_1stWon', 'w_2ndWon','w_SvGms', 'w_bpSaved', 'w_bpFaced']]\nlost_df = lost_df.rename(columns={'l_ace':'ace', 'l_df':'df', 'l_svpt':'svpt','l_1stIn':'1stIn', 'l_1stWon':'1stWon', 'l_2ndWon':'2ndWon', \\\n                                  'l_SvGms':'SvGms', 'l_bpSaved':'bpSaved', 'l_bpFaced':'bpFaced'})\nwon_df = won_df.rename(columns={'w_ace':'ace', 'w_df':'df', 'w_svpt':'svpt','w_1stIn':'1stIn', 'w_1stWon':'1stWon', 'w_2ndWon':'2ndWon',\\\n                                'w_SvGms':'SvGms', 'w_bpSaved':'bpSaved', 'w_bpFaced':'bpFaced'})\n\nall_players = pd.concat(data_frames, ignore_index=True)\nwon_df[\"won\"] = won_df['svpt']**0\nlost_df[\"won\"] = lost_df['svpt']*0\n\nbig_df = pd.concat([won_df, lost_df], ignore_index=True)\nbig_df = big_df.sample(frac=0.1, random_state=1)\nbig_df = big_df.drop(columns=['svpt'])\nbig_df.info\n# lost_df[\"ace_pct\"] = filtered_df.apply(lambda x: x.l_ace / x.l_svpt)\n# won_df[\"ace_pct\"] = filtered_df.apply(lambda x: x.w_ace / x.w_svpt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# util functions\n\ndef eval_for_conclusion(model_id, clf, test_x, test_y):\n    y_pred = clf.predict(test_x)\n    print(classification_report(test_y, y_pred))\n    print(confusion_matrix(test_y, y_pred))\n    accuracy = metrics.accuracy_score(test_y, y_pred)\n    precision = metrics.precision_score(test_y, y_pred)\n    recall = metrics.recall_score(test_y, y_pred)\n    print(\"Final {0} model accuracy:\".format(model_id), accuracy)\n    print(\"Final {0} model precision:\".format(model_id), precision) \n    print(\"Final {0} model recall:\".format(model_id), recall) \n    return {\"model\":model_id, \"recall\":recall, \"accuracy\":accuracy, \"precision\":precision}\n\ndef split_test_train(train_size, all_data):\n    msk = np.random.rand(len(all_data)) < train_size\n    train_df = all_data[msk]\n    test_df = all_data[~msk]\n    train_y = train_df[\"won\"]\n    train_x = train_df.drop(\"won\", axis=1)\n    test_y = test_df[\"won\"]\n    test_x  = test_df.drop(\"won\", axis=1)\n    return (train_x, train_y, test_x, test_y)\n\ndef cross_validate(all_data, model):\n    depth = []\n    all_y = all_data[\"won\"]\n    all_x  = all_data.drop(\"won\", axis=1)\n    for i in range(2,10):\n        # Perform n-fold cross validation \n        scores = cross_val_score(estimator=model, X=all_x, y=all_y, cv=i, n_jobs=4)\n        # print(\"i scores for cv: \", scores)\n        depth.append((i,scores.mean()))\n    # print(depth)\n    return depth\n    \ndef train_and_test(all_data, model):\n    test_scores = []\n    train_scores = []\n    times = []\n    for i in range(1,10):\n        (train_x, train_y, test_x, test_y) = split_test_train(0.1 * i, big_df)\n        #print(\"len test: \", len(test_x), \", len train: \", len(train_x))\n        start = time.time()\n        #TODO iterations\n        model.fit(train_x, train_y)\n        end = time.time()\n        times.append(end - start)\n        pred_test_y = model.predict(test_x) # TODO add wallclock time\n        test_score = round(model.score(test_x, test_y) * 100, 2)\n        pred_train_y = model.predict(train_x)\n        train_score = round(model.score(train_x, train_y) * 100, 2)\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n    return (test_scores, train_scores, times)\n\ndef plot_data(x_vars, x_label, all_y_vars, y_var_labels, y_label, title, y_bounds=None):\n    colors = ['red','orange','black','green','blue','violet']\n    plt.rcParams[\"figure.figsize\"] = (4,3)\n\n    i = 0\n    for y_var in all_y_vars:\n#         if i == 2: # don't plot when i = 1 for cv\n#             x_vars = x_vars[1:]\n        plt.plot(x_vars, y_var, 'o-', color=colors[i % 6], label=y_var_labels[i])\n        i += 1\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    if y_bounds != None:\n        plt.ylim(y_bounds)\n    leg = plt.legend();\n    plt.show()\n\ndef evaluate_model(all_data, model, model_id):\n    (test_scores, train_scores, times) = train_and_test(all_data, model)\n    cv_scores = cross_validate(all_data, model)\n    print(\"{0} train timings (seconds): {1}\".format(model_id, times))\n    print(\"{0} test set scores: {1} \".format(model_id, test_scores))\n    print(\"{0} train set scores: {1}\".format(model_id, train_scores))\n    print(\"{0} cross validation set scores: {1}\".format(model_id, cv_scores))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [test_scores, train_scores],\\\n              [\"test_scores\", \"train_scores\"], \"Accuracy\", \"{0} Accuracy Over Train/Test Split\".format(model_id), (50,103))\n    plot_data([x[0] for x in cv_scores], \"Number of folds\", [[x[1] for x in cv_scores]],\n             [\"cross_validation_accuracy\"], \"Accuracy\", \"{0} Accuracy Over Different Cross Validation Values of K\".format(model_id), (0.3,1))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [times],\n             [\"times\"], \"Train time in Seconds\", \"{0} Time Spent Training Over Train/Test Split\".format(model_id))\n    return (test_scores, train_scores, times, cv_scores)\n\ndef plot_grid_search(grid_results, plotting_func, title, x_label, y_label, grid_size, model_handles):\n    plt.rcParams[\"figure.figsize\"] = grid_size\n    means = grid_results.cv_results_['mean_test_score']\n    stds = grid_results.cv_results_['std_test_score']\n    params = grid_results.cv_results_['params']\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.subplots\n    ax = plt.subplot()\n    for mean, std, params in zip(means, stds, params):\n        #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n        plotting_func(mean, params, plt, ax)\n    if handles: plt.legend(handles=model_handles)\n    plt.show()\n\n\n#def grid_search(model, params, x_train, y_train, x_test, y_test):\n    \n\n#TODO come up with graphing function that takes in two arrays of test and train and plots them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(nn_train_x, nn_train_y, nn_test_x, nn_test_y) = split_test_train(0.1 * 8, big_df)\nscaler = StandardScaler()\nscaler.fit(nn_train_x)\nnn_train_x = scaler.transform(nn_train_x)\nnn_test_x = scaler.transform(nn_test_x)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def randomized_train_nn(rando_algo, train_x, train_y, test_x, test_y, accuracies, times, max_iters=100, max_attempts=3, \n                        early_stopping=False, mutation_prob=0.1, hidden_nodes=[5,5], pop_size=200, to_print=False):\n    nn_model = mlrose.NeuralNetwork(hidden_nodes = hidden_nodes, activation = 'tanh', \\\n                                     algorithm = rando_algo, max_iters = max_iters, \\\n                                     bias = True, is_classifier = True, learning_rate = 0.1,\n                                     clip_max = 5, max_attempts = max_attempts, random_state = 3,curve=True, early_stopping=early_stopping, mutation_prob=mutation_prob,pop_size=pop_size)\n    start = time.time()\n    nn_model.fit(train_x, train_y)\n    end = time.time()\n    time_elapsed = end - start\n    # Predict labels for train set and assess accuracy\n    y_train_pred = nn_model.predict(train_x)\n    #y_train_accuracy = accuracy_score(train_y, y_train_pred)\n    #print(\"{0} y train accuracy: {1}\".format(rando_algo,y_train_accuracy))\n    # Predict labels for test set and assess accuracy\n    y_test_pred = nn_model.predict(test_x)\n    y_test_accuracy = accuracy_score(test_y, y_test_pred)\n    \n    accuracies.append(y_test_accuracy)\n    times.append(time_elapsed)\n\n    if to_print:\n        print(classification_report(test_y, y_test_pred))\n        print(confusion_matrix(test_y, y_test_pred))\n        print(\"{0} y test accuracy: {1}\".format(rando_algo,y_test_accuracy))\n        print(\"{0} time elapsed: {1}\".format(rando_algo,time_elapsed))\n    return nn_model\n\ndef iterative_train(times, accuracies, iters_range, problem_text, early_stopping=False, mutation_prob=0.1):\n    nn_model = None\n    for i in iters_range:\n        nn_model = randomized_train_nn(problem_text, nn_train_x, nn_train_y, nn_test_x, nn_test_y, accuracies, times, i, \n                                       early_stopping=early_stopping, mutation_prob=mutation_prob)\n\n    plt.plot(iters_range,times,'o-')\n    plt.title(\"Neural Net Training Time over {0}\".format(problem_text))\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Time\")\n    plt.show()\n\n    plt.plot(iters_range,accuracies,'o-')\n    plt.title(\"Neural Net Test Accuracy over {0}\".format(problem_text))\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Highest Score\")\n    plt.show()\n    return nn_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hc_times = []\nhc_acc = []\niters_range = range(1, 2200, 200)\niterative_train(hc_times, hc_acc, iters_range, \"random_hill_climb\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hc_times = []\nhc_acc = []\n#iters_range = range(1, 2200, 200)\n#iterative_train(hc_times, hc_acc, iters_range, \"random_hill_climb\")\nhc_nn = randomized_train_nn(\"random_hill_climb\", nn_train_x, nn_train_y, nn_test_x, nn_test_y, ga_accuracies, ga_times,\\\n                            max_iters=2000, early_stopping=False, mutation_prob=.15, hidden_nodes=[2], pop_size=2000, max_attempts=10, to_print=True)\nplt.plot(range(1,len(hc_nn.fitness_curve)+1), hc_nn.fitness_curve)\nplt.title(\"Neural Net Fitness over genetic_alg\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Fitness\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sa_times = []\nsa_accuracies = []\niters_range = range(1, 2200, 200)\niterative_train(sa_times, sa_accuracies, iters_range, \"simulated_annealing\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sa_accuracies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"later_iters_range = range(2000, 3500, 500)\niterative_train(sa_times, sa_accuracies, later_iters_range, \"simulated_annealing\")\nx_axis = list(range(1, 2200, 200)) + [2500,3000]\n# nn_model = randomized_train_nn(\"simulated_annealing\", nn_train_x, nn_train_y, nn_test_x, nn_test_y,sa_times, sa_accuracies)\nplt.plot(x_axis,sa_accuracies,\"o-\")\nplt.title(\"Neural Net Test Accuracy over simulated_annealing\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Highest Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ga_times = []\nga_accuracies = []\niters_range = range(1,101)\n#ga_nn = iterative_train(ga_times, ga_accuracies, iters_range, \"genetic_alg\",early_stopping=True, mutation_prob=.3)\n\nga_nn = randomized_train_nn(\"genetic_alg\", nn_train_x, nn_train_y, nn_test_x, nn_test_y, ga_accuracies, ga_times,\\\n                            max_iters=100, early_stopping=True, mutation_prob=.1, hidden_nodes=[5,5], pop_size=2000, max_attempts=3, to_print=True)\nplt.plot(range(1,len(ga_nn.fitness_curve)+1), ga_nn.fitness_curve)\nplt.title(\"Neural Net Fitness over genetic_alg\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Fitness\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ga_nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_and_time_problem(run_problem, problem_name, to_print=True):\n    start = time.time()\n    best_state, best_fitness = run_problem()\n    end = time.time()\n    time_diff = end - start\n    if to_print:\n        print('The {} best state found is: {}'.format(problem_name, best_state))\n        print('The {} fitness at the best state is: {}'.format(problem_name, best_fitness))\n        print('The {} time elapsed to compute is: {}'.format(problem_name, time_diff))\n        print(\"\")\n    return(best_state, best_fitness, time_diff)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_time(algo, times, scores):\n    start = time.clock()\n    state, top_score = algo()\n    end = time.clock()\n    times.append(end - start)\n    scores.append(top_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hill_times = []\nhill_scores = []\nanneal_times = []\nanneal_scores = []\ngenetic_times = []\ngenetic_scores = []\nmimic_times = []\nmimic_scores = []\n\ndef run_tsp(num_points):\n    dist_list = []\n\n    for x in range(num_points):\n        for y in range(num_points):\n            random.seed(x+y)\n            dist_list.append((x, y, random.uniform(0.0, 1.0)))\n\n    fitness_dists = mlrose.TravellingSales(distances=dist_list)\n    tsp_problem = mlrose.TSPOpt(\n        length=num_points, fitness_fn=fitness_dists, maximize=True)\n\n    print (\"Traveling Salesman Problem with \", num_points, \" points\")\n\n    train_and_time(lambda : mlrose.random_hill_climb(problem=tsp_problem, max_attempts=1),hill_times, hill_scores)\n    train_and_time(lambda : mlrose.simulated_annealing(problem=tsp_problem, max_attempts=1), anneal_times, anneal_scores)\n    train_and_time(lambda : mlrose.genetic_alg(pop_size = 30, problem=tsp_problem, max_attempts=1), genetic_times, genetic_scores)\n    train_and_time(lambda : mlrose.mimic(pop_size = 30, problem=tsp_problem, max_attempts=1), mimic_times, mimic_scores)\n    \npoint_range = range(10,50,10)\nfor p in point_range:\n    run_tsp(p)\nplt.rcParams[\"figure.figsize\"] = (5,5)\n\nplt.plot(point_range,hill_times,'o-',point_range,anneal_times,'o-',point_range,genetic_times,'o-',point_range,mimic_times,'o-')\nplt.title(\"TSP Problem\")\nplt.xlabel(\"Num cities\")\nplt.ylabel(\"Time\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()\n\nplt.plot(point_range,hill_scores,'o-',point_range,anneal_scores,'o-', point_range,genetic_scores,'o-',point_range,mimic_scores,'o-')\nplt.title(\"TSP Problem\")\nplt.xlabel(\"Num cities\")\nplt.ylabel(\"Highest Score\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####### Continuous PEAKS\n\ncp_hill_times = []\ncp_hill_scores = []\ncp_anneal_times = []\ncp_anneal_scores = []\ncp_genetic_times = []\ncp_genetic_scores = []\ncp_mimic_times = []\ncp_mimic_scores = []\n\ndef run_continuous_peaks(num_points):\n    continuous_peaks = mlrose.DiscreteOpt(length = num_points,fitness_fn = mlrose.ContinuousPeaks(t_pct=0.15))\n    print (\"Continuous Peaks Problem with \", num_points, \" points\")\n    train_and_time(lambda : mlrose.random_hill_climb(problem=continuous_peaks, max_attempts=6),cp_hill_times, cp_hill_scores)\n    train_and_time(lambda : mlrose.simulated_annealing(problem=continuous_peaks, max_attempts=6), cp_anneal_times, cp_anneal_scores)\n    train_and_time(lambda : mlrose.genetic_alg(pop_size = 30, problem=continuous_peaks, max_attempts=6), cp_genetic_times, cp_genetic_scores)\n    train_and_time(lambda : mlrose.mimic(pop_size = 30, problem=continuous_peaks, max_attempts=6), cp_mimic_times, cp_mimic_scores)\n    \npoint_range = range(10,100,10)\nfor p in point_range:\n    run_continuous_peaks(p)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.plot(point_range,cp_hill_times,'o-',point_range,cp_anneal_times,'o-',point_range,cp_genetic_times,'o-')\nplt.title(\"Continuous Peaks Problem\")\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Time\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()\n\nplt.plot(point_range,cp_hill_scores,'o-',point_range,cp_anneal_scores,'o-', point_range,cp_genetic_scores,'o-',point_range,cp_mimic_scores,'o-')\nplt.title(\"Continuous Peaks Problem\")\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Highest Score\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cp_anneal_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####### KNAPSACK\nhill_times = []\nhill_scores = []\nanneal_times = []\nanneal_scores = []\ngenetic_times = []\ngenetic_scores = []\nmimic_times = []\nmimic_scores = []\n\ndef run_knapsack(num_points):\n    weights = list(np.random.randint(low = 1, high = 100, size = num_points))\n    values = list(np.random.randint(low = 1, high = 100, size = num_points))\n    knapsack = mlrose.DiscreteOpt(length = num_points,fitness_fn = mlrose.Knapsack(weights,values,0.2))\n    print (\"Knapsack Problem with \", num_points, \" points\")\n    train_and_time(lambda : mlrose.random_hill_climb(problem=knapsack, max_attempts=10),hill_times, hill_scores)\n    train_and_time(lambda : mlrose.simulated_annealing(problem=knapsack, max_attempts=10), anneal_times, anneal_scores)\n    train_and_time(lambda : mlrose.genetic_alg(problem=knapsack, max_attempts=1), genetic_times, genetic_scores)\n    train_and_time(lambda : mlrose.mimic(problem=knapsack, max_attempts=10, keep_pct=.20, fast_mimic=True), mimic_times, mimic_scores)\n    \npoint_range = range(10,100,20)\nfor p in point_range:\n    run_knapsack(p)\nplt.rcParams[\"figure.figsize\"] = (5,5)\n\nplt.plot(point_range,hill_times,'o-',point_range,anneal_times,'o-',point_range,genetic_times,'o-',point_range,mimic_times,'o-')\nplt.title(\"Knapsack Problem Times\")\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Time\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()\n\nplt.plot(point_range,hill_scores,'o-',point_range,anneal_scores,'o-', point_range,genetic_scores,'o-',point_range,mimic_scores,'o-')\nplt.title(\"Knapsack Problem Scores\")\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Highest Score\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nc_hill_times = []\nc_hill_scores = []\nc_anneal_times = []\nc_anneal_scores = []\nc_genetic_times = []\nc_genetic_scores = []\nc_mimic_times = []\nc_mimic_scores = []\n\ndef run_flip_flop(num_points):\n    flip_flop = mlrose.DiscreteOpt(length = num_points, fitness_fn = mlrose.FlipFlop(), max_val=2)\n    print (\"Flip flop Problem with \", num_points, \" points\")\n    train_and_time(lambda : mlrose.random_hill_climb(problem=flip_flop),c_hill_times, c_hill_scores)\n    train_and_time(lambda : mlrose.simulated_annealing(problem=flip_flop), c_anneal_times, c_anneal_scores)\n    train_and_time(lambda : mlrose.genetic_alg(problem=flip_flop), c_genetic_times, c_genetic_scores)\n    train_and_time(lambda : mlrose.mimic(problem=flip_flop, max_iters=100), c_mimic_times, c_mimic_scores)\n    \npoint_range = range(10,100,10)\nfor p in point_range:\n    run_flip_flop(p)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.plot(point_range,c_hill_times,'o-',point_range,c_anneal_times,'o-',point_range,c_genetic_times,'o-', point_range,c_mimic_times,'o-')\nplt.title(\"Flip flop Problem\")\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Time\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()\n\nplt.plot(point_range,c_hill_scores,'o-',point_range,c_anneal_scores,'o-', point_range,c_genetic_scores,'o-', point_range,c_mimic_times,'o-')\nplt.title(\"Flip Flop Problem\")\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Highest Score\")\nplt.legend([\"hill climb\", \"simulated annealing\",\"genetic algorithm\",\"mimic\"])\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}