{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import ensemble\nfrom sklearn import svm\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport math\nimport matplotlib.ticker as plticker\nimport matplotlib.patches as mpatches\nimport matplotlib.lines as mlines\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics \nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndata_frames = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        data_frames.append(pd.read_csv(file_path))\nbig_frame = pd.concat(data_frames, ignore_index=True)\nbig_frame.info()\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in big_frame:\n#     print (\"col is \", col, \" unique values:  \",big_frame[col].unique())\n#     print (\"#####\")\n#eval_df = pd.DataFrame(data={\"model\":[], \"recall\":[], \"accuracy\":[], \"precision\":[], \"training_time\": []})\neval_df = pd.DataFrame(columns=[\"model\", \"recall\", \"accuracy\", \"precision\"])\nbig_frame = big_frame.drop(columns=[\"tourney_id\",\"tourney_name\", \"draw_size\", \"tourney_date\", \"match_num\", \"winner_id\", \"winner_seed\", \"tourney_level\", \"winner_ioc\",\\\n                                    \"winner_rank_points\", \"winner_entry\",\"winner_name\", \"loser_id\", \"loser_seed\", \"loser_rank_points\", \"loser_entry\", \"loser_ioc\",\\\n                                    \"score\", \"round\", \"loser_name\", \"minutes\",\"surface\", \"winner_hand\", \"winner_ht\", \"winner_age\", \"winner_rank\", \"loser_hand\", \"loser_ht\",\\\n                                    \"loser_age\", \"loser_rank\" ])\nbig_frame = big_frame.dropna()\nbig_frame.info()\n#big_frame[\"surface\"].value_counts()\n# filtered_df = big_frame[(big_frame.surface == \"Hard\")] # | (big_frame.surface == \"Clay\" )] #\"Clay\", \"Grass\", \"Carpet\"])]\nfiltered_df = big_frame[(big_frame.best_of == 3)]\nbig_frame.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"filtered_df[\"best_of\"].value_counts()\nw\nw_ace        44451 non-null float64\nw_df         44451 non-null float64\nw_svpt       44451 non-null float64\nw_1stIn      44451 non-null float64\nw_1stWon     44451 non-null float64\nw_2ndWon     44451 non-null float64\nw_SvGms      44451 non-null float64\nw_bpSaved    44451 non-null float64\nw_bpFaced    44451 non-null float64\nl_ace        44451 non-null float64\nl_df         44451 non-null float64\nl_svpt       44451 non-null float64\nl_1stIn      44451 non-null float64\nl_1stWon     44451 non-null float64\nl_2ndWon     44451 non-null float64\nl_SvGms      44451 non-null float64\nl_bpSaved    44451 non-null float64\nl_bpFaced    44451 non-null float64\n\"\"\"\nlost_df = filtered_df[['l_ace', 'l_df', 'l_svpt','l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms', 'l_bpSaved', 'l_bpFaced']]\nwon_df = filtered_df[['w_ace', 'w_df', 'w_svpt', 'w_1stIn', 'w_1stWon', 'w_2ndWon','w_SvGms', 'w_bpSaved', 'w_bpFaced']]\nlost_df = lost_df.rename(columns={'l_ace':'ace', 'l_df':'df', 'l_svpt':'svpt','l_1stIn':'1stIn', 'l_1stWon':'1stWon', 'l_2ndWon':'2ndWon', \\\n                                  'l_SvGms':'SvGms', 'l_bpSaved':'bpSaved', 'l_bpFaced':'bpFaced'})\nwon_df = won_df.rename(columns={'w_ace':'ace', 'w_df':'df', 'w_svpt':'svpt','w_1stIn':'1stIn', 'w_1stWon':'1stWon', 'w_2ndWon':'2ndWon',\\\n                                'w_SvGms':'SvGms', 'w_bpSaved':'bpSaved', 'w_bpFaced':'bpFaced'})\n\nall_players = pd.concat(data_frames, ignore_index=True)\nwon_df[\"won\"] = won_df['svpt']**0\nlost_df[\"won\"] = lost_df['svpt']*0\n\nbig_df = pd.concat([won_df, lost_df], ignore_index=True)\nbig_df = big_df.sample(frac=0.1, random_state=1)\nbig_df = big_df.drop(columns=['svpt'])\nbig_df.info\n# lost_df[\"ace_pct\"] = filtered_df.apply(lambda x: x.l_ace / x.l_svpt)\n# won_df[\"ace_pct\"] = filtered_df.apply(lambda x: x.w_ace / x.w_svpt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# util functions\n\ndef eval_for_conclusion(model_id, clf, test_x, test_y):\n    y_pred = clf.predict(test_x)\n    print(classification_report(test_y, y_pred))\n    print(confusion_matrix(test_y, y_pred))\n    accuracy = metrics.accuracy_score(test_y, y_pred)\n    precision = metrics.precision_score(test_y, y_pred)\n    recall = metrics.recall_score(test_y, y_pred)\n    print(\"Final {0} model accuracy:\".format(model_id), accuracy)\n    print(\"Final {0} model precision:\".format(model_id), precision) \n    print(\"Final {0} model recall:\".format(model_id), recall) \n    return {\"model\":model_id, \"recall\":recall, \"accuracy\":accuracy, \"precision\":precision}\n\ndef split_test_train(train_size, all_data):\n    msk = np.random.rand(len(all_data)) < train_size\n    train_df = all_data[msk]\n    test_df = all_data[~msk]\n    train_y = train_df[\"won\"]\n    train_x = train_df.drop(\"won\", axis=1)\n    test_y = test_df[\"won\"]\n    test_x  = test_df.drop(\"won\", axis=1)\n    return (train_x, train_y, test_x, test_y)\n\ndef cross_validate(all_data, model):\n    depth = []\n    all_y = all_data[\"won\"]\n    all_x  = all_data.drop(\"won\", axis=1)\n    for i in range(2,10):\n        # Perform n-fold cross validation \n        scores = cross_val_score(estimator=model, X=all_x, y=all_y, cv=i, n_jobs=4)\n        # print(\"i scores for cv: \", scores)\n        depth.append((i,scores.mean()))\n    # print(depth)\n    return depth\n    \ndef train_and_test(all_data, model):\n    test_scores = []\n    train_scores = []\n    times = []\n    for i in range(1,10):\n        (train_x, train_y, test_x, test_y) = split_test_train(0.1 * i, big_df)\n        #print(\"len test: \", len(test_x), \", len train: \", len(train_x))\n        start = time.time()\n        #TODO iterations\n        model.fit(train_x, train_y)\n        end = time.time()\n        times.append(end - start)\n        pred_test_y = model.predict(test_x) # TODO add wallclock time\n        test_score = round(model.score(test_x, test_y) * 100, 2)\n        pred_train_y = model.predict(train_x)\n        train_score = round(model.score(train_x, train_y) * 100, 2)\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n    return (test_scores, train_scores, times)\n\ndef plot_data(x_vars, x_label, all_y_vars, y_var_labels, y_label, title, y_bounds=None):\n    colors = ['red','orange','black','green','blue','violet']\n    plt.rcParams[\"figure.figsize\"] = (4,3)\n\n    i = 0\n    for y_var in all_y_vars:\n#         if i == 2: # don't plot when i = 1 for cv\n#             x_vars = x_vars[1:]\n        plt.plot(x_vars, y_var, 'o-', color=colors[i % 6], label=y_var_labels[i])\n        i += 1\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    if y_bounds != None:\n        plt.ylim(y_bounds)\n    leg = plt.legend();\n    plt.show()\n\ndef evaluate_model(all_data, model, model_id):\n    (test_scores, train_scores, times) = train_and_test(all_data, model)\n    cv_scores = cross_validate(all_data, model)\n    print(\"{0} train timings (seconds): {1}\".format(model_id, times))\n    print(\"{0} test set scores: {1} \".format(model_id, test_scores))\n    print(\"{0} train set scores: {1}\".format(model_id, train_scores))\n    print(\"{0} cross validation set scores: {1}\".format(model_id, cv_scores))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [test_scores, train_scores],\\\n              [\"test_scores\", \"train_scores\"], \"Accuracy\", \"{0} Accuracy Over Train/Test Split\".format(model_id), (50,103))\n    plot_data([x[0] for x in cv_scores], \"Number of folds\", [[x[1] for x in cv_scores]],\n             [\"cross_validation_accuracy\"], \"Accuracy\", \"{0} Accuracy Over Different Cross Validation Values of K\".format(model_id), (0.3,1))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [times],\n             [\"times\"], \"Train time in Seconds\", \"{0} Time Spent Training Over Train/Test Split\".format(model_id))\n    return (test_scores, train_scores, times, cv_scores)\n\ndef plot_grid_search(grid_results, plotting_func, title, x_label, y_label, grid_size, model_handles):\n    plt.rcParams[\"figure.figsize\"] = grid_size\n    means = grid_results.cv_results_['mean_test_score']\n    stds = grid_results.cv_results_['std_test_score']\n    params = grid_results.cv_results_['params']\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.subplots\n    ax = plt.subplot()\n    for mean, std, params in zip(means, stds, params):\n        #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n        plotting_func(mean, params, plt, ax)\n    if handles: plt.legend(handles=model_handles)\n    plt.show()\n\n\n#def grid_search(model, params, x_train, y_train, x_test, y_test):\n    \n\n#TODO come up with graphing function that takes in two arrays of test and train and plots them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000, random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(big_df, neural_net_classifier, \"Tennis NeuralNet Baseline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n#     'activation': ['tanh', 'relu'],\n#     'solver': ['sgd', 'adam'],\n#     'learning_rate': ['constant','adaptive'],\nparameter_space = {\n    'activation': ['tanh', 'relu'],\n    'alpha': [10 ** i for i in range (-1, -8, -1)],\n    \"hidden_layer_sizes\": [(5,5), (10,10), (15,15)]\n    \n}\nnn_clf = GridSearchCV(neural_net_classifier, parameter_space, n_jobs=-1, cv=3)\n(nn_train_x, nn_train_y, nn_test_x, nn_test_y) = split_test_train(0.1 * 8, big_df)\nscaler = StandardScaler()\nscaler.fit(nn_train_x)\nnn_train_x = scaler.transform(nn_train_x)\nnn_test_x = scaler.transform(nn_test_x)\n\n\nnn_clf.fit(nn_train_x, nn_train_y)\neval_for_conclusion(\"Tennis Neural Network\", nn_clf, nn_test_x, nn_test_y)\n# Best parameters set\nprint('Best parameters found:\\n', nn_clf.best_params_, \"with score of: \", max(nn_clf.cv_results_['mean_test_score']))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pl\ndef plotting_func_nn(mean, params, plt, ax):\n    x_var = \"alpha\"\n    tick_spacing = 0.1\n    layer_colors = {\"(5, 5)\":\"orange\", \"(10, 10)\":\"red\", \"(15, 15)\":\"black\"}\n    activation_labels = {\"tanh\": \"o\", \"relu\":\"s\"}\n    #print(params[\"hidden_layer_sizes\"])\n    layer_color_idx = str(params[\"hidden_layer_sizes\"])\n    activation_idx = params[\"activation\"]\n    ax.plot(math.log(params[x_var],10), mean, activation_labels[activation_idx], color=layer_colors[layer_color_idx])\n    x_loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n    ax.xaxis.set_major_locator(x_loc)\n    y_loc = plticker.MultipleLocator(base=0.005) # this locator puts ticks at regular intervals\n    ax.yaxis.set_major_locator(y_loc)\nred_patch = mpatches.Patch(color='orange', label='(5, 5) layer')\norange_patch = mpatches.Patch(color='red', label='(10, 10) layer')\nblack_patch = mpatches.Patch(color='black', label='(15, 15) layer')\n\n\ntanh = mlines.Line2D([], [],marker='o',\n                         label='tanh')\nrelu = mlines.Line2D([], [],marker='s',\n                         label='relu')\nhandles = [red_patch,orange_patch,black_patch,tanh, relu]\n\nplot_grid_search(nn_clf, plotting_func_nn, \"Tennis NN accuracy as a function of log(alpha Param)\", \"log(alpha)\", \"accuracy\",(5,7), handles)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Decision Tree\n# dt_model = DecisionTreeClassifier()\n# (dt_test_scores, dt_train_scores, dt_times, dt_cv_scores) = evaluate_model(big_df, dt_model, \"Decision Tree\")\n# # choose best test split and k fold value\n# optimal_test_split = dt_test_scores.index(max(dt_test_scores)) * 0.1\n# print(\"max index for means wsa: \", optimal_test_split * 10)\n# (dt_grid_train_x, dt_grid_train_y, dt_grid_test_x, dt_grid_test_y) = split_test_train(optimal_test_split, big_df)\n# dt_param_grid = {\"criterion\":[\"gini\",\"entropy\"], \"max_depth\":[5, 10, 15, 30], \"splitter\":[\"best\", \"random\"]}\n# dt_grid_results = GridSearchCV(dt_model, dt_param_grid, cv=5).fit(dt_grid_train_x, dt_grid_train_y)\n# dt_best_params = dt_grid_results.best_params_\n# grid_dt_model = DecisionTreeClassifier(\n#     criterion=dt_best_params[\"criterion\"], max_depth=dt_best_params[\"max_depth\"])\n\n# grid_dt_model.fit(dt_grid_train_x, dt_grid_train_y)\n\n# pred_test_y = grid_dt_model.predict(dt_grid_test_x) #TODO add wallclock time\n# test_score = round(grid_dt_model.score(dt_grid_test_x, dt_grid_test_y) * 100, 2)\n# print(\"test score for decision tree model: \", test_score, dt_best_params)\n# #get other interesting information from the model\n\ndt_model = DecisionTreeClassifier()\n(dt_test_scores, dt_train_scores, dt_times, dt_cv_scores) = evaluate_model(big_df, dt_model, \"Tennis Decision Tree\")\n# choose best test split and k fold value\noptimal_test_split = dt_test_scores.index(max(dt_test_scores)) * 0.1\nprint(\"max index for means was: \", optimal_test_split * 10)\n(dt_grid_train_x, dt_grid_train_y, dt_grid_test_x, dt_grid_test_y) = split_test_train(optimal_test_split, big_df)\n#or\ndt_param_grid = {\"criterion\":[\"gini\",\"entropy\"], \"max_depth\":[x for x in range(2,31)], \"min_samples_split\":[3,5,7]}  #\"splitter\":[\"best\", \"random\"], \n# max depth below 6 didnt work for this dataset since the data is more complex\ndt_grid_results = GridSearchCV(dt_model, dt_param_grid, cv=5).fit(dt_grid_train_x, dt_grid_train_y)\n# dt_means = dt_grid_results.cv_results_['mean_test_score']\n# dt_stds = dt_grid_results.cv_results_['std_test_score']\n# dt_params = dt_grid_results.cv_results_['params']\n\n# max_depths = []\n# accuracies = []\n# plt.xlabel(\"Max Depth of Decision Tree\")\n# plt.ylabel(\"Accuracy\")\n# plt.title(\"Tennis Decision Tree Grid Search, Accuracy as a Function of Max_depth\")\n# for mean, std, params in zip(dt_means, dt_stds, dt_params):\n#     print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n#     accuracies.append(mean)\n#     max_depths.append(params[\"max_depth\"])\n# plt.plot(max_depths, accuracies, 'o', color=\"red\")\n# plt.show()\n# print(\"best params \", dt_grid_results.best_params_)\n# print(\"\\nClassification results for decision tree model on test set:\")\n# dt_y_pred = dt_grid_results.predict(dt_grid_test_x)\n# print(classification_report(dt_grid_test_y, dt_y_pred))\n# print(confusion_matrix(dt_grid_test_y, dt_grid_results.predict(dt_grid_test_x)))\n# print(\"DT model accuracy:\", metrics.accuracy_score(dt_grid_test_y, dt_grid_results.predict(dt_grid_test_x))) \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_func_dt(mean, params, plt, ax):\n    x_var = \"max_depth\"\n    #print(params[\"hidden_layer_sizes\"])\n    color_map = {\"gini\":\"b\", \"entropy\":\"red\"}\n    ax.plot(params[x_var], mean, \"o\", color=color_map[params[\"criterion\"]])\n\nblue_patch = mpatches.Patch(color='blue', label='gini')\nred_patch = mpatches.Patch(color='red', label='entropy')\nhandles = [blue_patch, red_patch]\nplot_grid_search(dt_grid_results, plotting_func_dt, \"Tennis Decision Tree Training Accuracy as a Function of Max_depth\", \"max_depth\", \"accuracy\",(6,4), handles)\n\neval_for_conclusion(\"Tennis Decision Tree\", dt_grid_results, dt_grid_test_x, dt_grid_test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(dt_grid_test_y, dt_grid_results.predict(dt_grid_test_x)))\nprint(\"DT model accuracy:\", metrics.accuracy_score(dt_grid_test_y, dt_grid_results.predict(dt_grid_test_x))) \nprint(\"DT model precision:\", metrics.precision_score(dt_grid_test_y, dt_grid_results.predict(dt_grid_test_x))) \nprint(\"DT model recall:\", metrics.recall_score(dt_grid_test_y, dt_grid_results.predict(dt_grid_test_x))) \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_classifier = KNeighborsClassifier()\nevaluate_model(big_df, knn_classifier, \"Tennis knn baseline\")\nknn_param_grid = {\"n_neighbors\":[x for x in range (2,21)] + [y*10 for y in range(3,11)]}\n(knn_grid_train_x, knn_grid_train_y, knn_grid_test_x, knn_grid_test_y) = split_test_train(0.8, big_df)\nknn_grid_results = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5).fit(knn_grid_train_x, knn_grid_train_y)\n\n# All results\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Tennis K nearest neighbor Grid Search, Accuracy as a Function of K\")\nknn_means = knn_grid_results.cv_results_['mean_test_score']\nknn_stds = knn_grid_results.cv_results_['std_test_score']\nknn_params = knn_grid_results.cv_results_['params']\nk_vals = []\nknn_accuracies = []\nfor mean, std, params in zip(knn_means, knn_stds, knn_params):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n    knn_accuracies.append(mean)\n    k_vals.append(params[\"n_neighbors\"])\nprint('Best parameters found:\\n', knn_grid_results.best_params_, \"with score of: \", max(knn_grid_results.cv_results_['mean_test_score']))\n\n    \neval_for_conclusion(\"Tennis KNN\", knn_grid_results, knn_grid_test_x, knn_grid_test_y)\n\n    \nplt.plot(k_vals, knn_accuracies, 'o', color=\"red\")\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_classifier = svm.SVC()\nevaluate_model(big_df, svm_classifier, \"Tennis svm_classifier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_parameter_space = {\n    'kernel': ['linear', 'rbf'],\n    'C': [ float(i) / 100 for i in range (1, 300, 5)]+[5,7,9],    \n}\nsvm_grid_clf = GridSearchCV(svm_classifier, svm_parameter_space, n_jobs=-1, cv=3)\n(svm_train_x, svm_train_y, svm_test_x, svm_test_y) = split_test_train(0.1 * 8, big_df)\nscaler = StandardScaler()\nscaler.fit(svm_train_x)\nsvm_train_x = scaler.transform(svm_train_x)\nsvm_test_x = scaler.transform(svm_test_x)\nsvm_grid_clf.fit(svm_train_x, svm_train_y)\n\nprint(\"best params \", svm_grid_clf.best_params_)\nsvm_grid_score = svm_grid_clf.score(svm_test_x,svm_test_y)\nprint(\"SVM grid search model test set score: \", svm_grid_score)\nprint('Best SVM parameters found through cv:\\n', svm_grid_clf.best_params_, \"with score of: \", max(svm_grid_clf.cv_results_['mean_test_score']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_func_svm(mean, params, plt, ax):\n    x_var = \"C\"\n    tick_spacing = 0.1\n    if mean < 0.7: return #don't need to show too many outliers to save space\n    #layer_colors = {\"(3,)\":\"orange\", \"(5,)\":\"red\", \"(7,)\":\"black\"}\n    kernel_labels = {\"linear\": \"o\", \"rbf\":\"o\"}\n    kernel_colors = {\"linear\": \"red\", \"rbf\":\"black\"}\n    #print(params[\"hidden_layer_sizes\"])\n    #layer_color_idx = str(params[\"hidden_layer_sizes\"])\n    kernel_idx = params[\"kernel\"]\n    ax.plot(params[x_var], mean, kernel_labels[kernel_idx], color=kernel_colors[kernel_idx]) #, color=layer_colors[layer_color_idx])\n#     x_loc = plticker.MultipleLocator(base=0.5) # this locator puts ticks at regular intervals\n#     ax.xaxis.set_major_locator(x_loc)\n\n    y_loc = plticker.MultipleLocator(base=0.005) # this locator puts ticks at regular intervals\n    ax.yaxis.set_major_locator(y_loc)\n    #plt.ylim(.76, .80)\n\nlinear = mlines.Line2D([], [],marker='o',\n                         label='linear', color=\"r\")\nrbf = mlines.Line2D([], [],marker='o',\n                         label='rbf', color=\"black\")\nhandles = [linear, rbf]#, red_patch, orange_patch, black_patch]\n\nplot_grid_search(svm_grid_clf, plotting_func_svm, \"Tennis SVM accuracy as a function of C\", \"C\", \"accuracy\",(5,8), handles)\neval_for_conclusion(\"Tennis SVM\", svm_grid_clf, svm_test_x, svm_test_y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_classifier = AdaBoostClassifier()\nevaluate_model(big_df, boost_classifier, \"Tennis Boosting_classifier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_parameter_space = {\n    'n_estimators': [i*10 for i in range(5,11)],\n    'learning_rate': [ float(i) / 100 for i in range (1, 150, 10)]\n}\nboost_grid_clf = GridSearchCV(boost_classifier, boost_parameter_space, n_jobs=-1, cv=3)\n(boost_train_x, boost_train_y, boost_test_x, boost_test_y) = split_test_train(0.1 * 8, big_df)\nscaler = StandardScaler()\nscaler.fit(boost_train_x)\nboost_train_x = scaler.transform(boost_train_x)\nboost_test_x = scaler.transform(boost_test_x)\nboost_grid_clf.fit(boost_train_x, boost_train_y)\n\nprint(\"best Boost params \", boost_grid_clf.best_params_)\nboost_grid_score = boost_grid_clf.score(boost_test_x,boost_test_y)\nprint(\"Bost grid search model test set score: \", boost_grid_score)\nprint('Best Boost parameters found through cv:\\n', boost_grid_clf.best_params_, \"with score of: \", max(boost_grid_clf.cv_results_['mean_test_score']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_func_boost(mean, params, plt, ax):\n    x_var = \"learning_rate\"\n    cmap = cm.hot\n    norm = Normalize(vmin=-110, vmax=-20)\n    ax.plot(params[x_var], mean,\"o\", color=cmap(norm(-1*params[\"n_estimators\"])))\n    #plt.ylim(.70, .80)\n    #plt.xlim(0, 1.8)\n\ncmap = cm.hot\nnorm = Normalize(vmin=-110, vmax=-20)\nyellow_patch = mpatches.Patch(color=cmap(norm(-50)), label='n_estimators=50')\nred_patch = mpatches.Patch(color=cmap(norm(-80)), label='n_estimators=80')\nblack_patch = mpatches.Patch(color=cmap(norm(-110)), label='n_estimators=110')\nhandles = [yellow_patch, red_patch, black_patch] #[linear, rbf]#, red_patch, orange_patch, black_patch]\n\n\nplot_grid_search(boost_grid_clf, plotting_func_boost, \"Tennis Boosting accuracy as a function of learning rate\", \"learning rate\", \"accuracy\",(5,8), handles)\n\neval_for_conclusion(\"Tennis Boosting\", boost_grid_clf, boost_test_x, boost_test_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\n\ncmap = cm.autumn\nnorm = Normalize(vmin=-20, vmax=10)\nprint(cmap(norm(5)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df = pd.DataFrame(columns=[\"model\", \"recall\", \"accuracy\", \"precision\"])\neval_df = eval_df.append(eval_for_conclusion(\"Tennis DT\", dt_grid_results, dt_grid_test_x, dt_grid_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Tennis Boosting\", boost_grid_clf, boost_test_x, boost_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Tennis SVM\", svm_grid_clf, svm_test_x, svm_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Tennis KNN\", knn_grid_results, knn_grid_test_x, knn_grid_test_y), ignore_index=True)\neval_df = eval_df.append(eval_for_conclusion(\"Tennis Neural Network\", nn_clf, nn_test_x, nn_test_y), ignore_index=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df.head()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}