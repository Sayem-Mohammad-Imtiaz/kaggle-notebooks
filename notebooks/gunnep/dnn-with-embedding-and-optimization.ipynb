{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataframe_1 = pd.read_csv(\"../input/deepnlp/Sheet_1.csv\")\ndataframe_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"mapping_flag = {'flagged': 1, 'not_flagged': 0}\n\ndataframe_1 = dataframe_1.replace({'class': mapping_flag})\n\nresponse = dataframe_1['response_text']\nflag = dataframe_1['class']\n\nprint(flag)\nprint(response)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#vectorize the sentences\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(min_df=0, lowercase=False)\nvectorizer.fit(response)\nvectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to use now basic NLP tools\nimport nltk\nprint(nltk.__version__)\nfrom nltk.corpus import wordnet\n\nword_ = wordnet.synsets(\"seem\")[0]\nword_ = np.asarray(word_)\n\nprint(word_)\n\nfor i in response:\n    word = 'Good'.lower()\n    if word in i:\n        print('Seems a good review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_words = ['awesome','good','nice','fun']\nnegative_words = ['awful','lame','horrible','bad'] \n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\n\nstop_words = set(stopwords.words('english'))\nprint(response[0])\n\nresponse_words = word_tokenize(response[0])\nresponse_words = [w for w in response_words if not w in stop_words]\n\nprint(response_words)\n\nfor i in response:\n    i = i.lower()\n    i = i.replace('!','')\n    i = i.replace('?','')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from textblob.classifiers import NaiveBayesClassifier\nfrom textblob import TextBlob\n\ntrain = [('I love this sandwich.', 'pos'),\n         ('This is an amazing place!', 'pos'),\n         ('I feel very good about these beers.', 'pos'),\n         ('This is my best work.', 'pos'),\n         (\"What an awesome view\", 'pos'),\n         ('I do not like this restaurant', 'neg'),\n         ('I am tired of this stuff.', 'neg'),\n         (\"I can't deal with this\", 'neg'),\n         ('He is my sworn enemy!', 'neg'),\n         ('My boss is horrible.', 'neg') ]\n\nmodel = NaiveBayesClassifier(train)\n\nfor i in range(0,len(response)):\n    print(model.classify(response[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras \nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.model_selection import train_test_split\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(response, flag, test_size=0.25, random_state=1000)\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's try to build a DNN for text processing\n#X_train = vectorizer.transform(sentences_train)\n#X_test  = vectorizer.transform(sentences_test)\n\nfrom keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nprint(accuracy)\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optimization in the hyperparameter space (credits: https://realpython.com/python-keras-text-classification/)\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Main settings\nepochs = 20\nembedding_dim = 50\nmaxlen = 100\noutput_file = 'data/output.txt'\n\ndef create_model_embedding(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n    model = Sequential()\n    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(10, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\ndef create_model_LSTM(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n    model = Sequential()\n    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(10, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# Run grid search for each source (yelp, amazon, imdb)\nfor source, frame in dataframe_1.groupby('class'):\n    print('Running grid search for data set :', source)\n\n    # Train-test split\n    sentences_train, sentences_test, y_train, y_test = train_test_split(\n        response, flag, test_size=0.25, random_state=1000)\n\n    # Tokenize words\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(sentences_train)\n    X_train = tokenizer.texts_to_sequences(sentences_train)\n    X_test = tokenizer.texts_to_sequences(sentences_test)\n\n    # Adding 1 because of reserved 0 index\n    vocab_size = len(tokenizer.word_index) + 1\n\n    # Pad sequences with zeros\n    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\n    # Parameter grid for grid search\n    param_grid = dict(num_filters=[32, 64, 128],\n                      kernel_size=[3, 5, 7],\n                      vocab_size=[vocab_size],\n                      embedding_dim=[embedding_dim],\n                      maxlen=[maxlen])\n    model = KerasClassifier(build_fn=create_model,\n                            epochs=epochs, batch_size=10,\n                            verbose=False)\n    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n                              cv=4, verbose=1, n_iter=5)\n    grid_result = grid.fit(X_train, y_train)\n\n    # Evaluate testing set\n    test_accuracy = grid.score(X_test, y_test)\n\n    # Save and evaluate results\n    #prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n    #if prompt.lower() not in {'y', 'true', 'yes'}:\n    #    break\n    #with open(output_file, 'a') as f:\n    s = ('Running {} data set\\nBest Accuracy : '\n         '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n    output_string = s.format(\n                source,\n            grid_result.best_score_,\n            grid_result.best_params_,\n            test_accuracy)\n    print(output_string)\n    #f.write(output_string)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}