{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>Zero Shot Classification</h1></center>\n<br>\n<center>Zero-shot learning is a problem setup in machine learning, where at test time, a learner observes samples from classes that were not observed during training, and needs to predict the category they belong to. This problem is widely studied in computer vision, natural language processing and machine perception.</center>\n\n<center><img src = https://miro.medium.com/max/576/1*7i5LhQ33_EdxMaPu3iteQg@2x.png></center>\n\n<center><h4>I will be using the HuggingFace Python package for predicting question tags for this StackOverflow dataset. I'm just a beginner with this so please feel free to comment if I can do something better. As always lets start with a meme.</h4></center>\n\n<br>\n<center><img src = https://miro.medium.com/max/450/1*SVWSZ-lOCcQNHk6r8apCbA.png></center>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from transformers import pipeline # HuggingFace Transformers Package\nimport pandas as pd\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/60k-stack-overflow-questions-with-quality-rate/data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The dataset is very stratghtforward. We are only interested in \n- Title\n- Body\n- Tags"},{"metadata":{},"cell_type":"markdown","source":"## Initializing the classifier\n\nIt takes the model type and device as input \n- device = -1 (CPU) [This will take atleast 2 hours for 100 rows of this dataset]\n- device = 0 (GPU) [So much faster]"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"classifier = pipeline(\"zero-shot-classification\",device = 0) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Minor Preprocessing\nEach tag is inside '<>' so we slice from index 1 to second last and split by '><' to get a list of tags. Similarly slicing the body. The length of the body fields is too large for 16 GB of RAM that Kaggle provides so we will be classifying based on the title only for now. Stay tuned for updates.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Tags'] = df['Tags'].apply(lambda x: x[1:-1].split('><'))\ndf['Body'] = df['Body'].apply(lambda x: x[3:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting the unique labels from the first 100 Rows \n\nlabels = []\nfor i in range(200):\n    labels.extend(df.iloc[i,]['Tags'])\nlabels = set(labels)\nall_labels = list(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most Common Tags in first 200 samples"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ncloud = ''\nfor i in all_labels:\n    cloud += i + ' '\n\nplt.subplots(figsize = (8,8))\n\nwordcloud = WordCloud (\n                    background_color = 'white',\n                    width = 1024,\n                    height = 1024\n                        ).generate(cloud)\nplt.imshow(wordcloud) # image show\nplt.axis('off') # to off the axis of x and y\nplt.savefig('Plotly-World_Cloud.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = []\ny = []\nfor i in tqdm(range(200)):\n    titles = df.iloc[i,]['Title']\n    tags = df.iloc[i,]['Tags']\n    op = classifier(titles, all_labels, multi_class=True)\n    labels = op['labels'] \n    scores = op['scores']\n    res_dict = {label : score for label,score in zip(labels, scores)}\n    sorted_dict = dict(sorted(res_dict.items(), key=lambda x:x[1],reverse = True)) #sorting the dictionary of labels in descending order based on their score\n    categories = []\n    for i, (k,v) in enumerate(sorted_dict.items()):\n        if(i > 3): #storing only the best 4 predictions\n            break\n        else:\n            categories.append(k)\n    y.append(tags)\n    y_pred.append(categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = pd.DataFrame(list(zip(y, y_pred)), columns =['Labels', 'Predicted_Labels']) \nout.to_csv('output.csv')\nout.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is evident that the Zero Shot Learner is able to predict tags with respectable accuracy. Now let's measure the performance by encoding the labels using Hamming Loss form SKLearn and also implement it from scratch."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_idx = {cat : i for i,cat in enumerate(all_labels)}  # Map of category and it's index to encode the o/p for evaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_trueEncoded = []\ny_predEncoded = []\nfor y_true, y_pred in zip(y, y_pred):\n    encTrue = [0] * len(all_labels)\n    for cat in y_true:\n        idx = cat_idx[cat]\n        encTrue[idx] = 1\n    y_trueEncoded.append(encTrue)\n    encPred = [0] * len(all_labels)\n    for cat in y_pred:\n        idx = cat_idx[cat]\n        encPred[idx] = 1\n    y_predEncoded.append(encPred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Hamming loss is the fraction of labels that are incorrectly predicted.\n\n\n[Refer this for Hamming Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nprint('Hamming Loss =', hamming_loss(y_trueEncoded,y_predEncoded))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Hamming loss from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = 0\nfor x, y in zip(y_trueEncoded,y_predEncoded):\n    temp = 0\n    for i in range(len(x)):\n        if x[i] == y[i]:\n            temp += 1\n    temp /= len(x)\n    loss += temp\nloss /= len(y_trueEncoded)\nprint('Hamming Loss =', 1 - loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\n- [1] [Hugging Face Github](https://github.com/huggingface/transformers)\n- [2] [Zero Shot Classification Pipeling](https://huggingface.co/transformers/main_classes/pipelines.html#zeroshotclassificationpipeline)\n- [3] [Kernel By Aayush Jain](https://www.kaggle.com/foolofatook/zero-shot-classification-with-huggingface-pipeline)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}