{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv', parse_dates=['Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. **We need to explore the data in more details**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum() / len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's we could understand ? \n* We have 145460 entries and 23 columns\n* We have some NA values but not more than 50% of column's\n* In data have 6 object's type, other is float64, and 1 is datetime64","metadata":{}},{"cell_type":"markdown","source":"**First at all, let's convert our date columns to make more data about dates and split this column**","metadata":{}},{"cell_type":"code","source":"df['Year'] = df.Date.dt.year\ndf['Month'] = df.Date.dt.month\ndf['Day'] = df.Date.dt.day\ndf['Dayofweek'] = df.Date.dt.dayofweek\ndf['Dayofyear'] = df.Date.dt.dayofyear\n\n# Drop original columns Date\ndf.drop('Date', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore the data\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert strings (objects) to categories\n**We'll use API pandas**","metadata":{}},{"cell_type":"code","source":"# Check which columns have string:\n\nfor label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Okay, 6 objects have a string type**","metadata":{}},{"cell_type":"code","source":"# This will turn all of the string values into category values\n\nfor label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        df[label] = content.astype('category').cat.as_ordered()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploration\n\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, time for fill missing values\n* We must know, that all our data must be numerical\n* There can't be any missing values","metadata":{}},{"cell_type":"code","source":"for label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which columns have a null values\n\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill numeric rows with the median\n\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            df[label+'_is_missing'] = pd.isnull(content)\n            df[label] = content.fillna(content.median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check again\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum() / len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use dropna() for remove rows with NaN.\n\nThe big advantage is that we won't lose a lot of data.","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum() / len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, time for tune categorical values into numerical","metadata":{}},{"cell_type":"code","source":"for label, content in df.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        df[label+'_is_missing'] = pd.isnull(content)\n        df[label] = pd.Categorical(content).codes+1\n\n# Why codes+1 ? Because pandas encodes missing categories as -1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore our data\n\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's see correlation matrix","metadata":{}},{"cell_type":"code","source":"df_tmp = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\n\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(30,20))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                 linewidth=0.5,\n                 fmt='.2f',\n                 cmap='YlGnBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Modelling\n**We'll use:**\n* Logistic Regression\n* KNearest Neighbors\n* Decision Trees\n* Random Forest\n* Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"But, before we start, me must to do train_test_split for split our data on X and y","metadata":{}},{"cell_type":"code","source":"# Train_test_split\n\nfrom sklearn.model_selection import train_test_split\n\nfeatures = df.drop('RainTomorrow',axis=1)  # Our X set\ntarget = df['RainTomorrow']  # Our y set\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nprint(accuracy_score(y_test, lr_predict))\nprint(classification_report(y_test, lr_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_predict = knn.predict(X_test)\nprint(accuracy_score(y_test, knn_predict))\nprint(classification_report(y_test, knn_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=7)\ntree.fit(X_train, y_train)\ntree_predict = tree.predict(X_test)\nprint(accuracy_score(y_test, tree_predict))\nprint(classification_report(y_test, tree_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(random_state=7)\nforest.fit(X_train, y_train)\nforest_predict = forest.predict(X_test)\nprint(accuracy_score(y_test, forest_predict))\nprint(classification_report(y_test, forest_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\n\nbayes = GaussianNB()\nbayes.fit(X_train, y_train)\nbayes_predict = bayes.predict(X_test)\nprint(accuracy_score(y_test, bayes_predict))\nprint(classification_report(y_test, bayes_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost\n\nimport xgboost as xgb\n\nxgb = xgb.XGBClassifier()\nxgb.fit(X_train, y_train)\nxgb_predict = xgb.predict(X_test)\nprint(accuracy_score(y_test, xgb_predict))\nprint(classification_report(y_test, xgb_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_default_scores = {\n    'Logistic Regression' : lr.score(X_test, y_test),\n    'KNearest Neighbors' : knn.score(X_test, y_test),\n    'Decision Tree' : tree.score(X_test, y_test),\n    'Random Forest Classifier' : forest.score(X_test, y_test),\n    'Naive Bayes GNB' : bayes.score(X_test, y_test),\n    'XGBoost' : xgb.score(X_test, y_test)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_default_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"default_models_compare = pd.DataFrame(models_default_scores, index=['accuracy'])\ndefault_models_compare.T.plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning with RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"# Logistic Regression Grid\nlr_grid = {'C' : np.logspace(-4,4,20),\n           'solver' : ['liblinear', 'saga']}\n\n# Random Forest Classifier Grid\nforest_grid = {'n_estimators' : np.arange(10,600,10),\n               'max_depth' : np.arange(1,12,1),\n               'min_samples_leaf' : np.arange(2,14,2),\n               'min_samples_split' : np.arange(2,14,2)}\n\n# Decision Tree Grid\n\ntree_grid = {'max_depth' : np.arange(1,9,1),\n             'max_features' : np.arange(1,12,1),\n             'min_samples_leaf' : np.arange(1,9,1),\n             'criterion' : ['gini','entropy']}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tune Logistic Regression with RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nnp.random.seed(7)\n\nlr_cv = RandomizedSearchCV(LogisticRegression(),\n                           param_distributions=lr_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\nlr_cv.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cv.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cv.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_y_preds = lr_cv.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tune Random Forest Classifier with RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"np.random.seed(7)\n\nforest_cv = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1,\n                                                      max_samples=10000),\n                               param_distributions=forest_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\nforest_cv.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_cv.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_cv.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_y_preds = forest_cv.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tune Decision Tree Classifier with Randomized Search","metadata":{}},{"cell_type":"code","source":"np.random.seed(7)\n\ntree_cv = RandomizedSearchCV(DecisionTreeClassifier(),\n                             param_distributions=tree_grid,\n                             cv=5,\n                             n_iter=20,\n                             verbose=True)\n\ntree_cv.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_cv.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_y_preds = tree_cv.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_models_scores = {\n    'Logistic Regression' : lr_cv.score(X_test, y_test),\n    'KNearest Neighbors' : knn.score(X_test, y_test),\n    'Decision Tree' : tree_cv.score(X_test, y_test),\n    'Random Forest Classifier' : forest_cv.score(X_test, y_test),\n    'Naive Bayes GNB' : bayes.score(X_test, y_test),\n    'XGBoost' : xgb.score(X_test, y_test)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before\nmodels_default_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After\nupdated_models_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we can see, we could improve our Logistic Regression, Decision Tree and Random Forest Classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best of best model is XGBoost\n\nfrom sklearn import metrics\n\nmetrics.plot_roc_curve(xgb, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our AUC score is 0.90 ! It's good result.\n\nprint(metrics.confusion_matrix(y_test, xgb_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print classification report\n\nprint(classification_report(y_test, xgb_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finally, our model have 86% of accuracy, \n# this is a very good result. Please, if you like my work, you can rate it and leave a comment, I will be very pleased.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}