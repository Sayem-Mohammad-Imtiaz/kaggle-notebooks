{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Married at First Sight (Analysis)\n\n\nThe Lifetime reality television show and social experiment, Married at First Sight, features men and women who sign up to marry a complete stranger they've never met before. Experts pair couples based on tests and interviews. After marriage, couples have only a few short weeks together to decide if they want to stay married or get a divorce. There have been 10 full seasons so far which provides interesting data to look at what factors may or may not play a role in their decisions at the end of eight weeks as well as longer-term outcomes since the show aired.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the libraries in place, let us import the data and see how it looks!","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Data = pd.read_csv('../input/married-at-first-sight/mafs.csv')\nData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks good. Now let us see the dimensions and our variables in the data set!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.shape #Dimensions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info() #Information about Data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we do see that our data has 68 observations, and 17 dimensions. All the names of the variables are written in the info output. Seems good so far.\n\n\nLet us make a copy of the data and work on it rather than using the original data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data = Data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us see some descriptive statistics about our dataset to get an idea of the values we're dealing with.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(my_data.describe(include=['O']))\nprint(my_data.describe()) #Descriptive Statistics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't see any misssing values so that seems fine. A key point to note that 25 of the couples were divorced after getting married which is quite high! Other descriptive interpretation can be made from the output above.\n\nWe also find out that DrPepperSchwartz was always present no matter what. So, it is not quite significant for our analysis as it is constant. So let us drop that variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data.drop(['DrPepperSchwartz'], axis=1,inplace = True)\nmy_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done! Let us find the important variables that might be useful for us!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to predict if a couple will stay married or not, features like Name, Occupation don't matter much. So, for the sake of this problem, let us drop those columns!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data.drop(['Name','Occupation'], axis=1,inplace = True)\nmy_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us convert the categorical variables into numerical ones!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nmy_data['Gender'] = enc.fit_transform(my_data['Gender'])\nmy_data['Status'] = enc.fit_transform(my_data['Status'])\nmy_data['Decision'] = enc.fit_transform(my_data['Decision'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! After careful consideration, the location where the show takes place isn't something that really concerns how well the marriage is going to go. Let us drop that as well!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data.drop(['Location'], axis=1,inplace = True)\nmy_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perfect. Let us see the correlations between variables to get a better idea!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,14))\ncorr_matrix = my_data.corr().round(2)\nsns.heatmap(data=corr_matrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do see some strong correlations in this plot. Let us just see the data for the final time before making a model to run based on this. A big strong correlations can be seen for the variables Couple and Season, as well between them. So we will consider them in our analysis when we train the model on our data.\n\nLet X be the predictors from the data and Y be the response variable aka Status.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = my_data.iloc[:, [3,4,6,7,8,9,10,11,12]].values\ny = my_data.iloc[:,5].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us split the data into training and testing data. Test data will be 30% of the total data which sounds quite reasonable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perfect! Now let us try two different methods. A good method for this task would be a Random Forest Classifier. The other would be deploying a simple Artificial Neural Network. But first, Let us try a SVM to see if it good results to check the linearity of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Support Vector Classifier\n\nfrom sklearn.svm import SVC \nclassifier=SVC(kernel='linear',random_state=0)\n#Fitting training data and making predictions on test data\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, now let us see how well our classifier does! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad! But it can surely be better. We get all 0's right but not the 1's. I'm sure we could do a better model. Let's try a Random Forest Classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\n\n#Fitting training data and making predictions on test data\n\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how our classifier performs!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh we did make a few false postives and but in overall sense, we do better than a SVM. Let us see the AUC Score!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = classifier.predict_proba(X_test)\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, probs[:,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"84%! Pretty Decent isn't it? Now let us try a deep learning model which can be of use as well. Here I try an Artifical Neural Network for this task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the basic libraries and components\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n#Building the classifier and adding the layers\n\nclassifier = Sequential()\nclassifier.add(Dense(units=5, kernel_initializer='uniform', activation='relu', input_dim=9))\nclassifier.add(Dense(units=5, kernel_initializer='uniform', activation='relu'))\nclassifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done! Now let us compile the clasiifier, and set the optimizer,loss function and the metric for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! Now we're ready to make predictions on the data. Hope we do get a good result!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the data to the training set and making predictions on the test set\nclassifier.fit(X_train,y_train,batch_size=1, epochs=100) \ny_pred=classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred #Probabilities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems okay so far. Now that we have probabilities, let us convert them to 1's and 0's and see how well did we do.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1 = (y_pred>0.5)\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred1)\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! We do get a similar result as the SVM. Let us see the AUC Score!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"85% is awesome. So in conclusion, we built two predictive models, using different methodologies to predict the status of marriages after 8 weeks for these contestants. A deep learning approach resulted in an AUC score of 85% while RF Classifier gave us 84%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}