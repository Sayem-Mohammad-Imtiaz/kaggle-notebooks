{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart <font color=\"red\">Failure</font> Clinical Records\nreached 99% precision! What did it cost? Recall"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.lifespan.io/wp-content/uploads/2017/10/shutterstock_488843971.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# Problem Description\n- To create a model in order to predict the likelihood of a patient dying due to heart failure.\n- This a binary clasification problem since the target class (Death Event) consists of two classes True or False"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install seaborn --upgrade\nimport pandas as pd # data manipulation\nimport numpy as np # linear algebra\n\n# data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import qqplot # normality check\nimport plotly.express as px\nfrom sklearn.tree import plot_tree # decision tree \n\n# data preprocessing\nfrom imblearn.over_sampling import SMOTE # deal with imbalance data\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer # scale data\n\n# classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression # linear classification\nfrom sklearn.svm import LinearSVC, SVC # support vector machines\nfrom sklearn.tree import DecisionTreeClassifier # tree based\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,\\\nAdaBoostClassifier, GradientBoostingClassifier\nimport xgboost as xgb\n\n# model evaluation and selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import classification_report, plot_roc_curve\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, accuracy_score, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=toc>Table of contents</a>\n1. [Data Exploration](#eda)\n2. [Data Prepartion](#data_prep)\n3. [Data Modelling and Hyperparameter Tuning](#model)\n4. [Model Evaluation](#eval)\n5. [Prediction on Test Data](#predict)\n6. [Conclusion](#conclude)"},{"metadata":{},"cell_type":"markdown","source":"# <a id=eda>1. Data Exploration</a>\n[Back to index](#toc)"},{"metadata":{},"cell_type":"markdown","source":"## Dataset and feature description"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Age:** age of patient (in years)\n\n**2. Anaemia:** Decrease of red blood cells or hemoglobin\n\n**3. High blood pressure:** If a patient has hypertension\n\n**4. Creatinine phosphokinase:** Level of the CPK enzyme in the blood (mcg/L)\n\n**5. Diabetes:** If the patient has diabetes\n\n**6. Ejection fraction:** Percentage of blood leaving the heart at each contraction\n\n**7. Sex:** Woman or man\n\n**8. Platelets:** Platelets in the blood (kiloplatelets/mL)\n\n**9. Serum creatinine:** Level of creatinine in the blood (mg/dL)\n\n**10. Serum sodium:** Level of sodium in the blood (mEq/L)\n\n**11. Smoking:** If the patient smokes\n\n**12. Time:** Follow-up period (in days)\n\n**13. (target) death event:** If the patient died during the follow-up period"},{"metadata":{},"cell_type":"markdown","source":"## Data shape"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data types"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = ['age', 'creatinine_phosphokinase', \n           'ejection_fraction', 'platelets', \n           'serum_creatinine', 'time']\ncategorical = ['anaemia', 'diabetes', 'high_blood_pressure', \n               'sex', 'smoking']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fix age data type"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.age = df.age.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values\nThere are no missing values"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"### Target\n- The target class or label is imbalanced"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target_count = df.DEATH_EVENT.value_counts()\ndeath_color = ['navy', 'crimson']\nwith plt.style.context('ggplot'):\n    plt.figure(figsize=(6, 5))\n    sns.countplot(data=df, x='DEATH_EVENT', palette=death_color)\n    for name , val in zip(target_count.index, target_count.values):\n        plt.text(name, val/2, f'{round(val/sum(target_count)*100, 2)}%\\n({val})', ha='center',\n                color='white', fontdict={'fontsize':13})\n    plt.xticks(ticks=target_count.index, labels=['No', 'True'])\n    plt.yticks(np.arange(0, 230, 25))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Numeric Features\n- features `creatinine_phosphokinase` and `serum_creatinine` are extremely positive or right skewed"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"colors = sns.color_palette(\"tab10\")\nwith plt.style.context('bmh'):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, (col, name) in enumerate(zip(colors, numeric)):\n        plt.subplot(3, 3, i+1)\n        sns.histplot(data=df, x=name, color=col)\n    plt.suptitle('Histograms of Numeric features', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(6, 2, figsize=(10, 20))\nplt.subplots_adjust(hspace=0.4)\naxes = axes.ravel()\nfor i, name, col in zip(np.arange(0, 14, 2), numeric, colors):\n    sns.boxplot(data=df, x=name, ax=axes[i], y='DEATH_EVENT', \n                orient='h', palette=death_color, showfliers=True)\n    sns.boxplot(data=df, x=name, ax=axes[i+1], y='DEATH_EVENT', \n                orient='h', palette=death_color, showfliers=False)\nplt.suptitle('Boxplot of Numeric features with repect to the target class\\n(with and without outliers)', \n             fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution Categorical Features w.r.t target class"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"colors = sns.color_palette(\"tab10\")\nwith plt.style.context('bmh'):\n    plt.figure(figsize=(12, 15))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, (col, name) in enumerate(zip(colors, categorical)):\n        plt.subplot(3, 2, i+1)\n        sns.countplot(data=df, x=name, hue='DEATH_EVENT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=data_prep>2. Data Preparation</a>\n[Back to index](#toc)"},{"metadata":{},"cell_type":"markdown","source":"## Separate features and target class"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, :-1]\ny = df['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fix Class Imbalance using SMOTE\nSMOTE is an oversampling technique where the synthetic samples are generated for the minority class, in our case, 1's "},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE(random_state=2021, n_jobs=-1, k_neighbors=5)\nsmote.fit(X, y)\nX_smote, y_smote = smote.fit_resample(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_smote.shape)\nprint(y_smote.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Transformation\nDuring EDA for the numeric features, the histograms of few features indicated skewness. Some of the features like `creatinine_phosphokinase` and `serum_creatinine` were extremely skewed. Skewed features like these can be made more Gaussian-like using power transforms or log transforms. For example: \n\n**1. creatinine_phosphokinase** using the log transformation can make data conform to normality. In this case log-transform does remove or reduce the skewness since the original data follows a log-normal distribution or approximately so. "},{"metadata":{},"cell_type":"markdown","source":"* The qq plot below shows the effect of log trasformation on creatinine_phosphokinase. QQ plot (or quantile-quantile plot) is a plot where the axes are purposely transformed in order to make a normal (or Gaussian) distribution appear in a straight line. In other words, a perfectly normal distribution would exactly follow a line with slope = 1 and intercept = 0."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nqqplot(df.creatinine_phosphokinase, fit=True, line='45', ax=ax[0])\nax[0].set_title('before transformation')\nqqplot(np.log10(df.creatinine_phosphokinase), fit=True, line='45', ax=ax[1])\nax[1].set_title('after transformation')\nplt.suptitle('q-q plot for creatinine_phosphokinase', fontweight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. serum_creatinine** using reciprocal transform (p = -1). This transformation has a radical effect as it reverses the order among the values of same sign, therefore, larger values become smaller and visa verse"},{"metadata":{"trusted":true},"cell_type":"code","source":"p = -1\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nqqplot(df.serum_creatinine, fit=True, line='45', ax=ax[0])\nax[0].set_title('before transformation')\n\nqqplot(df.serum_creatinine**p, fit=True, line='45', ax=ax[1])\nax[1].set_title('after transformation')\n\nplt.suptitle('q-q plot for serum_creatinine', fontweight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The power transformer of sklearn learn provides two methods to make the distribution gaussian-like\n- Boxcox\n- Yeo-johnson\n\nBoth these methods searches for the right value of p (just like in the above example) in order to make the distribution normal. Yeo-johnson is a upgraded version of Boxcox as it deals with the data with negative values"},{"metadata":{"trusted":true},"cell_type":"code","source":"pt = PowerTransformer(method='yeo-johnson')\nX_pt = pt.fit_transform(X_smote)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalise Data\nFinally, normalise the data using the min max scaler which scales the data to the 0-1 range. Scaling is required for ML algo like SVM, Logistic regression, knn which are sensitive to scaling and outliers (applicable for both classification and regression problems)."},{"metadata":{"trusted":true},"cell_type":"code","source":"mm = MinMaxScaler()\nX_scaled = mm.fit_transform(X_pt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of features after transformation and scaling"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_scaled, columns=X.columns).hist(figsize=(10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection using Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, \n                            class_weight='balanced', random_state=2021)\nrf.fit(X_scaled, y_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame(np.round(rf.feature_importances_*100, 2), index=X.columns, columns=['importance%'])\nfeature_imp = feature_imp.sort_values(by='importance%', ascending=False)\nfeature_imp.plot(kind='barh', figsize=(8, 5))\nplt.xlabel('percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_features = feature_imp.index[:3]\nimp_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_selected = pd.DataFrame(X_scaled, columns=X.columns)[imp_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_selected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=model>3. Data Modelling and Hyperparameter Tuning</a>\n[Back to index](#toc)\n\nTherefore, the goal is now to separate both the classes as shown the figure below\n\n`Note` All the classifiers have been tuned to maximize the f1 score instead of accuracy. F1 score is the harmonic mean of recall and precision. This score will favor classifiers with a similar precision and recall. I could have achieved high recall or precision but unfortunately, we cannot have it both ways as increasing precision reduces recall, and visa verse"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data = X_selected\nmodel_data['target'] = y_smote","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter_3d(model_data, x='time', y='serum_creatinine', z='ejection_fraction', color='target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(model_data.drop(['target'], axis=1), \n                                                    model_data['target'], \n                                                    test_size=0.25, \n                                                    random_state=2021, \n                                                    stratify=model_data['target']\n                                                   )\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 k-Nearest Neighbour Classifier\nLet start with  simple and a lazy learner where an object is classified by a plurality vote of its neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clf = KNeighborsClassifier(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_params = {\n    'n_neighbors': np.arange(2, 12)\n}\nknn_cv = GridSearchCV(knn_clf, knn_params, scoring='f1', n_jobs=-1, cv=10)\nknn_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimum value to maximize performance\nTherefore, highest f1 score is achieved by knn using 9 neighbours. "},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions on training data using cross val predict\ncross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is made by a model that never saw the data during training)."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_train_pred = cross_val_predict(knn_cv, X_train, y_train, cv=10, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report of training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, knn_train_pred, digits=4, target_names=['not gonna die', 'will die']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`NOTE` similar approach is used for all classifiers below"},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Logistic Regression\nLogistic regression uses logit function to compute the probability of the outcomes, in our case, the target class `Death Event`"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression(class_weight='balanced', random_state=2021, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper-parameter tuning using Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_params = {\n    'penalty': ['l1', 'l2', 'elasticnet'], \n    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], \n    #'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\nlr_cv = GridSearchCV(lr_clf, lr_params, scoring='f1', cv=10, n_jobs=-1)\nlr_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best parameter value to achieve the highest F1 score"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions on training data using cross val predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_train_pred = cross_val_predict(lr_cv, X_train, y_train, cv=10, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, lr_train_pred, digits=4, target_names=['not gonna die', 'will die']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Support Vector Machine (SVM)"},{"metadata":{},"cell_type":"markdown","source":"### Linear SVM classification (Hard Margin)\nAs seen from the 3D plot of the data. The problem doesnt not look like it can be separated using a hard margin svm since there is some noise in both the classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_svm_clf = SVC(kernel='linear', class_weight='balanced', random_state=2021)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # deicison boundries\n}\nlin_svm_cv = GridSearchCV(lin_svm_clf, params, scoring='f1', n_jobs=-1, cv=10)\nlin_svm_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_svm_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_svm_train_pred = cross_val_predict(lin_svm_cv, X_train, y_train, cv=10, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, lin_svm_train_pred, digits=4, target_names=['not gonna die', 'will die']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non-Linear Classification (Soft Margin)\nUsing rbf kernal"},{"metadata":{"trusted":true},"cell_type":"code","source":"rbf_svm = SVC(kernel='rbf', class_weight='balanced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # deicison boundries\n}\nrbf_svm_cv = GridSearchCV(rbf_svm, params, scoring='f1', n_jobs=-1, cv=10)\nrbf_svm_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rbf_svm_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rbf_svm_train_pred = cross_val_predict(rbf_svm_cv, X_train, y_train, cv=10, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, rbf_svm_train_pred, digits=4, target_names=['not gonna die', 'will die']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Decision Tree\n**Pros**\n- requires very little data preparation and doesn't require feature scaling or centering.\n- simple to understand and iterpret.\n\n**Cons**\n- Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. It is very likely that the model will not generalize well. One way to limit this problem is to use Principal Component Analysis which often results in a better orientation of the training data.\n- the main issue with Decision Trees is that they are very sensitive to small variations in the training data. Random Forests can limit this instability by averaging predictions over many trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=2021)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'criterion': ['gini', 'entropy'], \n    'max_depth': np.arange(2, 22, 2), # depth of tree\n    'min_samples_split': [2, 3, 4], # min. no. of samples a node must have before it splits \n    'min_samples_leaf': [1, 2, 3, 4] # min. non of samples a leaf node must have\n}\ndt_cv = GridSearchCV(dt_clf, params, scoring='f1', n_jobs=-1, cv=10)\ndt_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_train_pred = cross_val_predict(dt_cv, X_train, y_train, cv=10, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DT Visualized"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=2021, \n                                    max_depth=4, criterion='entropy', min_samples_split=2, \n                                     min_samples_leaf= 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_dt_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplot_tree(best_dt_clf, filled=True, \n          #feature_names=['time', 'serum_creatinine', 'ejection_fraction']\n         )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Random Forest\n- It is an ensemble(group of predictors) of decision trees.\n- It has all the hyperparameters of the decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_jobs=-1, random_state=2021, class_weight='balanced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    #'n_estimators': [100, 200, 300], \n    'max_depth': np.arange(2, 22, 1), \n    #'min_samples_split': [2, 3, 4], \n    #'min_samples_leaf': [1, 2, 3, 4], \n    'criterion': ['gini', 'entropy']\n}\nrf_cv = RandomizedSearchCV(rf, params, scoring='f1', n_jobs=-1, cv=10, random_state=2021, n_iter=20)\nrf_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_train_pred = cross_val_predict(rf_cv, X_train, y_train, cv=10, n_jobs=-1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, rf_train_pred, digits=4, target_names=['not gonna die', 'will die']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=eval>4. Model Evaluation</a>\n[Back to index](#toc)\n\nRandom Forest has outperformed all the other classifers in accuracy, precision, recall, f1 score and auc score. "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"models = ['kNN', 'Logistic Regression', 'Linear SVM', 'Non-Linear SVM', \n          'Decision Tree', 'Random Forest']\nmodel_colors = sns.color_palette(\"Dark2\")\naccuracy = []\nrecall = []\nprecision = []\nf1 = []\nauc = []\npredictions = [knn_train_pred, lr_train_pred, lin_svm_train_pred, \n               rbf_svm_train_pred, dt_train_pred, rf_train_pred]\n\nfor model_pred in predictions:\n    accuracy.append(accuracy_score(y_train, model_pred))\n    precision.append(precision_score(y_train, model_pred))\n    recall.append(recall_score(y_train, model_pred))\n    f1.append(f1_score(y_train, model_pred))\n    auc.append(roc_auc_score(y_train, model_pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, accuracy, color=model_colors)\n    for m, a in zip(models, accuracy):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Accuracy percentage (%)')\n    plt.title('Model comparison on training data using Accuracy')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, recall, color=model_colors)\n    for m, a in zip(models, recall):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Recall percentage (%)')\n    plt.title('Model comparison on training data using Recall')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, precision, color=model_colors)\n    for m, a in zip(models, precision):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('Precision percentage (%)')\n    plt.title('Model comparison on training data using Precision')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, f1, color=model_colors)\n    for m, a in zip(models, f1):\n        plt.text(m, a+0.01 , f'{round(a*100, 3)}%', ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('F1 percentage (%)')\n    plt.title('Model comparison on training data using F1 score')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    plt.bar(models, auc, color=model_colors)\n    for m, a in zip(models, auc):\n        plt.text(m, a+0.01 , round(a, 3), ha='center')\n    plt.xlabel('Models')\n    plt.ylabel('AUC')\n    plt.title('Model comparison on training data using AUC')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=predict>5. Predictions on Test Data</a>\n[Back to index](#toc)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_models = [knn_cv, lr_cv, lin_svm_cv, rbf_svm_cv, dt_cv, rf_cv]\nfor name, model in zip(models, best_models):\n    best_predictions = model.predict(X_test)\n    print(name.upper())\n    print(classification_report(y_test, best_predictions))\n    print(\"-------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model comparison on test data using ROC Curve\n- the ROC curve, plots the true positive rate (another name for recall) against the false positive rate (FPR). \n- Once again there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible\n- One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n- Random forest has the highest auc."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best_models = [knn_cv, lr_cv, lin_svm_cv, rbf_svm_cv, dt_cv, rf_cv]\nwith plt.style.context('ggplot'):\n    plt.figure(figsize=(10, 5))\n    for name, model in zip(models, best_models):\n        best_predictions = model.predict(X_test)\n        fpr, tpr, thresholds = roc_curve(y_test, best_predictions)\n        plt.plot(fpr, tpr, linewidth=2, label=name)\n        plt.plot([0, 1], [0, 1], 'k--') \n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Reccll)')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=conclude>Conclusion</a>\n[Back to index](#toc)"},{"metadata":{},"cell_type":"markdown","source":"- Instead of all the 13 features, only 3 features `time`, `serum_creatinine` and `ejection_fraction` are sufficient to model the data. Using top 7 or all the features resulted in overfitting. \n- Random Forest has the best performance as compared to other models on both train and test data"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}