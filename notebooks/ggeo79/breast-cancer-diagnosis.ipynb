{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cdb8c8a9-1a3f-c975-11f3-e1eadeb09433"},"source":"Breast Cancer diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45fb69a9-701e-fc64-b174-f06330669cd2"},"outputs":[],"source":"import numpy as np \nimport pandas as pd \ndf = pd.read_csv('../input/data.csv')\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c70b23aa-e9b9-af5d-8718-e03ee6d9de7c"},"outputs":[],"source":"# Check the data size\ndf.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7a17196-52d6-9bdb-10f1-fa686d02b8ae"},"outputs":[],"source":"df.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5b7a1d2-bdd3-0445-22d5-7403ef3bd0a0"},"outputs":[],"source":"df.drop(df.columns[32], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb8c0126-2fd2-bc37-8d38-98c64d528076"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n#%matplotlib inline\n\nsns.pairplot(df, vars=df.columns[2:5], hue='diagnosis', markers=['o', 's'], size=3, kind='reg')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e6b7dbc0-db79-39be-56c8-0478b05f0de9"},"source":"We can see some linear relationships from the above graph between radius-perimeter-area. (NOTE that I am trying for a few hours to upload/copy my notebook on kaggle and it just doesn't work.So, I have used only 3 columns from the data! instead of all data...)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08ecc5ce-62c1-b76a-a64d-1304a18cabff"},"outputs":[],"source":"# Use dummies in order to map the diagnosis labels to 0 and 1\ndf_new = pd.get_dummies(df)\ndf_new.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"175c0234-57f5-6093-5185-76126672fcf9"},"source":"Plot a correlation matrix."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4573438-619c-5b92-f241-73906f87b13b"},"outputs":[],"source":"cols = list(df_new.columns)\nfig, ax = plt.subplots(figsize=(26,20))\n# We take values[:, 1:] in order to ommit the id's.\ncm = np.corrcoef(df_new.values[:, 1:].T)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':8}, yticklabels=cols[1:], xticklabels=cols[1:])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ba1503d8-71e9-c550-c9b6-d37fb5343f8b"},"source":"From the above we can see that the area-perimeter-radius-concave_points are strongly correlated to the malignant case."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d048fb5f-d128-d3d5-0f2a-34b8c5114773"},"outputs":[],"source":"# Let's get back to the original data\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3ec8799-3654-7473-407c-6057991e9914"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\n\n# Take every line except from the columns with the id's and the labelled data (benigh, malignant)\nX = df.values[:, 2:].astype(float)\n# The labelled data\ny = df.values[:, 1]"},{"cell_type":"markdown","metadata":{"_cell_guid":"6d91e202-e82b-34c5-0306-cb5e6a967d63"},"source":"We are going to test different models and check their metrics."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c98b2f7-218d-f4d2-b4f8-47094f290853"},"outputs":[],"source":"# Prepare the models\nlr = LogisticRegression(C=1000.0, random_state=0)\nsvm_linear = SVC(kernel='linear', C=1.0, random_state=0, probability=True)\nsvm_nonlinear = SVC(kernel='rbf', C=10.0, gamma = 0.1, random_state=0, probability=True)\nrandom_forest = RandomForestClassifier(criterion='entropy', random_state=0)\ngaussian_nb = GaussianNB()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56ee3b9b-573f-3116-4cea-681eb4b06b33"},"outputs":[],"source":"labels = ['Logistic Regression', 'SVC linear', 'SVC non linear', 'Random Forest', 'Naive Bayes']\nmodels = [lr, svm_linear, svm_nonlinear, random_forest, gaussian_nb]\n# Apply cross validation score\nfor model, label in zip(models, labels):\n    scores = model_selection.cross_val_score(model, X, y, cv=5, scoring='accuracy')\n    print('Mean: {0}\\t Std: {1}\\t Label: {2}'.format(scores.mean(), scores.std(), label))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c3d0caca-bef1-a037-c4c8-3fb5d18ee0c3"},"source":"We can see that most models have similar accuracy except from the SVC non linear.\nLet's just test what about the voting classifier which aggregates the predictions of each classifier and predicts the class that gets the most votes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c9c74d3-3cb0-7a69-1812-91b1a829956e"},"outputs":[],"source":"voting = VotingClassifier(estimators=[('lr', lr), ('svm_linear', svm_linear), ('svm_nonlinear', svm_nonlinear), \n                                      ('random_forest', random_forest), ('gaussian_nb', gaussian_nb)],\n                        voting = 'soft', weights=[1,1,1,1,1])\n\nlabels = ['Logistic Regression', 'SVC linear', 'SVC non linear', 'Random Forest', 'Naive Bayes', 'Voting']\nmodels = [lr, svm_linear, svm_nonlinear, random_forest, gaussian_nb, voting]\n\nfor model, label in zip(models, labels):\n    scores = model_selection.cross_val_score(model, X, y, cv=5, scoring='accuracy')\n    print('Mean: {0}\\t Std: {1}\\t Label: {2}'.format(scores.mean(), scores.std(), label))"},{"cell_type":"markdown","metadata":{"_cell_guid":"61fd1a56-972b-7fe6-e699-5c93f47103e0"},"source":"We saw that voting is almost equal to random forest classifier.\nwe are going to use the random forest for training our data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3679d906-3a3c-9a49-9eb4-52c874953cbf"},"outputs":[],"source":"# Split data into training and test datasets.\n# Playing with the test size, I found that 20% gives the best results\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Standardize the features in order to have properties of standar normal distribution (mean = 0, std = 1)\nstd_sc = StandardScaler()\nX_train = std_sc.fit_transform(X_train)\nX_test = std_sc.fit_transform(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0852a001-e4d7-aeb5-c5a8-97ffe2cee04f"},"outputs":[],"source":"# Train our model\nrandom_forest.fit(X_train, y_train)\n# Get the feature importances attribute\nimportances = random_forest.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Plot feature importances\nfig, axes = plt.subplots(figsize=(14, 10))\nplt.bar(range(X_train.shape[1]), importances[indices], align='center')\nplt.title('Feature Importances')\nplt.xticks(range(X_train.shape[1]), df.columns[2:], rotation=90)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"44fd5d36-8380-cf69-d99a-3e3ee452fe69"},"source":"We can see that the most favored feature is the radius mean.We remember from the correlation matrix that the mean radius is strongly correlated to perimeter, texture and area features.The least favored is the fractal dimension.Keep in mind that feature importances are normalized so that they sum up to 1.0."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7e93617-3abb-1daa-3662-5b05933377aa"},"outputs":[],"source":"# Predict class\ny_pred = random_forest.predict(X_test)\nprint('Accuracy: {0}'.format(accuracy_score(y_test, y_pred)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"15c87ab7-6c7c-037e-d82e-0fe66f08a6ba"},"source":"Ok!Nice accuracy using random forest classifier!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}