{"cells":[{"metadata":{"_uuid":"ebc4f6e0009141b6869fc0d530f7a46847670895"},"cell_type":"markdown","source":"# This kernel is about linear regression between some features of database: \" Biomechanical features of orthopedic patients\"\n# I started with data analysis","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# read csv (comma separated value) into data\ndata = pd.read_csv('../input/column_2C_weka.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c73307c97838b91359838080c07738149b6f27d3"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fc73373407592d4eb6d2a9507ef9f5bb34ce132"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457b2f9a6317626d3acdc99856f1ecf7d7d5ce94"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6cbd4c0885da174edf4b2274f19857474ed1a52"},"cell_type":"code","source":"#First I visualize the seaborn heatmap to see  the correlation between the features to choose two with max. corr.\n#and these are pelvic_incidence and sacral_slope\nfig,ax = plt.subplots(figsize=(7,5))\n\nax = sns.heatmap(\ndata.corr(), \nannot=True, annot_kws={'size':8},\nlinewidths=.3,linecolor=\"blue\", fmt= '.2f',square=True,cmap=\"YlGnBu_r\",cbar=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For my kernel I used \"pelvic_incidence\" and \"sacral_slope\" for class \"Normal\"\ndata1 = data.loc[data['class'] =='Normal']\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73f46e11bbb9c493d75d88ec7fb2bd866a5119ef"},"cell_type":"code","source":"#numpy array for x_axis\n#numpy array for y_axis\nx = data1.pelvic_incidence.values\ny = data1.sacral_slope.values\n# Scatter\nplt.figure(figsize=[6,6])\nplt.scatter(x,y,color=\"magenta\")\nplt.xlabel('pelvic_incidence',fontsize = 25,color='blue')\nplt.ylabel('sacral_slope',fontsize = 25,color='blue')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Y = a*x + b\n# a = (n∑y¡x¡ - ∑y¡∑x¡) / n∑x¡² - (∑x¡)²\n# b = (∑y¡∑x¡² - ∑x¡∑y¡x¡) / n∑x¡² - (∑x¡)²","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Regression\ndenominator= x.dot(x)-x.mean()*x.sum()\na= (x.dot(y)-y.mean()*x.sum())/denominator\nb= (y.mean()*x.dot(x)-x.mean()*x.dot(y))/denominator\n    \nYhat= a*x + b  # best fitting line\n\nplt.figure(figsize=[6,6])\nplt.scatter(x,y,color=\"magenta\")\nplt.plot(x,Yhat)\nplt.xlabel('pelvic_incidence',fontsize = 25,color='blue')\nplt.ylabel('sacral_slope',fontsize = 25,color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R² = 1 -(Residual Sum of Squares / Total Sum of Squares)\n# RSS = ∑(y¡ - y¡^)²\n# TSS = ∑(y¡ - mean of y)²","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy\nd1= y-Yhat\nd2= y-y.mean()\nr2= 1-(d1.dot(d1)/d2.dot(d2))\nprint(\"the r-squared is:\",r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a5bf0ac2ebd087cd32df35af8ac466c354b1871"},"cell_type":"code","source":"# Linear Regression with sklearn\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nX = x.reshape(-1,1)\nY = y.reshape(-1,1)\nlr.fit(X,Y)\ny_pred = lr.predict(X)  \n\n# plot scatter\nplt.figure(figsize=[6,6])\nplt.scatter(X,Y,color=\"magenta\")\nplt.xlabel('pelvic_incidence',fontsize = 25,color='blue')\nplt.ylabel('sacral_slope',fontsize = 25,color='blue')\n\n#plot regression line\nplt.plot(X, y_pred)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy\nprint(\"R^2 score: \", lr.score(X,Y))\n#or\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \",r2_score(Y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multiple Linear Regression\n#load data (I took lumbar_lordosis_angle as x2)\nx = data1.iloc[:, [0,2]].values\n# I added a column of ones (noise)\nX = np.insert(x,0,1,axis=1)\nY = data1.sacral_slope.values\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=X[:,0],\n    y=X[:,1],\n    z=Y,\n    mode='markers',\n    marker=dict(\n        size=8,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    ),\n    \n)\n\ndata_ = [trace1]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data_, layout=layout)\n\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# w = (XᵀX)¯¹* XᵀY\n# w = np.linalg.solve((XᵀX)¯¹, XᵀY)\n# Y (predictions) is equal X.w","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, Y))\nYhat = np.dot(X, w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy\nd1 = Y - Yhat\nd2 = Y - Y.mean()\nr2 = 1 - d1.dot(d1) / d2.dot(d2)\nprint(\"the r-squared is:\", r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree Regression\n# I took first 15 samples of X and Y\n\nx = data1.iloc[:15,[0]].values.reshape(-1,1)\ny = data1.iloc[:15,[2]].values.reshape(-1,1)\n\nfrom sklearn.tree import DecisionTreeRegressor\ndes_tree_reg = DecisionTreeRegressor()\n\ndes_tree_reg.fit(x,y)\n\n\nx_new = np.linspace(min(x),max(x)).reshape(-1,1)\nx_new1 = np.arange(min(x),max(x),0.01).reshape(-1,1)\nyhat_new = des_tree_reg.predict(x_new)\nyhat_new1 = des_tree_reg.predict(x_new1)\nplt.subplot(111)\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new,yhat_new)\nplt.show()\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new1,yhat_new1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_for = RandomForestRegressor(n_estimators =100, random_state = 42)\n\nrandom_for.fit(x,y)\n\n\nx_new = np.linspace(min(x),max(x)).reshape(-1,1)\nx_new1 = np.arange(min(x),max(x),0.01).reshape(-1,1)\nyhat_new = random_for.predict(x_new)\nyhat_new1 = random_for.predict(x_new1)\nplt.subplot(111)\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new,yhat_new)\nplt.show()\nplt.scatter(x,y, c=\"m\")\nplt.plot(x_new1,yhat_new1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy\nfrom sklearn.metrics import r2_score\nyhat_rf = random_for.predict(x)\n\nprint(\"r_square score: \", r2_score(y,yhat_rf))\n# or:\nprint(\"r2: \", random_for.score(x,y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear regression with gradient descent\n# I took first 15 samples of X and Y and multiply them with 0.1\nx = data1.iloc[:15, [0,2]].values\nx = x * 0.1\n# I added a column of ones (noise)\nX = np.insert(x,0,1,axis=1)\ny = data1.sacral_slope.values\nY = y[:15] * 0.1\n\n\nprint(X.shape)\nprint(Y.shape)\nD = X.shape[1]\nN = X.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the data\nfig = plt.figure()\nax = fig.add_subplot(111, projection =\"3d\")\nax.scatter(X[:,0],X[:,1],Y,c=\"r\",marker=\"o\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost = []\nw = np.random.randn(D)/np.sqrt(D)\nlearning_rate = 0.001\n\nfor i in range(300):\n    Yhat = X.dot(w)\n    interval = Yhat - Y\n    w = w - learning_rate * X.T.dot(interval)\n    # mean squared error\n    # 1/N *(Yhat -Y)        for N ...samples\n    mse = interval.dot(interval) / N\n    cost.append(mse)\n\nplt.plot(cost)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(Yhat, label=\"prediction\")\nplt.plot(Y ,label=\"target\")\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy\nd1 = Y - Yhat\nd2 = Y - Y.mean()\nr2 = 1 - d1.dot(d1) / d2.dot(d2)\nprint(\"the r-squared is:\", r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear regression with l2 regularization(Ridge)\n# I took the same data as with gradient descent\nx = data1.iloc[:15, [0,2]].values\nx = x * 0.1\n# I added a column of ones (noise)\nX = np.insert(x,0,1,axis=1)\ny = data1.sacral_slope.values\nY = y[:15] * 0.1\n# I added +20 to the last 2 samples of Y\nY[-1] += 20\nY[-2] += 20\nprint(X.shape)\nprint(Y.shape)\nD = X.shape[1]\nN = X.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X[:,1],Y)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# maximum likelihood\nw_ml = np.linalg.solve(X.T.dot(X),X.T.dot(Y))\nYhat_ml = X.dot(w_ml)\n\nplt.scatter(X[:,1],Y)\nplt.plot(sorted(X[:,1]),sorted(Yhat_ml))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# l2 regularization (Ridge)\nl2 = 1000.0\nw_ridge = np.linalg.solve(l2*np.eye(3) + X.T.dot(X),X.T.dot(Y))\nYhat_ridge = X.dot(w_ridge)\n\nplt.scatter(X[:,1],Y)\nplt.plot(sorted(X[:,1]),sorted(Yhat_ml), label = \"maximum likelihood\")\nplt.plot(sorted(X[:,1]),sorted(Yhat_ridge), label = \"l2_reg/ridge\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I tried to give a short example about some different methods of linear regression.\n# I hope it was helpful","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\ndata[\"class\"] = [\"1\" if each ==\"Normal\" else \"0\" for each in data[\"class\"]]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"class\"].values\nx_ = data.drop([\"class\"],axis=1)\nprint(y.shape,x_.shape)\nx_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalisation\nx = (x_ - np.min(x_))/(np.max(x_) - np.min(x_)).values\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.2,random_state=42)\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ntest_acc = lr.score(x_test,y_test)\nprint(\"accuracy:\",test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}