{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # for linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re #regular expressions\nimport matplotlib.pyplot as plt  #graphes\nimport seaborn as sns #graphes\nimport string \nimport nltk #traitement de language humain\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline\n\n#On importe les bibliotheques necessaires","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On lit et copy les data sets pour le pre-processing\n\n#train = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\n#train_original=train.copy()\n\ntrainb = pd.read_csv('../input/sentiment-analys-dataset/Sentiment Analysis Dataset.csv',error_bad_lines=False)\ntrainb_original=trainb.copy()\n\ntest = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')\ntest_original=test.copy()\n\ntrainb.columns = ['id','label','todelete','tweet']\ntrainb = trainb.drop(\"todelete\", axis=1)\ntrainb['label'].replace({1: 0, 0: 1}, inplace=True)\n\ntrain=trainb[:31962]\n\ntrainb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = train.append(test,ignore_index=True,sort=True)\n#on peut visualiser le nouveau fichier avec ces deux commandes\n#combine.tail()\ncombine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cette fonction est pour enlever le @user des tweets\ndef remove_pattern(text,pattern):\n    \n    # re.findall() trouve tout le mots pattern dans un texte et les mets dans une liste r\n    r = re.findall(pattern,text)\n    \n    # re.sub() enleve la pattern des phrases de la data set\n    for i in r:\n        text = re.sub(i,\"\",text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine['Tidy_Tweets'] = np.vectorize(remove_pattern)(combine['tweet'], \"@[\\w]*\")\n#on ajoute une colonne avec les tweets pre-processed , dans cette exemple ils manquent le @user\ncombine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Maintenant on enleve la ponctuation , les chiffres et les characteres speciaux \ncombine['Tidy_Tweets'] = combine['Tidy_Tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n\n#combine.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#On enleve les petits mots insignifiant comme 'the' 'oh' 'and'  etc ...\ncombine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n\ncombine.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#on creer une liste a partir des tweets ordonnés\n\ntokenized_tweet = combine['Tidy_Tweets'].apply(lambda x: x.split())\n\n#tokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#on regroupe les mots de la meme famille \n\nfrom nltk import PorterStemmer\n\nps = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n\ncombine['Tidy_Tweets'] = tokenized_tweet\ncombine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction pour extraire les hashtags : \n\ndef Hashtags_Extract(x):\n    hashtags=[]\n    \n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r'#(\\w+)',i)\n        hashtags.append(ht)\n    \n    return hashtags\n\n#on extrait les hashtags positifs\nht_positive = Hashtags_Extract(combine['tweet'][combine['label']==0])\n\nht_positive[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#on extrait les hashtags negatifs \n\nht_negative = Hashtags_Extract(combine['tweet'][combine['label']==1])\n\nht_negative[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#on combine la list en une list 1xn (1dimension) \nht_negative_unnest = sum(ht_negative,[])\nht_positive_unnest = sum(ht_positive,[])\n\n\n#une representation des hashtags les plus utilisés \nword_freq_positive = nltk.FreqDist(ht_positive_unnest)\n\nword_freq_positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating d'un dataframe qui compte le redondance de chaque hashtag positif\n\ndf_positive = pd.DataFrame({'Hashtags':list(word_freq_positive.keys()),'Count':list(word_freq_positive.values())})\n\ndf_positive.head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#affichage de cette dataframe en forme de graphe \n\ndf_positive_plot = df_positive.nlargest(20,columns='Count')\n\nsns.barplot(data=df_positive_plot,y='Hashtags',x='Count')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#meme chose pour les negatifs\nword_freq_negative = nltk.FreqDist(ht_negative_unnest)\n\nword_freq_negative\n\ndf_negative = pd.DataFrame({'Hashtags':list(word_freq_negative.keys()),'Count':list(word_freq_negative.values())})\n\ndf_negative.head(10)\n\ndf_negative_plot = df_negative.nlargest(20,columns='Count')\n\nsns.barplot(data=df_negative_plot,y='Hashtags',x='Count')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Extraction des features grace au Bag of words \nfrom sklearn.feature_extraction.text import CountVectorizer\n\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n\n# bag-of-words  matrice de carac\nbow = bow_vectorizer.fit_transform(combine['Tidy_Tweets'])\n\ndf_bow = pd.DataFrame(bow.todense())\n\ndf_bow.head()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train_bow = bow[:31962]\n\ntrain_bow.todense()\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split\n\nx_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['label'],test_size=0.3,random_state=2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tfidf\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf=TfidfVectorizer(max_df=0.90, min_df=2,max_features=1000,stop_words='english')\n\ntfidf_matrix=tfidf.fit_transform(combine['Tidy_Tweets'])\n\ndf_tfidf = pd.DataFrame(tfidf_matrix.todense())\n\ndf_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf_matrix = tfidf_matrix[:31962]\n\ntrain_tfidf_matrix.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['label'],test_size=0.3,random_state=17)\n\n#on divise la data set sous des matrices d'entrainement et de test randomly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.linear_model import LogisticRegression\n\nLog_Reg = LogisticRegression(random_state=0,solver='lbfgs')\n\nLog_Reg.fit(x_train_bow,y_train_bow)\n\nprediction_bow = Log_Reg.predict_proba(x_valid_bow)\n\nprediction_bow","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nLog_Reg = LogisticRegression(random_state=0,solver='lbfgs')\n\nLog_Reg.fit(x_train_tfidf,y_train_tfidf)\n\nprediction_tfidf = Log_Reg.predict_proba(x_valid_tfidf)\n\nprediction_tfidf\n\n#log_reg prends ces matrices pour pouvoir classifier leur positivité selon la probabilité","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_bow = bow[31962:]\ntest_pred = Log_Reg.predict_proba(test_bow)\n\ntest_pred_int = test_pred[:,1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\n\ntest['label'] = test_pred_int\n\nsubmission = test[['id','label','tweet']]\nsubmission.to_csv('result.csv', index=False)\n\nres = pd.read_csv('result.csv')\nres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf\ntest_tfidf = tfidf_matrix[31962:]\ntest_pred = Log_Reg.predict_proba(test_tfidf)\n#on predit le reste de la matrice non labeled \ntest_pred_int = test_pred[:,1] >= 0.3\n#kanchdo les valeurs li kber mn 0.3 f probabilités bach kan7sbohom homa proba dominante\ntest_pred_int = test_pred_int.astype(np.int)\n\ntest['label'] = test_pred_int\n\nsubmission = test[['id','label','tweet']]\n\n\nsubmission.to_csv('resulttf.csv', index=False)\n\nrestf = pd.read_csv('resulttf.csv')\nrestf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"restf['Tidy_Tweets'] = np.vectorize(remove_pattern)(restf['tweet'], \"@[\\w]*\")\nrestf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data vizualization wordcloud \nfrom wordcloud import WordCloud,ImageColorGenerator\nfrom PIL import Image\nimport urllib\nimport requests\n\n\n\nall_words_positive = ' '.join(text for text in restf['Tidy_Tweets'][restf['label']==0])\n\n\n# combining the image with the dataset\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n\n# We use the ImageColorGenerator library from Wordcloud \n# Here we take the color of the image and impose it over our wordcloud\nimage_colors = ImageColorGenerator(Mask)\n\n# Now we use the WordCloud function from the wordcloud library \nwc = WordCloud(background_color='white', height=1500, width=4000,mask=Mask).generate(all_words_positive)\n\n# Size of the image generated \nplt.figure(figsize=(10,20))\n\n# Here we recolor the words from the dataset to the image's color\n# recolor just recolors the default colors to the image's blue color\n# interpolation is used to smooth the image generated \nplt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words_negative = ' '.join(text for text in restf['Tidy_Tweets'][restf['label']==1])\n\n\n# combining the image with the dataset\nMask = np.array(Image.open('../input/rdtweet/redtweet.png'))\n\n# We use the ImageColorGenerator library from Wordcloud \n# Here we take the color of the image and impose it over our wordcloud\nimage_colors = ImageColorGenerator(Mask)\n\n# Now we use the WordCloud function from the wordcloud library \nwc = WordCloud(background_color='white', height=1500, width=4000,mask=Mask).generate(all_words_negative)\n\n# Size of the image generated \nplt.figure(figsize=(10,20))\n\n\n# Here we recolor the words from the dataset to the image's color\n# recolor just recolors the default colors to the image's blue color\n# interpolation is used to smooth the image generated \nplt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine Res with train \ntotal = train.append(restf,ignore_index=True,sort=True)\n#on peut visualiser le nouveau fichier avec ces deux commandes\n#combine.tail()\ntotal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count neg\ncountneg = len(total['tweet'][total['label']==1])\nprint(countneg)\ncountpos = len(total['tweet'][total['label']==0])\nprint(countpos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Pie charts \n# Data to plot\nlabels = 'Positive', 'Negative'\nsizes = [countpos, countneg]\ncolors = ['aqua', 'lightsalmon']\n\n\n# Plot\nplt.pie(sizes, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True,radius=3, startangle=100)\n\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#individual tweets\n\ntwt = input(\"donnez votre tweet :\\n\")\n\ndata = {'id':['0'],\n        'Tidy_Tweets':[twt]}\ndif = pd.DataFrame(data)\ncombiine = combine.copy()\ncombiine = combine.append(dif,ignore_index=True,sort=True)\nboow = bow_vectorizer.fit_transform(combiine['Tidy_Tweets'])\n\n\nnew_bow = boow[-1]\nnew_pred = Log_Reg.predict_proba(new_bow)\n\npredicc = new_pred.tolist()\n\npredicc_unnest = sum(predicc,[])\n\n#affichage du resultat\nplt.rcParams['font.size'] = 15\nexplode = (0.1, 0.6)\nplt.pie(predicc_unnest, labels=labels, colors=colors,explode=explode,autopct='%1.1f%%', shadow=True,radius=2, startangle=0)\nplt.axis('off')\nplt.show()\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.metrics import f1_score\n\n# if prediction is greater than or equal to 0.3 than 1 else 0\n# Where 0 is for positive sentiment tweets and 1 for negative sentiment tweets\nprediction_int = prediction_bow[:,1]>=0.3\n\n# converting the results to integer type\nprediction_int = prediction_int.astype(np.int)\nprediction_int\n\n# calculating f1 score\nlog_bow = f1_score(y_valid_bow, prediction_int)\n\nlog_bow","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# Si la prediction est sup ou egale à 0.3 on lui donne 1 sinon 0\n#  0 positive  1  negative \nprediction_int = prediction_tfidf[:,1]>=0.3\n\nprediction_int = prediction_int.astype(np.int)\nprediction_int\n\n# calcule f1 score\nlog_tfidf = f1_score(y_valid_tfidf, prediction_int)\n\nlog_tfidf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}