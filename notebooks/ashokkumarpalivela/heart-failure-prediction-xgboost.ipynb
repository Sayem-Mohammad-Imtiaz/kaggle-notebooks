{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1> <u>Heart failure prediction</u></h1></center>\n<img src=\"https://images.pexels.com/photos/6765583/pexels-photo-6765583.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940\" width=\"50%\">\n<center><a href=\"https://www.pexels.com/photo/flower-petals-scattered-around-decorative-heart-6765583/\">Photo by Michelle Leman from Pexels</a></center>\n\n## Contents\n- [The problem and The data](#section1)\n    - [Understanding the problem](#subsection1)\n    - [About the dataset](#subsection2)\n- [Exploratory data analysis](#section2)\n- [Feature Engineering](#section3)\n- [Feature Selection](#section4)\n- [Modeling and Evaluation](#section5)\n    - [Decision tree Classifier](#tree)\n    - [Logistic Regression](#logistic)\n    - [Random Forest Classifier](#forest)\n- [Final Model](#final)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Importing python modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgbm\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import FunctionTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('precision', 2)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nplt.style.use('ggplot')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section1\"></a>\n# The problem and The data\n<a id=\"subsection1\"></a>\n## Understanding the problem\n<b>Let's understand the problem that we are going to solve.<br></b>\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.<br>\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management.<br>\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.<br>\n<b>We can build a machine learning model for predicting mortality caused by Heart Failure using other health factors of the patient.</b><br>\nIn ML terminology, a <b style=\"color:green;\"> Supervised Learning Binary Classifcation problem.</b>\n\n<a id=\"subsection2\"></a>\n## About the dataset\nThis dataset contains 12 features that can be used to predict mortality by heart failure.<br>\n<b> age </b> : Age of the patient <br>\n<b> anaemia </b> : 0 = N0, 1 = YES  <br>\n<b> creatinine_phosphokinase </b> : measure of creatinine phosphokinase level in bloodstream <br>\n<b> diabetes </b> : 0 = NO, 1 = YES <br>\n<b> ejection_fraction </b> : The measurement of the percentage of blood leaving the heart each time it contracts. <br>\n<b> high_blood_pressure </b> : 0 = NO, 1 = YES <br>\n<b> platelets </b> : Count of platelets <br>\n<b> serum_creatinine </b> : serum creatinine level <br>\n<b> serum_sodium </b> :  measure of sodium in the body<br>\n<b> sex </b> :  0 = FEMALE, 1 = MALE<br>\n<b> smoking </b> : 0 = NO, 1 = YES <br>\n<b> time </b> : the time at which DEATH_EVENT happened in days. For example; if the patient died, then it tells how many days it took to happen, if the patient survives, it tells how long recovery took.<br>\n<b> DEATH_EVENT </b> : 0 = NO, 1 = YES (target) <br>","metadata":{}},{"cell_type":"markdown","source":"### Loading the data into memory","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\nprint(\"Data loaded successfully!!\")\nprint(f\"There are {data.shape[0]} rows and {data.shape[1]} columns are present in the data.\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section2\"></a>\n# Exploratory Data Analysis\n\n","metadata":{}},{"cell_type":"code","source":"# random sample of data\ndata.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistical summary\ndata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Null values\ndata.isna().mean().to_frame(name=\"% of null values\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique values\ndata.nunique().to_frame(name=\"# of unique values\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Variable Separation\nSeparating the features based on their data type.","metadata":{}},{"cell_type":"code","source":"features = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n            'ejection_fraction', 'high_blood_pressure', 'platelets',\n            'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\n\ncontinuous_features = ['age','creatinine_phosphokinase','ejection_fraction',\n                       'platelets','serum_creatinine','serum_sodium','time']\n\ndiscrete_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n\ntarget = 'DEATH_EVENT'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target distribution","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,5))\nsns.countplot(x=data[target], ax=ax)\nax.set_xlabel(target, fontsize=13, fontweight='bold')\nfor patch in ax.patches:\n    height = patch.get_height()\n    width = patch.get_width()\n    new_width = width * 0.4\n    patch.set_width(new_width)\n    x = patch.get_x()\n    patch.set_x(x + (width - new_width) / 2)\n    ax.text(x=x + width/2, y=height, s=height, ha='center', va='bottom')\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of continuous features","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(4,2, figsize=(15,20))\naxes = np.ravel(axes)\nfor i, col in enumerate(continuous_features):\n    sns.distplot(a=data[col], ax=axes[i], bins=30, color='blue')\n    axes[i].set_title(f\" Distribution of {col}\")\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of discrete features","metadata":{}},{"cell_type":"code","source":"## source: https://stackoverflow.com/questions/64946868/on-changing-the-bar-width-of-a-countplot-the-relative-position-of-the-bars-get\ndisc_data = data[discrete_features].astype('category')\n\nfig, axes = plt.subplots(3,2, figsize=(13,15))\naxes=np.ravel(axes)\nfor i, col in enumerate(discrete_features):\n    sns.countplot(x=disc_data[col], ax=axes[i])\n    axes[i].set_title(col, fontsize=13, fontweight='bold')\n    for patch, label in zip(axes[i].patches, [\"NO\", \"YES\"]):\n        height = patch.get_height()\n        width = patch.get_width()\n        new_width = width * 0.4\n        patch.set_width(new_width)\n        patch.set_label(label)\n        x = patch.get_x()\n        patch.set_x(x + (width - new_width) / 2)\n        axes[i].text(x=x + width/2, y=height, s=height, ha='center', va='bottom')\n            \n    axes[i].legend(loc='lower right')\n    axes[i].margins(y=0.1)\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Continuous features Vs Target (Box plot)","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(4,2, figsize=(13,15))\naxes=np.ravel(axes)\n\nfor i,col in enumerate(continuous_features):\n    sns.boxplot(x = data[target].astype('category'), y = col, data=data, ax=axes[i])\n    axes[i].set_ylabel(col, fontweight='bold')\n    axes[i].set_xlabel(target, fontweight='bold')\n    axes[i].set_title(f'{col} vs target', fontsize=14)\n    \nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discrete features distribution w.r.t Target","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(15, 15))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, col in enumerate(discrete_features):\n    fltr = data[target] == 0\n    vc_a = data[fltr][col].value_counts().reset_index().rename({'index' : col, col: 'count'}, axis=1)\n\n    vc_b = data[~fltr][col].value_counts().reset_index().rename({'index' : col, col: 'count'}, axis=1)\n\n    vc_a[target] = 0\n    vc_b[target] = 1\n\n    df = pd.concat([vc_a, vc_b]).reset_index(drop = True)\n\n    sns.barplot(x = col, y = 'count', data = df , hue=target, ax=axes[i])\n    axes[i].set_title(col, fontweight='bold')\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discrete features Vs Target","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(15,15))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i, c in enumerate(discrete_features):\n    df = data[[c,target]].groupby(c).mean().reset_index()\n    sns.barplot(df[c], df[target], ax=axes[i])\n    for patch in axes[i].patches:\n        height = patch.get_height()\n        width = patch.get_width()\n        new_width = width * 0.4\n        patch.set_width(new_width)\n        x = patch.get_x()\n        patch.set_x(x + (width - new_width) / 2)\n    axes[i].set_ylabel('mean of target', fontsize=14)\n    axes[i].set_xlabel(c, fontsize=14, fontweight='bold')\n    \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation : Seems like Many discrete features are not so helpful in predicting target.<br>\nHigh_blood_pressure, anaemia are useful","metadata":{}},{"cell_type":"markdown","source":"### Correlation of features with target","metadata":{}},{"cell_type":"code","source":"corr_mat = data.corr()[target].sort_values(ascending=False).to_frame()\nplt.figure(figsize=(2,8))\nsns.heatmap(corr_mat, cmap='Blues', cbar=False, annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section3\"></a>\n# Feature Engineering\nBefore transforming the features it is better to split the data into train and test sets\n\n### Train test split\n<b>Training : </b>80% of data<br>\n<b>Testing : </b>20% of data","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(data, test_size=0.2, random_state=1, stratify=data[target])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Log transformation","metadata":{}},{"cell_type":"code","source":"transformer = FunctionTransformer(np.log)\n\ntrain[continuous_features] = transformer.fit_transform(train[continuous_features])\ntest[continuous_features] = transformer.transform(test[continuous_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessed data","metadata":{}},{"cell_type":"code","source":"X_train = train[features]\ny_train = train[target]\n\nX_test = test[features]\ny_test = test[target]\n\nprint(\"Train set : \", train.shape)\nprint(\"Test set : \", test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section4\"></a>\n# Feature Selection\n<b>Feature selection using Random forest</b> comes under the category of Embedded methods. Embedded methods combine the qualities of filter and wrapper methods. They are implemented by algorithms that have their own built-in feature selection methods. Some of the benefits of embedded methods are :\n- They are highly accurate.\n- They generalize better.\n- They are interpretable\n\n<a href=\"https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f\">Reference blog</a>","metadata":{}},{"cell_type":"code","source":"selector = SelectFromModel(\n    \n    RandomForestClassifier(n_estimators = 100,\n                           random_state=1),\n    threshold='median')\n\nselector.fit(X_train, y_train)\n\nselected_feat= X_train.columns[(selector.get_support())].tolist()\nprint(\"Best features : \",selected_feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importance of all features","metadata":{}},{"cell_type":"code","source":"importance = pd.Series(\n    selector.estimator_.feature_importances_.ravel(),\n    features).to_frame(name=\"feature importance\") \\\n.sort_values('feature importance', ascending=False)\nimportance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final data","metadata":{}},{"cell_type":"code","source":"X_train = X_train[selected_feat]\nX_test = X_test[selected_feat]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section5\"></a>\n# Modeling and Evaluation","metadata":{}},{"cell_type":"code","source":"results = {\"model\":[], \"CV f1-score\":[]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline model\ndef base_model(clf):\n    clf.fit(X_train, y_train)\n    train_preds = clf.predict(X_train)\n    test_preds = clf.predict(X_test)\n    print(\"Train f1 Score :\", f1_score(y_train, train_preds))\n    print(\"Test f1 Score :\", f1_score(y_test, test_preds))  ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''### K - Fold Cross validation ###\nStep 1: Randomly divide a dataset into k groups, or “folds”, of roughly equal size.\nStep 2: Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds.\nStep 3: Calculate the test F1-score on the observations in the fold that was held out.\nStep 4: Repeat this process k times, using a different set each time as the holdout set.\nStep 5: Calculate the average of the k test F1-scores to get the overall test F1-score.'''\n# Below function implements K-Fold cross validation.\n\ndef run_kfold(model, X_train, y_train, N_SPLITS = 10):\n    f1_list = []\n    oofs = np.zeros(len(X_train))\n    folds = StratifiedKFold(n_splits=N_SPLITS)\n    for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        \n        print(f'\\n------------- Fold {i + 1} -------------')\n        X_trn, y_trn = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        model.fit(X_trn, y_trn)\n        # Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = model.predict_proba(X_val)[:,1]\n        \n        fold_f1 = f1_score(y_val, preds_val.round())\n        f1_list.append(fold_f1)\n        \n        print(f'\\nf1 score for validation set is {fold_f1}') \n        \n        oofs[val_idx] = preds_val\n        \n    print(f'\\n----------------------------------')\n    mean_f1 = sum(f1_list)/N_SPLITS\n    print(\"\\nMean validation f1 score :\", mean_f1)\n    \n    oofs_score = f1_score(y_train, oofs.round())\n    print(f'\\nF1 score for oofs is {oofs_score}')\n    return oofs, mean_f1","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"tree\"></a>\n## Decision tree","metadata":{}},{"cell_type":"markdown","source":"### Base model","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier(random_state=1)\nbase_model(tree)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"params = {\n    'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [5, 10, 20, 30, 40, 50],\n    'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n    'max_leaf_nodes': [8, 16, 32, 64, 128,256],\n    'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2},\n                     {0: 1, 1: 3}, {0: 1, 1: 4}]\n}\n\nclf = RandomizedSearchCV(DecisionTreeClassifier(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K Fold - Cross validation","metadata":{}},{"cell_type":"code","source":"clf = DecisionTreeClassifier(random_state = 1,\n                             **search.best_params_)\noofs, mean_f1 = run_kfold(clf, X_train, y_train, N_SPLITS=5)\nresults['model'].append(\"Decision Tree\")\nresults['CV f1-score'].append(mean_f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"logistic\"></a>\n## Logistic Regression\n\n### Base model","metadata":{}},{"cell_type":"code","source":"log = LogisticRegression(random_state=1)\nbase_model(log)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"params = {\n    'penalty': ['l1', 'l2','elasticnet'],\n    'C':[0.0001, 0.001, 0.1, 1, 10, 100,1000],\n    'fit_intercept':[True, False],\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'class_weight':['balanced', None]\n}\n\nclf = RandomizedSearchCV(LogisticRegression(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K Fold - Cross validation","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression(random_state = 1,\n                         **search.best_params_)\n\noofs, mean_f1 = run_kfold(clf, X_train, y_train, N_SPLITS=5)\n\nresults['model'].append(\"Logistic regression\")\nresults['CV f1-score'].append(mean_f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"forest\"></a>\n## Random Forest Classifier\n\n### Base model","metadata":{}},{"cell_type":"code","source":"tree = RandomForestClassifier(random_state=1)\nbase_model(tree)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"params = {'bootstrap': [True, False],\n         'max_depth': [5,10, 20, 30, 50,None],\n         'max_features': ['auto', 'sqrt'],\n         'min_samples_leaf': [1, 2, 4],\n         'min_samples_split': [2, 5, 10],\n         'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2}, {0: 1, 1: 3}],\n         'n_estimators': [50, 100, 200, 300, 500]}\n\nclf = RandomizedSearchCV(RandomForestClassifier(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K fold - Cross validation","metadata":{}},{"cell_type":"code","source":"clf = RandomForestClassifier(random_state = 1,\n                         **search.best_params_)\n\noofs, mean_f1 = run_kfold(clf, X_train, y_train, N_SPLITS=5)\n\nresults['model'].append(\"Random Forest\")\nresults['CV f1-score'].append(mean_f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"final\"></a>\n# Final Model\n","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<b> Random Forest performing better.. Let's evaluate the results.!</b>","metadata":{}},{"cell_type":"code","source":"params = {'n_estimators': 100,\n          'min_samples_split': 5,\n          'min_samples_leaf': 4,\n          'max_features': 'auto',\n          'max_depth': 30, \n          'class_weight': \n          {0: 1, 1: 2}, \n          'bootstrap': True}\n\nfinal_model = RandomForestClassifier(random_state=1,\n                                     **params\n                                    )\nfinal_model.fit(X_train, y_train)\n\ntrain_preds = final_model.predict(X_train)\ntest_preds = final_model.predict(X_test)\n\nprint(\"Train f1 Score :\", f1_score(y_train, train_preds))\nprint(\"Test f1 Score :\", f1_score(y_test, test_preds))  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification report","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test,test_preds,normalize='true')\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False,fmt='.2f')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Thank you..!!\n--- &nbsp;  Ashok kumar","metadata":{}}]}