{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings('always')  \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n\nprint(f'Data Frame Shape (rows, columns): {df.shape}') \ndf.head()\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Analysis and Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue=\"quality\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize outcome of classes\nsns.countplot(data=df, x=\"quality\").set_title(\"Wine quality\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.relplot(data=df, x=\"\", y=\"\", hue=\"quality\", palette=\"bright\", height=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data preparation, balancing and cleanup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the library to import to be able to use random under sampler balancing technique.\nfrom imblearn.under_sampling import RandomUnderSampler\nwarnings.filterwarnings('ignore')\n \n# If you want to know when to balance a data set, just read here:\n# https://stats.stackexchange.com/questions/227088/when-should-i-balance-classes-in-a-training-data-set\n# But basically if you expect your classes to be equally rare, then you have to balance them.\n \n# There are various way to balance a dataset, one basic way is to under sample.\n# You can read more about these balancing techniques here: https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html\n \n# Setup our Under Sampler\nrUnderSampler = RandomUnderSampler(random_state=10) # Actually you can use any number here. This is just a random seed.\n\n# df.drop(columns=\"Species\", axis=0) \n# target = df[\"Species\"]\n# Perform random under sampling. Then pass in the features (the one without the classes nga df) and the target (katong puro classes lang based on my tutorial)\ndfBalancedFeatures, dfBalanceTarget = rUnderSampler.fit_resample(df.drop(columns=\"quality\", axis=0), df[\"quality\"])\n\nprint(f'New Shape of balanced features: {dfBalancedFeatures.shape}')\nprint(f'New Shape of balanced target: {dfBalanceTarget.shape}')\n# Visualize new classes distributions\nsns.countplot(dfBalanceTarget ).set_title('Balanced Data Set')\n \n# Now continue to just use the new balanced data frames on your notebook.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"classifiers set and Build Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perf_measure(actual, prediction):\n    cm = confusion_matrix (actual, prediction)\n    FP = cm.sum(axis=0) - np.diag(cm)  \n    FN = cm.sum(axis=1) - np.diag(cm)\n    TP = np.diag(cm)\n    TN = cm.sum() - (FP + FN + TP)\n\n    return(TP, FP, TN, FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sensitivity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TPR = (TP/(TP+FN)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TPR = (TP/(TP+FN))[1] \n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TPR\n\ndef specificity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TNR = (TN/(TN+FP)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = perf_measure(y_true, y_pred)\n        TNR = (TN/(TN+FP))[1]\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TNR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = {\n            'accuracy':make_scorer(accuracy_score), \n            'precision':make_scorer(precision_score, average='weighted',zero_division='warn'),\n            'f1_score':make_scorer(f1_score, average='weighted'),\n            'recall':make_scorer(recall_score, average='weighted'), \n            'sensitivity':make_scorer(sensitivity_score, mode=\"multiclass\"), \n            'specificity':make_scorer(specificity_score, mode=\"multiclass\"), \n           }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.svm import LinearSVC # Support Vector Machine\n# from sklearn.neighbors import KNeighborsClassifier #K-nearest Neighbors\n# from sklearn.cluster import KMeans #K-means\n\n# Instantiate the machine learning classifiers\nDTClassifier_model = DecisionTreeClassifier()\ngaussianNB_model = GaussianNB()\nLR_model = LogisticRegression(max_iter=10000)\nlinearSVC_model = LinearSVC(dual=False)\n#kNeighbors_model = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def models_evaluation(features, target, folds):    \n    # Perform cross-validation to each machine learning classifier\n    DTClassifier_result = cross_validate(DTClassifier_model, features, target, cv=folds, scoring=scoring)\n    gaussianNB_result = cross_validate(gaussianNB_model, features, target, cv=folds, scoring=scoring)\n    LR_result = cross_validate(LR_model, features, target, cv=folds, scoring=scoring)\n    linearSVC_result = cross_validate(linearSVC_model, features, target, cv=folds, scoring=scoring)\n    \n    \n    models_scores_table = pd.DataFrame({\n      'Decision Tree':[\n                        DTClassifier_result['test_accuracy'].mean(),\n                        DTClassifier_result['test_precision'].mean(),\n                        DTClassifier_result['test_recall'].mean(),\n                        DTClassifier_result['test_sensitivity'].mean(),\n                        DTClassifier_result['test_specificity'].mean(),\n                        DTClassifier_result['test_f1_score'].mean()\n                       ],\n      'Gaussian Naive Bayes':[\n                        gaussianNB_result['test_accuracy'].mean(),\n                        gaussianNB_result['test_precision'].mean(),\n                        gaussianNB_result['test_recall'].mean(),\n                        gaussianNB_result['test_sensitivity'].mean(),\n                        gaussianNB_result['test_specificity'].mean(),\n                        gaussianNB_result['test_f1_score'].mean()\n                              ],\n      'Logistic Regression':[\n                        LR_result['test_accuracy'].mean(),\n                        LR_result['test_precision'].mean(),\n                        LR_result['test_recall'].mean(),\n                        LR_result['test_sensitivity'].mean(),\n                        LR_result['test_specificity'].mean(),\n                        LR_result['test_f1_score'].mean()\n                            ],\n      'Support Vector Classifier':[\n                       linearSVC_result['test_accuracy'].mean(),\n                       linearSVC_result['test_precision'].mean(),\n                       linearSVC_result['test_recall'].mean(),\n                       linearSVC_result['test_sensitivity'].mean(),\n                       linearSVC_result['test_specificity'].mean(),\n                       linearSVC_result['test_f1_score'].mean()\n                                   ],\n         },\n\n      index=['Accuracy', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', ])\n        \n        \n    return(models_scores_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\nevaluationResult = models_evaluation(dfBalancedFeatures, dfBalanceTarget, 5)\nview = evaluationResult\nview = view.rename_axis('Test Type').reset_index() #Add the index names to the column. This will be used for our presentation\n\n# https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n# Re-Organizing our dataframe to fit our view need\nview = view.melt(var_name='Classifier', value_name='Value', id_vars='Test Type')\n# result\nsns.catplot(data=view, x=\"Test Type\", y=\"Value\", hue=\"Classifier\", kind='bar', palette=\"bright\", alpha=0.8, legend=True, height=5, margin_titles=True, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluationResult['Best Score'] = evaluationResult.idxmax(axis=1)\nevaluationResult","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}