{"cells":[{"metadata":{"hide_input":true},"cell_type":"markdown","source":"<h1><center> Predict Quality of Red Wine - Linear Regression Method </center> </h1>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"coverimage.jpg\">"},{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-Wine-Data\" data-toc-modified-id=\"Loading-Wine-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Loading Wine Data</a></span></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#Preparing-Data\" data-toc-modified-id=\"Preparing-Data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Preparing Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-X-and-y\" data-toc-modified-id=\"Create-X-and-y-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Create X and y</a></span></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Train Test Split</a></span></li></ul></li></ul></li><li><span><a href=\"#Analysis\" data-toc-modified-id=\"Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Linear Regression</a></span></li><li><span><a href=\"#Linear-Regression-With-Polynomial-Features\" data-toc-modified-id=\"Linear-Regression-With-Polynomial-Features-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Linear Regression With Polynomial Features</a></span></li><li><span><a href=\"#RidgeCV-Regression\" data-toc-modified-id=\"RidgeCV-Regression-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>RidgeCV Regression</a></span></li><li><span><a href=\"#LassoCV-Regression\" data-toc-modified-id=\"LassoCV-Regression-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>LassoCV Regression</a></span></li><li><span><a href=\"#ElasticNetCV\" data-toc-modified-id=\"ElasticNetCV-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>ElasticNetCV</a></span></li></ul></li><li><span><a href=\"#Next-Steps\" data-toc-modified-id=\"Next-Steps-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Next Steps</a></span></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThe dataset for this project was collected from <a href=\"https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\">kaggle - Red Wine Quality</a>. The data investigated here consists of 11 variables (based on physicochemical tests) and quality of red wine (score between 0 and 10).\n\nMain objective of the analysis is to focus on prediction. In this project, We will employ linear regression algorithms to find relationship between quality of the red wine and other input parameters. We will then choose the best candidate algorithm from preliminary results. The goal with this implementation is to construct a model that accurately predicts quality of the red wine. Here the predictand <i>(y-variable)</i>  is categorical, but for the regression we consider it as discreet numerical values."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, RidgeCV, LassoCV, ElasticNetCV \nfrom sklearn.pipeline import Pipeline\n\n# Mute the sklearn warning about regularization\nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Wine Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data shape: ', data.shape)\nprint(\"Data types: \")\nprint(data.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Data Info:\")\ndata_info = data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The total number of records: ', str(len(data.index)))\nprint('Column names: ', str(data.columns.tolist()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing data in our data set. Quality is a categorical variable from scale 1 to 10. it is our `y-variable` in this project."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Predictor </b>\n\n* Quality: The quality of wine from scale 1 - 10. \n\n<b> Features </b>\n\n* fixed acidity\n* volatile acidity\n* citric acid\n* residual sugar\n* chlorides\n* free sulfur dioxide\n* total sulfur dioxide\n* density\n* pH\n* sulphates\n* alcohol"},{"metadata":{},"cell_type":"markdown","source":"### Preparing Data"},{"metadata":{},"cell_type":"markdown","source":"Let first see the distributions of each variable."},{"metadata":{},"cell_type":"markdown","source":"Plotting a set of histograms:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.hist(figsize=(10, 10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fixed acidity, density, and pH values are normally distributed. Others are positively skewed except quality."},{"metadata":{},"cell_type":"markdown","source":"Let's look at the correlation coefficient. A coefficient close to 1 means that thereâ€™s a very strong positive correlation between the two variables. The diagonal line is the correlation of the variables to themselves, that's why they are 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr(method='pearson')\nfig = plt.subplots(figsize=(10, 10))\nsns.heatmap(corr,\n           xticklabels=corr.columns,\n           yticklabels=corr.columns,\n           cmap='YlOrBr',\n           annot=True,\n           );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* fixed acidity positively correlated with density and citric acid and negatively correlated with pH around 0.67 absolute correlation coeficient and negatively correlate with.\n\n    "},{"metadata":{},"cell_type":"markdown","source":"#### Create X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col = \"quality\"\n\nX = data.drop(y_col, axis=1)\ny = data[y_col]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('X:')\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('y: ')\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    random_state=72018)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply min-max scaler to normalize data. his ensures that each feature is treated equally when applying supervised learners."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train_s = scaler.fit_transform(X_train)\npd.DataFrame(X_train_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis"},{"metadata":{},"cell_type":"markdown","source":"We'll now:\n\n- Train the following models: Vanilla Linear, RidgeCV, LassoCV, ElasticNetCV\n- Compare accuracy scores\n- Compare root-mean square errors\n- Plot the results: prediction vs actual"},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LinearRegression()\nLR = LR.fit(X_train_s, y_train)\ny_train_pred = np.round(LR.predict(X_train_s))\nX_test_s = scaler.transform(X_test)\ny_test_pred = np.round(LR.predict(X_test_s))\n\nprint('r2 score for train data: ', r2_score(y_train.values, y_train_pred))\nprint('r2 score for test data: ', r2_score(y_test.values, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Model coeff': LR.coef_})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression With Polynomial Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"pf = PolynomialFeatures(degree=2, include_bias=False)\nX_pf = pf.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n                                                    random_state=72018)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_s = scaler.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LR.fit(X_train_s, y_train)\ny_train_pred = np.round(LR.predict(X_train_s))\nX_test_s = scaler.transform(X_test)\ny_test_pred = np.round(LR.predict(X_test_s))\n\nprint('r2 score for train data: ', r2_score(y_train.values, y_train_pred))\nprint('r2 score for test data: ', r2_score(y_test.values, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding polynomial features improve the training accuaracy compare to the simple linear regression. But lower the test r2 score."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Model coeff': LR.coef_}).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n\ndef rmse(ytrue, ypredicted):\n    return np.sqrt(mean_squared_error(ytrue, ypredicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinearRegression = LinearRegression().fit(X_train, y_train)\n\nlinearRegression_rmse = rmse(y_test, np.round(linearRegression.predict(X_test)))\n\nprint('linearRegression_rmse: ', linearRegression_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nax.plot(y_test, np.round(linearRegression.predict(X_test)), \n         marker='o', ls='', ms=3.0)\n\nlim = (0, y_test.max())\n\nax.set(xlabel='Actual Quality', \n       ylabel='Predicted Quality', \n       xlim=lim,\n       ylim=lim,\n       title='Linear Regression Results');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RidgeCV Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\nalphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n\nridgeCV = RidgeCV(alphas=alphas, \n                  cv=4).fit(X_train, y_train)\n\nridgeCV_rmse = rmse(y_test, np.round(ridgeCV.predict(X_test)))\n\nprint('ridgeCV.alpha:', ridgeCV.alpha_, 'ridgeCV_rmse: ' ,ridgeCV_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LassoCV Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\nalphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005])\n\nlassoCV = LassoCV(alphas=alphas2,\n                  max_iter=5e4,\n                  cv=3).fit(X_train, y_train)\n\nlassoCV_rmse = rmse(y_test, np.round(lassoCV.predict(X_test)))\n\nprint('lassoCV.alpha',lassoCV.alpha_, 'lassoCV_rmse',lassoCV_rmse)  # Lasso is slower","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can determine how many of these features remain non-zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Of {} coefficients, {} are non-zero with Lasso.'.format(len(lassoCV.coef_), \n                                                               len(lassoCV.coef_.nonzero()[0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNetCV"},{"metadata":{},"cell_type":"markdown","source":"Now try the elastic net, with the same alphas as in Lasso, and l1_ratios between 0.1 and 0.9"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\nl1_ratios = np.linspace(0.1, 0.9, 9)\n\nelasticNetCV = ElasticNetCV(alphas=alphas2, \n                            l1_ratio=l1_ratios,\n                            max_iter=1e4).fit(X_train, y_train)\nelasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))\n\nprint('elasticNetCV.alpha',elasticNetCV.alpha_, 'elasticNetCV.l1_ratio',elasticNetCV.l1_ratio_, 'elasticNetCV_rmse',elasticNetCV_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the RMSE calculation from all models is easiest in a table."},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_vals = [linearRegression_rmse, ridgeCV_rmse, lassoCV_rmse, elasticNetCV_rmse]\n\nlabels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n\nrmse_df = pd.Series(rmse_vals, index=labels).to_frame()\nrmse_df.rename(columns={0: 'RMSE'}, inplace=1)\nrmse_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also make a plot of actual vs predicted wine quality as before."},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nlabels = ['Ridge', 'Lasso', 'ElasticNet']\n\nmodels = [ridgeCV, lassoCV, elasticNetCV]\n\nfor mod, lab in zip(models, labels):\n    ax.plot(y_test, mod.predict(X_test), \n             marker='o', ls='', ms=3.0, label=lab)\n\n\nleg = plt.legend(frameon=True)\nleg.get_frame().set_edgecolor('black')\nleg.get_frame().set_linewidth(1.0)\n\nax.set(xlabel='Actual Quality', \n       ylabel='Predicted Quality', \n       title='Linear Regression Results');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: ElasticNet gives the smallest Root-mean-square error however. The best candidate based on Root-mean-square error and score results is ElasticNet Regression, therefore we recommend ElasticNet as a final model that best fits the data in terms of accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Next Steps\n\nWe could further try optimize ElasticNet using Stochastic gradient descent.\n\nLinear regression has low prediction accuracy. To predict the quality of wine with more accuracy, we could employ classification methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}