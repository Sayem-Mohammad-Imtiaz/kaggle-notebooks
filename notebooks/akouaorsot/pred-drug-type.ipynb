{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import operator as op\nimport random\nrandom.seed(123)\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.svm  import SVC\nimport sklearn.metrics as skm\n\nimport utils_data_prepping as udp\nimport utils_clf_models as clf\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,8)})\n\n\nfrom sklearn.exceptions import ConvergenceWarning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n                        module=\"sklearn\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/drug-classification/drug200.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Drug'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical Encoding \ncat_vars = ['Sex', 'BP', 'Cholesterol']\nfor i in cat_vars:\n    df[i+\"_cat\"] = df[i].astype('category').cat.codes\ndf.drop(cat_vars, axis=1, inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 1. Distribution of target variable","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=df, x='Drug')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Visualization of entire dataset","metadata":{}},{"cell_type":"code","source":"# Masking to show only one side of the matrix\ncorr = np.corrcoef(df.corr())                        \nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Axtual Correlation matrix as a heatmap\nsns.heatmap(df.corr(), annot=True, mask=mask, cmap=\"YlGnBu\")\nplt.show","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Relationship between Drug Cat and Na to Potassium Ration","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=df, x='Na_to_K', hue='Drug')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building & Evaluation","metadata":{}},{"cell_type":"code","source":"# Dictionary with model scores\nmodels = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"class knn():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.2, \n                                                            random_state = 2)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        errors = {}    \n        for k in range(1, 10):\n            model_k = KNeighborsClassifier(n_neighbors=k)\n            scores = cross_val_score(model_k, self.X_train, \n                                     self.Y_train, \n                                     cv=20, scoring='accuracy')\n            errors[k] = scores.mean()\n#         plt.plot(list(errors.keys()), list(errors.values()))\n#         plt.xlabel('Value of K for KNN')\n#         plt.ylabel('Cross-validated accuracy')\n        k_best = max(errors.items(), key=op.itemgetter(1))[0]\n        clf = KNeighborsClassifier(n_neighbors=k_best)\n        clf.fit(self.X_train, self.Y_train)\n        pred = clf.predict(self.X_test)\n        print(skm.classification_report(self.Y_test, pred))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n\nmodel = knn(df, 'Drug').pre_processing()\nmodels['knn'] = model.fit_pred_acc()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'knn')\nclf1.preprocess_split(0.3, 62)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Naive Bayes","metadata":{}},{"cell_type":"code","source":"class nb():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.25, \n                                                            random_state = 9)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = GaussianNB()\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n\nmodel = nb(df, 'Drug')\nmodel = model.pre_processing()\nmodels['nb'] = model.fit_pred_acc() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'guass_nb')\nclf1.preprocess_split(0.3, 142)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Logistic Regression","metadata":{}},{"cell_type":"code","source":"class logreg():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.25, \n                                                            random_state = 16)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = LogisticRegression()\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\nmodel = logreg(df, 'Drug')\nmodel = model.pre_processing()\nmodels['logreg'] = model.fit_pred_acc()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'log_reg')\nclf1.preprocess_split(0.3, 62)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Decision Tree","metadata":{}},{"cell_type":"code","source":"class tree():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.3, \n                                                            random_state = 2)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = DecisionTreeClassifier()\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n    \nmodel = tree(df, 'Drug')\nmodel = model.pre_processing()\nmodels['tree'] = model.fit_pred_acc()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'tree')\nclf1.preprocess_split(0.3, 142)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"class svc():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.15, \n                                                            random_state = 64)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = SVC(gamma='auto')\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n    \nmodel = svc(df, 'Drug')\nmodel = model.pre_processing()\nmodels['svc'] = model.fit_pred_acc()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using utils\n# X, y = udp.pre_processing(df, 'Drug')\n# clf1 = clf.Classifier(X, y, 'svc')\n# clf1.preprocess_split(0.3, 62)\n# clf1.fit_predict()\n# print('For the training set:')\n# clf1.metrics(printing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_df = pd.DataFrame.from_dict(models, orient='index', \n                              columns = ['accuracy'])\nscores_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}