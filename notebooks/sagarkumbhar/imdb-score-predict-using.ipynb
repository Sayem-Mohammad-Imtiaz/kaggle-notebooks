{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/imdb-5000-movie-dataset/movie_metadata.csv')\npd.options.display.max_columns=None\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy             as np\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport statsmodels.api   as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport scipy.stats as stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.spatial import distance\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nimport io\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAs the given columns are not relevant we drop the same 'color','director_name','actor_2_name','genres','actor_1_name','movie_title','actor_3_name','plot_keywords', 'movie_imdb_link','language','country"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['color','director_name','actor_2_name','genres','actor_1_name','movie_title','actor_3_name',\n         'plot_keywords','movie_imdb_link','language','country'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('title_year',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.content_rating.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.content_rating.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.content_rating=df['content_rating'].fillna('R')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.content_rating.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Tests"},{"metadata":{},"cell_type":"markdown","source":"### 1) Checking whether column content rating is significant or not.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats             import ttest_1samp,ttest_ind,chi2_contingency,chisquare, f_oneway, levene, bartlett, mannwhitneyu, normaltest,shapiro","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_stat, p_val = stats.shapiro(df['imdb_score'])\nshap_stat, p_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here p_value is smaller than 0.05 so we fail to reject null hypothesis.                                         \nHence we can conclude that imdb_score contains data that is not normal.                                    \nWe use manwhitneyu test for checking if the content rating column is significant or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"# mannwhitneyu for these two:\nfrom statsmodels.formula.api import ols\nmod = ols('imdb_score ~ content_rating', data = df).fit()\naov_table = sm.stats.anova_lm(mod, typ=2)\nprint(aov_table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As p_value is < 0.05 we reject null hypothesis.                                                 \nH0- Content_rating is not significant                                                    \nH1- Content_rating is significant"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdummy= pd.get_dummies(df['content_rating'], prefix='content_rating', drop_first=True).reset_index(drop=True)\ndfdummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.concat([df, dfdummy], axis=1)\ndf.drop('content_rating',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df:\n    df[i]=df[i].fillna(df[i].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df[['num_critic_for_reviews','duration','director_facebook_likes','actor_3_facebook_likes','actor_1_facebook_likes','gross'\n       ,'num_voted_users','facenumber_in_poster','cast_total_facebook_likes','num_user_for_reviews','budget','actor_2_facebook_likes'\n       ,'aspect_ratio','movie_facebook_likes']]\nfor i in df1:\n    sns.distplot(df[i])\n    plt.show()\n    print('KDE plot for ',i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From this we understand that all the columns except imdb_rating are not normal and are right skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df1:\n    sns.boxplot(df[i])\n    plt.show()\n    print('KDE plot for ',i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers present in almost all the independent variables.                                             \nTo treat these outliers we apply log to each independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df1:\n    df1[i] = df1[i].map(lambda i: np.log1p(i)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['facenumber_in_poster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nsns.boxplot(x=df['facenumber_in_poster'],y=df['imdb_score'])\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Dropping all the insignificant columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['num_critic_for_reviews','duration','director_facebook_likes','actor_3_facebook_likes','actor_1_facebook_likes','gross'\n       ,'num_voted_users','cast_total_facebook_likes','num_user_for_reviews','budget','actor_2_facebook_likes'\n       ,'aspect_ratio','facenumber_in_poster','movie_facebook_likes'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new=pd.concat([df,df1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df_new.drop('imdb_score',axis=1)\ny=df_new['imdb_score']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using standardscaler() to bring all the attributes to same scale."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX_scaled = sc.fit_transform(X)\n\nX = pd.DataFrame(X_scaled)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.ensemble import BaggingRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#declare the models\nlr  = LinearRegression()\nRF  = RandomForestRegressor(n_estimators= 49, random_state=1)\nknn = KNeighborsRegressor(n_neighbors= 10, weights= 'distance')\ndt  = DecisionTreeRegressor(max_depth= 6, min_samples_leaf= 13)\n\n\n#create a list of models\nmodels=[lr,RF ,knn, dt]\n\ndef score_model(xtrain,ytrain,xtest,ytest):\n    mod_columns=[]\n    mod=pd.DataFrame(columns=mod_columns)\n    i=0\n    #read model one by one\n    for model in models:\n        model.fit(xtrain,ytrain)\n        y_pred=model.predict(xtest)\n        \n        \n        \n        \n        #compute metrics\n        train_accuracy=model.score(xtrain,ytrain)\n        test_accuracy=model.score(xtest,ytest)\n        \n        #insert in dataframe\n        mod.loc[i,\"Model_Name\"]=model.__class__.__name__\n        mod.loc[i,\"Train_Accuracy\"]=round(train_accuracy,2)\n        mod.loc[i,\"Test_Accuracy\"]=round(test_accuracy,2)\n        \n        i+=1\n\n    \n    return(mod)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor, VotingRegressor\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Base models"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR=LinearRegression()\nkNN = KNeighborsRegressor()\n\nDT = DecisionTreeRegressor( random_state=0)\nRF = RandomForestRegressor( random_state=0)\nGBoost = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[]\nmodels.append(('LR',LR))\n\nmodels.append(('DT',DT))\n\nmodels.append(('RF',RF))\n\nmodels.append(('KNN', kNN))\n\n\nimport sklearn.model_selection as model_selection\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(shuffle=True, n_splits=7, random_state=0)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error' ) # fit, train, predict\n    results.append(np.sqrt(np.abs(cv_results)))    # negative mean squared error \n    names.append(name)\n    print('%s: %f (%f)'% (name, np.mean(np.sqrt(np.abs(cv_results))), np.var(np.sqrt(np.abs(cv_results)),ddof=1)))\n    \n# boxplot algorithm comparision\nfig = plt.figure()\nfig.suptitle('Algorithm Comparision')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as smf\nimport sklearn.model_selection as model_selection","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using N-Estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"#RF\nrmse_rf= []\nfor n_e in np.arange(1,50):\n    RF=RandomForestRegressor(n_estimators=n_e,random_state=0,criterion='mae')\n    kfold = model_selection.KFold(shuffle=True, n_splits=7, random_state=0)\n    mse = model_selection.cross_val_score(RF, X, y, cv=kfold, scoring='neg_mean_squared_error' )\n    rmse_rf.append(np.var(np.sqrt(np.abs(mse)), ddof=1))\nprint(np.argmin(rmse_rf))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bias error\n\nrmse_be= []\nfor n_e in np.arange(1,50):\n    RF=RandomForestRegressor(n_estimators=n_e,random_state=0,criterion='mae')\n    kfold = model_selection.KFold(shuffle=True, n_splits=7, random_state=0)\n    mse = model_selection.cross_val_score(RF, X, y, cv=kfold, scoring='neg_mean_squared_error' )\n    rmse_be.append(np.mean(np.sqrt(np.abs(mse))))\nprint(rmse_be)\nprint(\"n_estimatr\", np.argmin(rmse_be))\nprint('lowest be', np.min(rmse_be))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This gives the lowest bias error that is at 49th iteration."},{"metadata":{"trusted":true},"cell_type":"code","source":"RF=RandomForestRegressor(n_estimators=49,random_state=0,criterion='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_GB= []\n\nfor n_e in np.arange(1,30):\n    GBoost=GradientBoostingRegressor(n_estimators =n_e, random_state=0)\n    kfold = model_selection.KFold(shuffle=True, n_splits=7, random_state=0)\n    mse = model_selection.cross_val_score(GBoost, X, y, cv=kfold, scoring='neg_mean_squared_error' )\n    rmse_GB.append(np.mean(np.sqrt(np.abs(mse))))\nprint(rmse_GB)\nprint(np.argmin(rmse_GB))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The lowest bias error is at 29th position."},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN=KNeighborsRegressor(n_neighbors=3,weights='distance')\nDT=DecisionTreeRegressor(max_depth=5,min_samples_leaf=7,criterion='mae', random_state=0)\nRF=RandomForestRegressor(n_estimators=41,random_state=0,criterion='mae')\n\n\nGBoost=GradientBoostingRegressor(n_estimators=29)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[]\nmodels.append(('DT',DT))\n\nmodels.append(('RF',RF))\n\nmodels.append(('KNN', KNN))\n\nmodels.append(('GBoost', GBoost))\n\nimport sklearn.model_selection as model_selection\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(shuffle=True, n_splits=7, random_state=0)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error' ) # fit, train, predict\n    results.append(np.sqrt(np.abs(cv_results)))    # negative mean squared error \n    names.append(name)\n    print('%s: %f (%f)'% (name, np.mean(np.sqrt(np.abs(cv_results))), np.var(np.sqrt(np.abs(cv_results)),ddof=1)))\n    \n    # boxplot algorithm comparision\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparision')\n    ax = fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(names)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model in turn\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(shuffle=True, n_splits=7, random_state=0)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error' ) # fit, train, predict\n    results.append(np.sqrt(np.abs(cv_results)))    # negative mean squared error \n    names.append(name)\n    print('%s: %f (%f)'% (name, np.mean(np.sqrt(np.abs(cv_results))), np.var(np.sqrt(np.abs(cv_results)),ddof=1)))\n    \n    # boxplot algorithm comparision\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparision')\n    ax = fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(names)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from above observations, we can conclude that Randomn Forest is the best model with \n### bias error= 0.803170 \n### and variance error= (0.001632)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}