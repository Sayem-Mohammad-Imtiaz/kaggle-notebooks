{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlp imports\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Models and Layers imports\nimport tensorflow as tf\nfrom keras.layers import Dense,Embedding,LSTM,GRU\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntestdf = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nprint(\"Train Data: \",df.shape)\nprint(\"Test Data: \",testdf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A bit Analysis\ninspired by: [https://www.kaggle.com/utcarshagrawal/nlp-model-including-eda-and-data-cleaning](https://www.kaggle.com/utcarshagrawal/nlp-model-including-eda-and-data-cleaning). Do check it out for more details!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of Disaster/ Non Disaster tweet\ntmp = df.groupby('target').count()['text']\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target Distribution by Keywords\ntmp = pd.DataFrame()\ntmp['target_mean'] = df.groupby('keyword')['target'].transform('mean')\ntmp['keyword'] = df['keyword']\ntmp['target'] = df['target']\n\nfig = plt.figure(figsize=(8, 78), dpi=100)\n\nsns.countplot(y=tmp.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=tmp.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Max Length of the tweet in both target\ntmp = pd.DataFrame()\ntmp['len'] = df['text'].str.split().map(lambda x : len(x))\ntmp['target'] = df['target']\nprint(\"For 1: \", max(tmp[tmp['target']==1]['len']))\nprint(\"For 0: \", max(tmp[tmp['target']==0]['len']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's deal with Nan in keyword and Location column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillna(df):\n    #reuse them for any new data\n    for col in ['keyword','location']:\n        df[col] = df[col].fillna(f'no_{col}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fillna(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initially Text contains both lower and upper cases with stop words and characters that are not required. We are going to preprocess them a bit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def removetags(df):\n    df['text'] = df['text'].str.replace('https?://\\S+|www\\.\\S+','').str.replace('<.*?>','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocesstweet(text):\n    emoji = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji.sub(r'',text)\n    \n    table = str.maketrans('','',string.punctuation)\n    text = text.translate(table)\n    \n    text = re.sub('\\s+', ' ', text).strip() \n    text = text.lower()\n    new = ''\n    stop_words = set(stopwords.words(\"english\"))\n    for w in word_tokenize(text):\n        if w not in stop_words:\n            new += w\n            new += ' '\n    return new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def applypre(df):\n    removetags(df)\n    df['text'] = df['text'].apply(lambda x: preprocesstweet(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"applypre(df)\napplypre(testdf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data is Prepared and we can use text column to train our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimension of the embedding vector\nend_dim = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the Data Embedding matrix\nlines = list(df['text'].values)\ntweets = list()\nfor line in lines:\n    tweets.append(word_tokenize(line))\nprint(tweets[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(sentences=tweets,\n                size = end_dim,\n                window = 5,\n                workers = 4,\n                min_count = 1)\nwords = list(model.wv.vocab)\nprint(len(words))\nwords[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = \"/kaggle/working/wv.txt\"\nmodel.wv.save_word2vec_format(fname,binary=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat wv.txt | head -2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emd_idx = {}\nf = open(\"/kaggle/working/wv.txt\",encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coef = values[1:]\n    emd_idx[word] = coef\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tkn = Tokenizer()\ntkn.fit_on_texts(tweets)\nseq = tkn.texts_to_sequences(tweets)\n\nword_idx = tkn.word_index #index of words table  for the tweets\ntweet_pad = pad_sequences(seq,maxlen=31) #we saw this above","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createrunnableinput(df):\n    #for final test data predictions\n    lines = list(df['text'].values)\n    tweets = list()\n    for line in lines:\n        tweets.append(word_tokenize(line))\n    tkn = Tokenizer()\n    tkn.fit_on_texts(tweets)\n    seq = tkn.texts_to_sequences(tweets)\n\n    word_idx = tkn.word_index #index of words table  for the tweets\n    tweet_pad = pad_sequences(seq,maxlen=31) #we saw this above\n    \n    return tweet_pad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_idx)+1\nemd_matrix = np.zeros((num_words,end_dim))\nfor word, i in word_idx.items():\n    if i>num_words:\n        continue\n    emd_vector = emd_idx[word]\n    if emd_vector is not None:\n        emd_matrix[i] = emd_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our Embedding Matrix is ready. we can use this one in the Embedding layer for the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nemd_layer = Embedding(num_words,\n                     end_dim,\n                     embeddings_initializer=Constant(emd_matrix),\n                     input_length = 31,\n                     trainable = False)\nmodel.add(emd_layer)\nmodel.add(GRU(units = 32,dropout=0.2,recurrent_dropout=0.2))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss = \"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_split = 0.8\nindices = np.arange(tweet_pad.shape[0])\nnp.random.shuffle(indices)\n\ntweet_pad = tweet_pad[indices]\ntarget = target[indices]\n\ntrain_samples = int(tweet_pad.shape[0]*train_split)\n\nXtrain = tweet_pad[:train_samples]\nytrain = target[:train_samples]\n\nXval = tweet_pad[train_samples:]\nyval = target[train_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(Xtrain,ytrain,batch_size=32,epochs=20,validation_data=(Xval,yval))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtest = createrunnableinput(testdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = model.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame()\nsubmit['id'] = testdf['id']\nsubmit['target'] = ypred\nsubmit['target'] = submit['target'].apply(lambda x: 0 if x<0.5 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}