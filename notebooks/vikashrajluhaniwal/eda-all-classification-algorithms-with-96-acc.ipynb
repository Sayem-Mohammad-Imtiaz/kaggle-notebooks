{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](# 1.-Importing-necessary-libraries)   \n2. [**Loading data**](#2.-Loading-data)  \n3. [**Data preprocessing**](#3.-Data-preprocessing)  \n    3.a [**Checking for duplicates **](#3.a-Checking-for-duplicates)  \n    3.b [**Checking for missing values**](#3.b-Checking-for-missing-values)  \n    3.c [**Checking for class imbalance**](#3.c-Checking-for-class-imbalance)  \n4. [**Exploratory Data Analysis**](#4.-Exploratory-Data-Analysis)  \n    4.a [**Analysing tBodyAccMag-mean feature**](#4.a-Analysing-tBodyAccMag-mean-feature)  \n    4.b [**Analysing Angle between X-axis and gravityMean feature**](#4.b-Analysing-Angle-between-X-axis-and-gravityMean-feature)  \n    4.c [**Analysing Angle between Y-axis and gravityMean feature**](#4.c-Analysing-Angle-between-Y-axis-and-gravityMean-feature)   \n    4.d [**Visualizing data using t-SNE**](#4.d-Visualizing-data-using-t-SNE)\n5. [**Headline based similarity on new articles**](#6.-Headline-based-similarity-on-new-articles)  \n    5.a [**Logistic regression model with Hyperparameter tuning and cross validation**](#5.a-Logistic-regression-model-with-Hyperparameter-tuning-and-cross-validation)  \n    5.b [**Linear SVM model with Hyperparameter tuning and cross validation**](#5.b-Linear-SVM-model-with-Hyperparameter-tuning-and-cross-validation)  \n    5.c [**Kernel SVM model with Hyperparameter tuning and cross validation**](#5.c-Kernel-SVM-model-with-Hyperparameter-tuning-and-cross-validation)   \n    5.d [**Decision tree model with Hyperparameter tuning and cross validation**](#5.d-Decision-tree-model-with-Hyperparameter-tuning-and-cross-validation)  \n    5.e [**Random forest model with Hyperparameter tuning and cross validation**](#5.e-Random-forest-model-with-Hyperparameter-tuning-and-cross-validation)  "},{"metadata":{},"cell_type":"markdown","source":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/"},{"metadata":{},"cell_type":"markdown","source":"### 1. Importing necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Loading data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/human-activity-recognition-with-smartphones/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/human-activity-recognition-with-smartphones/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data preprocessing"},{"metadata":{},"cell_type":"markdown","source":"#### 3.a Checking for duplicates "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of duplicates in train : ',sum(train.duplicated()))\nprint('Number of duplicates in test : ', sum(test.duplicated()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.b Checking for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of missing values in train : ', train.isna().values.sum())\nprint('Total number of missing values in train : ', test.isna().values.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.c Checking for class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.title('Barplot of Activity')\nsns.countplot(train.Activity)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is almost same number of observations across all the six activities so this data does not have class imbalance problem. "},{"metadata":{},"cell_type":"markdown","source":"### 4. Exploratory Data Analysis\n\nBased on the common nature of activities we can broadly put them in two categories.\n- **Static and dynamic activities : **\n    - SITTING, STANDING, LAYING can be considered as static activities with no motion involved\n    - WALKING, WALKING_DOWNSTAIRS, WALKING_UPSTAIRS can be considered as dynamic activities with significant amount of motion involved    \n    \nLet's consider **tBodyAccMag-mean()** feature to differentiate among these two broader set of activities.\n\nIf we try to build a simple classification model to classify the **activity** using one variable at a time then probability density function(PDF) is very helpful to assess importance of a continuous variable."},{"metadata":{},"cell_type":"markdown","source":"#### 4.a Analysing tBodyAccMag-mean feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"facetgrid = sns.FacetGrid(train, hue='Activity', height=5,aspect=3)\nfacetgrid.map(sns.distplot,'tBodyAccMag-mean()', hist=False).add_legend()\nplt.annotate(\"Static Activities\", xy=(-.996,21), xytext=(-0.9, 23),arrowprops={'arrowstyle': '-', 'ls': 'dashed'})\nplt.annotate(\"Static Activities\", xy=(-.999,26), xytext=(-0.9, 23),arrowprops={'arrowstyle': '-', 'ls': 'dashed'})\nplt.annotate(\"Static Activities\", xy=(-0.985,12), xytext=(-0.9, 23),arrowprops={'arrowstyle': '-', 'ls': 'dashed'})\nplt.annotate(\"Dynamic Activities\", xy=(-0.2,3.25), xytext=(0.1, 9),arrowprops={'arrowstyle': '-', 'ls': 'dashed'})\nplt.annotate(\"Dynamic Activities\", xy=(0.1,2.18), xytext=(0.1, 9),arrowprops={'arrowstyle': '-', 'ls': 'dashed'})\nplt.annotate(\"Dynamic Activities\", xy=(-0.01,2.15), xytext=(0.1, 9),arrowprops={'arrowstyle': '-', 'ls': 'dashed'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the above density plot we can easily come with a condition to seperate static activities from dynamic activities.\n\n``` \nif(tBodyAccMag-mean()<=-0.5):\n    Activity = \"static\"\nelse:\n    Activity = \"dynamic\"\n```\n\nLet's have a more closer view on the PDFs of each activity under static and dynamic categorization."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.title(\"Static Activities(closer view)\")\nsns.distplot(train[train[\"Activity\"]==\"SITTING\"]['tBodyAccMag-mean()'],hist = False, label = 'Sitting')\nsns.distplot(train[train[\"Activity\"]==\"STANDING\"]['tBodyAccMag-mean()'],hist = False,label = 'Standing')\nsns.distplot(train[train[\"Activity\"]==\"LAYING\"]['tBodyAccMag-mean()'],hist = False, label = 'Laying')\nplt.axis([-1.02, -0.5, 0, 35])\nplt.subplot(1,2,2)\nplt.title(\"Dynamic Activities(closer view)\")\nsns.distplot(train[train[\"Activity\"]==\"WALKING\"]['tBodyAccMag-mean()'],hist = False, label = 'Sitting')\nsns.distplot(train[train[\"Activity\"]==\"WALKING_DOWNSTAIRS\"]['tBodyAccMag-mean()'],hist = False,label = 'Standing')\nsns.distplot(train[train[\"Activity\"]==\"WALKING_UPSTAIRS\"]['tBodyAccMag-mean()'],hist = False, label = 'Laying')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The insights obtained through density plots can also be represented using Box plots.\nLet's plot the boxplot of Body Accelartion Magnitude mean(tBodyAccMag-mean()) across all the six categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Activity', y='tBodyAccMag-mean()',data=train, showfliers=False)\nplt.ylabel('Body Acceleration Magnitude mean')\nplt.title(\"Boxplot of tBodyAccMag-mean() column across various activities\")\nplt.axhline(y=-0.7, xmin=0.05,dashes=(3,3))\nplt.axhline(y=0.020, xmin=0.35, dashes=(3,3))\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using boxplot again we can come with conditions to seperate static activities from dynamic activities.\n\n``` \nif(tBodyAccMag-mean()<=-0.8):\n    Activity = \"static\"\nif(tBodyAccMag-mean()>=-0.6):\n    Activity = \"dynamic\"\n``` \n\nAlso, we can easily seperate WALKING_DOWNSTAIRS activity from others using boxplot.\n\n``` \nif(tBodyAccMag-mean()>0.02):\n    Activity = \"WALKING_DOWNSTAIRS\"\nelse:\n    Activity = \"others\"\n```\n\nBut still 25% of WALKING_DOWNSTAIRS observations are below 0.02 which are misclassified as **others** so this condition makes an error of 25% in classification."},{"metadata":{},"cell_type":"markdown","source":"#### 4.b Analysing Angle between X-axis and gravityMean feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Activity', y='angle(X,gravityMean)', data=train, showfliers=False)\nplt.axhline(y=0.08, xmin=0.1, xmax=0.9,dashes=(3,3))\nplt.ylabel(\"Angle between X-axis and gravityMean\")\nplt.title('Box plot of angle(X,gravityMean) column across various activities')\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the boxplot we can observe that angle(X,gravityMean) perfectly seperates LAYING from other activities.\n``` \nif(angle(X,gravityMean)>0.01):\n    Activity = \"LAYING\"\nelse:\n    Activity = \"others\"\n```"},{"metadata":{},"cell_type":"markdown","source":"#### 4.c Analysing Angle between Y-axis and gravityMean feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.boxplot(x='Activity', y='angle(Y,gravityMean)', data = train, showfliers=False)\nplt.ylabel(\"Angle between Y-axis and gravityMean\")\nplt.title('Box plot of angle(Y,gravityMean) column across various activities')\nplt.xticks(rotation = 90)\nplt.axhline(y=-0.35, xmin=0.01, dashes=(3,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, using Angle between Y-axis and gravityMean we can seperate LAYING from other activities but again it leads to some misclassification error.  "},{"metadata":{},"cell_type":"markdown","source":"### 4.d Visualizing data using t-SNE"},{"metadata":{},"cell_type":"markdown","source":"Using t-SNE data can be visualized from a extremely high dimensional space to a low dimensional space and still it retains lots of actual information.\nGiven training data has 561 unqiue features, using t-SNE let's visualize it to a 2D space."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_for_tsne = train.drop(['subject', 'Activity'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntsne = TSNE(random_state = 42, n_components=2, verbose=1, perplexity=50, n_iter=1000).fit_transform(X_for_tsne)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.scatterplot(x =tsne[:, 0], y = tsne[:, 1], hue = train[\"Activity\"],palette=\"bright\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the two new components obtained through t-SNE we can visualize and seperate all the six activities in a 2D space. "},{"metadata":{},"cell_type":"markdown","source":"### 5. ML models"},{"metadata":{},"cell_type":"markdown","source":"#### Getting training and test data ready"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train.Activity\nX_train = train.drop(['subject', 'Activity'], axis=1)\ny_test = test.Activity\nX_test = test.drop(['subject', 'Activity'], axis=1)\nprint('Training data size : ', X_train.shape)\nprint('Test data size : ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.a Logistic regression model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C':np.arange(10,61,10), 'penalty':['l2','l1']}\nlr_classifier = LogisticRegression()\nlr_classifier_rs = RandomizedSearchCV(lr_classifier, param_distributions=parameters, cv=5,random_state = 42)\nlr_classifier_rs.fit(X_train, y_train)\ny_pred = lr_classifier_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\nprint(\"Accuracy using Logistic Regression : \", lr_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to plot confusion matrix\ndef plot_confusion_matrix(cm,lables):\n    fig, ax = plt.subplots(figsize=(12,8)) # for plotting confusion matrix as image\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n    yticks=np.arange(cm.shape[0]),\n    xticklabels=lables, yticklabels=lables,\n    ylabel='True label',\n    xlabel='Predicted label')\n    plt.xticks(rotation = 90)\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, int(cm[i, j]),ha=\"center\", va=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values,y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))  # plotting confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to get best random search attributes\ndef get_best_randomsearch_results(model):\n    print(\"Best estimator : \", model.best_estimator_)\n    print(\"Best set of parameters : \", model.best_params_)\n    print(\"Best score : \", model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting best random search attributes\nget_best_randomsearch_results(lr_classifier_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.b Linear SVM model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C':np.arange(1,12,2)}\nlr_svm = LinearSVC(tol=0.00005)\nlr_svm_rs = RandomizedSearchCV(lr_svm, param_distributions=parameters,random_state = 42)\nlr_svm_rs.fit(X_train, y_train)\ny_pred = lr_svm_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_svm_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\nprint(\"Accuracy using linear SVM : \",lr_svm_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values,y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred)) # plotting confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting best random search attributes\nget_best_randomsearch_results(lr_svm_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.c Kernel SVM model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linspace(2,22,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C':[2,4,8,16],'gamma': [0.125, 0.250, 0.5, 1]}\nkernel_svm = SVC(kernel='rbf')\nkernel_svm_rs = RandomizedSearchCV(kernel_svm,param_distributions=parameters,random_state = 42)\nkernel_svm_rs.fit(X_train, y_train)\ny_pred = kernel_svm_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_svm_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\nprint(\"Accuracy using Kernel SVM : \", kernel_svm_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values,y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred)) # plotting confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting best random search attributes\nget_best_randomsearch_results(kernel_svm_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.d Decision tree model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nparameters = {'max_depth':np.arange(2,10,2)}\ndt_classifier = DecisionTreeClassifier()\ndt_classifier_rs = RandomizedSearchCV(dt_classifier,param_distributions=parameters,random_state = 42)\ndt_classifier_rs.fit(X_train, y_train)\ny_pred = dt_classifier_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\nprint(\"Accuracy using Decision tree : \", dt_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values,y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred)) # plotting confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting best random search attributes\nget_best_randomsearch_results(dt_classifier_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.e Random forest model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nparams = {'n_estimators': np.arange(20,101,10), 'max_depth':np.arange(2,16,2)}\nrf_classifier = RandomForestClassifier()\nrf_classifier_rs = RandomizedSearchCV(rf_classifier, param_distributions=params,random_state = 42)\nrf_classifier_rs.fit(X_train, y_train)\ny_pred = rf_classifier_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\nprint(\"Accuracy using Random forest : \", rf_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values,y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred)) # plotting confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting best random search attributes\nget_best_randomsearch_results(rf_classifier_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nIn this kernel we built multiple different models using various classification algorithms. The accuracy obtained through these models is as follows - \n\n|  Logistic  |  Linear SVM  |  Kernel SVM  |  Decision Trees  | Random Forest |\n|------|------|------|------|------|\n|  96.20 | 96.84| 94.16 | 85.34 | 90.32 |"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}