{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Classification of heart disease data set using Decision Tree and Random Forest\n# Attributes in the dataset\n#age age in years\n#sex(1 = male; 0 = female)\n#cp chest pain type\n#trestbps resting blood pressure (in mm Hg on admission to the hospital)\n#chol  serum cholestoral in mg/dl\n#fbs(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n#restecg  resting electrocardiographic results\n#thalach  maximum heart rate achieved\n#exang  exercise induced angina (1 = yes; 0 = no)\n#oldpeak  ST depression induced by exercise relative to rest\n#slope  the slope of the peak exercise ST segment\n#ca  number of major vessels (0-3) colored by flourosopy\n# thal  3 = normal; 6 = fixed defect; 7 = reversable defect\n# import os\n# print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 1.1 Call data manipulation libraries\nimport pandas as pd\nimport numpy as np\npd.options.display.max_columns = 300\n# 1.2 Feature creation libraries\nfrom sklearn.random_projection import SparseRandomProjection as sr  # Projection features\nfrom sklearn.cluster import KMeans                    # Cluster features\nfrom sklearn.preprocessing import PolynomialFeatures  # Interaction features\n\n# 1.3 For feature selection\n# Ref: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria\n\n# 1.4 Data processing\n# 1.4.1 Scaling data in various manner\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n# 1.4.2 Transform categorical (integer) to dummy\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 1.5 Splitting data\nfrom sklearn.model_selection import train_test_split\n\n# 1.6 Decision tree modeling\n# http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n# http://scikit-learn.org/stable/modules/tree.html#tree\nfrom sklearn.tree import  DecisionTreeClassifier as dt\n\n# 1.7 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\n# 1.8 Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1.9 Misc\nimport os, time, gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6522d18357bc1a3e812c348fc89971e789dc89e6"},"cell_type":"code","source":"# 2 Read health.csv  into a dataframe df\ndf = pd.read_csv(\"../input/heart.csv\")\ndf.shape                            # 303 rows and 14 columns\ndf.head(10)                         # all rows with target = 1 at the top\ndf.tail(10)                         # all rows with target = 0 at the bottom\ndf.columns\ndf.describe()\ndf.columns\ndf.dtypes                           # all columns except for oldpeak is int. oldpeak is float\ndf.dtypes.value_counts()            #\ndf.target.value_counts()            # target is binary has 0 or 1 at 54%-45% \ndf.fbs.value_counts()\ndf.restecg.value_counts()\ndf.ca.value_counts()\n# 3 Check for missing values\ndf.isnull().sum().sum()             # none; sum is zero\ndf.isna().sum().sum()               # none; sum is zero\n\n# 3.3 Shuffle data as the target is all 0s at the beginning and all 1s at the end  of the input data\ndf = df.sample(frac = 1)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37840a3630ac10e787e6c0f59c13aa702e7fd508"},"cell_type":"code","source":"############################ BB. Feature Engineering #########################\n############################################################################\n############################ Using Statistical Numbers #####################\n#  4. Feature 1: Row sums of features 1:14. \n\ndf['sum'] = df.sum(numeric_only = True, axis=1)  # numeric_only= None is default\n# 5. create other statistical features\n#   \n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    df[i] = df.aggregate(i,  axis =1)\n\n\n# 6. Keep target feature separately\ntarget = df['target']\ntarget.tail(2)\n\n# 7.1 drop 'target' column from df\ndf.drop(columns = ['target'], inplace = True)\ndf.shape                # 303 X 20\n\n\n# 7.2. Store column names of our data in colNames\n\ncolNames = df.columns.values\ncolNames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a06247188ddb8ef883a8383d6144c3ae6a4c5f0"},"cell_type":"code","source":"############################################################################\n################ Feature creation Using Random Projections ##################\n# # 8. Generate features using random projections\ndf.shape                          #303 X 20\n\n# 12.2 Transform df to numpy array\n#      Henceforth we will work with array only\ntmp = df.values\ntmp.shape                         # (303, 20)\n\n# 13. Let us create 10 random projections/columns\n#     This decision, at present, is arbitrary\nNUM_OF_COM = 5\n\n# 13.1 Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)\n\n# 13.2 fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\nrp = rp_instance.fit_transform(tmp[:, :13])\n\n# 13.3 Look at some features\nrp[: 5, :  3]\n\n# 13.4 Create some column names for these columns\nrp_col_names = [\"r\" + str(i) for i in range(5)]\nrp_col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"baaa7625bdad75f200ab882350228edcf0a366ca"},"cell_type":"code","source":"###############################################################################\n############################ Feature creation using kmeans ####################\n\n# 14. Before clustering, scale data\n# 14.1 Create a StandardScaler instance\nse = StandardScaler()\n# 14.2 fit() and transform() in one step\ntmp = se.fit_transform(tmp)\n# 14.3\ntmp.shape               # 303 X 20 (an ndarray)\n\n\n# 16. Perform kmeans using 93 features.\n#     No of centroids is no of classes in the 'target'\ncenters = target.nunique()    # 2 unique classes\ncenters               # 2\n\n# 17.1 Begin clustering\nstart = time.time()\n\n# 17.2 First create object to perform clustering\nkmeans = KMeans(n_clusters=centers, # How many\n                n_jobs = 2)         # Parallel jobs for n_init\n\n\n\n# 17.3 Next train the model on the original data only\nkmeans.fit(tmp[:, : 13])\n\nend = time.time()\n(end-start)/60.0      # 1 minute\n\n# 18 Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size   # 303","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11af73f1daaa7e0d0df84fec50620fdf842cd882"},"cell_type":"code","source":"# 18. Cluster labels are categorical. So convert them to dummy\n\n# 18.1 Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)\n\n# 19.2 Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     # reshape(-1,1) recommended by fit()\n                                          # '-1' is a placeholder for actual\n# 19.3 Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    # 303 X 2 (as many as there are classes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd02f1dc73e7ea19410ddb9edc3e62cfa96fc508"},"cell_type":"code","source":"# 19.4 We will use the following as names of new nine columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4343bf64674a65b1076581d3ef46450ffdf21d7"},"cell_type":"code","source":"############################ Interaction features #######################\n# 21. Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )\n\n\n# 21.1 Consider only first 5 features\n#      fit and transform\npoly_features =  poly.fit_transform(tmp[:, : 5])\n\n\npoly_features.shape     # 303 X 15\n\n\n# 21.2 Generate some names for these 15 columns\npoly_names = [ \"poly\" + str(i)  for i in range(15)]\npoly_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d71f9f0a464016538996aaa70012e258518c5ed4"},"cell_type":"code","source":"################# concatenate all features now ##############################\n\n# 22 Append now all generated features together\n# 22 Append random projections, kmeans and polynomial features to tmp array\n\ntmp.shape          # 303 X 20\n\n#  22.1 If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, poly_features])\nelse:\n    tmp = np.hstack([tmp,rp, poly_features])       # No kmeans      <==\n\n\ntmp.shape         #  (303 X 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cc5b1e48bc5a557706298157f2b2a1f25b10821"},"cell_type":"code","source":"################## Model building -Decision Tree #####################\n# 23. Split train into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    tmp,\n                                                    target,\n                                                    test_size = 0.3)\n\n# 23.1\nX_train.shape    # 212 X 42  \nX_test.shape     # 91 X 42; \n\n\n# 24 Decision tree classification\n# 24.1 Create an instance of class\nclf = dt(min_samples_split = 5,\n         min_samples_leaf= 5\n        )\n\n\n\nstart = time.time()\n# 24.2 Fit/train the object on training data\n#      Build model\nclf = clf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60                     # 0.0001824 minute\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e577d51cbe632699ae7c19395af397985fdad22"},"cell_type":"code","source":"# 24.3 Use model to make predictions\nclasses = clf.predict(X_test)\n\n# 24.4 Check accuracy\n(classes == y_test).sum()/y_test.size      # 74.72%\n\n### Model Building Random Forest\n\n# 25. Instantiate RandomForest classifier\nclf = rf(n_estimators=50)\n\n# 25.1 Fit/train the object on training data\n#      Build model\n\nstart = time.time()\nclf = clf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60\n\n# 25.2 Use model to make predictions\nclasses = clf.predict(X_test)\n# 25.3 Check accuracy\n(classes == y_test).sum()/y_test.size      # 81.31%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e2ef0ab593cff7d80d6f61c38b50d2bfd59f5b"},"cell_type":"code","source":"################## Feature selection #####################\n##****************************************\n## Using feature importance given by model\n##****************************************\nlist(colNames)\n# 26. Get feature importance\nclf.feature_importances_        # Column-wise feature importance\nclf.feature_importances_.size   # 42\n\n\n# 26.1 To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       # If dummy_clusterlabels labels are defined\n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names      # No kmeans      <==\n\n# 26.1.1 So how many columns?\nlen(colNames)           # 64\n# 26.2 Create a dataframe of feature importance and corresponding\n#      column names. Sort dataframe by importance of feature\nfeat_imp = pd.DataFrame({\n                   \"importance\": clf.feature_importances_ ,\n                   \"featureNames\" : colNames\n                  }\n                 ).sort_values(by = \"importance\", ascending=False)\n\n\nfeat_imp.shape                   # 135 X 2 ; without kmeans: (126,2)\nfeat_imp.head(20)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"993a15ac17d11c05d8533cf65820207a637cb973"},"cell_type":"code","source":"# 26.3 Plot feature importance for first 20 features\ng = sns.barplot(x = feat_imp.iloc[  : 20 ,  1] , y = feat_imp.iloc[ : 20, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}