{"cells":[{"metadata":{"_uuid":"87843536d1751bc1feb9330f89601891a0d38885"},"cell_type":"markdown","source":"**1. Preparing DataFrame for further manipulation**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom gensim import parsing\nfrom sklearn.metrics import accuracy_score\nimport chardet\nfrom sklearn.metrics import roc_auc_score,confusion_matrix,classification_report\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#detects encoding of csv file\nwith open('../input/spam.csv', 'rb') as f:\n    result = chardet.detect(f.read())\n    \n#put csv file in a dataframe.\ndf = pd.read_csv(\"../input/spam.csv\", encoding = result['encoding'])\n\n\ndf = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndf['v1'] = df.v1.map({'ham':0, 'spam':1})\n\n#df['v1'] = df.v1.map({'ham':0, 'spam':1})\n# Any results you write to the current directory are saved as output.","execution_count":110,"outputs":[]},{"metadata":{"_uuid":"adf757387e6c024435500e009e5c246345d117bc"},"cell_type":"markdown","source":"**2. Check count of spam and ham messages**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Count observations in each label\ndf.v1.value_counts()","execution_count":111,"outputs":[]},{"metadata":{"_uuid":"bcd25a9cdd48f5cb48d3790c77d6640991851033"},"cell_type":"markdown","source":"so above counts matrix means that if we label every value value as 0 we'll get 86.6% accuracy. So moving ahead we'll also score not just on accuracy, but on precision and recall score as well.\nFor more details on roc_auc_score read:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n\n**3. Preprocessing data and creating test and train sets**"},{"metadata":{"trusted":true,"_uuid":"5e23917a8bb21d23e489c6970ef4907dba2c4574","collapsed":true},"cell_type":"code","source":"def parse(s):\n    parsing.stem_text(s)\n    return s\n\n#applying parsing to comments.\nfor i in range(0,len(df)):\n    df.iloc[i,1]=parse(df.iloc[i,1])\n    df.iloc[i,1]=df.iloc[i,1].lower()\nX, y = df['v2'].tolist(), df['v1'].tolist()\n\n#Train and test set split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n","execution_count":112,"outputs":[]},{"metadata":{"_uuid":"cb043c591dcaa31f51dca5e81f6a6f56337a9aab"},"cell_type":"markdown","source":"**4. Classify with Multinomial Naive Bayes classifier**"},{"metadata":{"trusted":true,"_uuid":"62b99efdb03a74abdfa00fe3345428135d6c3a65"},"cell_type":"code","source":"#Multinomial NB is the type of Naive Bayes which is often used to text classification.\n#for more info about multinomial Naive bayes check out http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n#Count vectorizer create a matrix of all SMS where each value represents the number of times the corresponding word appeared in that sms.\n#For more info on Count Vectorizer check out http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n#and https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n#tf–idf or TFIDF, short for term frequency–inverse document frequency\n#is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n#TFIDF significantly improves accuracy of a text classifier.\n#For more info on why we use TFIDF check out https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n\n#Use pipeline to carry out steps in sequence with a single object, this time we'll use Multinomial NB classifier\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n\n#train model\ntext_clf.fit(X_train, y_train)\n\n\n#predict class form test data \npredicted = text_clf.predict(X_test)\n\nprint(accuracy_score(y_test, predicted))\nprint(roc_auc_score(y_test,predicted))\nprint(confusion_matrix(y_test, predicted))","execution_count":113,"outputs":[]},{"metadata":{"_uuid":"c3c1798470c6307ee581e07504ce1280540fc401"},"cell_type":"markdown","source":"As you can see in the confusion matrix above, 59 of the values are wrongly classification. The roc_auc_score is .84.\n\n**5. Classification with Logistic Regression**"},{"metadata":{"trusted":true,"_uuid":"937e01b11fd4dec2973cad5d77ee06cc5cde0071"},"cell_type":"code","source":"#As you can see above, multinomial NB gives 96.70% accuracy\n#Use pipeline to carry out steps in sequence with a single object, this time we'll use Multinomial NB classifier\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression())])\n\n#train model\ntext_clf.fit(X_train, y_train)\n\n\n#predict class form test data \npredicted = text_clf.predict(X_test)\n\nprint(accuracy_score(y_test, predicted))\nprint(roc_auc_score(y_test,predicted))\nprint(confusion_matrix(y_test, predicted))","execution_count":114,"outputs":[]},{"metadata":{"_uuid":"3e104414b942b94f7ceff8296594ff4135677126"},"cell_type":"markdown","source":"Logistic Regression has given better accuracy than Multinomial regression.\n\n**6.  Classify with SVM with gaussian Kernel**"},{"metadata":{"trusted":true,"_uuid":"e555316ea82b4b4c57c3477a64818c552a119250"},"cell_type":"code","source":"#As you can see above, multinomial NB gives 96.70% accuracy\n#Use pipeline to carry out steps in sequence with a single object, this time we'll use SVM with gaussian kernel\n# for more info on different kernels check out http://scikit-learn.org/stable/modules/svm.html\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='rbf'))])\n\n#train model\ntext_clf.fit(X_train, y_train)\n\n#predict class form test data \npredicted = text_clf.predict(X_test)\n\nprint(accuracy_score(y_test, predicted))\nprint(roc_auc_score(y_test,predicted))\nprint(confusion_matrix(y_test, predicted))\n","execution_count":115,"outputs":[]},{"metadata":{"_uuid":"dedca7ce47019a15e6bb8700dba91c61c820b4bd"},"cell_type":"markdown","source":"roc_auc_score of .5 is very bad, also the accuracy is 86%. As you can see in the confusion matrix it has classified every value as ham, so the algo has taken no action here. Let's see how Polynomial kernel of SVM performs the classification.\n\n**7.  Classify with SVM with Polynomial  Kernel**"},{"metadata":{"trusted":true,"_uuid":"4d287b5d3cdd9464b6ddf2be41ba145192347a6a"},"cell_type":"code","source":"#Use pipeline to carry out steps in sequence with a single object, this time we'll use SVM classifier with polynomial kernel\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='poly'))])\n\n#train model\ntext_clf.fit(X_train, y_train)\n\n#predict class form test data \npredicted = text_clf.predict(X_test)\n\nprint(accuracy_score(y_test, predicted))\nprint(roc_auc_score(y_test,predicted))\nprint(confusion_matrix(y_test, predicted))","execution_count":116,"outputs":[]},{"metadata":{"_uuid":"b0ddf7b0283f49d306c91a82faee1764c26ddba1"},"cell_type":"markdown","source":"Same as gaussian kernel, polynomial kernel too is not fit for this problem, it has also classified every value as ham.\n\n**8.  Classify with SVM with Linear  Kernel**"},{"metadata":{"trusted":true,"_uuid":"beef7a68b95504228790fad1c2799daf3a5e0a68"},"cell_type":"code","source":"#SVM with polynomial kernel gives only 87.72% accuracy, which is same as gaussian kernel. Uptil now Multinomial NB has given highest\n#accuracy, i.e. 96.70%. Let's see if linear kernel in SVM is able to cross that score.\n#Linear kernel is considered most suitable for text classification\n#Use pipeline to carry out steps in sequence with a single object, this time we'll use SVM classifier with linear kernel\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='linear'))])\n\n#train model\ntext_clf.fit(X_train, y_train)\n\n#predict class form test data \npredicted = text_clf.predict(X_test)\n\nprint(accuracy_score(y_test, predicted))\nprint(roc_auc_score(y_test,predicted))\nprint(confusion_matrix(y_test, predicted))","execution_count":117,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ac7109b82b0320dd0402c2a2d2c479a37e9b89b8"},"cell_type":"markdown","source":"So SVM with linear kernel here is best suited for text classification in this problem."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}