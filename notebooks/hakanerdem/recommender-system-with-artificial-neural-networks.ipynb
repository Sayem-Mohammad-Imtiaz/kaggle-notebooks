{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T13:46:39.939987Z","iopub.execute_input":"2021-06-21T13:46:39.940464Z","iopub.status.idle":"2021-06-21T13:46:42.466352Z","shell.execute_reply.started":"2021-06-21T13:46:39.940393Z","shell.execute_reply":"2021-06-21T13:46:42.465193Z"}}},{"cell_type":"markdown","source":"Recommender systems can be used to evaluate cross selling opportunities on the domain of retail marketing. Further reading about the key concepts of \"recommender systems\",  \"cross selling\", \"collaborative filtering\" and \"deep learning\" and their applications, one can look related papers such as:\n\n1. Kamakura, W. A., Wedel, M., de Rosa, F., Mazzon, J. A. (2003), Cross-selling through database marketing: A mixed data factor analyzer for data augmentation and prediction. International Journal of Research in Marketing, 20, 45–65.\n\n2. Knott, A., Hayes, A., & Neslin, S. A. (2002), Next-product-to-buy models for cross-selling applications. Journal of Interactive Marketing, 16(3), 59–75.\n\n3. Thuring F., Nielsen J.P., Guillén M., Bolancé C.,(2012), Selecting prospects for cross-selling ﬁnancial products using multivariate credibility, Expert Systems with Applications 39, 8809–8816.\n\n4. Zhang S., Yao L., Sun A., Tay Y., (2018), Deep Learning based Recommender System: A Survey and New Perspectives. ACM Comput. Surv. 1(1), 1-35.\n\n5. Shi Y., Larson M., Hanjalic A., (2014), Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Comput. Surv. 47(1) 45. DOI: http://dx.doi.org/10.1145/2556270\n\n6. Hidasi B., Karatzoglou A., (2018), Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. In The 27th ACM International Conference on Information and Knowledge Management (CIKM ’18), October 22–26, 2018, Torino, Italy. ACM, New York, NY, USA, 10 pages. DOI: https://doi.org/10.1145/3269206.3271761\n\n.\n.","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we, mainly aimed at prediction of customers next products to buy using implicit feedback from purchase preferences, and mainly follow [Lazy Programmer Inc.'s](https://www.udemy.com/course/recommender-systems/) -which is a udemy course about recommender systems that we strongly recommend- methodology.\n\nDataset choosen, famous, Online Retail II. For detailed information please visit:\n\n[https://www.kaggle.com/mashlyn/online-retail-ii-uci](https://www.kaggle.com/mashlyn/online-retail-ii-uci)\n\nand for the original source:\n\n[UCI Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.version.VERSION)\nprint(keras.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T11:13:17.212014Z","iopub.execute_input":"2021-08-18T11:13:17.212509Z","iopub.status.idle":"2021-08-18T11:13:22.739908Z","shell.execute_reply.started":"2021-08-18T11:13:17.212411Z","shell.execute_reply":"2021-08-18T11:13:22.738909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data & Preparation","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/online-retail-ii-uci/online_retail_II.csv\",\n                   parse_dates=[\"InvoiceDate\"],\n                   dtype={\"Customer ID\":\"object\"})","metadata":{"execution":{"iopub.status.busy":"2021-08-18T11:13:28.810433Z","iopub.execute_input":"2021-08-18T11:13:28.810792Z","iopub.status.idle":"2021-08-18T11:13:31.788677Z","shell.execute_reply.started":"2021-08-18T11:13:28.810763Z","shell.execute_reply":"2021-08-18T11:13:31.78772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-18T11:13:35.958751Z","iopub.execute_input":"2021-08-18T11:13:35.959183Z","iopub.status.idle":"2021-08-18T11:13:36.064045Z","shell.execute_reply.started":"2021-08-18T11:13:35.959149Z","shell.execute_reply":"2021-08-18T11:13:36.063085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A brief of data cleaning\n\nDroping rows with missing values and irrelevant labels","metadata":{}},{"cell_type":"code","source":"df = df.dropna()\ndf = df.drop(df[df[\"Quantity\"]<0].index)\ndf = df.drop(df[df[\"StockCode\"].str.contains(\"TEST\")].index)\ndf = df.drop(df[df[\"StockCode\"]==\"POST\"].index)\n\ndf = df.sort_values(\"InvoiceDate\")","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:12:54.379939Z","iopub.execute_input":"2021-08-17T17:12:54.380294Z","iopub.status.idle":"2021-08-17T17:12:55.451081Z","shell.execute_reply.started":"2021-08-17T17:12:54.380257Z","shell.execute_reply":"2021-08-17T17:12:55.450073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common Functions\n\nFirst function can be used to obtain lists having unique elements. Second, for generating product purchase sequences and a target sequence having \"n_target\" length occuring after a sequence of product purchased. The last one for generating a negative sample.  ","metadata":{}},{"cell_type":"code","source":"def unique(list1):\n    list_set = set(list1)\n    unique_list = (list(list_set))\n    return unique_list\n\ndef generate_sequence(serie, n_target):\n    input_sequence = []\n    output_sequence = []\n    for x in serie:\n        x = unique(x)\n        if len(x)>n_target:\n            input_sequence.append(x[:-n_target])\n            output_sequence.append(x[-n_target:])\n    return input_sequence, output_sequence\n\ndef neg(x, corp, sample_size=1):\n    diff = np.setdiff1d(corp, list(x))\n    ind = np.random.permutation(len(diff))\n    return diff[ind[:int(sample_size*len(x))]]","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:13:53.403806Z","iopub.execute_input":"2021-08-17T17:13:53.4044Z","iopub.status.idle":"2021-08-17T17:13:53.427696Z","shell.execute_reply.started":"2021-08-17T17:13:53.404347Z","shell.execute_reply":"2021-08-17T17:13:53.426714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By following cells, we try to generate customers' purchase sequences of distinct products. ","metadata":{}},{"cell_type":"code","source":"by_customer = df.groupby(\"Customer ID\", as_index=False).agg(\n    {\"StockCode\": [lambda x: list(x)]}\n)\nsequential_df = by_customer[\"StockCode\"].rename(\n    columns={\"<lambda>\":\"purchase_sequence\"}\n)\nsequential_df[\"CustomerID\"] = by_customer[\"Customer ID\"]\nsequential_df[\"product_count\"] = sequential_df[\"purchase_sequence\"].apply(\n    lambda x: len(unique(list(x)))\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:14:24.262155Z","iopub.execute_input":"2021-08-17T17:14:24.262521Z","iopub.status.idle":"2021-08-17T17:14:24.715314Z","shell.execute_reply.started":"2021-08-17T17:14:24.262491Z","shell.execute_reply":"2021-08-17T17:14:24.714528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose some hyperparameter values arbitrarily but it can be a good practice to look at some statistics like below: number of distinct products purchased.  ","metadata":{}},{"cell_type":"code","source":"sequential_df[\"product_count\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:14:32.90994Z","iopub.execute_input":"2021-08-17T17:14:32.910326Z","iopub.status.idle":"2021-08-17T17:14:32.924769Z","shell.execute_reply.started":"2021-08-17T17:14:32.91029Z","shell.execute_reply":"2021-08-17T17:14:32.923736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_target = 1\nn_frequency = 3\nprod_embedding_size = 16\nuser_embedding_size = 16\n\ncorp = sequential_df.explode(\"purchase_sequence\")[\"purchase_sequence\"].unique()\nfrequent_df = sequential_df[(sequential_df[\"product_count\"]>n_frequency)]\n\ninput_seq, output_seq = generate_sequence(\n    frequent_df[\"purchase_sequence\"],\n    n_target\n    )\n\nfrequent_df[\"input_sequence\"] = input_seq\nfrequent_df[\"output_sequence\"] = output_seq\nfrequent_df = frequent_df[[\"CustomerID\", \"input_sequence\", \"output_sequence\"]]\nfrequent_df = frequent_df.explode(\"input_sequence\")\nfrequent_df[\"purchase\"] = 1\nfrequent_df = frequent_df.set_index(\"CustomerID\", drop=True)\nfrequent_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:15:01.315642Z","iopub.execute_input":"2021-08-17T17:15:01.315993Z","iopub.status.idle":"2021-08-17T17:15:01.95752Z","shell.execute_reply.started":"2021-08-17T17:15:01.315964Z","shell.execute_reply":"2021-08-17T17:15:01.956561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Negative Sampling\n\nSince all instances prepared so far represent positive-only feedback, we try to supply some negative information to the model. Negative instances are chosen from products not purchased for a particular customer.\n> sample_size=1 \n\nmeans there is 1 non-purchased product to be selected randomly.","metadata":{}},{"cell_type":"code","source":"new_df = frequent_df.reset_index().groupby(\"CustomerID\").agg({\"input_sequence\": (lambda x: list(x))})\nnew_df[\"neg\"] = new_df[\"input_sequence\"].apply(lambda y: neg(y, corp, 5))\nndf = new_df.explode(\"neg\")[[\"neg\"]]\nndf[\"purchase\"] = 0\nndf = ndf.rename(columns={\"neg\":\"input_sequence\"})\n\npdf = frequent_df[[\"input_sequence\", \"purchase\"]]\n\nsample_df = pdf.append(ndf)\nsample_df = sample_df.reset_index()\nsample_df = sample_df.sort_values(\"CustomerID\", ignore_index=True)\n\ndisplay(sample_df.info())\ndisplay(sample_df.head(50))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:17:34.915262Z","iopub.execute_input":"2021-08-17T17:17:34.91563Z","iopub.status.idle":"2021-08-17T17:18:41.982263Z","shell.execute_reply.started":"2021-08-17T17:17:34.915595Z","shell.execute_reply":"2021-08-17T17:18:41.981295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding\n\nAs a last step we try to encode user and product features. Method taken from [keras.io](https://keras.io/examples/structured_data/collaborative_filtering_movielens/) examples. We take the data as train & validation, but the better practice is holding out some samples in advance as test data.    ","metadata":{}},{"cell_type":"code","source":"cust_ids = sample_df[\"CustomerID\"].unique().tolist()\ncust2cust_encoded = {x: i for i, x in enumerate(cust_ids)}\ncust_encoded2cust = {i: x for i, x in enumerate(cust_ids)}\nprod_ids = corp\nprod2prod_encoded = {x: i for i, x in enumerate(prod_ids)}\nprod_encoded2prod = {i: x for i, x in enumerate(prod_ids)}\nsample_df[\"cust\"] = sample_df[\"CustomerID\"].map(cust2cust_encoded)\nsample_df[\"prod\"] = sample_df[\"input_sequence\"].map(prod2prod_encoded)\n\nnum_custs = len(cust2cust_encoded)\nnum_prods = len(prod2prod_encoded)\nsample_df[\"purchase\"] = sample_df[\"purchase\"].values.astype(np.float32)\n\nprint(\n    \"Number of Customers: {}, Number of Products: {}, Purchase: {}, Not Purchase: {}\".format(\n        num_custs, num_prods, 1, 0\n    )\n)\n\nsample_df = sample_df.sample(frac=1, random_state=52)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:27:04.046946Z","iopub.execute_input":"2021-08-17T17:27:04.047301Z","iopub.status.idle":"2021-08-17T17:27:05.605802Z","shell.execute_reply.started":"2021-08-17T17:27:04.047271Z","shell.execute_reply":"2021-08-17T17:27:05.605039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Matrix Factorization","metadata":{}},{"cell_type":"markdown","source":"In the first part, we try to implement a collaborative filtering system using embedding layers for user-item instances. Python code mostly adapted from the notebooks of:   \n\n* [colinmorris-1](https://www.kaggle.com/colinmorris/embedding-layers)\n* [colinmorris-2](https://www.kaggle.com/colinmorris/matrix-factorization)\n* [rajmehra03-1](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n* [rajmehra03-2](https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization)\n* [rounakbanik](https://www.kaggle.com/rounakbanik/movie-recommender-systems)\n* [keras.io examples](https://keras.io/examples/structured_data/collaborative_filtering_movielens/)\n\nand we try to evaluate the system based on the instructions from: \n\n[jamesloy](https://www.kaggle.com/jamesloy/deep-learning-based-recommender-systems)\n\n","metadata":{}},{"cell_type":"code","source":"cust_input = layers.Input(shape=(1,), name=\"cust_id\", dtype=tf.int32)\nprod_input = layers.Input(shape=(1,), name=\"prod_id\", dtype=tf.int32)\n \ncust_embedding = layers.Embedding(num_custs,\n                                  user_embedding_size,\n                                  name=\"cust_emb\")\ncust_bias = layers.Embedding(num_custs, 1, name=\"cust_bias\")\n \nprod_embedding = layers.Embedding(num_prods,\n                                  prod_embedding_size,\n                                  name=\"prod_emb\")\nprod_bias = layers.Embedding(num_prods, 1, name=\"prod_bias\")\n \ncust_vector = cust_embedding(cust_input)\ncust_bias = cust_bias(cust_input)\nprod_vector = prod_embedding(prod_input)\nprod_bias = prod_bias(prod_input)\n \ndot_cust_product = layers.Dot(name=\"Dot\", axes=2)([cust_vector, prod_vector])\noutput = layers.Add(name=\"Add\")([dot_cust_product, cust_bias, prod_bias])\noutput = layers.Flatten(name=\"Flat\")(output)\n \nmodel_X = keras.Model([cust_input, prod_input], output, name=\"model_x\")\n \nmodel_X.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n \nmodel_X.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:26.204188Z","iopub.execute_input":"2021-08-17T17:29:26.204555Z","iopub.status.idle":"2021-08-17T17:29:26.259888Z","shell.execute_reply.started":"2021-08-17T17:29:26.204507Z","shell.execute_reply":"2021-08-17T17:29:26.258982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                   mode=\"min\",\n                                   verbose=1,\n                                   patience=5)\n\nhistory = model_X.fit([sample_df[\"cust\"].values, sample_df[\"prod\"].values],\n                      sample_df[\"purchase\"].values,\n                      batch_size=256,\n                      epochs=20,\n                      verbose=1,\n                      validation_split=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:29:29.433788Z","iopub.execute_input":"2021-08-17T17:29:29.434142Z","iopub.status.idle":"2021-08-17T17:35:05.76714Z","shell.execute_reply.started":"2021-08-17T17:29:29.434106Z","shell.execute_reply":"2021-08-17T17:35:05.766416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"embedding loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"val\"], loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:35:29.988229Z","iopub.execute_input":"2021-08-17T17:35:29.988561Z","iopub.status.idle":"2021-08-17T17:35:30.147257Z","shell.execute_reply.started":"2021-08-17T17:35:29.988532Z","shell.execute_reply":"2021-08-17T17:35:30.146337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi-Layer Perceptron","metadata":{}},{"cell_type":"code","source":"hidden_units = [128, 64]\n \nuser_id_input = layers.Input(shape=(1,), name=\"user_id\", dtype=tf.int32)\nprod_id_input = layers.Input(shape=(1,), name=\"prod_id\", dtype=tf.int32)\nuser_embedded = layers.Embedding(num_custs,\n                                 user_embedding_size, \n                                 input_length=1,\n                                 embeddings_regularizer=keras.regularizers.l2(1e-7),\n                                 name=\"user_embedding\")(user_id_input)\nprod_embedded = layers.Embedding(num_prods,\n                                 prod_embedding_size,\n                                 input_length=1,\n                                 embeddings_regularizer=keras.regularizers.l2(1e-6),\n                                 name=\"prod_embedding\")(prod_id_input)\n \nconcatenated = layers.Concatenate(name=\"concat\")([user_embedded, prod_embedded])\nout = layers.Flatten(name=\"flat\")(concatenated)\n \nfor n_hidden in hidden_units:\n    out = layers.Dense(n_hidden,\n                       activation=\"relu\",\n                       kernel_regularizer=keras.regularizers.l2(0.001))(out)\n    out = layers.Dropout(0.4)(out)\n    out = layers.BatchNormalization()(out)\n\nout = layers.Dense(1, activation=\"sigmoid\", name=\"prediction\")(out)\n \nmodel_Y = keras.Model(inputs = [user_id_input, prod_id_input],\n                           outputs = out, name=\"model_y\")\n \nmodel_Y.compile(loss=keras.losses.MeanSquaredError(),\n                     optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n \nmodel_Y.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:38:47.57022Z","iopub.execute_input":"2021-08-17T17:38:47.570597Z","iopub.status.idle":"2021-08-17T17:38:47.666313Z","shell.execute_reply.started":"2021-08-17T17:38:47.570563Z","shell.execute_reply":"2021-08-17T17:38:47.665356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                   mode=\"min\",\n                                   verbose=1,\n                                   patience=5)\n\nhistory = model_Y.fit([sample_df[\"cust\"].values, sample_df[\"prod\"].values],\n                      sample_df[\"purchase\"].values,\n                      batch_size=256,\n                      epochs=20,\n                      verbose=1,\n                      validation_split=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:39:22.544074Z","iopub.execute_input":"2021-08-17T17:39:22.544414Z","iopub.status.idle":"2021-08-17T17:51:53.902364Z","shell.execute_reply.started":"2021-08-17T17:39:22.544383Z","shell.execute_reply":"2021-08-17T17:51:53.901454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"mlp loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"val\"], loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:52:30.78548Z","iopub.execute_input":"2021-08-17T17:52:30.785877Z","iopub.status.idle":"2021-08-17T17:52:30.927931Z","shell.execute_reply.started":"2021-08-17T17:52:30.785844Z","shell.execute_reply":"2021-08-17T17:52:30.927322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Collaborative Filtering","metadata":{}},{"cell_type":"markdown","source":"Neural Collaborative Filtering (NCF) is one of the recommendation system frameworks, based on neural networks, proposed by He, et. al. (2017). According to them a neural network can develop a model by learning item user interactions as a key factor of a collaboritive filtering from implicit feedback. Python code mostly developed thanks to beautiful notebooks like:\n\n[fuzzywizard](https://www.kaggle.com/fuzzywizard/rec-sys-collaborative-filtering-dl-techniques#4-Matrix-Factorization-using-Deep-Learning-(Keras))\n\n[rajmehra03](https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization)\n\nPlease see for detailed information about NCF:\n\nHe, X., Liao, L., Zhang, H., Nie, L., Hu, X.,Chua, T.,  (2017), Neural Collaborative Filtering. WWW'17: Proceedings of the 26th International Conference on World Wide Web 173–182 DOI: http://dx.doi.org/10.1145/3038912.3052569","metadata":{}},{"cell_type":"code","source":"hidden_units = [128, 64]\n\nuser_id_input = layers.Input(shape=(1,), name=\"user_id\", dtype=tf.int32)\nprod_id_input = layers.Input(shape=(1,), name=\"prod_id\", dtype=tf.int32)\nuser_embedded = layers.Embedding(num_custs,\n                                 user_embedding_size, \n                                 input_length=1,\n                                 embeddings_regularizer=keras.regularizers.l2(1e-7),\n                                 name=\"user_emb\")(user_id_input)\ncust_bias = layers.Embedding(num_custs, 1, name=\"cust_bias\")(user_id_input)\n \nprod_embedded = layers.Embedding(num_prods,\n                                  user_embedding_size,\n                                  embeddings_regularizer=keras.regularizers.l2(1e-6),\n                                  name=\"prod_emb\")(prod_id_input)\nprod_bias = layers.Embedding(num_prods, 1, name=\"prod_bias\")(prod_id_input)\n \ndot_cust_product = layers.Dot(name=\"Dot\", axes=2)([user_embedded, prod_embedded])\nX = layers.Add(name=\"Add\")([dot_cust_product, cust_bias, prod_bias])\nX = layers.Flatten(name=\"Flat\")(X)\n\nconcatenated = layers.Concatenate(name=\"concat\")([user_embedded, prod_embedded])\nY = layers.Flatten(name=\"flat\")(concatenated)\n \nfor n_hidden in hidden_units:\n    Y = layers.Dense(n_hidden,\n                     activation=\"relu\",\n                     kernel_regularizer=keras.regularizers.l2(0.001))(Y)\n    Y = layers.Dropout(0.4)(Y)\n    Y = layers.BatchNormalization()(Y)\n\nY = layers.Dense(1, activation=\"sigmoid\", name=\"prediction\")(Y)\n\nZ = layers.Add(name=\"final\")([X, Y])\n\nmodel_Z = keras.Model(inputs = [user_id_input, prod_id_input],\n                      outputs = Z, name=\"model_z\")\n \nmodel_Z.compile(loss=keras.losses.MeanSquaredError(),\n                optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n \nmodel_Z.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:54:14.441674Z","iopub.execute_input":"2021-08-17T17:54:14.442035Z","iopub.status.idle":"2021-08-17T17:54:14.5564Z","shell.execute_reply.started":"2021-08-17T17:54:14.442004Z","shell.execute_reply":"2021-08-17T17:54:14.555499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                   mode=\"min\",\n                                   verbose=1,\n                                   patience=5)\n\nhistory = model_Z.fit([sample_df[\"cust\"].values, sample_df[\"prod\"].values],\n                      sample_df[\"purchase\"].values,\n                      batch_size=256,\n                      epochs=20,\n                      verbose=1,\n                      validation_split=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T17:54:54.039404Z","iopub.execute_input":"2021-08-17T17:54:54.03977Z","iopub.status.idle":"2021-08-17T18:07:55.372245Z","shell.execute_reply.started":"2021-08-17T17:54:54.039734Z","shell.execute_reply":"2021-08-17T18:07:55.371138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"ncf loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"val\"], loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T18:10:43.291895Z","iopub.execute_input":"2021-08-17T18:10:43.292259Z","iopub.status.idle":"2021-08-17T18:10:43.449413Z","shell.execute_reply.started":"2021-08-17T18:10:43.292227Z","shell.execute_reply":"2021-08-17T18:10:43.448511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\nWe try to measure the model performance by providing candidate products to the model and evaluating the outputs. Candidate products are merged with 49 products selected from non-purchased products and a target product which respresented in output_sequence variable. If target product occures in the top k of the model outputs, we count this event as a hit.\n\nOn the other hand; Hidasi and Karatzoglou (2018) define \"recall@k\" as an evaluatinon metric as \"the proportion of cases having the desired item amongst the top-k items in all test cases.\" Moreover, one another evaluation metric is \"MRR@k\", which is the average of reciprocal ranks of the target items. The reciprocal rank is set to zero if the rank is above k.","metadata":{}},{"cell_type":"code","source":"def get_metrics(model, k, size=1000):  \n    hit = 0\n    mrr = 0\n    counter = size\n    for s in range(counter):\n        cust_id = sample_df[\"CustomerID\"].unique()[s]\n        cust_encoder = cust2cust_encoded.get(cust_id)\n        purchased = frequent_df[(frequent_df.index==cust_id) & (frequent_df[\"purchase\"]==1)]\n        candidates = frequent_df[~frequent_df[\"input_sequence\"].isin(purchased[\"input_sequence\"].values)][\"input_sequence\"][:49]\n        candidates = set(candidates).intersection(set(prod2prod_encoded.keys()))\n        candidates = candidates.union(set(frequent_df[frequent_df.index==cust_id][\"output_sequence\"].values[0]))\n        candidates = [[prod2prod_encoded.get(x)] for x in list(candidates)]\n        ids = np.stack([[cust_encoder]]*len(candidates))\n        y_pred = model.predict([ids, np.array(list(candidates), dtype=\"int32\")]).flatten()\n        t = frequent_df.loc[(frequent_df.index==cust_id), \"output_sequence\"].values[0][0]\n        recommend = []\n        rr = 0\n        for i in range(k):\n            p = prod_encoded2prod.get(candidates[y_pred.argsort()[-(i+1)]][0])\n            recommend.append(p)\n            if (p==t):\n                rr = 1/(i+1)\n                hit = hit + 1\n        mrr = mrr + rr\n        \n    return (hit/counter), (mrr/counter)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:02:05.664818Z","iopub.execute_input":"2021-08-17T19:02:05.665241Z","iopub.status.idle":"2021-08-17T19:02:05.675304Z","shell.execute_reply.started":"2021-08-17T19:02:05.665199Z","shell.execute_reply":"2021-08-17T19:02:05.674301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recallx, mrrx = get_metrics(model_X, 10)\nprint(\"Recall@: \", recallx)\nprint(\"MRR@: \", mrrx)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:02:10.870306Z","iopub.execute_input":"2021-08-17T19:02:10.870712Z","iopub.status.idle":"2021-08-17T19:08:29.428925Z","shell.execute_reply.started":"2021-08-17T19:02:10.870671Z","shell.execute_reply":"2021-08-17T19:08:29.427708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recally, mrry = get_metrics(model_Y, 10)\nprint(\"Recall@: \", recally)\nprint(\"MRR@: \", mrry)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:09:10.731149Z","iopub.execute_input":"2021-08-17T19:09:10.731518Z","iopub.status.idle":"2021-08-17T19:15:28.807521Z","shell.execute_reply.started":"2021-08-17T19:09:10.731484Z","shell.execute_reply":"2021-08-17T19:15:28.80645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recallz, mrrz = get_metrics(model_Z, 10)\nprint(\"Recall@: \", recallz)\nprint(\"MRR@: \", mrrz)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T19:17:06.63168Z","iopub.execute_input":"2021-08-17T19:17:06.632064Z","iopub.status.idle":"2021-08-17T19:23:21.548399Z","shell.execute_reply.started":"2021-08-17T19:17:06.632032Z","shell.execute_reply":"2021-08-17T19:23:21.547358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Critiques\n\nPlease criticise this study and faulty issues other than hyperparameter tuning. Any comment is more precious than upvotes for this fresh notebook. To compare metrics please see: https://medium.com/decathlondevelopers/building-a-rnn-recommendation-engine-with-tensorflow-505644aa9ff3. They developed a model for more than 10,000 different products. \n\nSome hyperparameters which should be tuned.\n\n* Number and unit numbers of *hidden_units*\n* *prod_embedding_size* and *user_embedding_size*\n* regularizers, learning rate, activation functions, batch size, dropout rates\n* *n_frequency* number of frequent products\n* *sample_size* number of negative samples corresponding to a positive sample\n\nSome topics which are ambiguous:\n\n* Can prediction performance be upgraded for this model? \n* Is there a need for negative sampling? Is there a room for improvement by adjusting negative sample size?\n* Are there more suitable or effective techniques to measure the performance of the model?\n* Can execution time be shortened?\n* Any other effective ways to predict next-product-to-buy using deep learning?\n.\n.\n\nSorry for language...\nThanks in advance","metadata":{}}]}