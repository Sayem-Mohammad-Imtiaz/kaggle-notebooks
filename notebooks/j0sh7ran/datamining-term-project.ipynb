{"cells":[{"metadata":{},"cell_type":"markdown","source":"## References:\n* Data Set:\n    * https://www.kaggle.com/jvanelteren/boardgamegeek-reviews\n* Existing work references:\n    * https://www.kaggle.com/jvanelteren/exploring-the-13m-reviews-bgg-dataset  \n* Code References:  \n    * pre-built sentiment analyzers:\n        * https://github.com/cjhutto/vaderSentiment  \n        * https://textblob.readthedocs.io/en/dev/api_reference.html#module-textblob.classifiers  \n    * variety of NaiveBayes Classifiers:\n        * https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\n        * https://datascience.stackexchange.com/questions/53100/training-textblob-with-16k-rows-of-labeled-data-wont-work-only-few-are-working\n    * featrure extraction:\n        * https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"},{"metadata":{},"cell_type":"markdown","source":"# PACKAGES UNAVALIABLE HERE, USE JUPYTER NOTEBOOK ON LOCAL MACHINE\n## Vader Sentiment doesn't properly load in via pip install or isnt pre-installed here."},{"metadata":{},"cell_type":"markdown","source":"# Term Project\nJoshua Tran, 1001296598, CSE-5334\n## Objective:\nThe objective of this project is to produce a classifier that can take a look at a given review and produce a predicted rating. These reviews will be based off the datasets defined in the references for the 15 million reviews.\nBased off these criteria, I'll be implementing a NaiveBayes classifier along side with some pre-built sentiment analyzers to predict the rating of these reviews. However to accomplish this, a few things need to be noted about what has been done.\n\n### pre-processing the data:\nThe data is vast and large, some of the data needs to pre-processed to ease training, and to get rid of non-helpful information. From the given set of reviews, to simplifiy processing we've made all the classifications into integers 0-10 for 11 total classifications. keeping the floating point representations left review classification values in the thousands of total classifications and seemed over complicated and undesirable. So this has been cleaned up.  \nAdditionally, I rid the data set from reviews that have no comment, because no comments doesn't help us predict actual comments."},{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# base computation libraries\nimport numpy as np\nimport pandas as pd\nfrom os import path\nimport matplotlib.pyplot as plt\n\n# data splitting\nfrom sklearn.model_selection import train_test_split\n\n# feature extration\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\n\n# sentiment analyzers\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n# naive bayes classifiers\nfrom sklearn.naive_bayes import MultinomialNB\nfrom textblob.classifiers import NaiveBayesClassifier\n\n# classifier saving\nimport pickle\n\n# needed for textblob custom:\n# import nltk\n#nltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# naive bayes classifiers\nfrom sklearn.naive_bayes import MultinomialNB\nfrom textblob.classifiers import NaiveBayesClassifier\n\n# classifier saving\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grabbing the data\nSimply, we grab the data here from the raw file, then round the ratings to be integer values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# path to reviews\nReviewPath = \"../input/boardgamegeek-reviews/bgg-15m-reviews.csv\"\ndf = pd.read_csv(ReviewPath).iloc[:, 1:]\ndf['rating']=df['rating'].round()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Examining the given data:\nLooks like with the given data we're given the following columns:\n* A user\n* A rating\n* A comment\n* An ID\n* A name of a game    \n\nHere we can additionally see that there are a bunch of reviews that have \"NaN\" comments in value and wont help us."},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So lets remove those \"NaN\" comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(subset = ['comment'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Details of the data:\nIn the next two code blocks we can start to see what our data is looking like after we've cleaned it up a bit. First we can see that our count has drastically decreased to just below 3 million reviews that we can use. additionally the reviews average to just around a rating of 7.  \nAdditionally we can see in the chart below that the heavier concentration of review is towards 6 to 10 ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rating'].hist(bins=10)\nplt.xlabel('rating of review')\nplt.ylabel('number of reviews')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking a look at sentiment analysis: \"Vader\" Sentiment\nNow that we've taken a look at the data. Lets take a closer look at some sentiment analyzers.\nFirst we have \"Vader\" sentiment. This will take a given string and find the polarity of how positive, negative, and neutrual it is. Here we initiallize our analyzer, and give the first comment in our data to it and look at the scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\nprint(df.iloc[1]['comment'])\nsentiment_polarity_score_example = analyzer.polarity_scores(df.iloc[1]['comment'])\nprint(\"result of sentiment polarity scores:\\n\",sentiment_polarity_score_example)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying sentiment analysis to our data set.\nBelow we have a method to be used to apply our vader sentiment analyzer to the data set. The idea here is to use the values generated by our analyzer to produce numerical values to use that will weight a Naive Bayes classifier to produce predictions on what the actual weight of the sentiment is.  \nIt is important to note that this processing takes a little bit of time, so i've generated a routine to check if the the processed sentiment has already been done, and if it has then we can just pull that data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def features(x):\n    result = analyzer.polarity_scores(x)\n    return pd.Series([result['neg'], result['neu'], result['pos'], result['compound']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"polarity_score_path = 'C:\\\\Users\\\\j0sh7\\\\OneDrive\\\\Desktop\\\\Fall 2020\\\\Data mining\\\\term proj\\\\vader-processed.csv'\nif(path.exists(polarity_score_path)):\n    vader_processed_df = pd.read_csv(polarity_score_path)\n# exporting data if it does not exists\nelse:\n    df[['neg polarity','neu polarity','pos polarity','compound polarity']] = df['comment'].apply(features)\n    #df.to_csv(polarity_score_path,index=False)\n    vader_processed_df = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking a look at the Vader processed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vader_processed_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vader_processed_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a helper sub-routine to generate our accuracy of datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"def findAccuracy(actual, predictions):\n    return ( np.sum(actual == predictions)/len(actual) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data\nNow that we've applied our sentiment analysis to generate some features to determine a classification given a review. We need to seperate our data into a Training set and a Testing set. Below we generate a training set of 80% of the reviews and 20% of the reviews."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(np.stack(vader_processed_df[['neg polarity','neu polarity','pos polarity']].to_numpy()),vader_processed_df['rating'].values, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using the Naive Bayes classifier\nHere we use our Naive bayes classifier, along with a variety of different smoothing paramters to adjust the alpha value (lap-lace smoothing value) four our classifier. However as the data shows below, the hyper parameter applied has no effect on this dataset. and seems to maintain it's accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"smoothingHyperParameter = [10.0, 5.0, 2.0, 1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]\nfor hyperParam in smoothingHyperParameter:\n    MultinomialNB_clf = MultinomialNB(alpha=hyperParam)\n    MultinomialNB_clf.fit(X_train, y_train)\n    result = MultinomialNB_clf.predict(X_test)\n    print(f\"result of hyper parameter: {hyperParam} = {findAccuracy(y_test, result)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking a Closer look at the results\nIf we take a closer look, we can see that this implementation centers the classifications and information around just the values of 6 and 8 when we classify. Additionally the difference between the actual and predicted values seem to have a distribution around 0 to 3. With a few thousad from 4 to 7 in difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"MultinomialNB_clf = MultinomialNB()\nMultinomialNB_clf.fit(X_train, y_train)\nresult = MultinomialNB_clf.predict(X_test)\nprint(f\"result: {np.unique(result)}\")\nprint(f\"result: {findAccuracy(y_test, result)}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = np.abs(result - y_test)\ndiff_df = pd.DataFrame(data=diff, columns=[\"difference\"])\ndiff_df['difference'].hist(bins=10)\nplt.xlabel('difference between values')\nplt.ylabel('number of differences')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using TextBlob sentiment analyzer and Custom Classifier\nNext lets try some new stuff. Here we try to do something similar to our previous approach. However, lets try to use TextBlob instead. Interestingly enough, TextBlob actually gives us the ability to use a custom classifer to predict what our sentiment is. Rather than just exclusively looking at positive and negative values.  \nHOWEVER, this training and classification process is EXTREMELY SLOW. So just to take a look at this, we'll be minimizing the data to around 10 thousand data points that will be split similarly to how we did previously in distribution for test and training sets.  \nThe way this analyzer works is that it uses a feature extractor to send our given sets in and then apply a classifier(NaiveBayes) on it. However, for it to start producing prediction values it needs an initial comment/classification.  \nAdditionally it uses an nltk classifier and wraps around it, making it slower than the sci-kit classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"Xdrop, X, yDrop, y = train_test_split(df[['comment','rating']],df['rating'], test_size=(10000 / df.shape[0]))\nX_train_TB, X_test_TB, y_train_TB, y_test_TB = train_test_split(X, y, test_size=.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_TB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_TB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Formatting the Data for TextBlob\nthe classifier needs our records stored in this format"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_records = list(X_train_TB.to_records(index=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that this classifier takes a long while to train itself for just 10 thousand records."},{"metadata":{"trusted":true},"cell_type":"code","source":"%time cl = NaiveBayesClassifier(train_records)\n%time initializer = cl.classify('this is a initializer, for classification')\nprint(initializer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_records = list(X_test_TB.to_records(index=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nfor item in test_records:\n    results.append(cl.classify(item[0]))\nresults_TB = np.array(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the results of this classifier\nTaking a look at this classifier we can see that the precision has decreased due to the data size being lower. If we take a closer look we can see that the ratios of the difference between the predicted values and actual values have decreased though showing a slight increase in \"accruacy\" rather than precision. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(findAccuracy(y_test_TB, results))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_TB = np.abs(results_TB - y_test_TB)\ndiff_df_TB = pd.DataFrame(data=diff_TB, columns=[\"difference\"])\ndiff_TB.hist(bins=10)\nplt.xlabel('difference between values')\nplt.ylabel('number of differences')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_TB.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mixing the findings from Vader, TextBlob, and vanilla Sci-kit\nAfter testing out each method used here. I found that Sci-kit's resources are incredibly fast and optimized, so In practice I'd like to use an approach with it. Additionally we found that smoothing these didn't have too much of an effect on the overall data.  \nFrom Vader, I found that it actually might be best to just transition to losing the sentiment values from training and just use each word as a set of features for our training, as seen in TextBlob due to the increase of accruacy in results. However, TextBlob's problem with processing large amounts of data comes from its feature extraction method. So to speed this up we'll use essentially the same method it uses but port it to sci-kit.  \nHere we break our data as usual into train and test. Then create a pipeline with a vectorized feature extractor for each review, then train it on the NaiveBayes classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df[['comment','rating']],df['rating'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking a look at the results\nWe can see that the precision and accruacy has increased. Creating a smaller grouping closer to zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n%time model.fit(X_train['comment'], X_train['rating'])\n%time results_model = model.predict(X_test['comment'])\nprint(findAccuracy(y_test, results_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = np.abs(results_model - y_test)\ndiff.hist(bins=10)\nplt.xlabel('difference between values')\nplt.ylabel('number of differences')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Packaging up best model for our implemented model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#filename = '\\finalized_model.sav'\n#pickle.dump(model_model, open(filename, 'wb'))\n#loaded_model = pickle.load(open(filename, 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%time results_load = loaded_model.predict(X_test['comment'])\n#print(findAccuracy(y_test, results_load))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}