{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A. **Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"Initially the data must be analysed to detect outliers, impute missing values and find patterns in\ndata. Feature Engineering is also an important part of this step, to determine features thata re not so useful and to create new features that correlate more to the target.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following can be noted from the info function.\n1. **id** is an unique identifier, it has no role in prediction. It has to be dropped.\n2. The target variable is **stroke**.\n3. The following are numerical inputs {**age**, **hypertension**, **heart_disease**, **avg_glucose_level**, **bmi**}.\n4. The following are categorical inputs {**gender**, **ever_married**, **work_type**, **residence_type**, **smoking_status**}.\n5. There are missing values in **bmi** column.","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"id\"], inplace=True)\ndf.hist(bins=50, figsize=(20, 15))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. The data distribution of numerical parameters are fairly robust\n7. **heart_disease** and **hypertension** are binary values rather than being a continuous distribution. It might be better to convert them to strings and then use one hot encoding for such discrete classes.","metadata":{}},{"cell_type":"code","source":"corr_matrix = df.corr()\ncorr_matrix[\"stroke\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. All the numerical factors seem to be lightly correlated to the target column. The correlation is also positive for all the numerical columns. None of the numerical features seem to be useless and hence all needs to be kept.\n9. **bmi** is the least correlated so we may find more useful features later on that can be considered.","metadata":{}},{"cell_type":"code","source":"print(\"Distribution of gender\")\nprint(df[\"gender\"].value_counts(), \"\\n\")\n\nprint(\"Distribution of ever_married\")\nprint(df[\"ever_married\"].value_counts(), \"\\n\")\n\nprint(\"Distribution of work_type\")\nprint(df[\"work_type\"].value_counts(), \"\\n\")\n\nprint(\"Distribution of Residence_type\")\nprint(df[\"Residence_type\"].value_counts(), \"\\n\")\n\nprint(\"Distribution of smoking_status\")\nprint(df[\"smoking_status\"].value_counts(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"B. **Data cleaning and preparation**","metadata":{}},{"cell_type":"markdown","source":"1. Caterogical data **gender** has just a single instance of **other**. This can be safely removed.","metadata":{}},{"cell_type":"code","source":"df = df[df[\"gender\"] != \"Other\"]\nprint(\"Distribution of gender\")\nprint(df[\"gender\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Next let's split the data into train and test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop(columns=[\"stroke\"])\ny = df[\"stroke\"]\n\nnp.random.seed(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Now lets process the data through a pipeline so that ML ready data is obtained after the pipeline.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_cols = [\"age\", \"hypertension\", \"heart_disease\", \"avg_glucose_level\", \"bmi\"]\ncat_cols = [\"gender\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]\n\nnum_pipeline = Pipeline([\n    (\"imputer\", KNNImputer()),\n    (\"std_scale\", StandardScaler())\n])\n\npipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_cols),\n    (\"cat\", OneHotEncoder(), cat_cols)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_prepared = pipeline.fit_transform(X_train)\ny_prepared = np.array(y_train)\nprint(X_prepared.shape)\nprint(y_prepared.shape)\n\nX_test_prepared = pipeline.transform(X_test)\ny_test_prepared = np.array(y_test)\nprint(X_test_prepared.shape)\nprint(y_test_prepared.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Let's determine the important features by training a Random Forest Classifier. We wouldn't bother much about hyperparameter tuning, since we are just interested in feature importances. We can expect the importance to remain roughly same.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=500)\nforest_clf.fit(X_prepared, y_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\ncat_encoder = pipeline.named_transformers_[\"cat\"]\nfor attribute, categories in zip(cat_cols, cat_encoder.categories_):\n    for category in categories:\n        print(forest_clf.feature_importances_[i], attribute, category)\n        i += 1\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not single attribute which is unimportant. However it is clear that children and people without heart disease have almost no hance of having a stroke.","metadata":{}},{"cell_type":"markdown","source":"C. **Resampling**","metadata":{}},{"cell_type":"markdown","source":"It is clear that the dataset is highly skewed. So we need to undersample the majority class or oversample the minority class.","metadata":{}},{"cell_type":"code","source":"true_mask = y_prepared == 1\nfalse_mask = y_prepared != 1\n\nX_true, y_true = X_prepared[true_mask], y_prepared[true_mask]\nX_false, y_false = X_prepared[false_mask], y_prepared[false_mask]\n\nprint(X_true.shape)\nprint(X_false.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = X_false.shape[0]\nmini_batch_size = X_true.shape[0]\n\npermuted_indices = np.random.permutation(batch_size)\nstart_indices = range(0, batch_size, mini_batch_size)\nX_batch, y_batch = list(), list()\nfor i in range(len(start_indices)):\n    try:\n        start, stop = start_indices[i], start_indices[i+1]\n        indices = permuted_indices[start:stop]\n    except:\n        start = start_indices[i]\n        indices = permuted_indices[start:]\n    \n    X_temp = X_false[indices]\n    X_minibatch = np.vstack([X_temp, X_true])\n    \n    y_temp = y_false[indices]\n    y_temp = np.reshape(y_temp, newshape=(y_temp.shape[0], -1))\n    y_true = np.reshape(y_true, newshape=(y_true.shape[0], -1))\n    y_minibatch = np.vstack([y_temp, y_true])\n    \n    permutation = np.random.permutation(len(X_minibatch))\n    \n    X_batch.append(X_minibatch[permutation])\n    y_batch.append(y_minibatch[permutation])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"D. **Outlier Detection**","metadata":{}},{"cell_type":"markdown","source":"We would use three algorithms to detect outliers. If all algorithms predict that instance is an outlier we would discard those instances.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\n\nlof = LocalOutlierFactor(n_jobs=-1)\nIF = IsolationForest(n_jobs=-1, random_state=0, bootstrap=True)\nsvm = OneClassSVM()\n\nfor i in range(len(X_batch)):\n    y1 = lof.fit_predict(X_batch[i])\n    y2 = IF.fit_predict(X_batch[i])\n    y3 = svm.fit_predict(X_batch[i])\n    y_ = y1 + y2 + y3\n    \n    X_batch[i] = X_batch[i][y_ != -3]\n    y_batch[i] = y_batch[i][y_ != -3]\n    print(f\"Minibatch {i+1} : \", X_batch[i].shape, y_batch[i].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"E. **Model Preparation**","metadata":{}},{"cell_type":"markdown","source":"First let's create the base estimators that would be used","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n\nlog_reg = LogisticRegression(max_iter=50000, class_weight=\"balanced\", n_jobs=-1)\nsvm_clf = LinearSVC(max_iter=50000, class_weight=\"balanced\")\ntree_clf = DecisionTreeClassifier(class_weight=\"balanced\")\nextra_clf = ExtraTreeClassifier(class_weight=\"balanced\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next the final aggregator model is made. Since multi layer stacking is used, the aggreagtor function itself is stack.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nestimators = [\n    (\"svm\", svm_clf),\n    (\"tree\", tree_clf),\n    (\"extra_tree\", extra_clf)\n]\n\naggregator = StackingClassifier(\n    estimators=estimators,\n    final_estimator=log_reg,\n    n_jobs=-1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally the base layer of the stack is made. They all use bagging or boosting in order to combine the base estimators.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\n\nlog_adaboost = AdaBoostClassifier(\n    base_estimator=log_reg, n_estimators=100,\n    algorithm=\"SAMME.R\"\n)\n\nlog_bag = BaggingClassifier(\n    log_reg, n_estimators=100, n_jobs=-1,\n    max_samples=1.0, bootstrap=True,\n    max_features=1.0, bootstrap_features=True,\n    oob_score=True\n)\n\nsvm_adaboost = AdaBoostClassifier(\n    base_estimator=svm_clf, n_estimators=100,\n    algorithm=\"SAMME\"\n)\n\nsvm_bag = BaggingClassifier(\n    svm_clf, n_estimators=100, n_jobs=-1,\n    max_samples=1.0, bootstrap=True,\n    max_features=1.0, bootstrap_features=True,\n    oob_score=True\n)\n\nrandom_forest = RandomForestClassifier(\n    n_estimators=100, n_jobs=-1,\n    bootstrap=True, oob_score=True,\n    class_weight=\"balanced_subsample\", min_samples_leaf=0.05\n)\n\nextra_trees = ExtraTreesClassifier(\n    n_estimators=100, n_jobs=-1,\n    bootstrap=True, oob_score=True,\n    class_weight=\"balanced_subsample\", min_samples_leaf=0.05\n)\n\ngb_boost = GradientBoostingClassifier(\n    n_estimators=100, subsample=0.8,\n    min_samples_leaf=0.1, min_samples_split=0.2\n)\n\nxg_boost = XGBClassifier(\n    n_estimators=100, n_jobs=-1\n)\n\nestimators = [\n    (\"logistic_bagging\", log_bag),\n    (\"logistic_boosting\", log_adaboost),\n    (\"svm_bagging\", svm_bag),\n    (\"svm_boosting\", svm_adaboost),\n    (\"random_forest\", random_forest),\n    (\"extra_trees\", extra_trees),\n    (\"gradient_boosting\", gb_boost),\n    (\"xg_boost\", xg_boost)\n]\n\nminibatch_classifier = StackingClassifier(\n    estimators=estimators,\n    final_estimator=aggregator,\n    n_jobs=-1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we would one instance of the multi stack for each of the minibatches and use soft voting to determine the final prediction.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\nclass Classifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, estimator):\n        self.estimator_ = estimator\n        self.fitted_estimators_ = []\n        self.classes_ = [0, 1]\n        \n    def fit(self, X, y):\n        for X_minibatch, y_minibatch in zip(X, y):\n            estimator = clone(self.estimator_)\n            estimator.fit(X_minibatch, y_minibatch.ravel())\n            self.fitted_estimators_.append(estimator)\n            \n    def predict_proba(self, X):\n        probability = []\n        for estimator in self.fitted_estimators_:\n            probability.append(estimator.predict_proba(X))\n        probability = np.array(probability)\n        return np.mean(probability, axis=0)\n    \n    def predict(self, X):\n        probability = self.predict_proba(X)\n        return np.argmax(probability, axis=1)        \n        \nclassifier = Classifier(minibatch_classifier)\nclassifier.fit(X_batch, y_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"F. **Model Evaluation**","metadata":{}},{"cell_type":"markdown","source":"Now let's compute the various performance metrices according to the predictions on the test set. Notice that since stacking classifier is used there is no hyperparameter to tune. Later we may attempt to tune each component of the stacking classifier separately.","metadata":{}},{"cell_type":"code","source":"y_predict = classifier.predict(X_prepared)\ny_test_predict = classifier.predict(X_test_prepared)\nprint(y_predict)\nprint(y_test_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix, plot_det_curve\nfrom sklearn.metrics import plot_precision_recall_curve, plot_roc_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion matrix and various curves that characterize classifier on training set.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(20, 15))\nplot_confusion_matrix(classifier, X_prepared, y_prepared, ax=ax[0, 0])\nplot_det_curve(classifier, X_prepared, y_prepared, ax=ax[0, 1])\nplot_precision_recall_curve(classifier, X_prepared, y_prepared, ax=ax[1, 0])\nplot_roc_curve(classifier, X_prepared, y_prepared, ax=ax[1, 1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation metrices on training set","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy : \", accuracy_score(y_prepared, y_predict))\nprint(\"Precision : \", precision_score(y_prepared, y_predict))\nprint(\"Recall : \", recall_score(y_prepared, y_predict))\nprint(\"F1_score : \", f1_score(y_prepared, y_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion matrix and various curves that characterize the classifier on test set.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(20, 15))\nplot_confusion_matrix(classifier, X_test_prepared, y_test_prepared, ax=ax[0, 0])\nplot_det_curve(classifier, X_test_prepared, y_test_prepared, ax=ax[0, 1])\nplot_precision_recall_curve(classifier, X_test_prepared, y_test_prepared, ax=ax[1, 0])\nplot_roc_curve(classifier, X_test_prepared, y_test_prepared, ax=ax[1, 1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation metrices on test set.","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy : \", accuracy_score(y_test_prepared, y_test_predict))\nprint(\"Precision : \", precision_score(y_test_prepared, y_test_predict))\nprint(\"Recall : \", recall_score(y_test_prepared, y_test_predict))\nprint(\"F1_score : \", f1_score(y_test_prepared, y_test_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}