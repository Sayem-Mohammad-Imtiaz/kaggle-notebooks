{"cells":[{"metadata":{},"cell_type":"markdown","source":"## In this notebook\n* Exploratory Data Analysis\n* Data Preprocessing\n    1. Feature Selection\n    2. Outlier Treatment\n    3. Data Transformation\n    4. Encoding \n* Checking Performance of models without performing hyperparameter tuning\n\nAny suggestions and comments are welcome :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing important Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np   \nimport pandas as pd    \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport scipy.stats as stats\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,train_test_split,cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom scipy.stats import randint\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#Models\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data\n# df = pd.read_csv('train.csv')\ndf = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking top 5 rows of data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Shape\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 381109 insured customers and 11 variables containing their information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Information\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Region_Code & Policy_Sales_Channel to integer\ndf.Region_Code = df.Region_Code.astype('int64')\ndf.Policy_Sales_Channel = df.Policy_Sales_Channel.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.Region_Code = test.Region_Code.astype('int64')\ntest.Policy_Sales_Channel = test.Policy_Sales_Channel.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking description of numerical columns\ndf.describe(include = 'number')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking value counts for numerical columns\nfor col in df.columns:\n    if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n        print(col,\":\",df[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Region_Code and Policy_Sales_Channel are nominal in nature\n* Vintage is the count (number of days) so ordinal in nature\n* Driving_License, Previously_Insured and Response have binary categories\n* Age and Annual_Premium are continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking description of object columns\ndf.describe(include = 'object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for duplicates\ndups = df.duplicated()\ndups.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the duplicates\ndf.drop_duplicates(inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variable count\ngraph = sns.countplot(df.Response)\nfor p in graph.patches:\n    graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),ha='center', va='bottom',color= 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Response.value_counts().plot(kind='pie', autopct='%1.0f%%');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is highly imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counts of Categorical variables\nfig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    graph = sns.countplot(df[x[i]])\n    \n    for p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.35, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing continuous variables\nfig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(14, 14)\na = sns.distplot(df['Age'] , ax=axes[0][0])\na.set_title(\"Age\",fontsize=15)\na = sns.boxplot(df['Age'] , orient = \"v\" , ax=axes[0][1])\na.set_title(\"Age\",fontsize=15)\n\na = sns.distplot(df['Annual_Premium'] , ax=axes[1][0])\na.set_title(\"Annual_Premium\",fontsize=15)\na = sns.boxplot(df['Annual_Premium'] , orient = \"v\" , ax=axes[1][1])\na.set_title(\"Annual_Premium\",fontsize=15)\nplt.show()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables are highly right skewed and premium column has a lot of outliers too."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 regions with highest number of insurers\nlabels= df['Region_Code'].value_counts()[:10].keys()\nvalues= df['Region_Code'].value_counts()[:10]\n\nplt.figure(figsize = (15, 5))\ngraph = sns.barplot(x = labels, y = values)\n\nfor p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 policy channels covering highest number of insurers\nlabels= df['Policy_Sales_Channel'].value_counts()[:10].keys()\nvalues= df['Policy_Sales_Channel'].value_counts()[:10]\n\nplt.figure(figsize = (15, 5))\ngraph = sns.barplot(x = labels, y = values)\n\nfor p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.Vintage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pearson Correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),annot=True,mask=np.triu(df.corr(),+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not much correlation between variables except slight negative correlation between Age and Policy Sales channel."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    graph = sns.countplot(df[x[i]],hue = df['Response'])\n    \n    for p in graph.patches:\n        graph.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Gender seems to have similar reponse wrt vehicle insurance\n* There are very few customers who didn't have driving license on them\n* Hardly any customer has opted for vehicle insurance if they were already insured\n* Very few customers have their vehicle age > 2years\n* Customers whose vehicle was not previously damaged didn't seem to be interested in vehicle insurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df['Vehicle_Damage'],df['Previously_Insured'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df['Previously_Insured'],df['Response'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df['Vehicle_Age'],df['Previously_Insured'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    sns.pointplot(df[x[i]],df['Response'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Males have a higher chance to be interested in vehicle insurance than females\n* Customer having driving license have a higher chance to be interested in vehicle insurance\n* Customer who don't already have vehicle insurance are more likely to be interested in vehicle insurance\n* Customer having vehicle age >2 years are highly likely to be interested in vehicle insurance followed by 1-2 years of age\n* Customer who got his/her vehicle damaged in the past have a higher chance to be interested in vehicle insurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3,ncols=3,  figsize=(20,20))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Age','Annual_Premium','Vintage']\nfor i in range(0,len(x)):\n    sns.barplot(df['Response'],df[x[i]],ax=axes[i][0])\n    sns.violinplot(df['Response'],df[x[i]],ax=axes[i][1])\n    sns.boxplot(df['Response'],df[x[i]],ax=axes[i][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Customer who are interested in vehicle insurance have a higher age bracket\n* Annual premium for both groups are similar with people interested having slightly higher mean annual premium\n* There is no significance of number of days of insurer associated with the company,  their distribution is same regardless of the response"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(20,12))\nfig.subplots_adjust(hspace = .3, wspace=.2)\nx = ['Gender','Vehicle_Age','Vehicle_Damage','Driving_License','Previously_Insured']\nfor i in range(0,len(x)):\n    ax=fig.add_subplot(2,3,i+1).set_title(x[i])\n    sns.barplot(df[x[i]],df['Annual_Premium'],hue = df['Response'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No difference in annual premium charged wrt gender\n* Vehicles having age >2 years have higher annual premium'\n* **Customers who are interested in vehicle insurance have a lower annual premium**\n* People not having driving license have a slightly higher annual premium\n* **Customers who are previously insured and interested in vehicle insurance have a lower annual premium**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot('Vehicle_Age','Age',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean age of customers with vehicle age < 1 year is around early 20s. It is around late 40s for vehicle age between 1-2 years and mid 50s for vehicle age > 2 years"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot('Vehicle_Damage','Age',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean age of cutomers who got his/her vehicle damaged in the past is higher and around mid 40s as compared to who didn't had their vehicles damaged with mean age around early 30s"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pointplot('Vehicle_Damage','Driving_License',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The people who didn't got their vehicle damaged in the past had a slightly higher proportion of having a driving license."},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature_importance = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature_importance.replace({'< 1 Year': 0,'1-2 Year': 1,'> 2 Years': 2},inplace=True)\ndf_feature_importance = pd.get_dummies(df_feature_importance, columns=['Gender','Vehicle_Damage'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_importance(model):\n    x=pd.DataFrame(model.feature_importances_*100,index=X_train.columns).sort_values(by=0,ascending=False)\n    plt.figure(figsize=(12,7))\n    sns.barplot(x[0],x.index,palette='rainbow')\n    plt.ylabel('Feature Name')\n    plt.xlabel('Feature Importance in %')\n    plt.title('Feature Importance Plot')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy all the predictor variables into X dataframe\nX = df_feature_importance.drop('Response', axis=1)\n\n# Copy target into the y dataframe\ny = df_feature_importance['Response']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb_classfr = lgb.LGBMClassifier(objective='binary', \n         boosting_type = 'gbdt', \n         n_estimators = 10000)\nlgb_classfr.fit(X_train, y_train, early_stopping_rounds=200, verbose = 200, eval_set = [(X_test, y_test)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance(lgb_classfr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance(rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that driving license has been provided with importance next to 0 hence we will go ahead and drop the variable in upcoming steps"},{"metadata":{},"cell_type":"markdown","source":"## Outlier Treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_outlier = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df_outlier['Annual_Premium'].quantile(0.25)\nQ3 = df_outlier['Annual_Premium'].quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(3 * IQR)\nupper_range= Q3+(3 * IQR)\nprint('Number of Outliers:')\n((df_outlier['Annual_Premium'] < (lower_range)) | (df_outlier['Annual_Premium'] > (upper_range))).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((((df_outlier['Annual_Premium'] < (lower_range)) | (df_outlier['Annual_Premium'] > (upper_range))).sum())/df_outlier.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Upper Range: {}\\nLower Range:{}'.format(upper_range,lower_range))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_outlier['Annual_Premium']=np.where(df_outlier['Annual_Premium']>upper_range,upper_range,df_outlier['Annual_Premium'])\ndf_outlier['Annual_Premium']=np.where(df_outlier['Annual_Premium']<lower_range,lower_range,df_outlier['Annual_Premium'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df_outlier['Annual_Premium'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_outlier['Annual_Premium'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = test['Annual_Premium'].quantile(0.25)\nQ3 = test['Annual_Premium'].quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(3 * IQR)\nupper_range= Q3+(3 * IQR)\nprint('Number of Outliers:')\n((test['Annual_Premium'] < (lower_range)) | (test['Annual_Premium'] > (upper_range))).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Annual_Premium']=np.where(test['Annual_Premium']>upper_range,upper_range,test['Annual_Premium'])\ntest['Annual_Premium']=np.where(test['Annual_Premium']<lower_range,lower_range,test['Annual_Premium'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age and annual premium highly skewed. Not normally distributed. Both right skewed with non negative values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to plot a histogram and a Q-Q plot\n# side by side, for a certain variable\ndef diagnostic_plots(X,var):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    plt.title(var)\n    X.hist()\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(X, dist=\"norm\", plot=plt)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def box_plots(X,Y,var1,var2):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    plt.title(var1)\n    sns.boxplot(X)\n\n    plt.subplot(1, 2, 2)\n    plt.title(var2)\n    sns.boxplot(Y)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(df_outlier[i],i)\n    print('Skewness for',i,\":\",df_outlier[i].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logarithmic Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(np.log(df_outlier[i]+1),i)\n    print('Skewness for',i,\":\",np.log(df_outlier[i]+1).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plots(np.log(df_outlier['Age']+1),np.log(df_outlier['Annual_Premium']+1),'Age','Annual_Premium')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sqaure Root Tranformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(df_outlier[i]**(1/2),i)\n    print('Skewness for',i,\":\",(df_outlier[i]**(1/2)).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plots(df_outlier['Age']**(1/2),df_outlier['Annual_Premium']**(1/2),'Age','Annual_Premium')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cube Root Tranformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(df_outlier[i]**(1/3),i)\n    print('Skewness for',i,\":\",(df_outlier[i]**(1/3)).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plots(df_outlier['Age']**(1/3),df_outlier['Annual_Premium']**(1/3),'Age','Annual_Premium')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reciprocal transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Age','Annual_Premium']:\n    diagnostic_plots(1/(df_outlier[i]+1),i)\n    print('Skewness for',i,\":\",(1/(df_outlier[i]+1)).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plots(1/(df_outlier['Age']+1),1/(df_outlier['Annual_Premium']+1),'Age','Annual_Premium')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BoxCox Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_outlier['Age_boxcox'], param = stats.boxcox(df_outlier.Age) \ndf_outlier['Annual_Premium_boxcox'], param = stats.boxcox(df_outlier.Annual_Premium) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Age_boxcox'], param = stats.boxcox(test.Age) \ntest['Annual_Premium_boxcox'], param = stats.boxcox(test.Annual_Premium) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Age_boxcox','Annual_Premium_boxcox']:\n    diagnostic_plots(df_outlier[i],i)\n    print('Skewness for',i,\":\",df_outlier[i].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plots(df_outlier['Age_boxcox'],df_outlier['Annual_Premium_boxcox'],'Age','Annual_Premium')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_outlier.drop(['id','Age','Annual_Premium','Driving_License'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['id','Age','Annual_Premium','Driving_License'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_outlier.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df_outlier.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.replace({'< 1 Year': 0,'1-2 Year': 1,'> 2 Years': 2},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.replace({'< 1 Year': 0,'1-2 Year': 1,'> 2 Years': 2},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dummy variable encoding\ndf1 = pd.get_dummies(df1, columns=['Gender','Vehicle_Damage'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.get_dummies(test, columns=['Gender','Vehicle_Damage'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pearson Correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(df1.corr(),annot=True,mask=np.triu(df1.corr(),+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A strong positive correlation exists between Vehicle age and age\n* A strong negative correlation exists between Vehicle damage and previously insured"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking level of multicollinearity with VIF of all variables\nX = df1.drop('Response', axis=1)\nvif = [variance_inflation_factor(X.values, ix) for ix in range(X.shape[1])] \ni=0\nfor column in X.columns:\n  print (column ,\"--->\",  vif[i])\n  i = i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value is pretty high for age variable, hence we will drop it off and again check the level of multicolliearity of other variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.drop('Age_boxcox',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop('Age_boxcox',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df1.drop('Response', axis=1)\nvif = [variance_inflation_factor(X.values, ix) for ix in range(X.shape[1])] \ni=0\nfor column in X.columns:\n  print (column ,\"--->\",  vif[i])\n  i = i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the values are around 5 which is acceptable hence we will proceed with the remaining variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in Policy_Sales_Channel: {}\\nUnique values in Region_Code: {}\".format(df1.Policy_Sales_Channel.nunique(),df1.Region_Code.nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Mean encoding for Policy_Sales_Channel and Region_Code\ndf_mean_encode = df1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encod_type_Region_Code = df_mean_encode.groupby('Region_Code')['Response'].mean()\nencod_type_Policy_Sales_Channel = df_mean_encode.groupby('Policy_Sales_Channel')['Response'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mean_encode.loc[:, 'Region_Code'] = df_mean_encode['Region_Code'].map(encod_type_Region_Code)\ndf_mean_encode.loc[:, 'Policy_Sales_Channel'] = df_mean_encode['Policy_Sales_Channel'].map(encod_type_Policy_Sales_Channel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[:, 'Region_Code'] = test['Region_Code'].map(encod_type_Region_Code)\ntest.loc[:, 'Policy_Sales_Channel'] = test['Policy_Sales_Channel'].map(encod_type_Policy_Sales_Channel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mean_encode.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Frequency encoding for Policy_Sales_Channel and Region_Code\ndf_frequency_encode = df1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_frequency_map_region = df_frequency_encode.Region_Code.value_counts().to_dict()\ndf_frequency_map_Policy_Sales_Channel = df_frequency_encode.Policy_Sales_Channel.value_counts().to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_frequency_encode['Region_Code'] = df_frequency_encode['Region_Code'].map(df_frequency_map_region)\ndf_frequency_encode['Policy_Sales_Channel'] = df_frequency_encode['Policy_Sales_Channel'].map(df_frequency_map_Policy_Sales_Channel)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_frequency_encode.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KDD encoding"},{"metadata":{},"cell_type":"markdown","source":"One hot encoding only the top 10 categories of Policy_Sales_Channel and Region_Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"## KDD encoding for Policy_Sales_Channel and Region_Code\ndf_KDD_encode = df1.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to create the dummy variables for the most frequent labels\n# we can vary the number of most frequent labels that we encode\ndef one_hot_encoding_top_x(df, variable, x):\n    \n    top_x_labels = [y for y in df[variable].value_counts().sort_values(ascending=False).head(x).index]\n    \n    for label in top_x_labels:\n        df[variable+'_'+str(label)] = np.where(df[variable]==label, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_encoding_top_x(df_KDD_encode, 'Region_Code', 10)\none_hot_encoding_top_x(df_KDD_encode, 'Policy_Sales_Channel', 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_KDD_encode.drop(['Region_Code','Policy_Sales_Channel'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_KDD_encode.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting & Scaling Data"},{"metadata":{},"cell_type":"markdown","source":"* df_mean_encode\n* df_frequency_encode\n* df_KDD_encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"mm = MinMaxScaler()\ndef split_data(dataframe):\n    # Copy all the predictor variables into X dataframe\n    X = dataframe.drop('Response', axis=1)\n    \n    # Copy target into the y dataframe\n    y = dataframe['Response']\n    \n    # Split X and y into training and test set in 70:30 ratio\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)\n    \n    # Transform data using MinMaxScaler\n    X_trains = mm.fit_transform(X_train)\n    X_tests = mm.transform(X_test)\n    return X_trains,X_tests,y_train,y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Models"},{"metadata":{},"cell_type":"markdown","source":"**Models tested:**\n* Naive Bayes (as benchmark)\n* Decision Tree\n* Random Forest\n* Logistic Regression\n* LDA\n* XGBoost\n* LightGBM\n\nFirst building the models without any hyperparameters to check impact of encoding results on different models."},{"metadata":{},"cell_type":"markdown","source":"## Mean Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trains,X_tests,y_train,y_test = split_data(df_mean_encode);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = mm.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_mean_encode = pd.DataFrame({'accuracy_train':[],'accuracy_test':[],'f1_score_train':[],'f1_score_test': [],'recall_train': [], 'recall_test': [],'precision_train': [],'precision_test': [],'AUC_train': [],'AUC_test': []})\nmodels = [GaussianNB(), DecisionTreeClassifier(random_state=1), RandomForestClassifier(random_state=1), LogisticRegression(random_state = 1), LinearDiscriminantAnalysis(), xgb.XGBClassifier(random_state=1), LGBMClassifier(random_state=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    model.fit(X_trains, y_train)\n    results_mean_encode = results_mean_encode.append({'accuracy_train':metrics.accuracy_score(y_train, model.predict(X_trains)),\n                                                      'accuracy_test': metrics.accuracy_score(y_test, model.predict(X_tests)),\n                                                      'f1_score_train': metrics.f1_score(y_train, model.predict(X_trains)),\n                                                      'f1_score_test': metrics.f1_score(y_test, model.predict(X_tests)),\n                                                      'recall_train': metrics.recall_score(y_train, model.predict(X_trains)), \n                                                      'recall_test': metrics.recall_score(y_test, model.predict(X_tests)),\n                                                      'precision_train': metrics.precision_score(y_train, model.predict(X_trains)),\n                                                      'precision_test': metrics.precision_score(y_test, model.predict(X_tests)),\n                                                      'AUC_train': roc_auc_score(y_train,model.predict_proba(X_trains)[:,1]),\n                                                      'AUC_test': roc_auc_score(y_test,model.predict_proba(X_tests)[:,1])\n                                                     }, ignore_index=True)\n    \nresults_mean_encode['Models'] = ['Naive Bayes','Decision Tree','Random Forest','Logistic Regression','LDA','XGBoost','LightGBM']\nresults_mean_encode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Frequency Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trains,X_tests,y_train,y_test = split_data(df_frequency_encode);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_frequency_encode = pd.DataFrame({'accuracy_train':[],'accuracy_test':[],'f1_score_train':[],'f1_score_test': [],'recall_train': [], 'recall_test': [],'precision_train': [],'precision_test': [],'AUC_train': [],'AUC_test': []})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    model.fit(X_trains, y_train)\n    results_frequency_encode = results_frequency_encode.append({'accuracy_train':metrics.accuracy_score(y_train, model.predict(X_trains)),\n                                                                'accuracy_test': metrics.accuracy_score(y_test, model.predict(X_tests)),\n                                                                'f1_score_train': metrics.f1_score(y_train, model.predict(X_trains)),\n                                                                'f1_score_test': metrics.f1_score(y_test, model.predict(X_tests)),\n                                                                'recall_train': metrics.recall_score(y_train, model.predict(X_trains)), \n                                                                'recall_test': metrics.recall_score(y_test, model.predict(X_tests)),\n                                                                'precision_train': metrics.precision_score(y_train, model.predict(X_trains)),\n                                                                'precision_test': metrics.precision_score(y_test, model.predict(X_tests)),\n                                                                'AUC_train': roc_auc_score(y_train,model.predict_proba(X_trains)[:,1]),\n                                                                'AUC_test': roc_auc_score(y_test,model.predict_proba(X_tests)[:,1])\n                                                               }, ignore_index=True)\n    \nresults_frequency_encode['Models'] = ['Naive Bayes','Decision Tree','Random Forest','Logistic Regression','LDA','XGBoost','LightGBM']\nresults_frequency_encode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KDD Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trains,X_tests,y_train,y_test = split_data(df_KDD_encode);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_KDD_encode = pd.DataFrame({'accuracy_train':[],'accuracy_test':[],'f1_score_train':[],'f1_score_test': [],'recall_train': [], 'recall_test': [],'precision_train': [],'precision_test': [],'AUC_train': [],'AUC_test': []})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    model.fit(X_trains, y_train)\n    results_KDD_encode = results_KDD_encode.append({'accuracy_train':metrics.accuracy_score(y_train, model.predict(X_trains)),\n                                                    'accuracy_test': metrics.accuracy_score(y_test, model.predict(X_tests)),\n                                                    'f1_score_train': metrics.f1_score(y_train, model.predict(X_trains)),\n                                                    'f1_score_test': metrics.f1_score(y_test, model.predict(X_tests)),\n                                                    'recall_train': metrics.recall_score(y_train, model.predict(X_trains)), \n                                                    'recall_test': metrics.recall_score(y_test, model.predict(X_tests)),\n                                                    'precision_train': metrics.precision_score(y_train, model.predict(X_trains)),\n                                                    'precision_test': metrics.precision_score(y_test, model.predict(X_tests)),\n                                                    'AUC_train': roc_auc_score(y_train,model.predict_proba(X_trains)[:,1]),\n                                                    'AUC_test': roc_auc_score(y_test,model.predict_proba(X_tests)[:,1])\n                                                    }, ignore_index=True)\n    \nresults_KDD_encode['Models'] = ['Naive Bayes','Decision Tree','Random Forest','Logistic Regression','LDA','XGBoost','LightGBM']\nresults_KDD_encode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best results considering performance of all models seems to be from mean encoding method hence we will proceed with that for model building. In that best AUC scores were for XGBoost and it will be considered for submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trains,X_tests,y_train,y_test = split_data(df_mean_encode);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = mm.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = xgb.XGBClassifier(random_state=1)\nXGB.fit(X_trains,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result =  XGB.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame({'id': test1.id, 'Response': result})\nsubmit.to_csv('Submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The F1-score, recall and precision values are coming very poor for the models which is not desirable as per the business problems hence I would be applying sampling technique and tuning the hyperparameters in an attempt to increase performance of models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}