{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"It is the kernel that I have tried and compiled from the courses of [İbrahim Cebeci](https://www.udemy.com/user/ibrahim-cebeci-2/) (Language of the courses is Turkish: [Doğal Dil İşleme A-Z™: (NLP)](https://www.udemy.com/course/dogal-dil-isleme/)), which is has more than 6 courses on Udemy.\n\n# Content\n\n\n#### [1.StopWords - Stemmer - Count Vectorizer](#1.)\n#### [2.Reg.Exp.- Lemmatization - Bag of Words](#2.)\n#### [3.NLTK - Word2Vec(SkipGram,CBOW) - Glove](#3.)\n\n![](https://iili.io/JGUWve.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"1.\"></a> \n# 1.StopWords - Stemmer - Count Vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nimport nltk\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords.words('english')\n\nyorumlar = pd.read_csv('../input/restaurant-reviews/Restaurant_Reviews.csv')\nyorumlar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regular Expression\n### SparceMatrix-StopWords-Stemmer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import re #regular expression\n#Preprocessing (Önişleme) -- SparceMatrix-StopWords-Stemmer\nderlem = []\nfor i in range(len(yorumlar)):\n    yorum = re.sub('[^a-zA-Z]',' ',yorumlar['Review'][i])\n    yorum = yorum.lower()\n    yorum = yorum.split()\n    yorum = [ps.stem(kelime) for kelime in yorum if not kelime in set(stopwords.words('english'))] #stopwords.words('turkish')\n    yorum = ' '.join(yorum) #vektor için list değil string hali gerekli\n    derlem.append(yorum)\n    \nderlem[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CountVectorizer\n### Bag of Words (BOW)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feautre Extraction ( Öznitelik Çıkarımı) -- CountVectorizer\n#Bag of Words (BOW)\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 2000) #kelımelerden en cok kullanılan kac tane alınsın \nX = cv.fit_transform(derlem).toarray() # bağımsız değişken\ny = yorumlar.iloc[:,1].values # bağımlı değişken\nprint(X[0:5])\nprint(y[0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# visualize number of digits classes\nplt.figure(figsize=(15,7))\nsns.countplot(y)\nplt.title(\"Liked\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\n\ny_pred = gnb.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\n#print(cm)\nprint(\"score: \",gnb.score(X_test,y_test))\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"2.\"></a> \n# 2.Reg.Exp.- Lemmatization - Bag of Words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk # natural language tool kit\nnltk.download(\"stopwords\")      # corpus diye bir kalsore indiriliyor\nfrom nltk.corpus import stopwords  # sonra ben corpus klasorunden import ediyorum\n\nimport nltk as nlp\nlemma = nlp.WordNetLemmatizer()\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n\n# %% import twitter data\ndata = pd.read_csv(r\"../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv\",encoding = \"latin1\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data.gender,data.description],axis=1)\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(axis = 0,inplace = True)\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender]\nprint(data.head())\nprint(data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clening data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% clening data \ndescription_list = []\nfor description in data.description:\n    # regular expression RE mesela \"[^a-zA-Z]\"\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n    # description = description.split()\n    # split yerine tokenizer kullanabiliriz\n    # split kullanırsak \"shouldn't \" gibi kelimeler \"should\" ve \"not\" diye ikiye ayrılmaz ama word_tokenize() kullanirsak ayrilir\n    description = nltk.word_tokenize(description)\n    # stopwords (irrelavent words) gereksiz kelimeler\n    description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n    # lemmatazation loved => love   gitmeyecegim = > git\n    lemma = nlp.WordNetLemmatizer()\n    description = [ lemma.lemmatize(word) for word in description] #[ ps.stem(word) for word in description]\n    description = \" \".join(description)  #vektor için list değil string hali gerekli\n    description_list.append(description)\ndescription_list[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bag of Words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer # bag of words yaratmak icin kullandigim metot\nmax_features = 5000 #kelımelerden en cok kullanılan kac tane alınsın\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x bağımsız değişken\nprint(\"en sik kullanilan {} kelimeden bazıları: {}\".format(max_features,count_vectorizer.get_feature_names()[0:5]))\n\ny = data.iloc[:,0].values   # male or female classes\nx = sparce_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# visualize number of digits classes\nplt.figure(figsize=(15,7))\nsns.countplot(y)\nplt.title(\"male or female classes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)\n\n# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n#%% prediction\ny_pred = nb.predict(x_test)\n\nprint(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"3.\"></a> \n# 3.NLTK - Word2Vec(SkipGram,CBOW) - Glove","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## NLTK-Tokenize","execution_count":null},{"metadata":{"trusted":true,"_uuid":"91089a96def02f009eae6f7a607cbca06bb4f873"},"cell_type":"code","source":"# NLTK-Tokenize\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ntext = \"Alan Mathison Turing was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7ad9d0cd5583550451243b0b21cb52336fd75a4"},"cell_type":"code","source":"text.split()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd2e7f59b45a892996fc797d944083d74fec56c"},"cell_type":"code","source":"word_tokenize(text)[0:10] #kelıme tokenlestırme","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ae07f0404b5da99d6e4022e4c9d73b1eb2bb010"},"cell_type":"code","source":"sent_tokenize(text)[0:10] #cumle tokenlestırme","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLTK-StopWords","execution_count":null},{"metadata":{"trusted":true,"_uuid":"728279c5e90df95fb2c96077b38f7005139844c9"},"cell_type":"code","source":"# NLTK-StopWords\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ntext = 'Fazıl Say is a Turkish pianist and composer who was born in Ankara, described recently as \"not merely a pianist of genius; but undoubtedly he will be one of the great artists of the twenty-first century\".'\n\nstopwords = stopwords.words('english')\nprint(stopwords[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc3ea891ad8f31780306f0454a96879ee7e7a898"},"cell_type":"code","source":"words = word_tokenize(text)\nfiltered_words = []\nfor word in words:\n    if word not in stopwords:\n        filtered_words.append(word)\n        \nfiltered_words[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLTK-Stemmer","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6444b7849b36d27b2efaf518b492badda11e8b79"},"cell_type":"code","source":"# NLTK-Stemmer\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n#words = ['Boyunluk', 'Boynu', 'Boylar', 'Boyun', 'Boy']\n\nfor w in words:\n    print(ps.stem(w))\n    \n#kelimenin sonundaki ekler kesiliyor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLTK-Part of Speech Tagging","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1c769865bc25657272d193c567eaaf33aa18e508"},"cell_type":"code","source":"# NLTK-Part of Speech Tagging\nimport nltk\n\ntext = 'Friedrich Wilhelm Nietzsche was a German philosopher, cultural critic, composer, poet, philologist, and a Latin and Greek scholar whose work has exerted a profound influence on Western philosophy and modern intellectual history. He began his career as a classical philologist before turning to philosophy. He became the youngest ever to hold the Chair of Classical Philology at the University of Basel in 1869 at the age of 24. Nietzsche resigned in 1879 due to health problems that plagued him most of his life; he completed much of his core writing in the following decade. In 1889 at age 44, he suffered a collapse and afterward, a complete loss of his mental faculties. He lived his remaining years in the care of his mother until her death in 1897 and then with his sister Elisabeth Förster-Nietzsche. Nietzsche died in 1900.'\ntokenized = nltk.word_tokenize(text)\ntokenized[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44ee917792c47d76fcc0e63354e4a4411f10b68d"},"cell_type":"code","source":"\"\"\"\nCC     coordinating conjunction\nCD     cardinal digit\nDT     determiner\nEX     existential there (like: \"there is\" ... think of it like \"there exists\")\nFW     foreign word\nIN     preposition/subordinating conjunction\nJJ     adjective 'big'\nJJR    adjective, comparative 'bigger'\nJJS    adjective, superlative 'biggest'\nLS     list marker 1)\nMD     modal could, will\nNN     noun, singular 'desk'\nNNS    noun plural 'desks'\nNNP    proper noun, singular 'Harrison'\nNNPS   proper noun, plural 'Americans'\nPDT    predeterminer 'all the kids'\nPOS    possessive ending parent's\nPRP    personal pronoun I, he, she\nPRP$   possessive pronoun my, his, hers\nRB     adverb very, silently,\nRBR    adverb, comparative better\nRBS    adverb, superlative best\nRP     particle give up\nTO     to go 'to' the store.\nUH     interjection errrrrrrrm\nVB     verb, base form take\nVBD    verb, past tense took\nVBG    verb, gerund/present participle taking\nVBN    verb, past participle taken\nVBP    verb, sing. present, non-3d take\nVBZ    verb, 3rd person sing. present takes\nWDT    wh-determiner which\nWP     wh-pronoun who, what\nWP$    possessive wh-pronoun whose\nWRB    wh-abverb where, when\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c3fa8346699e0083267ad7b00dc4656da31d671"},"cell_type":"code","source":"nltk.pos_tag(tokenized)[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLTK-named entitiy recognition","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d272ce9daf1cd2b8cd0d71f55a0b6d3e6628251e"},"cell_type":"code","source":"# NLTK-named entitiy recognition\n\nimport nltk\ntext = \"Steve Jobs was an American entrepreneur and business magnate. He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc., chairman and majority shareholder of Pixar, a member of The Walt Disney Company's board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT. Jobs is widely recognized as a pioneer of the microcomputer revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak. \"\ntokenized = nltk.word_tokenize(text)\nprint(tokenized[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b4025cc459717bee65101d8a54bbc0aa3db0ad0"},"cell_type":"code","source":"tagged = nltk.pos_tag(tokenized)\nprint(tagged[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ee5a0d5367f1b670541aced1f05984246f23bf0"},"cell_type":"code","source":"named_ent = nltk.ne_chunk(tagged)\nprint(named_ent[0:10])\n#named_ent.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2dc5445cd47c898086d8cec166df1966fa20778"},"cell_type":"code","source":"\"\"\"\nNE Türü         \tÖrnek\nORGANIZATION    \tGeorgia-Pacific Corp., WHO\nPERSON          \tEddy Bonte, President Obama\nLOCATION        \tMurray River, Mount Everest\nDATE            \tJune, 2008-06-29\nTIME            \ttwo fifty a m, 1:30 p.m.\nMONEY           \t175 million Canadian Dollars, GBP 10.40\nPERCENT         \ttwenty pct, 18.75 %\nFACILITY        \tWashington Monument, Stonehenge\nGPE             \tSouth East Asia, Midlothian\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLTK-Lemmatizing","execution_count":null},{"metadata":{"trusted":true,"_uuid":"8bdd6dbd9521a2349747d905b7fd08d2d8348c9f"},"cell_type":"code","source":"# NLTK-Lemmatizing\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\nfor w in words:\n    print(lem.lemmatize(w))\n#kelimenin sözlükteki köküne iniliyor (morfolojik)\n\nprint(lem.lemmatize('drove', 'v'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec","execution_count":null},{"metadata":{"trusted":true,"_uuid":"e6f61d7355e8b29a6950e317bdd5808b5e300a86"},"cell_type":"code","source":"#Word2Vec\nimport numpy as np\n#numpy vektor ve matrısler uzerıne ıslem yapmamızı sağlar\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Word2Vec corpus üzerinde tüm kelimelerin üzerinden geç\n# her kelimenin etrafındaki kelimeleri tahmin et\n# iki kelime birbirne ne kadar sık bulunuyorsa vektöre yansıt\nf = open('../input/hurriyet/hurriyet.txt', 'r', encoding='utf8')\ntext = f.read()\nt_list = text.split('\\n')\n\ncorpus = []\n\nfor cumle in t_list:\n    corpus.append(cumle.split())\nprint(corpus[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9671530475321d91fdf06c9094db26b842e396db"},"cell_type":"code","source":"model = Word2Vec(corpus, size=100, window=5, min_count=5, sg=1) #genelde size 5-300 uzunlugunda vektor olustururuz #gozetımsız ogrenme yontemı\n#size 100 uzunlugunda vektor\n#window sol ve sagda bakılacak kelıme sayısı\n#sg 1 ise skip-gram kullanılacak, default olarak cbow kullanılıyor.\n#mın_count kelıme en az kac kere gecıyorsa al \nmodel.wv['ankara']\n# wv word vektorun kısaltılmısı\n#kelime vektörü, word vector, word embedding, embedding hep aynı şeyi ifade ediyor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3f8c8238e64c5066658bdff82813b5aa87c208f"},"cell_type":"code","source":"print(model.wv.most_similar('almanya'))\n#yazdıgınız kelıme kelıme haznesınde yoksa hata alırsınız.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6197e409f1b591ab2a4374c2a454512089a11e57"},"cell_type":"code","source":"#model.save('word2vec.model')\n#model = Word2Vec.load('word2vec.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8df87248700a65baad5f7161523bbb62125124c2"},"cell_type":"code","source":"def closestwords_tsneplot(model, word):\n    '''\n     bu sınıf verılen modelı ıle kelımeyı alır\n     kelımeye en yakın kelımelerın vektorlerını bır dızıye atarız\n     TSNE ıle bu vektorlerı grafıge donustururuz\n    '''\n    word_vectors = np.empty((0,100)) # en yakın olanları lısteyi hazırladık\n    word_labels = [word] #kelımeyı dızı halıne getırdık\n    \n    close_words = model.wv.most_similar(word) #yakın olan kelımeler bulundu\n    \n    word_vectors = np.append(word_vectors, np.array([model.wv[word]]), axis=0) #gelen kelımenın vektoru eklendı\n    \n    for w, _ in close_words: #w kelimenın kendısı dıgerı ıse yakınlık oranı\n        word_labels.append(w)\n        word_vectors = np.append(word_vectors, np.array([model.wv[w]]), axis=0) #yakın kelımelerın vektorlerı de eklendı\n        #boylece  gelen kelıme ve yakın kelımeler word_labels, bunların vektorlerı ise word_vectors akta\n        \n    tsne = TSNE(random_state=0) #kelımeleı grafıge doken kutuphane\n    Y = tsne.fit_transform(word_vectors)\n    \n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    \n    plt.scatter(x_coords, y_coords)\n    \n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(5, -2), textcoords='offset points')\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6912550c8be677064c411a9483ceb93f88be902b"},"cell_type":"code","source":"closestwords_tsneplot(model, 'almanya')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glove","execution_count":null},{"metadata":{"trusted":true,"_uuid":"5447b11f4af54e61818ba15d12e81a59e57877f7"},"cell_type":"code","source":"def read_data(file_name):\n    with open(file_name,'r') as f:\n        word_vocab = set() # not using list to avoid duplicate entry\n        word2vector = {}\n        for line in f:\n            line_ = line.strip() #Remove white space\n            words_Vec = line_.split()\n            word_vocab.add(words_Vec[0])\n            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocab))\n    return word_vocab,word2vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c01dd1f91dfbe0fbcd282e0c640fb3defd888d3"},"cell_type":"code","source":"from gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.models import KeyedVectors # global vectors for word representations (Glove 2014)\n'''\nglove_input =  read_data('../input/glove6b100dtxt/glove.6B.100d.txt')\nword2vec_output =  read_data('../input/glove-vec/glove.6B.100d.word2vec') #word2vec yazıyor ama glove kullanılacak word2vec yazmasının amacı gensım yuklemeyı kolaylastırma\nglove2word2vec(glove_input, word2vec_output)\n\n\nmodel = KeyedVectors.load_word2vec_format(word2vec_output, binary=False)\nmodel['istanbul']\nmodel.most_similar('ankara') #bu yontem word2vec vardı ama glove 6 mılyar kelıme ıcınde daha basarılı\nmodel.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)\n#topn demek sadece n tane goster\nmodel.most_similar(positive=['berlin', 'turkey'], negative=['ankara'], topn=1)\nmodel.most_similar(positive=['teach', 'doctor'], negative=['treat'], topn=1)\n\n\n'''\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}