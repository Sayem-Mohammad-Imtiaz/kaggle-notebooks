{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Disease Prediction using Neural Networks\n\nThis project will focus on predicting heart disease using neural networks. Based on attributes such as blood pressure, cholestoral levels, heart rate, and other characteristic attributes, patients will be classified according to varying \ndegrees of coronary artery disease. This project will utilize a dataset of 303 patients and distributed by the UCI Machine Learning Repository. \n\nMachine learning and artificial intelligence is going to have a dramatic impact on the health field; as a result, familiarizing yourself with the data processing techniques appropriate for numerical health data and the most widely used algorithms for classification tasks is an incredibly valuable use of your time! In this tutorial, we will do exactly that. \n\nWe will be using some common Python libraries, such as pandas, numpy, and matplotlib. Furthermore, for the machine learning side of this project, we will be using sklearn and keras. Import these libraries using the cell below to ensure you have them correctly installed. \n\n\n## Update(15.05.2020):\nI applied a few steps to this kernel that I learned from the book \"Deep Learning with Python\". After these steps, model success increased. The important thing here is the need to do these steps. You can find many kernels on the page of Fran√ßois Chollet Github, the author of the book. One of these kernels is the source https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/4.4-overfitting-and-underfitting.ipynb\n\n### Steps Added\n\n* Stratified Train/Test-split in scikit-learn\n\n* Normalization\n\n    It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. \n\n* Adding Dropout \n\n    Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of output features of the layer during training. \n\n* Adding Weight Regularization\n\n    You may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\n\n    A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to only take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n    * L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n    * L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n    \n### Here is the result\n![](https://iili.io/JEop0x.png)\n\n\n\n# **Content**\n\n1. [Importing the Dataset](#1.)\n1. [Create Training and Testing Datasets](#2.)\n1. [Building and Training the Neural Network](#3.)\n1. [Improving Results - A Binary Classification Problem](#4.)\n1. [Results and Metrics](#5.)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib\nimport keras\n\nprint('Python: {}'.format(sys.version))\nprint('Pandas: {}'.format(pd.__version__))\nprint('Numpy: {}'.format(np.__version__))\nprint('Sklearn: {}'.format(sklearn.__version__))\nprint('Matplotlib: {}'.format(matplotlib.__version__))\nprint('Keras: {}'.format(keras.__version__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"1.\"></a> \n# 1.Importing the Dataset\n\nThe dataset is available through the University of California, Irvine Machine learning repository. Here is the URL:\n\nhttp:////archive.ics.uci.edu/ml/datasets/Heart+Disease\n\nThis dataset contains patient data concerning heart disease diagnosis that was collected at several locations around the world. There are 76 attributes, including age, sex, resting blood pressure, cholestoral levels, echocardiogram data, exercise habits, and many others. To data, all published studies using this data focus on a subset of 14 attributes - so we will do the same. More specifically, we will use the data collected at the Cleveland Clinic Foundation.\n\nTo import the necessary data, we will use pandas' built in read_csv() function. Let's get started!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the csv\ncleveland = pd.read_csv('../input/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the shape of the DataFrame, so we can see how many examples we have\nprint( 'Shape of DataFrame: {}'.format(cleveland.shape))\nprint (cleveland.loc[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the last twenty or so data points\ncleveland.loc[280:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove missing data (indicated with a \"?\")\ndata = cleveland[~cleveland.isin(['?'])]\ndata.loc[280:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop rows with NaN values from DataFrame\ndata = data.dropna(axis=0)\ndata.loc[280:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the shape and data type of the dataframe\nprint(data.shape)\nprint(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform data to numeric to enable further analysis\ndata = data.apply(pd.to_numeric)\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print data characteristics, usings pandas built-in describe() function\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot histograms for each variable\ndata.hist(figsize = (12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.age,data.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True,fmt='.1f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_unique=sorted(data.age.unique())\nage_thalach_values=data.groupby('age')['thalach'].count().values\nmean_thalach=[]\nfor i,age in enumerate(age_unique):\n    mean_thalach.append(sum(data[data['age']==age].thalach)/age_thalach_values[i])\n    \nplt.figure(figsize=(10,5))\nsns.pointplot(x=age_unique,y=mean_thalach,color='red',alpha=0.8)\nplt.xlabel('Age',fontsize = 15,color='blue')\nplt.xticks(rotation=45)\nplt.ylabel('Thalach',fontsize = 15,color='blue')\nplt.title('Age vs Thalach',fontsize = 15,color='blue')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"2.\"></a> \n# 2.Create Training and Testing Datasets\n\nNow that we have preprocessed the data appropriately, we can split it into training and testings datasets. We will use Sklearn's train_test_split() function to generate a training dataset (80 percent of the total data) and testing dataset (20 percent of the total data). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(data.drop(['target'], 1))\ny = np.array(data['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = X.mean(axis=0)\nX -= mean\nstd = X.std(axis=0)\nX /= std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create X and Y datasets for training\nfrom sklearn import model_selection\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=42, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the data to categorical labels\nfrom keras.utils.np_utils import to_categorical\n\nY_train = to_categorical(y_train, num_classes=None)\nY_test = to_categorical(y_test, num_classes=None)\nprint (Y_train.shape)\nprint (Y_train[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"3.\"></a> \n# 3.Building and Training the Neural Network\n\nNow that we have our data fully processed and split into training and testing datasets, we can begin building a neural network to solve this classification problem. Using keras, we will define a simple neural network with one hidden layer. Since this is a categorical classification problem, we will use a softmax activation function in the final layer of our network and a categorical_crossentropy loss during our training phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom keras import regularizers\n\n# define a function to build the keras model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation='softmax'))\n    \n    # compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nmodel = create_model()\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model to the training data\nhistory=model.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs=50, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"4.\"></a> \n# 4.Improving Results - A Binary Classification Problem\n\nAlthough we achieved promising results, we still have a fairly large error. This could be because it is very difficult to distinguish between the different severity levels of heart disease (classes 1 - 4). Let's simplify the problem by converting the data to a binary classification problem - heart disease or no heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert into binary classification problem - heart disease or no heart disease\nY_train_binary = y_train.copy()\nY_test_binary = y_test.copy()\n\nY_train_binary[Y_train_binary > 0] = 1\nY_test_binary[Y_test_binary > 0] = 1\n\nprint(Y_train_binary[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a new keras model for binary classification\ndef create_binary_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(16, input_dim=13, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(8, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nbinary_model = create_binary_model()\n\nprint(binary_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the binary model on the training data\nhistory=binary_model.fit(X_train, Y_train_binary, validation_data=(X_test, Y_test_binary), epochs=50, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"5.\"></a> \n# 5.Results and Metrics\n\nThe accuracy results we have been seeing are for the training data, but what about the testing dataset? If our model's cannot generalize to data that wasn't used to train them, they won't provide any utility. \n\nLet's test the performance of both our categorical model and binary model.  To do this, we will make predictions on the training dataset and calculate performance metrics using Sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate classification report using predictions for categorical model\nfrom sklearn.metrics import classification_report, accuracy_score\n\ncategorical_pred = np.argmax(model.predict(X_test), axis=1)\n\nprint('Results for Categorical Model')\nprint(accuracy_score(y_test, categorical_pred))\nprint(classification_report(y_test, categorical_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate classification report using predictions for binary model\nfrom sklearn.metrics import classification_report, accuracy_score\n# generate classification report using predictions for binary model \nbinary_pred = np.round(binary_model.predict(X_test)).astype(int)\n\nprint('Results for Binary Model')\nprint(accuracy_score(Y_test_binary, binary_pred))\nprint(classification_report(Y_test_binary, binary_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}