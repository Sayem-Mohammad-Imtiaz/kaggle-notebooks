{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src =\"https://kubrick.htvapps.com/htv-prod-media.s3.amazonaws.com/images/nhjxygyi-1559831904.jpg?crop=1.00xw:1.00xh;0,0&resize=900:*\"></center>\n<h1><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Introduction</div></center></h1>\n\nüå® Motivation\n> I have been listing to the news of rain forcast since childhood and 99% of the times, they are incorrect üòÖ! I used to think that, how can the people forcating the rain be wrong so many times, is it that difficult to predict rain. This motivated me to take up this challange of forcating the rain by myself and see if I can do better than them.\n\nüéØ Goal\n> The goal of this notebook is to predict rain on the next day using the data science and ML skills with high accuracy.\n\nü§ñ Artificial Neural Netwok\n> I will be predicting the rain using ANN model. One can use other classification algorithms also, but I wanted to tackle this problem with the most sofisticated algorithms we have and one of them is ANN. The focus is on getting a high accuracy and not interpreting the model or finding the feature importance, this is another reason for selecting ANN. So, let's get started!","metadata":{}},{"cell_type":"markdown","source":"<h1><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Reading and Cleaning the Data üßπ</div></center></h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-14T19:23:00.395694Z","iopub.execute_input":"2021-07-14T19:23:00.396115Z","iopub.status.idle":"2021-07-14T19:23:00.401537Z","shell.execute_reply.started":"2021-07-14T19:23:00.396078Z","shell.execute_reply":"2021-07-14T19:23:00.400092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the data\ndf = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:00.406976Z","iopub.execute_input":"2021-07-14T19:23:00.407465Z","iopub.status.idle":"2021-07-14T19:23:01.154025Z","shell.execute_reply.started":"2021-07-14T19:23:00.407423Z","shell.execute_reply":"2021-07-14T19:23:01.152621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the data types\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:01.155766Z","iopub.execute_input":"2021-07-14T19:23:01.156094Z","iopub.status.idle":"2021-07-14T19:23:01.276273Z","shell.execute_reply.started":"2021-07-14T19:23:01.156055Z","shell.execute_reply":"2021-07-14T19:23:01.274801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The datatype of `Date` is object so I will change it to date time for easy handling of dates","metadata":{}},{"cell_type":"code","source":"# Changing the data types of Date to datetime\ndf['Date'] = pd.to_datetime(df['Date'])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:01.280493Z","iopub.execute_input":"2021-07-14T19:23:01.280856Z","iopub.status.idle":"2021-07-14T19:23:01.34002Z","shell.execute_reply.started":"2021-07-14T19:23:01.280826Z","shell.execute_reply":"2021-07-14T19:23:01.338613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now lets analyse the target variable `RainTomorrow`. I will be checking the missing values and class imbalance.","metadata":{}},{"cell_type":"code","source":"# Checking for the missing values in the target variable\ndf['RainTomorrow'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:01.342117Z","iopub.execute_input":"2021-07-14T19:23:01.342499Z","iopub.status.idle":"2021-07-14T19:23:01.36455Z","shell.execute_reply.started":"2021-07-14T19:23:01.342467Z","shell.execute_reply":"2021-07-14T19:23:01.363453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> `RainTomorrow` has 3267 missing values. As `RainTomorrow` is to be predicted so we can't impute for missing values. Thus, we have to drop the the rows with missing values.","metadata":{}},{"cell_type":"code","source":"# Droping the missing values\ndf = df.dropna(subset = ['RainTomorrow'])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:01.365959Z","iopub.execute_input":"2021-07-14T19:23:01.366301Z","iopub.status.idle":"2021-07-14T19:23:01.409897Z","shell.execute_reply.started":"2021-07-14T19:23:01.36624Z","shell.execute_reply":"2021-07-14T19:23:01.40905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for the class imbalance\nfig = plt.figure(figsize = (10, 6))\naxis = sns.countplot(x = 'RainTomorrow', data = df);\naxis.set_title('Class Distribution for the target feature', size = 16);\n\nfor patch in axis.patches:\n    axis.text(x = patch.get_x() + patch.get_width()/2, y = patch.get_height()/2, \n            s = f\"{np.round(patch.get_height()/len(df)*100, 1)}%\", \n            ha = 'center', size = 40, rotation = 0, weight = 'bold' ,color = 'white')\n    \naxis.set_xlabel('Rain Tomorrow', size = 14)\naxis.set_ylabel('Count', size = 14);","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:01.411092Z","iopub.execute_input":"2021-07-14T19:23:01.411613Z","iopub.status.idle":"2021-07-14T19:23:01.73065Z","shell.execute_reply.started":"2021-07-14T19:23:01.411566Z","shell.execute_reply":"2021-07-14T19:23:01.729142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The class is imbalance when the minority class has only 5-10% data. We have 22.4% data belonging to the minority class, so, there is no class imbalance. Now, let's look at other features.\n\n> I will be creating new features of `day` and `month` from `Date` column, which are cyclic in nature. If I do not do any preprocessing on them and directly feed them to ANN, the ANN can give more or less importance based on the values. Eg. days will have values from 1 to 31, so ANN thinks that value 31 is more than 1, but actually they are just days so our model can go wrong. Thus, I will be performing a transformation on these features to make them cyclic.\n\n<center><img src = \"https://www.math.hkust.edu.hk/~machiang/1013/Notes/sine_2.gif\"></center>\n\n> A circle is the projection of cyclic pattern. This concept is used here, to make feature transformations.","metadata":{}},{"cell_type":"markdown","source":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Feature Engineering üìêüìè</div></center></h3>","metadata":{}},{"cell_type":"code","source":"# months and days in a cyclic continuous feature.\n\ndef encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n    return data\n\ndf['month'] = df['Date'].dt.month\ndf = encode(df, 'month', 12)\n\ndf['day'] = df['Date'].dt.day\ndf = encode(df, 'day', 31)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:01.732379Z","iopub.execute_input":"2021-07-14T19:23:01.73283Z","iopub.status.idle":"2021-07-14T19:23:01.804069Z","shell.execute_reply.started":"2021-07-14T19:23:01.732783Z","shell.execute_reply":"2021-07-14T19:23:01.803069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at the transformed features\n\nplt.style.use('ggplot')\nfig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize = (12, 4), constrained_layout = True)\n\nax1 = sns.lineplot(x = 'month', y = 'day', data = df, estimator = None, ax=ax1)\nax2 = sns.scatterplot(x = 'day_sin', y = 'day_cos', data = df, ax = ax2)\nax3 = sns.scatterplot(x = 'month_sin', y = 'month_cos', data = df, ax = ax3)\n\nax1.set_title('Original Day Distribution')\nax2.set_title('Cyclic encoding of Day')\nax3.set_title('Cyclic encoding of Month')\n\nfig.suptitle('Feature Engineering for Cyclic Features', size = 16, y = 1.1);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-14T19:23:01.806795Z","iopub.execute_input":"2021-07-14T19:23:01.807449Z","iopub.status.idle":"2021-07-14T19:23:04.3041Z","shell.execute_reply.started":"2021-07-14T19:23:01.807408Z","shell.execute_reply":"2021-07-14T19:23:04.302956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Before doing any preprocessing, I will split the data into train and test set. The reason for that is, we don't see the test data, so all the preprocessing should be based on the train data. If we perform the preprocessing based on test data, it means that we did some cheating üòõ as we looked at the test data.\n\n> I am splitting the data into train and test with ratio of 80% - 20% (randomly chosen), and chossing `stratify` which will keep the proportion of target variable equal in both train and test data.  ","metadata":{}},{"cell_type":"code","source":"# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, train_size = 0.8, random_state  = 99, stratify = df['RainTomorrow'])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:04.306185Z","iopub.execute_input":"2021-07-14T19:23:04.306544Z","iopub.status.idle":"2021-07-14T19:23:04.658576Z","shell.execute_reply.started":"2021-07-14T19:23:04.306511Z","shell.execute_reply":"2021-07-14T19:23:04.657767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Cleaning Categorical Features üöø</div></center></h3>","metadata":{}},{"cell_type":"code","source":"# Let's first handle missing values for catergorical data\ncategorical_col = df_train.select_dtypes('object').columns[:-1].to_list()\ndf_train[categorical_col].isnull().mean()*100","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:04.659808Z","iopub.execute_input":"2021-07-14T19:23:04.66042Z","iopub.status.idle":"2021-07-14T19:23:04.740681Z","shell.execute_reply.started":"2021-07-14T19:23:04.660378Z","shell.execute_reply":"2021-07-14T19:23:04.739638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> As the missing values are less than 10%, I will impute them with the mode.","metadata":{}},{"cell_type":"code","source":"# Imputing with the mode\nfor col in categorical_col:\n    df_train[col].fillna(df_train[col].mode()[0], inplace = True)\n    df_test[col].fillna(df_train[col].mode()[0], inplace = True) # Imputing test data using train data","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:04.742078Z","iopub.execute_input":"2021-07-14T19:23:04.742433Z","iopub.status.idle":"2021-07-14T19:23:04.982833Z","shell.execute_reply.started":"2021-07-14T19:23:04.7424Z","shell.execute_reply":"2021-07-14T19:23:04.981999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Cleaning Numeric Features üöø</div></center></h3>\n\n> The data cleaning of numeric features involves following things:\n> - Handling missing values\n> - Removing multicollinearity\n> - Removing outliers\n\n> Let's do them one by one","metadata":{}},{"cell_type":"markdown","source":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Handling Missing Values</div></center></h4>\n\n> Missing data present various problems. First, the absence of data reduces statistical power, which refers to the probability that the test will reject the null hypothesis when it is false. Second, the lost data can cause bias in the estimation of parameters. Third, it can reduce the representativeness of the samples.","metadata":{}},{"cell_type":"code","source":"# Missing values for numeric data\nnumeric_col = df.describe().columns.to_list()\ndf_train[numeric_col].isnull().mean()*100","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:04.984034Z","iopub.execute_input":"2021-07-14T19:23:04.984496Z","iopub.status.idle":"2021-07-14T19:23:05.159228Z","shell.execute_reply.started":"2021-07-14T19:23:04.984454Z","shell.execute_reply":"2021-07-14T19:23:05.158494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> `Evaporation`, `Sunshine`, `Cloud9am` and `Cloud3pm` have large missing values, so we will first look at these features and then handle the missing values for remaing numeric features.","metadata":{}},{"cell_type":"code","source":"# Let's explore the features having high missing values\ncols = ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n\nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(4,2, figsize = (12, 8), constrained_layout = True)\n\nfor i, num_var in enumerate(cols): \n    sns.kdeplot(data = df_train, x = num_var, ax = ax[i][0],\n                fill = True, alpha = 0.6, linewidth = 1.5)\n    ax[i][0].set_ylabel(num_var)\n    ax[i][0].set_xlabel(None)\n    \n    sns.histplot(data = df_train, x = num_var, ax = ax[i][1])\n    ax[i][1].set_ylabel(None)\n    ax[i][1].set_xlabel(None)\n    \nfig.suptitle('Features having high missing values (>35%)', size = 16);","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:05.160381Z","iopub.execute_input":"2021-07-14T19:23:05.160792Z","iopub.status.idle":"2021-07-14T19:23:10.081155Z","shell.execute_reply.started":"2021-07-14T19:23:05.160751Z","shell.execute_reply":"2021-07-14T19:23:10.079587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Except `Evaporation`, all other three have distributed data, so we will impute the missing values with the median, and impute missing values for`Evaporation` with mean.","metadata":{}},{"cell_type":"code","source":"# Droping the columns with high missing values (>35%) and distributed data\n# for dataframe in [df_train, df_test]:\n#     dataframe.drop(columns = ['Sunshine', 'Cloud9am', 'Cloud3pm'], axis = 1, inplace = True)\n\nfor dataframe in [df_train, df_test]:\n    for cols in ['Sunshine', 'Cloud9am', 'Cloud3pm']:\n        dataframe[cols].fillna(df_train[cols].median(), inplace = True)\n\n    dataframe['Evaporation'].fillna(df_train['Evaporation'].mean(), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:10.082914Z","iopub.execute_input":"2021-07-14T19:23:10.083299Z","iopub.status.idle":"2021-07-14T19:23:10.108761Z","shell.execute_reply.started":"2021-07-14T19:23:10.083264Z","shell.execute_reply":"2021-07-14T19:23:10.107145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now I will remove the missing values from the remaining numerical features as they are <10%. One can also impute them with mean/ median whichever is appropriate.","metadata":{}},{"cell_type":"code","source":"# Removing the missing values from the remaining numerical features as they are <10%.\n# numeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n#                'Humidity9am','Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n\n# for dataframe in [df_train, df_test]:\n#     for col in numeric_col:\n#         # Imputing missing values with median based on train set\n#         dataframe[col].fillna(df_train[col].median(), inplace = True)\n\ndf_train.dropna(inplace = True)\ndf_test.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:10.111103Z","iopub.execute_input":"2021-07-14T19:23:10.111662Z","iopub.status.idle":"2021-07-14T19:23:10.244476Z","shell.execute_reply.started":"2021-07-14T19:23:10.111552Z","shell.execute_reply":"2021-07-14T19:23:10.243241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Removing multicollinearity üßëüë¶</div></center></h4>\n\n> Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant. Also it affects storage and speed.","metadata":{}},{"cell_type":"code","source":"# Checking for the correlation between the numeric features\n# Correlation between numeric variables\n\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n               'Humidity9am','Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm',\n              'Sunshine', 'Cloud9am', 'Cloud3pm', 'Evaporation']\n\nfig=plt.figure(figsize=(16,12))\naxis=sns.heatmap(df_train[numeric_col].corr(), annot=True, linewidths=3, square=True, cmap='Blues', fmt=\".0%\")\n\naxis.set_title('Correlation between the features', fontsize=16);\naxis.set_xticklabels(numeric_col, fontsize=12)\naxis.set_yticklabels(numeric_col, fontsize=12, rotation=0);","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:10.246526Z","iopub.execute_input":"2021-07-14T19:23:10.246952Z","iopub.status.idle":"2021-07-14T19:23:12.020312Z","shell.execute_reply.started":"2021-07-14T19:23:10.246917Z","shell.execute_reply":"2021-07-14T19:23:12.018793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> #### Strong correlation between\n\n`Temp3pm` and `MaxTemp`\n\n`Pressure3pm` and `Pressure9am`\n\n`Temp9am` and `MinTemp`\n\n`Temp9am` and `MaxTemp`\n\n`Temp3pm` and `Temp9am`\n\n> We will remove one of the features in each pair, to avoid multicollinearity","metadata":{}},{"cell_type":"code","source":"# Droping the columns\nfor dataframe in [df_train, df_test]:\n    dataframe.drop(['Temp3pm', 'Pressure3pm', 'Temp9am'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:12.021739Z","iopub.execute_input":"2021-07-14T19:23:12.022076Z","iopub.status.idle":"2021-07-14T19:23:12.044235Z","shell.execute_reply.started":"2021-07-14T19:23:12.022044Z","shell.execute_reply":"2021-07-14T19:23:12.042825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Removing Outliers üëöüëöüëöüëöü©±üëö</div></center></h4>","metadata":{}},{"cell_type":"code","source":"# Let's look at the outliers and the distribution of the numeric features\n\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n               'Humidity9am','Humidity3pm', 'Pressure9am', 'Sunshine', 'Cloud9am', 'Cloud3pm', 'Evaporation']\n\nplt.style.use('seaborn')\nfig, axis = plt.subplots(13, 2, figsize = (12, 24))\nfor i, num_var in enumerate(numeric_col):\n    \n    # Checking for the outliers using boxplot\n    sns.boxplot(y = num_var, data = df_train, ax = axis[i][0], color = 'skyblue')\n    \n    # Checking for the distribution using kdeplot\n    sns.kdeplot(x = num_var, data = df_train, ax = axis[i][1], color = 'skyblue',\n               fill = True, alpha = 0.6, linewidth = 1.5)\n    \n    axis[i][0].set_ylabel(f\"{num_var}\", fontsize = 12)\n    axis[i][0].set_xlabel(None)\n    axis[i][1].set_xlabel(None)\n    axis[i][1].set_ylabel(None)\n\nfig.suptitle('Analysing Numeric Features', fontsize = 16, y = 1)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:12.046146Z","iopub.execute_input":"2021-07-14T19:23:12.046542Z","iopub.status.idle":"2021-07-14T19:23:22.971984Z","shell.execute_reply.started":"2021-07-14T19:23:12.046506Z","shell.execute_reply":"2021-07-14T19:23:22.97094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Many numeric features have data points beyond IQR. I am considering a threshold of 5 percentile, for outlier removal, i.e any point beyound 95 percentile and below 5 percentile is considerd as outlier and will be removed.\n\n> The threshold of 5 percentile is choosen at random, you can very well consider other values for the threshold also.","metadata":{}},{"cell_type":"code","source":"threshold = 0.05\nfor col in numeric_col:\n    \n    # Lower and upper threshold\n    lower_threshold = df_train[col].quantile(threshold)\n    upper_threshold = df_train[col].quantile(1-threshold)\n    \n    # Dropping the values below lower threshold and beyond upper threshold\n    df_train = df_train[(df_train[col]>=lower_threshold) & (df_train[col]<=upper_threshold)]\n    df_test = df_test[(df_test[col]>=lower_threshold) & (df_test[col]<=upper_threshold)]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:22.973709Z","iopub.execute_input":"2021-07-14T19:23:22.974299Z","iopub.status.idle":"2021-07-14T19:23:23.18489Z","shell.execute_reply.started":"2021-07-14T19:23:22.974235Z","shell.execute_reply":"2021-07-14T19:23:23.183522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> üë∂ Transforming Features üë®</div></center></h3>\n\n> Feature transformation is the process of modifying your data but keeping the information. These modifications will make Machine Learning algorithms understanding easier, which will deliver better results. We will reduce repetition, improve performance, and data integrity","metadata":{}},{"cell_type":"code","source":"# Converting 'Yes' and 'No' to '1' and '0' respectively\ndf_train['RainTomorrow'] = df_train['RainTomorrow'].map(dict({'Yes':1, 'No':0}))\ndf_test['RainTomorrow'] = df_test['RainTomorrow'].map(dict({'Yes':1, 'No':0}))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:23:23.186666Z","iopub.execute_input":"2021-07-14T19:23:23.187015Z","iopub.status.idle":"2021-07-14T19:23:23.20287Z","shell.execute_reply.started":"2021-07-14T19:23:23.186984Z","shell.execute_reply":"2021-07-14T19:23:23.201416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the features not required for model\ndf_train.drop(['Date', 'day', 'month'], axis = 1 ,inplace = True)\ndf_test.drop(['Date', 'day', 'month'], axis = 1 ,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:32:01.985727Z","iopub.execute_input":"2021-07-14T19:32:01.98614Z","iopub.status.idle":"2021-07-14T19:32:02.001201Z","shell.execute_reply.started":"2021-07-14T19:32:01.986102Z","shell.execute_reply":"2021-07-14T19:32:01.999012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into y and X\ny_train = df_train.pop('RainTomorrow')\nX_train = df_train\n\ny_test = df_test.pop('RainTomorrow')\nX_test = df_test","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:32:15.015191Z","iopub.execute_input":"2021-07-14T19:32:15.01558Z","iopub.status.idle":"2021-07-14T19:32:15.020881Z","shell.execute_reply.started":"2021-07-14T19:32:15.015547Z","shell.execute_reply":"2021-07-14T19:32:15.019414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now the data is ready for preprocessing, let's convert categorical variables into one hot encodings\nX_train = pd.get_dummies(X_train, drop_first = True).reset_index(drop = True)\nX_test = pd.get_dummies(X_test, drop_first = True).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:32:18.704585Z","iopub.execute_input":"2021-07-14T19:32:18.705049Z","iopub.status.idle":"2021-07-14T19:32:18.76464Z","shell.execute_reply.started":"2021-07-14T19:32:18.705011Z","shell.execute_reply":"2021-07-14T19:32:18.763444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the categorical columns\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n               'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n               'Humidity3pm', 'Pressure9am', 'Cloud9am', 'Cloud3pm',\n               'month_sin', 'month_cos', 'day_sin', 'day_cos']\n\ncategorical_col = [i for i in X_train.columns if i not in numeric_col]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:32:20.083807Z","iopub.execute_input":"2021-07-14T19:32:20.084285Z","iopub.status.idle":"2021-07-14T19:32:20.091007Z","shell.execute_reply.started":"2021-07-14T19:32:20.08423Z","shell.execute_reply":"2021-07-14T19:32:20.089547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now the data is ready for preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\n\nX_train_scale = pd.DataFrame(scalar.fit_transform(X_train[numeric_col]), columns = numeric_col) # fit_transform on train\nX_test_scale = pd.DataFrame(scalar.transform(X_test[numeric_col]), columns = numeric_col) # only transform on test","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:32:21.594523Z","iopub.execute_input":"2021-07-14T19:32:21.594894Z","iopub.status.idle":"2021-07-14T19:32:21.627747Z","shell.execute_reply.started":"2021-07-14T19:32:21.594862Z","shell.execute_reply":"2021-07-14T19:32:21.62637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating final train and test data\nX_train_final = pd.concat([X_train_scale, X_train[categorical_col]], axis = 1)\nX_test_final = pd.concat([X_test_scale, X_test[categorical_col]], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:32:23.652551Z","iopub.execute_input":"2021-07-14T19:32:23.65293Z","iopub.status.idle":"2021-07-14T19:32:23.666755Z","shell.execute_reply.started":"2021-07-14T19:32:23.6529Z","shell.execute_reply":"2021-07-14T19:32:23.665419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> Creating ANN</div></center></h1>","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:25:28.346324Z","iopub.execute_input":"2021-07-14T19:25:28.346732Z","iopub.status.idle":"2021-07-14T19:25:35.308596Z","shell.execute_reply.started":"2021-07-14T19:25:28.3467Z","shell.execute_reply":"2021-07-14T19:25:35.307695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the ANN\nmodel = Sequential()\n\n# layers\nmodel.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train_final.shape[1]))\nmodel.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy', keras.metrics.AUC()])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:00:42.683162Z","iopub.execute_input":"2021-07-14T20:00:42.683576Z","iopub.status.idle":"2021-07-14T20:00:42.758096Z","shell.execute_reply.started":"2021-07-14T20:00:42.683545Z","shell.execute_reply":"2021-07-14T20:00:42.756843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For live plotting\nfrom IPython.display import clear_output\nclass live_accuracy(keras.callbacks.Callback):\n    plt.style.use('ggplot')\n    \n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.accuracy = []\n        self.val_accuracy = []\n        self.auc = []\n        \n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        plt.xlim([0, epochs-1])\n        plt.ylim([0.5, 1.0])\n        plt.title('Training and Validation Accuracy', size = 16)\n        plt.xlabel('epochs')\n        plt.ylabel('accuracy')\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.accuracy.append(logs.get('accuracy'))\n        self.val_accuracy.append(logs.get('val_accuracy'))\n        self.auc.append(logs.get('auc'))\n        \n        self.i += 1\n        \n        clear_output(wait=True)\n        \n        plt.plot(self.x, self.accuracy, label=\"train_accuracy\")\n        plt.plot(self.x, self.val_accuracy, label=\"val_accuracy\")\n        \n#         plt.text(x = epochs-2, y = 0.7,\n#                 s = f\"AUC : {round(self.auc[-1],2)}\",\n#                 ha = 'center', size = 14, rotation = 0, color = 'black',\n#                 bbox=dict(boxstyle=\"round,pad=1\", fc='none', ec=\"black\", lw=2))\n           \n        plt.legend()\n        plt.show();\n        \nplot_accuracy = live_accuracy()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-14T20:00:56.966972Z","iopub.execute_input":"2021-07-14T20:00:56.967487Z","iopub.status.idle":"2021-07-14T20:00:56.985701Z","shell.execute_reply.started":"2021-07-14T20:00:56.967436Z","shell.execute_reply":"2021-07-14T20:00:56.984191Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the ANN\nepochs = 20\nbatch_size = 32\n\nhistory = model.fit(X_train_final, y_train, batch_size = batch_size, epochs = epochs,\n                    callbacks=[plot_accuracy], validation_split = 0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:01:01.582548Z","iopub.execute_input":"2021-07-14T20:01:01.582906Z","iopub.status.idle":"2021-07-14T20:03:24.563118Z","shell.execute_reply.started":"2021-07-14T20:01:01.582875Z","shell.execute_reply":"2021-07-14T20:03:24.561961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> Model Evaluation üîé</div></center></h2>","metadata":{}},{"cell_type":"code","source":"# Model Evaluation\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict_classes(X_test_final)\n\nmatrix = confusion_matrix(y_test, y_pred)\n\nplt.style.use('seaborn-dark')\nfig, axis1 = plt.subplots(1, 1, figsize=(10, 6), constrained_layout = True)\n\n# Threshold = 0.5\n\naxis1 = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                    linewidths=3, square=True, ax = axis1, annot_kws={\"fontsize\":16})\naxis1.set_title(f\"Confusion Matrics\", fontsize=16, y=1.05);\naxis1.set_xlabel('Predicted', fontsize=12)\naxis1.set_ylabel('Actual', fontsize=12)\naxis1.set_xticklabels([0,1], fontsize=12 )\naxis1.set_yticklabels([0,1], fontsize=12, rotation=0);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-14T20:04:00.268804Z","iopub.execute_input":"2021-07-14T20:04:00.269205Z","iopub.status.idle":"2021-07-14T20:04:01.135722Z","shell.execute_reply.started":"2021-07-14T20:04:00.269168Z","shell.execute_reply":"2021-07-14T20:04:01.134412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:04:04.173211Z","iopub.execute_input":"2021-07-14T20:04:04.173606Z","iopub.status.idle":"2021-07-14T20:04:04.195592Z","shell.execute_reply.started":"2021-07-14T20:04:04.173572Z","shell.execute_reply":"2021-07-14T20:04:04.194491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Around 84% accurate!","metadata":{}},{"cell_type":"markdown","source":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> Upvote if you like it! This helps me motivate to produce more notebooks for the community üòä </div></center></h3>","metadata":{}}]}