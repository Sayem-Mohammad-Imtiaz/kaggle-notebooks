{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Wine Quality From Physical Properties\n\nIn this notebook, we make a ML model to predict the quality of wine from labeled data. Our procedure is\n1. Perform exploratory data analysis\n2. Test out several ML models with minimal hyperparameter tuning\n3. Tune the hyperparameters of one of the best peforming models\n4. Further characterization of the best performing model"},{"metadata":{},"cell_type":"markdown","source":"## Dataset definition\n\nThis data is from \n\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties.\nIn Decision Support Systems, Elsevier, 47(4):547-553, 2009. \n\nIt consists of a quality ranking and measured physical attributes for 1599 different vinho verde red wines from Portugal. The data was collected from May 2004 to February 2007.\n\nThe quality is based on sensory data. Possible values are integers between 1 and 10, although 3 is the minimum and 8 the maximum of observed values.\n\nThere are 11 physical attributes and all are numerical and continuous. They are (note 1 liter = 1 dm^3)\n- Fixed acidity (g(tartaric acid)/dm^3):\n- Volatile acidty (g(acetic acid)/dm^3) \n- Citric acid (g/dm^3)\n- Residual sugar (g/dm^3)\n- Chlorides (g(sodium chloride)/dm^3)\n- Free sulfur dioxide (mg/dm^3)\n- Total sulfur dioxide (mg/dm^3)\n- Density (g/cm^3)\n- pH: Acidity of wine, lower numbers are more acidic\n- Sulphates: (g(potassium sulphate)/dm^3)\n- Alcohol (%/vol)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\nwine = pd.read_csv(r'/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = wine.drop('quality', axis='columns').columns.tolist()\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = pd.DataFrame()\nsummary['dtype'] = wine.dtypes\nsummary['unique'] = wine.nunique(axis=0)\nsummary['missing'] = wine.isnull().sum()\nsummary['mean'] = wine.mean()\nsummary['std'] = wine.std()\nsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conveinently, there are no missing values. The variables have somewhat different scales with a typical standard deviation of ~1, but the density having a standard deviation of only 0.0019."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wine.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has 11 features and one target (quality). There are 1599 instances in the data. The number of instances is much larger than the number of features, so we won't try dimensionality reduction techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='quality', data=wine)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the wines are in the middle qualities of 5 or 6. Very few wines are at the extremes of 3 and 8. Some wines are 4 and 7."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nsns.heatmap(wine.corr(), annot=True, fmt='.1f', cmap='coolwarm', center=0)\nplt.title('Pearson Correlation of Variables')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the features have a strong correlation by themselves with quality. A few have moderate correlation with quality though: Alcohol content, volatile acidity and sulphates. Some of the features have significant correlations with eachother that are not surprising, such as pH with fixed acidity and density with alcohol content. Features like pH and density are not too skewed and have a non-zero center. Other features like total sulfur dioxide and sugar have the peak of their distribution near zero and are strongly right-skewed. Alcohol has a peak near 10%, but is also strongly right-skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axs = plt.subplots(len(features), 1, sharex=True, figsize=(4, 20))\nfor i, variable in enumerate(features):\n    sns.boxplot(y=variable, x='quality', data=wine, ax=axs[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axs = plt.subplots(len(features), 1, sharex=True, figsize=(5, 24))\nfor i, variable in enumerate(features):\n    sns.pointplot(x='quality', y=variable, data=wine, ax=axs[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are clearly correlated with quality. For some features like chlorides and sulphates, the relationship looks approximately linear. For free and total sulfur dioxide, however, the relationship is significantly nonlinear. Thus a regressor that is capable of representing nonlinear relationships between features and the target may perform better."},{"metadata":{},"cell_type":"markdown","source":"## 2. Testing Models with Minimal Tuning"},{"metadata":{},"cell_type":"markdown","source":"We'll evaluate several common ML models with minimal hyperparameter tuning based on their mean squared error. We'll hold out 10 percent of the data for final estimation of performance and use 10-fold cross-validation for model selection.\n\nThe models we evaluate are\n  - Dummy Regressor that always predicts mean of training set\n  - Linear regression\n  - Ridge regression\n  - k-Nearest Neighbors\n  - Decision Tree\n  - Extra Trees Regressor\n  - Lightgbm Regressor\n  - Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = wine.drop('quality', axis='columns')\ny = wine['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split off some data for testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Dict\n\ndef evaluate_model(estimator: BaseEstimator, cv: int =10) -> Dict[str, float]:\n    \"\"\"Print and return cross validation of model\n    \"\"\"\n    scoring = 'neg_mean_squared_error'\n    scores = cross_validate(estimator, X_train, y_train, return_train_score=True, cv=cv, scoring=scoring)\n    train_mean, train_std = -1*scores['train_score'].mean(), scores['train_score'].std()\n    print(f'Train MSE: {train_mean} ({train_std})')\n    val_mean, val_std = -1*scores['test_score'].mean(), scores['test_score'].std()\n    print(f'Validation MSE: {val_mean} ({val_std})')\n    fit_mean, fit_std = scores['fit_time'].mean(), scores['fit_time'].std()\n    print(f'Fit time: {fit_mean} ({fit_std})')\n    score_mean, score_std = scores['score_time'].mean(), scores['score_time'].std()\n    print(f'Score time: {score_mean} ({score_std}')\n    result = {\n        'Train MSE': train_mean,\n        'Train std': train_std,\n        'Validation MSE': val_mean,\n        'Validation std': val_std,\n        'Fit time (s)': fit_mean,\n        'Score time (s)': score_mean,\n    }\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = DummyRegressor()\ndummy_result = evaluate_model(dummy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear = LinearRegression()\nlinear_result = evaluate_model(linear)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nridge = Pipeline([\n    ('scale', StandardScaler()),\n    ('ridge', RidgeCV())\n])\nridge_result = evaluate_model(ridge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = Pipeline([\n    ('scale', StandardScaler()),\n    ('knn', KNeighborsRegressor(n_neighbors=50)),\n])\n#knn = KNeighborsRegressor()\nknn_result = evaluate_model(knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeRegressor(max_depth=4)\ndt_result = evaluate_model(dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_tree = ExtraTreesRegressor()\nextra_tree_result = evaluate_model(extra_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = lgbm.LGBMRegressor()\nlgb_result = evaluate_model(lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef create_nn_model() -> Sequential:\n    \"\"\"Create neural network model\"\"\"\n    model = Sequential()\n    model.add(Dense(100, input_dim=11, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n    return model\n\nnn = Pipeline([\n    ('scale', StandardScaler()),\n    ('nn', KerasRegressor(build_fn=create_nn_model, epochs=150, batch_size=50, verbose=0))\n])\n# we only use 3-fold CV for the neural network since it trains slower\nnn_result = evaluate_model(nn, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize Performances\npd.DataFrame({\n    'dummy': dummy_result,\n    'linear': linear_result,\n    'ridge': ridge_result,\n    'knn': knn_result,\n    'dt': dt_result,\n    'extra_trees': extra_tree_result,\n    'lgbm': lgb_result,\n    'nn': nn_result,\n}).transpose().sort_values(by='Validation MSE', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Extra Trees and LGBM models have similar validation scores that are both significantly better than the other models we tested."},{"metadata":{},"cell_type":"markdown","source":"## 3. Tuning Hyperparameters of Chosen Model: Extra Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List\n\ndef plot_param_search(estimator: BaseEstimator, parameter: str, parameter_values: List):\n    \"\"\"Plot training and validation MSE as a function of parameter values\n    \"\"\"\n    param_grid = {parameter: parameter_values}\n    estimator_cv = GridSearchCV(estimator, return_train_score=True, param_grid=param_grid, scoring='neg_mean_squared_error', cv=10)\n    estimator_cv.fit(X_train, y_train)\n    results = estimator_cv.cv_results_\n    f, axs = plt.subplots(2, 1, sharex=True)\n    n_splits = estimator_cv.n_splits_\n    axs[0].errorbar(parameter_values, -1*results['mean_train_score'], yerr=results['std_train_score']/np.sqrt(n_splits))\n    axs[1].errorbar(parameter_values, -1*results['mean_test_score'], yerr=results['std_test_score']/np.sqrt(n_splits))\n    axs[0].set_ylabel('Train MSE')\n    axs[1].set_ylabel('Validation\\nMSE')\n    axs[1].set_xlabel(parameter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter = 'n_estimators'\nparameter_values = [10, 20, 50, 100, 200, 500]\net = ExtraTreesRegressor()\nplot_param_search(et, parameter, parameter_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter = 'min_samples_split'\nparameter_values = [2, 3, 4, 5, 10, 20, 50]\net = ExtraTreesRegressor(n_estimators=100)\nplot_param_search(et, parameter, parameter_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter = 'max_features'\nparameter_values = [1, 2, 4, 8, 11]\net = ExtraTreesRegressor(n_estimators=100, min_samples_split=5)\nplot_param_search(et, parameter, parameter_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_model = ExtraTreesRegressor(n_estimators=100, min_samples_split=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(tuned_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameter tuning didn't significantly change the MSE. It seems the defaults were already pretty good."},{"metadata":{},"cell_type":"markdown","source":"## 4. Further Characterization of Tuned Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = tuned_model\nbest_model.fit(X_train, y_train)\nprint('Mean Squared Error of Tuned Model on Test Set:')\nprint(mean_squared_error(best_model.predict(X_test), y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test, best_model.predict(X_test), s=1)\nplt.title('Wine Quality')\nplt.xlabel('Ground Truth')\nplt.ylabel('Prediction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = best_model.predict(X_test)\nmse_list = []\nfor score in range(3, 9):\n    at_score = (y_test == score)\n    mse = mean_squared_error(y_test[at_score], y_predict[at_score])\n    mse_list.append(mse)\nplt.figure()\nplt.plot(np.arange(3, 9), mse_list)\nplt.xlabel('Ground Truth Score')\nplt.ylabel('Test Set MSE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model overestimates the quality of the low scoring wines and underestimates the quality of high scoring wines. The MSE of for with low or high predicted scores is higher than those in the middle. These also account for a small percent of the total wines though. One could look into developing a different model if, for example, estimating the scores of the highest rated wines is more important than the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = pd.Series(best_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nfeature_importances.plot(kind='bar')\nplt.title('Feature Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As could be expected from the exploratory data analysis, alcohol, volatile acidity and sulphates are all important features in the model. Alcohol content is the most important feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import plot_partial_dependence\nplt.figure(figsize=(12, 12))\nplot_partial_dependence(best_model, X_train, X_train.columns, ax=plt.gca())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has a significant and nonlinear partial dependence on alcohol and sulphates. The model has a more modest partial dependence on other features."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}