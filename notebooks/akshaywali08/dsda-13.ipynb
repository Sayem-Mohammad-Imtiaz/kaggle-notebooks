{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve, auc, classification_report\nfrom sklearn import metrics\nimport keras\nfrom keras import Sequential, regularizers\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder \nsns.set_style(\"whitegrid\")  # Set style of seaborn\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nimport math\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"id":"aLnAsox3wNMa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adonis=pd.read_csv('../input/top-personality-dataset/2018-personality-data.csv')\n##adonis['userid']=adonis['useri']\nadonis.columns=adonis.columns.str.lstrip()#columns name contains empty spaces\nadonis.head()","metadata":{"id":"8ZJ8yWPt7bc6","outputId":"49b67adf-7b42-4645-aaf9-3b89317255eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adonis.info()","metadata":{"id":"FlUsOllB7bX6","outputId":"117f399a-d1e5-4c8f-b579-0828b22e117e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"socratic=pd.read_csv('../input/top-personality-dataset/2018_ratings.csv')\nsocratic.columns=socratic.columns.str.lstrip()#column names contains empty space\nsocratic.head()","metadata":{"id":"7_Rrx93-7bS7","outputId":"478f8f19-810a-43be-af32-123bea878c51","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num = yaga.select_dtypes(include=[np.float, np.int])\ndf_num","metadata":{"id":"t1zOvvrjAPAt","outputId":"fe47b8a7-4902-4f9e-ef11-f0e9e7553af4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a new dataframe with only the 'Close' column\ndata = adonis.filter(['emotional_stability'])\n#Converting the dataframe to a numpy array\ndataset = data.values\n#Get /Compute the number of rows to train the model on\ntraining_data_len = math.ceil( len(dataset) *.8)","metadata":{"id":"kpi8KpXoAOe-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scale the all of the data to be values between 0 and 1 \nscaler = MinMaxScaler(feature_range=(0, 1)) \nscaled_data = scaler.fit_transform(dataset)","metadata":{"id":"tejl7iamAOMd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the scaled training data set \ntrain_data = scaled_data[0:training_data_len  , : ]\n#Split the data into x_train and y_train data sets\nx_train=[]\ny_train = []\nfor i in range(60,len(train_data)):\n    x_train.append(train_data[i-60:i,0])\n    y_train.append(train_data[i,0])","metadata":{"id":"blin-KvDAN1u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert x_train and y_train to numpy arrays\nx_train, y_train = np.array(x_train), np.array(y_train)","metadata":{"id":"mtuxncz1_LeG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape the data into the shape accepted by the LSTM\nx_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))","metadata":{"id":"sQdjtVyK_QjN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True,input_shape=(x_train.shape[1],1)))\nmodel.add(LSTM(units=50, return_sequences=False))\nmodel.add(Dense(units=25))\nmodel.add(Dense(units=1))\n\n#Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')","metadata":{"id":"FJQqwClK_Vsh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\nmodel.fit(x_train, y_train, batch_size=32, epochs=20)","metadata":{"id":"xh0eqdoo_hon","outputId":"fd97605f-cb01-4af1-c549-f31380bd2282","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test data set\ntest_data = scaled_data[training_data_len - 60: , : ]\n#Create the x_test and y_test data sets\nx_test = []\ny_test =  dataset[training_data_len : , : ] #Get all of the rows from index 1603 to the rest and all of the columns (in this case it's only column 'Close'), so 2003 - 1603 = 400 rows of data\nfor i in range(60,len(test_data)):\n    x_test.append(test_data[i-60:i,0])","metadata":{"id":"ENy634tj_0JQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert x_test to a numpy array \nx_test = np.array(x_test)","metadata":{"id":"aYmpkIrdAAp0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape the data into the shape accepted by the LSTM\nx_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))","metadata":{"id":"sFVnS0y_AF-V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting the models predicted price values\npredictions = model.predict(x_test) \npredictions = scaler.inverse_transform(predictions)#Undo scaling","metadata":{"id":"qpEaMxK6AOkE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate/Get the value of RMSE\nrmse=np.sqrt(np.mean(((predictions- y_test)**2)))\nrmse","metadata":{"id":"MVUw22XhATwv","outputId":"3f7447e0-1263-493d-e371-835ab662e001","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot/Create the data for the graph\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions'] = predictions\n#Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('emotional_stability', fontsize=18)\nplt.ylabel('Y label', fontsize=18)\nplt.plot(train['emotional_stability'])\nplt.plot(valid[['emotional_stability', 'Predictions']])\nplt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show the valid and predicted prices\nvalid","metadata":{"id":"VH-uQ27LA6Fn","outputId":"761923f0-566b-437d-815e-534e51fe25b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##DNN","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"vBipKMoTIq69","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adonis","metadata":{"id":"pIEz7UOqIdVx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg = adonis['emotional_stability'] + adonis['agreeableness']+adonis['extraversion']+adonis['conscientiousness']+adonis['openness']\nadonis[\"Temprament\"]=avg/5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adonis = adonis[['openness','agreeableness','emotional_stability','conscientiousness','extraversion','Temprament']]","metadata":{"id":"4aIwmTtSIPm-","outputId":"83e554ef-ed51-4d1c-ef9b-03e1e910a6b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adonis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = adonis.iloc[:, :-1].values\ny = adonis.iloc[:, -1].values","metadata":{"id":"jMg0xWL5xQ46","outputId":"8f7c4ecc-d12a-42ca-b9e3-cb3a1c7fec15","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,random_state = 0,test_size = 0.2)","metadata":{"id":"ZLPFzuGlHwwy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential([ \n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # observe the loss and metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs=30, batch_size=16, validation_data=(x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[mse, mae] = model.evaluate(x_test, y_test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(x_test)\nprint('Temarament = {} and Predicted Temparament = {}'.format(y_test[1], ypred[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create a fucntion to make it easy for multiple calls\ndef build_model():   \n    model = keras.Sequential([ \n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # observe the loss and metrics\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us visit K-fold validation\nk = 4\nnum_val_samples = len(x_train) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #%d' % i)\n    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples] \n    val_labels = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate( \n        [x_train[:i * num_val_samples],\n         x_train[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_labels = np.concatenate(\n        [y_train[:i * num_val_samples],\n         y_train[(i + 1) * num_val_samples:]],\n        axis=0)\n    \nmodel = build_model() \nmodel.fit(partial_train_data, partial_train_labels, \n          epochs=num_epochs, batch_size=1, verbose=0)\nval_mse, val_mae = model.evaluate(val_data, val_labels, verbose=0) \nall_scores.append(val_mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_scores)\nprint(np.mean(all_scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model() \nmodel.fit(x_train, y_train, epochs=16, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(x_test, y_test)\nprint(test_mse_score, test_mae_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot loss metrics\nax1 = plt.plot(history.history[\"loss\"])\nax2 = plt.plot(history.history[\"val_loss\"])\nplt.legend([\"loss\", \"val_loss\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot mae metrics\nax1 = plt.plot(history.history[\"mae\"])\nax2 = plt.plot(history.history[\"val_mae\"])\nplt.legend([\"mae\", \"val_mae\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Case 2- Employee Churn Rate**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.models import load_model\nfrom tensorflow.keras.utils import plot_model\n\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/human-resource-analytics-dataset/hr_data_preprocessed.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df = df.drop('is_smoker', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['average_montly_hours']=df['average_montly_hours'].fillna(df['average_montly_hours'].mean())\n#df['time_spend_company']=df['time_spend_company'].fillna(df['time_spend_company'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, ax = plt.subplots(figsize=(24,12)) # figsize\n#sns.heatmap(df.corr(),annot=True,linewidths=.5,fmt='.1g',cmap= 'coolwarm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['left']\nx = df.drop('left', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)\nprint('%s , %s , %s , %s' % (len(x_train), len(y_train), len(x_test), len(y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dnn1():\n    dnn = Sequential()\n    dnn.add(Dense(units=64, kernel_initializer='uniform', activation='relu', input_dim=20))\n    dnn.add(Dense(units=32, kernel_initializer='uniform', activation='relu'))\n    dnn.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n    dnn.add(Dense(units=8, kernel_initializer='uniform', activation='relu'))\n    dnn.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n    dnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return dnn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_dnn1()\nmodel.fit(x_train, y_train, \n          epochs=30, batch_size=16, verbose=0)\nprint(model.evaluate(x_train, y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding regularization layers i.e Dropot layers\ndef build_dnn2():\n    dnnmodel = Sequential()\n    dnnmodel.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n    dnnmodel.add(Dropout(0.5))\n    dnnmodel.add(Dense(32, activation='relu', kernel_initializer='uniform'))\n    dnnmodel.add(Dropout(0.5))\n    dnnmodel.add(Dense(16, activation='relu', kernel_initializer='uniform'))\n    dnnmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    dnnmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return dnnmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = build_dnn2()\nmodel1.fit(x_train, y_train, \n          epochs=30, batch_size=16, verbose=0)\nprint(model1.evaluate(x_train, y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using three-fold validation to check how model would perform on useen data\nk = 3\nnum_val_samples = len(x_train) // k\nall_scores = []\nfor i in range(k):\n    print('processing fold #%d' % i)\n    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples] \n    val_labels = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate( \n        [x_train[:i * num_val_samples],\n         x_train[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_labels = np.concatenate(\n        [y_train[:i * num_val_samples],\n         y_train[(i + 1) * num_val_samples:]],\n        axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dnn1 = build_dnn1()\ndnn2 = build_dnn2()\ndnn1.fit(partial_train_data, partial_train_labels, \n          epochs=30, batch_size=16, verbose=0)\ndnn2.fit(partial_train_data, partial_train_labels, \n          epochs=30, batch_size=16, verbose=0)\nscores_val_dnn1 = dnn1.evaluate(val_data, val_labels, verbose=0)\nscores_val_dnn2 = dnn2.evaluate(val_data, val_labels, verbose=0)\nprint(\"Scores_dnn1: \\n\",scores_val_dnn1[1]*100)\nprint(\"Scores_dnn2: \\n\",scores_val_dnn2[1]*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(dnn1, to_file='dnn1.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(dnn2, to_file='dnn2.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = dnn1.predict(x_test)\ny_pred = [round(i[0]) for i in pred]\naccuracy_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = dnn2.predict(x_test)\ny_pred1 = [round(i[0]) for i in pred1]\naccuracy_score(y_test, y_pred1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}