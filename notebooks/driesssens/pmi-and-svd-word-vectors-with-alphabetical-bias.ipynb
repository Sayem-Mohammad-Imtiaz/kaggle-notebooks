{"cells":[{"metadata":{"_uuid":"1c1d9246a826785ad74accf0bacaeaf2f44a7d4c","_cell_guid":"b8fe67d9-06b6-4dff-84f3-8d90ddf13f7e"},"cell_type":"markdown","source":"This is an implementation of \"word vectors\" based on Chris Moody's blog post: http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/\n\nThe implementation is by Alex Klibisz. I added the last chunk of code to demonstrate a bias in the vectors: alphabetically-near words are more likely to be nearest neighbors in terms of cosine similarity."},{"metadata":{"_uuid":"bd36a41066ff93e87453ec02db1ea01deeb75f2d","_cell_guid":"52de9db7-3a8c-45a0-a931-a641e452fbb9","trusted":true,"collapsed":true},"cell_type":"code","source":"from __future__ import print_function, division\nfrom collections import Counter\nfrom itertools import combinations\nfrom math import log, floor\nfrom pprint import pformat\nfrom scipy.sparse import csc_matrix\nfrom scipy.sparse.linalg import svds\nfrom string import punctuation\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport string as strin\nprint('Ready')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b1ff6b6132a643b0af01b8ed1cf7cf4a6abe08e","_cell_guid":"e09e7fc7-8067-4b44-bccb-7bcba2acab6f","trusted":true,"collapsed":true},"cell_type":"code","source":"# 1. Read and preprocess titles from HN posts.\npunctrans = str.maketrans(dict.fromkeys(punctuation))\ndef tokenize(title):\n    x = title.lower() # Lowercase\n    x = x.encode('ascii', 'ignore').decode() # Keep only ascii chars.\n    x = x.translate(punctrans) # Remove punctuation\n    return x.split() # Return tokenized.\n\nt0 = time()\ndf = pd.read_csv('../input/HN_posts_year_to_Sep_26_2016.csv', usecols=['title'])\ntexts_tokenized = df['title'].apply(tokenize)\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(df)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc98825b0e6d2043eb16d72b6d5f037a1b6f4504","_cell_guid":"102e4461-2249-4972-929d-a22b6eb9fc0f","trusted":true,"collapsed":true},"cell_type":"code","source":"# 2a. Compute unigram and bigram counts.\n# A unigram is a single word (x). A bigram is a pair of words (x,y).\n# Bigrams are counted for any two terms occurring in the same title.\n# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\nt0 = time()\ncx = Counter()\ncxy = Counter()\nfor text in texts_tokenized:\n    for x in text:\n        cx[x] += 1\n    for x, y in map(sorted, combinations(text, 2)):\n        cxy[(x, y)] += 1\nprint('%.3lf seconds (%.5lf / iter)' %\n      (time() - t0, (time() - t0) / len(texts_tokenized)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"561f177ea5a9e45558cc7a4b9d924dc4e7279e84","_cell_guid":"eeb6ef79-5747-404c-a3eb-2a8f8afca4fc","trusted":true,"collapsed":true},"cell_type":"code","source":"# 2b. Remove frequent and infrequent unigrams.\n# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n# very frequently or infrequently. This decreases the vocab size substantially.\nprint('%d tokens before' % len(cx))\nt0 = time()\nmin_count = (1 / 1000) * len(df)\nmax_count = (1 / 50) * len(df)\nfor x in list(cx.keys()):\n    if cx[x] < min_count or cx[x] > max_count:\n        del cx[x]\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))\nprint('%d tokens after' % len(cx))\nprint('Most common:', cx.most_common()[:25])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"971abff6c59afd7f2b6c159ad38e0911afd905e5","_cell_guid":"c2f26fc4-0867-4a6b-9165-f1fd0b213b09","trusted":true,"collapsed":true},"cell_type":"code","source":"# 2c. Remove frequent and infrequent bigrams.\n# Any bigram containing a unigram that was removed must now be removed.\nt0 = time()\nfor x, y in list(cxy.keys()):\n    if x not in cx or y not in cx:\n        del cxy[(x, y)]\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35020b2f5f97d504d6f12e57530bf692d9d10fd0","_cell_guid":"2624f811-19f3-4a3f-bd27-93d6bd5041bd","trusted":true,"collapsed":true},"cell_type":"code","source":"# 3. Build unigram <-> index lookup.\nt0 = time()\nx2i, i2x = {}, {}\nfor i, x in enumerate(cx.keys()):\n    x2i[x] = i\n    i2x[i] = x\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68a693d75730777740beddecd1363dfde4cb97a8","_cell_guid":"00442677-9d7c-4e8c-a990-9046265aa1c2","trusted":true,"collapsed":true},"cell_type":"code","source":"# 4. Sum unigram and bigram counts for computing probabilities.\n# i.e. p(x) = count(x) / sum(all counts).\nt0 = time()\nsx = sum(cx.values())\nsxy = sum(cxy.values())\nprint('%.3lf seconds (%.5lf / iter)' %\n      (time() - t0, (time() - t0) / (len(cx) + len(cxy))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cee3977bc3623795a2d17a8d914902b03477e5d","_cell_guid":"c9fa5b0c-c26d-4d0e-a652-ee0e6caf22d0","trusted":true,"collapsed":true},"cell_type":"code","source":"# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n# The probabilities are computed on the fly using the sums from above.\nt0 = time()\npmi_samples = Counter()\ndata, rows, cols = [], [], []\nfor (x, y), n in cxy.items():\n    rows.append(x2i[x])\n    cols.append(x2i[y])\n    data.append(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)))\n    pmi_samples[(x, y)] = data[-1]\nPMI = csc_matrix((data, (rows, cols)))\nprint('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\nprint('%d non-zero elements' % PMI.count_nonzero())\nprint('Sample PMI values\\n', pformat(pmi_samples.most_common()[:10]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74bc7b7db97854c092e9c9a4dbedfa1978fba773","_cell_guid":"71729529-e255-4840-aa37-40c3fb5da450","trusted":true,"collapsed":true},"cell_type":"code","source":"# 6. Factorize the PMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n# This part replaces the stochastic gradient descent used by Word2vec\n# and other related neural network formulations. We pick an arbitrary vector size k=20.\nt0 = time()\nU, _, _ = svds(PMI, k=20)\nprint('%.3lf seconds' % (time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdc1ca820c5c1e5fae06fcc90b2fd88bc2f33e52","_cell_guid":"a977236a-f5e9-43a3-aed1-895046f164de","trusted":true,"collapsed":true},"cell_type":"code","source":"# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\nt0 = time()\nnorms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\nU /= np.maximum(norms, 1e-7)\nprint('%.3lf seconds' % (time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dd5b44c3dcba9239ac98085f706de3c35f85112","_cell_guid":"5d8257bb-24a0-465d-83eb-f7c3f5bac6a6","trusted":true,"collapsed":true},"cell_type":"code","source":"# 8. Show some nearest neighbor samples as a sanity-check.\n# The format is <unigram> <count>: (<neighbor unigram>, <similarity>), ...\n# From this we can see that the relationships make sense.\nk = 5\nfor x in ['facebook', 'twitter', 'instagram', 'messenger', 'hack', 'security', \n          'deep', 'encryption', 'cli', 'venture', 'paris']:\n    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n    s = ''\n    # Compile the list of nearest neighbor descriptions.\n    # Argpartition is faster than argsort and meets our needs.\n    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:\n        if i2x[i] == x: continue\n        xy = tuple(sorted((x, i2x[i])))\n        s += '(%s, %.3lf) ' % (i2x[i], dd[i])\n    print('%s, %d\\n %s' % (x, cx[x], s))\n    print('-' * 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca080864f7f8577ef93233a872919c5e4d2029e3","collapsed":true},"cell_type":"code","source":"# 9. Measure the alphabetical bias between similar words, i.e. check the average alphabetical\n# distance between a word and its closest neighbors. Similarity should be quite independent\n# of positions in the alphabet, so this value should not be too low.\n\ndef alphabetical_distance(word_a, word_b):\n    # For simplicity, just the distance between first characters\n    return abs(strin.ascii_lowercase.index(word_a[0]) - strin.ascii_lowercase.index(word_b[0]))\n    \nn_neighbours = 10\n\naverage_distances = []\n\nfor word in cx.keys():\n    dd = np.dot(U, U[x2i[word]]) \n    distances = []\n    \n    for i in np.argpartition(-1 * dd, n_neighbours + 1)[:n_neighbours + 1]:\n        if i2x[i] == word: continue\n        \n        try:\n            distances.append(alphabetical_distance(word, i2x[i]))\n        except ValueError:\n            pass  # Happens when first character of either `word` or `i2x[i]` is numerical\n        \n    try:\n        average_distances.append(sum(distances) / float(len(distances)))\n    except ZeroDivisionError:\n        pass  # Happens when first character of `word` is numerical\n    \nprint(\"Average alphabetical distance: {}\".format(sum(average_distances) / float(len(average_distances))))\nprint(\"Histogram:\")\nprint(sorted(Counter([floor(x) for x in average_distances]).items()))\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}