{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import csv\nimport numpy as np\n\ntitles = []\nlabels = []\nwith open('/kaggle/input/fake-and-real-news-dataset/True.csv', 'r') as f:\n    csvreader = csv.reader(f, delimiter=',')\n    next(csvreader)\n    for row in csvreader:\n        titles.append(row[0].strip())\n        labels.append(1)\n        \nfake_titles = []\nwith open('/kaggle/input/fake-and-real-news-dataset/Fake.csv', 'r') as f:\n    csvreader = csv.reader(f, delimiter=',')\n    next(csvreader)\n    for row in csvreader:\n        titles.append(row[0].strip())\n        labels.append(0)\n\nlabels = np.array(labels)\n        \nprint(len(titles))\nprint(len(labels))\nprint(titles[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\noov_token = '<OOV>'\nnum_words = 10000\nmax_len = 35\ntrain_size = 32000\ntest_size = 8000\n\ntokenizer = Tokenizer(num_words=num_words, oov_token=oov_token, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\'\\t\\n’”…')\ntokenizer.fit_on_texts(titles)\nprint(len(tokenizer.word_index))\n# print(tokenizer.word_index)\n\nsequences = tokenizer.texts_to_sequences(titles)\npadded = pad_sequences(sequences, maxlen=max_len)\n\nprint(padded.shape)\n\ndef get_indices(from_ind1, to_ind1, from_ind2, to_ind2):\n    indices = np.concatenate((np.arange(from_ind1, to_ind1), np.arange(from_ind2, to_ind2)))\n    return indices\n\npadded_len = len(padded)\n\ntrain_indices = get_indices(0, \n                             train_size // 2, \n                             padded_len - (train_size // 2), \n                             padded_len)\ntrain_x = padded[train_indices]\ntrain_y = labels[train_indices]\n\ntest_indices = get_indices((train_size // 2), \n                            (train_size // 2) + (test_size // 2), \n                            padded_len - (train_size // 2) - (test_size // 2), \n                            padded_len - (train_size // 2))\ntest_x = padded[test_indices]\ntest_y = labels[test_indices]\n\nprint(len(train_x))\nprint(len(test_x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from keras import models, layers\nfrom keras.optimizers import RMSprop\nfrom keras import regularizers\nimport keras\n\nkeras.backend.clear_session()\n\nmodel = models.Sequential([\n    layers.Embedding(num_words, 256, input_length=max_len),\n    layers.Conv1D(32, \n                    4, \n                    activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)\n                ),\n    layers.MaxPooling1D(),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(32, \n                    activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01),\n                ),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid'),\n])\n\noptimizer = RMSprop(lr=1e-5)\n\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"epochs = 500\nbatch_size = 128\nhistory = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_x, test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nfrom_index = 100\nepochs = range(from_index, len(acc) + 1)\n\nplt.plot(epochs, acc[from_index - 1:], 'r', label='Training acc')\nplt.plot(epochs, val_acc[from_index - 1:], 'b', label='Validation acc')\nplt.legend()\n\nplt.show()\n\nplt.clf()\n\nplt.plot(epochs, loss[from_index - 1:], 'r', label='Training loss')\nplt.plot(epochs, val_loss[from_index - 1:], 'b', label='Validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}