{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> SPAM Detector: Text Classify </h1>\n\n<img src=\"https://raw.githubusercontent.com/deepankarkotnala/Email-Spam-Ham-Classifier-NLP/master/images/email_spam_ham.png\" width=\"50%\" />\n\nCreated: 2020-09-15\n\nLast updated: 2020-09-15\n\nKaggle Kernel made by üöÄ <a href=\"https://www.kaggle.com/rafanthx13\"> Rafael Morais de Assis</a>\n\nThis is an initial kernel, in the future it will be filled with the theoretical part and more details about NLP"},{"metadata":{},"cell_type":"markdown","source":"## Problem Description\n\nClassify SPAM or HAM\n\nMeaning of HAM\n\n> The term ‚Äòham‚Äô was originally coined by SpamBayes sometime around 2001and is currently defined and understood to be ‚ÄúE-mail that is generally desired and isn't considered spam.‚Äù - [link](https://blog.barracuda.com/2013/10/03/ham-v-spam-whats-the-difference/)"},{"metadata":{},"cell_type":"markdown","source":"https://blog.barracuda.com/2013/10/03/ham-v-spam-whats-the-difference/\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Table Of Contents (TOC) <a id=\"top\"></a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [EDA](#index03)\n+ [Text Cleaning](#index04)\n  - [Def functions](#index05)\n  - [Execute pre-processing](#index06)\n+ [Split in train and Test](#index07)\n+ [Develop Model](#index08)\n+ [Evaluate Model](#index09)\n+ [Conclusion](#index10)\n\n\n## Import Libs and DataSet <a id='index01'></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/sms-spam-collection-dataset/spam.csv'\ndf = pd.read_csv(file_path, encoding='latin-1')\ndf = df[['v1', 'v2']]\ndf.columns = ['label', 'message']\n\nprint(\"DataSet = {} rows and {} columns\".format(df.shape[0], df.shape[1]))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Snippets <a id='index02'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def plot_nn_loss_acc(history):\n    fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n    # summarize history for accuracy\n    axis1.plot(history.history['accuracy'], label='Train', linewidth=3)\n    axis1.plot(history.history['val_accuracy'], label='Validation', linewidth=3)\n    axis1.set_title('Model accuracy', fontsize=16)\n    axis1.set_ylabel('accuracy')\n    axis1.set_xlabel('epoch')\n    axis1.legend(loc='upper left')\n    # summarize history for loss\n    axis2.plot(history.history['loss'], label='Train', linewidth=3)\n    axis2.plot(history.history['val_loss'], label='Validation', linewidth=3)\n    axis2.set_title('Model loss', fontsize=16)\n    axis2.set_ylabel('loss')\n    axis2.set_xlabel('epoch')\n    axis2.legend(loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def plot_words_distribution(mydf, target_column, title='Words distribution', x_axis='Words in column'):\n    # adaptade of https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc\n    # def statistics\n    len_name = target_column +'_len'\n    mydf[len_name] = np.array(list(map(len, mydf[target_column])))\n    sw = mydf[len_name]\n    median = sw.median()\n    mean   = sw.mean()\n    mode   = sw.mode()[0]\n    # figure\n    fig, ax = plt.subplots()\n    sns.distplot(mydf[len_name], bins=mydf[len_name].max(),\n                hist_kws={\"alpha\": 0.9, \"color\": \"blue\"}, ax=ax,\n                kde_kws={\"color\": \"black\", 'linewidth': 3})\n    ax.set_xlim(left=0, right=np.percentile(mydf[len_name], 95)) # Dont get outiliers\n    ax.set_xlabel(x_axis)\n    ymax = 0.025\n    plt.ylim(0, ymax)\n    # plot vertical lines for statistics\n    ax.plot([mode, mode], [0, ymax], '--', label=f'mode = {mode:.2f}', linewidth=4)\n    ax.plot([mean, mean], [0, ymax], '--', label=f'mean = {mean:.2f}', linewidth=4)\n    ax.plot([median, median], [0, ymax], '--', label=f'median = {median:.2f}', linewidth=4)\n    ax.set_title(title, fontsize=20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import time\n\ndef time_spent(time0):\n    t = time.time() - time0\n    t_int = int(t) // 60\n    t_min = t % 60\n    if(t_int != 0):\n        return '{}min {:.3f}s'.format(t_int, t_min)\n    else:\n        return '{:.3f}s'.format(t_min)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\nthis_labels = ['HAM','SPAM']\n\ndef class_report(y_real, y_my_preds, name=\"\", labels=this_labels):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_real, y_my_preds), '\\n')\n    print(classification_report(y_real, y_my_preds, target_names=labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA <a id='index03'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['message_lenght'] = np.array(list(map(len, df['message'])))\n\nmin_len = df['message_lenght'].min()\nmax_len = df['message_lenght'].max()\n\nprint('min len:', min_len)\nprint(df[df.message_lenght == min_len].message.iloc[0], '\\n')\n\nprint('max len:', max_len)\nprint(df[df.message_lenght == max_len].message.iloc[0])\n\ndf = df.drop(['message_lenght'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df.label, 'Unbalanced DataSet of SPAM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_words_distribution(df, 'message', title='Words distribution all data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_words_distribution( df.query('label == \"ham\"'), 'message', title='Words distribution to ham')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_words_distribution( df.query('label == \"spam\"'), 'message', title='Words distribution to spam')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Cleaning <a id='index04'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>"},{"metadata":{},"cell_type":"markdown","source":"### Def functions <a id='index05'></a>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n\"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n\"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\",\n\"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n\"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n\"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n\"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n\"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n\"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n\"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n\"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n\"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n\"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n\"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n\"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora',\n'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pok√©mon': 'pokemon'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re\n\n# Stop Words in Python. Is better search in a 'set' structure\nstops = set(stopwords.words(\"english\"))  \n\ndef clean_text( text ):\n    # 1. Remove non-letters        \n    text = re.sub(\"[^a-zA-Z]\", \" \", text) \n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    # 2. Convert to lower case, split into individual words\n    text = text.lower().split()                                             \n    # 3. Remove stop words\n    meaningful_words = [w for w in text if not w in stops]   \n    # 4. Join the words back into one string separated by space.\n    return( \" \".join( meaningful_words )) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ndef lematizer(text):\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = \" \".join(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Execute Text Pre-Procesing <a id='index06'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_column = 'clean_message'\n\ndf[clean_column] = df['message'].apply(lambda x: clean_contractions(x, contraction_mapping))\n\ndf[clean_column] = df[clean_column].apply(lambda x: correct_spelling(x, mispell_dict))\n\ndf[clean_column] = df[clean_column].apply(clean_text)\n\ndf[clean_column] = df[clean_column].apply(lematizer)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split in train and test <a id='index07'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[clean_column].values\ny = df['label'].replace({'ham': 0, 'spam': 1})\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Develop Model <a id='index08'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizer\nmaxlen = 130\nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(np.concatenate((x_train, x_test), axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert x_train\nlist_tokenized_train = tokenizer.texts_to_sequences(x_train) # convert string to numbers, \nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen) # create a array of 130 spaces and put all words in end\n\n## Convert x_test\nX_tt = tokenizer.texts_to_sequences(x_test)\nX_tt = pad_sequences(X_tt, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 5\nhistory = model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, validation_data=(X_tt, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model <a id='index09'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(X_tt)\n\nclass_report(y_test, y_pred, \"Keras Neural Net LSTM\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_nn_loss_acc(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyper Tuning a RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(np.concatenate((x_train, x_test), axis=0))\n\nX_train_dtm = vect.transform(x_train)\nX_test_dtm = vect.transform(x_test)\nx_full_dtm = vect.transform( np.concatenate((x_train, x_test), axis=0) )\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit( x_full_dtm )\nX_train_dtm_tfft = tfidf_transformer.transform(X_train_dtm)\nX_test_dtm_tfft  = tfidf_transformer.transform(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef optimize_random_forest(mx_train, my_train, my_hyper_params, hyper_to_search, hyper_search_name, cv=4, scoring='accuracy'):\n    \"\"\"search best param to unic one hyper param\n    @mx_train, @my_train = x_train, y_train of dataset\n    @my_hyper_params: dict with actuals best_params: start like: {}\n      => will be accumulated and modified with each optimization iteration\n      => example stater: best_hyper_params = {'random_state': 42, 'n_jobs': -1}\n    @hyper_to_search: dict with key @hyper_search_name and list of values to gridSearch:\n    @hyper_search_name: name of hyperparam\n    \"\"\"\n    if(hyper_search_name in my_hyper_params.keys()):\n        del my_hyper_params[hyper_search_name]\n    if(hyper_search_name not in hyper_to_search.keys()):\n        raise Exception('\"hyper_to_search\" dont have {} in dict'.format(hyper_search_name))\n        \n    t0 = time.time()\n        \n    rf = RandomForestClassifier(**my_hyper_params)\n    \n    grid_search = GridSearchCV(estimator = rf, param_grid = hyper_to_search, \n      scoring = scoring, n_jobs = -1, cv = cv)\n    grid_search.fit(mx_train, my_train)\n    \n    print('took', time_spent(t0))\n    \n    data_frame_results = pd.DataFrame(\n        data={'mean_fit_time': grid_search.cv_results_['mean_fit_time'],\n        'mean_test_score_'+scoring: grid_search.cv_results_['mean_test_score'],\n        'ranking': grid_search.cv_results_['rank_test_score']\n         },\n        index=grid_search.cv_results_['params']).sort_values(by='ranking')\n    \n    print('The Best HyperParam to \"{}\" is {} with {} in {}'.format(\n        hyper_search_name, grid_search.best_params_[hyper_search_name], grid_search.best_score_, scoring))\n    \n    my_hyper_params[hyper_search_name] = grid_search.best_params_[hyper_search_name]\n    \n    \"\"\"\n    @@my_hyper_params: my_hyper_params appends best param find to @hyper_search_name\n    @@data_frame_results: dataframe with statistics of gridsearch: time, score and ranking\n    @@grid_search: grid serach object if it's necessary\n    \"\"\"\n    return my_hyper_params, data_frame_results, grid_search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_hyper_params = {'random_state': 42, 'n_jobs': -1} # Stater Hyper Params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = {'min_samples_split': [2, 5, 10]}\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'min_samples_split')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = {'n_estimators': [50, 100, 200, 400, 800, 1600, 2000] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'n_estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = { 'max_depth': [100, 110, 115, 200, 400, None] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'max_depth')\n\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = { 'max_features': ['auto', 'log2', None] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'max_features')\n\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = { 'criterion' :['gini', 'entropy'] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'criterion')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = { 'min_samples_split' :[ 8, 9, 10, 11, 12] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'min_samples_split')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = { 'max_leaf_nodes' :[ 2, 100, 200, 300, 400, None] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'max_leaf_nodes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_hyper_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier() \nforest = forest.fit(X_train_dtm, y_train)\ny_pred = forest.predict(X_test_dtm)\n\nclass_report(y_test, y_pred, 'Random Forest Normal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(**best_hyper_params) \nforest = forest.fit(X_train_dtm, y_train)\ny_pred = forest.predict(X_test_dtm)\n\nclass_report(y_test, y_pred, 'Random Forest HyperTuned')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion <a id='index10'></a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC</a>\n\n**References**\n\nhttps://www.kaggle.com/nilanml/imdb-review-deep-model-94-89-accuracy\n\nThis is an initial kernel, in the future it will be filled with the theoretical part and more details about NLP, how a kernel to start understand NLP."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}