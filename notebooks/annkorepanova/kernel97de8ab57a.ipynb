{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import TweetTokenizer\n\n# Any results you write to the current directory are saved as output.","execution_count":115,"outputs":[{"output_type":"stream","text":"['imdb_master.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/imdb_master.csv\", encoding=\"ISO-8859-1\")","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.groupby(['type']).count())","execution_count":117,"outputs":[{"output_type":"stream","text":"       Unnamed: 0  review  label   file\ntype                                   \ntest        25000   25000  25000  25000\ntrain       75000   75000  75000  75000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = data[data.type == \"test\"]\ntrain_data = data[data.type == \"train\"][data.label != \"unsup\"]","execution_count":118,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))","execution_count":119,"outputs":[{"output_type":"stream","text":"25000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef foo(data):\n    train_x = []\n    train_y = []\n    for i in range(len(data)):\n        train_x.append([i.lower() for i in TweetTokenizer().tokenize(data.iloc[i].review) if i.isalpha()])\n        label = data.iloc[i].label\n        if label == \"neg\":\n            train_y.append(0)\n        elif label == \"pos\":\n            train_y.append(1)\n    return train_x, train_y","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, train_y = foo(train_data)","execution_count":121,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter1 = 0\ncounter0 = 0\nc = 0\nfor i in train_y:\n    if i == 1:\n        counter1 += 1\n    else: counter0 += 1\n    c +=1\nprint(counter0, counter1)","execution_count":122,"outputs":[{"output_type":"stream","text":"12500 12500\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_x = []\nval_y = []\nfor i in range(3000):\n    ind = random.randint(0, len(train_x)-1)\n    val_x.append(train_x[ind].copy())\n    val_y.append(train_y[ind])\n    del train_x[ind]\n    del train_y[ind]","execution_count":123,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c0 = 0\nc1 = 0\nfor i in val_y:\n    if i == 0:\n        c0 += 1\n    else: c1 += 1\nprint(c0, c1)","execution_count":124,"outputs":[{"output_type":"stream","text":"1502 1498\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x, test_y =  foo(test_data)","execution_count":125,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ncounter = Counter()\nfor i in train_x:\n    for j in i:\n        counter[j] += 1\n","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(counter.most_common(3))","execution_count":127,"outputs":[{"output_type":"stream","text":"[('the', 296667), ('and', 144573), ('a', 143533)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(counter.most_common()[:5])","execution_count":128,"outputs":[{"output_type":"stream","text":"[('the', 296667), ('and', 144573), ('a', 143533), ('of', 128582), ('to', 119533)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabular = {}\nfor num, i in enumerate(counter.most_common()):\n    vocabular[i[0]] = num+1\n    if num > 10000:\n        break","execution_count":129,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(vocabular))","execution_count":130,"outputs":[{"output_type":"stream","text":"10002\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(x):\n    train_tokenize_x = []\n    for i in x:\n        temp = []\n        for j in i:\n            tokenize = vocabular.get(j)\n            if tokenize is not None:\n                temp.append(tokenize)\n        train_tokenize_x.append(temp)\n    for i in range(len(train_tokenize_x)):\n        if len(train_tokenize_x[i]) < 200:\n            train_tokenize_x[i] += [0 for i in range(200-len(train_tokenize_x[i]))]\n        else:\n            train_tokenize_x[i] = train_tokenize_x[i][:200]\n    return train_tokenize_x","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokenize_x = tokenize(train_x)\nprint(train_tokenize_x[3])","execution_count":132,"outputs":[{"output_type":"stream","text":"[51, 10, 13, 110, 55, 760, 520, 68, 342, 5, 1, 698, 5, 64, 8079, 9, 13, 28, 4, 104, 98, 10, 281, 16, 55, 760, 18, 11, 13, 1, 60, 28, 69, 2479, 44, 4, 222, 89, 10, 65, 109, 106, 8079, 341, 40, 967, 2, 10, 95, 25, 1421, 44, 1, 337, 4, 55, 116, 195, 9, 47, 3, 1812, 9930, 2, 2027, 332, 401, 4, 168, 6492, 2, 3012, 4975, 2197, 1617, 6, 28, 4, 55, 486, 849, 18, 8079, 6, 31, 224, 1, 232, 401, 4, 584, 4, 24, 570, 8, 1, 409, 4, 1617, 383, 169, 3, 444, 9451, 2397, 8, 5, 1, 430, 4, 3, 207, 31, 1, 1819, 3763, 2010, 31, 4168, 570, 111, 821, 1, 19, 4393, 42, 54, 706, 2028, 54, 213, 2, 6, 8, 9030, 6594, 11, 19, 409, 66, 27, 113, 4664, 14, 8, 827, 709, 32, 4049, 4, 396, 5, 3, 49, 9226, 1098, 4, 1201, 2, 833, 18, 2197, 1617, 6, 54, 5152, 1, 19, 6, 2027, 572, 2, 705, 18, 668, 12, 10, 304, 65, 54, 1958, 16, 39, 2293, 15, 96, 4, 1, 100, 280, 10, 392, 60, 5879, 15, 11, 4918, 4, 6212, 8]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_tokenize_x = tokenize(val_x)\nprint(val_tokenize_x[1])","execution_count":133,"outputs":[{"output_type":"stream","text":"[3, 191, 320, 4, 428, 7856, 18, 10, 95, 21, 250, 12, 26, 91, 11, 17, 10, 13, 62, 649, 7, 7, 7856, 204, 1703, 509, 5, 1, 142, 6905, 9, 119, 21, 525, 3, 6667, 1311, 20, 3, 494, 7856, 935, 5, 933, 1124, 186, 1, 100, 46, 6, 54, 186, 7247, 2, 5446, 5446, 46, 23, 924, 96, 135, 16, 49, 70, 103, 79, 8, 1, 2235, 279, 1, 680, 948, 1, 6403, 16, 4241, 6216, 2, 380, 126, 6, 1655, 4241, 6216, 138, 21, 25, 215, 11, 17, 7247, 2, 13, 251, 63, 113, 18, 54, 331, 565, 2, 229, 1123, 115, 360, 1255, 2, 1953, 9, 163, 12, 108, 6, 29, 41, 2717, 2589, 2, 610, 1064, 337, 4, 1, 170, 13, 1846, 6709, 1, 203, 843, 59, 13, 2932, 35, 1, 193, 6905, 752, 1003, 8, 11, 17, 7, 7, 7856, 95, 25, 934, 692, 1, 216, 18, 82, 4, 1, 3182, 400, 1, 17, 26, 4448, 56, 2, 40, 15, 1, 2014, 4, 235, 26, 91, 11, 18, 46, 6, 54, 931, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokenize_x = tokenize(test_x)\nprint(test_tokenize_x[0])","execution_count":134,"outputs":[{"output_type":"stream","text":"[264, 163, 428, 42, 3118, 44, 3, 17, 15, 224, 1145, 70, 1589, 1150, 35, 1, 1198, 1957, 2168, 793, 4, 59, 46, 23, 50, 158, 10, 40, 115, 21, 439, 41, 96, 4, 1, 100, 87, 4, 169, 25, 2679, 8, 1, 3882, 2, 105, 23, 1622, 378, 20, 2, 89, 1484, 341, 72, 284, 31, 59, 57, 10, 115, 21, 439, 1, 105, 69, 138, 62, 439, 41, 6, 3, 50, 8684, 1, 421, 6, 26, 249, 126, 14, 530, 34, 1276, 121, 70, 241, 309, 179, 85, 2, 266, 54, 3975, 4, 3, 3882, 24, 60, 700, 5, 27, 1741, 122, 397, 51, 69, 23, 71, 479, 1, 356, 92, 206, 4, 11, 3922, 677, 169, 29, 41, 2679, 69, 23, 543, 133, 6, 2638, 5, 27, 1, 113, 16, 54, 2472, 39, 54, 1170, 127, 9, 13, 29, 10, 95, 80, 5, 372, 35, 1527, 9, 126, 32, 531, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self, v_size):\n        super().__init__()\n        self.l1 = torch.nn.Embedding(v_size, 50, padding_idx=0)\n        self.l2 = torch.nn.LSTM(50, 500, batch_first=True)\n        self.l3 = torch.nn.Linear(500, 1)\n        \n    def forward(self, x):\n        out = self.l1(x)\n        out, (h_t, h_c) = self.l2(out)\n        out = self.l3(h_t)\n        return torch.sigmoid(out)","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(len(vocabular)+2).cuda()","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mydataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        \n    def __getitem__(self, n):\n        return torch.tensor(data=self.x[n]), torch.FloatTensor(data=[self.y[n]])\n    \n    def __len__(self):\n        return len(self.x)","execution_count":137,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = Mydataset(train_tokenize_x, train_y)\ntrain_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)","execution_count":138,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataset = Mydataset(val_tokenize_x, val_y)\nval_dataloader = DataLoader(val_dataset, batch_size=20)","execution_count":139,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = Mydataset(test_tokenize_x, test_y)\ntest_dataloader = DataLoader(test_dataset, batch_size=20)","execution_count":140,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim = torch.optim.Adam(model.parameters(), lr=0.002)\ncriterion = torch.nn.BCELoss()","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfor i in range(7):\n    c = 0\n    model.train()\n    for x, y in train_dataloader:\n        optim.zero_grad()\n        x, y = x.cuda(), y.cuda()\n        out = model(x)[0]\n        loss = criterion(out, y)\n        loss.backward()\n        optim.step()\n        if c % 50 == 0:\n            print(c/len(train_dataloader))\n        c += 1\n    model.eval()\n    y_labels = []\n    y_predicted = []\n    with torch.no_grad():\n        c = 0\n        for x, y in val_dataloader:\n            y_labels.extend(y.tolist())\n            x, y = x.cuda(), y.cuda()\n            out = model(x)[0]\n            out_value = []\n            for i in out.tolist():\n                if i[0] > 0.5:\n                    out_value.append(1)\n                else:\n                    out_value.append(0)\n            y_predicted.extend(out_value)\n            if c % 50 == 0:\n                print(c/len(val_dataloader))\n            c += 1\n    print(classification_report(y_labels, y_predicted))\n        \n    \n    ","execution_count":null,"outputs":[{"output_type":"stream","text":"0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n0.22727272727272727\n0.2727272727272727\n0.3181818181818182\n0.36363636363636365\n0.4090909090909091\n0.45454545454545453\n0.5\n0.5454545454545454\n0.5909090909090909\n0.6363636363636364\n0.6818181818181818\n0.7272727272727273\n0.7727272727272727\n0.8181818181818182\n0.8636363636363636\n0.9090909090909091\n0.9545454545454546\n0.0\n0.3333333333333333\n0.6666666666666666\n              precision    recall  f1-score   support\n\n         0.0       0.53      0.20      0.29      1502\n         1.0       0.51      0.82      0.63      1498\n\n   micro avg       0.51      0.51      0.51      3000\n   macro avg       0.52      0.51      0.46      3000\nweighted avg       0.52      0.51      0.46      3000\n\n0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n0.22727272727272727\n0.2727272727272727\n0.3181818181818182\n0.36363636363636365\n0.4090909090909091\n0.45454545454545453\n0.5\n0.5454545454545454\n0.5909090909090909\n0.6363636363636364\n0.6818181818181818\n0.7272727272727273\n0.7727272727272727\n0.8181818181818182\n0.8636363636363636\n0.9090909090909091\n0.9545454545454546\n0.0\n0.3333333333333333\n0.6666666666666666\n              precision    recall  f1-score   support\n\n         0.0       0.79      0.87      0.83      1502\n         1.0       0.85      0.77      0.81      1498\n\n   micro avg       0.82      0.82      0.82      3000\n   macro avg       0.82      0.82      0.82      3000\nweighted avg       0.82      0.82      0.82      3000\n\n0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n0.22727272727272727\n0.2727272727272727\n0.3181818181818182\n0.36363636363636365\n0.4090909090909091\n0.45454545454545453\n0.5\n0.5454545454545454\n0.5909090909090909\n0.6363636363636364\n0.6818181818181818\n0.7272727272727273\n0.7727272727272727\n0.8181818181818182\n0.8636363636363636\n0.9090909090909091\n0.9545454545454546\n0.0\n0.3333333333333333\n0.6666666666666666\n              precision    recall  f1-score   support\n\n         0.0       0.84      0.82      0.83      1502\n         1.0       0.83      0.85      0.84      1498\n\n   micro avg       0.84      0.84      0.84      3000\n   macro avg       0.84      0.84      0.84      3000\nweighted avg       0.84      0.84      0.84      3000\n\n0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n0.22727272727272727\n0.2727272727272727\n0.3181818181818182\n0.36363636363636365\n0.4090909090909091\n0.45454545454545453\n0.5\n0.5454545454545454\n0.5909090909090909\n0.6363636363636364\n0.6818181818181818\n0.7272727272727273\n0.7727272727272727\n0.8181818181818182\n0.8636363636363636\n0.9090909090909091\n0.9545454545454546\n0.0\n0.3333333333333333\n0.6666666666666666\n              precision    recall  f1-score   support\n\n         0.0       0.80      0.88      0.84      1502\n         1.0       0.87      0.78      0.82      1498\n\n   micro avg       0.83      0.83      0.83      3000\n   macro avg       0.83      0.83      0.83      3000\nweighted avg       0.83      0.83      0.83      3000\n\n0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n0.22727272727272727\n0.2727272727272727\n0.3181818181818182\n0.36363636363636365\n0.4090909090909091\n0.45454545454545453\n0.5\n0.5454545454545454\n0.5909090909090909\n0.6363636363636364\n0.6818181818181818\n0.7272727272727273\n0.7727272727272727\n0.8181818181818182\n0.8636363636363636\n0.9090909090909091\n0.9545454545454546\n0.0\n0.3333333333333333\n0.6666666666666666\n              precision    recall  f1-score   support\n\n         0.0       0.86      0.82      0.84      1502\n         1.0       0.82      0.86      0.84      1498\n\n   micro avg       0.84      0.84      0.84      3000\n   macro avg       0.84      0.84      0.84      3000\nweighted avg       0.84      0.84      0.84      3000\n\n0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n0.22727272727272727\n0.2727272727272727\n0.3181818181818182\n0.36363636363636365\n0.4090909090909091\n0.45454545454545453\n0.5\n0.5454545454545454\n0.5909090909090909\n0.6363636363636364\n0.6818181818181818\n0.7272727272727273\n0.7727272727272727\n0.8181818181818182\n0.8636363636363636\n0.9090909090909091\n0.9545454545454546\n0.0\n0.3333333333333333\n0.6666666666666666\n              precision    recall  f1-score   support\n\n         0.0       0.86      0.81      0.83      1502\n         1.0       0.82      0.86      0.84      1498\n\n   micro avg       0.84      0.84      0.84      3000\n   macro avg       0.84      0.84      0.84      3000\nweighted avg       0.84      0.84      0.84      3000\n\n0.0\n0.045454545454545456\n0.09090909090909091\n0.13636363636363635\n0.18181818181818182\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_labels = []\ny_predicted = []\nwith torch.no_grad():\n    c = 0\n    for x, y in test_dataloader:\n        y_labels.extend(y.tolist())\n        x, y = x.cuda(), y.cuda()\n        out = model(x)[0]\n        out_value = []\n        for i in out.tolist():\n            if i[0] > 0.5:\n                out_value.append(1)\n            else:\n                out_value.append(0)\n        y_predicted.extend(out_value)\n        if c % 100 == 0:\n            print(c/len(test_dataloader))\n        c += 1\nprint(classification_report(y_labels, y_predicted))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}