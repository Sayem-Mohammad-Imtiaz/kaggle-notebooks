{"cells":[{"metadata":{"trusted":true,"_uuid":"9c07a60aa33345953e1ddeb3f8bb19c5efad93fd"},"cell_type":"code","source":"import os\nimport time\nimport progressbar\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nimport keras\nimport sys, time, os, warnings \nimport numpy as np\nimport pandas as pd \nfrom collections import Counter \nfrom keras.preprocessing.image import load_img\nfrom nltk.tokenize import word_tokenize\nwarnings.filterwarnings(\"ignore\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"## The location of the caption file\n#dir_Flickr_text = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/results.csv\"\n#dir_Flickr_jpg = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images\"\ndir_Flickr_text = \"../input/flickr8k-sau/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\ndir_Flickr_jpg = \"../input/flickr8k-sau/flickr8k-sau/Flickr_Data/Images\"\n\njpgs = os.listdir(dir_Flickr_jpg)\nprint(\"The number of jpg flies in Flicker30k: {}\".format(len(jpgs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c0cf027a3a4944f4140a1c3d60e8e36e45e13ab"},"cell_type":"code","source":"## loading as dataframe\ndef load_csv(directory):\n    desc=dict()\n    text = pd.read_csv(directory, delimiter='|',header=None,names=[\"filename\",\"index\",\"caption\"])\n    text = text.iloc[1:,:]\n    df_new = text[text.iloc[:,2].notnull()]\n    print(df_new.iloc[:5,:])\n    return df_new  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f14b16153331f5b51a956cb32cf7bb71d93ffbf"},"cell_type":"code","source":"\n\nfile = open(dir_Flickr_text,'r')\ntext = file.read()\nfile.close()\n\n\ndatatxt = []\nfor line in text.split('\\n'):\n    col = line.split('\\t')\n    if len(col) == 1:\n        continue\n    w = col[0].split(\"#\")\n    datatxt.append(w + [col[1].lower()])\n\ndf_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n\n\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c76172e0c1e49a4c79b413daecd7183a1f231b4b"},"cell_type":"markdown","source":"\ndf_txt= load_csv(dir_Flickr_text)\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())"},{"metadata":{"trusted":true,"_uuid":"fbb673a74d9f20143e2503ea6fae79aa6cefa983"},"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm in uni_filenames[:npic]:\n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n    image_load = load_img(filename, target_size=target_size)\n    \n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0,i,caption,fontsize=20)\n    count += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"137770604892d1621f19cb208289052d9323ac6b"},"cell_type":"code","source":"def df_word(df_txt):\n    vocabulary = []\n    for i in range(len(df_txt)):\n        temp=df_txt.iloc[i,2]\n        vocabulary.extend(temp.split())\n    print('Vocabulary Size: %d' % len(set(vocabulary)))\n    ct = Counter(vocabulary)\n    dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n    dfword = dfword.sort_values(\"count\",ascending=False)\n    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n    return(dfword)\ndfword = df_word(df_txt)\ndfword.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"311cfddbf7a542434f3a5c9ee76b4c29bb315ba9"},"cell_type":"code","source":"topn = 50\n\ndef plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n    plt.figure(figsize=(20,3))\n    plt.bar(dfsub.index,dfsub[\"count\"])\n    plt.yticks(fontsize=20)\n    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n\nplthist(dfword.iloc[:topn,:],\n        title=\"The top 50 most frequently appearing words\")\nplthist(dfword.iloc[-topn:,:],\n        title=\"The least 50 most frequently appearing words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ff3d8ae52c75f88f787a3d589e7356d5e08eac5"},"cell_type":"code","source":"import string\ndef remove_punctuation(text_original):\n    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n    return(text_no_punctuation)\n\ndef remove_single_character(text):\n    text_len_more_than1 = \"\"\n    for word in text.split():\n        if len(word) > 1:\n            text_len_more_than1 += \" \" + word\n    return(text_len_more_than1)\n\ndef remove_numeric(text,printTF=False):\n    text_no_numeric = \"\"\n    for word in text.split():\n        isalpha = word.isalpha()\n        if printTF:\n            print(\"    {:10} : {:}\".format(word,isalpha))\n        if isalpha:\n            text_no_numeric += \" \" + word\n    return(text_no_numeric)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01e31bb22be3c26c64df6a0239f04e2356ac4f85","scrolled":true},"cell_type":"code","source":"def text_clean(text_original):\n    text = remove_punctuation(text_original)\n    text = remove_single_character(text)\n    text = remove_numeric(text)\n    return(text)\n\nwith progressbar.ProgressBar(max_value=len(df_txt.caption.values)) as bar:\n    for i, caption in enumerate(df_txt.caption.values):\n        newcaption = text_clean(caption)\n        df_txt[\"caption\"].iloc[i] = newcaption\n        bar.update(i)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab2ad8cde01dc1b82a55855bc0b26d9f7e1111fe","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"dfword = df_word(df_txt)\nplthist(dfword.iloc[:topn,:],\n        title=\"The top 50 most frequently appearing words\")\nplthist(dfword.iloc[-topn:,:],\n        title=\"The least 50 most frequently appearing words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4d83e6c91f60f0ea7a0277be1925c060a23d58c"},"cell_type":"code","source":"from copy import copy\ndef add_start_end_seq_token(captions):\n    caps = []\n    for txt in captions:\n        txt = 'startseq ' + txt + ' endseq'\n        caps.append(txt)\n    return(caps)\ndf_txt0 = copy(df_txt)\ndf_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\ndf_txt0.head(5)\ndel df_txt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5b941ecb6dc4a2d9e6d25d7d134d382a18b5640"},"cell_type":"code","source":"from keras.applications import VGG16\n\nmodelvgg = VGG16(include_top=True,weights=None)\n## load the locally saved weights \nmodelvgg.load_weights(\"../input/vgg16-weights-image-captioning/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\nmodelvgg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0fe068902e61e5b8b84773713e3b27c334a46ec"},"cell_type":"code","source":"from keras import models\nmodelvgg.layers.pop()\nmodelvgg = models.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-1].output)\n## show the deep learning model\nmodelvgg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27f224723f4cc0a094bec3f4d813d9dbaac64d71"},"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom collections import OrderedDict\n\nimages = OrderedDict()\nnpix = 224\ntarget_size = (npix,npix,3)\nwith progressbar.ProgressBar(max_value=len(jpgs)) as bar:\n    for i,name in enumerate(jpgs):\n        # load an image from file\n        filename = dir_Flickr_jpg + '/' + name\n        image = load_img(filename, target_size=target_size)\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        nimage = preprocess_input(image)\n        y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n        images[name] = y_pred.flatten()\n        bar.update(i)\n    #print(i,filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45c41171a8d0c0afe4f005e9d894b4459060fd98"},"cell_type":"code","source":"dimages, keepindex = [],[]\nnd=(df_txt0[\"index\"].values)\nb = [(int(i)==0) for i in nd]\n#for i in nd:\n #   print(int(i)==0)\n#df_txt0 = df_txt0.loc[b,: ]\ndf_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n\nfor i, fnm in enumerate(df_txt0.filename):\n    if fnm in images.keys():\n        dimages.append(images[fnm])\n        keepindex.append(i)\n        \nfnames = df_txt0[\"filename\"].iloc[keepindex].values\ndcaptions = df_txt0[\"caption\"].iloc[keepindex].values\ndimages = np.array(dimages)\nprint(df_txt0[\"index\"][:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6acd485dde5d11e73386d04ba3c68265d6ad2d4"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n## the maximum number of words in dictionary\ncount_words=22000\n#nb_words = 31782\ntokenizer = Tokenizer(num_words=8000)\ntokenizer.fit_on_texts(dcaptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"vocabulary size : {}\".format(vocab_size))\ndtexts = tokenizer.texts_to_sequences(dcaptions)\nprint(dtexts[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b36b767ca11c8429fdb3792c0858f96c5a569a"},"cell_type":"code","source":"prop_test, prop_val = 0.2, 0.2 \n\nN = len(dtexts)\nNtest, Nval = int(N*prop_test), int(N*prop_val)\n\ndef split_test_val_train(dtexts,Ntest,Nval):\n    return(dtexts[:Ntest], \n           dtexts[Ntest:Ntest+Nval],  \n           dtexts[Ntest+Nval:])\n\ndt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\ndi_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\nfnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fd48680f08937e9eacce1a58db86139e18ec48e"},"cell_type":"code","source":"maxlen = np.max([len(text) for text in dtexts])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0be04ecb6535bfb214e0a93fe4cb51d7dc2e6b89"},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef preprocessing(dtexts,dimages):\n    N = len(dtexts)\n    print(\"# captions/images = {}\".format(N))\n\n    assert(N==len(dimages))\n    Xtext, Ximage, ytext = [],[],[]\n    for text,image in zip(dtexts,dimages):\n\n        for i in range(1,len(text)):\n            in_text, out_text = text[:i], text[i]\n            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()\n            out_text = to_categorical(out_text,num_classes = vocab_size)\n\n            Xtext.append(in_text)\n            Ximage.append(image)\n            ytext.append(out_text)\n\n    Xtext  = np.array(Xtext)\n    Ximage = np.array(Ximage)\n    ytext  = np.array(ytext)\n    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n    return(Xtext,Ximage,ytext)\n\n\nXtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\nXtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n# pre-processing is not necessary for testing data\n#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15cbdc9b4a17199274a7f37307d534ab6b3ce2ea"},"cell_type":"code","source":"from keras import layers\nprint(vocab_size)\n## image feature\n\ndim_embedding = 64\n\ninput_image = layers.Input(shape=(Ximage_train.shape[1],))\nfimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n## sequence model\ninput_txt = layers.Input(shape=(maxlen,))\nftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\nftxt = layers.LSTM(256,name=\"CaptionFeature\")(ftxt)\n## combined model for decoder\ndecoder = layers.add([ftxt,fimage])\ndecoder = layers.Dense(256,activation='relu')(decoder)\noutput = layers.Dense(vocab_size,activation='softmax')(decoder)\nmodel = models.Model(inputs=[input_image, input_txt],outputs=output)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ac5130bd346aef2693901abf7687c85c18b30d2"},"cell_type":"code","source":"start = time.time()\n#checkpoint_path = \"training_1/cp.ckpt\"\n#checkpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create checkpoint callback\n#cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n #                                                save_weights_only=True,\n  #                                               verbose=2)\n\nhist = model.fit([Ximage_train, Xtext_train], ytext_train, \n                  epochs=7, verbose=2, \n                  batch_size=64,\n                  validation_data=([Ximage_val, Xtext_val], ytext_val))\n                #callbacks = [cp_callback])\nend = time.time()\nprint(\"TIME TOOK {:3.2f}MIN\".format((end - start )/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45efce17423163a8e8c913e9a5faddbc0f0c9677"},"cell_type":"code","source":"print(Ximage_train.shape,Xtext_train.shape,ytext_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"125044cdd5c646cec415b69ababe2a0df949453f"},"cell_type":"code","source":"for label in [\"loss\",\"val_loss\"]:\n    plt.plot(hist.history[label],label=label)\nplt.legend()\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c91a93567a45838312e7896a2093a09cd1e122e4"},"cell_type":"code","source":"index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\ndef predict_caption(image):\n    '''\n    image.shape = (1,4462)\n    '''\n\n    in_text = 'startseq'\n\n    for iword in range(maxlen):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence],maxlen)\n        yhat = model.predict([image,sequence],verbose=0)\n        yhat = np.argmax(yhat)\n        newword = index_word[yhat]\n        in_text += \" \" + newword\n        if newword == \"endseq\":\n            break\n    return(in_text)\n\n\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n    ## images \n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.text(0,0.5,caption,fontsize=20)\n    count += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8a9126dc723a4897e979326d71ff4c87a301b4e"},"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nindex_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n\n\nnkeep = 5\npred_good, pred_bad, bleus = [], [], [] \ncount = 0 \nfor jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n    count += 1\n    if count % 200 == 0:\n        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n    \n    caption_true = [ index_word[i] for i in tokenized_text ]     \n    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    caption = caption.split()\n    caption = caption[1:-1]## remove startreg, and endreg\n    \n    bleu = sentence_bleu([caption_true],caption)\n    bleus.append(bleu)\n    if bleu > 0.7 and len(pred_good) < nkeep:\n        pred_good.append((bleu,jpgfnm,caption_true,caption))\n    elif bleu < 0.3 and len(pred_bad) < nkeep:\n        pred_bad.append((bleu,jpgfnm,caption_true,caption))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85af8b6b985836c1240f3bccfefe8bb0308d5947"},"cell_type":"code","source":"\ndef plot_images(pred_bad):\n    def create_str(caption_true):\n        strue = \"\"\n        for s in caption_true:\n            strue += \" \" + s\n        return(strue)\n    npix = 224\n    target_size = (npix,npix,3)    \n    count = 1\n    fig = plt.figure(figsize=(10,20))\n    npic = len(pred_bad)\n    for pb in pred_bad:\n        bleu,jpgfnm,caption_true,caption = pb\n        ## images \n        filename = dir_Flickr_jpg + '/' + jpgfnm\n        image_load = load_img(filename, target_size=target_size)\n        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        count += 1\n\n        caption_true = create_str(caption_true)\n        caption = create_str(caption)\n        \n        ax = fig.add_subplot(npic,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,1)\n        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n        count += 1\n    plt.show()\n\nprint(\"Bad Caption\")\nplot_images(pred_bad)\nprint(\"Good Caption\")\nplot_images(pred_good)\n ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}