{"cells":[{"metadata":{"_uuid":"b7e855155a95500e075eba351b3b29a30a9aa61f"},"cell_type":"markdown","source":"# Rain in Australia\n\nThis project makes a prediction model to predict the weather for the next day in Australia, if it is going to rain or not, through a binary outcome in a model trained.\n\n\n### About data\n\nThis dataset have datas about registrations and observation about wheater by meteorological stationsin some days in Australia.\n\nThe target is the features **RainTomorrow** that respond the question: \"Will it rain tomorrow in Australia? Yes or no\".\n\n### Sources\n\nThe sources about this data is Kaggle in: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/home.\nFor more details about the data and source access link."},{"metadata":{"_uuid":"ce6a6cfbb64d61cdefcc4c055d4d748e17abbbf0"},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true,"_uuid":"ceaa3667b42d1a121a8a00bb7139c8a329a50ade"},"cell_type":"code","source":"# For explore data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# For escalation of values\nfrom scipy import stats\n\n# For machine learning modeles\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.ensemble import  AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn import preprocessing\n\n# For the validation of models\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom numpy import mean\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2057fa794df325590cde063689dc4589d92142f3"},"cell_type":"markdown","source":"### Data analysis\n\nFirst we will access the data and view the dataframe."},{"metadata":{"trusted":true,"_uuid":"c8540af2d56b22ca15ace122e7f5bfb0035a1808"},"cell_type":"code","source":"df = pd.read_csv('../input/weatherAUS.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf26ab2621e954c0fc88c8e9bcb9168b8b220165"},"cell_type":"code","source":"# The Shape of dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a1a69b10df691e0e99489cdfdd98083570d4cb2"},"cell_type":"markdown","source":"By orientation of the source will not use the feature &Quot;RISK_MM\". For more information: https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/discussion/78316."},{"metadata":{"trusted":true,"_uuid":"36b617804a9f6fbe6d5d30f447f7e66a79382dc6"},"cell_type":"code","source":"# Removing \"RISK_MM\"\ndf = df.drop(columns='RISK_MM')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56644d47cee334a0b4d0a6e9458791e7a96a6bbd"},"cell_type":"markdown","source":"Accessing the information from each feature and their quantities."},{"metadata":{"trusted":true,"_uuid":"bca08f1b98bdcc4d2108eed2905aea8a306f7bbe"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d432486bc6c226196cf833c1c269c4476da8f562"},"cell_type":"markdown","source":"Looking this information about data above and some vizualition about some features with **value.counts**, vizualization that spend so much space that is best hides them, we remove some features:\n\n* Location: The question is \"Will it rain tomorrow in Australia\", so this dosen't have much importance about teh question.\n* Date: This dates works more like a index about information and they are not continuity, just random.\n* Evaporation, Sunshine, Cloud9am e Cloud3pm : This features have much null values and a large grade. Therefore it is better to remove them.\n\n### Preparing the data\n\nLet's start removing the unnecessary features for the models"},{"metadata":{"trusted":true,"_uuid":"26f6dc9565eaffdfe2fc9728bed1a0a0f7323a10"},"cell_type":"code","source":"df = df.drop(columns=['Location','Date', 'Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], axis=1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bf4bc07704f78ed7a129c09ea7942a026a98a86"},"cell_type":"markdown","source":"Now the features have values of quantity similars."},{"metadata":{"trusted":true,"_uuid":"4cfc3c448d06750f108b23e52bf2e7c0baf45816"},"cell_type":"code","source":"# Shape vizualization\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb6cbce81c1d4dfaa290a34071902bdbb4bd3e97"},"cell_type":"markdown","source":"This step is to remove null values, a importante step for predictions, and cheking if they have been removed."},{"metadata":{"trusted":true,"_uuid":"1e24c25e7f75d0edb1789b17d3dde1bdcc585e02"},"cell_type":"code","source":"df = df.dropna(how='any')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"514369827cf51c282e77b5e22e6369c885a5395f"},"cell_type":"markdown","source":"Looking if there are outlier in some features"},{"metadata":{"trusted":true,"_uuid":"53a19b42ce05de660a0aa3e4616e9e6f31e6dfdc"},"cell_type":"code","source":"# Ploting blockspots\nsns.boxplot(x=df['MinTemp'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bcb8e0846a35bde1ab9425098b42bf423e1aa3e"},"cell_type":"code","source":"sns.boxplot(x=df['MaxTemp'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a216e72c66c6dccf0fa47bc11787673d7b0ec36c"},"cell_type":"code","source":"sns.boxplot(x=df['WindGustSpeed'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcb19cebcd2a3484d48a1c440355845bed252e9a"},"cell_type":"code","source":"sns.boxplot(x=df['Rainfall'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfec06c20299d2970bb164b01a4870db96d7f004"},"cell_type":"code","source":"sns.boxplot(x=df['WindSpeed9am'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b630de4d2f106644d37d8ab4a914fb6b166581ef"},"cell_type":"code","source":"sns.boxplot(x=df['Humidity9am'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6f57fb2aaae2092b439826a489b69e35a844f42"},"cell_type":"markdown","source":"An exemple of outliers can be observed in the **Humidity9am** graph. There as some values of 0 and 100 about Humidity, this values are unreals to be found in a open space.\n\nTo remove outliers I will apply the Z-score technic. This technic do a escalation of all the values of the dataframe and relates to the average and the standard challenge, generating a score for each value. The value of the average is 0 and those who are within a normal range of standard deviations are between 1 and -1. Thus the values that have scores of near pf 3 and -3 are considered outliers, can be removed from the dataframe."},{"metadata":{"trusted":true,"_uuid":"4c568560e0a479b1d674a12b89ac60c1ea4b858a"},"cell_type":"code","source":"# Appling a escalation in numerics datas using \"get_numeric_data\".\nz = np.abs(stats.zscore(df._get_numeric_data()))\n# Print a table with z-scores\nprint(z)\n# Removing outliers\ndf= df[(z < 3).all(axis=1)]\n# Looking the new shape of dataframe\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d897eada0dff9e09db45b63de292387b812d10d"},"cell_type":"markdown","source":"Cheking the quantity of value now in all features"},{"metadata":{"trusted":true,"_uuid":"a5b70012c0cd64582acab72f5eaf0a6a5b3a3b75"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84dbea56aa3f2e1e1dd527b6d069cd9ee7a3eb66"},"cell_type":"markdown","source":"Now I will change some cagetorical features about wind direction to numerical features (**WindGustDir, WindDir3pm, WindDir9am**).\n\nLooking the quantity of wind direction"},{"metadata":{"trusted":true,"_uuid":"4f0fd6f8342e0e73fc9376b9cceb731d55d43bb3"},"cell_type":"code","source":"len(df.WindGustDir.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37e72e7caacb6ea2d6b61a88660614fa3f131172"},"cell_type":"markdown","source":"Changing the categorical featuries using dummies funcition."},{"metadata":{"trusted":true,"_uuid":"5a38ecf4ca8779c8198e8d68e85700cd231d8bbe"},"cell_type":"code","source":"# List of features that will be changed\nwinds = ['WindGustDir', 'WindDir3pm', 'WindDir9am']\n    \n# Doing the transformation with \"get_dummies\"\ndf = pd.get_dummies(df, columns=winds)\n\n# Cheking the new shape\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"837776c5606da122dfe85ea7126ac3cf16951a55"},"cell_type":"markdown","source":"Now the dataframe has more 48 columns, that repalce the 3 old columns about wind direction. However for mathematical purposes we need drop one columns for each group created in function of a wind direction feature, because the values of this columns need be independent among them."},{"metadata":{"trusted":true,"_uuid":"9c0abb0f8e7f2a0e3e0af9129bd18bd4a638be38"},"cell_type":"code","source":"# Removing one collumns of each group\ndf = df.drop(['WindGustDir_WSW', 'WindDir3pm_SSW', 'WindDir9am_NNE'], axis =1)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddbce0c30d230a9659e9c9fb5502b93dbdd17a83"},"cell_type":"markdown","source":"Converting the values of **RainToday e RainTomorrow** to 0 e 1, that represents labels of a binary system."},{"metadata":{"trusted":true,"_uuid":"33ea49f288019d22dfaa6668d56c8869f77fd4ea"},"cell_type":"code","source":"df['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8309d9c637736953846951ce6f0f1f80aa0943cd"},"cell_type":"markdown","source":"Cheking the conversion"},{"metadata":{"trusted":true,"_uuid":"f9244f4835c85d2fae89240d90a4b140f22ea597"},"cell_type":"code","source":"df.RainToday.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eb2b597e866dfd429c6e7dd17f3f6920fb7a0ec"},"cell_type":"code","source":"df.RainTomorrow.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2345807ff7e7423b2f4ef104e527b4dd61ad515a"},"cell_type":"markdown","source":"The last step before start work with modes of machine learning was to make the escalation of all values to be between 0 and 1. This will help the values as pressure (units of thousands) and temperature (units of tens) do not distort the models, since they have different proportions. This escalation does not alter the differences between values in the same category."},{"metadata":{"trusted":true,"_uuid":"f3167e9805fb47137683dbe8d8d65e51f2b9e91f"},"cell_type":"code","source":"# Doing the escalation using \"MinMaxScale\" model\nscaler = preprocessing.MinMaxScaler()\n# Training the model\nscaler.fit(df)\n# Changing data \ndf = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)\n# Returning the data frama after the escalation\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8500c1b9f46ca256e98f38fa311cc16b386cbe27"},"cell_type":"markdown","source":"### Predictive models\n\nThe first step of the process will be to determine what are the features  has greater relevance for the predictive models. For this I will fit the function **SelectKBest** to generate scores of features to see which are most relevant."},{"metadata":{"trusted":true,"_uuid":"f3747a2fac43d82a78bb87bddb6e55e9fe371515"},"cell_type":"code","source":"# Splinting the data in features (X) and labels (y)\nX = df.loc[:,df.columns!='RainTomorrow']\ny = df[['RainTomorrow']]\n# Using função SelectKBest and determining the parameters numbers of features, K = 58\nselector = SelectKBest(chi2, k=58)\n# Traning\nselector.fit(X, y)\n# Returning scores\nscores = selector.scores_\n# Creating a list for features names\nlista = df.columns\nlista = [x for x in lista if x != 'RainTomorrow']\n# Creationg a dictionaty with the features name list and scores  \nunsorted_pairs = zip(lista, scores)\nsorted_pairs = list(reversed(sorted(unsorted_pairs, key=lambda x: x[1])))\nk_best_features = dict(sorted_pairs[:58])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67a90e9bc7524d387c6ede06b2cc30d50a825ed1"},"cell_type":"markdown","source":"Ploting a bar graphic about features scores."},{"metadata":{"trusted":true,"_uuid":"5791255201feb4743ed1955b663d8f72ce7294be"},"cell_type":"code","source":"# Ploting the graphic area\nplt.figure(figsize=(20,7),facecolor = 'w',edgecolor = 'w')\n# Ploting the bar graphic\np = plt.bar(range(len(k_best_features)), list(k_best_features.values()), align='center')\nplt.xticks(range(len(k_best_features)), list(k_best_features.keys()))\n# Editing the names\nplt.xticks(rotation='90')\nplt.title('K best features scores')\nplt.xlabel('Features')\nplt.ylabel('Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05faa337108e27156083fdc7f31a221f47558e1a"},"cell_type":"markdown","source":"Now we can see that **RainToday** is the most important feature for the models. The features about wind direction don't have high scores. I will use just features that have scores above 1% (71 points) **RainToday** score (7136 points)."},{"metadata":{"trusted":true,"_uuid":"332332f578822e13920792cc345ca751d4566c23"},"cell_type":"code","source":"# Creating a list of features names with score above 71 points\nK_values = []\nfor key in k_best_features:\n    if float(k_best_features[key]) >= float(0.01 * k_best_features['RainToday']):\n        K_values.append(key)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9c98d6c9ca13a785d4f7c01667ff52ce471e956"},"cell_type":"markdown","source":"Spliting the new group of features (X) and labels (y) for the models"},{"metadata":{"trusted":true,"_uuid":"09dff79caa87d7b9ddad92c676d341d6b599fc21"},"cell_type":"code","source":"df_predi = df[K_values + ['RainTomorrow']]\nX = df[K_values]\ny = df['RainTomorrow']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7b14b144ed6481d2443b4c4ed44072cbdaae0f5"},"cell_type":"markdown","source":"Now I will analyze how many characteristics the models will have a good performace.\nThe models that I will work:\n\n* Logistic Regression\n* Decision Tree\n* Kmeans\n"},{"metadata":{"trusted":true,"_uuid":"5597c89189366b4a788629739870bea3ff8eb2bc"},"cell_type":"code","source":"# Cirando uma lista para contagens de features de K_values, no caso 31\nn_features_list = list(range(2,len(K_values)+1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03ccb7d792476d7ce4cbc8a6f87131a7ad0557fd"},"cell_type":"markdown","source":"Although we have chosen the best features for models, we also assess the quantity of features the models will get the best results, so let's create a loop for the models returning the accuracy of results. For each interaction will be removed from the feature that has the lowest score."},{"metadata":{"trusted":true,"_uuid":"2ea71736406b08ada3f1ebd6a9f52f498b80feda"},"cell_type":"code","source":"# Creating list for each model to append reseults\n# For Logistic Regression\naccuracy_LR=[]\n# For Decision Tree\naccuracy_dt=[]\n# For Kmeans\naccuracy_Kmeans=[]\n\n# Creating a loop for the number of features unsing the list of names features\nfor n in n_features_list:  \n    \n    # Splinting the values for the training and test sets with \"train_test_split\"\n    # We will leave 20% of the data for test and the rest for training.\n    features_train, features_test, labels_train, labels_test = train_test_split(df[K_values[:n]], y, test_size=0.2, random_state=42)\n\n    # Applying Logistic Regression model\n    l_clf = LogisticRegression()\n    # Training\n    l_clf.fit(features_train, labels_train)\n    # Doing the prediction\n    prediction_lr = l_clf.predict(features_test)\n    # Append the values of accuracy in a list\n    accuracy_LR.append(accuracy_score(labels_test, prediction_lr))\n    \n    # The steps are the same for others models\n    \n    # For Decision Tree\n    dt_clf = DecisionTreeClassifier(random_state=0)\n    dt_clf.fit(features_train, labels_train)\n    prediction_dt = dt_clf.predict(features_test)\n    accuracy_dt.append(accuracy_score(labels_test, prediction_dt))\n    \n    # For Kmeans\n    k_clf = KMeans(n_clusters=2)\n    k_clf.fit(features_train, labels_train)\n    prediction_k = k_clf.predict(features_test)\n    accuracy_Kmeans.append(accuracy_score(labels_test, prediction_k))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a55fe54307b470522d81a43b721804af9ab93dc8"},"cell_type":"markdown","source":"To understande better the models performace changing the number of features, I created a graphic about accuracy."},{"metadata":{"trusted":true,"_uuid":"fef60956e83ddba92ab475b6562f8671d96a33ec"},"cell_type":"code","source":"# Ploting the graphic area\nplt.figure(figsize=(9,6),facecolor = 'w',edgecolor = 'w')\n\n# Ploting the graphic about accuracy x number of features for each model\n# Losgistic Regression\nline1 = plt.plot(n_features_list, accuracy_LR, 'b', label='LR')\n# Decision Tree\nline2 = plt.plot(n_features_list, accuracy_dt, 'r', label= 'dt')\n# Kmeans\nline3 = plt.plot(n_features_list, accuracy_Kmeans, 'g', label= 'Kmean')\n\n# Editing the names \nplt.legend(('Logistic Regression', 'Decision tree', 'Kmean'), loc = 'best')\n# Editing labels\nplt.title('accuracy x features')\nplt.ylabel('accuracy score')\nplt.xlabel('n features')\n# Editing grids\nplt.xticks(n_features_list)\nplt.grid(b='true',which='both', axis='both')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dac0ec24b281622e37d80f42ba8c3c7f82a44a70"},"cell_type":"markdown","source":"Now we can see that the models had a good performace with 3 features **(RainToday, rainfall and humidity3pm)**, only Kmeans who had the same income with more features. We'll use this quantity of 3 going forward to compare the 3 models. The next step is determine the best parameters for each of the methods using **GridSearchCV**, that will be applied for in to least 3 parameters.\n"},{"metadata":{"trusted":true,"_uuid":"9b7b1a8a12905580b998553647c9482f2e29edd8"},"cell_type":"code","source":"# Splinting data to test and traing with 3 features\nX = df[K_values[:3]]\ny = df['RainTomorrow']\nfeatures_train, features_test, labels_train, labels_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02d1dee2453b610ea3acea6a83f36f5cde125ebb"},"cell_type":"markdown","source":"For Logistic Regression I will change:\n\n* Solver - Algorithm to use in the optimization problem (newton-cg, lbfgs, liblinear, sag, saga)\n* C - Inverse of regularization strength $(0.01, 0.1, 10, 105,^10^{10}, 10^{15}, 10^{20})$\n* tol - Tolerance for stopping criteria $(10^{-20}, 10^{-15}, 10^{-10}, 10^{-5}, 0.01, 0.1, 10)$"},{"metadata":{"trusted":true,"_uuid":"2e62e85fe28b72796a8b02c170804bdab900a3fd"},"cell_type":"code","source":"# Creating a list with parameters \nparameters = {'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'), 'C':[0.01, 0.1, 10, 10**5,10**10,10**15,10**20],'tol':[10**-20,10**-15,10**-10,10**-5,0.01, 0.1, 10]}\n# Applying the model\nl_clf = LogisticRegression()\nclf = GridSearchCV(l_clf, parameters)\nclf.fit(features_train, labels_train)\n# Outout of parameters \nbest_l_clf = clf.best_estimator_\nclf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2897e6c6b747f188b41e9c4c12c5964562dabe87"},"cell_type":"markdown","source":"For Decision Tree:\n\n* Criterion - the function to measure the quality of a split (gini ou entropy)\n* min_sample_leaf - The minimum number of samples required to be at a leaf node (1-5)\n* max_depth - The maximum depth of the tree. Ela pode chegar ao máximo das min_sample_leaf (1-5)\n* class_weight - Weights associated with classes in the form (balanced)"},{"metadata":{"trusted":true,"_uuid":"a650d98dabd8f3f146d90839ab29f5b916ab3304"},"cell_type":"code","source":"# Creating a list with parameters \nparameters = { 'criterion': ('gini', 'entropy'), 'min_samples_leaf' : range(1, 5), 'max_depth' : range(1, 5), 'class_weight': ['balanced'] }\n# Applying the model\ndt_clf = DecisionTreeClassifier(random_state=0)\nclf = GridSearchCV(dt_clf, parameters)\n# Outout of parameters \nclf.fit(features_train, labels_train)\nclf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93d91444148f78b42750b1d85371149188d75951"},"cell_type":"markdown","source":"For Kmeans:\n\n* algorithm - K-means algorithm to use (auto, full, elkan)\n* tol - Relative tolerance with regards to inertia to declare convergence $(10^{-20},10^{-15},10^{-10},10^{-5},0.01, 0.1, 10)$\n* n_init - Number of time the k-means algorithm will be run with different centroid seeds (10,25,50,75,100,200)"},{"metadata":{"trusted":true,"_uuid":"766b9afe97d651cabfeb53b567f1fa5d7acaf3f3"},"cell_type":"code","source":"# Creating a list with parameters \nparameters = {'algorithm':('auto', 'full', 'elkan'), 'tol':[10**-20,10**-15,10**-10,10**-5,0.01, 0.1, 10], 'n_init': [10,25,50,75,100,200], 'algorithm': ('auto', 'full', 'elkan')}\n# Applying the model\nk_clf = KMeans(n_clusters=2)\nclf = GridSearchCV(k_clf, parameters)\n# Outout of parameters \nclf.fit(features_train, labels_train)\nclf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b8acc1d287a3938ac69e573dc8b559cd3c98a47"},"cell_type":"markdown","source":"Now with the optimization of parameters for each prediction models, I will make the prediction for rain using the method of validation.\n\nMaking the storage of configurations for each model. Here we will see other parameters not mentioned above, they are other options for adjustments. How were not assessed, are returned in the standard form of each model."},{"metadata":{"trusted":true,"_uuid":"19d6f25d544a9708cbc7bab551327eda1d95c940"},"cell_type":"code","source":"# Logistic Regression\nl_clf = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='saga', tol=0.1,\n          verbose=0, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab2695759140cc8f7f0c4b184275feb46bd2faeb"},"cell_type":"markdown","source":"In decision tree will add the AdaBoostClassifier to try to improve the result."},{"metadata":{"trusted":true,"_uuid":"90b14b27a3ab2c661c54c304936187423194d44f"},"cell_type":"code","source":"# Decision Tree com AdaBoost\ndt_clf = AdaBoostClassifier(DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n            splitter='best'), n_estimators=50, learning_rate=.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"092525a1fb6b1ce198b1de85a2dc05a8f5ab738e"},"cell_type":"code","source":"# Kmean\nk_clf =  KMeans(algorithm='elkan', copy_x=True, init='k-means++', max_iter=300,\n    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n    random_state=None, tol=1e-20, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"622f2b1e329e6394891c436a16da322f3545ae2b"},"cell_type":"markdown","source":"The model validation consists in making thousand interactions of predictive models and return the accuracy, precision and recall for a list. From the results make an average of this values to have a result more near of the real value to determine the best model.\nWhich allows you to make these thousand interactions to validate the models is that the function **train_test_split**. It will select in each interaction a set of different database for testing and training, thus covering a larger quantity of formations to which the model train and generate more reliable results, once covering unique combinations.\n\nPrecision and reacll can best help in binary prediction that only accuracy, avoiding overfitings.\n\nCreating the function for validation:"},{"metadata":{"trusted":true,"_uuid":"86b1bd337f4e649c1af0e4b43c5dd4cd245927fb"},"cell_type":"code","source":"def avaliacao_clf(clf, features, labels, n_iters=1000):\n    print (clf)\n    \n    # Creating list for outputs\n    accuracy = []\n    precision = []\n    recall = []\n    first = True\n    \n    # Creating a loop to thousand interactions\n    for tentativa in range(n_iters):\n        \n        # Splinting data to test and traing\n        features_train, features_test, labels_train, labels_test = train_test_split(X, y, test_size=0.3)\n\n        # Applying the model\n        clf.fit(features_train, labels_train)\n        predictions = clf.predict(features_test)\n        # Appending accuracy\n        accuracy.append(accuracy_score(labels_test, predictions))\n        # Appending precision\n        precision.append(precision_score(labels_test, predictions))\n        # Appending recall\n        recall.append(recall_score(labels_test, predictions))\n\n    # Taking the average of metrics for evaluating and implementing the results\n\n    print (\"precision: {}\".format(mean(precision)))\n    print (\"recall:    {}\".format(mean(recall)))\n    print (\"accuracy:    {}\".format(mean(accuracy)))\n    \n    return mean(precision), mean(recall), mean(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4002f7a779187053f8593d8cfa18f39cef731442"},"cell_type":"markdown","source":"Doing the validation for logistic regression:"},{"metadata":{"trusted":true,"_uuid":"e935478a4d913492e1387422ff4449f8dfac27be"},"cell_type":"code","source":"avaliacao_clf(l_clf, X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fb9be06dceefb3312c77ed2f95caa997ea9950b"},"cell_type":"markdown","source":"For Decision Tree:"},{"metadata":{"trusted":true,"_uuid":"381b5f5c4a664b98b74d9d4996eb20b248043971"},"cell_type":"code","source":"avaliacao_clf(dt_clf, X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1b6db1d8f492b8e1498549eb9485ce75e0c99c2"},"cell_type":"markdown","source":"For Kmeans:"},{"metadata":{"trusted":true,"_uuid":"2e88844b6a8f768f572e889922c97eaadc2f36f5"},"cell_type":"code","source":"avaliacao_clf(k_clf, X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e7ac33f461ad40ea228b7b75c28f43a573d943a"},"cell_type":"markdown","source":"### Conclusion\n\nThe resuts for accuracy, precision e recall are in the table below:\n\n\n| Model                | Precision   | Recall      | Accuracy    |\n|---------------------------|-------------|-------------|-------------|\n| Decision Tree - Adboost   | 0.454       | 0.694       | 0.763       |\n| KMeans                    | 0.366       | 0.467       | 0.637       |\n| Logistic Regression       | 0.721       | 0.378       | 0.837       |\n\nFrom these results we can conclude that the model of Logistic Regression is the best option between models because it has the best results of precision, having the best performance in generating true positives, recall, evaluating with precision has the return of best result of correct data and accuracy best among the three methods."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}