{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt #visualization\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# additional imports:\nimport seaborn as sns\nimport re\nimport sys\nfrom time import sleep\nfrom tqdm.notebook import tqdm\nimport warnings;\nwarnings.filterwarnings('ignore');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/bengaluru-house-price-data/Bengaluru_House_Data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning:"},{"metadata":{},"cell_type":"markdown","source":"Lets check the null or nan values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove unnecessary columns \ndf1 = df.drop(['area_type','society','balcony','availability'],axis='columns')\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.dropna()\ndf1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check 'size' column\ndf1['size'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we need numeric values, so we will remove bedroom and BHK strings from all values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# FUNCTION to remove string from row values.\n# Nan values will be replaced by 0\ndef remove_string(x):\n    x = str(x)\n    if x == 'nan':\n        x = np.NaN\n    else:\n        x = int(x.split(\" \")[0])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create new column for the cleaned values of size column:\ndf1['BHK'] = df1['size'].apply(lambda x: remove_string(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['BHK'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[df1.BHK > 20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.total_sqft.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some values in range format. like 1133-1384"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to catch all non numeric and abnormal values:\ndef catch_abnormal_val(series):\n    err_val = []\n    for x in series:\n        try:\n            float(x)\n        except:\n            err_val.append(x)\n    return err_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catch_abnormal_val(df1['total_sqft'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not just range values, we can see there are some numeric values written in sqft, perch, acres, yards, cents and ground formats."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets modify the range format values first:\n# function that will identfy range format values and convert them to single float value:\ndef convert_rng_val(x):\n    values = x.split('-')\n    if len(values) == 2:\n        return (float(values[0])+float(values[1]))/2 #return float mean value of range\n    try:\n        return float(x) #return remaining values in float.\n    except:\n        return x #return other abnormal value as it is.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(convert_rng_val('1200'))\nprint(convert_rng_val('1200-2349'))\nprint(convert_rng_val('1200sqft. Meter'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* sq Meters to sqft: 10.764 * sq.meters\n* sqYards to sqft: 9 * sqYards\n* gunta to sqft: 1089 * gunta\n* acres to sqft: 43560 * acres\n* perch to sqft: 272.25 * perch\n* Grounds to sqft: 2400 * ground\n* Cents to sqft: 435.6 * cent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sqmt_to_sqft(x):\n    \"\"\"convert sq.meters to sqft\"\"\"\n    return x * 10.764\n\ndef sqyards_to_sqft(x):\n    \"\"\"convert sq.yards to sqft\"\"\"\n    return x * 9\n\ndef gunta_to_sqft(x):\n    \"\"\"convert gunta to sqft\"\"\"\n    return x * 1089\n\ndef acres_to_sqft(x):\n    \"\"\"convert acres to sqft\"\"\"\n    return x * 43560\n\ndef perch_to_sqft(x):\n    \"\"\"convert perch to sqft\"\"\"\n    return x * 272.25\n\ndef grounds_to_sqft(x):\n    \"\"\"convert grounds to sqft\"\"\"\n    return x * 2400\n\ndef cents_to_sqft(x):\n    \"\"\"convert cents to sqft\"\"\"\n    return x * 435.6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All functions are ready now will make one parent function to clean our 'total_sqft' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_total_sqft(y):\n    try:\n        y = float(y)\n    except:\n        if \"-\" in y:\n            y = round(convert_rng_val(y),1)\n        elif \"Sq. Meter\" in y:\n            y = round(sqmt_to_sqft(float(re.findall('\\d+',y)[0])),1)\n        elif \"Sq. Yards\" in y:\n            y = sqyards_to_sqft(float(re.findall('\\d+',y)[0]))\n        elif \"Guntha\" in y:\n            y = gunta_to_sqft(float(re.findall('\\d+',y)[0]))\n        elif \"Acres\" in y:\n            y = acres_to_sqft(float(re.findall('\\d+',y)[0]))\n        elif \"Perch\" in y:\n            y = perch_to_sqft(float(re.findall('\\d+',y)[0]))\n        elif \"Grounds\" in y:\n            y = grounds_to_sqft(float(re.findall('\\d+',y)[0])) \n        elif \"Cents\" in y:\n            y = round(cents_to_sqft(float(re.findall('\\d+',y)[0])),1)\n        return y\n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_total_sqft(\"13Sq. Yards\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets clean our column and create a cleaned version of it:\ndf1['total_sqft_cleaned'] = df1['total_sqft'].apply(lambda x : clean_total_sqft(x))\n# lets check for abnormal values now :\ncatch_abnormal_val(df1['total_sqft_cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove unecessary columns:\ndf2 = df1.drop(['size','total_sqft'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All columns are cleaned and no nan values remaining.\nNow we can proceed to feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df2.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3['location'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df3['location'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check number of classes in each categorical columns:\ncategorical_cols = df3.select_dtypes(include='object').columns\nfor col in categorical_cols:\n    print(f'Number of classes in {col} : {df2[col].nunique()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new feature for detecting outliers:\ndf3['price_per_sqft'] = df3['price']*100000/df3['total_sqft_cleaned']\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking location statistics:\ndf3['location'] = df3['location'].apply(lambda x: x.strip())\nlocation_stats = df3.groupby('location')['location'].agg('count').sort_values(ascending=False)\nlocation_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will tag the locations which are having very less counts as others."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Locations with less than 10 count:\nlocations_stats_less_than_10 = location_stats[location_stats<=10]\nlocations_stats_less_than_10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3['location'] = df3['location'].apply(lambda x : \"other\" if x in locations_stats_less_than_10 else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3['location'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The unique values have been reduced.\n\nNow we will detect and remove outliers first. Suppose we have a threshold value for per room sqft given by realestate expert. \nUsing this we can find out anomalies in our data and remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Per room sqft threshold be 300sqft: \ndf3 = df3[~(df3.total_sqft_cleaned/df3.BHK < 300)]\ndf3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3['price_per_sqft'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For normal distribution of data, we will keep price values which are near to mean and std.\nOutliers are all above mean+standard_deviation and below mean+standard_deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to remove outliers from price_per_sqft based on locations.\n# As every location will have different price range.\ndef remove_price_outlier(df_in):\n    df_out = pd.DataFrame()\n    for key, subdf in df_in.groupby('location'):\n        avg_price = np.mean(subdf.price_per_sqft)\n        std_price = np.std(subdf.price_per_sqft)\n        # data without outliers: \n        reduced_df = subdf[(subdf.price_per_sqft>(avg_price-std_price)) & (subdf.price_per_sqft<=(avg_price+std_price))]\n        df_out =pd.concat([df_out, reduced_df], ignore_index=True)\n    return df_out\ndf4 = remove_price_outlier(df3)\ndf4.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It was found that in some rows price of 2BHK is very less than 1 BHK. So we  will remove outliers based on BHK for each location. That is we can remove those n BHK apartments whose price_per_sqft is less than mean price_per_sqft of n-1 BHK."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to remove BHK outliers:\ndef remove_bhk_outliers(df_in):\n    exclude_indices = np.array([])\n    for location, location_subdf in df_in.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_subdf in df_in.groupby('BHK'):\n            bhk_stats[bhk] = {\n                'mean':np.mean(bhk_subdf.price_per_sqft),\n                'std':np.std(bhk_subdf.price_per_sqft),\n                'count':bhk_subdf.shape[0]\n            }\n        for bhk, bhk_subdf in location_subdf.groupby('BHK'):\n            stats = bhk_stats.get(bhk-1) #statistics of n-1 BHK\n            if stats and stats['count'] > 5:\n                exclude_indices = np.append(exclude_indices, bhk_subdf[bhk_subdf.price_per_sqft<(stats['mean'])].index.values)\n    return df_in.drop(exclude_indices, axis='index')\n        \ndf5 = remove_bhk_outliers(df4)\ndf5.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize to see number of data points for price_per_sqft\nplt.hist(df5.price_per_sqft, rwidth=0.8)\nplt.xlabel(\"Price Per Sqft.\")\nplt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Data Cleaning section, we encountered that there exist number of bathrooms more than 10.\nLets tackle that in this section."},{"metadata":{"trusted":true},"cell_type":"code","source":"df5[df5.bath>10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize to see data points based on number of bathrooms:\nplt.hist(df5.bath, rwidth=0.8)\nplt.xlabel(\"Number of Bathrooms\")\nplt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can consider the following for bathroom outlier, that **we cannot have (number of bathrooms) more than  (number of bedrooms)+2**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df5[df5.bath > df5.BHK+2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These rows are our outliers for bathrooms."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove bathroom outliers:\ndf6 = df5[df5.bath<df5.BHK+2]\ndf6.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df6.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers removal is done. Now we can remove the extra column \"price_per_sqft\""},{"metadata":{"trusted":true},"cell_type":"code","source":"df7 = df6.drop(['price_per_sqft'], axis=1)\ndf7.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As location variable is an categorical feature, we will create dummy columns for location feature using get dummies function."},{"metadata":{"trusted":true},"cell_type":"code","source":"location_dummies = pd.get_dummies(df7.location)\nlocation_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As this generated binary columns of locations, it is obvious that if any one the row value is 1 then rest are 0. So we will remove one column.\nWhenever there are N classes in a feature, we keep N-1 dummies for it. Here we will drop 'other' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df8 = pd.concat([df7, location_dummies.drop('other', axis='columns')], axis='columns')\ndf8.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Location Column:\ndf9 = df8.drop(['location'], axis='columns')\ndf9.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df9.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature engineering is done. We are now at 9th pipe stage and can proceed further for Prediction model building."},{"metadata":{},"cell_type":"markdown","source":"## Model Building\n### Data Spliting: (Independent: X, Dependent:y) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Independent variables:\nX = df9.drop('price', axis='columns')\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dependent Variable:\ny = df9['price']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Spliting: (train and test)\n(X_train, y_train, X_test, y_test)\n\nWe will keep 20% of sample data for test and rest for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression: \nfrom  sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\nprint(f'Score: {lin_reg.score(X_test, y_test)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-fold validation for Linear Regression:\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\ncv1 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\ncross_val_score(LinearRegression(), X, y, cv=cv1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Selection and Parameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"Using GridSearch lets find out best model among Linear reg, Lasso reg, DecisionTree reg"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_model_grid_search(X, Y, tqdm=tqdm):\n    algos = {\n        'Linear_regression' : {\n            'model' : LinearRegression(),\n            'params': {\n                'normalize':[True, False]\n             }\n          },  \n         'Lasso' : {\n             'model': Lasso(),\n             'params': {\n                  \"max_iter\": [1, 5, 10],\n                 'alpha': [0.02, 0.024, 0.025, 0.026, 0.03, 0.05, 0.5, 1,2],\n                 'selection':['random', 'cyclic'],\n                  'normalize':[True, False]\n             }\n          },\n         'Ridge' : {\n             'model' : Ridge(),\n             'params': {\n                  \"max_iter\": [1, 5, 10],\n                 'alpha': [0.05, 0.1, 0.5, 1, 5, 10, 200, 230, 250,265, 270, 275, 290, 300, 500],\n                  'normalize':[True, False]\n             }\n         },\n        'ElasticNet' : {\n             'model' : ElasticNet(),\n             'params' : {\n                 \"max_iter\": [1, 5, 10],\n                 'alpha': [0, 0.01, 0.02, 0.03, 0.05, 0.5, 1, 0.05, 0.1, 0.5, 1, 5, 10, 100],\n                 'l1_ratio': np.arange(0.0, 1.0, 0.1),\n                 'normalize':[True, False]\n             } \n         },\n          'Decision_tree': {\n              'model': DecisionTreeRegressor(),\n              'params': {\n                  'criterion' : ['mse', 'friedman_mse'],\n                  'splitter': ['best', 'random']\n              }\n          }\n    }\n    values = (algos.items())\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    print(f'Grid Search CV Initiated..' )    \n    with tqdm(total=len(values), file=sys.stdout) as pbar:\n        for algo_name, config in algos.items():\n            pbar.set_description('Processed')\n            gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n            gs.fit(X,Y)\n            scores.append({\n                'model': algo_name,\n                'best_score': gs.best_score_,\n                'best_params': gs.best_params_\n            })\n            pbar.update(1)\n            print(f'Grid search CV for {algo_name} done')\n        print(\"Grid Search CV completed!\")\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = find_best_model_grid_search(X, y)\nmodels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, we can see Ridge is performing well. Lasso and Ridge are actually any other regression model. They are regularization methods of linear regressions.\nTo know more about Lasso and Ridge:https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/ .\nLets collect its best parameters and proceed to re-train our model using these parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge best parameters:\nmodels.loc[3]['best_params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-train using best parameter:\nmodel = Ridge(alpha=0.1, max_iter=1)\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction:\nypred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the test vs predicted data:\nplt.scatter(ypred, y_test)\nplt.title('Actual Price vs Predicted Price (in Lacs)')\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the absolute errors\nerrors = abs(ypred - y_test)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / y_test)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.coef_,X.columns,columns=['Coefficient'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Model is ready!! \n\n## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction Function\ndef predict_price(location, sqft, bath, bhk, data=X):\n    loc_index = np.where(data.columns==location)[0][0]\n    x = np.zeros(len(data.columns)) #init a new array with zero values.\n    x[0] = bath\n    x[1] = bhk\n    x[2] = sqft\n    if loc_index >= 0:\n        x[loc_index] = 1\n    return model.predict([x])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_price('1st Phase JP Nagar',1000,2,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Indira Nagar is most expensive in Bengaluru. Lets predict\npredict_price('Indira Nagar',1000,2,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving ml model as pickle file:\nimport pickle\nwith open('bengaluru_home_price_model.pickle','wb') as f:\n    pickle.dump(model,f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving column names as a json file:\nimport json\ncolumns = {\n    'data_columns' : [col.lower() for col in X.columns]\n}\nwith open('columns.json', 'w') as f:\n    f.write(json.dumps(columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}