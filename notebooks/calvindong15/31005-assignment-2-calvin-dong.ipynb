{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # split the dataframe into training and test datasets\nfrom sklearn.metrics import accuracy_score # Determine how many instances the model has guessed correctly\nfrom sklearn.metrics import classification_report # Get performance measures\nfrom timeit import default_timer as timer # Time how long it takes to perform python operations\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will try to predict UCR part, so we will be using:\nDistrict, month, day and hour \n\nWe will remove Offense code group, offense code description, occured on date, street, lat, long and location "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/crimes-in-boston/crime.csv', encoding=\"latin1\")\n# Picking best columns to be used in training\nbostonCrimes = df[[\"DISTRICT\",\"MONTH\", \"DAY_OF_WEEK\", \"HOUR\", \"UCR_PART\"]]\nbostonCrimes = bostonCrimes.dropna(subset = [\"DISTRICT\",\"MONTH\", \"DAY_OF_WEEK\", \"HOUR\", \"UCR_PART\"])\n\n# Binning values\nmonthBins = [0,4,8,12]\nhourBins = [0,6,12,18,24]\nbostonCrimes[\"MONTH\"] = pd.cut(bostonCrimes[\"MONTH\"], monthBins)\nbostonCrimes[\"HOUR\"] = pd.cut(bostonCrimes[\"HOUR\"], hourBins, include_lowest = True)\nprint(bostonCrimes)\ndfTest = pd.read_csv('../input/gini-test-dataset/test dataset.csv')\nweather = dfTest.drop(columns = [\"Day\"])\nprint(weather)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tree:\n    def __init__(self, data, target):\n        self.rootNode = None\n        self.data = data\n        self.target = target\n        self.maxTreeDepth = self.data.shape[1] - 1 # Max tree depth will be the amount of attributes that are present in the dataset\n        self.predictedData = None\n    \n    def Fit(self, data, target):\n        self.rootNode = TreeNode(data, target, self.maxTreeDepth)\n        self.rootNode.Make()\n        \n    def Predict(self, data, target):\n        dataOrig = data\n        data = data.drop(columns = target)\n        predictedList = pd.DataFrame()\n        decisionMade = False\n        nextNode = False\n        for i in range(len(data)):\n            decisionMade = False\n            nextNode = False\n            instance = data.iloc[[i]] # Get instance to predict\n            currentNode = self.rootNode\n            while not decisionMade:\n                if currentNode.leafNode:\n                    instance.insert(0, f\"Predicted_{target}\",currentNode.prediction , True) # Add prediction decision to instance\n                    frames = [predictedList, instance] \n                    predictedList = pd.concat(frames) # Add instance to dataframe to be returned to user\n                    decisionMade = True\n                    nextNode = True\n                else:\n                    for children in currentNode.children: \n                        if (instance[currentNode.splitAttribute].unique() == children.decision): # If no decision made, find next node in tree to go to \n                            currentNode = children\n                            nextNode = True\n                            break\n                                 \n                if not nextNode: # Exception for when the instance can not find a path through the decision tree\n                    instance.insert(0, f\"Predicted_{target}\", \"Nan\", True) # Add prediction decision to instance\n                    frames = [predictedList, instance] \n                    predictedList = pd.concat(frames) # Add instance to dataframe to be returned to user\n                    decisionMade = True\n                nextNode = False\n        \n        self.predictedData = predictedList\n        print(\"\\naccuracy if the model is: \", end='')\n        print(accuracy_score(dataOrig[self.target], predictedList[f\"Predicted_{target}\"]))\n        print(classification_report(dataOrig[self.target], predictedList[f\"Predicted_{target}\"]))\n        return predictedList\n    \n        \n    def PrintTree(self):\n        unprocessed = [] # A stack to be used for depth first traversal of the tree\n        unprocessed.append(self.rootNode)\n        rootExceptions = []\n        treePrinted = False\n        leafPrinted = False\n        processedLeafs = []\n        \n        print(\"Printing decision tree\")\n        print (f\"Targets are {self.data[self.target].unique()}\")\n        while not (len(unprocessed) == 0):\n            processedString = \"\"\n            tempString = \"\"\n            leafPrinted = False\n            temp = unprocessed[-1]\n            unprocessed.pop()\n            if not temp.leafNode:\n                for children in temp.children:\n                    unprocessed.append(children)\n                        \n            else:\n                currentNode = temp\n                while not leafPrinted: # Travel back up the tree to root node so that branch can be printed\n                    if currentNode.leafNode:\n                        tempString = f\"{currentNode.decision}: {currentNode.prediction}\"\n                        currentNode = currentNode.parent\n                    elif currentNode.parent == None:\n                        tempString = f\"{currentNode.splitAttribute} \"\n                        leafPrinted = True\n                    else:\n                        tempString = f\"{currentNode.decision}\" + \", \" + currentNode.splitAttribute + \" is \"\n                        currentNode = currentNode.parent\n                    processedString = tempString + processedString \n                \n                print(processedString)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class TreeNode:     \n    def __init__(self, data, target, nodeDepth):\n        self.children = []\n        self.parent = None\n        self.leafNode = False\n        self.nodeDepth = nodeDepth # Not a user defined node depth, node depth is used here to determine if all attributes have been split in a branch\n        self.splitAttribute = None\n        self.decision = None\n        self.prediction = None\n        self.data = data\n        self.target = target\n        self.targetValues = self.data[self.target].unique()\n        self.size = self.data.shape[0]\n        #self.minNodeSize = 2\n    \n    def Make(self):\n        lowestGini = 1\n        if (len(self.data[self.target].unique()) == 1 or self.nodeDepth == 0):\n            self.CreateNewLeafNode()\n            return\n\n        for attribute in self.data:\n            if (attribute == self.target):\n                continue\n            else:\n                returnedWeightedGini = self.CalculateGiniIndex(attribute)\n                #print(f\"Weighted gini {attribute} returned a weighted gini of {returnedWeightedGini}\")\n                if (returnedWeightedGini < lowestGini):\n                    lowestGini = returnedWeightedGini\n                    self.splitAttribute = attribute\n        # print(f\"Split attribute will be {self.splitAttribute}, as it returned a weighted gini of {lowestGini}\")\n        \n        # The new split has now been calculated and new nodes formed\n        for classes in self.data[self.splitAttribute].unique():\n            self.children.append(TreeNode(self.data.loc[self.data[self.splitAttribute] == classes], self.target, self.nodeDepth - 1))\n            self.children[-1].decision = classes # Set class for next node to split on\n            self.children[-1].parent = self # make reference to itself as the parent node of the child\n            self.children[-1].Make()\n                \n        # This was code testing performance using minimum node sizes as a stopping criterion        \n        \"\"\"createLeaf = False\n        for uniqueAttVar in self.data[self.splitAttribute].unique():\n            newData = self.data.loc[self.data[self.splitAttribute] == uniqueAttVar]\n            if (newData.shape[0] < self.minNodeSize):\n                createLeaf = True\n             \n            \n        if not createLeaf:\n            for uniqueAttVar in self.data[self.splitAttribute].unique():\n                self.children.append(TreeNode(self.data.loc[self.data[self.splitAttribute] == uniqueAttVar], self.target, self.nodeDepth - 1))\n                self.children[-1].decision = uniqueAttVar # Set decision for next node to split on\n                self.children[-1].Make()\n        else:\n            self.CreateNewLeafNode()\n            return\"\"\"\n    \n    def CalculateGiniIndex(self, attribute):\n        weightedGini = 0\n        for classes in self.data[attribute].unique():\n            giniIndex = 1 # gini index forula is 1 - sum(squared probabilties for each class)\n            sortedCla = self.data.loc[self.data[attribute] == classes] # Get all instances for a unique value in every attribute\n            numOfInst = sortedCla.shape[0]\n            if (numOfInst == 0): # Skip to avoid division by 0\n                continue \n            for targets in self.targetValues:\n                numOfTarInst = sortedCla.loc[sortedCla[self.target] == targets].shape[0] # Find how many instances of each class occurs when looking at a certain target\n                giniIndex = giniIndex - pow(numOfTarInst/numOfInst, 2)\n            \n            #print(f'Gini index of {uniqueAttVar} is {giniIndex}')\n            weightedGini = weightedGini + ((numOfInst/self.size)*giniIndex) # Keep adding calculations for weighted gini for every unique attribute value found\n        #print(f'Weighted gini for {attribute} is {weightedGini}')\n        return weightedGini  \n    \n    def CreateNewLeafNode(self):\n        self.leafNode = True\n        highestCount = 0\n        for uniqueClass in self.data[self.target].unique():\n            numOf = self.data.loc[self.data[self.target] == uniqueClass].shape[0]\n            if (numOf > highestCount):\n                self.prediction = uniqueClass\n        #print(f\"Lead node here for {self.splitAttribute} and prediction is {self.prediction} and decision is {self.decision}\\n\")\n        return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When running the code, an expected warning that will pop up is\n\n> /opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n _warn_prf(average, modifier, msg_start, len(result))\n \n This warning comes from line 48 of the Tree class, and refers to sklearn.metrics.classification_report being unable to calculate the f-score for a classification, as this classification does not exist in one of the datasets provided to the function.  This results in division by 0. \n \n The missing classification here comes from the predicted dataset and is \"Nan\". In this implementation of decision tree, if the prediction function comes across an instance for which it can not predict a classification using the tree, than the instance's predicted classification is set as \"Nan\"."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"if __name__ == \"__main__\":\n    trainingData, testingData = train_test_split(bostonCrimes, train_size = 20000, test_size = 2000) # Boston Crimes dataset contains 317218 instances after processing, \n                                                                                           # making a prediction on this can takes upwards of thirty minutes if all instances are used\n    start = timer() \n    tree = Tree(weather, \"Decision\")# Making tree for weather dataset\n    tree.Fit(weather, \"Decision\")\n    tree.Predict(weather, \"Decision\")\n    tree.PrintTree()\n    end = timer()\n    print(f\"Time to complete: {end - start}\")\n    \n    start = timer()\n    treeBost = Tree(trainingData, \"UCR_PART\") # Making tree for Boston crimes dataset\n    treeBost.Fit(trainingData, \"UCR_PART\")\n    predictedData = treeBost.Predict(testingData, \"UCR_PART\")\n    treeBost.PrintTree()\n    end = timer()\n    print(f\"Time to complete: {end - start}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}