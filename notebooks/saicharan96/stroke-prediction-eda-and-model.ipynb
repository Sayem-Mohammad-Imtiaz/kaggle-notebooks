{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing required libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,accuracy_score,roc_auc_score,confusion_matrix,roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# step 1 : Reading and understanding the data","metadata":{}},{"cell_type":"code","source":"# reading the data and displaying the head of the data\ndf=pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the shape\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking info \ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking five point summary of data\ndf.describe(include = 'all')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# step 2 - checking for duplicates and missing values ","metadata":{}},{"cell_type":"markdown","source":"__Here we are checking each id value to check ther duplicates in the data and we are taking sum of all the boolean values and equating it to zero to check if it is True__","metadata":{}},{"cell_type":"code","source":"# checking for duplicates\nsum(df['id'].duplicated())==0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the null values in DataFrame\nround(df.isnull().sum()*100/len(df),2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__As we can see BMI is the only variable containing null values . lets check this columns for more details__","metadata":{}},{"cell_type":"code","source":"df['bmi'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution plot for BMI\nplt.figure(figsize=[12,8])\nsns.distplot(df['bmi'])\nplt.axvline(df['bmi'].mean(),label='mean',color='r')\nplt.axvline(df['bmi'].median(),label='median',color='g')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__we can see from the above plot that both mean and median are close to each other. so we can replace null values with  median because median is less affected by outliers than mean__","metadata":{}},{"cell_type":"code","source":"# replacing null values with median\ndf.bmi.fillna(28.1,axis=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for null values after null value treatment\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# step 3 : Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"__A) univariate analysis__","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mapping 0 to no and 1 to yes for hyper tension and heart_disease variables\ncols=['hypertension','heart_disease']\nfor col in cols:\n    df[col]=df[col].map({1:'Yes',0:'No'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating numerical and categorical columns in each list\nnum_cols=list(df.select_dtypes(include=np.number).columns)\nprint(num_cols)\ncat_cols=list(df.select_dtypes(include='object').columns)\nprint(cat_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking distribution of numerical columns\nplt.figure(figsize=[15,10])\nfor col in enumerate(num_cols[1:]):\n    plt.subplot(2,2,col[0]+1)\n    sns.distplot(df[col[1]])\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# countplots for categorical columns","metadata":{}},{"cell_type":"code","source":"# checking countplot for categorical columns \nplt.figure(figsize=[20,15])\nfor col in enumerate(cat_cols):\n    plt.subplot(4,2,col[0]+1)\n    sns.countplot(df[col[1]])\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## b) Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"# scatter plot for age vs average_glucose_level\nplt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['avg_glucose_level'],color='cyan')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatter plot for age vs bmi\nplt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['bmi'],color='orange')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatter plot for avg_glucose_level vs bmi\nplt.figure(figsize=[15,10])\nsns.scatterplot(df['avg_glucose_level'],df['bmi'],color='g')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking countplot with stroke for categorical columns \nplt.figure(figsize=[20,18])\nfor col in enumerate(cat_cols):\n    plt.subplot(4,2,col[0]+1)\n    sns.countplot(df[col[1]],hue=df['stroke'])\n    plt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pair plot \nplt.figure(figsize=[20,12])\nsns.pairplot(data=df,hue='stroke')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['avg_glucose_level'],hue=df['stroke'],color='cyan')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[15,10])\nsns.scatterplot(df['age'],df['bmi'],hue=df['stroke'])\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[15,10])\nsns.scatterplot(df['avg_glucose_level'],df['bmi'],hue=df['stroke'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## c)Multivariate analysis","metadata":{}},{"cell_type":"code","source":"# correlation matrix\ndf.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# heatmap\nplt.figure(figsize=[10,6])\nsns.heatmap(df.corr(),cmap='RdYlGn',annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4 : Outlier Treatment","metadata":{}},{"cell_type":"code","source":"# checking for outliers in numerical columns\nplt.figure(figsize=[15,10])\nfor col in enumerate(num_cols[1:-1]):\n    plt.subplot(2,2,col[0]+1)\n    sns.boxplot(df[col[1]])\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__we can see that avg_glucose_level and bmi are having so many outliers so treating them with various preprocessing techniques and iqr iqr capping method__","metadata":{}},{"cell_type":"code","source":"# IQR capping method\n# x = df.describe()\n# for i in num_cols[2:-1]:\n#     q1=x.loc['25%',i]\n#     q3=x.loc['75%',i]\n#     iqr=q3-q1\n#     uppl=q3+(1.5*iqr)\n#     lowl=q1-(1.5*iqr)\n#     df[i]=df[i].apply(lambda x:uppl if x>uppl else x )\n#     df[i]=df[i].apply(lambda x: lowl if x<lowl else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# plt.figure(figsize=[15,10])\n# for col in enumerate(num_cols[1:-1]):\n#     plt.subplot(2,2,col[0]+1)\n#     sns.boxplot(df[col[1]])\n#     plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 5: Dummies Creation","metadata":{}},{"cell_type":"code","source":"x=df.drop(['stroke','id'],axis=1)\ny=df['stroke']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating dummies\nxd=pd.get_dummies(x,drop_first=True)\nxd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the correlation after creating the dummies\nplt.figure(figsize=[20,10])\nsns.heatmap(xd.corr(),annot=True,cmap='RdYlGn')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6 : Train Test Split ","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(xd,y,test_size=0.3,random_state=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the shape of x_train x_test y_train y_test\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_scale=['age','avg_glucose_level','bmi']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating scaler instance\nscaler=MinMaxScaler()\n\n# fit transform for x_train\nx_train[cols_to_scale]=scaler.fit_transform(x_train[cols_to_scale])\n\n# transforming for x_test\nx_test[cols_to_scale]=scaler.transform(x_test[cols_to_scale])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7 : Model building","metadata":{}},{"cell_type":"markdown","source":"## Base model","metadata":{}},{"cell_type":"code","source":"# creating an instance for logistic regression\nlogreg=LogisticRegression(solver='liblinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred=logreg.predict(x_train)\ny_train_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_train,y_train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_prob=logreg.predict_proba(x_train)[:,1]\ny_train_prob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(y_train,y_train_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_train,y_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(12, 8))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr,tpr,thresholds=metrics.roc_curve(y_train_pred,y_train_prob,drop_intermediate=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_roc(y_train,y_train_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred=logreg.predict(x_test)\naccuracy_score(y_test,y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test,y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final model : gradient boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV,StratifiedKFold\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc=GradientBoostingClassifier()\n\nparams={'n_estimators':sp_randint(50,250),'max_depth':sp_randint(1,15),\n        'learning_rate':sp_uniform(0,0.5),'learning_rate':range(0,2)}\n\nr_search=RandomizedSearchCV(estimator=gbc,param_distributions=params,cv=3,n_iter=10,scoring='roc_auc',\n                           random_state=4,n_jobs=-1)\n\nprint(r_search.fit(xd,y))\nprint(r_search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbc=GradientBoostingClassifier(**r_search.best_params_,random_state=4)\ngbc.fit(x_train,y_train)\ny_train_pred=gbc.predict(x_train)\ny_train_prob=gbc.predict_proba(x_train)[:,1]\nprint('train - confusion matrix : ','\\n',confusion_matrix(y_train,y_train_pred))\nprint('train - accuracy score : ','\\n', accuracy_score(y_train,y_train_pred))\nprint('train - AUC : ', roc_auc_score(y_train,y_train_prob))\n\ny_pred=gbc.predict(x_test)\ny_prob=gbc.predict_proba(x_test)[:,1]\nprint('test - confusion matrix : ','\\n',confusion_matrix(y_test,y_pred))\nprint('test - accuracy score : ','\\n', accuracy_score(y_test,y_pred))\nprint('test - AUC : ', roc_auc_score(y_test,y_prob))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_roc(y_train,y_train_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_roc(y_test,y_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}