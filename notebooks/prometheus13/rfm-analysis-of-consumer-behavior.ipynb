{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T13:57:08.363739Z","iopub.execute_input":"2021-05-22T13:57:08.364279Z","iopub.status.idle":"2021-05-22T13:57:08.380147Z","shell.execute_reply.started":"2021-05-22T13:57:08.364248Z","shell.execute_reply":"2021-05-22T13:57:08.379246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **What is customer behavior?** \n\nThe decisions and instincts that make a customer buy a certain product or service can be described as customer behavior.\n\nWith the advent of targeted marketing, traditional marketing techniques are getting obsolete with every new day. The rise of digital marketing, where every customer is shown advertisements particular to their interests and habits, has taken over the world.\n\nThis insight into customer’s interests and habits is obtained through an extensive customer behavior analysis approach. We will try to implement a very basic level of this approach that will include finding the products that are selling more and at which time of the day. Then we will group the customers according to their buying habits.","metadata":{}},{"cell_type":"markdown","source":"# **Why is it important?**\n\nDo you know that the average attention span of a person is at an all-time low? This means that an average advertiser or salesperson has only seven seconds to grasp a customer’s attention before they move to another product as there are so many options available for them to choose from.\n\nA customer will only be interested in your product if they somehow get convinced that it aligns with their interests and habits.\n\n\n\nDo users prefer the products of a specific brand?\n\nWhat is the user’s activity(view, cart, buy) throughout the day?\n\nItems from which brands and categories are most preferred by users?\n\nCan we effectively conduct targeted marketing?","metadata":{}},{"cell_type":"markdown","source":"# Brand analysis \n\nA brand is a term that differentiates one product from another. In this analysis, we will review whether people like to purchase products with a popular brand or a product without a brand.\n\nFor this analysis, only the products actually bought by the users will be considered. In our dataset, the products which have no brand are given a NaN value.\nThis will be done in two steps:\n\nSeparate the original DataFrame into two DataFrames. One with all the products with brands and one with all the products without brands.\n\nFetch all those rows from the two DataFrames where the event_type value is purchase.\n\nAs a final result, two Dataframes will be obtained containing the brand products with and without, that was purchased.","metadata":{}},{"cell_type":"code","source":"#BRAND_ANALYSIS\n\ndf = pd.read_csv('../input/ecommerce-behavior-data-from-multi-category-store/2019-Nov.csv') # Reading the data from file\n\nprint(df.head())","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-22T13:57:13.268416Z","iopub.execute_input":"2021-05-22T13:57:13.269079Z","iopub.status.idle":"2021-05-22T14:01:02.029226Z","shell.execute_reply.started":"2021-05-22T13:57:13.26904Z","shell.execute_reply":"2021-05-22T14:01:02.028145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1\n\n# Fetch rows with brand\nwith_brand = df[df['brand'].notna()]\n\n# Fetch rows without brand\nwithout_brand = df[df['brand'].isna()]\n\n# Step 2\n\n# Purchased products with brands\nwith_brand = with_brand[with_brand['event_type'] == 'purchase']\nprint(with_brand)\n\n# Purchased products without brands\nwithout_brand = without_brand[without_brand['event_type'] == 'purchase']\nprint(without_brand)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:01:16.715896Z","iopub.execute_input":"2021-05-22T14:01:16.716268Z","iopub.status.idle":"2021-05-22T14:01:43.215854Z","shell.execute_reply.started":"2021-05-22T14:01:16.716238Z","shell.execute_reply":"2021-05-22T14:01:43.214845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get length of original dataframe with purchased products\norg = len(df[df['event_type'] == 'purchase'])\n\n# Divide the length of with_brand dataframe with length org dataframe\nbrand_p = len(with_brand) / org\nprint(brand_p * 100)\n\n# Divide the length of without_brand dataframe with length org dataframe\nbrand_a = len(without_brand) / org\n\nprint(brand_a * 100)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:01:55.225645Z","iopub.execute_input":"2021-05-22T14:01:55.226062Z","iopub.status.idle":"2021-05-22T14:02:03.979036Z","shell.execute_reply.started":"2021-05-22T14:01:55.226031Z","shell.execute_reply":"2021-05-22T14:02:03.977938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the above output, approximately 92% of the purchased products were associated with a brand, and only 8% of products without a brand were bought.\n\n# **THE HYPOTHESIS**\n\nA hypothesis can be drawn based on the above results.\n\nFor marketers, most of the marketing budget should be allotted to the advertisement of branded products.\n\nFor inventors or entrepreneurs, always introduce the product with a brand name because products without a brand have a very low probability of getting bought.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Users activity**\n\nAs mentioned in the previous lesson, the user can perform three actions that get recorded in the dataset.\n\nview: The user can view an item.\n\ncart: The user can add the item to the cart.\n\npurchase: The user can purchase the item.\n\nAnalyzing the view and purchasing actions of the user across the different timelines in a month can provide very important information as to at what time most of the users visit the site. When such times are known, resources can be allocated according to that information to optimize performance.\n\nFor example, if we know that a significant amount of users visit the site on Sunday just to view the products, resources from other components can be transferred to viewing components to enhance the user experience. Similarly, the same approach can be used on other components if we know at what times certain, user activity is preferred.\n\nLet’s apply this approach to our data and review what analysis can be drawn from it.","metadata":{}},{"cell_type":"markdown","source":"# **Preprocessing** \n\nBefore we move to extract information, some preprocessing needs to be done on our initial DataFrame. The time values are separated from the event_time column and are made into separate columns. The day, week_day, and hour are computed for each event_time value.","metadata":{}},{"cell_type":"code","source":"\n#Convert the type of event_time column to datetime\ndf['event_time'] = pd.to_datetime(df.event_time)\n\n# Calculate and add relevant columns to track users activity\ndf[\"week_day\"] = df['event_time'].map(lambda x: x.dayofweek + 1)\ndf[\"day\"] = df['event_time'].map(lambda x: x.day)\ndf[\"hour\"] = df['event_time'].map(lambda x: x.hour)\n\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:02:13.627077Z","iopub.execute_input":"2021-05-22T14:02:13.627425Z","iopub.status.idle":"2021-05-22T14:09:16.230349Z","shell.execute_reply.started":"2021-05-22T14:02:13.627396Z","shell.execute_reply":"2021-05-22T14:09:16.229312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Weekly analysis**\n\nIn this part, we will review a weekly analysis of the number of views. This will reveal the day of the week on which the most or least number of views occur for the website.","metadata":{}},{"cell_type":"code","source":"\n# Get all the view events of all users\nviewed = df[df['event_type'] == 'view']\n\n# Plot the number views against all week days in a line chart\nview_plot = viewed.groupby('event_type')['week_day'].value_counts().sort_index().plot(kind = 'line', figsize = (15,6))\n\n# Set properties of the plot\nview_plot.set_xlabel('Day of the week',fontsize = 15)\nview_plot.set_ylabel('Number of Views',fontsize = 15)\nview_plot.set_title('Number of views for different Week Days',fontsize = 15)\nview_plot.set_xticklabels(('Mon','Tue','Wed','Thur','Fri', 'Sat','Sun'), rotation = 'horizontal', fontsize = 15)\n\n#plot the graph\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:36:53.523558Z","iopub.execute_input":"2021-05-22T14:36:53.524052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*In the above graph, it can be observed that most items are viewed during the working days instead of on the weekends. This represents the aggregated number of website views for all the weekdays of October 2019.*","metadata":{}},{"cell_type":"markdown","source":"# **Hourly analysis**\n\nIn this part, an hourly analysis of the number of views will be created. This will reveal at which hour of the day the most and least number of views occur for the website.","metadata":{}},{"cell_type":"code","source":"#Convert the type of event_time column to datetime\ndf['event_time'] = pd.to_datetime(df.event_time)\n\ndf[\"week_day\"] = df['event_time'].map(lambda x: x.dayofweek + 1)\ndf[\"day\"] = df['event_time'].map(lambda x: x.day)\ndf[\"hour\"] = df['event_time'].map(lambda x: x.hour)\n\n# Get all the view events of all users\nviewed = df[df['event_type'] == 'view']\n\n# Plot the number views against all 24 hours of the days in a bar chart\nview_plot = viewed.groupby('event_type')['hour'].value_counts().sort_index().plot(kind = 'bar', figsize = (15,6))\n\n# Set properties of the plot\nview_plot.set_xlabel('Hour',fontsize = 15)\nview_plot.set_ylabel('Number of Views',fontsize = 15)\nview_plot.set_title('Number of views for different Hours of Days',fontsize = 15)\nview_plot.set_xticklabels(range(1,32), rotation='horizontal', fontsize=15)\n\n#plot the graph\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above graph, it can be observed that most items are viewed in the working hours instead of the free hours. The number of views starts increasing from the start of the day, reaching their peak between 3 and 5 P.M. Then it starts to drop. This is the combined result for each day of October 2019.\n\nThe code is exactly the same for the weekly analysis. On line 16, just the week_day parameter is changed using the hour parameter, and some properties are renamed according to the new analysis.\n\n# **The hypothesis**\n\nFrom the above weekly and hourly analysis, it can be observed that most of the users like to browse the items during working hours of working days. Other time slots are also important but at these time slots, most resources should be allocated to the viewing or browsing component of the website to optimize and enhance user experience which in turn brings profit.\n\nTry doing the same weekly and hourly analysis for the number of products purchased to determine whether the view results hold for the purchase part or not.\n\nThe most common problem faced by any business is inventory management. Sometimes business owners either have too much of a product that is not being sold or too little of a product whose demand is very high. This can cause a substantial loss to a company’s profits and reputation. For more information on this problem, refer here.\n\nIf we somehow know what products from which brands and categories are selling the most in the market, then inventory management can be optimized to some level. Here, products from which brand and category were bought the most will be determined.\n\nTop brands #\nFirst, the data will be read and the event_time column will be converted to DateTime format. Then, the following steps will be performed to obtain the top brands.","metadata":{}},{"cell_type":"markdown","source":"# Identifying Famous Brands and Categories\nThe most common problem faced by any business is inventory management. Sometimes business owners either have too much of a product that is not being sold or too little of a product whose demand is very high. This can cause a substantial loss to a company’s profits and reputation. For more information on this problem, refer here.\n\nIf we somehow know what products from which brands and categories are selling the most in the market, then inventory management can be optimized to some level. Here, products from which brand and category were bought the most will be determined.","metadata":{}},{"cell_type":"markdown","source":"# Top brands \n\nFirst, the data will be read and the event_time column will be converted to DateTime format. Then, the following steps will be performed to obtain the top brands.","metadata":{}},{"cell_type":"code","source":"\n# Get rows where products are purchased\npurchase = df[df['event_type'] == 'purchase']\n\n# Group the DataFrame on brands\ntop_brands = purchase.groupby('brand')\n\n# Get number of products bought by computing length of each grouped brand\ntop_brands = top_brands['brand'].agg([len])\n\n# Sort the result on obtained length in descending order\ntop_brands.sort_values('len', ascending = False, inplace = True)\n\nprint(top_brands)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*According to this, Samsung is the most famous brand whose products are being bought in excessive quantities.*","metadata":{}},{"cell_type":"markdown","source":"# Top categories \nThe same steps as above will be performed here, but instead of the brand column, the category_code column will be used.","metadata":{}},{"cell_type":"code","source":"\n\n# Get rows where products are purchased\npurchase = df[df['event_type'] == 'purchase']\n\n# Group the DataFrame on category_code\ntop_catg = purchase.groupby('category_code')\n\n# Get number of products bought by computing length of each grouped category_code\ntop_catg = top_catg['category_code'].agg([len])\n\n# Sort the result on obtained length in descending order\ntop_catg.sort_values('len', ascending = False, inplace = True)\n\nprint(top_catg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same technique and codes to find the top brands are used to get the top categories. Only the brand column is replaced with the category_code column.\n\nAccording to the above output, the smartphone category is the most famous among others. The difference in the number of products bought for other categories is clearly visible.\n\n# **The hypothesis** \n\nAccording to the above analysis, the top brands all include mobile and mobile accessory companies. The top category is the smartphone category, which has over 300,000+ sales, and the other categories don’t even come close to this number. It can be concluded that all products that come under the smartphone category should be in abundance in the inventory with only the top five or six top brands.","metadata":{}},{"cell_type":"markdown","source":"# RFM analysis \n\nRFM is a categorizing technique that uses the previous purchasing behavior of the customers to divide customers into groups so that an optimal marketing strategy can be developed for each individual. RFM stands for recency, frequency, and monetary, respectively.\n\nRecency: How many days have passed since a customer has bought an item\n\nFrequency: How many orders a customer has placed\n\nMonetary: How much money a customer has spent\n\n# Need for RFM analysis \n\nThis technique efficiently categorizes the customers into specific rank-based groups taking into account their past online behaviors.\n\nThis can help marketers and advertisers target each group of consumers separately, enabling them to cater to the needs of groups instead of each individual.\n\nThis technique also informs us of the most and least profit yielding customers so relevant resources can be deployed to each group according to their needs.\n\nIf the results of this technique are correctly used, then even customers who don’t engage in much activity(view, cart, buy) can be influenced to be high potential customers.\n\nRFM technique and steps to perform #\nIn this process, the customers are separated into four groups under each of the RFM metrics, i.e., recency, frequency, and monetary. This means we’ll have a maximum of (4 x 4 x 4) sixty-four groups to deal with, which is not very large considering that the total number of customers can be in the thousands. Quantiles will be used to divide the customers into groups.\n\nThe following steps will be performed to get the final list of segmented customers.\n\n**Step 1: Get the purchase data of all customers.**","metadata":{}},{"cell_type":"code","source":"# Get rows with event_type equals purchase\npurchased = df[df['event_type'] == 'purchase']\n\n# Filter relevant data from Data Frame\npurchased = purchased[['user_id', 'user_session', 'event_time' ,'price']]\n\nprint(purchased)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can tell which customer placed how many orders of what price at what time. As we have filtered out all purchase event types from the data set, the number of user_session against a single user_id gives the number of orders an individual user placed. A user_session against a particular user_id could be repeated as a user might have made multiple purchases during a single visit, and a user_session could be different as well.\n\n**Step 2: Compute the RFM metrics for each customer.**","metadata":{}},{"cell_type":"code","source":"# Compute the R, F, and M values for each user\nrfm = purchased.groupby('user_id').agg({'event_time': lambda date: ((purchased['event_time'].max()) - date.max()),\n                                    'user_session': lambda num: num.count(),\n                                    'price': lambda price: price.sum()})\nprint(rfm)\n\n\nrfm['event_time'] = rfm['event_time'].apply(lambda days: int(str(days).split(' days')[0]) + 1)\n\n\nrfm.columns=['recency','frequency','monetary']\n\nprint(rfm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 3: Compute ranks for each RFM metric using quantiles.**\n\nNow that we have the correct R, F, and M values, it’s time to rank them. It should be noted that in the case of recency, the lower the value the better, but for frequency and monetary, the higher the value the better. So, recency is inverse of frequency and monetary.\n\nAs mentioned above, each of the RFM values needs to be divided into four groups, so quantiles are used to categorize the R, F, and M values into correct groups. You can refresh the quantiles function here.\n\nThe 1st, 2nd, and 3rd quantiles of the recency, frequency, and monetary columns are calculated from the rfm DataFrame and then converted to dictionary objects for easy access.\n\nquantiles = rfm.quantile(q=[0.25,0.50,0.75])\n\nquantiles = quantiles.to_dict()\n\nprint(quantiles)\n\n\nNow, for each R, F, and M metric in the rfm DataFrame, their values will be compared with their quantile values and will be assigned a rank between 1 and 4 based on the comparison. Here, 1 indicates the highest rank and 4 indicates the lowest rank.\n\nThe following functions will compute ranks for the R, F, and M values in the rfm DataFrame.","metadata":{}},{"cell_type":"code","source":"# Compute Ranks for Recency metric\ndef Compute_R(val,metric,quantile):\n    if val <= quantile[metric][0.25]:\n        return 1\n    elif val <= quantile[metric][0.50]:\n        return 2\n    elif val <= quantile[metric][0.75]: \n        return 3\n    else:\n        return 4\n    \n# Compute Ranks for Frequency & Monetary metrics\ndef Compute_FM(val,metric,quantile):\n    if val <= quantile[metric][0.25]:\n        return 4\n    elif val <= quantile[metric][0.50]:\n        return 3\n    elif val <= quantile[metric][0.75]: \n        return 2\n    else:\n        return 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both functions in the above code snippet have the same parameters and return values and are only different in their comparisons. As low values are better for recency and high values are better for frequency and monetary, two functions are created.\n\nThe val parameter is the value of the R, F, or M metric from the rfm DataFrame that is compared by their respective quantile values.\n\nThe metric parameter can be recency, frequency or monetary and is used to access the correct quantile value from the quantiles dictionary.\n\nThe quantile parameter is the calculated quantiles dictionary which contains the 1st, 2nd, and 3rd quantiles of the R, F, and M metrics from the rfm DataFrame.\n\nThe above functions compare the input values with the quantile values of the respective metrics and return ranks based on the mentioned conditions. For recency, the lower the value the higher the rank. For frequency and monetary, the higher the value the higher the rank.","metadata":{}},{"cell_type":"code","source":"# Compute new column with recency rank of that row\nrfm['R_rank'] = rfm_new['recency'].apply(Compute_R, args=('recency',quartiles))\n\n# Compute new column with frequency rank of that row\nrfm['F_rank'] = rfm_new['frequency'].apply(Compute_FM, args=('frequency',quartiles))\n\n# Compute new column with monetary rank of that row\nrfm['M_rank'] = rfm_new['monetary'].apply(Compute_FM, args=('monetary',quartiles))\n\nprint(rfm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three new rank columns are created for the R, F, and M metrics. The apply() function is used on each of the respective columns of the rfm DataFrame. The Compute_R() function is used for recency, and the Compute_FM() function is used for frequency and monetary. The second and third parameters of the functions are placed in the args parameter of the apply() function.\n\nThe above output displays the new resultant DataFrame with ranks of each of the RFM metrics.\n\n**Step 4: Combine the RFM values to obtain a combined RFM Score.**\n\nNow, the individual R, F, and M metrics are combined to generate the RFM score which is then added as a column in our rfm dataframe.","metadata":{}},{"cell_type":"code","source":"# Convert RFM values to type string\nR = rfm.R_rank.astype(str)\nF = rfm.F_rank.astype(str)\nM = rfm.M_rank.astype(str)\n\n# Compute new colum with combined RFM values\nrfm['RFM_Score'] = R + F + M\n\nprint(rfm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 5: Sort the final DataFrame in ascending order.**\n\nNow, the final DataFrame is sorted in ascending order according to the RFM Score to get customer groups from best to worst.","metadata":{}},{"cell_type":"code","source":"# Sort the DataFrame by RFM_Socre values\nrfm = rfm.sort_values('RFM_Score')\n\nprint(rfm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table below, the RFM score is mentioned with the corresponding customer group, what it means to have that RFM score, and what type of marketing strategy can be developed to deal with that group of customers. You should keep in mind that these groups are not a standard and can be adjusted to one’s requirement or problem.\n\nCustomer Type                        RFM Score             Explanation                       Marketing Strategy\nBest Customers                         111         Bought most recently and frequently,\n                                                      and spends the most money             Introduce new products\n                                                 \nCurrent Custome                        1XX             Bought most recently             Upsell products related to current                                                                                                     purchase\nLoyal Customers                        X1X            Bought most frequently        Use R and M metrics to further segment\n\nBig Spenders                           XX1            Spends the most money                   Suggest costly products\n\nAbsent Customers                       411       Purchased frequently and spent the most      Suggest products with                                                                                                             discounts\nAbsent Common Customers                444       Purchased long ago, purchased few                                                                                                 and spent little                    Pay least amount of attention\n\nThe X in the RFM score indicates that any value can occur, and it does not affect the result as long as the 1 stays in its position.\n\nWe just reduced the marketing load from managing thousands of customers to managing only sixt-four groups. Resources of the website can be allocated in the best possible way if the above information is correctly calculated and used.","metadata":{}}]}