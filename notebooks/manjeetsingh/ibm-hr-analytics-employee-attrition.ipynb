{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import preprocessing\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the dataset\ndf= pd.read_csv('../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint(\"The dataset has %d rows and %d columns.\" % df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Check Missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = df.isnull().sum() * 100 / len(df)\npercent_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Attrition.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target variable is imbalanced. We can try to use sampling to fix this but we will do this later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['Age'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['DailyRate'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['DistanceFromHome'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['Education'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['EnvironmentSatisfaction'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['HourlyRate'], hue=df['Attrition'])\nfig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nsns.countplot(df['Age'], hue=df['Attrition'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"------  Data Types  ----- \\n\",df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ncorr = df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr,cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop few variables which doesnt look helpful\ndf = df.drop(['EmployeeCount','EmployeeNumber'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One hot encoding to fix the variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset =  df.drop(['OverTime','MaritalStatus','JobRole','Gender','Department','EducationField','BusinessTravel','Over18'], axis=1)\nBusinessTravel = pd.get_dummies(df.BusinessTravel).iloc[:,1:]\nDepartment = pd.get_dummies(df.Department).iloc[:,1:]\nOverTime = pd.get_dummies(df.OverTime).iloc[:,1:]\nMaritalStatus = pd.get_dummies(df.MaritalStatus).iloc[:,1:]\nJobRole = pd.get_dummies(df.JobRole).iloc[:,1:]\nGender = pd.get_dummies(df.Gender).iloc[:,1:]\nEducationField = pd.get_dummies(df.EducationField).iloc[:,1:]\nOver18 = pd.get_dummies(df.Over18).iloc[:,1:]\ndataset = pd.concat([dataset,Over18,BusinessTravel,Department,OverTime,MaritalStatus,JobRole,Gender,], axis=1)\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"------  Data Types  ----- \\n\",dataset.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =  dataset.drop(['Attrition'], axis=1)\ny = dataset['Attrition']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RANDOM Forest Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10,random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier =  RandomForestClassifier(n_estimators = 400,random_state = 42)\nclassifier.fit(X_train, y_train)  \npredictions = classifier.predict(X_test)\nfrom sklearn.metrics import classification_report, accuracy_score\nprint(classification_report(y_test,predictions ))  \nprint(accuracy_score(y_test, predictions ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axis1) = plt.subplots(1,1,figsize=(15,10))\nfeat_importances = pd.Series(classifier.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBOOST Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 70% training and 30% test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nxgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.08, objective= 'binary:logistic',n_jobs=-1).fit(X_train, y_train)\nprint('Accuracy of XGB classifier on training set: {:.2f}'\n       .format(xgb_model.score(X_train, y_train)))\nprint('Accuracy of XGB classifier on test set: {:.2f}'\n       .format(xgb_model.score(X_test[X_train.columns], y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = xgb_model.predict(X_test)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=45)  # 80% training and 20% test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)\ny_pred = gb.predict(X_test)#Import scikit-learn metrics module for accuracy calculation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Compare all the models we build","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nclassifier.fit(X_train, y_train)\n\nrf_predict_probabilities = classifier.predict_proba(X_test)[:,1]\ngb_predict_probabilities = gb.predict_proba(X_test)[:,1]\ny_predict_xgb = xgb_model.predict_proba(X_test)[:,1]\n\n\ngb_fpr, gb_tpr, _ = roc_curve(y_test, gb_predict_probabilities)\ngb_roc_auc = auc(gb_fpr, gb_tpr)\n\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_predict_probabilities)\nrf_roc_auc = auc(rf_fpr, rf_tpr)\n\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, y_predict_xgb)\nxgb_roc_auc = auc(xgb_fpr, xgb_tpr)\n\n\nplt.figure()\nplt.plot(gb_fpr, gb_tpr, color='darkorange',\n         lw=2, label='random Forrest (area = %0.2f)' % gb_roc_auc)\nplt.plot(rf_fpr, rf_tpr, color='darkgreen',\n         lw=2, label='Gradient Boosting (area = %0.2f)' % rf_roc_auc)\nplt.plot(xgb_fpr, xgb_tpr, color='black',\n         lw=2, label='XGBoost (area = %0.2f)' % xgb_roc_auc)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_for_submission = xgb_model.predict(X_test).astype(int)\npred_for_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_probs = xgb_model.predict_proba(X_test)\nxgb_probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"XGBoost is clearly outperforming Random Forest. But none of these models are really good.  It can be imporved by hyper tuning the parametere or by doing resampling by SMOTE. I will add that later","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}