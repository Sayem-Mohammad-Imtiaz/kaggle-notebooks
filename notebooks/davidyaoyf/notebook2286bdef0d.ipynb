{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Calculates cost with regularization </h2>\n<h2> Note that $ \\theta_{0} $ is not included in the calculation of cost for regularization in the second summation</h2>\n<h1> $$ \\frac{1}{2m}( \\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})^2) + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$ </h1>\n<h3> where m = number of training examples, $y^{(i)}$ and $x^{(i)}$ are training example i, $\\lambda$ is regularization constant </h3>\n<h3> $$h_{\\theta}(x^{(i)}) = \\theta_{0}+\\theta_{1}x^{(i)}_{1}+...+\\theta_{n}x^{(i)}_{n}$$ </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_cost(theta, X, y, lbd): # theta is dimensions n x 1, X is dimensions m x n, y is dimensions m x 1\n    m = X.shape[0]\n    cost = (1/(2*m))*(np.square((np.matmul(X,theta.transpose()) - y))).sum() + (lbd/(2*m)) * np.square(theta).sum() # vectorized implementation of mean squared error as cost function\n    cost -= (lbd/(2*m)) * theta[0] ** 2 # theta zero is not regularized\n    return cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Vectorized implementation gradient calculation together with regularization </h2>\n<h2> Note that for $\\theta_{0}$, regularization is not needed </h2>\n<h1> $$ \\frac{\\partial J}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m} \\theta_{j}$$ </h1>\n<h3> for j = 1,2,...,n where n is the number of weights theta, and m is the number of training examples </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_grad(theta, X, y, lbd): # theta is dimensions n x 1, X is dimensions m x n, y is dimensions m x 1\n    m = X.shape[0]\n    grad = (1/m)*(np.matmul(np.matmul(X,theta.transpose()) - y,X) + lbd*theta) # vectorized implementation of gradient\n    grad[0] -= (1/m)*lbd*theta[0] # theta_zero is not regularized\n    return grad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_reg(X_train, y_train, alpha, lbd, iterations, X_test=None, y_test=None): # X is dimensions m x n, y is dimensions m x 1, alpha is learning rate\n    theta = np.random.rand(X_train.shape[1]) # randomly initiates weights\n    m = X_train.shape[0]\n    costs_train = []\n    costs_test = []\n    for i in range(iterations):\n        costs_train.append(calculate_cost(theta, X_train, y_train, lbd))\n        theta -= alpha * calculate_grad(theta, X_train ,y_train, lbd)\n        if (X_test is not None and y_test is not None):\n            costs_test.append(calculate_cost(theta, X_test, y_test, lbd))\n    x_graph = np.arange(0,iterations,1);    \n    plt.plot(x_graph,costs_train, label='train') \n    if (X_test is not None and y_test is not None):\n        plt.plot(x_graph,costs_test, label='test')\n    plt.legend()\n    return theta  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X,theta):\n    return np.matmul(X,theta.transpose())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cost(X, y, theta):\n    pred = predict(X,theta)\n    plt.scatter(X[:,1],y)\n    plt.plot(X[:,1],pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(X, mean, std):\n    return (X-mean) / std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Applying linear regression model on data from kaggle </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/real-estate-price-prediction/Real estate.csv\",index_col=None)\ndisplay(df)\ndisplay(df.isnull().sum()) # check for nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=df.sample(frac=0.9,random_state=0) #random state is a seed value\ntest=df.drop(train.index)\n\ntrain_x = train.loc[:,df.columns != \"Y house price of unit area\"]\ntrain_y = train.loc[:,\"Y house price of unit area\"]\ntest_x = test.loc[:,df.columns != \"Y house price of unit area\"]\ntest_y = test.loc[:,\"Y house price of unit area\"]\n\ntrain_mean = train_x.mean(axis=0) # mean normalization\ntrain_std = train_x.std(axis=0)\ntrain_x = normalize(train_x,train_mean ,train_std)\ntest_x = normalize(test_x,train_mean ,train_std )\n\ntrain_x.insert(0, 'One', 1) # adding column of ones for theta that is independent of features\ntest_x.insert(0, 'One', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"theta = linear_reg(train_x.values, train_y.values, 0.05, 0.001, 200, test_x.values, test_y.values)\ntheta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = predict(test_x.values, theta)\nprint(mean_absolute_error(pred,test_y))\nprint(max(test_y) - min(test_y))\nprint('r2 Score : ', r2_score(test_y, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Using sklearn linear regression library </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit (train_x.values, train_y.values)\ny_pred = reg.predict(test_x)\nprint('r2 Score : ', r2_score(test_y, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}