{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Decision Tree Terminology](#1) \n\n[CART - Classification and Regression Trees](#2)\n\n[Entropy](#3)\n\n[Information Gain](#4)\n\n[Gini index](#5)\n\n[Pruning](#6)\n\n[Importing Libraries and Dataset](#7)\n\n[Exploratory data analysis ](#8)\n\n[Separating Features and Target](#9)\n\n[Splitting Dataset to training and test data](#10)\n\n[Decision Tree Creation](#11)\n\n[ Creation of Decision Tree using Gini Index](#12)\n\n[Creation of Decision Tree using with entropy](#13)\n\n[Confusion Matrix](#14)\n\n[Conclusion](#15)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 1. Decision Tree Terminology","metadata":{}},{"cell_type":"markdown","source":"A Decision Tree learning is a predictive modeling approach. It is used to address classification problems in statistics, data mining, and machine learning.","metadata":{}},{"cell_type":"markdown","source":"It is having a tree-like structure upside down and represents decisions or for decision-making. It can handle high dimension data and have good accuracy.\n\nThe topmost node is called the root node which has no incoming edges. An internal node represents a test or an attribute and each branch represents an outcome of a test and each terminal node or leaf holds a class. It has one incoming edge and has two or more outgoing edges. Terminal node or Leaf node represents a class node and has exactly one incoming node and no outgoing node.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# 2. CART - Classification and Regression Trees","metadata":{}},{"cell_type":"markdown","source":"Tree analogy is generally represented by CART known as Classification And Regression Tree. CART is simple to understand, interpret, visualize and requires little effort for data preparation. Moreover, it performs feature selection. Regression trees are mainly used when the target variable is numerical. Here value obtained by a terminal node is always the mean or average of the responses falling in that region. As a result, if any unseen data or observation will predict with the mean value. Classification is used when the target variable is categorical. Here value obtained by a terminal node is the mode of response falling in that region and any unseen data or observation in this region will make a prediction based on the mode value.\nEven though CART is simple and has great advantages, but it can lead to overfitting if data is not properly handled. Moreover, it can lead to instability, if there is a small variation in data.","metadata":{}},{"cell_type":"markdown","source":"While growing a tree below points are to be considered :\n\n* Features to choose\n* Conditions for splitting \n* To know where to stop\n* Pruning\n\n\nThe decision to make a strategic split heavily affects the accuracy of the tree and the decision criteria for regression and classification trees will be different. Entropy/Information gain or Gini Index can be used for choosing the best split. Entropy and Information gain go hand in hand.\n\n\nFor a given dataset with different features, to decide which feature to be considered as the root node and which feature should be the next decision node and so on, information gain of each feature should be known. The feature which has maximum information gain will be considered as the root node. To calculate information gain first we should calculate the entropy.\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# 3. Entropy ","metadata":{}},{"cell_type":"markdown","source":"Entropy is a measure of disorder or impurity in the given dataset. In the decision tree, messy data are split based on values of the feature vector associated with each data point. With each split, the data becomes more homogenous which will decrease the entropy. However, some data in some nodes will not be homogenous, where the entropy value will not be small. The higher the entropy, the harder it is to draw any conclusion. When the tree finally reaches the terminal or leaf node maximum purity is added.\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# 4. Information Gain","metadata":{}},{"cell_type":"markdown","source":"The Information Gain measures the expected reduction in entropy. Entropy measures impurity in the data and information gain measures reduction in impurity in the data. The feature which has minimum impurity will be considered as the root node. \n\nInformation gain is used to decide which feature to split on at each step in building the tree. The creation of sub-nodes increases the homogeneity, that is decreases the entropy of these nodes. The more the child node is homogeneous, the more the variance will be decreased after each split. Thus Information Gain is the variance reduction and can calculate by how much the variance decreases after each split.\n\nInformation gain of a parent node can be calculated as the entropy of the parent node subtracted entropy of the weighted average of the child node.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# 5. Gini index","metadata":{}},{"cell_type":"markdown","source":"The Gini index can also be used for feature selection. The tree chooses the feature that minimizes the Gini impurity index. The higher value of the Gini Index indicates the impurity is higher. Both Gini Index and Gini Impurity are used interchangeably. The Gini Index or Gini Impurity favors large partitions and is very simple to implement. It performs only binary split. For categorical variables, it gives the results in terms of \"success\" or \"failure\".","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n# 6. Pruning","metadata":{}},{"cell_type":"markdown","source":"When the tree is fully grown up, it is liking to overfit data due to noise or outliers which can lead to anomalies in decision trees. Which in turn leads to poor accuracy. This can be handled by using pruning.\nPruning is the process of removing redundant comparisons or removing subtrees. Pruning reduces unnecessary comparisons and achieves better performance. Pruned trees are less complex, smaller, and easy to understand. There are two approaches for pruning, the pre-pruning approach in which splitting or partition of the tree is halted at a particular node whereas in post-pruning approach removes subtree from the full tree. A subtree is pruned at a node. It is done by removing the branches at a node and replacing it with a leaf node.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n# 7. Importing Libraries and Dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:17.580266Z","iopub.execute_input":"2021-08-26T18:33:17.58068Z","iopub.status.idle":"2021-08-26T18:33:17.595528Z","shell.execute_reply.started":"2021-08-26T18:33:17.580646Z","shell.execute_reply":"2021-08-26T18:33:17.594135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing libraries matplotlib and seaborn","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn  as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:17.597497Z","iopub.execute_input":"2021-08-26T18:33:17.597971Z","iopub.status.idle":"2021-08-26T18:33:18.656725Z","shell.execute_reply.started":"2021-08-26T18:33:17.597921Z","shell.execute_reply":"2021-08-26T18:33:18.65562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing data set","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/mushroom-classification/mushrooms.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:18.658727Z","iopub.execute_input":"2021-08-26T18:33:18.659057Z","iopub.status.idle":"2021-08-26T18:33:18.701381Z","shell.execute_reply.started":"2021-08-26T18:33:18.659027Z","shell.execute_reply":"2021-08-26T18:33:18.700395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n# 8. Exploratory data analysis ","metadata":{}},{"cell_type":"code","source":"#To see the first five rows of the dataset we can use dataset.head()\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:18.703141Z","iopub.execute_input":"2021-08-26T18:33:18.703573Z","iopub.status.idle":"2021-08-26T18:33:18.750497Z","shell.execute_reply.started":"2021-08-26T18:33:18.703535Z","shell.execute_reply":"2021-08-26T18:33:18.749222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['class'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:18.752373Z","iopub.execute_input":"2021-08-26T18:33:18.752822Z","iopub.status.idle":"2021-08-26T18:33:18.764603Z","shell.execute_reply.started":"2021-08-26T18:33:18.752773Z","shell.execute_reply":"2021-08-26T18:33:18.763262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The class column is target and it has two clasification which describes if mushroom is poisonous or edible. In class column posionous is p and edible is e.","metadata":{}},{"cell_type":"code","source":"# To see if there is any null values in the dataset\ndataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:18.766214Z","iopub.execute_input":"2021-08-26T18:33:18.766671Z","iopub.status.idle":"2021-08-26T18:33:18.811247Z","shell.execute_reply.started":"2021-08-26T18:33:18.766634Z","shell.execute_reply":"2021-08-26T18:33:18.809227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the features are categorical and there is no missing value.","metadata":{}},{"cell_type":"code","source":"#To find number of rows and column\ndataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:33:18.812601Z","iopub.execute_input":"2021-08-26T18:33:18.812916Z","iopub.status.idle":"2021-08-26T18:33:18.818827Z","shell.execute_reply.started":"2021-08-26T18:33:18.812878Z","shell.execute_reply":"2021-08-26T18:33:18.817403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(dataset['class'])","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:36:17.60978Z","iopub.execute_input":"2021-08-26T18:36:17.61039Z","iopub.status.idle":"2021-08-26T18:36:17.962568Z","shell.execute_reply.started":"2021-08-26T18:36:17.610329Z","shell.execute_reply":"2021-08-26T18:36:17.96149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n# 9. Separating Features and Target","metadata":{}},{"cell_type":"markdown","source":"Target is in column class. So X will have all values apart from column class and y will have column class","metadata":{}},{"cell_type":"code","source":"X = dataset.drop(['class'],axis=1)\ny = dataset['class']","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:24:15.743934Z","iopub.execute_input":"2021-08-26T18:24:15.744293Z","iopub.status.idle":"2021-08-26T18:24:15.751229Z","shell.execute_reply.started":"2021-08-26T18:24:15.744262Z","shell.execute_reply":"2021-08-26T18:24:15.750192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As all the values in the dataset are categorical.\nX can be encoded using pandas dummy variable and y using LabelEncoder.\n\nDummy variable creates a separte column for each unique value of the column, where as LabelEncoder encodes target labels with value between 0 and n_classes-1. LabelEncoder should be used to encode target values, i.e. y, and not the input X.","metadata":{}},{"cell_type":"code","source":"X = pd.get_dummies(X)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:11.776383Z","iopub.execute_input":"2021-08-26T18:04:11.77666Z","iopub.status.idle":"2021-08-26T18:04:11.834564Z","shell.execute_reply.started":"2021-08-26T18:04:11.776633Z","shell.execute_reply":"2021-08-26T18:04:11.833514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:11.835705Z","iopub.execute_input":"2021-08-26T18:04:11.83595Z","iopub.status.idle":"2021-08-26T18:04:11.968055Z","shell.execute_reply.started":"2021-08-26T18:04:11.835927Z","shell.execute_reply":"2021-08-26T18:04:11.967075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For y variable encoding is done as\nPoisonous = p -> 1\nEdible = e -> 0","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n# 10. Splitting Dataset to training and test data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:11.969201Z","iopub.execute_input":"2021-08-26T18:04:11.969468Z","iopub.status.idle":"2021-08-26T18:04:12.035408Z","shell.execute_reply.started":"2021-08-26T18:04:11.969441Z","shell.execute_reply":"2021-08-26T18:04:12.034472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape , X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:12.037672Z","iopub.execute_input":"2021-08-26T18:04:12.037976Z","iopub.status.idle":"2021-08-26T18:04:12.044312Z","shell.execute_reply.started":"2021-08-26T18:04:12.037934Z","shell.execute_reply":"2021-08-26T18:04:12.043406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape , y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:12.045681Z","iopub.execute_input":"2021-08-26T18:04:12.046078Z","iopub.status.idle":"2021-08-26T18:04:12.055462Z","shell.execute_reply.started":"2021-08-26T18:04:12.046039Z","shell.execute_reply":"2021-08-26T18:04:12.05443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n# 11. Decision Tree Creation","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:12.056767Z","iopub.execute_input":"2021-08-26T18:04:12.057477Z","iopub.status.idle":"2021-08-26T18:04:12.221643Z","shell.execute_reply.started":"2021-08-26T18:04:12.057406Z","shell.execute_reply":"2021-08-26T18:04:12.220892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:12.222544Z","iopub.execute_input":"2021-08-26T18:04:12.22293Z","iopub.status.idle":"2021-08-26T18:04:12.226302Z","shell.execute_reply.started":"2021-08-26T18:04:12.222901Z","shell.execute_reply":"2021-08-26T18:04:12.225632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n# 12. Creation of Decision Tree using Gini Index","metadata":{}},{"cell_type":"code","source":"#Using the Decision Tree Classifier with splitting criterion as Gini impurity, the maximum depth of the tree is 3.\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_gini.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:12.227352Z","iopub.execute_input":"2021-08-26T18:04:12.227728Z","iopub.status.idle":"2021-08-26T18:04:12.264603Z","shell.execute_reply.started":"2021-08-26T18:04:12.227702Z","shell.execute_reply":"2021-08-26T18:04:12.26352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the tree\nplt.figure(figsize=(12,8))\n\ntree.plot_tree(clf_gini.fit(X_train, y_train)) ","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:12.265912Z","iopub.execute_input":"2021-08-26T18:04:12.266261Z","iopub.status.idle":"2021-08-26T18:04:13.107928Z","shell.execute_reply.started":"2021-08-26T18:04:12.266231Z","shell.execute_reply":"2021-08-26T18:04:13.106788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict the values \ny_pred_gini = clf_gini.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.109441Z","iopub.execute_input":"2021-08-26T18:04:13.109839Z","iopub.status.idle":"2021-08-26T18:04:13.117288Z","shell.execute_reply.started":"2021-08-26T18:04:13.109796Z","shell.execute_reply":"2021-08-26T18:04:13.116146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overfitting occurs when accuracy for traning set is high and test set is very low comparing to training set. Overfitting is very common problem with decision tree.","metadata":{}},{"cell_type":"code","source":"#Predict the value using X train for accuracy comparision \ny_pred_train_gini = clf_gini.predict(X_train)\n\ny_pred_train_gini","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.118828Z","iopub.execute_input":"2021-08-26T18:04:13.11923Z","iopub.status.idle":"2021-08-26T18:04:13.135677Z","shell.execute_reply.started":"2021-08-26T18:04:13.119189Z","shell.execute_reply":"2021-08-26T18:04:13.134533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Determine the accuracy score\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))\n#Accuracy Score for training set\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.137211Z","iopub.execute_input":"2021-08-26T18:04:13.137601Z","iopub.status.idle":"2021-08-26T18:04:13.148261Z","shell.execute_reply.started":"2021-08-26T18:04:13.137559Z","shell.execute_reply":"2021-08-26T18:04:13.147162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n# 13. Creation of Decision Tree using with entropy","metadata":{}},{"cell_type":"code","source":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_en.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.149784Z","iopub.execute_input":"2021-08-26T18:04:13.150199Z","iopub.status.idle":"2021-08-26T18:04:13.177618Z","shell.execute_reply.started":"2021-08-26T18:04:13.150157Z","shell.execute_reply":"2021-08-26T18:04:13.176904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\ntree.plot_tree(clf_en.fit(X_train, y_train)) ","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.178445Z","iopub.execute_input":"2021-08-26T18:04:13.178666Z","iopub.status.idle":"2021-08-26T18:04:13.872904Z","shell.execute_reply.started":"2021-08-26T18:04:13.178644Z","shell.execute_reply":"2021-08-26T18:04:13.871789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict the values \ny_pred_en = clf_en.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.876322Z","iopub.execute_input":"2021-08-26T18:04:13.87679Z","iopub.status.idle":"2021-08-26T18:04:13.884707Z","shell.execute_reply.started":"2021-08-26T18:04:13.876754Z","shell.execute_reply":"2021-08-26T18:04:13.883801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict the value using X train for accuracy comparision\ny_pred_train_en = clf_en.predict(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.886189Z","iopub.execute_input":"2021-08-26T18:04:13.886473Z","iopub.status.idle":"2021-08-26T18:04:13.902741Z","shell.execute_reply.started":"2021-08-26T18:04:13.886444Z","shell.execute_reply":"2021-08-26T18:04:13.901468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Model accuracy score with criterion entropy: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.904401Z","iopub.execute_input":"2021-08-26T18:04:13.904726Z","iopub.status.idle":"2021-08-26T18:04:13.91331Z","shell.execute_reply.started":"2021-08-26T18:04:13.904697Z","shell.execute_reply":"2021-08-26T18:04:13.91191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training set score: {:.4f}'.format(clf_en.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(clf_en.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.91505Z","iopub.execute_input":"2021-08-26T18:04:13.915468Z","iopub.status.idle":"2021-08-26T18:04:13.932204Z","shell.execute_reply.started":"2021-08-26T18:04:13.915422Z","shell.execute_reply":"2021-08-26T18:04:13.931064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n# 14. Confusion Matrix ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import  f1_score","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.933721Z","iopub.execute_input":"2021-08-26T18:04:13.934177Z","iopub.status.idle":"2021-08-26T18:04:13.940046Z","shell.execute_reply.started":"2021-08-26T18:04:13.934131Z","shell.execute_reply":"2021-08-26T18:04:13.938676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_en)\n\nprint('Confusion matrix\\n\\n', cm)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.941285Z","iopub.execute_input":"2021-08-26T18:04:13.941568Z","iopub.status.idle":"2021-08-26T18:04:13.956655Z","shell.execute_reply.started":"2021-08-26T18:04:13.941543Z","shell.execute_reply":"2021-08-26T18:04:13.955706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(cm, annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.0f',ax=ax)\nplt.show()\nplt.savefig('ConfusionMatrix.png')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:13.958006Z","iopub.execute_input":"2021-08-26T18:04:13.958519Z","iopub.status.idle":"2021-08-26T18:04:14.208425Z","shell.execute_reply.started":"2021-08-26T18:04:13.958474Z","shell.execute_reply":"2021-08-26T18:04:14.207523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_en))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:14.209914Z","iopub.execute_input":"2021-08-26T18:04:14.210557Z","iopub.status.idle":"2021-08-26T18:04:14.224073Z","shell.execute_reply.started":"2021-08-26T18:04:14.210509Z","shell.execute_reply":"2021-08-26T18:04:14.222757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score = f1_score(y_test, y_pred_en)\nprint(\"F1 Score:\",f1_score)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:04:14.225484Z","iopub.execute_input":"2021-08-26T18:04:14.225882Z","iopub.status.idle":"2021-08-26T18:04:14.234377Z","shell.execute_reply.started":"2021-08-26T18:04:14.22584Z","shell.execute_reply":"2021-08-26T18:04:14.233267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n# 15. Conclusion","metadata":{}},{"cell_type":"markdown","source":"Decision-Tree Classifier model using both gini index and entropy have only very very small difference in model accuracy and training set accuracy, so there is no sign of overfitting.","metadata":{}}]}