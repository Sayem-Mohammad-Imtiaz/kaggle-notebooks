{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import skew, kurtosis\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Data Processing libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Importing models\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No Null Values?\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out columns that are not necessarily discrete\nfor col in data.select_dtypes(np.number).columns:\n    if data[col].nunique() > 5:\n        print(col, data[col].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out column names\nprint(data.columns.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the discrete values (values lesser than 5 count as being discrete)\ndisc_vars = 'sex cp fbs restecg exng slp caa thall output'.split()\ndisc_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the distributions of each discrete variable\nfig, axes = plt.subplots(3,3,figsize=(14,14))\nfor var,ax in zip(disc_vars,axes.flat):\n    sns.countplot(data=data,x=var,hue='output',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some observations:\n- Samples with a 'thall' value of 2 are VERY likely to be positive instances\n- Samples with an 'exng' value of 1 are more likely to be positive instances\n- People with 'restecg' value 1 are slightly more likely to be at risk\n- Samples with an 'slp' value of 2 are very likely to be positive instances\n- Samples with a 'caa' value of 0 are VERY likely to be positive instances\n- It seems that females (sex=0) are more likely to be positive instances\n- fbs really is not a useful feature\n- Those with a 'cp' value of 2 are more at risk\n- Our target class has a good distribution of different classes","metadata":{}},{"cell_type":"code","source":"# Capture the 'less discrete' features\ncont_vars = [var for var in data.columns if var not in disc_vars]\nprint(len(cont_vars))\ncont_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3,2,figsize=(14,14))\nfor var,ax in zip(cont_vars, axes.flat):\n    sns.distplot(data[var],ax=ax,bins=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apart from 'oldpeak' everything else has a balanced distribution. This special feature is skewed so we have to normalize it.","metadata":{}},{"cell_type":"code","source":"# Fixing the positive skewness with log transform\ndata['oldpeak'] = np.log1p(data['oldpeak'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(data['oldpeak'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for Categorical Variables\ndata.select_dtypes(exclude=np.number).columns.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there are no Null Values, no Categorical Variables. Recall we still have some variables that take on discrete values so we should One-Hot Encode them. On top of this we should scale our data.","metadata":{}},{"cell_type":"code","source":"# Make a copy so we can see the difference with cross validation if scaling, encoding made a difference\ndata_copy = data.copy()\n\n# Get rid of the target values so we can use a pipeline to transform the features\ndata_copy.drop('output',axis=1,inplace=True)\ny = data['output']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a pipeline for scaling and transforming our data\ndisc = [i for i in disc_vars if i != \"output\"]\npipeline = ColumnTransformer([\n    (\"numeric\",StandardScaler(),cont_vars),\n    (\"discrete\",OneHotEncoder(),disc)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the Transformations\ndata_copy = pipeline.fit_transform(data_copy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test whether the transformations made a difference using cross validation and a simple LR model\nmodel = LogisticRegression()\nprint(\"Scores with transformed dataset:\")\nprint(cross_val_score(model,data_copy,y,cv=10).mean())\nmodel2 = LogisticRegression()\nprint(\"Scores with plain dataset:\")\nprint(cross_val_score(model2,data.drop('output',axis=1),y,cv=10).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So our transformed dataset looks to feed better to models, though not by much. Perhaps this is made more apparent using more powerful models like RandomForests.","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nprint(\"Scores with transformed dataset:\")\nprint(cross_val_score(model,data_copy,y,cv=10).mean())\nmodel2 = RandomForestClassifier()\nprint(\"Scores with plain dataset:\")\nprint(cross_val_score(model2,data.drop('output',axis=1),y,cv=10).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again it does not look like much of an improvement but we shall stick with it and train an ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata = data_copy\nX_train,X_test,y_train,y_test = train_test_split(data,y,test_size=0.2,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make our models and train them\n# Lets add in LinearSVC too\nfrom sklearn.svm import LinearSVC\nrf = RandomForestClassifier(n_estimators=120,random_state=42)\nsvm = SVC(max_iter=100)\nmlp = MLPClassifier(random_state=42)\nlog_reg = LogisticRegression(random_state=42)\nknn = KNeighborsClassifier()\nada = AdaBoostClassifier()\nlsvc = LinearSVC(max_iter=100, tol=20, random_state=42)\n\nestimators = [rf,svm,mlp,log_reg,knn,ada,lsvc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for estimator in estimators:\n    print(\"Training the \",estimator)\n    estimator.fit(X_train,y_train)\n    print(estimator.score(X_test,y_test))\nprint(\"Done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KNN worked very well here- for our ensemble we shall only keep the four best estimators.","metadata":{}},{"cell_type":"code","source":"named_estimators = [\n    (\"rf\",rf),(\"svm\",svm),(\"log_reg\",log_reg),(\"knn\",knn)\n]\nvoting_classifier = VotingClassifier(named_estimators)\n\nprint(\"Training...\")\nvoting_classifier.fit(X_train,y_train)\nprint(\"Done.\\n Score: \")\n# See how it scores\nprint(voting_classifier.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Ensemble was disappointing. The KNN Classifier won out in the end with an accuracy of $90.16$%","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix for the Hard Voting Classifier:\")\nprint(confusion_matrix(voting_classifier.predict(X_test),y_test))\nprint(\"Confusion Matrix for the KNN Classifier:\")\nprint(confusion_matrix(knn.predict(X_test),y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Confusion Matrices are not dissimilar so we can safely say that the KNN Classifier is the victor.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}