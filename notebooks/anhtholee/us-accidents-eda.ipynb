{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis on US Accidents dataset"},{"metadata":{},"cell_type":"markdown","source":"## Load libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nplt.style.use('ggplot')\nsns.set()\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.labelsize'] = 16","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define some util functions"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\"\n    Reduce dataframe's memory usage\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16',\n                'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64']\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int' or str(col_type)[:4] == 'uint':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min >= np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min >= np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min >= np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimisation is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(\n        100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def corr_plot(data, title, method='pearson', figsize=(13,8)):\n    \"\"\"\n    Plot the correlation matrix\n    \"\"\"\n    mname = {\n        'pearson': 'Pearson correlation',\n        'kendall': 'Kendall Tau correlation',\n        'spearman': 'Spearman rank correlation'\n    }\n    corr = data.corr(method=method)\n    fig, (ax) = plt.subplots(1, 1, figsize=figsize)\n    ax.set_title(\"{} ({})\".format(title, mname[method]))\n    ax = sns.heatmap(\n        corr,\n        vmin=-1, vmax=1, center=0,\n        cmap=sns.diverging_palette(20, 220, n=200),\n        square=True\n    )\n    ax.set_yticklabels(\n        ax.get_yticklabels(),\n        rotation=0,\n        horizontalalignment='right'\n    )\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=90,\n        horizontalalignment='right'\n    )\n    ax.set_ylim(corr.shape[0], 0)\n    return fig, (ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and preprocess the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ndf = pd.read_csv(\"/kaggle/input/us-accidents/US_Accidents_May19.csv\").pipe(reduce_mem_usage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lowercase all columns\ndf.columns = map(str.lower, df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe(include='O').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for nulls, missing values"},{"metadata":{},"cell_type":"markdown","source":"The nulls are described clearly in the dataset description."},{"metadata":{},"cell_type":"markdown","source":"### Time columns engineering\nFor further timeseries manipulation, we need columns representing datetime components"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\ndf = df.assign(\n    start_time=lambda df: pd.to_datetime(df.start_time),\n    end_time=lambda df: pd.to_datetime(df.end_time),\n    weather_timestamp=lambda df: pd.to_datetime(df.weather_timestamp),\n    time_span=lambda df: df.end_time - df.start_time,\n    time_span_hour=lambda df: df.time_span / np.timedelta64(1, 'h'),\n    time_span_minute=lambda df: df.time_span_hour * 60,\n    start_hour=lambda df: df.start_time.dt.hour,\n    start_month=lambda df: df.start_time.dt.month,\n    start_dow=lambda df: df.start_time.dt.weekday_name,\n    start_dom=lambda df: df.start_time.dt.day,\n    end_hour=lambda df: df.end_time.dt.hour,\n    end_month=lambda df: df.end_time.dt.month,\n    end_dow=lambda df: df.end_time.dt.dayofweek,\n    end_dom=lambda df: df.end_time.dt.day,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA "},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = corr_plot(df.drop(['start_time', 'end_time', 'time_span', 'weather_timestamp'], axis=1), title='Correlation plot', method='pearson', figsize=(15, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Source distribution\nLet's look at the main sources of the accident reports"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = df['source'].value_counts().plot(kind='bar')\nax.set_title(\"Report count by source\")\nax.set_xlabel(\"Source\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### State distribution\nNext let's see the distribution of accidents by state"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(17,8))\nax = df['state'].value_counts().plot(kind='bar')\nax.set_title(\"Accident count by state\")\nax.set_xlabel(\"State\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,8))\nax = df['state'].value_counts().sort_values(ascending=True).tail(10).plot(kind='barh')\nax.set_title(\"Accidents by state - Top 10 states with most accidents\")\nax.set_ylabel(\"State\")\nax.get_xaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,8))\nax = df['state'].value_counts().sort_values(ascending=True).head(10).plot(kind='barh')\nax.set_title(\"Accidents by state - Top 10 states with the least number of accidents\")\nax.set_ylabel(\"State\")\nax.get_xaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Severity\nHow about the accidents' severity"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,8))\nax = df['severity'].value_counts().plot(kind='bar')\nax.set_title(\"Accident count by severity\")\nax.set_xlabel(\"Severity\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at severity by state also. Here is the table showing top 10 states with highest average severity"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.groupby('state').agg(\n    accident_count=('id','count'),\n    mean_severity=('severity', 'mean'),\n    median_severity=('severity', 'median'),\n    std_severity=('severity', 'std')\n).sort_values(by=['mean_severity', 'accident_count'], ascending=[False, False]).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here is the distribution of severity based on geolocation (inspired by [this kernel](https://www.kaggle.com/biphili/road-accidents-in-us)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nax = df.plot(kind='scatter', x='start_lng', y='start_lat', label='Severity', c='severity', cmap=plt.get_cmap('jet'), colorbar=True,alpha=0.4, figsize=(14,8))\nax.set_title(\"Severity distribution by location\")\n# ax.set_xlabel(\"Severity\")\nax.legend()\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accident time\nLet's see when did the accidents normally occur"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,8))\nax = df['start_hour'].value_counts().sort_index().plot(kind='bar')\nax.set_title(\"Accident starts at which hour?\")\nax.set_xlabel(\"Hour\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution has peaks at 7am-8am and 4pm-5pm. How about the days of week"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17,8))\nax = df['start_dow'].value_counts().plot(kind='bar')\nax.set_title(\"Accident starts at which day of week?\")\nax.set_xlabel(\"Hour\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So most accidents occured during weekdays. How about the duration?"},{"metadata":{},"cell_type":"markdown","source":"### Accident duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.time_span_hour.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the discrepancies - negative time span?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.time_span_hour < 0].id.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are only 13 records, let's remove them for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[(df['time_span_hour'] > 0)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the accidents with lengths less than 24h, which account for 99.95% of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.query(\"time_span_hour < 24\")['id'].count() / df.id.count()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\n# ax = df.query(\"time_span_hour <= 24\")['time_span_hour'].hist(bins=50)\nax = sns.kdeplot(df.query(\"time_span_hour <= 24\")['time_span_hour'], shade=True)\nax.set_title(\"Accident length (in hour) - Accidents that happened within 24 hours\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can we look deeper into the duration (in minutes) ? Let's look at accidents that happened within 2 hours"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\n# ax = df.query(\"time_span_hour <= 24\")['time_span_hour'].hist(bins=50)\nax = sns.kdeplot(df.query(\"time_span_hour <= 2\")['time_span_minute'], shade=True)\nax.set_title(\"Accident length (in minute) - Accidents that happened within 2 hours\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's also visualise the duration distribution in different severity levels"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,8))\nax = sns.boxplot(x='severity', y='time_span_hour', data=df[df['time_span_hour'] <= 24])\nax.set_title(\"Duration distribution by severity\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the accidents with low severity levels have smaller time span in general."},{"metadata":{},"cell_type":"markdown","source":"### Timezone"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(13,8))\nax = df['timezone'].value_counts().plot(kind='bar')\nax.set_title(\"Accident count by timezone\")\nax.set_xlabel(\"Timezone\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Length of road extent affected"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['distance(mi)'] = df['distance(mi)'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['distance(mi)'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['distance(mi)'].quantile(.99)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 99% of the distances affected are less than 5 mile. So we filter out all of the distances more than 5 before plotting the distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\n# ax = df.query(\"time_span_hour <= 24\")['time_span_hour'].hist(bins=50)\nax = sns.kdeplot(df[df['distance(mi)'] < 5]['distance(mi)'], shade=True)\nax.set_title(\"Length of road affected (in mile)\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How the distance differs for different severity levels?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,8))\nax = sns.boxplot(x='severity', y='distance(mi)', data=df[df['distance(mi)'] < 5])\nax.set_title(\"Distance distribution by severity\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}