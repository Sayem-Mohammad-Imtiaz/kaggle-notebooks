{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Univariate Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" # This is part of a series on AI by ISTE Manipal. Read more posts on AI at:\n https://instagram.com/istemanipal?igshid=eb1cyeqm3pvr","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import all necessary libraries\nFor this tutorial we use numpy for array manipulation and operations, matplotlib to visualize our data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split#just to split dataset into training set and validation set\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import all the datasets\nWe use a function read_csv() from the library pandas to load the dataset( a comma separated value file) into a variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path=\"../input/weatherww2/Summary of Weather.csv\"#filepath for the dataset\ndata=pd.read_csv(path)#read dataset(csv file) from our filepath into a dataframe\ndata2=pd.read_csv(\"../input/weatherww2/Weather Station Locations.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take a look at our data**\nThis is helpful as we need to figure out how our training set looks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's analyse our dataset further before working on the model.\nWe will use a property dhape to get value of the dimension of the dataset. Further we will analyse the mean statistical distributions of various columns like mean, max value etc. This is important to understand as it will directly affect the choice of parameters we make later in the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()#use a built in function to get all properties ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the data\nThis is perhaps the most important step in exploratory data anaylsis i.e visualisation of data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot(x=\"MinTemp\",y=\"MaxTemp\",style='o')\nplt.xlabel('MinTemp')\nplt.ylabel('MaxTemp')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"iloc is used to extract values from a dataframe. \nFor more usage read the official documentation at:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"to_numpy() converts a pandas dataframe to a numpy array.\nreshape with arguments -1 and 1 simply tells numpy to convert a 1D array to a 2D array. Don't worry its not changing any values , it just makes (n, ) array to (n,1) array, so that we can further work upon this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data.iloc[:,[4,5]]#extract MinTemp and Maxtemp into another dataframe\nx=df.iloc[:,0].to_numpy().reshape(-1,1)\ny=df.iloc[:,1].to_numpy().reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just a sanity check here\nprint(x.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To understand how the function of train_test_split, https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#For a sanity check ,print shape of datasets\nprint(\"Size of training set:\\n\",x_train.shape)\nprint(y_train.shape)\nprint(\"Size of testing set:\\n\",x_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialise parameters for the model\nThe initial weights for the model are set as 0. However we can also use random numbers to initialise the parameters (Try it yourself!! Does changing initial value of the parameters have any effect on the final output? If yes, then how can you define the change? If no, what causes the invariance?. )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params=np.zeros((2,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The params vector actually consists of both the weight and the bias. So the first row is the bias and the second one is the weight. \nNote that it is not necessary to take the bias in the parameter vector . We have taken it in this manner for convienience. In fact when we graduate to neural networks, we will use bias separately.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iterations=60000\nlearning_rate=0.001","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try adjusting the learning rate and see if it fits the graph better !\nCan we adjust learning rate depending upon how we are doing? For example, speeding it up when we have heavy losses , slowing it down when we approach the minimum loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=np.hstack((np.ones_like(x_train),x_train))#simply add a column of 1s in the training set ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define a function computeCost which will calculate the cost for a given dataset and weights**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCost(x,y,w):\n    temp=np.dot(x,w)-y\n    return np.sum(np.power(temp,2))/(2*len(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Computing the initial cost when the model is untrained","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"J=computeCost(x_train,y_train,params)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Descent\nFinally apply gradient descent to adjust parameters. For more information about Gradient Descent follow this post :https://www.instagram.com/p/CC0Puu3BCuZ/?utm_source=ig_web_copy_link","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradientDescent(x,y,w,learning_rate,iterations):\n    for i in range(iterations):\n        temp=np.dot(x,w)-y\n        temp=np.dot(x.T,temp)\n        w=w-(learning_rate/len(y))*temp\n    return w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params=gradientDescent(x_train,y_train,params,learning_rate,iterations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute the final cost once we have trained the model.\n\nTry changing the initial parameters, learning rate , iterations , and see what changes it has on initial and final values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(computeCost(x_train,y_train,params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the curve \nPlot a straight line according to the parameters calculated by the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x_train[:,1],y_train)\nplt.plot(x_train,np.dot(x_train,params))\nplt.xlabel(\"Minimum Temperature\")\nplt.ylabel(\"Maximum Tempearture\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step we have to validate the parameters using the testing dataset(x_test,y_test). Stay tuned for the tutorial for hypertuning of parameters and validation of datasets.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}