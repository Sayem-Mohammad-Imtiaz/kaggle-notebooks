{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight')\n\nfrom math import sqrt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nnp.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom keras.preprocessing.sequence import TimeseriesGenerator\n!pip install livelossplot\nfrom livelossplot.keras import PlotLossesCallback","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\nReading the market data of BAJAJFINSV stock and preparing a training dataset and validation dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nifty50-stock-market-data/BAJAJFINSV.csv\")\ndf.set_index(\"Date\", drop=False, inplace=True)\n# df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[:,['Date', 'Prev Close', 'Open', 'High', 'Low', 'Last', 'Close']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the target variable **Close** over time"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Close.plot(figsize=(14, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get sizes of each of the datasets\nnum_cv = int(0.2*len(df))\nnum_test = int(0.2*len(df))\nnum_train = len(df) - num_cv - num_test\nprint(\"num_train = \" + str(num_train))\nprint(\"num_cv = \" + str(num_cv))\nprint(\"num_test = \" + str(num_test))\n\n# Split into train, cv, and test\ntrain = df[:num_train][['Date', 'Close']]\ncv = df[num_train:num_train+num_cv][['Date', 'Close']]\ntrain_cv = df[:num_train+num_cv][['Date', 'Close']]\ntest = df[num_train+num_cv:][['Date', 'Close']]\n\nprint(\"train.shape = \" + str(train.shape))\nprint(\"cv.shape = \" + str(cv.shape))\nprint(\"train_cv.shape = \" + str(train_cv.shape))\nprint(\"test.shape = \" + str(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_x_y(data, N, offset):\n    \"\"\"\n    Split data into x (features) and y (target)\n    \"\"\"\n    x, y = [], []\n    for i in range(offset, len(data)):\n        x.append(data[i-N:i])\n        y.append(data[i])\n    x = np.array(x)\n    y = np.array(y)\n    \n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N=9\n#offset value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\n\ntrain_scaled = scaler.fit_transform(np.array(train['Close']).reshape(-1,1))\nprint(\"scaler.data_min_ = \" + str(scaler.data_min_))\nprint(\"scaler.data_max_ = \" + str(scaler.data_max_))\n\n# Split into x and y\nx_train, y_train = get_x_y(train_scaled, N, N)\nprint(\"x_train.shape = \" + str(x_train.shape))\nprint(\"y_train.shape = \" + str(y_train.shape))\n\n# x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1)) # (446, 7, 1)\n# print(\"x_train.shape = \" + str(x_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the cv dataset according the min and max obtained from train set\ntrain_cv_scaled  = scaler.transform(np.array(train_cv['Close']).reshape(-1,1))\n\n# Split into x and y\nx_cv, y_cv = get_x_y(train_cv_scaled, N, num_train)\nprint(\"x_cv.shape = \" + str(x_cv.shape))\nprint(\"y_cv.shape = \" + str(y_cv.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we scale the train_cv set, for the final model\nscaler_final = MinMaxScaler(feature_range=(0, 1))\ntrain_cv_scaled_final = scaler_final.fit_transform(np.array(train_cv['Close']).reshape(-1,1))\nprint(\"scaler_final.data_min_ = \" + str(scaler_final.data_min_))\nprint(\"scaler_final.data_max_ = \" + str(scaler_final.data_max_))\n\n# Scale the test dataset according the min and max obtained from train_cv set\ntest_scaled  = scaler_final.transform(np.array(test['Close']).reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(units=50))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x_train, y_train, epochs=50, batch_size=1, callbacks=[PlotLossesCallback()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mape(y_true, y_pred): \n    \"\"\"\n    Compute mean absolute percentage error (MAPE)\n    \"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do prediction\nest = model.predict(x_cv)\nest_inv = scaler.inverse_transform(est)\n\n# Get correct scale of y_cv\ny_cv_inv = scaler.inverse_transform(y_cv)\n\n# Calculate RMSE\nrmse_bef_tuning = sqrt(mean_squared_error(y_cv_inv, est_inv))\nprint(\"RMSE = %0.3f\" % rmse_bef_tuning)\n\n# Calculate MAPE\nmape_pct_bef_tuning = get_mape(y_cv_inv, est_inv)\nprint(\"MAPE = %0.3f%%\" % mape_pct_bef_tuning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot adjusted close over time\nrcParams['figure.figsize'] = 10, 8 # width 10, height 8\n\nest_df = pd.DataFrame({'est_inv': est_inv.reshape(-1), \n                       'y_cv_inv': y_cv_inv.reshape(-1),\n                       'Date': cv['Date']})\n\nax = train.plot(x='Date', y='Close', style='b-', grid=True)\nax = cv.plot(x='Date', y='Close', style='y-', grid=True, ax=ax)\nax = test.plot(x='Date', y='Close', style='g-', grid=True, ax=ax)\nax = est_df.plot(x='Date', y='est_inv', style='r-', grid=True, ax=ax)\nax.legend(['train', 'dev', 'test', 'est'])\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Close\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport math\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_pred_eval_model(x_train_scaled, \\\n                          y_train_scaled, \\\n                          x_cv_scaled, \\\n                          y_cv_scaled, \\\n                          scaler, \\\n                          lstm_units=50, \\\n                          dropout_prob=0.5, \\\n                          optimizer='adam', \\\n                          epochs=1, \\\n                          batch_size=1):\n    '''\n    Train model, do prediction, scale back to original range and do evaluation\n    Use LSTM here.\n    Returns rmse, mape and predicted values\n    Inputs\n        x_train_scaled  : e.g. x_train_scaled.shape=(451, 9, 1). Here we are using the past 9 values to predict the next value\n        y_train_scaled  : e.g. y_train_scaled.shape=(451, 1)\n        x_cv_scaled     : use this to do predictions \n        y_cv_scaled     : actual value of the predictions (scaled)\n        scaler          : scaler that is used to fit_transform train set\n        lstm_units      : lstm param\n        dropout_prob    : lstm param\n        optimizer       : lstm param\n        epochs          : lstm param\n        batch_size      : lstm param\n    Outputs\n        rmse            : root mean square error\n        mape            : mean absolute percentage error\n        est             : predictions\n    '''\n    # Create the LSTM network\n    model = Sequential()\n    model.add(LSTM(units=lstm_units, return_sequences=True, input_shape=(x_train_scaled.shape[1],1)))\n    model.add(Dropout(dropout_prob)) # Add dropout with a probability of 0.5\n    model.add(LSTM(units=lstm_units))\n    model.add(Dropout(dropout_prob)) # Add dropout with a probability of 0.5\n    model.add(Dense(1))\n\n    # Compile and fit the LSTM network\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    model.fit(x_train_scaled, y_train_scaled, epochs=epochs, batch_size=batch_size, verbose=0)\n    \n    # Do prediction\n    est_scaled = model.predict(x_cv_scaled)\n    est = scaler.inverse_transform(est_scaled)\n    \n    # Get correct scale of y_cv\n    y_cv = scaler.inverse_transform(y_cv_scaled)\n\n    # Calculate RMSE and MAPE\n    rmse = math.sqrt(mean_squared_error(y_cv, est))\n    mape = get_mape(y_cv, est)\n    \n    return rmse, mape, est","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_label = 'N'\nparam_list = range(2, 60)\n\nerror_rate = {param_label: [], 'rmse': [], 'mape_pct': []}\ntic = time.time()\nfor param in tqdm_notebook(param_list):\n    \n    # Split train into x and y\n    x_train_scaled, y_train_scaled = get_x_y(train_scaled, param, param)\n\n    # Split cv into x and y\n    x_cv_scaled, y_cv_scaled = get_x_y(train_cv_scaled, param, num_train)\n    \n    # Train, predict and eval model\n    rmse, mape, _ = train_pred_eval_model(x_train_scaled, \\\n                                          y_train_scaled, \\\n                                          x_cv_scaled, \\\n                                          y_cv_scaled, \\\n                                          scaler, \\\n                                          lstm_units=50, \\\n                                          dropout_prob=0.3, \\\n                                          optimizer='adam', \\\n                                          epochs=10, \\\n                                          batch_size=1)\n    \n    # Collect results\n    error_rate[param_label].append(param)\n    error_rate['rmse'].append(rmse)\n    error_rate['mape_pct'].append(mape)\n    \nerror_rate = pd.DataFrame(error_rate)\ntoc = time.time()\nprint(\"Minutes taken = \" + str((toc-tic)/60.0))\nerror_rate   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot RMSE \nrcParams['figure.figsize'] = 10, 8 # width 10, height 8\n\nax = error_rate.plot(x='N', y='rmse', style='bx-', grid=True)\nax = error_rate.plot(x='N', y='mape_pct', style='rx-', grid=True, ax=ax)\nax.set_xlabel(\"N\")\nax.set_ylabel(\"RMSE/MAPE(%)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get optimum value for param\ntemp = error_rate[error_rate['rmse'] == error_rate['rmse'].min()]\nN_opt = temp['N'].values[0]\nprint(\"min RMSE = %0.3f\" % error_rate['rmse'].min())\nprint(\"min MAPE = %0.3f%%\" % error_rate['mape_pct'].min())\nprint(\"optimum \" + param_label + \" = \" + str(N_opt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nstatistics like mean, standard deviation for three sets of lagged values, one previous day, one looking back 7 days and another looking back 30 days as a proxy for last week and last month metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)\nlag_features = [\"High\", \"Low\", \"Open\"]\nwindow1 = 3\nwindow2 = 5\nwindow3 = 7\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_5d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_5d = df_rolled_5d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_5d = df_rolled_5d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\n\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_5d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_7d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_5d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_7d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Date\", drop=False, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Date = pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.week\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into train and validation along with features.     \n* **train:** Data from 26th May, 2008 to 31st December, 2018.\n* **valid:** Data from 1st January, 2019 to 31st December, 2019."},{"metadata":{"trusted":true},"cell_type":"code","source":"exogenous_features = ['High_mean_lag3', 'High_mean_lag5', 'High_mean_lag7',\n                       'High_std_lag3', 'High_std_lag5', 'High_std_lag7', 'Low_mean_lag3',\n                       'Low_mean_lag5', 'Low_mean_lag7', 'Low_std_lag3', 'Low_std_lag5',\n                       'Low_std_lag7', 'Volume_mean_lag3', 'Volume_mean_lag5',\n                       'Volume_mean_lag7', 'Volume_std_lag3', 'Volume_std_lag5',\n                       'Volume_std_lag7', 'Prev Close_mean_lag3', 'Prev Close_mean_lag5',\n                       'Prev Close_mean_lag7', 'Prev Close_std_lag3', 'Prev Close_std_lag5',\n                       'Prev Close_std_lag7', 'month', 'week', 'day', 'day_of_week']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = df[exogenous_features+[\"Close\"]]\nvalues = values.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = scaled[:2772, :]\ntest = scaled[2772:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    LSTM(50, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])),\n    Dropout(0.2),\n    Dense(20),\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(train_X, train_y, epochs=100, batch_size=10, validation_data=(test_X, test_y), callbacks=[PlotLossesCallback()], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(LSTM(\n    input_dim=1,\n    output_dim=50,\n    return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(\n    100,\n    return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(\n    output_dim=1))\nmodel.add(Activation('linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n# invert scaling for forecast\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = model.predict(test_generator)\ntest_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test[:-6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions[:-1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = [i[0] for i in test_predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(test_predictions[:-1]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Forecast_LSTM'] = pd.Series(test_predictions[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[[\"Close\"]].plot(figsize=(14, 7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RMSE of LSTM:\", np.sqrt(mean_squared_error(df_test.Close, df_test.Forecast_LSTM)))\nprint(\"\\nMAE of LSTM:\", mean_absolute_error(df_test.Close, df_test.Forecast_LSTM))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying Predicting next 3 values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_close = df_train['Close'].to_list()\ntrain_target2 = np.array([train_close[i:i+3] for i in range(len(train_close)-2)])\n\ntest_close = df_test['Close'].to_list()\ntest_target2 = np.array([test_close[i:i+3] for i in range(len(test_close)-2)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset2 = train_dataset[:-2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset2 = test_dataset[:-4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use last 2 values\ntrain_generator2 = TimeseriesGenerator(train_dataset2, train_target2, length=2, batch_size=1)\n\n# use last 2 values\ntest_generator2 = TimeseriesGenerator(test_dataset2, test_target2, length=2, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    LSTM(200, activation='relu', input_shape=(2, 28)),\n    Dropout(0.15),\n    Dense(3)\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(train_generator2,epochs=90, callbacks=[PlotLossesCallback()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}