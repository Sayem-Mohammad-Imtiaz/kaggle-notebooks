{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\nHere I will be performing EDA on our spam/ham dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\ndf = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\ndf = df.rename(columns={\"v1\":\"labels\", \"v2\":\"text\"})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.labels.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing spam with 1 and ham with 0\ndf['spam']=df['labels']\nfor i,j in df.iterrows():\n    # i is index\n    # j is (labels, text)\n    if j['labels']=='ham':\n        j['spam'] = 0\n    else:\n        j['spam']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing of SMS\n\nThis task involves :<br>\n1. Tokenization\n2. Vectorization\n3. TF-IDF resemblency\n\n## Removal of punctuations and stop-words\n\n### Punctuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nprint(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stop-words\n\nStop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text,\nand which may be removed to avoid them being construed as signal for prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nprint(stopwords.words('english')[10:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def punctuation_stopwords_removal(sms):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in sms if ch not in string.punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_sms = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_sms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(punctuation_stopwords_removal(\"Hello we need to send this report by EOD.!!! yours sincerely, Purva\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of most common words in spam and ham SMS\n\nHere, we will be making use of `collections.Counter`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ndata_ham = df[df['spam']==0].copy()\ndata_spam = df[df['spam']==1].copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_ham[:2])\nprint(data_spam[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ham.loc[:, 'text'] = data_ham['text'].apply(punctuation_stopwords_removal)\nprint(data_ham[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_data_ham = data_ham['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_data_ham[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_spam.loc[:, 'text']=data_spam['text'].apply(punctuation_stopwords_removal)\nprint(data_spam[:1])\n#words_data_spam = data_spam['text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_data_spam = data_spam['text'].tolist()\nprint(words_data_spam[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_list = []\nfor sublist in words_data_ham:\n    for word in sublist:\n        ham_list.append(word)\n\nspam_list = []\nfor sublist in words_data_spam:\n    for word in sublist:\n        spam_list.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_count = Counter(ham_list)\nspam_count = Counter(spam_list)\n\nham_top_30_words = pd.DataFrame(ham_count.most_common(30), columns=['word', 'count'])\nspam_top_30_words = pd.DataFrame(spam_count.most_common(30), columns=['word', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=ham_top_30_words, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=spam_top_30_words, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BOW with CountVectorizer\n\nIn this scheme, features and samples are defined as follows: each individual token occurrence frequency (normalized or not) is treated as a feature.\nthe vector of all the token frequencies for a given document is considered a multivariate sample.<br>\nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors.<br>\nThis specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation.\nDocuments are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\n<img src=\"https://github.com/purvasingh96/Talking-points-global-hackathon/blob/master/assets/word2vec_architectures.png?raw=1\" width=\"500\"></img>\n\nIn this kernel we apply the CountVectorizer from sklearn as BOW model. : [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer=punctuation_stopwords_removal).fit(df['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(bow_transformer.vocabulary_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_spam = df['text'][8]\nbow_sample_spam = bow_transformer.transform([sample_spam])\nprint(sample_spam)\nprint(bow_sample_spam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Printing bag of words for sample 1')\nrow, cols = bow_sample_spam.nonzero()\nfor col in cols:\n    print(bow_transformer.get_feature_names()[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nprint(np.shape(bow_sample_spam))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_ham = df['text'][4]\nbow_sample_ham = bow_transformer.transform([sample_ham])\nprint(sample_ham)\nprint(bow_sample_ham)\nrows, cols = bow_sample_ham.nonzero()\nprint('Printing ')\nfor col in cols:\n    print(bow_transformer.get_feature_names()[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF on BOW\n\nTF-IDF expects a bag-of-words (integer values) training corpus during initialization. During transformation, it will take a vector and return another vector of the same dimensionality.<br>\n\nTF-IDF stands for \"Term Frequency, Inverse Document Frequency\".<br>\n\n* It is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents.\n* If a word appears frequently in a document, it's important. Give the word a high score. But if a word appears in many documents, it's not a unique identifier. Give the word a low score.<br>\n\n* Therefore, common words like *\"the\"* and *\"for\"*, which appear in many documents, will be scaled down. Words that appear frequently in a single document will be scaled up.<br>\n\nIn other words:\n* TF(w) = `(Number of times term w appears in a document) / (Total number of terms in the document).`\n* IDF(w) = `log_e(Total number of documents / Number of documents with term w in it).`\nFor example\nConsider a document containing 100 words wherein the word 'tiger' appears 3 times.\n* The term frequency (i.e., tf) for 'tiger' is then:<br>\n    TF = (3 / 100) = 0.03.\n* Now, assume we have 10 million documents and the word 'tiger' appears in 1000 of these. Then, the inverse document frequency (i.e., idf) is calculated as:<br>\n`IDF = log(10,000,000 / 1,000) = 4.`\nThus, the Tf-idf weight is the product of these quantities:\nTF-IDF = 0.03 * 4 = 0.12.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### TfidfTransformer from sklearn\n\nBoth tf and tf–idf can be computed as follows using sklearn's [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\n# bag of words in vectorized format\nbow_data = bow_transformer.transform(df['text'])\nprint(bow_data[:1])\ntfidf_transformer = TfidfTransformer().fit(bow_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_sample_ham = tfidf_transformer.transform(bow_sample_ham)\nprint('Sample HAM : ')\nprint(tfidf_sample_ham)\n\ntfidf_sample_spam = tfidf_transformer.transform(bow_sample_spam)\nprint('Sample SPAM : ')\nprint(tfidf_sample_spam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data_tfidf = tfidf_transformer.transform(bow_data)\nprint(final_data_tfidf)\nprint(np.shape(final_data_tfidf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train test split\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_tfidf_train, data_tfidf_test, label_train, label_test = train_test_split(final_data_tfidf, df[\"spam\"], test_size=0.3, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results Visualization Methods\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", square=True, cbar=False)\n    #  \n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Classifier for Spam/Ham Classification \n\nHere we will be using Naive Bayes' [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) to classify emails into spam/ham category. <br>One important thing to note in this part of coding section is that numpy didnt manage to figure out that datatype of `label_train` was float64 and by default it set the datatypt to a generic object.<br>\nIn order to solve this issue, we need to explicitly define dataype of `label_train` as `np.asarray(label_train, dtype=\"float64\")`.\n\n#### Results\n\nUpon applying NaiveBayes Classifier, we have achieved 96.5% accuracy.<br>\nUpon analysis of ROC charecterstics, we have achieved 97.698 as area under the curve (auc)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tfidf_train = data_tfidf_train.A\ndata_tfidf_test = data_tfidf_test.A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_tfidf_train.dtype)\nprint(label_train.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nspam_detect_model_MNB = MultinomialNB()\nspam_detect_model_MNB.fit(data_tfidf_train, np.asarray(label_train, dtype=\"float64\"))\npred_test_MNB = spam_detect_model_MNB.predict(data_tfidf_test)\nacc_MNB = accuracy_score(np.asarray(label_test, dtype=\"float64\"), pred_test_MNB)\nprint(acc_MNB)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thr = roc_curve(np.asarray(label_test, dtype=\"float64\"), spam_detect_model_MNB.predict_proba(data_tfidf_test)[:,1])\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc_knn4 = auc(fpr, tpr) * 100\nplt.legend([\"AUC {0:.3f}\".format(auc_knn4)]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", square=True, cbar=False)\n    #  \n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(np.asarray(label_test, dtype=\"float64\"), pred_test_MNB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}