{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re  # For preprocessing\nimport pandas as pd  # For data handling\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport spacy  # For preprocessing\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/southparklines/All-seasons.csv\")\ndata.head()\n# data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n## Preprocessing\nOnly two columns that I want to keep are:\n* \"Character\": that is the character who speaks;\n* \"Line\": that is the raw text from the line of dialogue."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove column \"Season\" \ndata = data.drop(['Season'], axis = 1)\n# Remove column \"Episode\" \ndata = data.drop(['Episode'], axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning data\nFirst, I have to remove the stopwords and non-alphabetic characters for each line of dialogue.\nI followed a tutorial to use spaCy library to preprocessing data and speed up cleaning process."},{"metadata":{},"cell_type":"markdown","source":"![NLP_Pipeline](http://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\n# load english language model. Disable Named Entity Recognition ('ner') and 'parser' in Natural Language Processing (nlp) for speed (check the image)\nnlp = spacy.load('en', disable=['ner', 'parser'])\n\ndef cleaning(doc):\n    # Lemmatizes and removes stopwords\n    # doc needs to be a spacy Doc object\n    # token.lemma_ is the base form of the word (for example: token.text_= APPLE token.lemma_= apple)\n    # token.is_stop is a boolean value that represent if the word is one of the most common words on the language(for example: \"for\", \"is\"..) \n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    ''' print the different parameter for token in doc\n        for token in doc:\n            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n    '''\n    # Word2Vec uses context words to learn the vector representation of a target word,\n    # if a sentence is only one or two words long,\n    # the benefit for the training is very small\n    if len(txt) > 2:\n        return ' '.join(txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removes non-alphabetic characters\nbrief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in data['Line'])\n# Use spaCy.pipe() attribute to speed-up the cleaning process\nt = time()\ntxt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n# print(txt)\nprint('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New DataFrame contains data without duplicates in one column named \"Clean\"\ndata_clean = pd.DataFrame({'Clean': txt})\ndata_clean = data_clean.dropna().drop_duplicates()\ndata_clean.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect bigrams (common phrases) from a list of sentences. For example 'mrs_garrison'\nfrom gensim.models.phrases import Phrases, Phraser\n# As Phrases() takes a list of list of words as input\nsent = [row.split() for row in data_clean['Clean']]\n# Creates the relevant phrases from the list of sentences\nphrases = Phrases(sent, min_count=30, progress_per=10000)\n# Export the trained model = use less RAM, faster processing\nbigram = Phraser(phrases)\n# Transform the corpus based on the bigrams detected\nsentences = bigram[sent]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find most frequent words\nCheck the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams by printing most frequent words in dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The \"defaultdict\" will simply create any items that you try to access (provided of course they do not exist yet).\n# This is useful to avoid that Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary.\nword_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n## Training model\nSeparate the training in 3 steps:\n1. Word2Vec() : set up the parameters of the model one-by-one.\n2. build_vocab() : builds the vocabulary from a sequence of sentences and thus initialized the model.\n3. trains the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\n\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cores = multiprocessing.cpu_count() # Count the number of cores in a computer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set up parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the vocabulary table"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trains the model\n* total_examples = int - Count of sentences;\n* epochs = int - Number of iterations (epochs) over the corpus - [10, 20, 30]"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n## Exploring the model\n### Most similar to\nChecking similarity between main characters of South Park and other words of dialogues."},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"eric\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what the bigram \"eric_cartman\" gives by comparison:"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"eric_cartman\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About Kenny:"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"kenny\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About Chef:"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"chef\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Similarities\nSimilarity between two words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.similarity(\"chef\", 'singer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.similarity(\"kyle\", 'jewish')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Odd-One-Out\nAsk to the model to give us the word that does not belong to the list."},{"metadata":{},"cell_type":"markdown","source":"Which of these character is white?"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.doesnt_match(['chef', 'token_black', 'stanley'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which of these is not a mother?"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.doesnt_match(['liane', 'sheila', 'bebe'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The answer is Bebe Stevens."},{"metadata":{},"cell_type":"markdown","source":"### Analogy difference\nCreate a kind of proposition between words.\nFor example if 'bebe' is 'popular' which word does 'chef' match?"},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"bebe\", \"popular\"], negative=[\"chef\"], topn=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case Bebe Stevens seems to be the \"popular\" girls' secondary leader, after Wendy Testaburger.\nThe algorithm have found the adjective most frequently used for Chef."},{"metadata":{},"cell_type":"markdown","source":"## t-SNE visualizations\nt-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.\nTo make the visualizations more relevant, we will look at the relationships between a query word (in **red**), its most similar words in the model (in **blue**), and other words from the vocabulary (in **green**)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 19 dimensions with PCA\n    reduc = PCA(n_components=19).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10 Most similar words vs. 8 Random words\nShow with t-SNE visualization, where the vector representation of 'eric', his 10 most similar words from the model, as well as 8 random ones, lies in a 2D graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'eric', ['dog', 'bird', 'ah', 'kill', 'bob', 'hat', 'drink', 'bebe'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}