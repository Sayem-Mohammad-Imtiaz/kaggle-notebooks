{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BUILDING DATA FRAMES FROM SCRATCH**\n\n\nWe can build dataframe from dictionaries\n* zip() method: This function returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.\n* Adding new column\n* Broadcasting: Create new column and assign a value to entire column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data frames from dictionary\ncountry = [\"Brazil\",\"Canada\", \"Germany\",\"UK\",\"Egypt\"]\npopulation = [\"211.742.618\",\"38.014.184\",\"83.694.929\",\"57.086.075\",\"94.400.000\"]\nlist_label = [\"country\",\"population\"]\nlist_col = [country,population]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\ndf = pd.DataFrame(data_dict)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add new columns\ndf[\"continent\"] = [\"S.America\",\"N.America\",\"Europe\",\"Europe\",\"Africa\"]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"NoLA\"] = 0 #number of living aliens \ndf             #Broadcasting entire column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VISUAL EXPLORATORY DATA ANALYSIS**\n\n1. Plot\n2. Subplot\n3. Histogram:\n * bins: number of bins\n * range(tuble): min and max values of bins\n * normed(boolean): normalize or not, between 0 and 1\n * cumulative(boolean): compute cumulative distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting all data \ndata1 = data.loc[:,[\"trestbps\",\"chol\",\"thalach\"]]\n# it is confusing\ndata1.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subplots\ndata1.plot(subplots = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot  \ndata1.plot(kind = \"scatter\",x=\"trestbps\",y = \"chol\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hist plot  \ndata1.plot(kind = \"hist\",y = \"thalach\",bins = 10,range= (0,250))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata1.plot(kind = \"hist\",y = \"thalach\",bins = 50,range= (0,250),ax = axes[0])\ndata1.plot(kind = \"hist\",y = \"thalach\",bins = 50,range= (0,250),ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**STATISTICAL EXPLORATORY DATA ANALYSIS**\n\n\n* count: number of entries\n* mean: average of entries\n* std: standart deviation\n* min: minimum entry\n* 25%: first quantile\n* 50%: median or second quantile\n* 75%: third quantile\n* max: maximum entry"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**INDEXING PANDAS TIME SERIES**\n\n* datetime = object\n* parse_dates(boolean): Transform date to ISO 8601 (yyyy-mm-dd hh:mm:ss ) format"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_list = [\"2020-03-16\",\"2020-12-14\"]\nprint(type(time_list[1])) # As you can see date is string\n\ndatetime_object = pd.to_datetime(time_list) #turn to the datetime but actually it was string\nprint(type(datetime_object))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data.tail()\ndate_list = [\"2020-03-16\",\"2020-03-19\",\"2021-03-10\",\"2021-03-16\",\"2021-04-16\"]\ndatetime_object = pd.to_datetime(date_list) \ndata2[\"date\"] = datetime_object\n# lets make date as index\ndata2= data2.set_index(\"date\")\ndata2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data2.loc[\"2021-04-16\"])\nprint(\"\\n\")\nprint(data2.loc[\"2020-03-16\":\"2021-03-10\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RESAMPLING PANDAS TIME SERIES**\n\n1. Resampling: statistical method over different time intervals\n\nNeeds string to specify frequency like \"M\" = month or \"A\" = year\n2. Downsampling: reduce date time rows to slower frequency like from daily to weekly\n3. Upsampling: increase date time rows to faster frequency like from daily to hourly\n4. Interpolate: Interpolate values according to different methods like ‘linear’, ‘time’ or index’"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample(\"A\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample(\"M\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample(\"M\").first().interpolate(\"linear\") #interpolate from first value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.resample(\"M\").mean().interpolate(\"linear\") #interpolate with mean","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}