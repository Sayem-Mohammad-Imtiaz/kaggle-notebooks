{"cells":[{"metadata":{},"cell_type":"markdown","source":"** This is an Insurance Challenge **\nhttps://www.kaggle.com/uciml/caravan-insurance-challenge\n\nThis data set used in the CoIL 2000 Challenge contains information on customers of an insurance company. The data consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The data was collected to answer the following question: Can you predict who would be interested in buying a caravan insurance policy and give an explanation why?\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/caravan-insurance-challenge/caravan-insurance-challenge.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the % of each null values.\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent_1 = df.isnull().sum()/df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool... No null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the NULL data using Seaborn HeatMap.\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check the data-types"},{"metadata":{"trusted":true},"cell_type":"code","source":"categ_features = df.select_dtypes(include='object').columns\nnumeric_features = df.select_dtypes(include='int').columns\ndisplay(categ_features, numeric_features, len(categ_features), len(numeric_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ORIGIN is the only field / column which is of Categorical type. Lets check its unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ORIGIN'].unique()\n# So this df data-set contains both train and test data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the count of Train and Test data.\ndf['ORIGIN'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we do have 5822 train and 4000 as test data. Will separate the data going forward."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the count of Train and Test data.\ndf['CARAVAN'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pearson Correlation\n\nplt.figure(figsize=(20,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndf.hist(column=numeric_features[1:5])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split into Train and Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split DataFrame in train and test\ntrain_df = df.loc[:5821]\ntest_df = df.loc[5822:]\ndisplay(train_df['ORIGIN'].unique(), test_df['ORIGIN'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the count of Train and Test data.\ndisplay(train_df['CARAVAN'].value_counts(), test_df['CARAVAN'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Train Data-set into train and valid data-sets, on which we will train our model. Before that will get our X and y.\n\nX = train_df.drop(['CARAVAN'], axis = 1)\n \ny = train_df['CARAVAN']\n \ndisplay(X.head(), y.head())    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the train_df into 2 DF's aka X_train, X_valid, y_train, y_valid.\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_valid.shape, y_valid.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df \nX_test  = test_df.drop(['CARAVAN'], axis = 1)\ny_test = test_df['CARAVAN']\nprint (X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we do have proper data splitting.. will drop the column 'ORIGIN'.\nX_train.drop('ORIGIN', inplace = True, axis = 1)\nX_valid.drop('ORIGIN', inplace = True, axis = 1)\nX_test.drop('ORIGIN', inplace = True, axis = 1)\n\nprint (X_train.shape, y_train.shape)\nprint (X_valid.shape, y_valid.shape)\nprint (X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Performance matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the model\n\nY_valid_pred_lr = logreg.predict(X_valid)\nY_test_pred_lr = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LogisticRegression (on Train and Valid Data-Set) --> \")\nprint(\"Score : \", round(logreg.score(X_train, y_train) * 100, 2) )\n\nprint(\"Accuracy Score : \", round(accuracy_score(y_valid, Y_valid_pred_lr) * 100, 2) )\n\nprint(\"Confusion Matrix : \" )\ndisplay( confusion_matrix(y_valid, Y_valid_pred_lr) )\n\nprint(\"ROC AUC Score : \", roc_auc_score(y_valid, Y_valid_pred_lr) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LogisticRegression (Test Data-Set) --> \")\nprint(\"Score : \", round(logreg.score(X_test, y_test) * 100, 2) )\n\nprint(\"Accuracy Score : \", round(accuracy_score(y_test, Y_test_pred_lr) * 100, 2) )\n\nprint(\"Confusion Matrix : \" )\ndisplay( confusion_matrix(y_test, Y_test_pred_lr) )\n\nprint(\"ROC AUC Score : \", roc_auc_score(y_test, Y_test_pred_lr) )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}