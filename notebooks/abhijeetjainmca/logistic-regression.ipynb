{"cells":[{"metadata":{"_uuid":"411b9457-42ec-4514-aefb-a5cf46f6b31f","_cell_guid":"346f43df-c964-4c77-b2b8-fd35e7ff9d15","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\ndf_emp_data = pd.read_csv('../input/hr-analytics-case-study/general_data.csv')\ndf_emp_survey = pd.read_csv('../input/hr-analytics-case-study/employee_survey_data.csv')\n\ndf_emp_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_emp_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing nulls of Employee data columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_emp_data.TotalWorkingYears.fillna(0,inplace=True)\ndf_emp_data.NumCompaniesWorked.fillna(0,inplace=True)\ndf_emp_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_emp_survey.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_emp_survey.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing nulls of Employee Survey columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_emp_survey['EnvironmentSatisfaction'].fillna(df_emp_survey['EnvironmentSatisfaction'].mode()[0],inplace=True)\ndf_emp_survey['JobSatisfaction'].fillna(df_emp_survey['JobSatisfaction'].mode()[0],inplace=True)\ndf_emp_survey['WorkLifeBalance'].fillna(df_emp_survey['WorkLifeBalance'].mode()[0],inplace=True)\ndf_emp_survey.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merging Employee Data and Employee Survey"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_emp_data = pd.merge(df_emp_data,df_emp_survey,on='EmployeeID')\ndf_emp_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_emp_data,x='BusinessTravel',hue='Gender')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_emp_data,x='Attrition',hue='Gender')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_emp_data,x='Department',hue='Attrition')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(data=df_emp_data,x='EducationField',hue='Attrition')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping columns not needed for predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_working_ds = df_emp_data.drop(['EmployeeCount','Over18','StandardHours','EmployeeID'],axis=1)\n\nfinal_working_ds.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing the variance in Ages. Currently there are 78 different values. Instead creating ranges of Ages and imputing these ranges."},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_age(age):\n    if (age>15 & age<=30):\n        return 1\n    elif (age>30 & age<=45):\n        return 2\n    elif (age>45 & age<=60):\n        return 3\n\nfinal_working_ds['Age']=final_working_ds.Age.apply(impute_age)\n\nfinal_working_ds.Gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Dummies for Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_dummy = pd.get_dummies(final_working_ds['Gender'],drop_first=True)\n\n#Concatinating the newly created dummy of Gender into Dataset\n\nfinal_working_ds = pd.concat([final_working_ds,gender_dummy],axis=1)\n\n#Dropping the existed column of Gender as Dummy variable is added\n\nfinal_working_ds.drop(['Gender'],inplace=True,axis=1)\n\nfinal_working_ds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using LabelEncoder to encode the Categorical values to numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nLabelE_BusinessTravel = LabelEncoder()\nfinal_working_ds['BusinessTravel'] = LabelE_BusinessTravel.fit_transform(final_working_ds['BusinessTravel'])\nLabelE_Department = LabelEncoder()\nfinal_working_ds['Department'] = LabelE_Department.fit_transform(final_working_ds['Department'])\nLabelE_EducationField = LabelEncoder()\nfinal_working_ds['EducationField'] = LabelE_EducationField.fit_transform(final_working_ds['EducationField'])\nLabelE_JobRole = LabelEncoder()\nfinal_working_ds['JobRole'] = LabelE_JobRole.fit_transform(final_working_ds['JobRole'])\nLabelE_MaritalStatus = LabelEncoder()\nfinal_working_ds['MaritalStatus'] = LabelE_MaritalStatus.fit_transform(final_working_ds['MaritalStatus'])\nLabelE_Attrition = LabelEncoder()\nfinal_working_ds['Attrition'] = LabelE_Attrition.fit_transform(final_working_ds['Attrition'])\n\nfinal_working_ds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling the features as it contains income"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscalr = StandardScaler()\nscalr.fit(final_working_ds.drop('Attrition',axis=1))\nscaled_features = scalr.transform(final_working_ds.drop('Attrition',axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating X and y for train test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"colms = final_working_ds.columns.delete(1)\nX = pd.DataFrame(scaled_features,columns=colms)\ny = final_working_ds['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the Logistic Regression model, fitting it on Training data and predicting y for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_Reg = LogisticRegression()\n\nlog_Reg.fit(X_train,y_train)\n\ny_pred = log_Reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the classification report and the confusion matrix of actual y values of TEST data and predicted values of TEST data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\n\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}