{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Telecom Churn Prediction","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"## What is the churn rate?\n- Wikipedia states that the churn rate (also called attrition rate) measures the number of individuals or items moving out of a collective group over a specific period. \n- It applies in many contexts, but the mainstream understanding of churn rate is related to the business case of customers that stop buying from you.","metadata":{}},{"cell_type":"markdown","source":"## Importance of customer churn prediction:\n- The impact of the churn rate is clear, so we need strategies to reduce it. \n- Predicting churn is a good way to create proactive marketing campaigns targeted at the customers that are about to churn. \n- Forecasting customer churn with the help of machine learning is possible. \n- Machine learning and data analysis are powerful ways to identify and predict churn.\n- Churn is a one of the biggest problem in the telecom industry. \n- Research has shown that the average monthly churn rate among the top 4 wireless carriers in the US is 1.9% - 2%.","metadata":{}},{"cell_type":"markdown","source":"<font color = 'blue'>\nContent: \n\n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n    * [Univariate Variable Analysis](#3)\n        * [Categorical Variable](#4)\n        * [Numerical Variable](#5)\n1. [Basic Data Analysis](#6)\n1. [Missing Value](#7)\n    * [Find Missing Value](#8)\n    * [Fill Missing Value](#9)\n1. [Visualization](#10)    \n    * [Box plot of numerical features](#11)\n1. [Outlier Detection](#12)\n1. [Feature Engineering](#13)    \n    * [One-hot encoding](#14)\n    * [Ascending ranking of correlations between feaures and churn](#15)\n1. [Modeling](#16)\n    * [Train - Test Split](#17)        \n    * [Trial and Conclusion](#18)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#19)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt # for visualization\nimport seaborn as sns # for visualization\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") #Ignore certain system-wide alerts\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n# 1 | Load and Check Data","metadata":{}},{"cell_type":"code","source":"d = pd.read_csv('../input/telecom-customer/Telecom_customer churn.csv')\ndf = d.copy() # I just made a copy for easier experimentation during code writing.\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape) # row x columns of data\nprint(df.ndim) # dimension of data\nprint(df.size) # size of data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include=['O']) # Only object columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe() # Only numerical columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The table above has a statistical summary of the dataset. \n- It contains number, mean, standard deviation, minimum and maximum values for each feature. \n- Although the values in the table provide a summary of the data set, they do not make sense for the machine learning model.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n# 2 | Variable Description\n- 1 rev_Mean: Mean monthly revenue (charge amount)\n- 2 mou_Mean: Mean number of monthly minutes of use\n- 3 totmrc_Mean: Mean total monthly recurring charge\n- 4 da_Mean: Mean number of directory assisted calls\n- 5 ovrmou_Mean: Mean overage minutes of use\n- 6 ovrrev_Mean: Mean overage revenue\n- 7 vceovr_Mean: Mean revenue of voice overage\n- 8 datovr_Mean: Mean revenue of data overage\n- 9 roam_Mean: Mean number of roaming calls\n- 10 change_mou: Percentage change in monthly minutes of use vs previous three month average\n- 11 change_rev: Percentage change in monthly revenue vs previous three month average\n- 12 drop_vce_Mean: Mean number of dropped (failed) voice calls\n- 13 drop_dat_Mean: Mean number of dropped (failed) data calls\n- 14 blck_vce_Mean: Mean number of blocked (failed) voice calls\n- 15 blck_dat_Mean: Mean number of blocked (failed) data calls\n- 16 unan_vce_Mean: Mean number of unanswered voice calls\n- 17 unan_dat_Mean: Mean number of unanswered data calls\n- 18 plcd_vce_Mean: Mean number of attempted voice calls placed\n- 19 plcd_dat_Mean: Mean number of attempted data calls placed\n- 20 recv_vce_Mean: Mean number of received voice calls\n- 21 recv_sms_Mean: N\n- 22 comp_vce_Mean: Mean number of completed voice calls\n- 23 comp_dat_Mean: Mean number of completed data calls\n- 24 custcare_Mean: Mean number of customer care calls\n- 25 ccrndmou_Mean: Mean rounded minutes of use of customer care calls\n- 26 cc_mou_Mean: Mean unrounded minutes of use of customer care (see CUSTCARE_MEAN) calls\n- 27 inonemin_Mean: Mean number of inbound calls less than one minute\n- 28 threeway_Mean: Mean number of three way calls\n- 29 mou_cvce_Mean: Mean unrounded minutes of use of completed voice calls\n- 30 mou_cdat_Mean: Mean unrounded minutes of use of completed data calls\n- 31 mou_rvce_Mean: Mean unrounded minutes of use of received voice calls\n- 32 owylis_vce_Mean: Mean number of outbound wireless to wireless voice calls\n- 33 mouowylisv_Mean: Mean unrounded minutes of use of outbound wireless to wireless voice calls\n- 34 iwylis_vce_Mean: N\n- 35 mouiwylisv_Mean: Mean unrounded minutes of use of inbound wireless to wireless voice calls\n- 36 peak_vce_Mean: Mean number of inbound and outbound peak voice calls\n- 37 peak_dat_Mean: Mean number of peak data calls\n- 38 mou_peav_Mean: Mean unrounded minutes of use of peak voice calls\n- 39 mou_pead_Mean: Mean unrounded minutes of use of peak data calls\n- 40 opk_vce_Mean: Mean number of off-peak voice calls\n- 41 opk_dat_Mean: Mean number of off-peak data calls\n- 42 mou_opkv_Mean: Mean unrounded minutes of use of off-peak voice calls\n- 43 mou_opkd_Mean: Mean unrounded minutes of use of off-peak data calls\n- 44 drop_blk_Mean: Mean number of dropped or blocked calls\n- 45 attempt_Mean: Mean number of attempted calls\n- 46 complete_Mean: Mean number of completed calls\n- 47 callfwdv_Mean: Mean number of call forwarding calls\n- 48 callwait_Mean: Mean number of call waiting calls\n- 49 churn: Instance of churn between 31-60 days after observation date\n- 50 months: Total number of months in service\n- 51 uniqsubs: Number of unique subscribers in the household\n- 52 actvsubs: Number of active subscribers in household\n- 53 new_cell: New cell phone user\n- 54 crclscod: Credit class code\n- 55 asl_flag: Account spending limit\n- 56 totcalls: Total number of calls over the life of the customer\n- 57 totmou: Total minutes of use over the life of the cus\n- 58 totrev: Total revenue\n- 59 adjrev: Billing adjusted total revenue over the life of the customer\n- 60 adjmou: Billing adjusted total minutes of use over the life of the customer\n- 61 adjqty: Billing adjusted total number of calls over the life of the customer\n- 62 avgrev: Average monthly revenue over the life of the customer\n- 63 avgmou: Average monthly minutes of use over the life of the customer\n- 64 avgqty: Average monthly number of calls over the life of the customer\n- 65 avg3mou: Average monthly minutes of use over the previous three months\n- 66 avg3qty: Average monthly number of calls over the previous three months\n- 67 avg3rev: Average monthly revenue over the previous three months\n- 68 avg6mou: Average monthly minutes of use over the previous six months\n- 69 avg6qty: Average monthly number of calls over the previous six months\n- 70 avg6rev: Average monthly revenue over the previous six months\n- 71 prizm_social_one: Social group letter only\n- 72 area: Geogrpahic area\n- 73 dualband: Dualband\n- 74 refurb_new: Handset: refurbished or new\n- 75 hnd_price: Current handset price\n- 76 phones: Number of handsets issued\n- 77 models: Number of models issued\n- 78 hnd_webcap: Handset web capability\n- 79 truck: Truck indicator\n- 80 rv: RV indicator\n- 81 ownrent: Home owner/renter status\n- 82 lor: Length of residence\n- 83 dwlltype: Dwelling Unit type\n- 84 marital: Marital Status\n- 85 adults: Number of adults in household\n- 86 infobase: InfoBase match\n- 87 income: Estimated income\n- 88 numbcars: Known number of vehicles\n- 89 HHstatin: Premier household status indicator\n- 90 dwllsize: Dwelling size\n- 91 forgntvl: Foreign travel dummy variable\n- 92 ethnic: Ethnicity roll-up code\n- 93 kid0_2: Child 0 - 2 years of age in household\n- 94 kid3_5: Child 3 - 5 years of age in household\n- 95 kid6_10: Child 6 - 10 years of age in household\n- 96 kid11_15: Child 11 - 15 years of age in household\n- 97 kid16_17: Child 16 - 17 years of age in household\n- 98 creditcd: Credit card indicator\n- 99 eqpdays: Number of days (age) of current equipment\n- 100 Customer_ID: N","metadata":{}},{"cell_type":"code","source":"# We want to observe the types of variables in the dataset and whether they contain null values.\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from the data description, we can see that Customer_ID is unique - therefor it not provides us information we can learn.\ndf.drop([\"Customer_ID\"], axis = 1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We want to list the columns in 3 categories.\ndef columns_categories(data_set):\n    object_columns = []\n    float_columns = []\n    int_columns = []\n    other_columns = []\n    n,m,s=0,0,0\n    for i in data_set.columns.values:\n        if data_set[i].dtypes=='object':\n            object_columns.append(i)\n            n+=1\n        if data_set[i].dtypes=='int':\n            int_columns.append(i)\n            m+=1\n        if data_set[i].dtypes=='float':\n            float_columns.append(i)\n            s+=1\n    print('object(',n,'):\\n',object_columns)\n    print('int(',m,'):\\n',int_columns)\n    print('float(',s,'):\\n',float_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_categories(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n# Univariate Variable Analysis\n- Categorical Variable: 'churn', 'new_cell', 'crclscod', 'asl_flag', 'prizm_social_one', 'area', 'dualband', 'refurb_new', 'hnd_webcap', 'ownrent', 'dwlltype', 'marital', 'infobase', 'HHstatin', 'dwllsize', 'ethnic', 'kid0_2', 'kid3_5', 'kid6_10', 'kid11_15', 'kid16_17', 'creditcd'\n\n- Numerical Variable: 'months', 'uniqsubs', 'actvsubs', 'totcalls', 'adjqty', 'avg3mou', 'avg3qty', 'avg3rev', 'rev_Mean', 'mou_Mean', 'totmrc_Mean', 'da_Mean', 'ovrmou_Mean', 'ovrrev_Mean', 'vceovr_Mean', 'datovr_Mean', 'roam_Mean', 'change_mou', 'change_rev', 'drop_vce_Mean', 'drop_dat_Mean', 'blck_vce_Mean', 'blck_dat_Mean', 'unan_vce_Mean', 'unan_dat_Mean', 'plcd_vce_Mean', 'plcd_dat_Mean', 'recv_vce_Mean', 'recv_sms_Mean', 'comp_vce_Mean', 'comp_dat_Mean', 'custcare_Mean', 'ccrndmou_Mean', 'cc_mou_Mean', 'inonemin_Mean', 'threeway_Mean', 'mou_cvce_Mean', 'mou_cdat_Mean', 'mou_rvce_Mean', 'owylis_vce_Mean', 'mouowylisv_Mean', 'iwylis_vce_Mean', 'mouiwylisv_Mean', 'peak_vce_Mean', 'peak_dat_Mean', 'mou_peav_Mean', 'mou_pead_Mean', 'opk_vce_Mean', 'opk_dat_Mean', 'mou_opkv_Mean', 'mou_opkd_Mean', 'drop_blk_Mean', 'attempt_Mean', 'complete_Mean', 'callfwdv_Mean', 'callwait_Mean', 'totmou', 'totrev', 'adjrev', 'adjmou', 'avgrev', 'avgmou', 'avgqty', 'avg6mou', 'avg6qty', 'avg6rev', 'hnd_price', 'phones', 'models', 'truck', 'rv', 'lor', 'adults', 'income', 'numbcars', 'forgntvl', 'eqpdays'","metadata":{}},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n## Categorical Variable","metadata":{}},{"cell_type":"code","source":"obj_col = df.select_dtypes(include = 'object').columns\nobj_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Churn relation with categorical columns","metadata":{}},{"cell_type":"code","source":"# new_cell vs churn\nsns.countplot(x= \"new_cell\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('new_cell')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# crclscod vs churn\nsns.countplot(x= \"crclscod\", hue=\"churn\", data=df);\nplt.xticks(rotation = 90)\nplt.show()\ndf.groupby('crclscod')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# asl_flag vs churn\nsns.countplot(x= \"asl_flag\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('asl_flag')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prizm_social_one vs churn\nsns.countplot(x= \"prizm_social_one\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('prizm_social_one')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# area vs churn\nsns.countplot(x= \"area\", hue=\"churn\", data=df);\nplt.xticks(rotation = 90)\nplt.show()\ndf.groupby('area')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dualband vs churn\nsns.countplot(x= \"dualband\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('dualband')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# refurb_new vs churn\nsns.countplot(x= \"refurb_new\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\n\ndf.groupby('refurb_new')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hnd_webcap vs churn\nsns.countplot(x= \"hnd_webcap\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('hnd_webcap')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"ownrent\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('ownrent')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"dwlltype\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('dwlltype')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"marital\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('marital')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"infobase\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('infobase')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"HHstatin\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('HHstatin')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"dwllsize\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('dwllsize')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= \"ethnic\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('ethnic')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creditcd vs churn\nsns.countplot(x= \"creditcd\", hue=\"churn\", data=df);\nplt.xticks()\nplt.show()\ndf.groupby('creditcd')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n## Numerical Variable","metadata":{}},{"cell_type":"code","source":"df.iloc[:,:].hist(bins=50,figsize=(23,74),layout=(20,4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n# 3 | Basic Data Analysis","metadata":{}},{"cell_type":"code","source":"stay = df[(df['churn'] ==0) ].count()[1]\nchurn = df[(df['churn'] ==1) ].count()[1]\nprint (\"num of pepole who stay: \"+ str(stay))\nprint (\"num of pepole who churn: \"+ str(churn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ratio of those who churn and those who don't\nsizes = [48401,47647]\nlabels='NO','YES'\nexplode = (0, 0.1)  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode,autopct='%1.1f%%',shadow=True, startangle=75 )\nax1.axis('equal') \nax1.set_title(\"Client Churn Distribution\")\n\nax1.legend(labels)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n# 4 | Missing Value\n- Find Missing Value\n- Fill Missing Value","metadata":{}},{"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n## Find Missing Value","metadata":{}},{"cell_type":"code","source":"df.columns[df.isnull().any()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features with missing values\nmiss = df.isnull().sum().sort_values(ascending = False).head(44)\nmiss_per = (miss/len(df))*100\n\n# Percentage of missing values\npd.DataFrame({'No. missing values': miss, '% of missing data': miss_per.values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n## Fill Missing Value","metadata":{}},{"cell_type":"code","source":"# We dropped the columns that seem to have no significant contribution to the model.\ndf.drop(['numbcars','dwllsize','HHstatin','ownrent','dwlltype','lor','income','adults','prizm_social_one','infobase','crclscod'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hnd_webcap']=df['hnd_webcap'].fillna('UNKW') # Handset web capability\n\ndf['avg6qty']=df['avg6qty'].fillna(df['avg6qty'].mean()) # Billing adjusted total number of calls over the life of the customer\ndf['avg6rev']=df['avg6rev'].fillna(df['avg6rev'].mean()) # Average monthly revenue over the life of the customer\ndf['avg6mou']=df['avg6mou'].fillna(df['avg6mou'].mean()) # Average monthly minutes of use over the life of the customer\n\ndf['change_mou']=df['change_mou'].fillna(df['change_mou'].mean()) # Percentage change in monthly minutes of use vs previous three month average\ndf['change_rev']=df['change_rev'].fillna(df['change_rev'].mean()) # Percentage change in monthly revenue vs previous three month average\n\ndf['rev_Mean']=df['rev_Mean'].fillna(df['rev_Mean'].mean())\ndf['totmrc_Mean']=df['totmrc_Mean'].fillna(df['totmrc_Mean'].mean())\ndf['da_Mean']=df['da_Mean'].fillna(df['da_Mean'].mean())\ndf['ovrmou_Mean']=df['ovrmou_Mean'].fillna(df['ovrmou_Mean'].mean())\ndf['ovrrev_Mean']=df['ovrrev_Mean'].fillna(df['ovrrev_Mean'].mean())\ndf['vceovr_Mean']=df['vceovr_Mean'].fillna(df['vceovr_Mean'].mean())\ndf['datovr_Mean']=df['datovr_Mean'].fillna(df['datovr_Mean'].mean())\ndf['roam_Mean']=df['roam_Mean'].fillna(df['roam_Mean'].mean())\ndf['mou_Mean']=df['mou_Mean'].fillna(df['mou_Mean'].mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#VISUALIZATION OF NAN  VALUES\nimport missingno as msno\nmsno.matrix(df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(df.isnull().sum()>0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_categories(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = ['months', 'uniqsubs', 'actvsubs', 'totcalls', 'avg3qty', 'avg3rev','rev_Mean', 'mou_Mean', 'totmrc_Mean', 'da_Mean', 'ovrmou_Mean', 'datovr_Mean', \n                      'roam_Mean', 'change_mou', 'change_rev', 'drop_vce_Mean', 'drop_dat_Mean', 'blck_vce_Mean', 'blck_dat_Mean', 'unan_vce_Mean', 'unan_dat_Mean', \n                      'plcd_vce_Mean', 'plcd_dat_Mean', 'recv_vce_Mean', 'recv_sms_Mean', 'custcare_Mean', 'ccrndmou_Mean', 'threeway_Mean', 'mou_cvce_Mean', \n                      'mou_cdat_Mean', 'mou_rvce_Mean', 'owylis_vce_Mean', 'mouowylisv_Mean', 'iwylis_vce_Mean', 'mouiwylisv_Mean', 'peak_vce_Mean', 'peak_dat_Mean', \n                      'mou_peav_Mean', 'mou_pead_Mean', 'opk_vce_Mean', 'opk_dat_Mean', 'mou_opkv_Mean', 'drop_blk_Mean', 'callfwdv_Mean', 'callwait_Mean', 'totmou', \n                      'totrev', 'avgrev', 'avgmou', 'avgqty', 'avg6mou', 'avg6rev', 'hnd_price', 'phones', 'models', 'truck', 'rv', 'forgntvl', 'eqpdays']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in numerical_features:    \n    f_sqrt= (lambda x: np.sqrt(abs(x)) if (x>=1) or (x<=-1) else x)\n    df[i] = df[i].apply(f_sqrt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"10\"></a><br>\n# 5 | Visualization","metadata":{}},{"cell_type":"markdown","source":"<a id = \"11\"></a><br>\n# Box plot of numerical features","metadata":{}},{"cell_type":"code","source":"# Box plot of numerical features\nfig, ax = plt.subplots(15, 4, figsize = (20, 50))\nax = ax.flatten()\nfor i, c in enumerate(numerical_features):\n    sns.boxplot(x = df[c], ax = ax[i], palette = 'Set3')\n# plt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- columns without outliers : \n- 'months', 'uniqsubs', 'actvsubs', 'totcalls', 'avg3qty', 'avg3rev','rev_Mean', 'mou_Mean', 'totmrc_Mean', 'da_Mean', 'ovrmou_Mean', 'datovr_Mean', \n   'roam_Mean', 'change_mou', 'change_rev', 'drop_vce_Mean', 'drop_dat_Mean', 'blck_vce_Mean', 'blck_dat_Mean', 'unan_vce_Mean', 'unan_dat_Mean', \n   'plcd_vce_Mean', 'plcd_dat_Mean', 'recv_vce_Mean', 'recv_sms_Mean', 'custcare_Mean', 'ccrndmou_Mean', 'threeway_Mean', 'mou_cvce_Mean', \n   'mou_cdat_Mean', 'mou_rvce_Mean', 'owylis_vce_Mean', 'mouowylisv_Mean', 'iwylis_vce_Mean', 'mouiwylisv_Mean', 'peak_vce_Mean', 'peak_dat_Mean', \n   'mou_peav_Mean', 'mou_pead_Mean', 'opk_vce_Mean', 'opk_dat_Mean', 'mou_opkv_Mean', 'drop_blk_Mean', 'callfwdv_Mean', 'callwait_Mean', 'totmou', \n   'totrev', 'avgrev', 'avgmou', 'avgqty', 'avg6mou', 'avg6rev', 'hnd_price', 'phones', 'models', 'truck', 'rv', 'forgntvl', 'eqpdays'","metadata":{}},{"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n# 6 | Outlier Detection","metadata":{}},{"cell_type":"code","source":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return outlier_indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[detect_outliers(df,['uniqsubs', 'actvsubs'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop outliers\ndf = df.drop(detect_outliers(df,['uniqsubs', 'actvsubs']),axis = 0).reset_index(drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n# 7 | Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"<a id = \"14\"></a><br>\n# One-hot encoding\n- Before looking at the correlation, let's make the categorical variables numerical with get_dummies.","metadata":{}},{"cell_type":"code","source":"# Unique variables of object columns\nencoding_col=[]\nfor i in df.select_dtypes(include='object'):   \n    print(i,'-->',df[i].nunique())\n    encoding_col.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding for variables with more than 2 categories\ndf2 = df.copy()\ndf2 = pd.get_dummies(df2, drop_first=True, columns = encoding_col, prefix = encoding_col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df.shape)\ndisplay(df2.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n# print(corr_matrix)\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n# Drop features \ndf.drop(df[to_drop], axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"15\"></a><br>\n# Ascending ranking of correlations between feaures and churn","metadata":{}},{"cell_type":"code","source":"c = df.corr()['churn'].abs()\nsc = c.sort_values()\nsc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = dict(sc.tail(40))\nb = a.keys()\nprint(sorted(b))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30,11))\nsns.heatmap(df2[b].corr(), annot = True, fmt = \".2f\",robust=True,linewidths=1.3,linecolor = 'gold')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Correlation of \"churn\" with other variables:\nplt.figure(figsize=(15,8))\ndf2[b].corr()['churn'].sort_values(ascending = False).plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"16\"></a><br>\n# 8 | Modelling","metadata":{}},{"cell_type":"markdown","source":"<a id = \"17\"></a><br>\n# Train - Test Split","metadata":{}},{"cell_type":"code","source":"# Import Machine learning algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Import metric for performance evaluation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix, ConfusionMatrixDisplay\n\n#Split data into train and test sets\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dependent and independent variables were determined.\nX = df2.drop('churn', axis=1)\ny = df2['churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining the modelling function\ndef modeling(alg, alg_name, params={}):\n    model = alg(**params) #Instantiating the algorithm class and unpacking parameters if any\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n      \n    #Performance evaluation\n    def print_scores(alg, y_true, y_pred):\n        print(alg_name)\n        acc_score = accuracy_score(y_true, y_pred)\n        print(\"accuracy: \",acc_score)\n        pre_score = precision_score(y_true, y_pred)\n        print(\"precision: \",pre_score)\n        rec_score = recall_score(y_true, y_pred)                            \n        print(\"recall: \",rec_score)\n        f_score = f1_score(y_true, y_pred, average='weighted')\n        print(\"f1_score: \",f_score)        \n    print_scores(alg, y_test, y_pred)\n    \n    \n    cm = confusion_matrix(y_test, y_pred)\n    #Create the Confusion Matrix Display Object(cmd_obj). \n    cmd_obj = ConfusionMatrixDisplay(cm, display_labels=['churn', 'notChurn'])\n\n    #The plot() function has to be called for the sklearn visualization\n    cmd_obj.plot()\n\n    #Use the Axes attribute 'ax_' to get to the underlying Axes object.\n    #The Axes object controls the labels for the X and the Y axes. It also controls the title.\n    cmd_obj.ax_.set(\n                    title='Sklearn Confusion Matrix with labels!!', \n                    xlabel='Predicted Churn', \n                    ylabel='Actual Churn')\n    #Finally, call the matplotlib show() function to display the visualization of the Confusion Matrix.\n    plt.show()\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running RandomForestClassifier model\nRF_model = modeling(RandomForestClassifier, 'Random Forest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGBM model\nLGBM_model = modeling(lgb.LGBMClassifier, 'Light GBM')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Decision tree\ndt_model = modeling(DecisionTreeClassifier, \"Decision Tree Classification\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Naive bayes \nnb_model = modeling(GaussianNB, \"Naive Bayes Classification\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ada Boost\nada_model=modeling(AdaBoostClassifier, \"Ada Boost Classifier\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting\ngbm_model=modeling(GradientBoostingClassifier, \"Gradient Boosting Classifier\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"18\"></a><br>\n## Trial and Conclusion","metadata":{}},{"cell_type":"code","source":"clf = lgb.LGBMClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmd_obj = ConfusionMatrixDisplay(cm, display_labels=['1', '0'])\ncmd_obj.plot()\ncmd_obj.ax_.set(\n                    title='Confusion Matrix with labels!!', \n                    xlabel='Predicted Churn', \n                    ylabel='Actual Churn'\n                    )\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'Models':['RF','LGBM','DT', 'NB','ADA','GBM'], 'Prediction':[0.616, 0.631, 0.552, 0.539 ,0.614, 0.626]})\nax = df.plot.barh(x='Models', y='Prediction', rot=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"19\"></a><br>\n# Hyperparameter Tuning -- Grid Search -- Cross Validation\nWe will compare 2 ML classifier and evaluate mean accuracy of each of them by stratified cross validation.\n* Random Forest\n* Light GBM","metadata":{}},{"cell_type":"code","source":"random_state = 42\nclassifier = [RandomForestClassifier(random_state = random_state),\n             lgb.LGBMClassifier(random_state = random_state)]\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlgbm_params = {'n_estimators': [100, 500, 1000],\n                'subsample': [0.6, 0.8, 1.0],\n                'max_depth': [3, 4, 5],\n                'learning_rate': [0.1,0.01,0.02],\n                \"min_child_samples\": [5,10,20]}\n\nclassifier_param = [rf_param_grid,                   \n                   lgbm_params]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cv_result = []\n# best_estimators = []\n# for i in range(len(classifier)):\n#     clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n#     clf.fit(X_train,y_train)\n#     cv_result.append(clf.best_score_)\n#     best_estimators.append(clf.best_estimator_)\n#     print(cv_result[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[ \"RandomForestClassifier\",\"LGBMClassifier\"]})\n\n# g = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\n# g.set_xlabel(\"Mean Accuracy\")\n# g.set_title(\"Cross Validation Scores\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}