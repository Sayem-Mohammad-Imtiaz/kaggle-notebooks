{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"We're interested in Soybean prices. We know that supply and demand affect price, so we look into the data ... In the US, soybeans are planted from around May and harvested from around September. From May of each year the USDA projects the size of the crop, along with the various usages of soybeans and the estimated ending stocks at the end of the (marketing) year.\n\nThe simplest measure of scarcity is \"Stocks to Use\" ratio = Estimated Ending Stocks / Estimated Yearly Use"},{"metadata":{"trusted":false,"_uuid":"ff9727797a87802210a30b93782eceb6cb4d2a86"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport scipy.stats as stats\n\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\ndateparse0 = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d') for d in dates]\nsoy = pd.read_csv(\"../input/soybean_nearby.txt\", parse_dates = [\"dates\"], date_parser=dateparse0)\nsoy.index = soy[\"dates\"]\ndel soy[\"dates\"]\n\ndef get_SupplyUse( fname_proj, MktYrs ):\n    ByYear_SupplyUse = {}\n    WASDE_dtstrs = []\n    for mktyr in MktYrs:\n        data = pd.read_csv( \"../input/\" + fname_proj + mktyr + \".csv\" )\n        data[\"date\"] = pd.to_datetime(data[\"Date\"])\n        data.index = data['date']\n        del data[\"date\"]\n        data[\"s2u\"] = data[\"Ending Stocks\"] / data[\"Total Use\"]\n        ByYear_SupplyUse[ mktyr ] = data\n        WASDE_dtstrs = np.hstack( (WASDE_dtstrs, data[\"Date\"].values) )\n    return ByYear_SupplyUse, np.unique(WASDE_dtstrs)\n\nSupplyUse, WASDE_dtStrs = get_SupplyUse( \"USDAProj_Soybean_\", [\"2007to2008\",\"2008to2009\",\"2009to2010\",\"2010to2011\",\\\n                                                            \"2011to2012\",\"2012to2013\",\"2013to2014\",\"2014to2015\",\\\n                                                            \"2015to2016\",\"2016to2017\",\"2017to2018\"])","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"df96fcd009c1718a3ea34c50ff5afbb3bcc6228a"},"cell_type":"markdown","source":"The USDA projections for the current calendar year begin in May"},{"metadata":{"trusted":false,"_uuid":"fddddddd21c5a95d36938959cc4aede74625247d"},"cell_type":"code","source":"def corn_soy_mktyrs( dt1 ):\n    #USDA Projections for new crop year (harvested and marketed in Sep) start in May.\n    if( dt1.month < 5 ): \n        NewCrop_mktyr = str( dt1.year - 1 ) + \"to\" + str( dt1.year )\n        OldCrop_mktyr = str( dt1.year - 2 ) + \"to\" + str( dt1.year - 1 )\n    else:\n        NewCrop_mktyr = str( dt1.year ) + \"to\" + str( dt1.year + 1 )\n        OldCrop_mktyr = str( dt1.year - 1 ) + \"to\" + str(dt1.year )\n    return( [ NewCrop_mktyr, OldCrop_mktyr ] )\n\nNearby = []\nS2U = []\nplot_dts = []\nfor dtstr in WASDE_dtStrs:\n    Curr_dt = datetime.strptime(dtstr, \"%Y/%m/%d\")\n    if( Curr_dt > pd.to_datetime( soy.index[0]) ):\n        new_crop_year, old_crop_year = corn_soy_mktyrs( Curr_dt )\n        s2u = SupplyUse[ new_crop_year ][\"s2u\"][:Curr_dt][-1]\n        S2U.append( s2u )\n        Nearby.append( soy[:Curr_dt][\"nearby_close\"][-1])\n        plot_dts.append( Curr_dt )\n\nplt.subplots(figsize=(15,5))\nplt.plot_date( plot_dts, Nearby, 'bo-')\nplt.ylabel(\"Soybean Nearby Price (cents)\")\n\nax2 = plt.twinx()\nax2.plot_date( plot_dts, S2U, 'ro-')\nplt.xlabel('date')\nax2.set_ylabel(\"Stocks to Use Ratio\")\nplt.show()        \n        \nInvS2U = np.divide( 1.0, np.array(S2U) )\nplt.plot( S2U, Nearby, 'o', color=\"blue\" )\nplt.xlabel(\"Projected Ending Stocks to Total Use Ratio\")\nplt.ylabel(\"Soybean Nearby Price (cents)\")\nplt.title(\"Price vs Projected Scarcity\")\nplt.show()\n\nplt.plot( InvS2U, Nearby, 'o', color=\"blue\" )\nplt.xlabel(\"1/Projected Ending Stocks to Total Use Ratio\")\nplt.ylabel(\"Soybean Nearby Price (cents)\")\nplt.title(\"Price vs 1/Projected Scarcity\")\nplt.show()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"a2a35c4d608141cfc6e5be292756ef7714ddc798"},"cell_type":"markdown","source":"It's more sensible to look at an individual contract rather than the nearby prices."},{"metadata":{"trusted":false,"_uuid":"ea0d566ed0becf032fa488f73347a9018ed67fbd"},"cell_type":"code","source":"soy = pd.read_csv(\"../input/soybean_JUL14.txt\", index_col=0)\nsoy = soy.loc[ soy[\"TotalOpenInt\"] != 0 ] # the original data source had some erroneous files with TotalOpenInterest = Zero\nplt.subplots(figsize=(15,5))\nplt.plot_date( soy.index, soy[\"Close\"], '.-', color=\"blue\")\nplt.ylabel(\"Soybean JUL14 Close Price (cents)\")\nax2 = plt.twinx()\nax2.plot_date( soy.index, soy[\"TotalOpenInt\"], '.-', color=\"black\")\nax2.set_ylabel(\"Total Open Interest\")\nplt.show()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"9e10ed28850fc8d6f104c94d43ae70505f73e90e"},"cell_type":"markdown","source":"it looks like fluctuations in Price switch between moving in step (or in counterpart) with fluctuations in Total Open Interest (for all Soybean contracts). We can register this change using sliding Spearman correlation."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c448974fa9857950c79ce5e1e53f1a42c1d6dd6f"},"cell_type":"code","source":"df = soy[[\"Close\", \"TotalOpenInt\"]].copy()\nspearmanWindow = 10 \nspearmanVec = [ None for p in range(0,spearmanWindow)]\nfor ii in range(10, len(df[\"Close\"])):\n    newR = stats.spearmanr( df[\"Close\"][ ii-10: ii], df[\"TotalOpenInt\"][ ii-10: ii])[0] \n    spearmanVec.append( newR )\ndf[\"Spearman\"] = spearmanVec","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"b66f78228949a12e2a12b20ddee4c32bfea7433d"},"cell_type":"markdown","source":"Adding Stocks to Use Ratio into our data frame"},{"metadata":{"trusted":false,"_uuid":"5d87cff11da6b93efe2538e8b0ae0583550b443c"},"cell_type":"code","source":"last_S2U = []\nfor ii in df.index:\n    Curr_dt = pd.to_datetime(ii )\n    new_crop_year, old_crop_year = corn_soy_mktyrs( Curr_dt  )\n    s2u_new_vals = SupplyUse[ new_crop_year ][\"s2u\"][:ii]\n    if len(s2u_new_vals ): # In May before the WASDE report date there'll be nothing returned here\n        s2u = s2u_new_vals[-1]\n    else:\n        s2u = SupplyUse[ old_crop_year ][\"s2u\"][:ii][-1]\n    last_S2U.append( s2u )\ndf[\"S2U\"] = last_S2U\ndf[\"InvS2U\"] = df[\"S2U\"].apply( lambda x: 1.0 / float( x ) )\ndf = df.iloc[ spearmanWindow:, :] # need to drop the first NaN entries\ndf.head(5)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"c7106004ca5d432f7b85a153f82486156d8f6439"},"cell_type":"markdown","source":"So. Can we build any kind of model with this data? Will the behaviour of the preceding X days help us forecast what tomorrow's price will be? Let's try building an LSTM model ..."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bab878a8e8a361078b30dd7732f966f257b7a024"},"cell_type":"code","source":"# Some of the columns we don't normalize until after we've split into Test and Training Sets.\n# this is because we want them scaled over the whole training set, not just small windows\ndef normalize_windows( window_data, colsToNormalize, colsNotNormalized ):\n    normalized_data=[]\n    normalizing_vals=[]\n    for window in window_data:\n        window_norm = np.empty_like(window)\n        norm_vals = []\n        for c in colsToNormalize:\n            firstVal = float(window[0,c])\n            norm_col = [ ((float(p) / firstVal ) - 1 ) for p in window[:,c]]\n            window_norm[:,c] = norm_col \n            norm_vals.append( firstVal )\n        for cc in colsNotNormalized:    \n            window_norm[ :, cc ] = [ p for p in window[:,cc]] \n        normalized_data.append( window_norm )\n        normalizing_vals.append( norm_vals )\n    return [ normalized_data, np.array(normalizing_vals) ]    \n\ndef denormalize_windows( scValues, norm_values ):\n    output = []\n    for ii in range( scValues.shape[0]):\n        scV_row = scValues[ii,:]\n        norm_row = norm_values[ii]\n        out_row = np.multiply( norm_row, scV_row + 1 )   \n        output.append( list(out_row) )\n    return np.array(output )  \n\ndata = df[[\"Close\", \"TotalOpenInt\", \"Spearman\", \"InvS2U\"]].values  \ntrain_percent = 0.9\nseq_len = 20\nsequence_length = seq_len + 1\nresult = []\nfor index in range( len(data) - sequence_length): \n    result.append( data[ index: index + sequence_length ]) \n\nresult, norm_vals = normalize_windows( result, [0],  [1,2,3] )   ","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"236c307c4cfd32df35597421543cd4a2dfcb910a"},"cell_type":"markdown","source":"Split the da into Train & Test sets"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c1a3e74fcab75fc25376a925d3f84a4db4f227af"},"cell_type":"code","source":"result = np.array( result ) \nrow = round( train_percent * result.shape[0]) \n    \nX_train = result[:int(row), :-1, 1:]\ny_train = result[:int(row), -1, 0]\nnorm_vals_train = norm_vals[:int(row)]\n    \nX_test = result[ int(row):, :-1, 1:]\ny_test = result[ int(row):, -1,  0]\nnorm_vals_test = norm_vals[int(row):]\n\nscX = MinMaxScaler(feature_range=(-1,1))\n# Want to fit the Scaler to only the training sets\ndummyX = X_train.copy()\ntallX = dummyX[0].copy()\nfor ii in range( 1, len(dummyX) ):\n    tallX = np.append( tallX, dummyX[ii] , axis=0 )\nscX.fit( tallX )\n\nfor ii in range( 0, len(X_train)):\n    X_train[ii] = scX.transform( X_train[ii] )\n\nfor ii in range( 0, len(X_test)):\n    X_test[ii] = scX.transform( X_test[ii] )\n\nscY = MinMaxScaler()\ny_train = scY.fit_transform( y_train.reshape(-1,1) )\ny_test = scY.transform( y_test.reshape(-1,1) )","execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f10e00ce230e7f968cd50104b83673cae788638a"},"cell_type":"markdown","source":"Build an LSTM Model"},{"metadata":{"trusted":false,"_uuid":"790dc088eef112b80b3c5e206392e224bf0297ac"},"cell_type":"code","source":"def build_model( layers ):\n    dropout = 0.2\n    model = Sequential()\n    # Add LSTM layer with 20% dropout regularization\n    model.add(LSTM(50, input_shape=(layers[0], layers[1]), return_sequences=True))\n    model.add(Dropout(dropout)) \n    model.add(LSTM(50, return_sequences=False))\n    model.add(Dropout(dropout)) \n    model.add(Dense(32, kernel_initializer=\"uniform\", activation=\"relu\"))\n    model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"linear\"))\n     \n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    return model\n\nmodel = build_model( [X_train.shape[1], X_train.shape[2],  1 ])\n\nmodel.fit( X_train, y_train, batch_size=32, epochs=100)\ny_pred = model.predict( X_test )","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"How does the output look?"},{"metadata":{"trusted":false,"_uuid":"7a3b06e4b82d9c4352e15f81e6a7d5dda9a09c24"},"cell_type":"code","source":"invSc_YPred = scY.inverse_transform( y_pred )\ninvSc_Y_test = scY.inverse_transform( y_test )\n\nplt.subplots(figsize=(15,5))\nplt.plot( invSc_YPred, marker=\".\", color=\"red\", label=\"Prediction\")\nplt.plot( invSc_Y_test, marker=\".\", color=\"blue\", label=\"Actual\")\nplt.legend(loc=\"best\")\nplt.title(\"Predicted and Actual\")\nplt.show()","execution_count":9,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ac6fcf96a8b5e8b9f8027578cd23002df9f6b69a"},"cell_type":"markdown","source":"And if we convert back to Prices?"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d4961ecee4280cf8f8f4ccadad0492d926de6601"},"cell_type":"code","source":"from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n                               AutoMinorLocator)\n\ninv_y_test = denormalize_windows( invSc_Y_test, norm_vals_test )\ninv_y_test_pred = denormalize_windows( invSc_YPred, norm_vals_test )\n\nplt.subplots(figsize=(15,5))\nplt.plot( inv_y_test_pred, marker='.', color=\"red\", label=\"Prediction\")\nplt.plot( inv_y_test, marker='.',  color=\"blue\", label=\"Actual\")\nplt.legend(loc=\"best\")\nplt.title(\"Close Pxs - Predicted and Actual\")\nax = plt.gca()\nmajorLocator = MultipleLocator(10)\nmajorFormatter = FormatStrFormatter('%d')\nminorLocator = MultipleLocator(2)\nax.xaxis.set_major_locator(majorLocator)\nax.xaxis.set_major_formatter(majorFormatter)\n    \n# for the minor ticks, use no labels; default NullFormatter\nax.xaxis.set_minor_locator(minorLocator)\nax.grid(which=\"minor\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2da05905dca8cd8f3dec326e238457e683bfa430"},"cell_type":"markdown","source":"We can also build a RandomForest (Long/Short) classification model based on the RNN – to predict the future direction at particular points in time. For the purposes of training the classification model, we take as ground truth, whether or not the mean value of the price in the subsequent 2 weeks was above or below the price at each turning point. This will probably be a future kernel."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f4ed77a4598a2a432a21d02407327519f3022da3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}