{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here I m going to run Support Vector machine with different kernels(linear,gaussian,polynomial) and also tune the various parameters such as C ,gamma and degree to find out the best performing model "},{"metadata":{"trusted":true},"cell_type":"code","source":"#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#sns.set()\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the comma separated values file into the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/voice.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking the correlation between each feature¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"corr = df.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['meanfreq'].drop('meanfreq', axis=0).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['meanfreq'] == corr['meanfreq'].drop('meanfreq', axis=0).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['meanfreq'].drop('meanfreq', axis=0)[corr['meanfreq'] == corr['meanfreq'].drop('meanfreq', axis=0).max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    print([corr[i].drop(i, axis=0)[corr[i] == corr[i].drop(i, axis=0).max()]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nax = sns.heatmap(\n    corr, \n    vmax=.8,\n    square=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking whether there is any null values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of male: {}\".format(df[df.label == 'male'].shape[0]))\nprint(\"Number of female: {}\".format(df[df.label == 'female'].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Separating features and labels\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X=df.iloc[:, :-1]\n#X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('label', axis=1)\nX.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Converting string value to int type for labels¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ny = df['label']\n# Encode label category\n# male -> 1\n# female -> 0\ngender_encoder = LabelEncoder()\ny = gender_encoder.fit_transform(y)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_encoder.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Standardisation\nStandardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). It is useful to standardize attributes for a model. Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the data to be between -1 and 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How StandardScaler works (starting)\n\nThe standard score of a sample x is calculated as:\n\nz = (x - u) / s\nwhere u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [[0, 0], [0, 0], [1, 1], [1, 1]]\nscaler = StandardScaler()\nprint(scaler.fit(data))\nprint(scaler.mean_)\nprint(scaler.transform(data))\nprint(scaler.transform([[2, 2]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((0.5 **2) * 4 / 4) ** 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(0 - 0.5)/0.25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array(data)\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.std(a, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(a, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"over--- How StandardScaler works"},{"metadata":{},"cell_type":"markdown","source":"# Splitting dataset into training set and testing set for better generalisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running SVM with default hyperparameter.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn import metrics\nsvc=SVC() #Default hyperparameters\nsvc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=svc.predict(X_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score for train:', metrics.accuracy_score(y_train,y_pred_train))\nprint('Accuracy Score for test:', metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Default Linear kernel¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='linear')\nsvc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=svc.predict(X_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score for train:', metrics.accuracy_score(y_train,y_pred_train))\nprint('Accuracy Score for test:', metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Default RBF kernel¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='rbf')\nsvc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=svc.predict(X_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score for train:', metrics.accuracy_score(y_train,y_pred_train))\nprint('Accuracy Score for test:', metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude from above that svm by default uses rbf kernel as a parameter for kernel\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Default Polynomial kernel\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='poly')\nsvc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train=svc.predict(X_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score for train:', metrics.accuracy_score(y_train,y_pred_train))\nprint('Accuracy Score for test:', metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Polynomial kernel is performing poorly.The reason behind this maybe it is overfitting the training dataset\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Performing K-fold cross validation with different kernels¶\n"},{"metadata":{},"cell_type":"markdown","source":"# CV on Linear kernel¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nsvc=SVC(kernel='linear')\nscores = cross_val_score(svc,X,y, cv=10, scoring='accuracy')#cv is cross validation\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see above how the accuracy score is different everytime.This shows that accuracy score depends upon how the datasets got split."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In K-fold cross validation we generally take the mean of all the scores.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# CV on rbf kernel¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='rbf', gamma='auto')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') #cv is cross validation\nprint(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV on Polynomial kernel¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='poly',gamma='auto')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') #cv is cross validation\nprint(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When K-fold cross validation is done we can see different score in each iteration.This happens because when we use train_test_split method,the dataset get split in random manner into testing and training dataset.Thus it depends on how the dataset got split and which samples are training set and which samples are in testing set.\n\nWith K-fold cross validation we can see that the dataset got split into 10 equal parts thus covering all the data into training as well into testing set.This is the reason we got 10 different accuracy score.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Taking all the values of C and checking out the accuracy score with kernel as linear.\n\n\nThe C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.\n\nThus for a very large values we can cause overfitting of the model and for a very small value of C we can cause underfitting.Thus the value of C must be chosen in such a manner that it generalised the unseen data well"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range = list(range(1, 26))\nacc_score=[]\nfor c in C_range:\n    svc = SVC(kernel='linear', C=c)\n    score = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(score.mean())\nprint(acc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(C_range, acc_score)\nplt.xticks(np.arange(0,27,2))\nplt.xlabel('Value of C for SVC')\nplt.ylabel('Cross-Validated Accuracy');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we can see that accuracy has been close to 97% for C=1 and C=6 and then it drops around 96.8% and remains constant.\n\nLet us look into more detail of what is the exact value of C which is giving us a good accuracy score"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_range=list(np.arange(0.1,6,0.1))\nacc_score=[]\nfor c in C_range:\n    svc = SVC(kernel='linear', C=c)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nC_values=list(np.arange(0.1,6,0.1))\nplt.plot(C_values,acc_score)\nplt.xticks(np.arange(0.0,6,0.3))\nplt.xlabel('Value of C for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score is highest for C=0.1.¶\n"},{"metadata":{},"cell_type":"markdown","source":"# Taking kernel as rbf and taking different values gamma¶\nTechnically, the gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range=[0.0001,0.001,0.01,0.1,1,10,100]\nacc_score=[]\nfor g in gamma_range:\n    svc = SVC(kernel='rbf', gamma=g)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range=[0.0001,0.001,0.01,0.1,1,10,100]\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(gamma_range,acc_score)\nplt.xlabel('Value of gamma for SVC ')\nplt.xticks(np.arange(0.0001,100,5))\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for gamma=10 and 100 the kernel is performing poorly.We can also see a slight dip in accuracy score when gamma is 1.Let us look into more details for the range 0.0001 to 0.1."},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range=[0.0001,0.001,0.01,0.1]\nacc_score=[]\nfor g in gamma_range:\n    svc = SVC(kernel='rbf', gamma=g)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range=[0.0001,0.001,0.01,0.1]\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(gamma_range,acc_score)\nplt.xlabel('Value of gamma for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score increases steadily and raches its peak at 0.01 and then decreases till gamma=1.Thus Gamma should be around 0.01.\n\nLet us look into more detail for gamma values"},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range=[0.01,0.02,0.03,0.04,0.05]\nacc_score=[]\nfor g in gamma_range:\n    svc = SVC(kernel='rbf', gamma=g)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma_range=[0.01,0.02,0.03,0.04,0.05]\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(gamma_range,acc_score)\nplt.xlabel('Value of gamma for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there is constant decrease in the accuracy score as gamma value increase.Thus gamma=0.01 is the best parameter."},{"metadata":{},"cell_type":"markdown","source":"# Taking polynomial kernel with different degree¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"degree=[2,3,4,5,6]\nacc_score=[]\nfor d in degree:\n    svc = SVC(kernel='poly', degree=d, gamma='auto')\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"degree=[2,3,4,5,6]\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(degree,acc_score,color='r')\nplt.xlabel('degrees for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score is high for third degree polynomial and then there is drop in the accuracy score as degree of polynomial increases.Thus increase in polynomial degree results in high complexity of the model and thus causes overfitting."},{"metadata":{},"cell_type":"markdown","source":"# Now performing SVM by taking hyperparameter C=0.1 and kernel as linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc= SVC(kernel='linear',C=0.1)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\naccuracy_score= metrics.accuracy_score(y_test,y_predict)\nprint(accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With K-fold cross validation(where K=10)¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nsvc=SVC(kernel='linear',C=0.1)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is slightly good without K-fold cross validation but it may fail to generalise the unseen data.Hence it is advisable to perform K-fold cross validation where all the data is covered so it may predict unseen data well."},{"metadata":{},"cell_type":"markdown","source":"# Now performing SVM by taking hyperparameter gamma=0.01 and kernel as rbf"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc= SVC(kernel='rbf',gamma=0.01)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\nmetrics.accuracy_score(y_test,y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With K-fold cross validation(where K=10)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='linear',gamma=0.01)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now performing SVM by taking hyperparameter degree=3 and kernel as poly¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc= SVC(kernel='poly',degree=3, gamma='auto')\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\naccuracy_score= metrics.accuracy_score(y_test,y_predict)\nprint(accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With K-fold cross validation(where K=10)¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(kernel='poly',degree=3, gamma='auto')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us perform Grid search technique to find the best parameter¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_model = SVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_parameters = {'C':(np.arange(0.1, 1, 0.1)), 'kernel':['linear'],\n                    'C':(np.arange(0.1, 1, 0.1)), 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel':['rbf'],\n                    'C':(np.arange(0.1, 1, 0.1)), 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel':['poly'], 'degree': [2,3,4]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmodel_svm = GridSearchCV(svm_model, tuned_parameters, cv=10, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel_svm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_svm.best_score_)\nprint(model_svm.best_params_)\nprint(model_svm.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmodel_svm = GridSearchCV(svm_model, tuned_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_svm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_svm.best_params_)\nprint(model_svm.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_svm.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# use different kernel train with different parameters."},{"metadata":{},"cell_type":"markdown","source":"For SVC default--rbf"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=svc.predict(X_test)\nprint(metrics.accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C':(np.arange(0.1, 1, 0.1)), 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel':['rbf']}\nmodel = GridSearchCV(SVC(), param_grid)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.best_params_)\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For linear kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C':(np.arange(0.1, 1, 0.1)), 'kernel':['linear']}\nmodel = GridSearchCV(SVC(), param_grid)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.best_params_)\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Poly kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C':(np.arange(0.1, 1, 0.1)), 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel':['poly'], 'degree': [2,3,4]}\nmodel = GridSearchCV(SVC(), param_grid)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the best model goes to rbf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# re-run the rbf kernel\nparam_grid = {'C':(np.arange(0.1, 1, 0.1)), 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel':['rbf']}\nmodel = GridSearchCV(SVC(), param_grid)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for original training data score\ny_pred_train = model.predict(X_train)\nprint(metrics.accuracy_score(y_train, y_pred_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#score for original training data\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it looks like the bias and variance for rbf are well balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the rbf kernel with K-fold\nparam_grid = {'C':(np.arange(0.1, 1, 0.1)), 'gamma': [0.01,0.02,0.03,0.04,0.05], 'kernel':['rbf']}\nmodel = GridSearchCV(SVC(), param_grid, cv=10, scoring='accuracy')\nmodel.fit(X_train, y_train)\nprint(model.best_score_)\nprint(model.best_params_)\nprint(model.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We used k-fold to produce the best model and we will continue to predict X_test data to be able to get the accuracy score.\nmodel_k_fold = model.best_estimator_\ny_pred_k_fold = model_k_fold.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred_k_fold))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This verified that our paramters:\n    \n    {'C': 0.9, 'gamma': 0.05, 'kernel': 'rbf'}\n    \n    {'C': 0.7000000000000001, 'gamma': 0.04, 'kernel': 'rbf'}\n    \nproduced the same result.\n\nIt proves that data spliting for this data set does not weigh in much"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}