{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Bank marketing analyis using Machine Learning</center></h1>","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement","metadata":{}},{"cell_type":"markdown","source":"Improve bank marketing of a bank by analyzing their past marketing campaign data and recommending which customer to target.\n\nThe aim of this project is to devise such a machine leaning prediction algorithm, the bank can better target its customers and channelize its mrketing efforts. ","metadata":{}},{"cell_type":"markdown","source":"### Data Attributes ","metadata":{}},{"cell_type":"markdown","source":"The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n\n\n**Attribute Information:**\n\n1 - age (numeric)\n\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n\n8 - contact: contact communication type (categorical: 'cellular','telephone')\n\n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n16 - balance: balance od the customer\n\n17 - y - has the client subscribed a term deposit? (binary: 'yes','no')","metadata":{}},{"cell_type":"markdown","source":"<a id='import_lib'></a>\n## 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"# import 'Pandas' \nimport pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import MinMaxScaler \n\n\n# import various functions from sklearn \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom IPython.display import Image \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import StackingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='set_options'></a>\n## 2. Set Options","metadata":{}},{"cell_type":"code","source":"# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n# return an output value upto 6 decimals\npd.options.display.float_format = '{:.6f}'.format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Read_Data'></a>\n## 3. Read Data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/bank-marketing-dataset/bank.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='data_preparation'></a>\n## 4. Data Analysis and Preparation","metadata":{}},{"cell_type":"markdown","source":"<a id='Data_Understanding'></a>\n### 4.1 Understand the Dataset","metadata":{}},{"cell_type":"markdown","source":"<a id='Data_Shape'></a>\n### 4.1.1 Data Dimension","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Data_Types'></a>\n### 4.1.2 Data Types\nData has a variety of data types. The main types stored in pandas dataframes are object, float, int64, bool and datetime64. In order to learn about each attribute, it is always good for us to know the data type of each column.","metadata":{}},{"cell_type":"markdown","source":"**1. Check data types**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Summary_Statistics'></a>\n### 4.1.3 Summary Statistics\n**1. For numerical variables, we use .describe()**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. For categorical features, we use .describe(include=object)**","metadata":{}},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Missing_Values'></a>\n### 4.1.4 Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are no missing values in the dataset**","metadata":{}},{"cell_type":"markdown","source":"### Visualize Missing Values using Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nsns.heatmap(df.isnull(), cbar=False)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='correlation'></a>\n### 4.1.5 Correlation","metadata":{}},{"cell_type":"markdown","source":"#### Corelation heatmap","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li>Correlation is the extent of linear relationship among numeric variables</li>\n    <li>It indicates the extent to which two variables increase or decrease in parallel</li>\n    <li>The value of a correlation coefficient ranges between -1 and 1</li>\n    <li> Correlation among multiple variables can be represented in the form of a matrix. This allows us to see which pairs are correlated</li>\n    </ul>\n    ","metadata":{}},{"cell_type":"code","source":"cor=df.corr()\nplt.figure(figsize=(15, 8))\n\nsns.heatmap(cor,annot=True)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This reveals a clear relationship among age, balance, duration, and campaign.**\n\n        To investigate more about correlation, a correlation matrix was plotted with all qualitative variables. Clearly, “campaign outcome” has a strong correlation with “duration”, a moderate correlation with “previous contacts”, and mild correlations between “balance”, “month of contact” and “number of campaign”. Their influences on campaign outcome will be investigated further in the machine learning part.","metadata":{}},{"cell_type":"markdown","source":"<a id='categorical'></a>\n### 4.1.6 Analyze Categorical Variables\n\nCategorical variables are those in which the values are labeled categories. The values, distribution, and dispersion of categorical variables are best understood with bar plots.","metadata":{}},{"cell_type":"code","source":"df.describe(include=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_categoric_features = df.select_dtypes(include='object').drop(['deposit'], axis=1)\nfig, ax = plt.subplots(3, 2, figsize=(25, 20))\nfor variable, subplot in zip(df_categoric_features, ax.flatten()):\n    countplot = sns.countplot(y=df[variable], ax=subplot )\n    countplot.set_ylabel(variable, fontsize = 30) \nplt.tight_layout()   \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df_categoric_features:\n    print(i.upper())\n    print(df[i].value_counts())\n    print( )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='numerical'></a>\n### 4.1.6 Analyze Numerical Variables","metadata":{}},{"cell_type":"code","source":"df_num=df.select_dtypes(include=np.number)\nplt.figure(figsize=(15, 8))\nfor i in df_num:\n    sns.boxplot(df[i])\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [15,8]\ndf.drop('deposit', axis = 1).hist()\nplt.tight_layout()\nplt.show()  \nprint('Skewness:')\ndf.drop('deposit', axis = 1).skew()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Scaling the data'></a>\n###  4.1.7 Scaling The Data","metadata":{}},{"cell_type":"code","source":"df_num=df.select_dtypes(include=np.number)\nX_scaler = StandardScaler()\nnum_scaled = X_scaler.fit_transform(df_num)\nX = pd.DataFrame(num_scaled, columns = df_num.columns)\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.skew()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='encoding'></a>\n### Encoding the categorical variable","metadata":{}},{"cell_type":"code","source":"df_cat=df.select_dtypes(exclude=np.number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat=df_cat.drop('deposit',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_encode=pd.get_dummies(df_cat,columns=df_cat.columns)\nX_encode.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_encode.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=pd.concat([X,X_encode],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Target variable'></a>\n### 4.1.8 Target variable","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(df.deposit)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['deposit'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df['deposit']\nfor i in range(len(y)):\n    if y[i] == 'yes':\n        y[i] = 1\n    else:\n        y[i] = 0 \ny=y.astype('int')\ny.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='imbalance data'></a>\n# Handling the imbalanced data","metadata":{}},{"cell_type":"markdown","source":"<a id='Train test split'></a>\n# Train Test Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size =   0.3, random_state = 10) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='log_full model'></a>\n# Logistic Regression Full Model","metadata":{}},{"cell_type":"code","source":"score_card = pd.DataFrame(columns=[\"Model Name\",'Prob.Cutoff',\"Stability\",\"r2_score\", 'AUC', 'Precision', 'Recall',\n                                       'Accuracy', 'Kappa', 'f1-score'])\ndef update_score_card(Model_name,model,cutoff='-',stability=\"Stable\"):\n    y_pred_prob = model.predict(X_test)\n    y_pred = [ 0 if x < cutoff else 1 for x in y_pred_prob]\n    global score_card\n    score_card = score_card.append({\"Model Name\":Model_name,\n                                    \"Prob.Cutoff\":cutoff,\n                                    'Stability': stability,\n                                    \"r2_score\":model.prsquared,\n                                    'AUC' : metrics.roc_auc_score(y_test, y_pred_prob),\n                                    'Precision': metrics.precision_score(y_test, y_pred),\n                                    'Recall': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)\n    return score_card","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_report(model):\n    test_pred = model.predict(X_test)\n    return(classification_report(y_test, test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logisticRegression(x,y,lr):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size =   0.3, random_state = 10) \n    \n    # describes info about train and test set \n    print(\"Number transactions X_train dataset: \", X_train.shape) \n    print(\"Number transactions y_train dataset: \", y_train.shape) \n    print(\"Number transactions X_test dataset: \", X_test.shape) \n    print(\"Number transactions y_test dataset: \", y_test.shape) \n    \n    # train the model on train set \n    lr.fit(X_train, y_train) \n    \n    predictions = lr.predict(X_test) \n\n    # print classification report \n    print(classification_report(y_test, predictions)) \n\n    cm = confusion_matrix(y_test, predictions, labels=lr.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)\n    disp.plot() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc(model):\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    plt.plot(fpr, tpr)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.title('ROC curve for Bank marketing Classifier', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n    plt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(model):\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n    plt.xticks(fontsize = 20)\n    plt.yticks(fontsize = 20)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nlogreg = sm.Logit(y_train, X_train).fit()\nprint(logreg.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('AIC: ',logreg.aic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_odds = pd.DataFrame(np.exp(logreg.params), columns= ['Odds']) \ndf_odds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_prob = logreg.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nyoudens_table = pd.DataFrame({'TPR': tpr,\n                             'FPR': fpr,\n                             'Threshold': thresholds})\nyoudens_table['Difference'] = youdens_table.TPR - youdens_table.FPR\nyoudens_table = youdens_table.sort_values('Difference', ascending = False).reset_index(drop = True)\nyoudens_table.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_prob = logreg.predict(X_test)\ny_pred_prob.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_prob = logreg.predict(X_test)\ny_pred = [ 0 if x < 0.69 else 1 for x in y_pred_prob]\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nconf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False,linewidths = 0.1, annot_kws = {'size':25})\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TN = cm[0,0]\nTP = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = TP / (TP+FP)\nprint('Precision:',precision)\nrecall = TP / (TP+FN)\nprint('Recall:',recall)\nspecificity = TN / (TN+FP)\nprint('Specificity:',specificity)\nf1_score = 2*((precision*recall)/(precision+recall))\nprint('f1_score:',f1_score)\naccuracy = (TN+TP) / (TN+FP+FN+TP)\nprint('Accuracy:',accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kappa = cohen_kappa_score(y_test, y_pred)\nprint('kappa value:',kappa)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve for Bank marketing Classifier (Full Model)', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred_prob),4)))\nplt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"Simple Logistic Regression\",logreg,cutoff=0.69,stability=\"Stable\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='guassiannb_model'></a>\n# Guassian naive bayes model","metadata":{}},{"cell_type":"code","source":"def update_score_card(Model_name,model,cutoff=\"-\",stability=\"Stable\"):\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    y_pred=model.predict(X_test)\n    global score_card\n    score_card = score_card.append({\"Model Name\":Model_name,\n                                    \"Prob.Cutoff\":cutoff,\n                                    'Stability': stability,\n                                    \"r2_score\":metrics.r2_score(y_test, y_pred),\n                                    'AUC' : metrics.roc_auc_score(y_test, y_pred_prob),\n                                    'Precision': metrics.precision_score(y_test, y_pred),\n                                    'Recall': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)\n    return score_card","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB()\ngnb_model = gnb.fit(X_train, y_train)\nplot_confusion_matrix(gnb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(gnb_model)\nprint(test_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(gnb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"gNB Classifier\",gnb_model,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='knn_model'></a>\n# KNN model","metadata":{}},{"cell_type":"code","source":"knn_classification = KNeighborsClassifier(n_neighbors = 3)\nknn_model = knn_classification.fit(X_train, y_train)\nplot_confusion_matrix(knn_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(knn_model)\nprint(test_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(knn_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"KNN Classifier\",knn_model,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='decisiontree'></a>\n# DECISION TREE","metadata":{}},{"cell_type":"code","source":"decision_tree_classification =DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\ndecision_tree = decision_tree_classification.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(decision_tree)\nprint(test_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(decision_tree)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"Decision Tree Classifier\",decision_tree,stability=\"Good\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='randomforest'></a>\n\n# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"rf_classification = RandomForestClassifier(n_estimators = 10, random_state = 10)\nrf_model = rf_classification.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(rf_model)\nprint(test_report) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(rf_classification)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"Random Forest Classifier\",rf_model,stability=\"Good\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='boosting'></a>\n\n# BOOSTING TECHNIQUES","metadata":{}},{"cell_type":"markdown","source":"<a id='ADAboost'></a>\n\n## ADABoost","metadata":{}},{"cell_type":"code","source":"ada_model = AdaBoostClassifier(n_estimators = 40, random_state = 10)\nada_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(ada_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(ada_model)\nprint(test_report) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(ada_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"ADAboost classifier\",ada_model,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='gradboost'></a>\n\n## Gradient Boost","metadata":{}},{"cell_type":"code","source":"gboost_model = GradientBoostingClassifier(n_estimators = 150, max_depth = 10, random_state = 10)\ngboost_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(gboost_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(gboost_model)\nprint(test_report) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(gboost_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"Gradient boost classifier\",gboost_model,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='xgboost'></a>\n\n## XG Boost","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBClassifier(max_depth = 10, gamma = 1)\nxgb_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(xgb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(xgb_model)\nprint(test_report) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(xgb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"XGBClassifier\",xgb_model,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking Classifier","metadata":{}},{"cell_type":"code","source":"base_learners = [('rf_model', RandomForestClassifier(criterion = 'entropy', max_depth = 10, max_features = 'sqrt', \n                                                     max_leaf_nodes = 8, min_samples_leaf = 5, min_samples_split = 2, \n                                                     n_estimators = 50, random_state = 10)),\n                 ('KNN_model', KNeighborsClassifier(n_neighbors = 17, metric = 'euclidean')),\n                 ('NB_model', GaussianNB())]\nstack_model = StackingClassifier(estimators = base_learners, final_estimator = GaussianNB())\nstack_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(stack_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_report = get_test_report(stack_model)\nprint(test_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(stack_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"Stacking classifier\",stack_model,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='knngrid'></a>\n# KNN model Tuned","metadata":{}},{"cell_type":"code","source":"tuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}\nknn_classification = KNeighborsClassifier()\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 5, \n                        scoring = 'accuracy')\nknn_grid.fit(X_train, y_train)\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_classification = KNeighborsClassifier(metric='euclidean',n_neighbors=19)\nknn_model_tuned = knn_classification.fit(X_train, y_train)\nplot_confusion_matrix(knn_model_tuned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(get_test_report(knn_model_tuned))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(knn_model_tuned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_score_card(\"KNN classifier tuned\",knn_model_tuned,stability=\"Moderate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(gnb_model)\nplot_roc(knn_model)\nplot_roc(knn_model_tuned)\nplot_roc(decision_tree)\nplot_roc(rf_classification)\nplot_roc(ada_model)\nplot_roc(gboost_model)\nplot_roc(xgb_model)\nplot_roc(stack_model)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n\nplt.plot([0, 1], [0, 1],'r--')\n\nplt.title('ROC curve analysis', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\nplt.legend(prop={'size':13}, loc='lower right')\nplt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare the performance of the Different Models built**","metadata":{}},{"cell_type":"markdown","source":"1. Accuracy score for selected features for different models range between 0.75 - 0.94\n\n2. Precision Score for selected features for different models range between 0.74 - 0.95\n\n3. Gaussian Naïve Bayes Classifier has overall low precision, recall, accuracy and kappa score compared to other models built.\n\n4. Simple Logistic Regression for selected features has a stable nature whereas both Random Forest and Decision Tree has Good nature of stability.","metadata":{}},{"cell_type":"markdown","source":"**Which metric did we choose and why?**","metadata":{}},{"cell_type":"markdown","source":"The next step after implementing a machine learning algorithm is to find out how effective is the model based on metric and datasets. Different performance metrics are used to evaluate different Machine Learning Algorithms. For example a classifier used to distinguish between images of different objects; we can use classification performance metrics such as, Precision score,accuracy score , recall score and Cross val score etc.\n\nThe machine learning model cannot be simply tested using the training set, because the output will be prejudiced, because the process of training the machine learning model has already tuned the predicted outcome to the training dataset. Therefore in order to estimate the generalization error, the model is required to test a dataset which it hasn’t seen yet; giving birth to the term testing dataset.\n\nTherefore for the purpose of testing the model, we would require a labelled dataset. This can be achieved by splitting the training dataset into training dataset and testing dataset. This can be achieved by various techniques such as, k-fold cross validation.","metadata":{}},{"cell_type":"markdown","source":"**Which model has better performance on the test set?**","metadata":{}},{"cell_type":"markdown","source":"For binary classification model evaluation between random forest and logistic regression, our work focused on four distinct simulated datasets:    \n(1) increasingthe variance in the explanatory and noise variables,     \n(2) increasing the number of noise variables,     \n(3) increasing the number of explanatory variables,     \n(4) increasing the number of observations.\n\nTo benchmark and comparing classification scores between different classification models built, metrics such as accuracy, area under the curve, true positive rate, false positive rate, and precision were analyzed.\n\nKNN classifier tuned has got better accuracy score compared to other models, hence we can say that it has a better performance.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion ","metadata":{}},{"cell_type":"markdown","source":"According to the analysis made across, a target customer profile can be established. The most responsive customers possess following features:\n\n-> Feature 1: age < 30 or age > 60      \n-> Feature 2: students or retired people       \n-> Feature 3: specific months (dec, may, oct)     \n\nBy applying the supervised learning classification techniques, using ensemble learning models, and boosting technqiues the estimation models were successfully built. With all the respective models, the bank will be able to predict a customer's response in the telemarketing campaign before calling the customer. In this way, the bank can allocate more marketing efforts to the clients who are classified as highly likely to accept term deposits, and call less to those who are unlikely to make term deposits. ","metadata":{}}]}