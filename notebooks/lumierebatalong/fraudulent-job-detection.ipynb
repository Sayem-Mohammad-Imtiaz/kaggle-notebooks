{"cells":[{"metadata":{},"cell_type":"markdown","source":"# real and fake job postings\n\nIn this notebook, we are going to look for a fraudulent job posted by someone. So, we will do:\n\n> Perform Exploratory Data Analysis on the dataset to identify interesting insights from this dataset.\n\n> Run a contextual embedding model to identify the most similar job descriptions.\n\n> Identify key traits/features (words, entities, phrases) of job descriptions which are fraudulent in\nnature.\n    \n> Create a classification model that uses text data features and meta-features and predict which job\ndescription are fraudulent or real.\n\nLet's go.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nfrom spacy import displacy\nfrom wordcloud import WordCloud, STOPWORDS\nfrom warnings import filterwarnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 10000)\npd.set_option('display.max_colwidth', 300)\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create class\nclass jobposting:\n    \"\"\"\n    This class help me to do some task that I need. It contain 7 attributes:\n    \n    - read: read a csv file\n    - multi_categorical_plot\n    - feature_value_counts\n    - catplot_multi\n    - cleaning_preparation\n    - extract_most_popular_words\n    \n    \n    \"\"\"\n    \n    def __init__(self, data=None, cols=None, target='fraudulent'):\n        \n        self.data = data # feature\n        self.cols = cols # feature columns name\n        self.targ = target # target \n        \n    #Read csv file\n    def read(self, file, index_cols = None):\n        return pd.read_csv(file, index_col=index_cols)\n    \n    def multi_categorical_plot(self, data):\n    \n        \"\"\" plot a categorical feature\n        \n            data: float64 array  n_observationxn_feature\n        \n        \"\"\"\n        # Find a feature that type is object\n        string = []\n        for i in data.columns:\n            if data[i].dtypes == \"object\":\n                string.append(i)\n    \n        fig = plt.figure(figsize=(20,20))\n        fig.subplots_adjust(wspace=0.4, hspace = 0.1)\n        for i in range(1,len(string)+1):\n            ax = fig.add_subplot(2,2,i)\n            sns.countplot(y=string[i-1], data=data, hue=self.targ, orient = 'h', ax=ax)\n            ax.set_title(f\" {string[i-1]} countplot\")\n            \n    def feature_value_counts(self, data):\n        \"\"\"\n        counts a unique values\n        \"\"\"\n        \n        print('{}\\n'.format(data.value_counts()))\n            \n    def catplot_multi(self, data):\n        \"\"\" plot multi catplot\"\"\"\n    \n    \n        cols = data.columns\n        \n        gp = plt.figure(figsize=(20,20))\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(2,2,i)\n            sns.catplot(x =cols[i-1],  data=data)\n            ax.set_title('{}'.format(cols[i-1]))\n            \n    \n    def cleaning_preparation(self, data):\n        \"\"\"\n            This function cleans and prepares data\n        \n        \"\"\"\n        \n        import string\n        \n        # convert a text to lower case\n        data['text'] = data['text'].str.lower()\n        \n        # Splitting and Removing Punctuation from the Text\n        all_data = data['text'].str.split(' ')\n        \n        #Joining the Entire Text\n        all_data_cleaned = []\n        \n        for text in all_data:\n            text = [x.strip(string.punctuation) for x in text]\n            all_data_cleaned.append(text)\n            \n        text_data = [\" \".join(text) for text in all_data_cleaned]\n        final_text_data = \" \".join(text_data)\n        \n        return final_text_data\n    \n    def word_cloud(self, data):\n        \"\"\"\n        this function plot word cloud\n        \"\"\"\n        \n        stopwords = set(STOPWORDS) # initialize a stop words\n        stopwords.update(['will', 'be', 'you', 'are', 'looking', 'must', 'for', 'look', 'within' ])\n        \n        # we apply our data to WordCloud\n        wordcloud_data = WordCloud(stopwords=stopwords, background_color=\"white\",\\\n                                   max_font_size=50, max_words=100).generate(data)\n        \n        plt.figure(figsize = (20,20))\n        plt.imshow(wordcloud_data, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.show()\n        \n    \n    def extract_most_popular_words(self, data):\n        \"\"\"\n        this function extract a most popular words using in this data.\n        \"\"\"\n        #import package\n        import collections\n        \n        stopwords = set(STOPWORDS)\n        stopwords.update(['will', 'be', 'you', 'are', 'looking', 'must', 'for', 'look', 'within' ])\n        \n        # we filters a words and count it.\n        filtered_words_data = [word for word in data.split() if word not in stopwords]\n        counted_words_data = collections.Counter(filtered_words_data)\n\n        word_count_data = {}\n\n        #we take 30 first words most used.\n        for letter, count in counted_words_data.most_common(30):\n            word_count_data[letter] = count\n    \n        # show a words.\n        for i,j in word_count_data.items():\n            print('Word: {0}, count: {1}'.format(i,j))\n            \n    def processed_corpus(self, data):\n        \n        \"\"\"\n        This function \n        \"\"\"\n        import string\n        from spacy.lang.en.stop_words import STOP_WORDS\n        from spacy.lang.en import English\n        \n        # Create our list of punctuation marks\n        punctuations = string.punctuation\n        \n        # create our list of stopwords\n        stop_words = STOP_WORDS\n        \n        # Load English tokenizer, tagger, parser, NER and word vectors\n        parser = English()\n        \n        corpus = [] #coprus\n        for text in data['text']:\n            \n            mytokens = parser(text)\n       \n        \n            # Lemmatizing each token and converting each token into lowercase\n            mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_\\\n                        for word in mytokens ]\n\n        \n            # Removing stop words\n            mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n            \n            corpus.append(mytokens)\n        \n            \n        return corpus  # return preprocessed text data\n    \n    \n    def getSimilarities(self, corpus, query_text):\n        \n        #import package\n        from gensim import corpora\n        from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\n        from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n\n        #dictionary for corpus\n        dictionary = corpora.Dictionary(corpus)\n\n       \n        #We create the bag of words representation for our corpus\n        BoW_corpus = [dictionary.doc2bow(text) for text in corpus]\n\n        # create a model word to vec\n        model = Word2Vec(corpus, size=20, min_count=1)\n        \n        #Computes cosine similarities between word embeddings and retrieves the closest word embeddings\n        #by cosine similarity for a given word embedding.\n        termsim_index = WordEmbeddingSimilarityIndex(model.wv)\n\n        # construct similarity matrix and Builds a sparse term similarity matrix using\n        #a term similarity index.\n        similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary) \n\n        #Compute soft cosine similarity against a corpus of documents by storing the index matrix in memory.\n        docsim_index = SoftCosineSimilarity(BoW_corpus, similarity_matrix, num_best=10)\n        \n        #we split and do bow of our query real job\n        query_bow = dictionary.doc2bow(query_text.split())\n        \n        #calculate similarity of query to each doc from bow_corpus\n        return docsim_index[query_bow]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = '/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myjob = jobposting()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job = myjob.read(file, index_cols='job_id')\njob.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check missing value\na = job.isnull().sum()\na[a>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data = job.fillna(value=' ')# impute a missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we create country feature using location\ndef split(feature):\n    l = feature.split(',')\n    return l[0]\n\njob_data['country'] = job_data.location.apply(split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"need_objcols_eda = ['employment_type', 'required_experience','required_education', 'function', 'fraudulent']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"need_numcols_eda = ['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"myjob.multi_categorical_plot(job_data[need_objcols_eda])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see employment type, require experience and required education have a fraudulent job.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country = job_data.groupby(by=['country', 'fraudulent'])['location'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country = country.sort_values(by ='location',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.barplot(x='country', y='location', hue='fraudulent', data=country[:10])\nplt.title('10th countries most posted job')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From this graph, most fraudulent job come from US**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,20), dpi=100)\nsns.countplot(y='industry', data=job_data, hue='fraudulent')\nplt.title('Industry job posting')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify key traits/features (words, entities, phrases) of job descriptions which are fraudulent in nature. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fakejobDescription = job_data[job_data.fraudulent==1][['title','description']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fakejobDescription['text'] = fakejobDescription['title'] + ':  ' + fakejobDescription['description']\n\ndel fakejobDescription['description']\ndel fakejobDescription['title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fakejobDescription.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_corpus = myjob.cleaning_preparation(fakejobDescription)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of characters of text corpus is: {}'.format(len(text_corpus)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\n#dispay 200 first characters\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text_corpus[:1000])\ndisplacy.render(doc, style=\"ent\", jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmyjob.word_cloud(text_corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Amazing, fraudulent job use the words that can trust someone it is real. And also, People cannot know that this job is fake if he does not check.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#we give a 30 words most used.\nmyjob.extract_most_popular_words(text_corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify the most similar job descriptions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"realjobDescription = job_data[job_data.fraudulent==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realjobDescription['text'] = realjobDescription['title'] + ' ' + realjobDescription['location'] + ' ' +\\\n                            realjobDescription['department'] + ' ' + \\\n                            realjobDescription['company_profile'] + ' ' + realjobDescription['description'] +\\\n                            realjobDescription['requirements'] + ' ' + realjobDescription['benefits'] + ' '+\\\n                            realjobDescription['required_education'] + ' ' +\\\n                            realjobDescription['required_experience'] + ' ' +\\\n                            realjobDescription['employment_type'] + ' ' + realjobDescription['industry'] +\\\n                            realjobDescription['function']\ndel realjobDescription['title']\ndel realjobDescription['location']\ndel realjobDescription['department'] \ndel realjobDescription['company_profile']\ndel realjobDescription['description']\ndel realjobDescription['requirements']\ndel realjobDescription['benefits']\ndel realjobDescription['required_education']\ndel realjobDescription['required_experience']\ndel realjobDescription['employment_type']\ndel realjobDescription['industry']\ndel realjobDescription['function']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realjobDescription.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.utils import simple_preprocess\n#preprocessed our documents\ncorpus = [simple_preprocess(text) for text in realjobDescription.text]\n%time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag = realjobDescription.text.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sims = myjob.getSimilarities(corpus, tag)\n%time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we display\nsims","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**index job: 4386, 4044, 3071, 2144, 8263, 1995, 14908, 6599, 12491 are similar with a index job 0**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#we check it below\nrealjob = realjobDescription.reset_index()\nrealjob[realjob.index.isin([0,4386, 4044, 3071, 2144, 8263, 1995, 14908, 6599, 12491])][['country', 'text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparison index job = 0, index job = 1995, index job = 4044 and \ntag1 = realjobDescription.text.iloc[0]\ntag2 = realjobDescription.text.iloc[1995]\n\n#dispay 200 first characters\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc1 = nlp(tag1)\ndoc2 = nlp(tag2)\n\ndoc1.user_data['title'] = 'Job description for index 0'\ndoc2.user_data['title'] = 'Job description for index 1995'\n\ndisplacy.render([doc1, doc2], style=\"ent\", jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that the index job 1995 and index job 0 are very similar if we see a entity of words. These two description job have almost sure the same vocabulary words** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparison index job = 0, index job = 1995, index job = 4044 and \n#we take a first real job where job_id = 1.\n%time\ntag1 = realjobDescription.text.iloc[0]\ntag2 = realjobDescription.text.iloc[14908]\n\n#dispay 200 first characters\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc1 = nlp(tag1)\ndoc2 = nlp(tag2)\n\ndoc1.user_data['title'] = 'Job description for index 0'\ndoc2.user_data['title'] = 'Job description for index 14908'\n\ndisplacy.render([doc1, doc2], style=\"ent\", jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create a classification model that uses text data features and meta-features and predict which job description are fraudulent or real.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data['text'] =job_data['title'] + ' ' + job_data['location'] + ' ' +\\\n                            job_data['department'] + ' ' + \\\n                            job_data['company_profile'] + ' ' + job_data['description'] +\\\n                            job_data['requirements'] + ' ' + job_data['benefits'] + ' '+\\\n                            job_data['required_education'] + ' ' +\\\n                            job_data['required_experience'] + ' ' +\\\n                            job_data['employment_type'] + ' ' + job_data['industry'] +\\\n                            job_data['function']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del job_data['title']\ndel job_data['location']\ndel job_data['department'] \ndel job_data['company_profile']\ndel job_data['description']\ndel job_data['requirements']\ndel job_data['benefits']\ndel job_data['required_education']\ndel job_data['required_experience']\ndel job_data['employment_type']\ndel job_data['industry']\ndel job_data['function']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UpNext","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}