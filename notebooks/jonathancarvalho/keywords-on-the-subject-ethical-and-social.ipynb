{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What has been published about ethical and social science considerations?\n## COVID-19 Open Research Dataset Challenge (CORD-19)\n\nLet's try to find what people are talking in the COVID-19 papers about ethical and social science.\nIn this aproach we are going to try a unsupervisioned method to find articles that match our need.\nThis can be useful to save time when we have a lot of data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### First, load the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"metadata = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see if we can find anything using the papers abstract"},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_filter = metadata[metadata.abstract.str.contains('ethics|ethical|social science|multidisciplinary research', \n                                                          regex= True, na=False)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(metadata_filter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We found 155 articles, lets take a look into one."},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_filter.abstract[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can read all 155 of then, or we can use unsupervisioned learning to find patterns inside the text.\n#### First let's prepare the data, removing anything that is not a word, and removing common words (stopwords)"},{"metadata":{"trusted":true},"cell_type":"code","source":"REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n    return text\n\nmetadata_filter['clean_abstract'] = metadata_filter['abstract'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now lets transform the words into a word matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\nX = vectorizer.fit_transform(metadata_filter['clean_abstract'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see if we have too many different subjects before getting the keywords. We use TSNE to reduce the word matrix to 2 dimensions, so we can plot the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(perplexity=4, random_state=42)\n\nX_tsne = tsne.fit_transform(X)\nX_tsne = pd.DataFrame(data=X_tsne, columns=['D1', 'D2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(ax=ax,x = 'D1', y = 'D2', data=X_tsne, alpha=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that we may have a central cluster and other sparse elements. Let's see if this is a real cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=2, random_state=42)\nkmeans.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tsne['CLUSTER'] = kmeans.predict(X)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(ax=ax,x = 'D1', y = 'D2', hue = 'CLUSTER', data=X_tsne, alpha=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We have a little separation (not the best one... maybe if we had more data it could be better) but let's work with it. Let's add the cluster information into the metadata DF"},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_filter['cluster'] = kmeans.predict(X)\nmetadata_filter.groupby('cluster')['cluster'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now let's try to find the subject to each cluster. We count how many time a word appear and run a LDA to group important words"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = metadata_filter.loc[metadata_filter['cluster'] == 0, 'clean_abstract']\nX2 = metadata_filter.loc[metadata_filter['cluster'] == 1, 'clean_abstract']\n\ntf_vectorizer1 = CountVectorizer(max_features=2000, stop_words='english')\ntf_vectorizer2 = CountVectorizer(max_features=2000, stop_words='english')\n\nX1 = tf_vectorizer1.fit_transform(X1)\nX2 = tf_vectorizer2.fit_transform(X2)\n\nlda1 = LatentDirichletAllocation(n_components=5, random_state=42)\nlda2 = LatentDirichletAllocation(n_components=5, random_state=42)\n\nlda1.fit(X1)\nlda2.fit(X2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's print the main topics and their keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top_words(lda1, tf_vectorizer1.get_feature_names(), 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_top_words(lda2, tf_vectorizer2.get_feature_names(), 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that the first cluster (the bigger one) has more subjects that we want, while the second has more clinical data.\n#### We went from 29500 articles to 116, so this method could be useful to optimize the search for important articles. We could use the full text, or add more words to the regex search, but for now is a good first try."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}