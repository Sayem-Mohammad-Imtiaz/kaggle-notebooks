{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Build the environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Import the library"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = SparkContext()\n\nspark = SparkSession.builder \\\n    .appName('Mushroom') \\\n    .getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mushrooms = spark.read.csv('/kaggle/input/mushroom-classification/mushrooms.csv', header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_cols = mushrooms.schema.names[1:]\n# 创建了Indexer后为之后的训练提供参照\nstr_indexers = [StringIndexer(inputCol=c, outputCol=c+'_idx') for c in in_cols]\n\n# a list of StringIndexer objects to convert strings to integer indices\n# each indexer is responsible for converting one feature column\n'''\nin_cols:\n id | category\n----|----------\n 0  | a\n 1  | b\n 2  | c\n 3  | d\n 4  | e\n \nstr_indexer\n id | category | category_idx\n----|----------|---------------\n 0  | a        | 0.0\n 1  | b        | 2.0\n 2  | c        | 1.0\n 3  | d        | 3.0\n 4  | e        | 3.0\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 先进行StringIndexer，再对数据创建onehot\nonehot_encoders = [OneHotEncoder(dropLast=False, inputCol=c+'_idx', outputCol=c+'_onehot') for c in in_cols]\n# a list of OneHotEncoder objects to convert integer indices of cat levels to one-hot encoded columns\n# each encoder is responsible fore encoding one feature column\n\nonehot_cols = [c+'_onehot' for c in in_cols]\nprint(onehot_cols[0:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 把所有的one-hot编码全部都集成到一个feature的1*n矩阵中\nfeat_assembler = VectorAssembler(inputCols=onehot_cols, outputCol='features')\n# a VectorAssembler object that assembles all the one-hot encoded columns into one column,\n# each row of which is a vector of all the numbers in those one-hot columns.\n# e.g.\n# +-----+-----+-----+-----+---------------------+\n# |cat_0|cat_1|cat_2|cat_3|             features|\n# +-----+-----+-----+-----+---------------------+\n# |    1|    0|    0|    0| [1.0, 0.0, 0.0, 0.0]|\n# |    0|    1|    0|    0| [0.0, 1.0, 0.0, 0.0]|\n# |    0|    0|    0|    1| [0.0, 0.0, 0.0, 1.0]|\n# +-----+-----+-----+-----+---------------------+","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_indexer = StringIndexer(inputCol=mushrooms.schema.names[0], outputCol='poisonous')\n# a StringIndexer object that converts <class> column's {e, p} to {0, 1}\n# Because there are more 'e' class in the sample, it will be encoded 0, since StringIndexer gives more frequent levels a lower index\n# Run `mushrooms.groupby('class').count().show()` in pyspark shell to see counts of each class\nmushrooms.groupby('class').count().show()\nmushrooms.groupby('cap-shape').count().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(stages=str_indexers+onehot_encoders+[feat_assembler, label_indexer])\n# +------------+---------------+--------------+-------------+---------------------+\n# |str_indexers|onehot_encoders|feat_assembler|label_indexer|             features|\n# +------------+---------------+--------------+-------------+---------------------+\n# |           1|              0|             0|            0| [1.0, 0.0, 0.0, 0.0]|\n# |           0|              1|             0|            0| [0.0, 1.0, 0.0, 0.0]|\n# |           0|              0|             0|            1| [0.0, 0.0, 0.0, 1.0]|\n# +------------+---------------+--------------+-------------+---------------------+\n\n# Use the pipeline object to transform our dataframe\nmushrooms_trans = pipeline \\\n                    .fit(mushrooms) \\\n                    .transform(mushrooms)\n\nmushrooms_trans = mushrooms_trans.withColumnRenamed('poisonous', 'label')\n\nmushrooms_train, mushrooms_val = mushrooms_trans.randomSplit([0.05, 0.95], seed=2021)\n\nmodel = RandomForestClassifier(labelCol = 'label', featuresCol = 'features',numTrees=10)\n\n#pipeline = Pipeline(stages=str_indexers+onehot_encoders+[feat_assembler, label_indexer, model])\n\nppl = Pipeline(stages=[model])\n\nparamGrid = ParamGridBuilder()\\\n    .addGrid(model.maxDepth, [6, 8, 10]) \\\n    .addGrid(model.numTrees, [100,200])\\\n    .addGrid(model.featureSubsetStrategy, ['onethird', 'sqrt', 'log2']).build()\n\ncrossval = CrossValidator(estimator=ppl,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2) \n\nmodel = crossval.fit(mushrooms_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.transform(mushrooms_val)\npred.select(\"probability\",\"prediction\",\"label\").show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pred.select(['probability', 'prediction', 'label'])\n# Select the columns relevant for evaluation\n# `results` looks like this:\n# +--------------------+----------+---------+\n# |         probability|prediction|poisonous|\n# +--------------------+----------+---------+\n# |[0.97024593961675...|       0.0|      0.0|\n# |[0.96303265951929...|       0.0|      0.0|\n# |[0.95909221894651...|       0.0|      0.0|\n# |[0.95958294573868...|       0.0|      0.0|\n# |[0.95580449199223...|       0.0|      0.0|\n# +--------------------+----------+---------+\n\nresults_collect = results.collect()\n# After .collect(), `results_collect` become a list of Row objects\n\ncorrect = results.withColumn('correct', (results.prediction==results.label).cast('integer')).select('correct')\n\naccuracy = correct.agg({'correct':'mean'}).collect()[0][0]\n\nprint('Test accuracy:', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 显示最优参数组合\nparameters = [\n    (\n        [\n            {key.name: paramValue} \n            for key, paramValue \n            in zip(\n                params.keys(), \n                params.values())\n        ], metric\n    ) \n    for params, metric \n    in zip(\n        model.getEstimatorParamMaps(), \n        model.avgMetrics\n    )\n]\nprint(sorted(parameters, key=lambda el: el[1], reverse=True)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}