{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning - Final project - Kerbidi Kim Lou / Malka Laura\n\n## Context\n\n\nFor this final project you are required to choose and define a business problem of which you will apply machine learning to. \n\n## Data\nYou shall choose a dataset from the datasets available on <a href=\"https://www.kaggle.com/datasets\">kaggle.com</a>.\n\nYou are free to choose any dataset you want, however, your choice should be motivated by something that interests you, for example:\n- your speciality or your future professional project\n- recent events in the world such as the USA election or the covid\n- etc.\n\nThe following are a selection of some datasets from Kaggle, you can choose one of them if you want. \n\n<a href=\"https://www.kaggle.com/c/home-credit-default-risk/data\">Home Credit Default Risk</a>  \n\n<a href=\"https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset\">COVID-19 Healthy Diet Dataset</a>\n\n<a href=\"https://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-dataset\">Bank marketing campaigns dataset</a>\n\n\n\n\n## What should you do ?\n\nMake a notebook telling interesting things about the data you have fetched, tell a story (or many) using everything you learned. Build predictive models and compare them.  \nYou have to submit at least a notebook and any resources you used (like images or any other files).\n\n\nYour final submission should include the following: \n\n- Problem definition\n- Data Exploration\n- Data Processing (Cleaning, etc.)\n- Features Selection\n- Features Engineering\n- Model Selection\n- Learning Curves analysis\n- Dimensionality Reduction\n- Results Visualization\n- Results Interpretation\n\n## Assessment \nHere are the criteria we will use to assess your work:\n\n### Is it meaningful?\nAs a machine learning expert you have to produce something meaningful enough, just plotting random data is not going to work. Like a story your analysis should have some kind of logical progression.\n\n### How well did you use the technical knowledge youâ€™ve been taught?\nObviously, the way you use everything you learned during the lectures is going to be assessed.\n\n### Cleanliness, aesthetics and clearness of your notebook\nIs your analysis full of unused code? Is it difficult to read? Have you tried to make it easy and enjoyable to read?\n\n### Innovation\nCreativity, surprising things or any good initiatives you take are potential bonus points.\n\n\n### Careful:\n\tThis work is individual, plagiarism is going to be measured by both machines and humans. Too many similarities between your work and any online or python buddy work will result in grade penalties.\n\n\nGood Luck!\n\n\n","metadata":{}},{"cell_type":"markdown","source":"--------------------------","metadata":{}},{"cell_type":"markdown","source":"Let's **upload the packages we would like to use.** ","metadata":{}},{"cell_type":"code","source":"pip install plotly_express==0.4.0 #this one will be useful to hover on one of our scatter plots and obtain the exact information on the points","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n# Common imports\nimport numpy as np\nimport numpy as em\nimport pandas as pd\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nimport plotly.express as px\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\nimport seaborn as sns\nsns.set_palette(\"Set2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n* [Dataset chosen on Kaggle](#chapter1)\n* [Problem definition](#chapter2)\n* [Data Exploration and Processing](#chapter3)\n    * [Value Types](#section_3_1)\n    * [Missing Values](#Section_3_2)\n    * [Analyzing extremes](#Section_3_3)\n        * [Alcoholic consumption](#section_3_3_1)\n        * [Undernourished](#section_3_3_2)\n        * [Obesity](#section_3_3_3)\n    * [Early visualizations](#Section_3_4)\n        * [World average diet](#section_3_4_1)\n        * [World covid cases](#section_3_4_2)   \n    * [Insights](#Section_3_5)  \n        * [Diet vs covid](#section_3_5_1)\n        * [Health state vs covid](#section_3_5_2)   \n* [Features Selection](#chapter4) \n    * [Most deaths: Belgium](#section_4_1)\n    * [Most confirmed cases: Montenegro](#Section_4_2)\n    * [Least deaths: Cambodia](#Section_4_3)\n    * [Least confirmed cases: Vanuatu](#Section_4_4)\n* [Features Engineering](#chapter5)\n    * [Handmade clustering](#section_5_1)\n        * [Mortality rate scoring](#section_5_1_1)\n        * [Confirmed cases scoring](#Section_5_1_2)\n    * [Unsupervised learning](#section_5_2)\n        * [Preparation](#section_5_2_1)\n        * [K-means](#Section_5_2_2)\n        * [Elbow method](#section_5_2_3)\n        * [Silhouette plots](#Section_5_2_4)\n* [Model Selection](#chapter6) \n    * [Creating train and test sets](#Section_6_1)\n    * [Performing features scaling](#Section_6_2)\n    * [Linear regression](#Section_6_3)\n    * [Random forest](#Section_6_4)\n* [Learning Curves analysis](#chapter7) \n* [Dimensionality Reduction](#chapter8)\n    * [Missing values filter](#Section_8_1)\n    * [High correlation filter](#Section_8_2)\n    * [Random forest](#Section_8_3)\n* [Results Visualization and Interpretation](#chapter9) \n    * [Obesity vs undernutrition: a happy medium?](#Section_9_1)\n        * [Obesity average diet](#Section_9_1_1)\n        * [Undernutrition average diet](#Section_9_1_2)    \n    * [Extreme covid results and diet: back to our 4 extremes](#Section_9_2)\n        * [Most deaths: Belgium](#section_9_2_1)\n        * [Most confirmed cases: Montenegro](#Section_9_2_2)\n        * [Least confirmed cases: Vanuatu](#Section_9_2_3)   \n        * [Least deaths: Cambodia](#Section_9_2_4)","metadata":{}},{"cell_type":"markdown","source":"--------------------------","metadata":{}},{"cell_type":"markdown","source":" ## Dataset chosen on Kaggle <a class=\"anchor\" id=\"chapter1\"></a>","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset","metadata":{}},{"cell_type":"markdown","source":"## Problem definition <a class=\"anchor\" id=\"chapter2\"></a>","metadata":{}},{"cell_type":"markdown","source":"We chose a dataset combining different types of **food,** world population **obesity and undernourished rate**, and **global covid cases count** from **around the world.** \n\nThe idea is to understand how a **healthy eating style could help combat the coronavirus,** distinguishing the diet patterns from countries with lower COVID infection rate.\n\nOur goal here is to **provide diet recommendations base on our findings.**\n\n\nEach dataset provides **different diet measure** different categories of food, depending on what we want to focus on, so we have \n- fat quantity, \n- energy intake (kcal), \n- food supply quantity (kg), \n- protein for different categories of food \n\nTo which have been added:\n- obesity rate\n- undernourished rate \n- the most up to date confirmed/deaths/recovered/active cases.","metadata":{}},{"cell_type":"markdown","source":"Let's start by **loading the data.**","metadata":{}},{"cell_type":"code","source":"fat_quantity = pd.read_csv(\"../input/covid19healthydietdataset/Fat_Supply_Quantity_Data.csv\")\nfood_kcal = pd.read_csv(\"../input/covid19healthydietdataset/Food_Supply_kcal_Data.csv\")\nfood_kg = pd.read_csv(\"../input/covid19healthydietdataset/Food_Supply_Quantity_kg_Data.csv\")\nprotein_quantity = pd.read_csv(\"../input/covid19healthydietdataset/Protein_Supply_Quantity_Data.csv\")\nsupply_food = pd.read_csv(\"../input/covid19-healthy-diet-dataset/Supply_Food_Data_Descriptions.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's **discover the different datasets.**","metadata":{}},{"cell_type":"code","source":"fat_quantity.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kcal.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"protein_quantity.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"supply_food.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In almost all dataset, the data are organized by countries. There are 170 countries in these datasets.\n<br>After we discovered the different columns in each dataset, we wanted to **focus on how each column data is calculated.**","metadata":{}},{"cell_type":"code","source":"fat_quantity.drop(['Obesity','Confirmed','Undernourished','Deaths','Recovered','Active', 'Population', 'Unit (all except Population)'], axis = 1, inplace = True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Sum of diet measures per Fat quantity :')\nprint(fat_quantity.sum(axis = 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we noticed that the different diet measures are described as their **percentage of prevalence in the total diet.** For Afghanistan for example, alcohol represents 0% of an inhabitant's diet. ","metadata":{}},{"cell_type":"code","source":"food_kg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we noticed that the **different rates for undernourished, obesity and COVID are in percentage of the total population.**","metadata":{}},{"cell_type":"code","source":"food_kg['Confirmed'].round() == (food_kg['Active'] + food_kg['Deaths'] + food_kg['Recovered']).round()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we wanted to make sure that **confirmed cases** are the result of the **sum of deaths, recovered and active case.**","metadata":{}},{"cell_type":"markdown","source":"Now that we know more about each value, **we start to explore.**","metadata":{}},{"cell_type":"markdown","source":"## Data Exploration and Processing <a class=\"anchor\" id=\"chapter3\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 1. Value Types  <a class=\"anchor\" id=\"section_3_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Let's dig into **different data types.**","metadata":{}},{"cell_type":"markdown","source":"We focus on one of the dataset since those are similar, and chose to **focus on the easiest to understand: food_kg.**","metadata":{}},{"cell_type":"code","source":"food_kg.dtypes.value_counts()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's go deeper and dig into **different data types for each variable.**","metadata":{}},{"cell_type":"code","source":"food_kg.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have data on **170 countries,** and the **Undernourished and Unit columns are considered as objects,** let's modify that.\n<br> First, we look at the unique values of this column : this help us understand wether we can directly convert it to float or if there is one or more string elements blocking.","metadata":{}},{"cell_type":"code","source":"food_kg['Unit (all except Population)'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Unit column only contains the % sign, indicating that all the column except the Population one are in percentages. It is important to know the unit used, but now that we know this information we can **delete this column which is no longer useful.**","metadata":{}},{"cell_type":"code","source":"food_kg = food_kg.drop(['Unit (all except Population)'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have fixed the Unit column problem we are going to **focus on the Undernourished one.** Again, we proceed to firstly look at the unique values of this column: helping us understand wHether we can directly convert it to float or if there is one or more string elements blocking.","metadata":{}},{"cell_type":"code","source":"food_kg['Undernourished'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, the problem seems to come from the **\"<2.5\"** value, as the float type does not support special characters. To fix that, we are going to replace all the \"<2.5\" values with **just \"2.5\".** ","metadata":{}},{"cell_type":"code","source":"food_kg[\"Undernourished\"] = food_kg[\"Undernourished\"].replace('<2.5','2.5')\nfood_kg['Undernourished'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have fixed the string elements in the column, we can actually **do the convertion to float.**","metadata":{}},{"cell_type":"code","source":"food_kg[\"Undernourished\"] = pd.to_numeric((food_kg[\"Undernourished\"]), downcast=\"float\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To confirm that the Undernourished column is now a float type, and to dive a bit deeper in the composition of the data set, we now look at **an overview of all information.**","metadata":{}},{"cell_type":"code","source":"food_kg.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have **six columns with missing values** (not reaching 170 values): obesity, undernourished, confirmed, deaths, recovered, and active.","metadata":{}},{"cell_type":"markdown","source":"### 2. Missing Values  <a class=\"anchor\" id=\"section_3_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Let's create a function to **check missing data** and unveil **the percentage of data missing** for each dataframe, as seen in the python bootcamp.","metadata":{}},{"cell_type":"code","source":"def missing_data(data):\n    nb_values = data.isnull().sum().sort_values(ascending = False) #contains the number of values missing\n    percent_values = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False) #contains the percentage of values missing\n    return pd.concat([nb_values, percent_values], axis=1, keys=['Number of Missing Values', 'Percentage of Missing Values'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply the function. We display **6 rows as we know we have 6 columns with missing values**.","metadata":{}},{"cell_type":"code","source":"missing_data(food_kg).head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **number of missing values is low** - from 3 to 9 missing values on 170 lines. Let's see which countries are concerned.","metadata":{}},{"cell_type":"code","source":"food_kg_missing = food_kg[food_kg.isna().any(axis=1)]\nfood_kg_missing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we **cannot approximate those values** nor find the exact missing values in terms of extraction date and population count taken into account for calculation. We thus decided we would **delete the countries** for which values are missing.","metadata":{}},{"cell_type":"code","source":"food_kg = food_kg.dropna(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Analyzing extremes <a class=\"anchor\" id=\"section_3_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"We then proceed exploring the data by describing it ; it gives us a **data description**. ","metadata":{}},{"cell_type":"code","source":"food_kg.describe()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be some **anomalies** in the dataset, with notably strong extremes, that we see in the **max range**. To test if the data is accurate, we quickly test some of the most striking figures.","metadata":{}},{"cell_type":"markdown","source":"#### a. Alcoholic consumption  <a class=\"anchor\" id=\"section_3_3_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"We start out test with the **Alcoholic Beverages** column ; one country average diet is supposedly composed of **15%** of alcoholic beverages. We first look at the corresponding country and then we do a short search on the internet to confirm or deny the information. ","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Alcoholic Beverages'] == food_kg['Alcoholic Beverages'].max()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the corresponding country is **Burkina Faso.** After searching it on the internet, it appears that in spite of Islam being the most prevalent religion in Burkina Faso (nearly 60% of the population according to a survey conducted in 2006), there is a high consumption of alcohol in the country. Indeed, in 2018, **22** liters of pure alcohol were consumed per year and per inhabitant (<a href='https://movendi.ngo/news/2020/07/03/burkina-faso-300000-liters-of-liquor-destroyed-in-ouagadougou/#:~:text=Per%20capita%20alcohol%20intake%20of,adults%20that%20number%20is%2046%25'>source</a>). In comparaison, in 2016 in France, it is **11.7** liters that are consumed per year per inhabitant (<a href='https://www.stop-alcool.ch/fr/l-alcool-en-general-2/statistiques-sur-la-consommation/quelques-chiffres-pour-la-france#:~:text=Avec%2011%2C7%20litres%20d,2'>source</a>).","metadata":{}},{"cell_type":"markdown","source":"#### b. Undernourished  <a class=\"anchor\" id=\"section_3_3_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Then, we move on the **Undernourished column:** ","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Undernourished'] == food_kg['Undernourished'].max()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The corresponding country is **Central African Republic.** Again, this information is also accurate : **79%** of the country's population was estimated to be living in poverty in 2018, thus being more susceptible to be undernourished (<a href='https://www.wfp.org/countries/central-african-republic#:~:text=The%20Central%20African%20Republic%20'>source</a>).","metadata":{}},{"cell_type":"markdown","source":"#### c. Obesity  <a class=\"anchor\" id=\"section_3_3_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"Finally, the last surprising figure is related to the **Obesity column:**","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Obesity'] == food_kg['Obesity'].max()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The corresponding country is **Kiribati**, an archipelago republic in Central Pacific Ocean. After looking up on the internet, it is once again true : the obesity rate of the country was as high as **46%** in 2016 (<a href='https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-020-09217-z#:~:text=In%20fact%2C%20in%202016%2C%20the,of%201.96%25%20%5B5%5D'> source</a>).","metadata":{}},{"cell_type":"markdown","source":"Although some data seem to be anormal at first sight, there are **no anomalies nor abnormal extremes** ; it is a good sign. Indeed, despite the **variety** of the data set, it seems like it still holds **accurate data**, that will enable us to conduct a relevant analysis. ","metadata":{}},{"cell_type":"markdown","source":"This table gives us an **approximation of how the world consumption under covid times**, as the data of over 170 countries are gathered here. \n\nThis help us get **an understanding of the world's average health**:\n- Obesity : 19% of the population of the 170 countries\n- Undernourished : 11% of the population of the 170 countries\n\nAnd learning more about the **covid pandemic**: \n- Confirmed : 1.2% of confirmed cases of covid among the latters\n- Deaths : 0.02% of deaths due to covid\n- Recovered : 0.8% of recovered patients from covid","metadata":{}},{"cell_type":"markdown","source":"### 4. Early visualizations  <a class=\"anchor\" id=\"section_3_4\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### a. World average diet <a class=\"anchor\" id=\"section_3_4_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"First, we want to **visualize the world average diet**. To do so, we create a variable called \"diet_mean\" where we put the dataset description. We select the first row with *iloc* in order to only have the mean of all columns. Then, we *drop* the columns related to covid or health. We also only choose the columns having a mean superior to 1% ; we thus select 11 product categories.","metadata":{}},{"cell_type":"code","source":"diet_mean = food_kg.describe().iloc[1]\ndiet_mean = pd.DataFrame(diet_mean).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active'], axis=0)\ndiet_mean = diet_mean.sort_values(by='mean', ascending=False).iloc[:11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diet_mean_plot = diet_mean.plot.pie(subplots=True, figsize=(15, 15), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking through the diet details, we can see that **vegetal products** are the most consumed, followed by **animal products and cereals.** This pie chart will help us in our analysis later on ; it will serve as a **refferal** to understand the differences in covid cases between countries, based on alimentation.","metadata":{}},{"cell_type":"markdown","source":"#### b. World covid cases <a class=\"anchor\" id=\"section_3_4_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now, we want to **visualize the world average covid state**. To do so, we use *plotly express* to have the possibility to **hover** on a scatter plot and see the statistics per country clearer as explained [here](https://plotly.com/python/hover-text-and-formatting/).  ","metadata":{}},{"cell_type":"code","source":"covid_stats_plot = px.scatter(food_kg, x='Confirmed', y='Deaths', hover_name='Country', size_max=30)\ncovid_stats_plot.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph gives a sense of **repartition of countries in function of their covid deaths and confirmed case**. Extremes can be easily spotted. ","metadata":{}},{"cell_type":"markdown","source":"### 5. Insights <a class=\"anchor\" id=\"section_3_5\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now that we know our dataset a bit better, let's analyze **correlations**, and **challenge our own bias**, especially regarding **obesity being an aggravating factor of covid.**\nTo do so we are going to drop all the mirror correlation (ex population/population, spices/spices, etc.). To apply the function below we used two sources : <a href='https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas'> one for abs and unstack</a> and <a href='https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.unstack.html'> one for dropping the head rows</a>. \nThat way we now have all the *'true'* correlations.","metadata":{}},{"cell_type":"markdown","source":"#### a. Diet vs covid <a class=\"anchor\" id=\"section_3_5_1\"></a>","metadata":{}},{"cell_type":"code","source":"corr_food=food_kg.corr(method='pearson')\ncorr_final=corr_food.abs().unstack().sort_values(ascending = False)\ncorr_final.drop(corr_final.head(32).index, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to create a list of the **strongest correlations with diets for 3 covid states**: confirmed, deaths, and recovered. We will not use the active covid state as it is part of the confirmed covid cases. \n\nTo do so we have to *drop* the health attributes for each list:","metadata":{}},{"cell_type":"code","source":"print('Confirmed')\ncorr_confirmed = corr_final['Confirmed'].head(15)\ncorr_confirmed = corr_confirmed.drop(['Recovered', 'Deaths', 'Active', 'Undernourished', 'Obesity'])\nprint(corr_confirmed)\nprint()\n\nprint('Deaths')\ncorr_deaths = corr_final['Deaths'].head(15)\ncorr_deaths = corr_deaths.drop(['Recovered', 'Confirmed', 'Active', 'Undernourished', 'Obesity'])\nprint(corr_deaths)\nprint()\n\nprint('Recovered')\ncorr_recovered = corr_final['Recovered'].head(14)\ncorr_recovered = corr_recovered.drop(['Confirmed', 'Deaths', 'Undernourished', 'Obesity'])\nprint(corr_recovered)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we **merge these lists in one**, giving us the most interesting diet attributes to compare to the health states.","metadata":{}},{"cell_type":"code","source":"corr_base = corr_deaths + corr_confirmed + corr_recovered\ncorr_base","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we know the **11 strongest correlations** of those 3 covid states, listed above.","metadata":{}},{"cell_type":"markdown","source":"We are going to visualize it a bit better by making a **heatmap for each of them**, solely based on the diets' attributes listed above:","metadata":{}},{"cell_type":"code","source":"corr_heatmap=food_kg[['Deaths','Animal Products','Animal fats','Cereals - Excluding Beer','Eggs','Meat','Milk - Excluding Butter','Pulses','Starchy Roots','Sugar & Sweeteners','Vegetal Products']]\nx=corr_heatmap.corr(method='pearson')\nplt.figure(figsize=(7,5), dpi= 80)\nsns.heatmap(x[['Deaths']].sort_values(by=['Deaths'],ascending=False),cmap='Pastel2_r',annot=True,linewidth=0.6)\nplt.title('Covid deaths cases diets')\nplt.xticks()\nplt.suptitle('Pearson Correlation Coefficient', size=18, va='top')\n\ncorr_heatmap=food_kg[['Confirmed','Animal Products','Animal fats','Cereals - Excluding Beer','Eggs','Meat','Milk - Excluding Butter','Pulses','Starchy Roots','Sugar & Sweeteners','Vegetal Products']]\nx=corr_heatmap.corr(method='pearson')\nplt.figure(figsize=(7,5), dpi= 80)\nsns.heatmap(x[['Confirmed']].sort_values(by=['Confirmed'],ascending=False),cmap='Pastel2_r',annot=True,linewidth=0.6)\nplt.title('Covid confirmed cases diets')\nplt.xticks()\n\ncorr_heatmap=food_kg[['Recovered','Animal Products','Animal fats','Cereals - Excluding Beer','Eggs','Meat','Milk - Excluding Butter','Pulses','Starchy Roots','Sugar & Sweeteners','Vegetal Products']]\nx=corr_heatmap.corr(method='pearson')\nplt.figure(figsize=(7,5), dpi= 80)\nsns.heatmap(x[['Recovered']].sort_values(by=['Recovered'],ascending=False),cmap='Pastel2_r',annot=True,linewidth=0.6)\nplt.title('Covid recovered cases diets')\nplt.xticks()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the first two diets, **confirmed and deaths cases diets, are very similar**. Indeed, the top 3 correlations are **animal products, milk (excluding butter), and animal fat**. When we compare that to the recovered cases diets, the **animal fat is significantly lower** in terms of correlation. Recovered cases diets have a **lesser correlation to meat** as well. \n\nThis could mean that in average, **recovered cases eat less meat and animal fat**. This is why [malnutrition is a threat-multiplier](https://globalnutritionreport.org/blog/nutrition-and-covid-19-malnutrition-threat-multiplier/).","metadata":{}},{"cell_type":"markdown","source":"#### b. Health state vs covid <a class=\"anchor\" id=\"section_3_5_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"After having compared diets to covid results, we now do the **same covid comparison to the health state** to see if there is a pattern as well:","metadata":{}},{"cell_type":"code","source":"corr_heatmap=food_kg[['Deaths','Confirmed','Recovered','Obesity','Undernourished']]\nx=corr_heatmap.corr(method='pearson')\nplt.figure(figsize=(10,8), dpi= 80)\nsns.heatmap(x,cmap='Pastel2_r',annot=True,linewidth=0.6)\nplt.title('Pearson Correlation Coefficient')\nplt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, we can now see that **obesity has a stronger correlation with covid deaths than recovery** and **undernourished patients has a stronger correlation with covid recovery than deaths.**","metadata":{}},{"cell_type":"markdown","source":"This could mean that in average, **obese patients are most likely to die from covid** while **undernourished are most likely to survive**. This is why [obesity worsens outcomes from covid](https://www.cdc.gov/obesity/data/obesity-and-covid-19.html).","metadata":{}},{"cell_type":"markdown","source":"Such results are to be **interpreted carefully** as many other factors are to be taken into account - for example, undernourished patients are most likely to be in emerging countries, where the population is very young and most likely to survive. ","metadata":{}},{"cell_type":"markdown","source":"## Features Selection <a class=\"anchor\" id=\"chapter4\"></a>","metadata":{}},{"cell_type":"markdown","source":"We decided to select **4 countries to focus on.** Those countries reflect extremes in terms of deaths and confirmed cases in relation to their population, and could therefore be **representative of some of our results.**","metadata":{}},{"cell_type":"markdown","source":"### 1. The one with the most deaths in relation to the population: Belgium <a class=\"anchor\" id=\"Section_4_1\"></a>","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Deaths'] == food_kg['Deaths'].max()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we have Belgium, \n[the world's worst affected country when it comes to the coronavirus mortality rate](https://www.bbc.com/news/world-europe-52491210).","metadata":{}},{"cell_type":"markdown","source":"### 2. The one with the most confirmed cases in relation to the population: Montenegro <a class=\"anchor\" id=\"Section_4_2\"></a>","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Confirmed'] == food_kg['Confirmed'].max()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we have Montenegro **the world's worst affected country when it comes to the coronavirus confirmed cases rate**.","metadata":{}},{"cell_type":"markdown","source":"### 3. The most populated one with the least deaths in relation to the population: Cambodia <a class=\"anchor\" id=\"Section_4_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"For this one we have several countries with very few covid-related deaths. For our analysis to be relevant we decided to choose among these countries the one with the largest population. To do so we sorted it by population.","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Deaths'] == food_kg['Deaths'].min()].sort_values(by='Population', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have Belgium's polar opposite, Cambodia, [one of the world's least affected country when it comes to the coronavirus mortality rate](https://www.abc.net.au/news/2020-12-04/cambodia-handling-covid-19-community-transmission-zero-deaths/12938226).","metadata":{}},{"cell_type":"markdown","source":"### 4. The one with the least confirmed cases in relation to the population: Vanuatu <a class=\"anchor\" id=\"Section_4_4\"></a>","metadata":{}},{"cell_type":"code","source":"food_kg[food_kg['Confirmed'] == food_kg['Confirmed'].min()].sort_values(by='Population', ascending=False)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eventually, we have Montenegro's polar opposite, Vanuatu, [the world's least affected country when it comes to the coronavirus confirmed cases rate](https://time.com/5910456/pacific-islands-covid-19-vanuatu/).","metadata":{}},{"cell_type":"markdown","source":"## Features Engineering <a class=\"anchor\" id=\"chapter5\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now we dive into feature engineering, as we try to **create new input features from your existing ones**. In this sense, we thought about creating a new column in which we would **score countries based on their coronavirus results** (again, confirmed and deaths rates). Such grading would allow us to start clustering countries based on the way they handled the situation. ","metadata":{}},{"cell_type":"markdown","source":"### 1. Handmade clustering <a class=\"anchor\" id=\"Section_5_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### a. Mortality rate scoring <a class=\"anchor\" id=\"Section_5_1_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"We create bins, knowing that covid mortality rates go from **0 to almost 15%** (from Cambodia to Belgium). We want to have **4 figures** (1 to 4 - 4 being the worse) so we **arbitrary** create **4 bins.** ","metadata":{}},{"cell_type":"code","source":"score_bins = [-0.1, 0.0375, 0.075, 0.1125, 0.15] #-1 because otherwise for some reason we don't get zeroes\ngrades = ['1','2','3','4']\ncats = pd.cut(food_kg.Deaths, score_bins, labels=grades)\nfood_kg['DeathsScore'] = cats\nfood_kg","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can test the repartition looking at the **occurrence of each grade.** The occurence is **not evenly shared** since the bins were arbitrary made.","metadata":{}},{"cell_type":"code","source":"food_kg.DeathsScore.value_counts()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can have a look at our 4 clusters **through a graph.** ","metadata":{}},{"cell_type":"code","source":"food_kg['DeathsScore'] = food_kg['DeathsScore'].astype(str)\nfood_kg['DeathsScore'] = food_kg['DeathsScore'].astype(float)\ncovid_man_cluster_conf = px.scatter(food_kg, x='Confirmed', y='Deaths', color='DeathsScore', hover_name='Country', size_max=30)\ncovid_man_cluster_conf.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our handmade clustering works but it is **not very representative.**  ","metadata":{}},{"cell_type":"markdown","source":"#### 2. Confirmed cases scoring <a class=\"anchor\" id=\"Section_5_1_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Same as above, we create bins, knowing that covid confirmed case rate go from **almost 0 to almost 6** (from Vanuatu to Montenegro). We want to have **4 figures** (1 to 4 - 4 being the worse) so we **arbitrary** create **4 bins.** ","metadata":{}},{"cell_type":"code","source":"score_bins_2 = [-1, 1.5, 3, 4.5, 6] #-1 because otherwise for some reason we don't get zeroes\ngrades = ['1','2','3','4']\ncats_2 = pd.cut(food_kg.Confirmed, score_bins_2, labels=grades)\nfood_kg['ConfirmedScore'] = cats_2\nfood_kg","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we can test the repartition looking at the **occurrence of each grade.** The occurence is **not evenly shared** since the bins were arbitrary made.","metadata":{}},{"cell_type":"code","source":"food_kg.ConfirmedScore.value_counts()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kg['ConfirmedScore'] = food_kg['ConfirmedScore'].astype(str)\nfood_kg['ConfirmedScore'] = food_kg['ConfirmedScore'].astype(float)\ncovid_man_cluster_conf = px.scatter(food_kg, x='Confirmed', y='Deaths', color='ConfirmedScore', hover_name='Country', size_max=30)\ncovid_man_cluster_conf.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, our handmade clustering works but it is **not very representative.** We can try to do better. ","metadata":{}},{"cell_type":"markdown","source":"### 2. Unsupervised clustering <a class=\"anchor\" id=\"Section_5_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now that we have try some handmade clustering through arbitrary bins, we can go for **unsupervised clustering** as we have seen in Lesson 5. ","metadata":{}},{"cell_type":"markdown","source":"#### a. Preparation <a class=\"anchor\" id=\"Section_5_2_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"We start by **converting the string column 'Country' into an integer**, creating a new dataset called *food_kg_int.* This dataset in now an **array,** storing values of same data type.","metadata":{}},{"cell_type":"code","source":"food_kg_country = food_kg[['Country']]\nfood_kg_drop = food_kg.drop('Country', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(food_kg_drop)\nfood_kg_scaled = scaler.transform(food_kg_drop)\nfood_kg_scaled","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kg_coun = food_kg[['Country']]\nfood_kg_coun.tail(3)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nfood_kg_coun_encoded = encoder.fit_transform(food_kg_coun)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder(sparse=False)\nfood_kg_coun_1hot = cat_encoder.fit_transform(food_kg_coun)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Country:\",food_kg_coun_1hot.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we **concatenate** the processed numerical and categorical features into **one matrix.**","metadata":{}},{"cell_type":"code","source":"food_kg_int = np.concatenate((food_kg_scaled, food_kg_coun_1hot), axis=1, out=None)\nfood_kg_int.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And then we create a new dataset, called *food_kg_ar* **focusing on confirmed cases and deaths**, that we turn into an **array** as well.","metadata":{}},{"cell_type":"code","source":"food_kg_ar = food_kg[['Confirmed','Deaths']].to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we **turn back into a list rather than an array for both.**","metadata":{}},{"cell_type":"code","source":"food_kg_pd = pd.DataFrame(food_kg_int)\nfood_kg_pd.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kg_li = food_kg[['Confirmed','Deaths']]\nfood_kg_li.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### b.  K-Means <a class=\"anchor\" id=\"Section_5_2_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"We start with **K-Means model,** since K-means is easy to implement and computationally very efficient. We **create 4 groups based on their feature similarities.**","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=5,\n            init='random',\n            n_init=10, \n            max_iter=300,\n            tol=1e-04,\n            random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we **have a look at the predicted clusters.**","metadata":{}},{"cell_type":"code","source":"food_kg_km = km.fit_predict(food_kg_int)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we check the **predicted clusters' centers (centroids).**","metadata":{}},{"cell_type":"code","source":"km.cluster_centers_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get the **labels found of the K-means.**","metadata":{}},{"cell_type":"code","source":"km.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we plot **the results of clustering.**","metadata":{}},{"cell_type":"code","source":"plt.scatter(food_kg_int[food_kg_km==0,0], \n            food_kg_int[food_kg_km==0,1], \n            s=50, \n            c='lightgreen', \n            marker='o', \n            label='cluster 1')\n\nplt.scatter(food_kg_int[food_kg_km==1,0], \n            food_kg_int[food_kg_km==1,1], \n            s=50, \n            c='orange', \n            marker='o', \n            label='cluster 2')\n\nplt.scatter(food_kg_int[food_kg_km==2,0], \n            food_kg_int[food_kg_km==2,1], \n            s=50, \n            c='lightblue', \n            marker='o', \n            label='cluster 3')\n\nplt.scatter(food_kg_int[food_kg_km==3,0], \n            food_kg_int[food_kg_km==3,1], \n            s=50, \n            c='green', \n            marker='o', \n            label='cluster 4')\n\nplt.scatter(km.cluster_centers_[:,0], \n            km.cluster_centers_[:,1], \n            s=250, \n            marker='*', \n            c='red', \n            label='centroids')\n\nplt.ylabel('Deaths')\nplt.xlabel('Confirmed')\nplt.title('Country clusters')\n\nplt.legend()\nplt.grid()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result is a bit **messy,** let's see if had chosen the right number of samples in the first place through the **elbow method.**","metadata":{}},{"cell_type":"markdown","source":"#### c.  Elbow method <a class=\"anchor\" id=\"Section_5_2_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"In order **to quantify the quality of our clustering**, we need to use **distortion** to **compare the performance of different K-means clusterings**. ","metadata":{}},{"cell_type":"code","source":"print('Distortion: %.2f' % km.inertia_)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use a graphical tool, the **elbow method,** to estimate the optimal number of clusters k for a given task.","metadata":{}},{"cell_type":"code","source":"distortions = []\nfor i in range(1, 12):\n    km = KMeans(n_clusters=i, \n                init='k-means++', \n                n_init=10, \n                max_iter=300, \n                random_state=0)\n    km.fit(food_kg_ar)\n    distortions.append(km.inertia_)\nplt.plot(range(1, 12), distortions , marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.title('Comparing the performance of different K-means clusterings')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distortion begins to decrease with not much significant change for **k=2** so 2 clusters would have been a better choice for this dataset.","metadata":{}},{"cell_type":"markdown","source":"#### d.  Silhouette plots <a class=\"anchor\" id=\"Section_5_2_4\"></a>","metadata":{}},{"cell_type":"markdown","source":"We compute the **silhouette score** of each sample to quantify the quality our clustering","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom matplotlib import cm\nfrom sklearn.metrics import silhouette_samples\n\nsilhouette_vals = silhouette_samples(food_kg_int, food_kg_km, metric='euclidean')\nsilhouette_vals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now **compute the mean silhouette coefficient of all samples.**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nsilhouette_score_ = silhouette_score(food_kg_int, food_kg_km)\nsilhouette_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we **create a plot of the silhouette coefficients** for a K-means clustering with **k=5.**","metadata":{}},{"cell_type":"code","source":"#Getting the clusters from food_kg_km\ncluster_labels = np.unique(food_kg_km)\nn_clusters = cluster_labels.shape[0]\n\ny_ax_lower, y_ax_upper = 0, 0\nyticks = []\n\n#For each cluster, getting the silhouette values and sort them\nfor i, c in enumerate(cluster_labels):\n    c_silhouette_vals = silhouette_vals[food_kg_km == c]\n    #sort them\n    c_silhouette_vals.sort()\n    \n    y_ax_upper += len(c_silhouette_vals)\n    \n    #specify the color with respect to the number of clusters\n    color = cm.jet(i / n_clusters)\n    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n            edgecolor='none', color=color)\n\n    yticks.append((y_ax_lower + y_ax_upper) / 2)\n    y_ax_lower += len(c_silhouette_vals)\n\n#Computing and plotting the average silhouette\nsilhouette_avg = silhouette_score(food_kg_int,food_kg_km)\n\n\nplt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n\nplt.yticks(yticks, cluster_labels + 1)\nplt.ylabel('Cluster')\nplt.xlabel('Silhouette coefficient')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a **negative difference** here, meaning the point is on average closer to the neighboring group than to its own: it is therefore **misclassified.**\n\nAs underlined by the silhouette above, our clustering is **not so great**, therefore we will **not pursue clustering** to continue our explanation of covid deaths thanks to other models. ","metadata":{}},{"cell_type":"markdown","source":"## Model Selection <a class=\"anchor\" id=\"chapter6\"></a>","metadata":{}},{"cell_type":"markdown","source":"Given this dataset and **the emphasis we have already laid on deaths** through clustering and classification, we thought it would be interesting to try identifying **the factors that are most likely to lead to a deceased person by modelling such data.**","metadata":{}},{"cell_type":"markdown","source":"### 1. Creating train and test sets <a class=\"anchor\" id=\"Section_6_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Let's **separate the data into a training and testing sets** using random selection and setting the ratio to 0.2.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(food_kg, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test and train sets **seem representatives.**","metadata":{}},{"cell_type":"markdown","source":"We **now drop the labels** from the training set and **create a new variable for the labels.**","metadata":{}},{"cell_type":"code","source":"food_kg_train = train_set.drop(\"Deaths\", axis=1) # drop labels for training set\nfood_kg_train_labels = train_set[\"Deaths\"].copy()\n\nfood_kg_test = test_set.drop(\"Deaths\", axis=1) # drop labels for test set\nfood_kg_test_labels = test_set[\"Deaths\"].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, **we don't need an imputer** or any additional manipulations since **we no longer have any missing values.** ","metadata":{}},{"cell_type":"code","source":"food_kg_train_num = food_kg_train.drop('Country', axis=1)\nfood_kg_test_num = food_kg_test.drop('Country', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Performing feature scaling <a class=\"anchor\" id=\"Section_6_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now we perform **features scaling** on the cleaned training and testing *food_kg* datasets.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(food_kg_train_num)\nfood_kg_train_num_scaled = scaler.transform(food_kg_train_num)\nfood_kg_test_num_scaled = scaler.transform(food_kg_test_num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kg_train_num_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we **preprocess the categorical input features.**","metadata":{}},{"cell_type":"code","source":"food_kg_train_coun = food_kg_train[['Country']]\nfood_kg_test_coun = food_kg_test[['Country']]\nfood_kg_test_coun.tail(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We transform our *food_kg* train categories and *food_kg* test categories **from categories into numerical data.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nfood_kg_train_coun_encoded = encoder.fit_transform(food_kg_train_coun)\nfood_kg_test_coun_encoded = encoder.fit_transform(food_kg_test_coun)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse=False)\nfood_kg_train_coun_1hot = cat_encoder.fit_transform(food_kg_train_coun)\nfood_kg_test_coun_1hot = cat_encoder.fit_transform(food_kg_test_coun)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Country:\",food_kg_train_coun_1hot.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_kg_train_num_scaled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We **concatenate** the processed numerical and categorical features **into one matrix.**","metadata":{}},{"cell_type":"code","source":"food_kg_train_prepared = np.concatenate((food_kg_train_num_scaled, food_kg_train_coun_1hot), axis=1, out=None)\nfood_kg_train_prepared.shape\nfood_kg_test_prepared = np.concatenate((food_kg_test_num_scaled, food_kg_test_coun_1hot), axis=1, out=None)\nfood_kg_test_prepared.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now **onto linear regression.**","metadata":{}},{"cell_type":"markdown","source":"### 3. Linear Regression <a class=\"anchor\" id=\"Section_6_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"We try to **model mortality** through linear regression. ","metadata":{}},{"cell_type":"code","source":"food_kg_train_prepared","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train a **linear regression model** on the prepared *food_kg* training set.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(food_kg_train_prepared, food_kg_train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now **we predict.**","metadata":{}},{"cell_type":"code","source":"food_kg_train_predictions = lin_reg.predict(food_kg_train_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we measure this regression modelâ€™s **RMSE** on the whole training set.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nlin_mse = mean_squared_error(food_kg_train_labels, food_kg_train_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's measure this regression modelâ€™s **MAE** on the whole training set.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nlin_mse = mean_squared_error(food_kg_train_labels, food_kg_train_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Random forest <a class=\"anchor\" id=\"Section_6_4\"></a>","metadata":{}},{"cell_type":"markdown","source":"Let's try a **random forest model** on the prepared *food_kg* training set.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(food_kg_train_prepared, food_kg_train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now **we predict.**","metadata":{}},{"cell_type":"code","source":"food_kg_train_predictions = forest_reg.predict(food_kg_train_prepared)\nforest_mse = mean_squared_error(food_kg_train_labels, food_kg_train_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's perform a **10 fold cross validation.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, food_kg_train_prepared, food_kg_train_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And display the **resulting scores:**","metadata":{}},{"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_scores(forest_rmse_scores)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Curves analysis <a class=\"anchor\" id=\"chapter7\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"We will use the following function to **plot learning curves with cross validation.** \n\nAs seen in class 6, the function generates 3 plots: \n- the **test and training** learning curve, \n- the **training samples** vs **fit times curve,**\n- the **fit times vs score curve.**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To define below X and y we used this <a href='https://www.dataquest.io/blog/learning-curves-machine-learning/'>source</a> to help us :","metadata":{}},{"cell_type":"code","source":"food_lc = food_kg.drop(['Country'], axis=1)\nfeatures = list(food_lc.columns)\ntarget = 'Country'\nX = food_kg[features]\ny = food_kg[target]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\ncross_val_strategy = ShuffleSplit(n_splits=100,test_size=0.2)\nplot_learning_curve(estimator=GaussianNB(), title='Learning Curves (Naive Bayes)', X=X, y=y, axes=None, ylim=None, cv=cross_val_strategy, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))\nplt.show()\n\ncv = ShuffleSplit(n_splits=10,test_size=0.2) \nplot_learning_curve(estimator=SVC(gamma=0.001), title=\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\", X=X, y=y, axes=None, ylim=None, cv=cv, train_sizes=np.linspace(.1, 1.0, 5))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nonetheless it doesn't seem to give us any conclusive results, so we are going to **move on** and **refocus on the different diets and their impacts.**","metadata":{}},{"cell_type":"markdown","source":"## Dimensionality Reduction <a class=\"anchor\" id=\"chapter8\"></a>","metadata":{}},{"cell_type":"markdown","source":"As seen in class 7, dimensionality reduction is a way to **reduce the number of features** in your dataset without having **to lose much information** and keep the modelâ€™s performance \n[(source)](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/).\nOur dataset is quite small, so we **only seeked basic dimensionality reduction techniques.**","metadata":{}},{"cell_type":"markdown","source":"### 1. Missing Values filter <a class=\"anchor\" id=\"Section_8_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"In the Data Exploration part, we created a *function* to show the **percentage of missing values** per category, and decided to **get rid** of the *Countries* that had missing values.\n\nIndeed, we were faced with **two choices**: imputing the missing values or dropping the variable. Knowing we could not recover the missing data due to the **uncertainty** in terms of extraction date and population count taken into account for calculation, we went for the second option. \n\n**Such filtering through missing values is was a form of dimensionality reduction.**","metadata":{}},{"cell_type":"markdown","source":"### 2. High Correlation filter <a class=\"anchor\" id=\"Section_8_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Another filtering we have used is the high correlation filter for  variables that have **similar trends** and are **likely to carry similar information.** \n\nIndeed, when we worked on correlations, we **focused on the most correlated diet attributes** to compare to the health states in different countries, and we **phased out some of them** (that were too similar or countained in one another).\n\nWe thus focused on **each covid state only** on *'Animal Products','Animal fats','Cereals - Excluding Beer','Eggs','Meat','Milk - Excluding Butter','Pulses','Starchy Roots','Sugar & Sweeteners'* and *'Vegetal Products'* **rather than the whole column assortment** provided in the first place.","metadata":{}},{"cell_type":"markdown","source":"### 3. Random forest filter <a class=\"anchor\" id=\"Section_8_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"Random forest can also be used for dimensionality reduction, offering a **built-in feature importance measurer**, helping us to select a **smaller subset of features** [(source)](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/).","metadata":{}},{"cell_type":"code","source":"features = food_kg.columns\nimportances = forest_reg.feature_importances_\nindices = np.argsort(importances)[-9:]  # We focus on top 10 features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we plot the **feature importance graph.**","metadata":{}},{"cell_type":"code","source":"plt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above graph, the key features are the **active cases rate** (contained in the confirmed rate we focused on), the **undernourished feature** and the **confirmed cases feature.** \n\nThe **mortality rate is also among the top 10 important features**, and so are the **diet indicators** we kept through our correlation filter (*'Alcoholic Beverages'*, *'Milk - Excluding Butter'* and more).","metadata":{}},{"cell_type":"markdown","source":"## Results Visualization and Interpretation <a class=\"anchor\" id=\"chapter9\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 1. Obesity vs undernutrition: a happy medium? <a class=\"anchor\" id=\"Section_9_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"We have established that **obesity and undernutrition are correlated to covid-cases.** We are now going to dive deeper analyzing the **diet in countries with each health attributes**, and see how covid impacted them.","metadata":{}},{"cell_type":"markdown","source":"#### a. Obesity average diet <a class=\"anchor\" id=\"Section_9_1_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"We take the **top 10 countries in terms of obesity rate** and put them in a variable so we can analyse and plot them :","metadata":{}},{"cell_type":"code","source":"obesity_set = food_kg[food_kg['Obesity'] == food_kg['Obesity']].sort_values(by='Obesity', ascending=False).head(10)\nobesity_mean = obesity_set.describe().iloc[1]\nobesity_mean = pd.DataFrame(obesity_mean).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active', 'ConfirmedScore','DeathsScore'], axis=0)\nobesity_mean = obesity_mean.sort_values(by='mean', ascending=False).iloc[:11]\nobesity_mean_plot = obesity_mean.plot.pie(subplots=True, figsize=(25, 10), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pie chart above **looks a lot like the average pie chart diet we made earlier for the world consumption.** The countries with the most obesity rate seem to consume more vegetables than people on average.","metadata":{}},{"cell_type":"markdown","source":"#### b. Undernutrition average diet  <a class=\"anchor\" id=\"Section_9_1_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Just like we did above for obesity average diet we take the **top 10 countries in terms of undernourished rate** and put them in a variable so we can analyse and plot them :","metadata":{}},{"cell_type":"code","source":"undernutrition_set = food_kg[food_kg['Undernourished'] == food_kg['Undernourished']].sort_values(by='Undernourished', ascending=False).head(10)\nundernutrition_mean = undernutrition_set.describe().iloc[1]\nundernutrition_mean = pd.DataFrame(undernutrition_mean).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active', 'ConfirmedScore','DeathsScore'], axis=0)\nundernutrition_mean = undernutrition_mean.sort_values(by='mean', ascending=False).iloc[:11]\nundernutrition_mean_plot = undernutrition_mean.plot.pie(subplots=True, figsize=(25, 10), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can **easily spot the differences.** Here undernourished people consume **way less animal products** and **much more starchy roots** than the world's consumption in average or the obese people on average. Moreover, they seem to be **consuming a bit more alcoholic beverages.**","metadata":{}},{"cell_type":"markdown","source":"### 2. Extreme covid results and diet: back to our 4 extremes <a class=\"anchor\" id=\"Section_8_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"To better understand the four countries that we have identified earlier and their dynamics we are going to **analyze each of their diet**, to see if it is linked to their results.","metadata":{}},{"cell_type":"markdown","source":"#### a. Belgium : the one with the more deaths cases in relation to its population <a class=\"anchor\" id=\"Section_9_2_1\"></a>","metadata":{}},{"cell_type":"code","source":"belgium_case = food_kg[food_kg['Deaths'] == food_kg['Deaths'].max()]\nbelgium_case","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is important to underline that Belgium has a **24.5% obesity rate, which could partly explained the high mortality rate.**","metadata":{}},{"cell_type":"code","source":"belgium_case = belgium_case.describe().iloc[1]\nbelgium_diet = pd.DataFrame(belgium_case).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active', 'ConfirmedScore','DeathsScore'], axis=0)\nbelgium_diet = belgium_diet.sort_values(by='mean', ascending=False).iloc[:11]\nbelgium_diet_plot = belgium_diet.plot.pie(subplots=True, figsize=(25, 10), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see here that Belgium population consumes **more animal products, milk (excluding butter) and alcohol than the average worlds' consumption**, or **even than the average obese diet.**","metadata":{}},{"cell_type":"markdown","source":"#### b. Montenegro : the one with the more confirmed cases in relation to its population <a class=\"anchor\" id=\"Section_9_2_2\"></a>","metadata":{}},{"cell_type":"code","source":"montenegro_diet = food_kg[food_kg['Confirmed'] == food_kg['Confirmed'].max()]\nmontenegro_diet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Montenegro has a high obesity rate of **24.9%** too, and a **quite high mortality rate.**","metadata":{}},{"cell_type":"code","source":"montenegro_diet = montenegro_diet.describe().iloc[1]\nmontenegro_diet = pd.DataFrame(montenegro_diet).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active', 'ConfirmedScore','DeathsScore'], axis=0)\nmontenegro_diet = montenegro_diet.sort_values(by='mean', ascending=False).iloc[:11]\nmontenegro_diet_plot = montenegro_diet.plot.pie(subplots=True, figsize=(25, 10), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Montenegro's population consumes **even more animal products, meat and milk (excluding butter) than the Belgium one.** ","metadata":{}},{"cell_type":"markdown","source":"#### c. Vanuatu : the one with the less confirmed cases in relation to its population <a class=\"anchor\" id=\"Section_9_2_3\"></a>","metadata":{}},{"cell_type":"code","source":"vanuatu_diet = food_kg[food_kg['Confirmed'] == food_kg['Confirmed'].min()].sort_values(by='Population', ascending=False)\nvanuatu_diet ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vanuatu is very interesting, as the country has also a **high obesity rate of 23.5%**. Nevertheless, its mortality rate and confirmed rate are one of the lowest. Thus, its **diet has to be quite different** than the others and could explain those differences : ","metadata":{}},{"cell_type":"code","source":"vanuatu_diet  = vanuatu_diet.describe().iloc[1]\nvanuatu_diet  = pd.DataFrame(vanuatu_diet).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active', 'ConfirmedScore','DeathsScore'], axis=0)\nvanuatu_diet  = vanuatu_diet.sort_values(by='mean', ascending=False).iloc[:11]\nvanuatu_diet_plot = vanuatu_diet.plot.pie(subplots=True, figsize=(25, 10), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vanuatu's population consumes **much less animal products than the first two of our examples, and consumes much more starchy roots and oilcrops.**","metadata":{}},{"cell_type":"markdown","source":"#### d. Cambodia : the one with the less deaths cases in relation to its population <a class=\"anchor\" id=\"Section_9_2_4\"></a>","metadata":{}},{"cell_type":"code","source":"cambodia_diet = food_kg[food_kg['Deaths'] == food_kg['Deaths'].min()].sort_values(by='Population', ascending=False)\ncambodia_diet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally Cambodia has a **low obesity rate**, and a **high undernourished one.** ","metadata":{}},{"cell_type":"code","source":"cambodia_diet = cambodia_diet.describe().iloc[1]\ncambodia_diet = pd.DataFrame(cambodia_diet).drop(['Deaths', 'Population','Undernourished','Obesity', 'Recovered', 'Confirmed', 'Active', 'ConfirmedScore','DeathsScore'], axis=0)\ncambodia_diet = cambodia_diet.sort_values(by='mean', ascending=False).iloc[:11]\ncambodia_diet_plot = cambodia_diet.plot.pie(subplots=True, figsize=(25, 10), autopct='%1.1f%%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cambodia's population **consume more starchy roots and cereals (excluding beer) than the average world's consumption.**","metadata":{}},{"cell_type":"markdown","source":"<br>\n<b>Conclusions","metadata":{}},{"cell_type":"markdown","source":"In conclusion, now that we have studied those pie charts, there seem to be **some patterns**. Firstly, as we already highlighted it, **obesity is indeed a risk factor**, leading in most cases to a high mortality rate. Moreover, it seems that **alcoholic beverages could also be a factor** ; we can notice it in the case of Belgium, which is quite similar to Montenegro, but differs in terms of alcoholic beverages consumption. That could explain the differences in mortality rate between the two countries.\n\nNonetheless, our conclusions only rely on the factors accessible in this dataset. Other factors such as **age, other health problems**, as well as the **country's covid-related politics** could influence the confirmed and mortality rates. Indeed, the reaction from some country such as Cambodia, which **promptly instaured a national lockdown, helped to control the virus propagation** (<a href='https://blogs.worldbank.org/health/what-explains-cambodias-effective-emergency-health-response-covid-19-coronavirus'>source</a>).","metadata":{}}]}