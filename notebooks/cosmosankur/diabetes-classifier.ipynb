{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"nbpresent":{"id":"898ce9ca-495e-4dfa-ace7-963835b311ba"}},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"nbpresent":{"id":"366742b7-b465-4239-9e91-b2fd11ad208e"}},"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"be1695ca-7512-4e97-adbc-0bd3b9c8855d"},"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"74a7276e-e4f2-448c-b45e-81207d6c6612"},"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"fe49336c-f11b-4a64-8fa5-1c445abbea66"},"trusted":true},"cell_type":"code","source":"#we can see 'Glucose' , 'BMI' and 'Age' are highly co-related with our data","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"4314690e-aa8e-43f0-af79-e40bccc66b69"},"trusted":true},"cell_type":"code","source":"x = df.iloc[:,:8]\ny =df.iloc[:,8]","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"d08dd6bf-d81a-4a62-81b3-fba4110d31a8"},"trusted":true},"cell_type":"code","source":"#rescaling our data for better calculatons\nfrom sklearn import preprocessing\nx = preprocessing.scale(x)","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"e8dd5f0c-ea73-4a49-8163-dfaa9459a394"},"trusted":true},"cell_type":"code","source":"x #preprocessed data","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"9c8bf9ea-145f-46cc-9643-ccf17c38bda9"},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"62636ccb-f0ac-4461-9ee9-f456ab81d835"}},"cell_type":"markdown","source":"LOGISTIC REGRESSION FOR CLASSIFICATION"},{"metadata":{"nbpresent":{"id":"6119e286-ece3-4d90-b34b-d5deb47519ed"},"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'Outcome',data=df,palette='hls')","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"e22d257f-09ae-4305-9909-66e9d4b7dda8"}},"cell_type":"markdown","source":"Most Test Subjects dont have Diabetes :)"},{"metadata":{"nbpresent":{"id":"c5b9f6b9-acf3-4829-9687-78a4cc74f088"},"trusted":true},"cell_type":"code","source":"#now import LogisticRegression library from sklearn\nfrom sklearn.linear_model import LogisticRegression\n#making a logistic regression objext\nlogit_reg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"d24c24dd-d45c-419f-8bd7-0dfde1754165"},"trusted":true},"cell_type":"code","source":"\n\n\n\n#splitting data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"013f2c73-43c8-4440-814f-3c8e08e92138"},"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"48f1d7b1-b519-4515-9b82-d6cba6c0e47a"},"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"c2c28211-8a92-47d6-abd1-d5f82ddeac79"},"trusted":true},"cell_type":"code","source":"logit_reg.fit(x_train,y_train)\n#fitting our model","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"8197230c-0727-461d-8adb-48c137b93e0a"}},"cell_type":"markdown","source":"now that our logistic regression model is ready , we can check its accuracy"},{"metadata":{"nbpresent":{"id":"6ac1a7dd-0d04-4358-8347-b66c19159b4d"},"trusted":true},"cell_type":"code","source":"pred_y = logit_reg.predict(x_test)\n#predicting on our test data","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"110e28e3-c7e8-4f41-961b-6f5540491b77"},"trusted":true},"cell_type":"code","source":"\n#checking accuracy of logistic regreesion model\nprint('score of model on test data is ',logit_reg.score(x_test,y_test))\nprint('score of model on train data is',logit_reg.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"fb55c448-e53c-4a84-bd5f-757d4656b4e1"},"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , classification_report\nconfusion_matrix = confusion_matrix(y_test, pred_y)\n\nprint('classification report is ',classification_report(y_test,pred_y))","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"1def2947-079a-4bc3-8a32-5548dc115229"},"trusted":true},"cell_type":"code","source":"#visualising confusion matrix\nsns.heatmap(confusion_matrix,annot= True)","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"1e8b182e-28c0-46cc-b427-5efca2c563f6"},"trusted":true},"cell_type":"code","source":"#checking statistical method\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y,x)\nresult=logit_model.fit()\nprint(result.summary())","execution_count":null,"outputs":[]},{"metadata":{"nbpresent":{"id":"af1e0154-9d1a-4f15-9fc5-f154ec1f7b01"}},"cell_type":"markdown","source":"#further we can improve model by removing those columns whose f-values(p>|z|) are greater than 0.05 to increase the efficency but i would first try other algorithm before doing this"},{"metadata":{"nbpresent":{"id":"924ec728-0983-4b5d-901b-4501f42eebdf"}},"cell_type":"markdown","source":"# 2. Trying KNN model"},{"metadata":{"nbpresent":{"id":"40fe70b3-8735-429b-8471-222edb5bdcf1"},"trusted":true},"cell_type":"code","source":"\n# we should also check how many k neaighbours we should have for our model for best results\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nk_range = range(1,26)\nscores = {}\nscore_list = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    y_pred = knn.predict(x_test)\n    scores[k] = metrics.accuracy_score(y_test,y_pred)\n    score_list.append(metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(k_range,score_list)\nplt.xlabel('value of k for knn')\nplt.ylabel('test set accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from this graph it is clear that optimum result lies near 21\nwe would try 21 "},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy on test data is',knn.score(x_test,y_test))\nprint('accuracy on train data is',knn.score(x_train,y_train))\nprint('model is generalising well')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# now we would use keras to make ANN for our predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = Sequential()\nclassifier.add(Dense(5,activation = 'relu',input_dim=8))\nclassifier.add(Dense(5,activation = 'relu'))\n#output layer\nclassifier.add(Dense(1,activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history =  classifier.fit(x_train,y_train,batch_size=10,epochs=20,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model_train=classifier.evaluate(x_train, y_train)\nprint('training set accuracy',eval_model_train)\neval_model_test = classifier.evaluate(x_test,y_test)\nprint('testing set accuracy',eval_model_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we would tune our keras model using gridcv"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV,KFold\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = 'normal',activation='relu'))\n    model.add(Dense(4,kernel_initializer = 'normal',activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    \n    adam = Adam(lr=0.01)\n    model.compile(loss = 'binary_crossentropy',optimizer=adam,metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create model\nmodel = KerasClassifier(build_fn=create_model,verbose = 0)\n\n#define grid search parameters\nbatch_size = [10,20,40]\nepochs = [10,50,100]\n\nparam_grid = dict(batch_size=batch_size,epochs=epochs)\ngrid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_result.best_score_,grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"best result is at 40 batch size and 10 epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"#turning learning rate and drop out rate\nfrom keras.layers import Dropout\n\n# Defining the model\n\ndef create_model(learning_rate,dropout_rate):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = learning_rate)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nlearning_rate = [0.001,0.01,0.1]\ndropout_rate = [0.0,0.1,0.2]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_result.best_score_,grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best results are at drop out rate of 0.2 and learning rate of 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(activation_function,init):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nactivation_function = ['softmax','relu','tanh','linear']\ninit = ['uniform','normal','zero']\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(activation_function = activation_function,init = init)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 2)\ngrid_result = grid.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_result.best_score_,grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(neuron1,neuron2):\n    model = Sequential()\n    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nneuron1 = [4,8,16]\nneuron2 = [2,4,8]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_result.best_score_,grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimum values of Hyperparameters are as follows :-\nBatch size = 40\nEpochs = 10\nDropout rate = 0.2\nLearning rate = 0.001\nActivation function = tanh\nKernel Initializer = uniform\nNo. of neurons in layer 1 = 16\nNo. of neurons in layer 2 = 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\n# Defining the model\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(16,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Fitting the model\n\nmodel.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_tuned =model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('classification report is',classification_report(y_pred_tuned,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_pred_tuned,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}