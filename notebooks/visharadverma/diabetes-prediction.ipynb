{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nimport scipy.stats as stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \npd.set_option(\"display.precision\", 2)\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf1.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EXPLORATORY DATA ANALYSIS "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some minimum values of 0, which may alter the data in a negative way, hence we need to handle them, one way or the other"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df1.copy(deep=True)\n\ndf2[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df2[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\ndf2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong> BMI and diabetesfunction vs Outcome </strong>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Continuous Variables')\nprint(df2[['BMI','DiabetesPedigreeFunction']].describe().transpose())\nprint('--'*40)\n\nsns.set_style('darkgrid')\nfig = plt.figure(figsize = (20,16))\nfig.subplots_adjust(hspace = .30)\n\nax1 = fig.add_subplot(221)\nax1.hist(df2['BMI'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\nax1.set_xlabel('BMI', fontsize = 15)\nax1.set_ylabel('Individuals',fontsize = 15)\nax1.set_title('BMI Spread',fontsize = 15)\n\nax2 = fig.add_subplot(223)\nax2.hist(df2['DiabetesPedigreeFunction'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\nax2.set_xlabel('DiabetesPedigreeFunction',fontsize = 15)\nax2.set_ylabel('Individuals',fontsize = 15)\nax2.set_title('DiabetesPedigreeFunction',fontsize = 15)\n\nax3 = fig.add_subplot(122)\nax3.scatter(x = df2[df2['Outcome']==0].BMI, y = df2[df2['Outcome']==0].DiabetesPedigreeFunction,\n                        alpha = .50,edgecolor= 'black',  c = 'grey', s= 75, label = 'No Diabetes')\nax3.scatter(x = df1[df1['Outcome']==1].BMI, y = df1[df1['Outcome']==1].DiabetesPedigreeFunction,\n                        alpha = .50,edgecolors= 'black',  c = 'lightgreen', s= 75, label = 'Diabetes')\nax3.set_xlabel('BMI')\nax3.set_ylabel('DiabetesPedigreeFunction')\nax3.set_title('BMI vs DiabetesPedigreeFunction')\nax3.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Integer Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Integer Variables')\nprint(df2[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','Age']].describe().transpose())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pregnancies"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.crosstab(df2['Pregnancies'],df2['Outcome']))\nprint(\"---\"*40)\n\n\nplot = df2.groupby(['Pregnancies','Outcome']).Pregnancies.count().unstack()\n# plt.bar(x = ((df1['Pregnancies']==0).count()), y=[x for x in range(768)])\n\np1 = plot.plot(kind='bar',stacked=True, title='Pregnancies and Outcome', color = ['#2F9C95','#A3F7B5'])\n\np1.set_xlabel('Pregnancies')\np1.set_ylabel(\"Individuals\")\nplt.legend(\"No Diabetes\",'Diabetes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_list = ['red' if i==1 else 'green' for i in df1.loc[:,'Outcome']]\npd.plotting.scatter_matrix(df1.loc[:, df1.columns != 'Outcome'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       edgecolor= \"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ^Original Data, with 0 values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\np=scatter_matrix(df2,figsize=(20, 15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two scatter matrices are more or less similar, but I did two coded 2 different matrices for the sake of it"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Outcome\", data=df1)\ndf1.loc[:,'Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Outcome',hue=\"Pregnancies\", data=df1, palette='RdBu_r')\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(df1.columns)\nfig = plt.figure(figsize=(10,10))\ny=0\ncols.pop()\n\nlabels = [\"Not Diabetic\",\"Diabetic\"]\n\nfor a in cols:\n    data = []\n    i, j = divmod(y, 3)\n    ax1 = plt.subplot2grid((3,3),(i,j))\n    \n    bel_avg = (df1[a]>df1[a].mean()).value_counts()[0]\n    data.append(bel_avg)\n    \n    ab_avg = (df1[a]>df1[a].mean()).value_counts()[1]\n    data.append(ab_avg)\n    \n    plt.pie(data,labels=labels)\n    plt.title(a)\n    y = y + 1\n    \n    #print(i,j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"DIABETES POSITIVE\")\nprint(df1[df1['Outcome'] == 1].mean(),'\\n')\nprint(\"DIABETES NEGATIVE\")\nprint(df1[df1['Outcome']== 0].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions - \n    <li>No signs of an imbalanced dataset.</li>\n    <li>789 rows, with 9 columns, one being the independant variable </li>\n    <li>There are some columns with high correlation, we need to take care of it in the \"Data Cleaning\" step</li>\n    <li>No Null Values, shall see further proof in the next section</li>\n    <li>Target variable is of int64 datatype</li>\n    <li>Features are majorly of int64 datatype too, with 2 being decimals, overall, all numeric values</li>\n    <li>The patients with diabetes have higher data values, except BMI, suggesting there <b>MAY</b> not be a link between diabetics and weight</li>"},{"metadata":{},"cell_type":"markdown","source":"### Now, let us ask some questions."},{"metadata":{},"cell_type":"markdown","source":"#### <H5>Q1) What is the Glucose Level of Diabetics, who have high (above average) insulin?</H5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The avg Glucose Level of Diabetics, who have high insulin have\",df1[df1['Insulin']>100][\"Glucose\"].mean(),\"units insulin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h5>Q2)What is the average BMI for Diabetics, who have an average Glucose and Blood Pressure?</h5>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average BMI for diabetics, who have average Glucose and Blood pressure have an average BMI of\",df1[(df1['Glucose'] < 120) & (df1['Glucose'] > 100) & (df1['BloodPressure']<70) & (df1['BloodPressure']>60)]['BMI'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"filt = df2['Outcome'] == 0\nfilt2 = df2['Outcome'] == 1\n\ndf_noDiabetes = df2.loc[filt]\ndf_hasDiabetes = df2.loc[filt2]\n\nfor i in range(len(cols)-1):\n    fig = plt.figure()\n    \n    fig.add_subplot(2,1,1).plot(df_hasDiabetes[cols[i]],np.zeros_like(df_hasDiabetes[cols[i]]),'o',label=\"Has Diabetes\")\n    fig.add_subplot(2,1,1).plot(df_noDiabetes[cols[i]],np.zeros_like(df_noDiabetes[cols[i]]),'o',label=\"Does not have diabetes\",alpha=0.4)\n    \n    plt.title(cols[i])\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These graphs are useful in telling us, where the diabetic patients reside in terms of these factors, and how far off or how in range they are when compared to the non-diabetic patients"},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(df2,hue='Outcome',height=5).map(plt.scatter,'BMI','Insulin').add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There isn't much to infer from this graph, except that there are non-diabetic people with higher BMI, suggesting that there <b>may not</b> be a relation between high weight and diabetes. Of course, these individuals may be of a taller height, therefore, it shall merely be speculation. Another point to note is that this dataset is talking about Type 2 diabetes  "},{"metadata":{},"cell_type":"markdown","source":"#### Multivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = df2.corr()\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, ax=ax, linewidths=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features are not <b>highly</b> correlated with each other, hence there is no such need to remove any such features. We might reduce the features ahead, but not on the basis of correlation"},{"metadata":{},"cell_type":"markdown","source":"## DATA CLEANING"},{"metadata":{},"cell_type":"markdown","source":"#### Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us replace the lesser null values with mean, and the higher null values with the median"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Glucose'] = df2['Glucose'].fillna(df2['Glucose'].mean())\ndf2['BloodPressure'] = df2['BloodPressure'].fillna(df2['BloodPressure'].mean())\ndf2['SkinThickness'] = df2['SkinThickness'].fillna(df2['SkinThickness'].median())\ndf2['Insulin'] = df2['Insulin'].fillna(df2['Insulin'].median())\ndf2['BMI'] = df2['BMI'].fillna(df2['BMI'].median())\n\n### Another way of replacing is using inplace = True\n\ndf2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df1.columns\n\nnum_cols = df1._get_numeric_data().columns\n\nlist(set(cols) - set(num_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are no Categorical Variables in this dataset, there is no need for encoding, hence we can again move forward"},{"metadata":{},"cell_type":"markdown","source":"#### Outlier Detection"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = df2.hist(bins=30, figsize=(15, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_array = list(df2.columns)\nf, axes = plt.subplots(round(len(columns_array)/3), 3,figsize=(12,5))  \ny = 0;\nfor name in columns_array:\n    i, j = divmod(y, 3)\n    sns.boxplot(x=df2[name], ax=axes[i, j])\n    y = y + 1\n\nf.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grubbs_test(x):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator/sd_x\n    #print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    #print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        return(\"No outliers\\n\")\n    else:\n        return(\"Outliers\\n\")\n    \nfor count, columns in enumerate(df2.columns):\n    print(count+1,columns_array[count])\n    print(grubbs_test(df2[columns]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<ol> <h6>We can draw the following conclusions</h6>\n    <li> Insulin and Diabetes Pedigree Function have a very high number of Outliers </li>\n    <li> We shall leave the outliers alone for now, but if we get a low accuracy score, we shall do some cleaning </li>\n</ol>"},{"metadata":{},"cell_type":"markdown","source":"### Modelling of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandard_scaler_X = StandardScaler()\nX =  pd.DataFrame(standard_scaler_X.fit_transform(df1.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.values\ny = df2.iloc[:,-1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"The Confusion Matrix is \",\"\\n\",cm)\nprint(\"Precision score: {}\".format(precision_score(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.heatmap(pd.DataFrame(cm),annot=True,fmt='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a Precision Score of 75%, using Logistic Regression, and without using any advanced techniques. Let us see if we can do better."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nl_scores = cross_val_score(LogisticRegression(), X, y)\nprint(f'Logistic Regression Scores, using Cross Validation are {l_scores}')\nprint(f\"The average of those scores is {np.average(l_scores)}\")\n\nprint(\"---\"*40)\n\nd_scores = cross_val_score(DecisionTreeClassifier(), X, y)\nprint(f'Decision Tree Scores, using Cross Validation are {d_scores}')\nprint(f\"The average of those scores is {np.average(d_scores)}\")\n\nprint(\"---\"*40)\n\ns_scores = cross_val_score(SVC(), X, y)\nprint(f'Support Vector Scores, using Cross Validation are {s_scores}')\nprint(f\"The average of those scores is {np.average(s_scores)}\")\n\nprint(\"---\"*40)\n\nr_scores = cross_val_score(RandomForestClassifier(n_estimators=40), X, y)\nprint(f'Support Vector Scores, using Cross Validation are {r_scores}')\nprint(f\"The average of those scores is {np.average(r_scores)}\")\n\nprint(\"---\"*40)\n\nk_scores = cross_val_score(KNeighborsClassifier(),X,y)\nprint(f'K-Nearest Neighbor Scores, using Cross Validation are {k_scores}')\nprint(f\"The average of those scores is {np.average(k_scores)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest score is that of SVC (Support Vector Classifier), hence we will use some GridSearchCV to find the best parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = SVC(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\n\naccuracy = grid_search.best_score_\nbest_params = grid_search.best_params_\n\nprint(f\"The accuracy is {accuracy}\")\nprint(f'The best parameters are {best_params}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is one of my first notebooks, hence I would greatly appreciate any form of feedback."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}