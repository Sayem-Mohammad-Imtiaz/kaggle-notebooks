{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root = \"/kaggle/input/unsw-nb15/\"\ntrain = pd.read_csv(root+\"UNSW_NB15_training-set.csv\")\ntest = pd.read_csv(root+\"UNSW_NB15_testing-set.csv\")\nlist_events = pd.read_csv(root+\"UNSW-NB15_LIST_EVENTS.csv\")\nfeatures = pd.read_csv(root+\"NUSW-NB15_features.csv\", encoding='cp1252')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to official site [here](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/), train and test data have 175341 and 82332 rows respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)\nif train.shape[0]<100000:\n    print(\"Train test sets are reversed. Fixing them.\")\n    train, test = test, train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['type'] = 'train'\ntest['type'] ='test'\ntotal = pd.concat([train, test], axis=0, ignore_index=True)\ntotal.drop(['id'], axis=1, inplace=True)\n# del train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('object')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize(df):\n    return (df-df.mean())/df.std()\n    \ndef min_max(df):\n    return (df-df.min())/(df.max() - df.min())\n\ndef normalize(df):\n    return pd.Dataframe(preprocessing.normalize(df), columns=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = reduce_mem_usage(total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List of Events"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_events.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_events.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_events['Attack category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_events['Attack subcategory'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head(features.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the Name column has camel case values\nfeatures['Name'] = features['Name'].str.lower()\n# the following 4 columns are address related and not in train dataset\nfeatures = features[~features['Name'].isin(['srcip', 'sport', 'dstip', 'dsport'])].reset_index()\nfeatures.drop(['index', 'No.'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"normal = train[train['label']==0]\nanomaly = train[train['label']==1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some difference with features file"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sorted(set(train.columns) - set(features['Name'].values)))\nprint(sorted(set(features['Name'].values) - set(train.columns)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the column names in features file are wrong and we are going to fix them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fix = {'ct_src_ ltm': 'ct_src_ltm', 'dintpkt': 'dinpkt', 'dmeansz': 'dmean', 'res_bdy_len': 'response_body_len', 'sintpkt': 'sinpkt', 'smeansz': 'smean'}\nfeatures['Name'] = features['Name'].apply(lambda x: fix[x] if x in fix else x)\nfeatures.to_csv('features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sorted(set(train.columns) - set(features['Name'].values)))\nprint(sorted(set(features['Name'].values) - set(train.columns)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still there are some differences. `stime` and `ltime` both refers to when the recording stared and lasted. So they shouldn't be valuable in training, hence not being in train set makes sence. `id` is just row number and rate might be something related to packed sending speed or data rate."},{"metadata":{},"cell_type":"markdown","source":"## Checking data types"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* categorical: state, service, proto\n* target  = attack_cat, label\n* integer but categorial = is_sm_ips_ports, ct_state_ttl, is_ftp_login\n* integer = spkts, dpkts, sbytes, dbytes, sttl, dttl, sload, dload, sloss, dloss, swin, dwin, stcpb, dtcpb, smean, dmean, trans_depth, response_body_len, ct_srv_src, ct_state_ttl, ct_dst_ltm, ct_src_dport_ltm, ct_dst_sport_ltm, ct_dst_src_ltm, ct_ftp_cmd, ct_flw_http_mthd, ct_src_ltm, ct_srv_dst, \n* decimal = dur, rate, sinpkt, dinpkt, sjit, djit, tcprtt, synack, ackdat"},{"metadata":{},"cell_type":"markdown","source":"# Correlation matrix\nWhy checking correlation is important ? Check these links:\n* [Why Feature Correlation Matters …. A Lot!](https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4) and \n* [Feature selection — Correlation and P-value](https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_correlation(data, method='pearson'):\n    correlation_matrix = data.corr(method='pearson') #  ‘pearson’, ‘kendall’, ‘spearman’\n    fig = plt.figure(figsize=(12,9))\n    sns.heatmap(correlation_matrix,vmax=0.8,square = True) #  annot=True, if fig should show the correlation score too\n    plt.show()\n    return correlation_matrix\n\ndef top_correlations(correlations, limit=0.9):\n    columns = correlations.columns\n    for i in range(correlations.shape[0]):\n        for j in range(i+1, correlations.shape[0]):\n            if correlations.iloc[i,j] >= limit:\n                print(f\"{columns[i]} {columns[j]} {correlations.iloc[i,j]}\")\ndef print_correlations(correlations, col1=None, col2=None):\n    columns = correlations.columns\n    for i in range(correlations.shape[0]):\n        for j in range(i+1, correlations.shape[0]):\n            if (col1 == None or col1==columns[i]) and (col2 == None or col2==columns[j]):\n                print(f\"{columns[i]} {columns[j]} {correlations.iloc[i,j]}\")\n                return\n            elif (col1 == None or col1==columns[j]) and (col2 == None or col2==columns[i]):\n                print(f\"{columns[i]} {columns[j]} {correlations.iloc[i,j]}\")\n                return\n            \ndef find_corr(df1, df2):\n    return pd.concat([df1, df2], axis=1).corr().iloc[0,1]\n\ndef corr(col1, col2='label', df=total):\n    return pd.concat([df[col1], df[col2]], axis=1).corr().iloc[0,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pearson"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = show_correlation(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_correlations(correlation_matrix, limit=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spearman"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = show_correlation(train, method='spearman')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_correlations(correlation_matrix, limit=0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most correlated features are :\n* spkts, sbytes, sloss \n* dpkts, dbytes, dloss\n* sinpkt, is_sm_ips_ports\n* swin, dwin\n* tcprtt, synack\n* ct_srv_src, ct_srv_dst, ct_dst_src_ltm, ct_src_dport_ltm, ct_dst_sport_ltm \n* is_ftp_login ct_ftp_cmd"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(total[['spkts', 'sbytes', 'sloss']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(total[['dpkts', 'dbytes', 'dloss']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(total[['sinpkt', 'is_sm_ips_ports']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(total[['swin', 'dwin']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# plot utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dual_plot(col, data1=normal, data2=anomaly, label1='normal', label2='anomaly', method=None):\n    if method != None:\n        sns.distplot(data1[col].apply(method), label=label1, hist=False, rug=True)\n        sns.distplot(data2[col].apply(method), label=label2, hist=False, rug=True)\n    else:\n        sns.distplot(data1[col], label=label1, hist=False, rug=True)\n        sns.distplot(data2[col], label=label2, hist=False, rug=True)\n    plt.legend()\n    \ndef catplot(data, col):\n    ax = sns.catplot(x=col, hue=\"label\", col=\"type\",data=data, kind=\"count\", height=5, legend=False, aspect=1.4)\n    ax.set_titles(\"{col_name}\")\n    ax.add_legend(loc='upper right',labels=['normal','attack'])\n    plt.show(ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical\nThese four columns are categorical: 'attack_cat', 'state', 'service', 'proto'. Among them 'attack_cat' isn't a feature.\nThese features are categorical but in integer form : 'is_sm_ips_ports', 'ct_state_ttl', 'is_ftp_login'."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_count_df(col, data=total):\n    df = pd.DataFrame(data[col].value_counts().reset_index().values, columns = [col, 'count'])\n    df['percent'] = df['count'].values*100/data.shape[0]\n    return df.sort_values(by='percent', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label\n0 for normal and 1 for attack records"},{"metadata":{"trusted":true},"cell_type":"code","source":"create_count_df('label', train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_count_df('label', test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it seems the dataset is pretty balanced, unlike real world data where attack scenarios are rare. Moreover, here attack connections are more than normal connections."},{"metadata":{},"cell_type":"markdown","source":"## State\nIndicates to the state and its dependent protocol, e.g. ACC, CLO, CON, ECO, ECR, FIN, INT, MAS, PAR, REQ, RST, TST, TXD, URH, URN, and (-) (if not used state)"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'state'\ncreate_count_df(col, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all other values those were few in train set, have been renamed to 'RST_and_others'\ntotal.loc[~total[col].isin(['FIN', 'INT', 'CON', 'REQ', 'RST']), col] = 'others'\ncatplot(total, col)\n# catplot(total[~total[col].isin(['INT', 'FIN', 'REQ', 'CON'])], col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Service\nhttp, ftp, smtp, ssh, dns, ftp-data ,irc  and (-) if not much used service. More than half of the service data are of - category. "},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'service'\ncreate_count_df(col, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot(total[~total[col].isin(['-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3'])], col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total.loc[~total[col].isin(['-', 'dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3']), col] = 'others'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## proto\nTransaction protocol. Normal connections of train data have only 5 protocols, where anomaly connections have 129. So we'll convert all other protocols into same value."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'proto'\ncreate_count_df(col, normal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_count_df(col, anomaly)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# icmp and rtp columns are in test, but not in train data\ntotal.loc[total[col].isin(['igmp', 'icmp', 'rtp']), col] = 'igmp_icmp_rtp'\ntotal.loc[~total[col].isin(['tcp', 'udp', 'arp', 'ospf', 'igmp_icmp_rtp']), col] = 'others'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## is_sm_ips_ports\nIf source and destination IP addresses equal and port numbers (sport/dport)  equal then, this variable takes value 1 else 0. Seems if it is 1, then the connection is always normal. This feature is highly correlated with sinpkt (0.94131890073567)."},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot(total, 'is_sm_ips_ports')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## is_ftp_login\nIf the ftp session is accessed by user and password then 1 else 0. In most of the cases session has no user and password. However there are values 2 and 4 which should not be there.\n\nThis feature is totally correlated with ct_ftp_cmd, which counts the number of ftp commands. So dropping this column should be ok."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'is_ftp_login'\nprint(corr('ct_ftp_cmd', col), corr('is_ftp_login', 'label'))\ncatplot(total, col)\ntotal.drop([col], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Integer Features\n## ct_state_ttl\nNo. for each state according to specific range of values for source/destination time to live (sttl/dttl)."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'ct_state_ttl'\ncatplot(total, col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ct_ftp_cmd\nNo of flows that has a command in ftp session. It has a very low correlation with target. Also is_ftp_login is highly correlated with it (0.9988554882922012)."},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot(total, 'ct_ftp_cmd')\ncorr('ct_ftp_cmd', 'label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ct_flw_http_mthd\nNo. of flows that has methods such as Get and Post in http service. Seems 0 has more anomaly values, however the correlation is very small with target."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'ct_flw_http_mthd'\ncatplot(total, col)\ncorr(col) # -0.012237160723","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_count_df(col, total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sbytes & dbytes\n* sbytes: Source to destination transaction bytes \n* dbytes: Destination to source transaction bytes\n\nThese 2 features are higly corelated to number of packets sent (spkts & dpkts). Actually, spkts * smean = sbytes. Also they are closely related to sloss and dloss. So we can drop these 2 here."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(find_corr(total['spkts']*total['smean'], total['sbytes'])) # 0.999999\nprint(find_corr(total['dpkts']*total['dmean'], total['dbytes'])) # 0.99999\nprint(corr('sbytes', 'sloss'), corr('dbytes', 'dloss')) # 0.995771577240429, 0.9967111338305503\ntotal.drop(['sbytes', 'dbytes'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## smean & dmean \nMean of the packet size transmitted. However is it just sbytes/spkts ? The correlation says it is. So we already have this \ninfo from those other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('smean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('dmean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['smean_log1p'] = total['smean'].apply(np.log1p)\ntotal['dmean_log1p'] = total['dmean'].apply(np.log1p)\n\n# -0.02837244879012871 -0.2951728296856902 -0.05807468815031313 -0.5111549621216057\nprint(corr('smean'), corr('dmean'), corr('smean_log1p'), corr('dmean_log1p'))\n# So we have better correlation with label after applying log1p. \ntotal.drop(['smean', 'dmean'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## spkts and dpkts\n* spkts : Source to destination packet count \n* dpkts: Destination to source packet count"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'spkts'\ndual_plot(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot(col, method=np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['spkts_log1p'] = total['spkts'].apply(np.log1p)\ntotal['dpkts_log1p'] = total['dpkts'].apply(np.log1p)\n\n# -0.043040466783819634 -0.09739388286233619 -0.3468819761209388 -0.45005074723539357\nprint(corr('spkts'), corr('dpkts'), corr('spkts_log1p'), corr('dpkts_log1p'))\n# So we have better correlation with label after applying log1p. \ntotal.drop(['spkts', 'dpkts'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sttl & dttl\n* sttl: Source to destination time to live value \n* dttl: Destination to source time to live value\n\nFor sttl most of the anomalies have live values around 65 and 250. Its correlation with the target value is high too.\nHowever, for dttl both types have nearly same distribution. So the correlation with target is very low."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'sttl'\ndual_plot(col) # 0.62408238, after applying log1p 0.61556952425","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'dttl'\ndual_plot(col) # corr -0.09859087338578788","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sloss & dloss\n* sloss: Source packets retransmitted or dropped \n* dloss: Destination packets retransmitted or dropped\n\nSloss is highly correlated with spkts and sbytes (more than .91). Similarly dloss is highly correlated with dpkts and dbytes. \nHowever, though packets sent is related loss of packets, this isn't quite linearly related like packet number and size. So we keep both for now.\n\nValues are mostly between 0 to 3. Yet some values are more than several thousands."},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('sloss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So log1p makes it easier to differentiate\ndual_plot('sloss', method=np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['sloss_log1p'] = total['sloss'].apply(np.log1p)\ntotal['dloss_log1p'] = total['dloss'].apply(np.log1p)\n# 0.001828274080103508 -0.07596097807462938 -0.3454351103223904 -0.3701913238787703\nprint(corr('sloss'), corr('dloss'), corr('sloss_log1p'), corr('dloss_log1p') )\ntotal.drop(['sloss', 'dloss'], axis=1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## swin & dwin\nTCP window advertisement value. Except 0 and 255 other values(1-254) occur mostly once only. So we can separate them into 3 groups. And we also see after binning their correlation with target remains same."},{"metadata":{"trusted":true},"cell_type":"code","source":"total['swin'].value_counts().loc[lambda x: x>1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['dwin'].value_counts().loc[lambda x: x>1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corr('swin'), corr('dwin'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('swin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected = ['swin', 'dwin']\nkbins = preprocessing.KBinsDiscretizer(n_bins=[3, 3], encode='ordinal', strategy='uniform')\ntotal[selected] = pd.DataFrame(kbins.fit_transform(total[selected]), columns=selected)\nprint(corr('swin'), corr('dwin'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## stcpb & dtcpb\nTCP base sequence number. It has a really big range, 0 to 5e9. However, anomaly connections are mostly around 0. "},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'stcpb'\ndual_plot(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot(col, method=np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['stcpb_log1p'] = total['stcpb'].apply(np.log1p)\ntotal['dtcpb_log1p'] = total['dtcpb'].apply(np.log1p)\n# -0.2665849100492664 -0.2635428109654134 -0.33898970769021913 -0.33835676091281974\nprint(corr('stcpb'), corr('dtcpb'), corr('stcpb_log1p'), corr('dtcpb_log1p'))\ntotal.drop(['stcpb', 'dtcpb'], axis=1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### tcprtt & synack & ackdat\n* tcprtt is the TCP connection setup round-trip time, the sum of ’synack’ and ’ackdat’.\n* synack: TCP connection setup time, the time between the SYN and the SYN_ACK packets.\n* ackdat : TCP connection setup time, the time between the SYN_ACK and the ACK packets.\n\nAs tcprtt, is just the sum of other two features, it doesn't add any extra info to our models. So we can just drop it for now.\nApplying preprocessing on synack and ackdat didn't improve much. From graph we can see, anomaly connections generally have values around 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"total.drop(['tcprtt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('synack')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('ackdat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## trans_depth\nRepresents the pipelined depth into the connection of http request/response transaction. After depth 5 to 172 occurences are few."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'trans_depth'\nprint(corr(col)) # -0.0022256544\ncreate_count_df(col, total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## response_body_len\nActual uncompressed content size of the data transferred from the server’s http service. \nThe values range between 0 to 5.24M."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'response_body_len'\ndual_plot(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total[\"response_body_len_log1p\"] = total[\"response_body_len\"].apply(np.log1p)\n\n# slight improve\n# -0.018930127454048158 -0.03261972203078345\nprint(corr('response_body_len'), corr('response_body_len_log1p'))\ntotal.drop(['response_body_len'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ct_srv_src\nNo. of connections that contain the same service and source address in 100 connections according to the last time. Most of the normal connections are within 10. It is highly correlated to ct_srv_dst."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'ct_srv_src'\nprint(total[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corr(col)) # 0.24659616767\ndual_plot(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ct_srv_dst\nNo. of connections that contain the same service and destination address in 100 connections according to the last time. It is highly correlated to ct_srv_src too. It has a slight better correlation with label than ct_srv_src. So the other one can be dropped to check for possible improvement."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'ct_srv_dst'\nprint(total[col].value_counts())\n# graph is same as ct_srv_src\ndual_plot(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.2478122357. they are very correlated 0.97946681, need to check whether dropping one benefits\nprint(corr('ct_srv_dst'), corr('ct_srv_src', 'ct_srv_dst'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ct_src_ltm & ct_dst_ltm\nNo. of connections of the same source/destination address in 100 connections according to the last recorder time.\nValues are well between 0 to 51 and very few values after 48. They are much correlated , but not to the point of dropping one."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'ct_src_ltm'\nprint(corr(col))\ncreate_count_df(col, total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corr('ct_dst_ltm'))\ncreate_count_df('ct_dst_ltm', total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr('ct_src_ltm', 'ct_dst_ltm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ct_src_dport_ltm & ct_dst_sport_ltm\n* ct_src_dport_ltm : No of connections of the same source address and the destination port in 100 connections according to the last time.\n* ct_dst_sport_ltm: No of connections of the same destination address and the source port in 100 connections according to the last time."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['ct_src_dport_ltm', 'ct_dst_sport_ltm']:\n    print(corr(col))\n    print(create_count_df(col, total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr('ct_src_dport_ltm', 'ct_dst_sport_ltm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decimal Features\n## dur \nrecorded total duration. Normal connections are mostly within 5. However, this feature has a poor correlation with label.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'dur'\nprint(corr(col)) # 0.0290961170, correlation gets worse after log1p\ndual_plot(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## rate\nThis feature isn't mentioned is feature list. It has value upto 1M. Anomaly connections are mostly around 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'rate'\nprint(corr(col))\ndual_plot(col) # cor 0.3358, after applying log1p it becomes 0.31581108","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sinpkt & dinpkt\n* sinpkt: Source interpacket arrival time (mSec)\n* dinpkt: Destination interpacket arrival time (mSec)\n\nsinpkt is highly correlated with is_sm_ips_ports (0.9421206). Will dropping one of them benefit ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'sinpkt'\ncorr(col, 'is_sm_ips_ports')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corr(col)) # corr -0.1554536980863\ndual_plot(col) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot(col, method=np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('dinpkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['sinpkt_log1p'] = total['sinpkt'].apply(np.log1p)\ntotal['dinpkt_log1p'] = total['dinpkt'].apply(np.log1p)\n\n# slight improve in correlation\n# -0.1554536980867726 -0.030136042428744566 -0.16119699304378052 -0.07408113676641241\nprint(corr('sinpkt'), corr('dinpkt'), corr('sinpkt_log1p'), corr('dinpkt_log1p'))\ntotal.drop(['sinpkt', 'dinpkt'], axis=1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sload & dload\n* sload: Source bits per second\n* dload: Destination bits per second\n\nThe values are really big and in bits."},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('sload')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('dload')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total['sload_log1p'] = total['sload'].apply(np.log1p)\ntotal['dload_log1p'] = total['dload'].apply(np.log1p)\n# 0.16524867685764016 -0.35216880416636837 0.3397788822586144 -0.5919440288535992\nprint(corr('sload'), corr('dload'), corr('sload_log1p'), corr('dload_log1p'))\ntotal.drop(['sload', 'dload'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sjit & djit\nSource and Destination jitter in mSec. Preprocessing didn't improve anything."},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('sjit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dual_plot('djit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"features.to_csv('features.csv', index=False)\ntrain = total[total['type']=='train'].drop(['type'], axis=1)\ntest = total[total['type']!='train'].drop(['type'], axis=1)\ntrain.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}