{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"dadf5c3b-6e5f-cca3-b088-cbf9c2c9b427"},"source":"**<h1> This is my first notebook publish in Kaggle. </h1>**\n*<h2>Hope I could add something</h2>*\n\n***\n\n*Please note that this notebook is in Python 2.7. So I dont know how well it will run in online here in Kaggle. However, if you use Python 2.7, you can download the notebook and run it locally in your system -----------*\n\n***"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12bc866c-bf64-5620-fb5c-254a09fbd511"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"},{"cell_type":"markdown","metadata":{"_cell_guid":"fde2caeb-dd1e-8904-44bc-9ce7724c9172"},"source":"# Part 1: Loading and Processing the data\n### The data is loaded as a DF and the last column with NaNs are removed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95537afa-bba0-91ff-d4c1-a92c477f9d0c"},"outputs":[],"source":"df = pd.read_csv(\"../input/data.csv\")\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07f6a4c7-04a1-7a42-ac62-d8e0c3a5989e"},"outputs":[],"source":"df.index = df['id']\ndf.drop([\"id\", \"Unnamed: 32\"], axis = 1, inplace = True)\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5127e8f-47d3-ffe8-5a9a-16bc4e0f46cf"},"source":"# Part 2: Running basic analysis and Visualizations"},{"cell_type":"markdown","metadata":{"_cell_guid":"9521b51a-c248-fc65-39fc-a2b4f91ab519"},"source":"### Dividing the dataset into ones which are malignant and ones which are benign and vialulizing all the features\n#### To find out which features may be important and which are not"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a507897-2975-5b0d-0aa6-02b2ed44ee06"},"outputs":[],"source":"df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8f9134b-53ba-9349-a3fa-292547df8972"},"outputs":[],"source":"df.boxplot(by = 'diagnosis', figsize = (20, 20))\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d9527b3-b8ee-bd95-1dd8-3dc89eac7b6f"},"source":"## Scaling the data\n#### Since the data is highly variable and any feature which has low variance will be neglected, in order to bring them to a level playing field, standard scaling is done."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3fc61306-384c-9830-c398-38641aa6271c"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler as ss\ndft = df.drop(['diagnosis'], axis = 1, inplace = False)\nscaler = ss().fit(dft)\nscaled_df = pd.DataFrame(scaler.transform(dft))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"195cd1c1-ed09-c042-bc85-24d375921603"},"outputs":[],"source":"scaled_df.index = df.index\nscaled_df.columns = dft.columns\nscaled_df['diagnosis'] = df['diagnosis']\nscaled_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"958d7df0-8eb2-eda6-97a0-2eb3d58e1c95"},"outputs":[],"source":"scaled_df.boxplot(by = \"diagnosis\", figsize = (20, 20))\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b2a5b683-5c36-2eaa-6981-3831d78cadb6"},"source":"### The above boxplots gives us a very simple way of visualizing all th variables at once. \n#### It gives an indication as to which variables might be good for predictions and which wont."},{"cell_type":"markdown","metadata":{"_cell_guid":"b31a45df-e033-4ff1-43b4-9df74f5efb78"},"source":"# Part 3: Subsetting the data to training and testing sets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3f09312-662d-8054-7eb5-f1b7f1792cb4"},"outputs":[],"source":"diagnosis = df.diagnosis\ndf.drop(['diagnosis'], axis = 1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25f63453-a171-3aaa-b80e-1de604dc10c5"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split as tspl\ndf_train, df_test, diag_train, diag_test = tspl(df, diagnosis, test_size = 0.33)"},{"cell_type":"markdown","metadata":{"_cell_guid":"15e2f467-442b-f2d5-ce78-78e700707eab"},"source":"# Part 4: Running the KNN Classifier"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3ad7b96-0594-bcd0-335b-5e469b1daf43"},"source":"### K Nearest Neighbour Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07977a34-50d6-69b0-8cd5-a5084e762985"},"outputs":[],"source":"from sklearn.neighbors import KNeighborsClassifier as KNN\nknn = KNN(n_neighbors = 5)\nknn.fit(df_train, diag_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06e9b836-9ac0-b33f-2707-82b110ff98a4"},"outputs":[],"source":"knn.score(df_test, diag_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c770cd9-46be-ea01-1296-ea0b220d6aa4"},"source":"### We seem to get a pretty good prediction using the KNN Classifier. \n#### However I would like to improve the classification accuracy.\n##### For this I would use something interesting:\n##### First I would take the difference in means among all the scaled features. And then use these differences to remove extra features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1528a9b-bb3c-dcac-cdeb-970407c272be"},"outputs":[],"source":"dft = df_train.copy()\nscaler = ss().fit(dft)\nscaled_dft = pd.DataFrame(scaler.transform(dft))\nscaled_dft.index = dft.index\nscaled_dft.columns = dft.columns\nscaled_dft['diagnosis'] = diag_train\nscaled_dft.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b64af737-c47b-8a19-3895-dfa3d689400b"},"outputs":[],"source":"grouped_mean = scaled_dft.groupby(['diagnosis']).mean()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e187462d-88d2-6618-3e16-63e4d55463b0"},"outputs":[],"source":"diff_col = np.array(grouped_mean.loc['B']) - np.array(grouped_mean.loc['M'])\ndiff_col = np.absolute(diff_col)\ndiff_col = list(diff_col)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dea0d584-ff8f-f1db-1396-dd69fbbc6b47"},"source":"### I removed a lot of columns from the dataframe. Those will not be used for predictions.\n#### These features did not show a good significant difference among the \"M\" and the \"B\" diagnosed patients."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"565ee698-4279-4cee-4e18-ec61015314eb"},"outputs":[],"source":"to_drop = []\ncolumns = df.columns\nfor i in range(len(columns)):\n    if diff_col[i] < 1:\n        to_drop.append(columns[i])\n    \ndftr = df_train.copy()\ndftr.drop(to_drop, axis = 1, inplace = True)\n\ndfts = df_test.copy()\ndfts.drop(to_drop, axis = 1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"373c9d48-da0f-7535-4095-5a7d133bec06"},"outputs":[],"source":"knn1 = KNN(n_neighbors = 5)\nknn1.fit(dftr, diag_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff4f4d70-04dd-ccc7-4536-ee44a7d09402"},"outputs":[],"source":"knn1.score(dfts, diag_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"31be8558-b67f-ee7a-f49a-bcf440be1f8b"},"source":"## It seems that the removal of all those columns didnot change the score of the prediction\n#### So the removal of the not so different features was the right step"},{"cell_type":"markdown","metadata":{"_cell_guid":"f44c20aa-f11b-81a1-e2ea-7c2b2c47bcb1"},"source":"## Now I will automate a process which will run the KNN Classifier for different values of N and find the best among them"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74626ba0-0f93-20c3-459a-2bc1c55ee36a"},"outputs":[],"source":"for i in range(1, 20):\n    knn = KNN(n_neighbors = i)\n    knn.fit(dftr, diag_train)\n    score = knn.score(dfts, diag_test)\n    print(\"N = \" + str(i) + \" :: Score = \" + str(score))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e68d7f8c-11b0-be46-5672-338bd0919f72"},"source":"## It seems that after n = 9, the score remains fairly constant at 0.95.\n### Also, at n = 5: score = 0.94.\nTherefore, the optimal n is totally at the user's discretion.\n\nPersonally I would prefer n to be as low as possible. Since at n=5, the model gives pretty good accuracy, I would go with n = 5"},{"cell_type":"markdown","metadata":{"_cell_guid":"aacc5711-78d1-7c61-3f8c-8d47c5d3c55e"},"source":"## Now I would be looking at the actual probabilities of the prediction\n### I will compute the prediction probabilities of the KNN classifier and check the statistics of the prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5d18c13-b06e-2db4-9ac5-46d3d1be64ca"},"outputs":[],"source":"knn = KNN(n_neighbors= 5)\nknn.fit(dftr, diag_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3cd21da-bac4-ca2a-2e7d-c1ce26395019"},"outputs":[],"source":"prediction_prob = knn.predict_proba(dfts)\npredictions = knn.predict(dfts)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"599bb93f-682d-8f39-1b8a-3273f3f6a501"},"outputs":[],"source":"from sklearn.metrics import confusion_matrix as cfm\ncfm(predictions, diag_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1efe2aec-6dbd-9077-af7d-b514a0dc76f4"},"source":"## Tabulating the confusion matrix at different values of the cutoff"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b988f206-c2f5-de90-a9df-b4bb6c61013f"},"outputs":[],"source":"def predict_cutoff(pred_prob, cutoff = 0.5):\n    prediction_var = []\n    for element in pred_prob:\n        if element[0] > cutoff:\n            prediction_var.append(\"B\")\n        else:\n            prediction_var.append(\"M\")\n    return np.array(prediction_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c1a8c1f-6416-3a51-ed6d-c4ae77ce5367"},"outputs":[],"source":"for i in range(1, 10):\n    i = float(i) / 10\n    print(\"Cutoff: \" + str(i) + \";\\n Confusion Matrix: \")\n    print(cfm(predict_cutoff(prediction_prob, i), diag_test))\n    print(\"\\n\\n\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9207edd-0eb3-0efa-1dc2-842c4773f03e"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}