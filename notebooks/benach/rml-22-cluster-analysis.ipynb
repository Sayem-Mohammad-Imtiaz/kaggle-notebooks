{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\n# Data Preprocessing - Data import\nos.chdir(r'/kaggle/input/telco-customer-churn')\nMy_data = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\nMy_data.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(My_data.describe())\nMy_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Cleansing and Analysis\nM_Values = My_data.isnull()\nM_Values.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize the missing data\nimport seaborn as sns\nsns.heatmap(data = M_Values, yticklabels=False, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace values for SeniorCitizen as a categorical feature\nMy_data['SeniorCitizen'] = My_data['SeniorCitizen'].replace({1:'Yes',0:'No'})\nnum_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\nMy_data[num_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_segment(column_name:str) -> 'grouped_dataframe':\n    segmented_df = My_data[[column_name, 'Churn']]\n    segmented_churn_df = segmented_df[segmented_df['Churn'] == 'Yes']\n    grouped_df = segmented_churn_df.groupby(column_name).count().reset_index().rename(columns = {'Churn':'Churned'})\n    total_count_df = segmented_df.groupby(column_name).count().reset_index().rename(columns = {'Churn':'Total'})\n    merged_df = pd.merge(grouped_df, total_count_df, how = 'inner', on = column_name)\n    merged_df['Percent_Churned'] = merged_df[['Churned','Total']].apply(lambda x: (x[0] / x[1]) * 100, axis=1) \n    return merged_df\n\ncategorical_columns_list = list(My_data.columns)[1:5] + list(My_data.columns)[6:18]\n\ngrouped_df_list = []\n\nfor column in categorical_columns_list:\n    grouped_df_list.append( categorical_segment( column ) )\n    \ngrouped_df_list[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Churn by categorical features\nimport matplotlib.pyplot as plt \nfor i , column in enumerate(categorical_columns_list):\n    fig, ax = plt.subplots(figsize=(13,5))\n    plt.bar(grouped_df_list[i][column] , [ 100 - i for i in grouped_df_list[i]['Percent_Churned'] ],width = 0.1, color = 'g')\n    plt.bar(grouped_df_list[i][column],grouped_df_list[i]['Percent_Churned'], bottom =  [ 100 - i for i in grouped_df_list[i]['Percent_Churned'] ],\n            width = 0.1, color = 'r')\n    plt.title('Percent Churn by ' + column)\n    plt.xlabel(column)\n    plt.ylabel('Percent Churned')\n    plt.legend( ('Retained', 'Churned') )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Churn by numerical features\ndef continous_var_segment(column_name:str) -> 'segmented_df':\n    segmented_df = My_data[[column_name, 'Churn']]\n    segmented_df = segmented_df.replace( {'Churn': {'No':'Retained','Yes':'Churned'} } )\n    segmented_df['Customer'] = ''\n    return segmented_df\n\ncontinous_columns_list = [list(My_data.columns)[18]] + [list(My_data.columns)[5]]\n\n\ncontinous_segment_list = []\n\nfor var in continous_columns_list:\n    continous_segment_list.append( continous_var_segment(var) )\n    \nimport seaborn as sns\nsns.set('talk')\n\nfor i, column in enumerate( continous_columns_list ):\n    fig, ax = plt.subplots(figsize=(8,11))\n    sns.violinplot(x = 'Customer', y = column, data = continous_segment_list[i], hue = 'Churn', split = True)\n    plt.title('Churn by ' + column)\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing  tenure and monthly charges and using K-means clustering to cluster churned customers based on them.\n\nfrom sklearn.cluster import KMeans \nfrom sklearn.preprocessing import MinMaxScaler\n\nmonthlyp_and_tenure = My_data[['MonthlyCharges','tenure']][My_data.Churn == 'Yes']\n\nscaler = MinMaxScaler()\nmonthly_and_tenure_standardized = pd.DataFrame( scaler.fit_transform(monthlyp_and_tenure) )\nmonthly_and_tenure_standardized.columns = ['MonthlyCharges','tenure']\n\nkmeans = KMeans(n_clusters = 3, random_state = 42).fit(monthly_and_tenure_standardized)\n\nmonthly_and_tenure_standardized['cluster'] = kmeans.labels_\n\nfig, ax = plt.subplots(figsize=(13,8))\nplt.scatter( monthly_and_tenure_standardized['MonthlyCharges'], monthly_and_tenure_standardized['tenure'],\n           c = monthly_and_tenure_standardized['cluster'], cmap = 'Spectral')\n\nplt.title('Clustering churned users by monthly Charges and tenure')\nplt.xlabel('Monthly Charges')\nplt.ylabel('Tenure')\n\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pre-processing the data using label encoding and one hot encoding to get it ready for ML Model\n\nMy_data_filtered = My_data.drop(['TotalCharges','customerID'], axis = 1)\n\ndef encode_binary(column_name:str):\n    global My_data_filtered\n    My_data_filtered = My_data_filtered.replace( { column_name: { 'Yes': 1 , 'No': 0 } }  )\n    \n\nbinary_feature_list = list(My_data_filtered.columns)[1:4] + [list(My_data_filtered.columns)[5]] \\\n+ [list(My_data_filtered.columns)[15]]  \\\n+ [list(My_data_filtered.columns)[18]]\n    \nfor binary_feature in binary_feature_list:\n    encode_binary(binary_feature)\n    \n\nMy_data_processed = pd.get_dummies( My_data_filtered, drop_first = True )\n\nMy_data_processed.head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"My_data.Churn.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing all the necessary librabries\nimport numpy as np \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport sklearn.metrics as metrics\n%matplotlib inline \n\nX = np.array( My_data_processed.drop( ['Churn'] , axis = 1 ) )\ny = np.array( My_data_processed['Churn'] )\n\nX_train,X_test,y_train,y_test = train_test_split( X, y , test_size = 0.2 , random_state = 42 )\n\ndef get_metrics( model ):\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)\n    y_actual = y_test \n    print()\n    print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n    print()\n    print('Accuracy on unseen hold out set:' , metrics.accuracy_score(y_actual,y_pred) * 100 , '%' )\n    print()\n    f1_score = metrics.f1_score(y_actual,y_pred)\n    precision = metrics.precision_score(y_actual,y_pred)\n    recall = metrics.recall_score(y_actual,y_pred)\n    score_dict = { 'f1_score':[f1_score], 'precision':[precision], 'recall':[recall]}\n    score_frame = pd.DataFrame(score_dict)\n    print(score_frame)\n    print()\n    fpr, tpr, thresholds = metrics.roc_curve( y_actual, y_prob[:,1] ) \n    fig, ax = plt.subplots(figsize=(8,6))\n    plt.plot( fpr, tpr, 'b-', alpha = 0.5, label = '(AUC = %.2f)' % metrics.auc(fpr,tpr) )\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend( loc = 'lower right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model 1: Random Forest \nrf = RandomForestClassifier( n_estimators = 20, n_jobs=-1, max_features = 'sqrt', random_state = 42 )\nparam_grid1 = {\"min_samples_split\": np.arange(2,11), \n              \"min_samples_leaf\": np.arange(1,11)}\nrf_cv = GridSearchCV( rf, param_grid1, cv=5, iid = False )\nrf_cv.fit(X_train,y_train)\nprint( rf_cv.best_params_ )\nprint( rf_cv.best_score_ )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}