{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Acknowledgement:** Many thank to Fabien Daniel for his advices and encouragements."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 Preview and Cleaning"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"d=pd.read_csv(\"../input/data.csv\", encoding='ISO-8859-1')\nd.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"miss = []\nfor col in d.columns:\n    i=d[col].isnull().sum()\n    miss_v_p = i*100/d.shape[0]\n    miss.append(miss_v_p)\n    print ('{} -----> {}%'.format(col, 100-i*100/d.shape[0]))\n\ndico = {'columns': d.columns, 'filling rate': 100-np.array(miss), 'taux nan': miss}\n#print(miss, dico['taux de remplissage'])\ntr=pd.DataFrame(dico)\nr = range(tr.shape[0])\nbarWidth=0.85\nplt.figure(figsize=(20,8))\nplt.bar(r, tr['filling rate'], color='#a3acff', edgecolor='white', width=barWidth)\nplt.bar(r, tr['taux nan'], bottom=tr['filling rate'], color ='#b5ffb9', edgecolor= 'white', width=barWidth)\nplt.title('fill rate representation')\nplt.xticks(r, tr['columns'], rotation='vertical')\nplt.xlabel('columns')\nplt.ylabel('filling rate')\nplt.margins(0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def count_lab(d, lab='Country'):\n    \"\"\"Compte le nombre d'éléments identique dans la colonne 'lab'\"\"\"\n    r=d.groupby(lab).count()\n    r['nb']=r.iloc[:,0]\n    r.sort_values(by= 'nb', ascending=False, inplace=True)\n    dico={lab: r.index, 'nb':r['nb']}\n    return pd.DataFrame(dico)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be particularly interested in **product** types"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"label='Description'\ng1=count_lab(d, lab=label).head(30)\n#display(g1)\nplt.figure(figsize=(8,30))\nplt.barh(g1['Description'].apply(str), g1['nb'])\nplt.gca().invert_yaxis()\nplt.axvline(x=2000, color='b')\nplt.text(2082, -1, '>2000', color='b')\nplt.axvline(x=1000, color='r')\nplt.text(780, -1, '<1000', color='r')\nplt.grid(True)\nplt.title('the 30 Most Present Products')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_price=d.loc[:,['Description', 'UnitPrice']].groupby('Description').mean()\nimport seaborn as sns\n\ndesc_price.boxplot('UnitPrice')\nplt.title('unit price distribution')\nplt.ylim((0,70))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Products are mainly cheap."},{"metadata":{},"cell_type":"markdown","source":"## Cleaning"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Rq: Missing Customer ID and Negative Quantities transactions seem to be related to delivery issues:')\nd[(d['CustomerID'].isnull())&(d['Quantity']<0)].sample(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(d[d['CustomerID'].isnull()].shape, d.shape, )\nd.dropna(axis=0, subset=['CustomerID'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Feature engineering\nIn this part, we will create some interesting features to characterize customers. In thematic clustering, we will use only 'entropy' and 'diversity index'. Other features can be use for RFM segmentation.\n## 2.1 On the Dataset : "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#the price of the transaction for a product\nd['TotalPrice']=d['UnitPrice']*d['Quantity']\n\n#The total of the purchase invoice\nInvoice = d.loc[:,['InvoiceNo', 'TotalPrice']].groupby('InvoiceNo').sum()\nInvoice.rename(columns={'TotalPrice':'InvoiceTotal'}, inplace=True)\nd=d.merge(Invoice, on='InvoiceNo')#rq: Here, merge remove any missing values from the pivot category.\n\n# the total amount spent in the DataFrame per customer\ntotal_y = d.loc[:,['CustomerID', 'TotalPrice']].groupby('CustomerID').sum()\ntotal_y.rename(columns={'TotalPrice': 'TotalYear'}, inplace=True)\nd=d.merge(total_y, on='CustomerID')\n\n\nd.sample(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n##### Automation function of the code above :"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"Fonctions relatives au variables de coûts\"\"\"\ndef cost_info(d):\n    \"\"\"Add to 'd' informative variables on cost. In case these variables already exist in 'd', they are reset.\"\"\"\n    for col in ['TotalPrice', 'nb_inv', 'TotalYear', 'InvoiceTotal']:\n        try:\n            d.drop(columns=[col], inplace=True)\n            print('Resetting the cost variable {} succeeded'.format(col))\n        except:\n            print('Column {} not present'.format(col))\n    \n    #prix du produit * quantité achetée\n    d['TotalPrice']=d['UnitPrice']*d['Quantity']\n    \n    \n    if type(d)==pd.Series:\n        d=pd.DataFrame([d])\n        \n    \n    #total par commande\n    Invoice = d.loc[:,['InvoiceNo', 'TotalPrice']].groupby('InvoiceNo').sum()\n    Invoice.rename(columns={'TotalPrice':'InvoiceTotal'}, inplace=True)\n    d=d.merge(Invoice, on='InvoiceNo')\n        \n    #Total dépensé par le client\n    total_y = d.loc[:,['CustomerID', 'TotalPrice']].groupby('CustomerID').sum()\n    total_y.rename(columns={'TotalPrice': 'TotalYear'}, inplace=True)\n    d=d.merge(total_y, on='CustomerID')\n        \n    #nombre de commandes sur l'année\n    invoices=d.loc[:, ['CustomerID', 'InvoiceNo', 'TotalYear']].groupby('InvoiceNo').mean()\n    nb_invoices = invoices.groupby('CustomerID').count()\n    nb_invoices.rename(columns={'TotalYear':'nb_inv'}, inplace=True)\n    d=d.merge(nb_invoices, on='CustomerID')\n        \n    \n    return d\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"Fonctions relatives au variables de coûts\"\"\"\ndef cost_info(d):\n    \"\"\"Add to 'd' informative variables on cost. In case these variables already exist in 'd', they are reset.\"\"\"\n    for col in ['TotalPrice', 'nb_inv', 'TotalYear', 'InvoiceTotal']:\n        try:\n            d.drop(columns=[col], inplace=True)\n            print('Resetting the cost variable {} succeeded'.format(col))\n        except:\n            print('Column {} not present'.format(col))\n    \n    #prix du produit * quantité achetée\n    d['TotalPrice']=d['UnitPrice']*d['Quantity']\n    \n    \n    if type(d)==pd.Series:\n        d=pd.DataFrame([d])\n        \n    \n    #total par commande\n    Invoice = d.loc[:,['InvoiceNo', 'TotalPrice']].groupby('InvoiceNo').sum()\n    Invoice.rename(columns={'TotalPrice':'InvoiceTotal'}, inplace=True)\n    d=d.merge(Invoice, on='InvoiceNo')\n        \n    #Total dépensé par le client\n    total_y = d.loc[:,['CustomerID', 'TotalPrice']].groupby('CustomerID').sum()\n    total_y.rename(columns={'TotalPrice': 'TotalYear'}, inplace=True)\n    d=d.merge(total_y, on='CustomerID')\n        \n    #nombre de commandes sur l'année\n    invoices=d.loc[:, ['CustomerID', 'InvoiceNo', 'TotalYear']].groupby('InvoiceNo').mean()\n    nb_invoices = invoices.groupby('CustomerID').count()\n    nb_invoices.rename(columns={'TotalYear':'nb_inv'}, inplace=True)\n    d=d.merge(nb_invoices, on='CustomerID')\n        \n    \n    return d\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1.2 DateTime variables:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nd['InvoiceDate']=pd.to_datetime(d['InvoiceDate'])\nnow=d['InvoiceDate'].max()\nnow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['date']=d['InvoiceDate'].apply(lambda x : x.date())\nd['hour']=d['InvoiceDate'].apply(lambda x : x.time())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Variable 'nb_inv': the number of transactions per customer"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_POS = d[d['Quantity']>0]#To not consider the refunds as transactions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"invoices=d_POS.loc[:, ['CustomerID', 'InvoiceNo', 'TotalYear']].groupby('InvoiceNo').mean()\nnb_invoices = invoices.groupby('CustomerID').count()\nnb_invoices.rename(columns={'TotalYear':'nb_inv'}, inplace=True)\n#display(nb_invoices)\nnb_invoices.sort_values(by='nb_inv', ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(nb_invoices['nb_inv'], bins=200)#plt.plot(nb_invoices, '.')\nplt.title('Number of customers for each number of orders')\nplt.xlim((0,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=d.merge(nb_invoices, on='CustomerID')\nd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## 2.2 New DataFrame : 'Date' which characterizing the customers\n ### 2.2.1 Time data:\n#### First acquisition, last acquisition"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfirst_date = d.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').min().rename(columns={ 'InvoiceDate' : 'first_date'})\nlast_date = d_POS.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').max().rename(columns={ 'InvoiceDate' : 'last_date'})# we use d_POS to not consider refunds\ndisplay(last_date.info())\n\ndate = first_date.merge(last_date, on='CustomerID')\n#display(second_last)\n#display(date)\ndate['since_last']=date['last_date'].apply(lambda x: x.date()-now.date())\ndate['since_first']=date['first_date'].apply(lambda x: x.date()-now.date())\ndate.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Penultimate acquisition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_unique=date['since_last']-date['since_first']>dt.timedelta(days=0)\n\n\nnot_u=date.loc[not_unique,:]\n\nd_not_u = pd.DataFrame(index=not_u.index)\n#d_not_u['CustomerID']=not_u.index\n\n\n\ninvoice=d[d['Quantity']>0].groupby('InvoiceNo').nth(0)#We do not count refunds\ninvoice['InvoiceNo']=invoice.index\n\n\nd_not_u=d_not_u.merge(invoice, on='CustomerID', how='inner')\nd_not_u=d_not_u.sort_values(['CustomerID','InvoiceDate'], ascending=False)\nsecond_last=d_not_u.loc[:,['CustomerID', 'InvoiceDate']].groupby('CustomerID').nth(1)\n\ndate=pd.concat([date, second_last.rename(columns={ 'InvoiceDate' : 'second_last'})], axis=1)\ndate['since_sec_last']=date['second_last'].apply(lambda x: x.date()-now.date())\ndate.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the number of days to integers\nfor col in ['since_last', 'since_first', 'since_sec_last']:\n    date[col]=date[col].apply(lambda x: pd.to_timedelta(x).days)\ndate.head()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_unique = date['second_last'].isnull().sum()\nnb_cust= date.shape[0]\nprint(\"{}% of customers only made one involved during the year, {} customers on {}\".format(round(nb_unique*100/nb_cust,2),\n                                                                                                               nb_unique,\n                                                                                                               nb_cust\n                                                                                                              ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.1 Customer diversity index 'ind_div' and entropy 'product_entropy'\nWe create these two features to better characterize customer behavior"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_t_p=d_POS['Description'].nunique()\nprint('total number of products : ',nb_t_p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Let us introduce 'quant_price' : the DataFrame of the quantity of each product per customers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#quantity of each product per customers\nquantity = d_POS.loc[:,['CustomerID', 'Description', 'Quantity']].groupby(['CustomerID', 'Description']).sum()\ncol_ind = quantity.index.get_level_values(0)\nquantity.reset_index(level=[0,1], inplace=True)\n\n# Let add the Unit price per product\nUnitPrice=d_POS.loc[:, ['Description', 'UnitPrice']].groupby('Description').mean()\nquant_price = quantity.merge(UnitPrice, on='Description')\n\ndisplay(quant_price.head())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n** - Entropy Products ** identifies the distribution of products purchased by the customer. Thus, it is close to zero if this distribution is uniform and increases as much as the probability of having a non-uniform distribution is large.<br>\n** note 1 **: We could define entropy products only on ** product quantities **, but it is ** better to weight by the price of the product.** Indeed a customer will tend to buy in larger quantities cheap products than expensive products. (That is why we add UnitPrice in our DataFrame 'quant_price')<br>\n![](http://)** note 2 **: The formula of entropy product is inspired by the RMSLE."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Becouse we focus on positive quantity, we must used the total monetary value per customer ('TotalYear') only on positive quantity : \nRefunds can skew our entropy, so we have to recalculate TotalYear on positive quantity only\"\"\"\nfor c in quantity['CustomerID'].unique():\n    \n    c_quanti = quant_price[quant_price['CustomerID']==c]\n    date.loc[c,'quantity_t']=c_quanti.loc[:,'Quantity'].sum()\n    date.loc[c,'nb_prod_diff']=c_quanti.shape[0]\n    \"\"\"cancellations can skew our entropy, so we have to recalculate TotalYear\"\"\"\n    moyT = (c_quanti['Quantity']*c_quanti['UnitPrice']).sum()/date.loc[c,'nb_prod_diff']\n    \n    date.loc[c, 'entropy_corrige']=np.sqrt(np.square(np.log(1+c_quanti['Quantity']*c_quanti['UnitPrice'])-np.log(1+moyT)).sum()/date.loc[c,'nb_prod_diff'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"![](http://)** - The Diversity Index ** mesur the diversity of product bought by one custumer therfore, identifies customers who are still buying the same products from those who buy a lot of different products. <br>\nFor tha same number of bought product, a customer who buys a lot of diffents products has a higher diversity index than a customer who would buy only one item. \n** note **: The formula of diversity index is simply the ratio : <br>  \n(the number of differents products purchased by the customer) / (the total number of products purchased by that consumer)."},{"metadata":{"trusted":true},"cell_type":"code","source":"date['ind_div']=((date['nb_prod_diff'])/date['quantity_t'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Automation function of the above code :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def entropie_prod(d):\n    d_POS=d[d['Quantity']>0]#Here we avoid the case where customer have a negative total quantity.\n    \n    #quantity of each product per customers\n    quantity = d_POS.loc[:,['CustomerID', 'Description', 'Quantity']].groupby(['CustomerID', 'Description']).sum()\n    col_ind = quantity.index.get_level_values(0)\n    quantity.reset_index(level=[0,1], inplace=True)\n    \n    # Let add the Unit price per product\n    UnitPrice=d_POS.loc[:, ['Description', 'UnitPrice']].groupby('Description').mean()\n    quant_price = quantity.merge(UnitPrice, on='Description')\n    \n    # the DataFrame we 'll return\n    date=pd.DataFrame(index=col_ind.unique(), columns=['quantity_t'])\n    \n    #Entropy Produit\n    \"\"\"Becouse we focus on positive quantity, we must used the total monetary value per customer ('TotalYear') only on positive quantity : \nRefunds can skew our entropy, so we have to recalculate TotalYear on positive quantity only\"\"\"\n    for c in quantity['CustomerID'].unique():\n    \n        c_quanti = quant_price[quant_price['CustomerID']==c]\n        date.loc[c,'quantity_t']=c_quanti.loc[:,'Quantity'].sum()\n        date.loc[c,'nb_prod_diff']=c_quanti.shape[0]\n        \n        \n        \"\"\"cancellations can skew our entropy, so we have to recalculate TotalYear\"\"\"\n        moyT = (c_quanti['Quantity']*c_quanti['UnitPrice']).sum()/date.loc[c,'nb_prod_diff']\n    \n        date.loc[c, 'entropy_corrige']=np.sqrt(np.square(np.log(1+c_quanti['Quantity']*c_quanti['UnitPrice'])-np.log(1+moyT)).sum()/date.loc[c,'nb_prod_diff'])\n    \n    #Diversity index\n    date['ind_div']=((date['nb_prod_diff'])/date['quantity_t'])\n    \n    return date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Thematic Clustering\n## 3.1 Vectorization:\n### New Dataframe'cust_them' : Distribution of words on the customer's market share\nHere, to simplify word processing, we will restrict the data to positive quantities.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_POS.head() #data restrict to positive quantity (not a refund or cancellation...)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef weights_words(d, label='Description', sep=\" \", nb=None, min_occ=None):\n    \"\"\"\n    Associate with each word present in the description of a product, a score that corresponds to the money spent by the customer in this product (quantity purchased * Unit price).\n     Rq: In practice d will be the DataFrame for a client only.\n     \n    Associe à chaque mots presents dans la description d'un produit, un score qui correspond à l'argent dépenser du client dans ce produit ( quantité acheté * prix Unitaire).\n    Rq: En pratique d sera le DataFrame pour un client uniquement.\n    \"\"\"\n    #count=dict()\n    words=[]\n    for ind in d.index:\n        #price=d.loc[ind, 'TotalPrice']\n        #if\n        words+=str(d.loc[ind, label]).split(sep)*int(d.loc[ind, 'TotalPrice']*10)#le rapport sera arrondie à 10 centimes près\n        #else:\n        #for w_neg in str(d.loc[ind, label]).split(sep)*int((-price)*10):\n                #words=words.remove(w_neg)\n       \n    count=Counter(words)        \n    if nb==None:\n        if min_occ==None:\n            return count\n        else:\n            c_words=pd.Series(count)\n            rare = c_words[c_words<min_occ].index\n            c_words.drop(index=rare, inplace=True)\n            return dict(c_words)\n    else:\n        return(dict(count.most_common(nb)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_cust_them(d):\n    \"\"\"\n    Returns a Dataframe that each client associates a proportion for each keyword.\n     This proportion represents the presence of the word on the customer's market share:\n         Each product has a description in which keywords are extracted.\n         These words are then weighted by the (price of the product) * (quantity purchased) / (total of the customer's expenses)\n         I thus obtain a score between 0 and 1. which corresponds to the basis of the proportion of purchases of a product for the customer.\n         As the products are broken down into words, I implicitly add up the scores (proportions) of the words that\n         are found in several products.\n         We have an indication of the interest (a posteriori) converted from the customer.\n     Rq: The hypothetical case where score> 1 is theoretically possible if the word appears several times in the description of the same product\n    \n    \n    Retourne un Dataframe qui a chaque client associe une proportion pour chaque mots clefs.\n    Cette proportion représente la presence du mot sur la part de marché du client:\n        A chaque produit correspond une description dans laquelle on extrait des mots clefs. \n        Ces mots sont ensuite pondérés par le (prix du produit)*(quantité achetée)/(total des dépenses du client)\n        J'obtiens donc un score entre 0 et 1. qui correspond à la base aux proportions d'achats d'un produit pour le client.\n        Les produits étant décomposés en mots, j'additionne implicitement les scores (proportions) des mot qui \n        se retrouvent dans plusieurs produit.\n        On a une indication de l'interet (à posteriori) converti du client.\n    Rq: Le cas hypothétique où score >1 est théoriquement possible ssi le mot apparait plusieur fois dans la description d'un meme produit.\n    \"\"\"\n    dt=d[d['Quantity']>0]\n    print('We keep positive quantities : ')\n    dt=cost_info(dt)\n    cust_them_dft = pd.DataFrame(index=dt['CustomerID'].unique())\n    \n    #Associate for each customer an affinity score on the most present words in the product description\n    #Associe pour chaque client un score d'affinité sur les mots les plus présent dans la description des produits.\n    for c in dt['CustomerID'].unique():\n        d_cust = dt[dt['CustomerID']==c]\n        y_total=d_cust.loc[:, 'TotalYear'].mean()\n        \n        l=weights_words(d_cust, min_occ=2)#None\n        \n        \n        for lab in l.keys():\n            cust_them_dft.loc[c, lab]=l[lab]\n        cust_them_dft.loc[c, :]=cust_them_dft.loc[c, :]/(y_total*10)\n        # Here I divide by the total quantities purchased from the customer\n        #rq: The factor 10 has been added to get an accuracy of 10 cents.\n        #Indeed the prices have been converted in integer * 10\n    return cust_them_dft\n\n\ndef cust_them_clean(cust_them, w_all_min=16, affiner=True, keep_max=False):\n    \"\"\"Cleans the DataFrame 'cust_them' by removing unnecessary words (columns).\n     it also removes columns with a total weight less than 'w_all_min'.\n    \"\"\"\n    try:\n        cust_them.drop(columns=[''], inplace=True)\n    except:\n        print(\"Column '' is not in the DataFrame\")\n        \n    #deletes words that are not nouns\n    is_not_noun = lambda pos: pos[:2]!='NN'\n    not_noun=[]\n    cust_them.columns = map(str.lower, cust_them.columns)\n    l_w_pos = nltk.pos_tag(cust_them.columns)\n    not_noun=[w for (w, pos) in l_w_pos if is_not_noun(pos)]    \n    \n    if affiner:\n        #list of arbitrary words to add:\n        l_words_int = ['photo' , 'girl', 'ceramic', 't-shirt', \n                   'origami', 'xmas', 'garden', 'gift', 'lantern', 'paint', 'marmalade', \n                   'poncho', 'bonbon', 'ivy', 'guitar', 'laser', 'boys', 'halloween', 'cloth', 't-light', 'baby', 'doormat']\n\n    for a in l_words_int:\n        try:\n        \n            not_noun.remove(a)\n            \n        except:\n            print(a, 'n is not in the list')\n    #List of words to remove (This corresponds to a retro active setting of words creating clusters when they are not significant enough (in the interpretation of the generated cluster)\n    if keep_max==False:\n        l_w_not_int=[]#'heart', 'retrospot', 'metal', 'design', 'holder', 'polkadot', 'regency','vintage',\n        for r in l_w_not_int:\n            not_noun.append(r)\n    #commit:\n    try:\n        cust_them = cust_them.drop(columns=not_noun)\n    except:\n        print('word not remove')\n    \n    cust_them.fillna(0, inplace=True)\n    \n    #Nous retirons les mots les moins présent:\n    w_importance = cust_them.sum().sort_values(ascending=False)\n    w_importance = w_importance[w_importance>w_all_min]\n    cust_them=cust_them.loc[:, w_importance.index]\n    \n    return cust_them\n\n\ndef for_cust_them(d, w_all_min=16, keep_max=False):\n    them = df_cust_them(d)\n    them_clean = cust_them_clean(them, w_all_min=w_all_min)\n    return them_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncust_them = for_cust_them(d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 First approach with Kmeans:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import cluster, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_clu = [i for i in range(3, 13)]\nsilh=[]\nfor n in nb_clu:\n    km_init = cluster.KMeans(n_clusters=n, random_state=0)\n    km_init.fit(cust_them)\n    s=metrics.silhouette_score(cust_them, km_init.labels_)\n    silh.append(s)\n    print('OK',n)\n    \nplt.plot(range(3, 13), silh)\nprint(silh.index(max(silh))+3, max(silh))\nprint('7 clu :',silh[7-3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let choose 8 clusters :"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nn=8\nkm_init = cluster.KMeans(n_clusters=n, random_state=0)#4ou7 rds 6 8clu ou 10 clusters\nkm_init.fit(cust_them)\n\nclusKM = pd.DataFrame(km_init.labels_, cust_them.index, columns=['km_them_t'])\n#display(clusKM)\nthem_clu=pd.concat([clusKM, cust_them], axis=1)\n#clusKM['CustomerID']=cust_them.index\n#d=d.merge(clusKM, on='CustomerID')\n\nthem_clu.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"dico_cl = dict()\nfor k in range(n):\n    print('cluster ',k)\n    temp=them_clu[them_clu['km_them_t']==k].describe().T\n    temp=temp.iloc[1:, :]\n    temp=temp[temp['50%']>0]\n    display(temp)\n    dico_cl[k]=temp\n    l1 = plt.plot(temp['mean'])\n    l2 = plt.plot(temp['50%'])\n    l3 = plt.plot(temp['25%'])\n    plt.legend(['mean','median', '25%'])\n    plt.xticks(rotation=40)\n    #plt.legend((l1, l2, l3), ('mean', 'median', '25%'))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 8 clusters. <br> * note: Taking into account the ** proportion of words on the budget invested per customer ** (rather than the quantity only), allowed to reveal a new category ('bottle' 'water ',' tea '..) which seems to correspond to rather expensive products (Thermos, tea service ...) *\n<br>\n** Rq **: The graphs above are easily interpretable: <br>\nThey correspond to the ** ownership of a word on the customer's market share **. <br>\n** For example **: In the ** cluster 3 **, 75% of customers have the words 'bottle' present on more than 25% of their market share. (similarly for the word 'water' etc ...) <br>\nOn the other hand, on average, these words (bottle and water) are each present in 41% of the market share of cluster customers.\nA little check seems to indicate that the thematic interest of these customers would be ** hot drink utensils ** ('thermos', 'hot chocolates', 'tea service' ...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Let name these Clusters\"\"\"\n#dico_them={6:'MetalSign', 1: 't-Light', 3:'Thermos', 2:'Bags', 0:'ChismasTime', 5:'Unspecified', 4:'RegencyTea', 7:'Doormat'}\ndico_them={0:'RegencyTea', 1:'MetalSign', 2:'Unspecified', 3:'thermos', 4:'t-Light', 5:'Bags', 6:'HeartDeco', 7:'ChrismasVintage'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This clutsering is a first step to dinsting thematics. But it's not enough. In add, one cluster (no.2) covers most of the data.<br>\nLet try to understand the Kmeans impact with visualisation with dimensions reduced:\n## 3.3 Cluster representation and switch to decomposition NMF:\nwe'll see that the decomposition NMF give very interesting results... \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import decomposition","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I choose n_components = number of cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"#n = nb of clusters\nnmf=decomposition.NMF(n_components=n, random_state=0)\nw=nmf.fit_transform(cust_them)\nw_clean = pd.DataFrame(w, index=cust_them.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordered_c=pd.DataFrame(km_init.predict(cust_them), index=cust_them.index, columns=['km_them'])\n#display(ordered_c)\nref_nmf = pd.concat([ordered_c ,w_clean], axis=1, sort=True)\nref_nmf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ref_nmf['legend']=ref_nmf['km_them'].apply(lambda x: dico_them[x])\n\nimport seaborn as sns\nfor i in range(7):\n    plt.figure(figsize=(20,20))\n    \n    sns.pairplot(ref_nmf.loc[:,['km_them', 'legend', i, i+1]] , hue='legend', \n                 palette=sns.color_palette())  #\"husl\", 8             \n    plt.title('plan factoriel {} et {}'.format(i+1, i+2))    \n    plt.xlim((-0.001,0.5))\n    plt.ylim((-0.001,0.5))\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Above, Our clusters are fitted on the decomposition NMF. On each factor plane, a cluster is distinguished by high values. Only plan number 4 (the 5th factorial plan) does not correspond to a specific cluster."},{"metadata":{},"cell_type":"markdown","source":"With the gaphs aboves, we can rename our factorials plans :"},{"metadata":{"trusted":true},"cell_type":"code","source":"l=['HeartDeco', 'Bags', 'ChrismasVintage', 'Thermos', 'Unspecified', 't-Light', 'MetalSign', 'RegencyTea']\n#l=['Unspecified','t-Light', 'Bags', 'Thermos','RegencyTea', 'MetalSign','ChismasTime','DoormatChismas']\nfor i in range(8):\n    ref_nmf.rename(columns={i : l[i]},inplace=True)\nref_nmf.head()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finaly, the NMF decomposition offers a better characterization of customer themes.** Now, customers have a score in each theme discovered. \n## 3.4 Final thematic clustering\nTo better characterize their relationship to themes, we will reinject our indices of diversity and entropy product."},{"metadata":{"trusted":true},"cell_type":"code","source":"d_clu = pd.concat([date.loc[:, ['entropy_corrige','ind_div']], ref_nmf.loc[:,l]], axis=1, sort=True)#'quantity_t','nb_prod_diff',","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_clu.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let scale the data to perform a clustering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale_v=StandardScaler()\nd_clu.loc[:,:]=scale_v.fit_transform(d_clu.loc[:,:])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d_clu.shape)\nnb_clu = [i for i in range(3, 23)]\nsilh=[]\nfor n in nb_clu:\n    km_gen= cluster.KMeans(n_clusters=n, random_state=1)\n    km_gen.fit(d_clu)\n    s=metrics.silhouette_score(d_clu, km_gen.labels_)\n    silh.append(s)\n    #print('OK pour ',n)\n    \nplt.plot(nb_clu, silh)\nprint(silh.index(max(silh))+3, 'silhouette :', max(silh))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_cl=14\nkm_gen = cluster.KMeans(n_clusters=n_cl, random_state=1)#4ou6\nkm_gen.fit(d_clu)\n\n#clusKM = pd.DataFrame(np.array([km_gen.labels_, d_clu.index]).T, columns=['categ', 'CustomerID'])\n#them_clu=clusKM.merge(d_clu, on='CustomerID')\n\nclusKM = pd.DataFrame(km_gen.labels_, index=d_clu.index, columns=['categ'])\nthem_clu=pd.concat([clusKM, d_clu], axis=1, sort=True)\n#clusKM['CustomerID']=cust_them.index\n#d=d.merge(clusKM, on='CustomerID')\n\nthem_clu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(n_cl):\n    print('cluster No.',k)\n    temp=them_clu[them_clu['categ']==k].describe().T\n    #temp.drop('CustomerID', inplace=True)\n    temp=temp.iloc[1:, :]\n    #temp=temp[temp['50%']>0]\n    display(temp)\n    plt.figure(figsize=(10,4))\n    l1 = plt.plot(temp['mean'])\n    l2 = plt.plot(temp['50%'])\n    l3 = plt.plot(temp['25%'])\n    l4 = plt.plot(temp['75%'])\n    plt.axhline(y=0, c='black', ls='--')\n    plt.xticks(rotation=40)\n    plt.title('No.{} (nb = {})'.format(k, temp.iloc[0,0]))\n    plt.legend(['mean', 'median', '25%', '75%'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We have reduce the size of the main cluster 'Unspecific' (cluster no.5)<br>\nNow, we can identify customers who have specific affinities with themes we didn't discorer (cluster no.7)<br>\nWe can also notice the correlation of themes among the clusters. For example, in cluster no.8, customers love home decorations, but they prefer hearts rather than regency style.<br>\n<br>\nThis completes the thematic clustering. <br>\n<br>\n\n We can use RFM segmentation to get more information about customer behavior (Frequency, Recency, Monetary Value). (The feature engineering is already done)<br>\nAnd finally, we can train a classifier to predict the customer behavior.\n### to be continued..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}