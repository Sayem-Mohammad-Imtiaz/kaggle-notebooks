{"cells":[{"metadata":{},"cell_type":"markdown","source":"### POKEMON MACHINE LEARNING - Predict Pokemon HP stats\n\n#### regression models: 1) Random Forest 2) XGBoost \n\nSteps:\n- understand datasets, choose features and target\n- ensure no data leakage in any features\n- study correlations between X & Y, if the relationship is weak, it is unlikely to contribute to the prediction\n\n- data prep, casting, removing un-helpful columns\n- split train/test datasets\n- decide how to impute/encode numerical OR categorical data\n\n- create pipelines for preprocessor and model\n- random forest model: experiment different parameters, compare MAE, use CV to get overall MAE, list importance\n- XGBoost model: experiment different parameters, compare MAE, use CV to get overall MAE, list importance\n"},{"metadata":{},"cell_type":"markdown","source":"## Understand Dataset\n**.columns** & **.describe()** - Min, Max, Average, etc"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndata = pd.read_csv('../input/pokemon/Pokemon.csv')\ndata.columns\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Study Correlations to Identify Noise\nFeature vs Feature : High = redundant = noise. (Action = Drop)<br>\nFeature vs Target  : Very low = no contribution to prediction = noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choosing Target - HP (Health Power)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.HP\nX_full = data\nX_sample = X_full.drop(['HP'], axis=1)\n# X_sample['Type 2'].fillna(value='None',inplace=True) #enable if no combine\nX_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choosing Features\n#### 1. Base stats (Atk, Sp.Atk, Def, Sp.Def, Speed) \n(Numerical) Use value as is. Impute missing values with MEAN value. \n\n#### 2. Is it Legendary? (Legendary) \n(Bool) Convert boolean to Integer 0/1. Use value as is.\n\n#### 3. Pokemon Types (Type 1 & Type 2)\n(Categorical) One-hot encode. Missing values = 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Legendary\nX_sample.Legendary = X_sample.Legendary.astype(int)\n\n### Pokemon Type\n\n# Get all unique types from column \"type 1\"\ntypes = np.unique(data['Type 1'].values)\n\n# Prepare columns for one-hot (combine)\n# for t in types:\n#     X_sample.insert(12, f'Type_{t}', 0)\n    \n# # One hot type 1 & type 2\n# for i, r in data.iterrows():\n#     type_1 = r['Type 1']\n#     X_sample.at[i, f'Type_{type_1}'] = 1\n    \n#     type_2 = r['Type 2']\n       \n#     if not type_2 == 'nan':\n#         X_sample.at[i, f'Type_{type_2}'] = 1\n        \nX_sample.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning & Dropping Features\n1. Unused values: **Index(#)**, **Name**\n\n2. Very low correlation: **Generation** - Does not correlate to any base stats\n\n3. Data leakage: **Total** - The sum of all base stats including HP. So it will be not available when model use for prediction.\n\n4. Leftovers after one-hot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_sample = X_sample.drop(['Name', '#', 'Generation', 'Total', 'Type 1', 'Type 2', 'Type_nan'], axis=1)\nX_sample = X_sample.drop(['Name', '#', 'Generation', 'Total'], axis=1)\nX_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split Train/Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# One Hot Using pd.get_dummies()\nX_sample = pd.get_dummies(X_sample)\nprint(X_sample.columns)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_sample, y, train_size=0.8, test_size=0.2, random_state=0)\nX_full_train, X_full_valid, y_full_train, y_full_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identifying Features to Impute/Encode\n#### 1. Numerical columns\nimpute missing values, some use MEAN, some use ZERO\n\n#### 2. Categorical columns\none hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import math\n# boolean_cols = [cname for cname in X_train.columns if X_train[cname].dtype == 'bool']\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64'] \n                  and 'Type' not in cname ]\nonehot_cols = [cname for cname in X_train.columns if 'Type' in cname ]\ncategorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == 'object']\ncols_with_missing = [cols for cols in numerical_cols if X_train[cols].isnull().any()]\n\n# print(\"Boolean columns: \", boolean_cols)\nprint(\"Numerical columns: \", numerical_cols)\nprint(\"OneHot columns: \", onehot_cols)\nprint(\"Categorical columns: \", categorical_cols)\nprint(\"Columns with missing values: \", cols_with_missing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Type 2 missing simply means the Pokemon does not have a secondary type"},{"metadata":{},"cell_type":"markdown","source":"## Model 1 - Random Forest\n#### pipeline(preprocessor + model), train, predict, cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessor\noh_transformer = SimpleImputer(strategy='constant', fill_value=0)\nnum_transformer = SimpleImputer(strategy='mean')\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=None)),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\ntransformers=[\n    ('oh', oh_transformer, onehot_cols),\n    ('num', num_transformer, numerical_cols),\n    ('cat', cat_transformer, categorical_cols)\n])\n\n# Model\nrf_model = RandomForestRegressor(n_estimators=250, criterion='mae', random_state=0, n_jobs=-1)\n\n# Pipeline\n# rf_pipeline = Pipeline(steps=[('model', rf_model)])\nrf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', rf_model)])\n\n# Fit model\nrf_pipeline.fit(X_train, y_train)\n\n# Predict\nrf_preds = rf_pipeline.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Need to decide how to impute categorical column \"type 2\". Currently I impute with empty string."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Result\nprint(\"Model#1: RandomForest - Performance Summary\\n----------\")\n\nrf_score = mean_absolute_error(y_valid, rf_preds)\nprint('MAE score (80/20 train-test split) :', rf_score)\n\nfrom sklearn.model_selection import cross_val_score\nrf_scores = -1 * cross_val_score(rf_pipeline, X_sample, y, cv=5, scoring='neg_mean_absolute_error')\n# print(\"MAE scores (5CVs): \", rf_scores)\nprint(\"MAE score (Cross Validation)       :\", rf_scores.mean())\nprint(\"Overall MAE =\", round(rf_scores.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OneHotEncoder: 14.02 || pd.get_dummies: 14.04\n\n### Feature Importance (Random Forest)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_importances = pd.DataFrame({'feature': X_valid.columns, 'rf_importance': np.round(rf_model.feature_importances_,3)})\nrf_importances = rf_importances.sort_values('rf_importance', ascending=False).set_index('feature')\nrf_importances[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 - XG Boost\n#### pipeline(model), train, predict, cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n# from sklearn.impute import SimpleImputer\n# from sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\n\n# Temporarily remove preprocessor because it throws error otherwise\n\n# Model\nxgb_model = XGBRegressor(\n            n_estimators=100,\n            learning_rate=0.05, n_jobs=2, verbosity=0, random_state=0)\n\nxgb_pipeline = Pipeline(steps=[('model', xgb_model)])\n# xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', xgb_model)])\n\n# Fit model\nxgb_pipeline.fit(X_train, y_train, \n                 model__early_stopping_rounds=3,\n                 model__eval_set=[(X_valid, y_valid)],\n                 model__verbose=0)\n\n# Predict\nxgb_preds = xgb_pipeline.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Result\nprint(\"Model#2: XGBoost - Performance Summary\\n----------\")\nxgb_score = mean_absolute_error(y_valid, xgb_preds)\nprint('MAE score (80/20 train-test split) :', xgb_score)\n\nfrom sklearn.model_selection import cross_val_score\nxgb_scores = -1 * cross_val_score(xgb_pipeline, X_sample, y, cv=5, scoring='neg_mean_absolute_error', verbose=0, error_score=0)\n# print(\"MAE scores (5CVs): \", rf_scores)\nprint(\"MAE score (Cross Validation)       :\", xgb_scores.mean())\nprint(\"Overall MAE =\", round(xgb_scores.mean(), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OneHotEncoder: error || pd.get_dummies: 13.83\n\n### Feature Importance (XG Boost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_importances = pd.DataFrame({'feature': X_valid.columns, 'xgb_importance': np.round(xgb_model.feature_importances_,3)})\nrf_importances = rf_importances.sort_values('xgb_importance', ascending=False).set_index('feature')\nrf_importances[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display first 5 predictions & actual result\nprint(\"First 5 Predictions (Random Forest): \", rf_preds.tolist()[:5])\nprint(\"First 5 Predictions (XGBoost)      : \", xgb_preds.tolist()[:5])\nprint(\"First 5 Actual result              : \", y_valid.tolist()[:5])\nX_full_valid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n* XG Boost slightly perform better than Random Forest\n* Both models are not accurate \n    * Insufficient model tuning? Wrong params?\n    * Not enough features? -> explore other datasets"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}