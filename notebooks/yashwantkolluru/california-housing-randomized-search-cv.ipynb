{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nimport pandas as pd\nimport random\nimport math\nimport sklearn.linear_model\n\nimport numpy as np\nnp.random.seed(1)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras import regularizers\n\nimport os\n#import tarfile\nimport urllib \n\n# checks Python version (version greater than 3.5 required)\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Checks Scikit version - (Scikit-Learn gretaer than 0.20 required)\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Directory to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"Housing_project_Example\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\n#Save figures\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HOUSING_PATH = os.path.join(\"../input/\", \"california-housing-prices/\")\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = load_housing_data()\n# info functions helps us to understand the data type of all the columns\nhousing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Bedrooms have few NAN values (replaced by median values, later in the code)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ocean proximity info\nprint(housing[\"ocean_proximity\"].value_counts())\n\n# describe function gives a summary like mean, median, std, count, etc for the numeric columns\nhousing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check if there are missing values in the data\nhousing.isnull().sum()\n\n## We can observe ``total_bedrooms`` column has 207 NAN values. It necessray to fix this before training the model \n#### There are 2 ways to treat NAN\n\n#### 1. Delete records which are missing \n#### 2. Fill the missing values using the mean or median - which in this case is a pretty much easier.\n\n#### To decide from all the options, we need to check the outliers.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Box plot\nimport seaborn as sns\nplt.figure(figsize=(15,5))\nsns.boxplot(y=\"total_bedrooms\",data=housing, orient=\"h\", palette=\"plasma\")\nplt.plot\n\n# There are a lot of outliers. They are filles using median, \n# (Mean could vary a lot because of outliers and can later impact the accuracy of our model)\n#Fixed in later step -","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analyzing the dependent variable - median house price .\n# Firstly Histogram\n\nplt.figure(figsize=(20,5))\nsns.set_color_codes(palette=\"bright\")\nsns.distplot(housing['median_house_value'],color='g')\n\n# We can see there is sudden increase in the median house value greater than 5,00,000, \n# this also could be outliers or also may be at some areas the prices could go really high. Need to fix this","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The bins parameter is used to customize the number of bins shown on the plots.s\nhousing.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Visulaization of data\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"cool\"), colorbar=True,\n    sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")\n\n# There are some high density areas in california, so we can say the price of house is a bit realted to\n# location as well. \n\n# Earlier with the data, I thought longitude & latitude would not be weak predictors\n# but after plotting the graph, we can conclude even they are useful features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Correlations\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Median income highly correlated with hose value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nsave_fig(\"scatter_matrix_plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we split the data, It can be observed that the feature - total_rooms has no significance, as this talks \n# about the rooms in the entire district. \n# Instead, we should find out, how many rooms are there in individual household, that would be more informative\n# for our analysis. and also numbers like population per household","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Attribute combintaions\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove this feature\nhousing.drop(\"total_rooms\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rooms per household also is correlated to median price value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the Data in to test and train\n#Dividing in to 5 bins (for stratification sampling\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\nprint(housing[\"income_cat\"].value_counts())\nplt.figure()\nhousing[\"income_cat\"].hist()\n# so number of incomes between 0 and 1.5 is 814, 1.5 and 3 is 6552 etc.\n# Now the challenge is to divide the data in same propotion in both train and test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the test data from overal set using stratification sampling\n#To split train and test data with consistency among the sets. \n#For instances, the income distribution must be same in both the sets. Done using sklearn.model_selection\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Both sets have similar (not exact) but similar counts across them.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing \"_cat\" to bring data to original state\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the data for Machine Learning algorithms\nhousing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n#print(housing.info())\n#print(housing_labels.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning (incomplete rows) at command - housing.isnull().sum() (Earlier)\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Intially dropping categorical columns (they are brought back later). Beasue median on cateorgical columns might impact them. \n#Imputer function to replace the NAN value present\n# columns with Numerical values\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n         ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\nprint(housing_num_tr.shape)\n#print(housing.shape)\n# but this (housing_num_tr) does not include hotencoder ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bringing back categorical columns with hotencoder function for data transformation.\n# Converting Categorical attribute to numeric\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_attribs = list(housing_num)\n#print(num_attribs)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)\nprint(housing_prepared)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trian the model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#RandomizedSearchCV method\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean scores for few of the combinations\ncvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = rnd_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores of each of the attributes\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation on test set\nfinal_model = rnd_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nfinal_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Actual vs. Predicted\n\ntest = pd.DataFrame({'Predicted':final_predictions,'Actual':y_test})\nfig= plt.figure(figsize=(16,8))\ntest = test.reset_index()\ntest = test.drop(['index'],axis=1)\nplt.plot(test[1::50])\nplt.legend(['Actual','Predicted'])\nsns.jointplot(x='Actual',y='Predicted',data=test,kind='reg',color=\"grey\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Thank you all for reading\n \n#### If you have any doubts please right back to me","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}