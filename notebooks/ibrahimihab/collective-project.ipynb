{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport os\nimport glob\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Dropout, Flatten\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport keras\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.layers import Conv2D, MaxPooling2D, SimpleRNN\nimport tensorflow as tf\nfrom keras.layers.normalization import BatchNormalization\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 2D_CONVOLUTIONAL NEURAL NETWORKS  -- # MAX ACC = 67%\n\n# Getting the path of the data\ndata_dir = '../input/gtzan-dataset-music-genre-classification/Data/images_original'\nimg_height = 255\nimg_width = 255\nbatch_size = 10\n\n# Creating the train and test dataset\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_height),\n  batch_size=batch_size)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_height),\n  batch_size=batch_size)\n\n# Creating the model\ndef build_CNN_model():\n    model = Sequential()\n    model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)))\n    model.add(Conv2D(32, kernel_size = (3,3),input_shape = (288, 432,3), padding = 'same' ,strides = 2, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size=(4,4)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Conv2D(64, kernel_size = (3,3),padding = 'same', strides = 2, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size=(4,4)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Conv2D(128, kernel_size = (3,3),padding = 'same' ,strides = 1, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size=(4,4)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Flatten())\n    model.add(Dense(100, activation = 'relu'))\n    model.add(Dense(10, activation = 'softmax'))\n    \n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n    \n    return model\n\n# Invoking the model and training it\ncnn_model = build_CNN_model()\ncnn_model.fit(train_ds, epochs = 150, validation_data = val_ds)\n\n# Evaluating the model\ncnn_model.evaluate(val_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA PROCESSING FOR THE MODELS\npath = '../input/gtzan-dataset-music-genre-classification/Data/features_30_sec.csv'\nfile = pd.read_csv(path)\nfile = file.drop(['filename', 'length'], axis = 1)\ny = file.label\nX = file.drop(columns = ['label'])\n\n# Processing the data(Standard scaling + Encoding)\nscaler = StandardScaler()\nX = scaler.fit_transform(np.array(X, dtype = float))\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\nlabel_encoder = LabelEncoder()\n\n# Label Encoding\ntransformed_train_y = label_encoder.fit_transform(train_y)\ntransformed_test_y = label_encoder.transform(test_y)\n\n# One_hot Encoding\ntr_y = pd.get_dummies(train_y)\nte_y = pd.get_dummies(test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tensor_train_X = tf.reshape(tf.convert_to_tensor(train_X), (800,58,1))\ntensor_test_X = tf.reshape(tf.convert_to_tensor(test_X), (200, 58, 1))\n\n# 2 1D_Covolutional NN --- MAX ACC = 66%\n\nmodel = Sequential()\n\nmodel.add(Conv1D(128, 8,strides = 2,kernel_regularizer=l2(0.02),\\\n                 input_shape=train_X.shape[1:], activation='sigmoid'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv1D(64, 6,strides = 2,kernel_regularizer=l2(0.02), activation='sigmoid'))\n# model.add(MaxPooling1D(pool_size=4)) \nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv1D(32, 11,strides = 2, kernel_regularizer=l2(0.02), activation='sigmoid'))\n# model.add(MaxPooling1D(pool_size=4)) \nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\n# model.add(Conv1D(16, 5, activation=LeakyReLU(0.25)))\n# model.add(MaxPooling1D(pool_size=4)) \n# model.add(BatchNormalization())\n# model.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='sigmoid'))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(64, activation='sigmoid')) \nmodel.add(BatchNormalization())\n\n# model.add(Dense(32, activation='sigmoid')) \n# model.add(Dense(16, activation='sigmoid')) \nmodel.add(Dense(10,activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'])\nHistory = model.fit(train_X, tr_y, batch_size = 20, epochs=150, verbose = 1, validation_data=(test_X, te_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the stoping by accuracy callback\nfrom keras.callbacks import Callback\nclass EarlyStoppingByAccuracy(Callback):\n    def __init__(self, monitor='accuracy', value=0.98, verbose=0):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n        if current >= self.value:\n            if self.verbose > 0:\n                print(\"Epoch %05d: early stopping THR\" % epoch)\n            self.model.stop_training = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 Multi_layered Perceptron model ---- MAX ACCURACY = 83.5%\ndef build_MLP_model():\n    model = Sequential([\n        Dense(265, activation = 'relu'),\n        BatchNormalization(),\n        \n        Dense(128, activation = 'relu'),\n        BatchNormalization(),\n        \n        Dense(128, activation = 'relu'),\n        BatchNormalization(),\n        \n        Dense(64, activation = 'relu'),\n        BatchNormalization(),\n        \n        Dense(10, activation = 'softmax')])\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nMLP_model = build_MLP_model()\n\nMLP_model.fit(train_X, transformed_train_y, batch_size = 10, epochs = 150, validation_data = (test_X, transformed_test_y),\\\n              callbacks = [EarlyStoppingByAccuracy(monitor = 'val_accuracy', value = 0.82)])\n\n    \n    \n\n# MLP_model_loaded =  keras.models.load_model('../input/my-model/third_83.5_acc model.h5')\n# MLP_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4 Creating Some classifier models (LR, RandomForrest ... etc.)\nrf = RandomForestClassifier(random_state = 0)\ndt = DecisionTreeClassifier(max_depth = 18, random_state = 0)\nlr = LogisticRegression(max_iter=10000)\ns = LinearSVC(random_state = 0, max_iter=i)\nKN = KNeighborsClassifier(n_neighbors = i)\ngb = GaussianNB()\nclassi = VotingClassifier(estimators=[('lr', lr),('x', x), ('dt', dt), ('rf', rf)])\nclassi.fit(train_X, transformed_train_y)\npreds_lr = classi.predict(test_X)\nnp.round(preds_lr)\ntest_acc = round(classi.score(test_X, transformed_test_y) * 100, 2)\nprint(\"Train Accuracy: \", round(classi.score(train_X, transformed_train_y) * 100, 2))\nprint(\"Test Accuracy: \", test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5 Creating an LSTM classifier model --- MAX ACCURACY = 71%\ntrain_X = tf.reshape(tf.convert_to_tensor(train_X), (800,58,1))\ntest_X = tf.reshape(tf.convert_to_tensor(test_X), (200, 58, 1))\ndef build_lstm_model():\n    model = Sequential()\n    model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True))\n    model.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\n    model.add(Dense(units=10, activation=\"softmax\"))\n    model.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n    return model\n\nlstm_model = build_lstm_model()\nlstm_model.fit(train_X, tr_y, epochs = 150, batch_size = 32,validation_data = (test_X, te_y))     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6 Creating an RNN  classifier model --- MAX ACCURACY = 63%\ntrain_X = tf.reshape(tf.convert_to_tensor(train_X), (800,58,1))\ntest_X = tf.reshape(tf.convert_to_tensor(test_X), (200, 58, 1))\nmodel = keras.Sequential()\nmodel.add(SimpleRNN(units=256, input_shape=(train_X.shape[1],train_X.shape[2]), activation=\"tanh\", dropout= 0.2, return_sequences=True))\nmodel.add(SimpleRNN(units=128, input_shape=(train_X.shape[1],train_X.shape[2]), activation=\"tanh\", dropout= 0.2))\nmodel.add(Dense(64, activation=\"relu\")) \nmodel.add(Dense(10, activation=\"softmax\"))\nmodel.compile(loss='mean_squared_error', optimizer='rmsprop' , metrics = ['accuracy'])\nmodel.summary()\nmodel.fit(train_X, tr_y, batch_size = 100, epochs = 150, validation_data = (test_X, te_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport glob\nimport librosa\ndef create_spectogram(path):\n    y, sr = librosa.load(path)\n    spect = librosa.feature.melspectrogram(y=y, sr=sr,n_fft=2048, hop_length=1024)\n    spect = librosa.power_to_db(spect, ref=np.max)\n    return spect.T\ndata = np.empty((1000, 640, 128))\nlabels = np.empty((1000, 1))\npath = '../input/gtzan-dataset-music-genre-classification/Data/genres_original'\n\ngenre_to_nums = {j: i for i, j in enumerate(os.listdir(path))}\nnums_to_genre = {i: j for i, j in enumerate(os.listdir(path))}\n\nfor folder in os.listdir(path):\n    new = path + \"/\" + folder + \"/*.wav\"\n    for row, wav_file in enumerate(glob.glob(new)):\n        data[row, :, :] = create_spectogram(wav_file).reshape((640, 128))\n        labels[row, :] = genre_to_nums[folder]\n        \n# def plot_spect(track_id):\n#     spect = create_spectogram(track_id)\n#     print(spect.shape)\n#     plt.figure(figsize=(10, 4))\n#     librosa.display.specshow(spect.T, y_axis='mel', fmax=8000, x_axis='time')\n#     plt.colorbar(format='%+2.0f dB')\n#     plt.show()\n    \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}