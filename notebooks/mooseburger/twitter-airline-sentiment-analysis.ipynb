{"cells":[{"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns","outputs":[],"metadata":{"_uuid":"763ec28397f5ad9b48d578bf2105ad77bc90cb2b","_cell_guid":"dc084519-7d55-4596-abe8-3a8c06bacf83","collapsed":true},"cell_type":"code","execution_count":14},{"source":"#Load and inspect data\ndf = pd.read_csv('../input/Tweets.csv')\ndf.info()","outputs":[],"metadata":{"_uuid":"445a41df07f6b53dca945a9e268cf100e711c546","_cell_guid":"cbf1f4a6-1200-4390-8b6a-6d869d20052e"},"cell_type":"code","execution_count":15},{"metadata":{"_uuid":"a12ef94878b06fd4cd3538ae01e35527d5ebfe9b","_cell_guid":"edf987d6-6ac1-4fc8-b81d-2171e6153e32"},"cell_type":"markdown","source":"We can see there's 14650 rows and 15 columns of data. There's a lot of missing data within some of these columns, but before we decide how to handle them. Let's first reduce the columns to the ones we might thing we are interested in. Let's print out a few lines of the dataframe and see what exactly is in each column."},{"source":"df.head()","outputs":[],"metadata":{"_uuid":"6e038cbdc4b392af01ec1ab65d9c2b8b8c6b52dc","_cell_guid":"79aa0a3b-7295-4434-9f9c-73a1218406fd"},"cell_type":"code","execution_count":16},{"metadata":{"_uuid":"f13826fe7d07546b2e6b476a5676b8a635bae7a6","_cell_guid":"2ecff2c3-8489-434c-aa47-8b31367c2cce"},"cell_type":"markdown","source":"   "},{"metadata":{"_uuid":"d010ab31660e1372eea8c2cf1445b12779f674bf","_cell_guid":"c0d14085-ee5c-4b5c-9503-fcff48041c63"},"cell_type":"markdown","source":"There's a lot of information here, but some of it we don't need. Getting rid of some of these columns will also help out with our previous problem of what to do with the missing data. it appears that we might only be interested in:\n\n#### ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'name', 'text', 'tweet_coord', 'tweet_created', 'airline']\n\nThe naming of these columns are the greatest either. So let's reduce the dataframe to the columns of interest, and rename them at the same time and set the dates as the index."},{"metadata":{"_uuid":"a0cc8b24c8547195a9fe01351d355ae7a2a32283","_cell_guid":"5d701446-f92e-4f56-9ca3-c573655de440"},"cell_type":"markdown","source":"      "},{"source":"def clean_df(df):\n    df = df.loc[: , ['airline_sentiment', \n                         'airline_sentiment_confidence',\n                         'negativereason',\n                         'negativereason_confidence',              \n                         'name',\n                         'text',\n                         'tweet_coord',\n                         'tweet_created',\n                         'airline']].rename(columns = {'airline_sentiment':'Rating',\n                                                             'airline_sentiment_confidence':'Rating_Conf',\n                                                             'negativereason':'Negative_Reason',\n                                                             'negativereason_confidence':'Reason_Conf',\n                                                             'name':'User',\n                                                             'text':'Text',\n                                                             'tweet_coord':'Coordinates',\n                                                             'tweet_created':'Date'}).set_index('Date')\n    return df\nclean_df(df).head(10)","outputs":[],"metadata":{"_uuid":"77110e88a2dfb8dab489c7d21e4b6dce0e5bedd5","_cell_guid":"acc22f5a-b84f-4e85-a9d9-d7af19111208"},"cell_type":"code","execution_count":17},{"metadata":{"_uuid":"5f054ea23b450c7176e9834362a26dd39dd940ca","_cell_guid":"becf8bb5-af9d-45c3-abcc-a83a38fde586"},"cell_type":"markdown","source":"Now that we have reduced our dataframe to it's desired form, we can now do some more investigating of our data. Whoever has compiled the data for us was nice enough to already extract the airlines from the tweet text and chronicle them for us in the \"airline\" column\". Let's inspect the airlines and get a general sense of how keen the public is on them."},{"source":"#Groupby airline, and reference the ratings column and then extract total count\nprint(clean_df(df).groupby('airline')['Rating'].count())\n","outputs":[],"metadata":{"_uuid":"723aabb824a9c4e837ed63868c1da9bcc4392cb6","_cell_guid":"71de65fa-c0a4-4a98-aa81-9e9fcaaea5ea"},"cell_type":"code","execution_count":18},{"metadata":{"_uuid":"717150eeb5166a72008abb10d16b6619b3c64d86","_cell_guid":"5cd4f3cf-4098-4adc-8ab3-bed68f7f9c5b"},"cell_type":"markdown","source":"By grouping the data by airlines and indexing each of their ratings, we can see that there are 2759 tweets pertaining to american airlines, 2222 to delta, etc etc. But this doesn't give us full insight as to how many of each tweet is postive, neutral, or negative. Lets regroup our dataframe and extract the lower level details of each airlines ratings."},{"source":"#groupby both airlines and rating and extract total count\nprint(clean_df(df).groupby(['airline','Rating']).count().iloc[:,0])","outputs":[],"metadata":{"_uuid":"2ce9839f63c784e7fab22f501b91af43c82871f9","_cell_guid":"5da09282-1923-4ad5-a0db-a7cf5c7fdbb9"},"cell_type":"code","execution_count":19},{"metadata":{"_uuid":"f5b3f14dc35c45732c3b0080922541d83ea851a6","_cell_guid":"d22d4d1e-a5da-441d-b36a-75e1d301f839"},"cell_type":"markdown","source":"After applying the groupby method, we can see that there's quite a few people that aren't too happy with United airlines. But this layout is very messy and a little hard to read. Nothing tells a story quite like pictures do, so let's illustrate what is above"},{"source":"#create a graph by calling our clean_data function and then plots the total number of each tweet rating (positive,negative, or neutral)\nax = clean_df(df).groupby(['airline','Rating']).count().iloc[:,0].unstack(0).plot(kind = 'bar', title = 'Airline Ratings via Twitter')\nax.set_xlabel('Rating')\nax.set_ylabel('Rating Count')\n\n\nplt.show()\n","outputs":[],"metadata":{"_uuid":"1f187e30b64dc3ee02e5d85e77a57a4d1dde4162","_cell_guid":"93848a4c-aaa9-43c0-92f2-2fc30171f142"},"cell_type":"code","execution_count":20},{"metadata":{"_uuid":"f2c7bc9048fd6f68041145053931a3a4de579766","_cell_guid":"20cdc835-6554-4ef0-980a-a561bcba35d1"},"cell_type":"markdown","source":"So now that we painted the picture, it's very clear that United is indeed not a fan favorite. US Airways is a very close second. But this isn't normalizing the data. We can see to the far right over there, that Virgin America isn't really talked about all that much. So we need to correlate the number of negative tweets to the total number of tweets sent about a particular airline. What percentage of all tweets are negative for each airline.\n"},{"source":"#Count of all tweet ratings for each airline (negative, neutral, positive)\nitemized_tweets = clean_df(df).groupby(['airline','Rating']).count().iloc[:,0]\n#Negative tweet total index for each airline:\n#American 0\n#Delta 3\n#southwest 6\n#US Airways 9\n#United 12\n#Virgin 15\n\n#Count of total tweets about an airline\ntotal_tweets = clean_df(df).groupby(['airline'])['Rating'].count()\n#Airline index in total tweets:\n#American 0\n#Delta 1\n#Southwest 2\n#US Airways 3\n#United 4\n#Virgin 5\n\n\n#Create a dictionary of percentage of negative tweets = (negative_tweets / total_tweets)\nmy_dict = {'American':itemized_tweets[0] / total_tweets[0],\n           'Delta':itemized_tweets[3] / total_tweets[1],\n           'Southwest': itemized_tweets[6] / total_tweets[2],\n           'US Airways': itemized_tweets[9] / total_tweets[3],\n           'United': itemized_tweets[12] / total_tweets[4],\n           'Virgin': itemized_tweets[15] / total_tweets[5]}\n\n#make a dataframe from the dictionary\nperc_negative = pd.DataFrame.from_dict(my_dict, orient = 'index')\n#have to manually set column name when using .from_dict() method\nperc_negative.columns = ['Percent Negative']\nprint(perc_negative)\nax = perc_negative.plot(kind = 'bar', rot=0, colormap = 'Blues_r', figsize = (15,6))\nax.set_xlabel('Airlines')\nax.set_ylabel('Percent Negative')\nplt.show()\n","outputs":[],"metadata":{"_uuid":"c6117c86c70c15d11f182826d8b828ca2c61ef02","_cell_guid":"c7dd3b4e-1991-4d6f-8d49-9cd2bf653e03"},"cell_type":"code","execution_count":21},{"metadata":{"_uuid":"261366763136fa04b44ce1cc008e9d0ad22097aa","_cell_guid":"23530ebf-f58c-4651-be54-5c4d53ea0a3a"},"cell_type":"markdown","source":"This levels the playing field a little bit, and here we can see that even though United had the most negative tweets by far, they still only made up only 68% of all their tweets. Whereas US Airways and American had higher percentages at 77 and 71 percent respectively. This means that they might not be talking about American or US Airways as much as United, but when they are, it's mostly negative. Lets see what percentage makes up the positive tweets for each airline"},{"source":"itemized_tweets = clean_df(df).groupby(['airline','Rating']).count().iloc[:,0]\n#Positve tweet total index for each airline:\n#American 2\n#Delta 5\n#southwest 8\n#US Airways 11\n#United 14\n#Virgin 17\n\ntotal_tweets = clean_df(df).groupby(['airline'])['Rating'].count()\n#Airline index in total tweets:\n#American 0\n#Delta 1\n#Southwest 2\n#US Airways 3\n#United 4\n#Virgin 5\n\n\n#Create a dictionary of percentage of positive tweets = (positive_tweets / total_tweets)\nmy_dict = {'American':itemized_tweets[2] / total_tweets[0],\n           'Delta':itemized_tweets[5] / total_tweets[1],\n           'Southwest': itemized_tweets[8] / total_tweets[2],\n           'US Airways': itemized_tweets[11] / total_tweets[3],\n           'United': itemized_tweets[14] / total_tweets[4],\n           'Virgin': itemized_tweets[17] / total_tweets[5]}\n\n#make a dataframe from the dictionary\nperc_positive = pd.DataFrame.from_dict(my_dict, orient = 'index')\n#have to manually set column name when using .from_dict() method\nperc_positive.columns = ['Percent Positive']\nprint(perc_positive)\nax = perc_positive.plot(kind = 'bar', rot=0, colormap = 'Blues_r', figsize = (15,6))\nax.set_xlabel('Airlines')\nax.set_ylabel('Percent Positve')\nplt.show()\n","outputs":[],"metadata":{"_uuid":"5c9c404bdd11797f10a0cd70b443d8548bbfce9e","_cell_guid":"e0db4020-7279-4659-8bae-81bd1dcf7018"},"cell_type":"code","execution_count":22},{"metadata":{"_uuid":"35d727f0166d2b0469a9758b806c1fd4a90ff679","_cell_guid":"0363f853-47fa-48ed-b35e-4cf1f1b913bb"},"cell_type":"markdown","source":"Oh US Airways, it started off looking like United was going to win crown of \"Worst Airlines\". But with only 9% positive tweets, it's more and more starting to look like I shouldn't fly with you. Obviously the last metric would be graphing out the neutral tweets, but that doesn't tell us a whole lot. Let's put all 3 categories together and paint one single picture instead of 3 seperate pages."},{"source":"#create a function that will concatenate our perc_negative, perc_neutral and perc_positive dataframes into one single dataframe\ndef merge_dfs(x,y,z):\n    #generate a list of the dataframes\n    list_of_dfs = [x,y,z]\n    #concatenate the dataframes, axis = 1 because they all have the same index, we just want to add the columns together\n    concatenated_dataframe = pd.concat(list_of_dfs, axis = 1)\n    return concatenated_dataframe","outputs":[],"metadata":{"_uuid":"c20331f721a322cc188c0f49bb23c987a2a749ba","_cell_guid":"32e8c528-1991-4fc2-b5f6-efa7158369db","collapsed":true},"cell_type":"code","execution_count":23},{"metadata":{"_uuid":"0a5b1a226b5ce2e67ec4681c32424a02059eb97c","_cell_guid":"7c764731-0485-4ba3-8e05-a393f8344c79"},"cell_type":"markdown","source":"We created a function that will concatnenate our previous dictionaries of percentages. Now we must create another dictionary of the percentages of neutral tweets, and then combine them all together."},{"source":"itemized_tweets = clean_df(df).groupby(['airline','Rating']).count().iloc[:,0]\n#Netural tweet total index for each airline:\n#American 1\n#Delta 4\n#southwest 7\n#US Airways 10\n#United 13\n#Virgin 16\n\ntotal_tweets = clean_df(df).groupby(['airline'])['Rating'].count()\n#Airline index in total tweets:\n#American 0\n#Delta 1\n#Southwest 2\n#US Airways 3\n#United 4\n#Virgin 5\n\n\n#Create a dictionary of percentage of positive tweets = (positive_tweets / total_tweets)\nmy_dict = {'American':itemized_tweets[1] / total_tweets[0],\n           'Delta':itemized_tweets[4] / total_tweets[1],\n           'Southwest': itemized_tweets[7] / total_tweets[2],\n           'US Airways': itemized_tweets[10] / total_tweets[3],\n           'United': itemized_tweets[13] / total_tweets[4],\n           'Virgin': itemized_tweets[16] / total_tweets[5]}\n\n#make a dataframe from the dictionary\nperc_neutral = pd.DataFrame.from_dict(my_dict, orient = 'index')\n#Have to manually set column name\nperc_neutral.columns = ['Percent Neutral']\n\n#call our function to concatenate all 3 dataframes of percentages\npercentage = merge_dfs(perc_neutral, perc_negative, perc_positive)\nprint(percentage)\n\n#graph all of our data\nax = percentage.plot(kind = 'bar', stacked = True, rot = 0, figsize = (15,6))\n#set x label\nax.set_xlabel('Airlines')\n#set y label\nax.set_ylabel('Percentages')\n#move the legend to the bottom of the graph since it wants to sit over all of our data and block it - stupid legend\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n          fancybox=True, shadow=True, ncol=5)\n\nplt.show()","outputs":[],"metadata":{"_uuid":"da063aa7c48ec8fb1b3b6914b5bbf6a148ce2ce3","_cell_guid":"b7b8465c-c088-4f57-925c-972ad78d8a04"},"cell_type":"code","execution_count":24},{"metadata":{"_uuid":"91c7cc87577234efa720e2f9e576db3d31805941","_cell_guid":"197009a7-a5ba-46f3-b3cd-5c58cde06857"},"cell_type":"markdown","source":"So we just did a good bit of analysis on our data, but we got so focused on what we've been working on, that we got sidetracked from inspecting all of our data. From initial analysis, it's looking like United made a lot of people unhappy. However we took advantage of one particular caveat. We assumed that whomever compiled all the data for us, correctly identified the airline being referenced in the tweet text. Lets look at some of the tweet text and compare it to the airline being referenced"},{"source":"observation = list(clean_df(df).reset_index().iloc[6750:6755,8])\ntweet_text = list(clean_df(df).reset_index().iloc[6750:6755,6])\n\nfor pos, item in enumerate(observation):\n    print('Airline as compiled: ' + str(item))\n    print('The actual tweet text: ')\n    print(tweet_text[pos], '\\n''\\n')","outputs":[],"metadata":{"_uuid":"3709fc625928c32da996ec5c581e6758465a1838","_cell_guid":"6bc59b68-bdfb-4533-acfe-332efee15b30"},"cell_type":"code","execution_count":25},{"metadata":{"_uuid":"1f37f6f9844069b0a270d5dc946f9e2a3e718baf","_cell_guid":"c8760776-0fea-42f3-821d-8b5a1c957b3a"},"cell_type":"markdown","source":"All that work, and the data in the table was incorrect. As we can see in those particular locations, Delta was tagged as the Airline being referenced in the tweet. But when we look at the actual text we can see it was really Jet Blue being referenced. We now must get rid of the 'Airline' column in the dataframe, parse all the tweet text and pullout the proper airline being referenced in all Tweets. Sure, whoever compiled this probably didn't get **All** of them wrong, but do you really want to scroll through 14k+ lines of data to check. Let's just make some code that parses through for us."},{"source":"#let's start by getting rid of the current 'Airline' column\n#'Airline' was the last column (8) so we just sliced the dataframe, stopping at column (7) 'Coordinates'\nnew_df = clean_df(df).iloc[:,0:7]\nnew_df.head()","outputs":[],"metadata":{"_uuid":"ac9d484d4eda408c14e0bc8b182427ced90e8f03","_cell_guid":"85aa0115-e6fc-4e35-b977-7d6ae216eb4f"},"cell_type":"code","execution_count":26},{"metadata":{"_uuid":"2c2d789e1dc60beebde685acd9d898994d6bd5ab","_cell_guid":"8d852570-ce59-4104-9c5e-bed49f2f7812"},"cell_type":"markdown","source":"We now need to extract the right airline from the tweet text. Examining it, we can see that all airlines references are preceded by an '@' symbol. We all know this is a common twitter tag, so by looking for this, we can extract the airline being talked about. Be wary though, some of the texts can have more than just the airline tagged in it. But this is where we will start, using a regular expression that will find all instances of the '@' symbol. Looking at the majority of the tweet text, the airline is usually the first '@' tag in the text in the event their are multiple. So we will make the general assumption that during our regular expression search, when there are multiple '@' tags, the airline will be the first one. "},{"source":"#first, create a new column called 'Airline'\n#Then reference the 'text' column to apply your regular expression function to\n#apply a lambda function that parses through each tweet text and searches for '@' symbol followed by any letter type\n#extract the first matched instance [0] in the event there are multiple\nnew_df['Airline'] = new_df.Text.apply(lambda x: re.findall('\\@[A-Za-z]+', x)[0])\n\n#check that our regular expression is working\nlist(new_df.Airline.head(10))","outputs":[],"metadata":{"_uuid":"e911911adae6361b5d1c38f8886ee1a7cf473dea","_cell_guid":"8f4c1d54-7ab0-4f58-8d8e-21a8651357a6"},"cell_type":"code","execution_count":27},{"metadata":{"_uuid":"461edb26c05a9e5c38ac110261642eef42043dc9","_cell_guid":"8d42ac92-1efd-438d-90e8-520038d1457a"},"cell_type":"markdown","source":"Ok, so with 14k+ lines of data, there's no telling what all our regular expression function found. So to help figure this out, lets list all the unique items it found. From this, we can see all the airlines it extracted, plus any other tag that was referenced first in a tweet text before the airline tag."},{"source":"#get all unique twitter tags and the count for how many times it appears in the column\ntwitter_tags = np.unique(new_df.Airline, return_counts = True)\n\n#compile twitter_tags so that it lists the unique tag and its total count side by side instead of 2 seperate arrays\ntwitter_tags_count = list(zip(twitter_tags[0],twitter_tags[1]))\ntwitter_tags_count","outputs":[],"metadata":{"_uuid":"8caa69cdcf334141f891ecefdf85d79064778d01","_cell_guid":"168dd888-1142-429e-a04f-7f49fb0699f8"},"cell_type":"code","execution_count":28},{"metadata":{"_uuid":"60b76530b0a1754642e60fa2beae639206a56836","_cell_guid":"e50b14c9-e1fe-45bf-959a-bfb60cfd5c62"},"cell_type":"markdown","source":"So our assumption that the airline is referenced before any other tag was a pretty good guess. We did return some other users twitter names, but for the most part, we got all airline references. We can also see that Jet blue was reference **Alot**. In our original dataframe, there was no mention of Jet Blue. Next, we need to go through the unique list and compile all the airlines referenced into a list. Notice that the airlines are referenced multiple times, this is because they are camel case. When searching for \"unique\" items, @virginAmerica is different than @virginamerica due to the one capital letter. So we need to refine our regular expression search, looking not only for the airlines, but ignoring the camel case so that all spellings of an airline are equal"},{"source":"#List of all airlines in the data as found from the tweets in search above\nairline_list = ['@virginamerica','@united','@southwestair','@americanair','@jetblue','@usairways']\n    \n#compile a regex search to seperate out only the airline tag and ignoring other users tags in the text\n#using the compile method is an easier way to input our \"match\" pattern into the search engine, especially in this event\n#when we are searching for mulitple airlines.\n#we are ignoring case, or capitaliztion  in order to negate all the uniquess we encountered in the list above\nairlines = re.compile('|'.join(airline_list), re.IGNORECASE)\n    \n#apply the compiled regex search and remove the twitter tag '@'\n#for example, the following code takes @AmericanAir and returns AmericanAir\nnew_df['Airline'] = new_df.Airline.apply(lambda x: np.squeeze(re.findall(airlines, x))).str.split('@').str[1]\nprint(list(new_df.Airline.head(10)))","outputs":[],"metadata":{"_uuid":"34ecb83fdb7ae4b9b62c695f72c3f7bb430dbd5a","_cell_guid":"cdaa907c-8ae9-4996-a477-9dff3f062802"},"cell_type":"code","execution_count":29},{"metadata":{"_uuid":"ebf9bc089190d58dc4782303626ed848b4299c31","_cell_guid":"bfab868d-5fca-45f8-95b8-f58ef8a51d20"},"cell_type":"markdown","source":"We have found only the airlines, and removed the '@' tag from them. But what happened to the rows where the airline didn't come first and we extracted a users name instead? Well by default, they were filled in with NaN since they didn't match anything in the matching patter. The code below uses boolean logic to filter the data frame and find the index locations of those tweets, and displays all '@' tags that was referenced in the tweet text. This step is needed so we could find those locations that did not have an airline in place, and manually set the values for these locations in the dataframe"},{"source":"no_airline = new_df.reset_index()\nno_airline = no_airline[no_airline.Airline.isnull()].Text.apply(lambda x: re.findall('\\@[A-Za-z]+', x))\nno_airline","outputs":[],"metadata":{"_uuid":"9d182ee14b4ccfae8fff71513fbb309619c11c66","_cell_guid":"badfa21d-e032-4c79-adca-4c46a4985dde"},"cell_type":"code","execution_count":30},{"metadata":{"_uuid":"3490c55c057d3580d08f14efddb7352caaa5ecbd","_cell_guid":"453aa091-821c-4f8b-9007-d5a3160eadca"},"cell_type":"markdown","source":"Now we can see where the null values are located because the first '@' tag is a user's name, and what airline was supposed to be referenced. Now we can manually set these, and then our 'Airline' column will be up to date and correct. One little issue here, indices 7330 and  8215 didn't have any '@' tag with an airline next to it, so we are just going to have to read the tweet text manually and extract that for ourselves. But don't worry, I did that for you already =) "},{"source":"#reset the index of our dataframe\nnew_df = new_df.reset_index()\n\n#compile a list of index locations of the tweets that return null and set their airline value to the appropriate\n#airline referenced in the tweet\nunited = [737,868,1088,4013]\nsouthwest = [4604,5614,5615,6136,6362]\njetblue = [6796,6811,6906]\nusairways = [7330, 8215,10243,10517,10799,10864,10874,10876,11430]\namerican = [11159,12222,12417,12585,13491,13979]\ndelta = [12038, 12039]\nnew_df.set_value(united,'Airline','united')\nnew_df.set_value(southwest,'Airline','southwestair')\nnew_df.set_value(jetblue,'Airline','jetblue')\nnew_df.set_value(usairways,'Airline','usairways')\nnew_df.set_value(american,'Airline','americanair')\nnew_df.set_value(delta,'Airline','delta')\n    \n#Since all airlines tweets are camel case in different orders, make all airlines uppercase so they are all equal\nnew_df.Airline = new_df.Airline.apply(lambda x: x.upper())\n    \n#create a dictionary to map the all uppercase airlines to the proper naming convention\nmap_airline = {'AMERICANAIR':'American Airlines',\n                'JETBLUE':'Jet Blue',\n                'SOUTHWESTAIR':'Southwest Airlines',\n                'UNITED': 'United Airlines',\n                'USAIRWAYS': 'US Airways',\n                'VIRGINAMERICA':'Virgin Airlines',\n                'DELTA':'Delta Airlines'}\n    \n#map the uppercase airlines to the proper naming convention\nnew_df.Airline = new_df.Airline.map(map_airline)\n\n#display our new airlines!!!\nnp.unique(new_df.Airline)","outputs":[],"metadata":{"_uuid":"f35b02a95fac1f0d05dbc27c44b53c6a97949867","_cell_guid":"1b0e8bff-9fd2-4ca2-837f-1aa74a94a991"},"cell_type":"code","execution_count":31},{"metadata":{"_uuid":"68273152e031a7c5a03cbc67179ad93842d369ad","_cell_guid":"dad30055-fd15-4864-b758-0322bfe3ee06"},"cell_type":"markdown","source":"Next order of business - Continuing to inspect our data before continuing on with any analysis! We don't need another headache. When trying to display truthful data, we need to decide which data is truthful. When deciding what to rate the tweet text, the compiler also added a column \"Rating Confidence\". This is a percentage of how sure they are that the rating the gave was the proper rating. Let's inspect some of these"},{"source":"rating = list(new_df.Rating)\nconf = list(new_df.Rating_Conf)\ntext = list(new_df.Text)\n\nfor i in range(10):\n    print(rating[i], '\\n', conf[i], '\\n', text[i],'\\n','\\n')\n    \n    ","outputs":[],"metadata":{"_uuid":"e2dd0caa7d15b6519576f6e9d2a8996274772dd5","_cell_guid":"8b1ac127-c688-4fee-9952-0b036fde113a"},"cell_type":"code","execution_count":32},{"metadata":{"_uuid":"6eaa4bbeaf7157e86d5258772f5c9bb7981d2311","_cell_guid":"1dbba78c-1b4e-4419-aaa5-4e11070ea41e"},"cell_type":"markdown","source":"Looking at few of the ratings and the corresponding text, it's pretty reasonable to say the rating and it's corresponding confidence percentage is more or less in the ballpark. Armed with this data, let's now make our data even more accurate. There are ratings given to an airline that the confidence is pretty low. Since we don't want to mislabel the airline, we are probably better suited to only analyze the data in which were are pretty sure that the rating is the correct one. This part is basically at the discretion of the user, but to me, 51% is winning. So let's filter our data down to those ratings that have only greater than 0.51 rating confidence percentages."},{"source":"#we could make this one line, but i'm breaking it up for readability\n#set our boolean variable so that it filters the dataframe for only instances where the rating conf is >0.51\n\n\nconf_df = new_df[new_df.Rating_Conf >= 0.51 ]\nprint(conf_df.info())\nconf_df.head(10)","outputs":[],"metadata":{"_uuid":"2b9725f74cc967e9e0756931e062b5934b6c1f08","_cell_guid":"1d22dce3-a6d3-4b80-82a3-d9af3e680205"},"cell_type":"code","execution_count":33},{"metadata":{"_uuid":"76993a003b7b420ae0b2f1dcb259007343f08e82","_cell_guid":"b724888f-ff27-4ad3-92f4-f38f0a80f6a2"},"cell_type":"markdown","source":"So surprisingly, improving our confidence statistics only reduced our data by some 200+ rows. Whoever compiled the data was pretty sure of themselves. Now, one final thing, the index is set as the date and time, but we don't realy like that format. I don't think there's much value in having it as low as the hours minutes and seconds. So let's convert this to just month day and year. Also, although not shown, the type of the date time in the index is **'string'**. When dealing with pandas, it's better to have the 'type' as pandas **'datetime'**. So let's convert as well"},{"source":"#create a copy of our original dataframe and reset the index\ndate = conf_df.reset_index()\n#convert the Date column to pandas datetime\ndate.Date = pd.to_datetime(date.Date)\n#Reduce the dates in the date column to only the date and no time stamp using the 'dt.date' method\ndate.Date = date.Date.dt.date\ndate.Date.head()\n","outputs":[],"metadata":{"_uuid":"4716580fc93bd7283634e595a67c00459d8b0adc","_cell_guid":"73a63c2a-70e0-43a9-a63c-fe77395c7ef2"},"cell_type":"code","execution_count":34},{"source":"conf_df = date\nconf_df.head()","outputs":[],"metadata":{"_uuid":"957b2d3ee72fe1a5e7b777aa485e52e72b5fbbff","_cell_guid":"b7bd9b8c-3564-4a1c-ada0-5d2b612d8b74"},"cell_type":"code","execution_count":35},{"metadata":{"_uuid":"c8566c34d8126c2fb3a9031ea165d03bd2a32ffc","_cell_guid":"3008f6a4-4c50-4dfb-a824-03aa4a8bb84a"},"cell_type":"markdown","source":"Ok, now we have our data looking much better, let's rerun the analysis we originally did and plot the results..... **AGAIN**"},{"source":"test = conf_df[conf_df.Airline != 'Delta Airlines'].groupby(['Airline','Rating']).count().iloc[:,0]\ntest","outputs":[],"metadata":{"_uuid":"a612267a94e65e0f1e1896b2c41ae22f7669b010","_cell_guid":"8976e4c4-41f0-448e-b841-47a430412ef4"},"cell_type":"code","execution_count":36},{"metadata":{"_uuid":"3ef7441026c56e6cc803ca6071960cc78f3e8b78","_cell_guid":"ade32461-fce2-4635-b421-48f77372feb8"},"cell_type":"markdown","source":"We need to create the seperate dataframes again for the percentage: positive, negative, and neutral. But instead of rewriting all the repeating code 3 times again, let's just write one function that we can pass the rating type we want to compile and let it do all the work for us for each rating."},{"source":"def percentages(df, rating = 'negative'):\n    if rating == 'negative':\n        i = 0\n        column = 'Percent Negative'\n    elif rating == 'neutral':\n        i = 1\n        column = 'Percent Neutral'\n    elif rating == 'positive':\n        i = 2\n        column = 'Percent Positive'\n        \n    #Count of all tweet ratings for each airline (negative, neutral, positive), remove Delta since it only has 2 entries total\n    itemized_tweets = df[df.Airline != 'Delta Airlines'].groupby(['Airline','Rating']).count().iloc[:,0]\n    #Rating tweet total index for each airline:\n    #American i\n    #Jet Blue i + 3\n    #southwest i + 6\n    #US Airways i + 9\n    #United i + 12\n    #Virgin i + 15\n\n    #Count of total tweets about an airline\n    total_tweets = df[df.Airline != 'Delta Airlines'].groupby(['Airline'])['Rating'].count()\n    #Airline index in total tweets:\n    #American 0\n    #Jet Blue 1\n    #Southwest 2\n    #US Airways 3\n    #United 4\n    #Virgin 5\n\n\n    #Create a dictionary of percentage of rating tweets = (rating_tweets / total_tweets)\n    my_dict = {'American':itemized_tweets[i] / total_tweets[0],\n                'Jet Blue':itemized_tweets[i + 3] / total_tweets[1],\n                'Southwest': itemized_tweets[i + 6] / total_tweets[2],\n                'US Airways': itemized_tweets[i + 9] / total_tweets[3],\n                'United': itemized_tweets[i + 12] / total_tweets[4],\n                'Virgin': itemized_tweets[i + 15] / total_tweets[5]}\n\n    #make a dataframe from the dictionary\n    perc_df = pd.DataFrame.from_dict(my_dict, orient = 'index')\n        \n    #have to manually set column name when using .from_dict() method\n    perc_df.columns = [column]\n        \n    return perc_df\n    \n","outputs":[],"metadata":{"_uuid":"f5b9bdfa935b5306867d5344d0490cd8087eaf0a","_cell_guid":"33e6957c-85ef-453f-8d2f-fb0cd1f8d44c","collapsed":true},"cell_type":"code","execution_count":37},{"metadata":{"_uuid":"6ad2676be946c9d73c76bf76733226cdf50aca48","_cell_guid":"4a5b5a82-0185-4899-ae09-c11cfe4f5f9b"},"cell_type":"markdown","source":"Great! we have our function, now let's use it to compile our percentages and plot them."},{"source":"#Create a df called negative that contains the percent negatives by calling the function above\nnegative = percentages(conf_df, 'negative')\n\n#Create a df called neutral that contains the percent neutrals by calling the function above\nneutral = percentages(conf_df, 'neutral')\n\n#Create a df called positive that contains the percent positives by calling the function above\npositive = percentages(conf_df, 'positive')\n\n#call the earlier function that merges all 3 data frames into one\nmerged_perc = merge_dfs(negative, positive, neutral)\nmerged_perc","outputs":[],"metadata":{"_kg_hide-output":true,"_cell_guid":"2601b707-e165-4a16-a6af-9c0e0b410860","_uuid":"6d7725ddef21094dd3cf90c91c0a41a5204869db"},"cell_type":"code","execution_count":38},{"source":"ax = merged_perc.plot(kind = 'bar', rot = 0, figsize = (15,6))\nplt.show()","outputs":[],"metadata":{"_uuid":"fb377e6d67c0147abe0c5f629ddd7b7e384f26a9","_cell_guid":"1df4a82f-ab98-4e7b-b4ad-18a786aad0a8"},"cell_type":"code","execution_count":39},{"metadata":{"_uuid":"20ffa47fefc0f519e09137b4ac6c6481664de3d8","_cell_guid":"f5929ae0-786e-4c94-8863-46002e5f9285"},"cell_type":"markdown","source":"We now have accurate data, and a clear picture of which airline just really upset people. US Airways, no wonder why you were acquired by American Airlines a year later. A lot of people were not happy for you. By why weren't they happy with you? Let's extract all the reasons why people were upset and aggregate them for each airline while plotting the results. "},{"source":"#function that reduces the dataframe to only the airline and the negative reasons, then extract the reasons and the frequency\n#each reason was referenced to an airline\ndef reason(df):\n    df = df.reset_index().loc[:,['Airline','Negative_Reason']].dropna().groupby(['Airline','Negative_Reason']).size()\n    return df\n\n#call the function and plot the results\nax1 = reason(conf_df).unstack(0).plot(kind = 'bar', figsize = (15,6), rot = 70)\n\nplt.show()","outputs":[],"metadata":{"_uuid":"10a575e422bde2e6cd5267e3b976db0f2b969394","_cell_guid":"9589e5cb-895e-49c0-993f-c16283f98d87"},"cell_type":"code","execution_count":40},{"metadata":{"_uuid":"56c338344fc13ffc33529fca78e525ac5521a090","_cell_guid":"e8544524-a4bd-42e2-a0dc-16b04e8f80d0"},"cell_type":"markdown","source":"Let's see how the negative tweets break down for each day of the data. It appears that we have about a weeks worth of tweet data here, so let's sort out only the negatives and see how much an airline got hated on each day"},{"source":"print(conf_df.Date.min())\nprint(conf_df.Date.max())","outputs":[],"metadata":{"_uuid":"c35cea288080751570705ef09a92c58b04b290bb","_cell_guid":"c97da391-8ccf-4232-b2e8-e4bfe4b6427b"},"cell_type":"code","execution_count":41},{"source":"#groupby by Date first making it the main index, then group by the airline, then finally the rating and see how many\n#of each rating an airline got for each date\nday_df = conf_df.groupby(['Date','Airline','Rating']).size()\nday_df","outputs":[],"metadata":{"_uuid":"43b240e15dfbcc0b0f746beddeba795e632fd74b","_cell_guid":"19b1a279-323a-4bad-8e20-9cf57b1d798f"},"cell_type":"code","execution_count":42},{"metadata":{"_uuid":"c78cfd252437d5cb19b68072c7130716e252af77","_cell_guid":"6118e7e6-28c2-4c2a-9edd-c69e436411f0"},"cell_type":"markdown","source":"We want to filter this down to just the negative counts for each airline. A neat little trick to conver this output into just another dataframe is simply by resetting the index"},{"source":"day_df = day_df.reset_index()\nday_df.head()","outputs":[],"metadata":{"_uuid":"b1644691bb4975bfcf38e0b214a10b71832cab34","_cell_guid":"165ea3cb-6ee8-477e-b355-c6c7c582aa5d"},"cell_type":"code","execution_count":43},{"metadata":{"_uuid":"8e5c506f6d1cdef7543bb4e73242c7211049252d","_cell_guid":"6aa65a3b-fcc5-442c-9dec-4ec50051fa76"},"cell_type":"markdown","source":"Next, we want to rename that '0' column to something more informational, like 'Count'. Also we can use boolean logic to filter the dataframe down to only 'negative' ratings"},{"source":"#rename the column\nday_df = day_df.rename(columns = {0:'Count'})\n#filter to only negative ratings\nday_df = day_df[day_df.Rating == 'negative'].reset_index()\n#Remove delta since it only has 2 entries\nday_df = day_df[day_df.Airline != 'Delta Airlines']\nday_df.head()","outputs":[],"metadata":{"_uuid":"df21fb6cc106ca05bce7fccbe98c0efb55683a3b","_cell_guid":"c272d39d-4376-4e9c-8da3-16de6898481a"},"cell_type":"code","execution_count":44},{"metadata":{"_uuid":"9803f181c12ef10484ac8ab7ada6a21b6c7c787c","_cell_guid":"f00ae7c7-560a-40d6-93ea-b0ece10ebd46"},"cell_type":"markdown","source":"We have an extra column in there that we don't really are about, so we can slice that out, and then regroup our new dataframe again using 'Date' as the main index and 'Airline' as the secondary index. From there, we can simply plot our results"},{"source":"#slice out the first 2 columns of the resultant dataframe\nday_df = day_df.iloc[:,1:5]\n\n#groupby and plot data\nax2 = day_df.groupby(['Date','Airline']).sum().unstack().plot(kind = 'bar', colormap = 'viridis', figsize = (15,6), rot = 70)\nlabels = ['American Airlines','Jet Blue','Southwest Airlines','US Airways','United Airlines','Virgin Airlines']\nax2.legend(labels = labels)\nax2.set_xlabel('Date')\nax2.set_ylabel('Negative Tweets')\nplt.show()","outputs":[],"metadata":{"_uuid":"5cea3eaf028083fbfd078cab40716d4b556962ce","_cell_guid":"6ecd5820-0a80-406a-91c3-47aa7bf2a5bc"},"cell_type":"code","execution_count":45},{"metadata":{"_uuid":"86ee09ddabf95c2121e02edeabcc001be19c32a5","_cell_guid":"0b1920dc-fa70-4a0b-a753-095b8301d11a","collapsed":true},"cell_type":"markdown","source":"I don't know what happend on the 23, but it looks like all the American Airlines flights decided to revolt against their customers all at once. Wow."},{"metadata":{"_uuid":"4ddbe9e61ae2d3097358364d0a528094895a40b0","_cell_guid":"db569693-9bd2-40ee-b831-8343f308772f"},"cell_type":"markdown","source":"### SUMMARY: AVOID US AIRWAYS\nOverall it appears that Virgin Airlines would be the best choice, but be careful of this decisions. Virgin was the least talked about Airline in the Twitter-verse, but just because people aren't talking about you doesn't mean they like you either! But let's roll the dice and fly with Sir Richard Branson"}],"nbformat":4,"metadata":{"language_info":{"name":"python","version":"3.6.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}