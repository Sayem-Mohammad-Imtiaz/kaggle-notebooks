{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"#Importing Numpy and Pandas\nimport numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Using the Social_Network_Ads Dataset\ndata = pd.read_csv('../input/Social_Network_Ads.csv')\ndata.drop(columns=['User ID','Gender',],axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4cb45e28344e7e245ab398e9f4f5272ef21d2129"},"cell_type":"code","source":"#Outcome\ny = data.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7a145fa49435ad9578ec2827f76a70cc99f2e1","collapsed":true},"cell_type":"code","source":"#Features\nX = data.iloc[:,:-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dffb1f3e19e19964995ac827bf55108b5815ff67"},"cell_type":"code","source":"#Splitting the  Dataset into Training and Testing Set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d4ed14782e114ae3282f20d3754121398e6d232"},"cell_type":"code","source":"#Performing Feature Scaling on Feature variables\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ff7415e3e0e0673d59051cfe6154c63d3312a32"},"cell_type":"code","source":"#y_pred, to store results of test set\n#len_x, to store number of features\n#w and b will be coefficients and intercept\ny_pred = []\nlen_x = len(X_train[0])\nw = []\nb = 0.2\nprint(len_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a228174207f4631be4f26a0cc05e379f3f58aa56"},"cell_type":"code","source":"#entries, to store number of rows in training set\nentries = len(X_train[:,0])\nentries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d4d6e47ee65c9c7404e60fcf8f05c11708546b3"},"cell_type":"code","source":"#Initially all coefficients w1,w2 etc set to 0\nfor weights in range(len_x):\n    w.append(0)\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"18dbd2196d72527a82d30ab88ed2aa8d10bd01ce"},"cell_type":"code","source":"#Sigmoid Function to return probability between 0 and 1\ndef sigmoid(z):\n    return (1/(1+np.exp(-z)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa0f87fdbf98591cb9f51b8dc7157dc399ca827","collapsed":true},"cell_type":"code","source":"#Predict the probability of y being 1 given features of X\ndef predict(inputs):\n    z = np.dot(w,inputs)+b\n    a = sigmoid(z)\n    return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4126f842d072ccd40019cc283b767a014e2ee074","collapsed":true},"cell_type":"code","source":"#Loss function\ndef loss_func(y,a):\n    J = -(y*np.log(a) + (1-y)*np.log(1-a))\n    return J         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc0ceb65c69f4ee0c3f28e050744229dc90c621b"},"cell_type":"code","source":"#dw will store d(Loss(a,y))/d(w(i)) where d denotes differentiation\n#db will store d(Loss(a,y))/db where d denotes differentiation\n#J, total loss\n#alpha, learning rate\ndw = []\ndb = 0\nJ = 0\nalpha = 0.1\nfor x in range(len_x):\n    dw.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4be38e9b500ae0c5a7134296a3055675c4fb2d8","collapsed":true},"cell_type":"code","source":"#Repeating the gradient descent process 1000 times\nfor iterations in range(1000):\n    for i in range(entries):\n        localx = X_train[i]                      \n        a = predict(localx)                     \n        dz = a - y_train[i]                     \n        J += loss_func(y_train[i],a)            \n        for j in range(len_x):                    \n            dw[j] = dw[j]+(localx[j]*dz)\n        db += dz\n    J = J/entries\n    db = db/entries\n    for x in range(len_x):\n        dw[x]=dw[x]/entries\n    for x in range(len_x):              #Updating the coefficients and intercept\n        w[x] = w[x]-(alpha*dw[x])       #w(x) = w(x) - learning_rate * dw(x)\n    b = b-(alpha*db)         \n    J=0\n#localx will be the i(th) row from training set\n#a will be the predicted value when features are localx\n#dz is the differentiation of d(Loss(a,y)) w.r.t dz where z = (w1 * x1) + (w2 * x2)+....+b \n#J is the total cost, only used to check if model is converging\n#Calculating Individual dw(dw1,dw2 etc...) where dw(j) = dz * x(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5479ccb6073ed1ea310ef7de01b2935fc3ec400e"},"cell_type":"code","source":"#Printing the coefficients\nprint(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a939c247b8a092f74c9843975612daa85c423621"},"cell_type":"code","source":"#Printing the intercept\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ae24169a21c7ac8ea0787f4a38a0de3e07a6b5","collapsed":true},"cell_type":"code","source":"#Predicting on test data and appending results to y_pred\nfor x in range(len(y_test)):\n    y_pred.append(predict(X_test[x]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"967ad1b72305ad792a5d50e4d8b8a07632f7b241"},"cell_type":"code","source":"for x in range(len(y_pred)):\n    #Displaying Actual vs Predicted Values\n    print('Actual ',y_test[x],' Predicted ',y_pred[x])\n    #Rounding off values of y_pred, round(y_pred[x]) can also be used\n    if y_pred[x]>=0.5:\n        y_pred[x]=1\n    else:\n        y_pred[x]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a59807150900082ab876ef0200c6c7f8f93e098c"},"cell_type":"code","source":"#Checking the number of Correct Results\ncount = 0\nfor x in range(len(y_pred)):\n    if(y_pred[x]==y_test[x]):\n        count=count+1\n#Displaying the accuracy\nprint('Accuracy:',(count/(len(y_pred)))*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d0835f4117b0ef5ad983f1fb8128c1903dcebc0"},"cell_type":"code","source":"#Analysing the test results\ncorrect1 = 0  #True results correctly classified(Will buy) \ncorrect0 = 0  #False results correctly classified(Will not buy)\nfalse_pos = 0 #False Positive\nfalse_neg = 0 #False Negative\nfor x in range(len(y_pred)):\n    if(y_pred[x]==1 and y_test[x]==1):\n        correct1 += 1\n    elif (y_pred[x]==0 and y_test[x]==0):\n        correct0 += 1\n    elif(y_pred[x]==0 and y_test[x]==1):\n        false_pos += 1\n    else: \n        false_neg += 1\nprint('Test Cases correctly classified :',correct1+correct0)\nprint('No of false positives :',false_pos)\nprint('No of false negatives :',false_neg)\n\n#Note : Confusion Matrix Could also be used     ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}