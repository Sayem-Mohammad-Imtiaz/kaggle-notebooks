{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the dataset\n\ndf = pd.read_csv(\"/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there is 614 rows and 13 columns in the above datasets\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describing the Dataset\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting info about the datasets\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To look for the null values\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the data is having some values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'] = df['Gender'].fillna(\"Female\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Gender'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Married'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Married'] = df['Married'].fillna(\"No\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Dependents'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Dependents'] = df['Dependents'].fillna(\"3+\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Self_Employed'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Self_Employed'] = df['Self_Employed'].fillna(\"Yes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Credit_History'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Credit_History'] = df['Credit_History'].fillna(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After handling the missing data individually we can see that there is no null values so we can do the remaining analysis smoothly"},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets separate the categorical and numerical columns \n\ncat_data = []\nnum_data = []\n\nfor i,c in enumerate(df.dtypes):\n    if c == object:\n        cat_data.append(df.iloc[:, i])\n    else:\n        num_data.append(df.iloc[:, i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting them into dataframe\n\ncat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical Data\n\nnum_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have already taken care of all the missing values so there is False values "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical Data\n\ncat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have already taken care of all the missing values so there is False values  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the ID Column from cat_data\ncat_data.drop('Loan_ID', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Label Encoder for the Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming the Categorical Columns\n\nfor i in cat_data:\n    cat_data[i] = le.fit_transform(cat_data[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\n    1.0 : 1,\n    0.0 : 0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data.loc[:, \"Credit_History\"] = num_data.Credit_History.map(mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concating the both the numerical and categorical column after the operations\n\ndf = pd.concat([cat_data, num_data], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Loan_Status', axis = 1)\ny = df['Loan_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the Data into train-test split\n\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the various model for training\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodels = {\n    'LogisticRegression' : LogisticRegression(random_state = 42),\n    'KNeighborsClassifier' : KNeighborsClassifier(),\n    'SVC' : SVC(random_state = 42),\n    'DecisionTreeClassifier' : DecisionTreeClassifier(max_depth = 5, random_state = 42)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the Functions\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.4f\\n  rec: %.4f\\n  f1: %.4f\\n  loss: %.4f\\n  acc: %.4f' % (pre, rec, f1, loss, acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the model\n\ndef train_eval(models, X, y):\n    for name, model in models.items():\n        print(name, ':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('#'*40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_eval(models, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usingthe Stratified K Fold to split the model\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n\ndef train_eval(models, X, y, folds):\n    # Since (iloc don't work on numpy array) we will change X & y to dataframe because we will use iloc\n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    \n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name, ':')\n        \n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train])\n            y_pred = model.predict(X.iloc[test])\n            ls.append(loss(y.iloc[test], y_pred, retu = True))\n            \n        print(pd.DataFrame(np.array(ls).mean(axis = 0), index = idx)[0])\n        \n        print('#'*40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_eval(models, X_train, y_train, skf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you liked this notebook please ipvote"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}