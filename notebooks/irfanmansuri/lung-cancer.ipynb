{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install dexplot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px \nimport plotly.graph_objs as go\nfrom plotly.offline import iplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/lung-cancer-dataset/lung_cancer_examples.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying to get the info from the dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Trying to see whether there is any null value or bot in the dataset\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its Cool that there is no null value in the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets describe the dataset\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see the number of unique values in each of the relevant columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of unique values in Smokes columns \nprint(df['Smokes'].nunique())\ndf['Smokes'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above output we can see that there are some serious smoker who can smokes upto 34 cigarettes in a day and some don't even smoke"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of unique values in AreaQ columns\nprint(df['AreaQ'].nunique())\ndf['AreaQ'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of unique values in Alkhol columns\nprint(df['Alkhol'].nunique())\ndf['Alkhol'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of unique values in Age columns\nprint(df['Age'].nunique())\ndf['Age'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since Name and Surname are in no way impacting the accuracy of the model so Dropping them\n\ndf.drop(['Name', 'Surname'], inplace = True, axis = 'columns')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's Visualize the  Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the distribution of Data about the persons who are suffering from \n# Lung Cancer or Not\n\nlabels = df['Result'].value_counts()[:].index\nvalues = df['Result'].value_counts()[:].values\n\ncolors=['#2678bf', '#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets visualize the distribution of the person who are smokes in \n# terms of the number cigrates smokes\n\n# I will plot here the bar graph for the top 10 smokers\n\nlabels = df['Smokes'].value_counts()[:10].index\nvalues = df['Smokes'].value_counts()[:10].values\n\ncolors=df['Smokes']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets visualize the AreaQ \nlabels = df['AreaQ'].value_counts().index\nvalues = df['AreaQ'].value_counts().values\n\ncolors=df['AreaQ']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets visualize the Alkhol\n\nlabels = df['Alkhol'].value_counts().index\nvalues = df['Alkhol'].value_counts().values\n\ncolors=df['Alkhol']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets visualize the number of cigarattes one Smoke and whether he suffers from \n# Lung Cancer or not\n\nimport dexplot as dxp\n\ndxp.count(val='Smokes', data=df, figsize=(4,3), split = 'Result', normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building and Trainig the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data\n\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Result'], axis = 'columns')\ny = df['Result']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the shape of the splitted data\n\nprint('The shape of X_train is {}'.format(X_train.shape))\nprint('The shape of X_test is {}'.format(X_test.shape))\nprint('The shape of y_train is {}'.format(y_train.shape))\nprint('The shape of y_test is {}'.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Definig the model to calculate the True  Positive, True Negative, False Positive and False Negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"def true_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: number of true positives\n    \"\"\"\n    \n    # initialize\n    tp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            tp += 1\n    return tp\n\ndef true_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: number of true negatives\n    \"\"\"\n    \n    # initialize\n    tn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 0:\n            tn += 1\n    return tn\n\ndef false_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: number of false positives\n    \"\"\"\n    \n    # initialize\n    fp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 1:\n            fp += 1\n    return fp\n\ndef false_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: number of true positives\n    \"\"\"\n    \n    # initialize\n    fn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 0:\n            fn += 1\n    return fn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1. Logistic Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel_log = LogisticRegression()\nmodel_log.fit(X_train, y_train)\npredict1 = model_log.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy Score"},{"metadata":{},"cell_type":"markdown","source":"Since we know the accuracy score of the model is given by using the formula\n\nAccuracy Score = (TP+TN)/(TP+TN+FP+FN)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: accuracy score\n    \"\"\"\n    \n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    \n    accuracy_score = (tp+tn)/(tp+tn+fp+fn)\n    return accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the accuracy score of the above model\n\naccuracy_score(y_test, predict1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above model we can see that the model seems to be 100% accurate since the data is small we can cross-check even but for the larger dataset 100% accuracy is clear indication of model being biased"},{"metadata":{},"cell_type":"markdown","source":"### Precision"},{"metadata":{},"cell_type":"markdown","source":"Precision = tp/(tp+fp)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definig the model\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: precision score\n    \"\"\"\n    \n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    precision = tp/(tp+fp)\n    return precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the precision score of the above model\n\n\nprecision(y_test, predict1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above model has 100% precision score, since the data is small we can cross-check even but for the larger dataset 100% accuracy is clear indication of model being biased"},{"metadata":{},"cell_type":"markdown","source":"### Recall"},{"metadata":{},"cell_type":"markdown","source":"Recall = tp/(tp+fn)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: recall score\n    \"\"\"\n    \n    tp = true_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    recall = tp/(tp+fn)\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the recall score of the above model\n\n\nrecall(y_test, predict1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### F1 Score"},{"metadata":{},"cell_type":"markdown","source":"Both precision and recall range from 0 to 1 and a value closer to 1 is better\n\nF1 = 2PR/(P+R)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the model\n\ndef f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate the True Positive\n    : param y_true: list of true values\n    : param y_pred: list of predicted values\n    : return: f1 score\n    \"\"\"\n    \n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    \n    score = 2*p*r/(p+r)\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the f1 score \n\nf1(y_test, predict1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the roc_auc_score\n\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test, predict1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, predict1)\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot = True, ax = ax)\nplt.title('Confusion Matrix for the Logistic Regression')\nplt.ylabel('True Value')\nplt.xlabel('Predicted Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2. Decision Tree Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_dtc = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, \n                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                                   max_features=None, random_state=None, max_leaf_nodes=None, \n                                   min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, \n                                   presort='deprecated', ccp_alpha=0.0)\nmodel_dtc.fit(X_train, y_train)\npredict2 = model_dtc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the accuracy score of the above model\n\naccuracy_score(y_test, predict2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the precision score of the above model\n\n\nprecision(y_test, predict2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the2recall score of the above model\n\n\nrecall(y_test, predict2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the f1 score \n\nf1(y_test, predict2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the roc_auc_score\n\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test, predict2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, predict2)\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot = True, ax = ax)\nplt.title('Confusion Matrix for the Decision Tree Classification')\nplt.ylabel('True Value')\nplt.xlabel('Predicted Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3. Random Forest Classification\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create a Gaussian Classifier\nmodel_rfc = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n                                   min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n                                   max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n                                   bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n                                   verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\nmodel_rfc.fit(X_train, y_train)\npredict3 = model_rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the accuracy score of the above model\n\naccuracy_score(y_test, predict3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the precision score of the above model\n\n\nprecision(y_test, predict3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the recall score of the above model\n\n\nrecall(y_test, predict3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the f1 score \n\nf1(y_test, predict3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the roc_auc_score\n\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test, predict3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, predict3)\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot = True, ax = ax)\nplt.title('Confusion Matrix for the Decision Tree Classification')\nplt.ylabel('True Value')\nplt.xlabel('Predicted Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 4. SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import svm model\nfrom sklearn import svm\n\n#Create a svm Classifier\nmodel_svm = svm.SVC(kernel='linear') # Linear Kernel\n\nmodel_svm.fit(X_train, y_train)\npredict4 = model_svm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the accuracy score of the above model\n\naccuracy_score(y_test, predict4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the precision score of the above model\n\n\nprecision(y_test, predict4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the recall score of the above model\n\n\nrecall(y_test, predict4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the f1 score \n\nf1(y_test, predict4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the roc_auc_score\n\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test, predict4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, predict4)\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot = True, ax = ax)\nplt.title('Confusion Matrix for the Decision Tree Classification')\nplt.ylabel('True Value')\nplt.xlabel('Predicted Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 5. Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\nmodel_NB = GaussianNB()\n\n# Train the model using the training sets\nmodel_NB.fit(X_train, y_train)\npredict5 = model_NB.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the accuracy score of the above model\n\naccuracy_score(y_test, predict5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the prec5sion score of the above model\n\n\nprecision(y_test, predict5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calcul5ting the recall score of the above model\n\n\nrecall(y_test, predict5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the f1 score \n\nf1(y_test, predict5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the roc_auc_score\n\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test, predict5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, predict5)\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot = True, ax = ax)\nplt.title('Confusion Matrix for the Decision Tree Classification')\nplt.ylabel('True Value')\nplt.xlabel('Predicted Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If you found this notebook please upvote it\n\n### Feel free to comment for any suggestions and queries"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}