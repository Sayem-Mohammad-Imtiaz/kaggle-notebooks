{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://imgur.com/shNBFGq\"><img src=\"https://i.imgur.com/shNBFGq.jpg\" title=\"source: imgur.com\" /></a>"},{"metadata":{},"cell_type":"markdown","source":"# Libraries "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS , ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"url='../input/all-trumps-twitter-insults-20152021/trump_insult_tweets_2014_to_2021.csv'\ndf=pd.read_csv(url)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop Nan"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Manipulation"},{"metadata":{},"cell_type":"markdown","source":"## Time Series columns "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from datetime import date\ndf['date']=pd.to_datetime(df['date'])\n#====\nL = ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']\ndf = df.join(pd.concat((getattr(df['date'].dt, i).rename(i) for i in L), axis=1))\ndf['year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count Hashtags in Tweets "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['hash'] = df['tweet'].apply(lambda word:word.count('#'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count Mentions in Tweets "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"df['men'] = df['tweet'].apply(lambda word:word.count('@'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tweet Length Characters & Class"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['tweet_length_ch']=df['tweet'].apply(lambda x:len(x))\ndf=df.loc[df['tweet_length_ch']<=280]\n\n#=== \ndf['tweet_length']=df['tweet_length_ch'].apply(lambda x:'short' if x <=130 else 'long')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Media "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['med'] = df['tweet'].apply(lambda word:word.count('https://t.co/'))\ndf['med'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_copy=df.copy()\ndf_copy2=df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA "},{"metadata":{},"cell_type":"markdown","source":"## Check Tweets Length "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df['tweet_length_ch'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tweet Length Distribution "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,5))\niris = df_copy['tweet_length_ch']\nsns.kdeplot(data=iris)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most Targets in a Tweet "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"insult_tw=df_copy.groupby('tweet',as_index=False).agg({'insult':'count'})\ninsult_tw.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"insult_tw_75 = insult_tw.loc[insult_tw['insult']==16]\nprint('Most tweet have insulted Targets is : ',insult_tw_75.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://imgur.com/8kjeKny\"><img src=\"https://i.imgur.com/8kjeKny.png\" title=\"source: imgur.com\" /></a>"},{"metadata":{},"cell_type":"markdown","source":"## The Media"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_media=df_copy.loc[df_copy['target']=='the-media']\nprint('Most insult word with The Media was : ',df_media['insult'].value_counts()[:1])\n#==============\ntweet_All = \" \".join(insul for insul in df_media.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100,colormap=\"inferno\", background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Joe Biden"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://imgur.com/mXAQKkV\"><img src=\"https://i.imgur.com/mXAQKkV.jpg\" title=\"source: imgur.com\" /></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_bide=df_copy.loc[df_copy['target']=='joe-biden']\nprint('Most insult word with joe biden was : ',df_bide['insult'].value_counts()[:1])\n#==============\ntweet_All = \" \".join(insul for insul in df_bide.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100,colormap='gray', background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hillary-Clinton"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://imgur.com/NzM9SSq\"><img src=\"https://i.imgur.com/NzM9SSq.png\" title=\"source: imgur.com\" /></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_hc=df_copy.loc[df_copy['target']=='hillary-clinton']\nprint('Most insult word with hillary-clinton was : ',df_hc['insult'].value_counts()[:1])\n#==============\ntweet_All = \" \".join(insul for insul in df_hc.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100,colormap=\"Blues\", background_color=\"skyblue\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Russia-Trump"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://imgur.com/avUKB1f\"><img src=\"https://i.imgur.com/avUKB1f.png\" title=\"source: imgur.com\" /></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_trump_russia =df_copy.loc[df_copy['target']=='trump-russia']\nprint('Most insult word with df_trump_russia was : ',df_trump_russia['insult'].value_counts()[:1])\n#==============\n\ntweet_All = \" \".join(insul for insul in df_trump_russia.insult)\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\nwordcloud_ALL = WordCloud(max_font_size=50,colormap=\"Reds\", max_words=100, background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization( EDA ) "},{"metadata":{},"cell_type":"markdown","source":"## Tweet Length Class - Pie Chart"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nlabels = 'Long', 'Short'\nsizes = [8748,1610]\nexplode = (0.1, 0)  \nplt.figure(figsize=(10,5))\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90);\nplt.axis('equal');  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mentions in Tweet Length "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.countplot(data=df,x='tweet_length',hue='men').set_title('Mention / Tweet Length');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Media in tweets "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,7))\nNo_Media= len(df[df['med']==0])\nMedia = len(df[df['med']>0])\nPlatform = ['NoMedia','Media']\nCount = [No_Media,Media]\n#====\nfig = px.pie(names = Platform,\n             values = Count,\n             title='Media/No Media',\n            color_discrete_sequence = px.colors.sequential.Agsunset)\nfig.update_traces(textposition='inside', textinfo='percent+label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3D Length-Hashtag-Mentions"},{"metadata":{},"cell_type":"markdown","source":"### X = Hashtag                      ,            Y = Mention            ,  Z = Tweet Length"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"d3 = df_copy[['tweet_length_ch','men','hash','tweet_length']]\nhashtag=df_copy['hash'].values\nmention=df_copy['men'].values\nlength=df_copy['tweet_length_ch'].values\nL= df_copy['tweet_length'].values\ntrace = go.Scatter3d(x=hashtag,y=mention,z=length,mode='markers',marker=dict(size=5,color=\"crimson\"))\nfig=go.Figure(data=[trace])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 10 Targets "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"r_op =df['target'].value_counts()\nr_op = r_op[:10]\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(20,6));\nr_op_vis = sns.barplot(r_op.index, r_op.values, alpha=0.8,palette=\"inferno\");\nplt.title('Trump Targets',fontsize=15);\nplt.ylabel('insults', fontsize=12);\nplt.xlabel('Target', fontsize=12);\nr_op_vis.set_xticklabels(rotation=30,labels=r_op.index,fontsize=15);\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most Target People by year  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.core.display import HTML\nHTML('''<div class=\"flourish-embed flourish-chart\" data-src=\"visualisation/5060515\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script></div>''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most insults appears "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\ntweet_All = \" \".join(insul for insul in df.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (10,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dftime = df.groupby('year',as_index=False).agg({'insult':'count'}).reset_index()\npx.line(x=dftime['year'],y=dftime['insult'],title='insult by year')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dftime_dw = df.groupby('dayofweek',as_index=False).agg({'insult':'count'}).reset_index()\npx.line(x=dftime_dw['dayofweek'],y=dftime_dw['insult'],title='insult by Daysofweek')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dftime_q = df.groupby('quarter',as_index=False).agg({'insult':'count'}).reset_index()\npx.line(x=dftime_q['quarter'],y=dftime_q['insult'],title='insult by quarter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tweets = df['tweet'].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_sentences = []\n\nfor word in tweets:\n    all_sentences.append(word)\n\nall_sentences\n\nlines = list()\nfor line in all_sentences:    \n    words = line.split()\n    for w in words: \n       lines.append(w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Punctuation\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import re\n\nlines = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in lines]\n\nlines\n\nlines2 = []\n\nfor word in lines:\n    if word != '':\n        lines2.append(word)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gettig Words roots"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\ns_stemmer = SnowballStemmer(language='english')\n\nstem = []\nfor word in lines2:\n    stem.append(s_stemmer.stem(word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top Mention Keywords"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"stem2 = []\n\nfor word in stem:\n    if word not in nlp.Defaults.stop_words:\n        stem2.append(word)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(stem2)\ndf = df[0].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = df[:20,]\n#== \npx.bar(df, x=df.values,y= df.index, color=df.index, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top Mention Organizations\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n#====== \ndef show_ents(doc):\n    if doc.ents:\n        for ent in doc.ents:\n            print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))\n#======\nnlp = spacy.load('en_core_web_sm') \nnlp.max_length = 2000000000000\n#=====\nstr1 = \" \" \nstem2 = str1.join(lines2)\n\nstem2 = nlp(stem2)\n\nlabel = [(X.text, X.label_) for X in stem2.ents]\n\ndf6 = pd.DataFrame(label, columns = ['Word','Entity'])\n\ndf7 = df6.where(df6['Entity'] == 'ORG')\n\ndf7 = df7['Word'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://www.pngitem.com/pimgs/m/41-412092_as-seen-on-abc-cbs-fox-nbc-cnn.png)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = df7[:20,]\nplt.figure(figsize=(10,5))\npx.bar(df, x=df.values,y= df.index, color=df.index, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top mention People"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm') \nnlp.max_length = 2000000000000\n\nstr1 = \" \" \nstem2 = str1.join(lines2)\n\nstem2 = nlp(stem2)\n\nlabel = [(X.text, X.label_) for X in stem2.ents]\n\ndf10 = pd.DataFrame(label, columns = ['Word','Entity'])\n\ndf10 = df10.where(df10['Entity'] == 'PERSON')\n\ndf11 = df10['Word'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://s3-eu-west-1.amazonaws.com/tutor2u-media/subjects/politics/Democrats.png?mtime=20150924080302)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = df11[:20,]\n\nplt.figure(figsize=(10,5))\n\ndf = df11[:20,]\nplt.figure(figsize=(10,5))\npx.bar(df, x=df.values,y= df.index, color=df.index, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis "},{"metadata":{},"cell_type":"markdown","source":"### Removing characters"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features=tweets.values\n#=== \nprocessed_features = []\n\nfor sentence in range(0, len(features)):\n    # Remove all the Http: urls\n    processed_feature = re.sub('(https?://\\S+)', '', str(features[sentence]))\n    \n    # Remove all the special characters\n    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n\n    # Remove all single characters\n    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n\n    # Remove single characters from the start\n    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n\n    # Substituting multiple spaces with single space\n    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n\n    # Removing prefixed 'b'\n    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n\n    # Converting to Lowercase\n    processed_feature = processed_feature.lower()\n\n    processed_features.append(processed_feature)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Subjectivity & Polarity"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df3=pd.DataFrame()\ndf3['Tweets']=processed_features\n#=======\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\n# Create a function to get the subjectivity\ndef getSubjectivity(text):\n   return TextBlob(text).sentiment.subjectivity\n\n# Create a function to get the polarity\ndef getPolarity(text):\n   return  TextBlob(text).sentiment.polarity\n\n\n# Create two new columns 'Subjectivity' & 'Polarity'\ndf3['Subjectivity'] = df3['Tweets'].apply(getSubjectivity)\ndf3['Polarity'] = df3['Tweets'].apply(getPolarity)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\ndef getAnalysis(score):\n if score < 0:\n  return 'Negative'\n elif score == 0:\n  return 'Neutral'\n else:\n  return 'Positive'\ndf3['Analysis'] = df3['Polarity'].apply(getAnalysis)\ndf3","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Neutral = len(df3[df3['Analysis']=='Neutral'])\nNegative = len(df3[df3['Analysis']=='Negative'])\nPositive = len(df3[df3['Analysis']=='Positive'])\nlabels = ['Negative','Positive','Neutral']\nvalues = [Negative,Positive,Neutral]\n#====\nimport plotly.graph_objects as go\ncolors = ['red','green', 'lightblue' ]\n\nfig = go.Figure(data=[go.Pie(labels=labels,\n                             values=values)])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,textposition='inside',\n                  marker=dict(colors=colors, line=dict(color='grey', width=1)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiments By Time "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_copy['tweet']=df_copy['tweet'].drop_duplicates(inplace=True)\ndf3['year']=df_copy['year']\n#=== \ndf_copy[['sentiment']]=df3['Analysis']\ndf_tim_sen = df_copy[['year','sentiment']]\ndf_copy['year'].value_counts()\n#=== \ndf_time_sen =pd.get_dummies(df_tim_sen).groupby('year').sum().reset_index()\ndf_time_sen =df_time_sen.sort_values('year',ascending=True)\n#=======\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(14,7))\nplt.plot(df_time_sen['year'] ,df_time_sen['sentiment_Negative'],marker='o',label='Negative') \nplt.plot(df_time_sen['year'] , df_time_sen['sentiment_Neutral'],color='blue',marker='*',label='Neutral')  \nplt.plot(df_time_sen['year'] ,df_time_sen['sentiment_Positive'],color='green',marker='+',label='Positive') \n#=== \nplt.annotate('High Negative insult tweets', xy=(2018, 850),  xycoords='data',\n            xytext=(0.8, 0.95), textcoords='axes fraction',\n            arrowprops=dict(facecolor='black', shrink=0.10),\n            horizontalalignment='center', verticalalignment='top',\n            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tweets Length in Sentiments "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_copy2.drop_duplicates(subset=['tweet'])\ndf3['insult']=df_copy2['insult']\ndf3['target']=df_copy2['target']\ndf3['med']=df_copy2['med']\ndf3['tweet_length']=df_copy2['tweet_length']\n#==== \nplt.figure(figsize=(14,5))\nsns.countplot(x='Analysis',data=df3,hue='tweet_length',palette=\"inferno\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Media in Sentiments "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.countplot(x='Analysis',data=df3,hue='med',palette=\"Oranges\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ADVANCED ANIMATED TARGET CARDS \n### Shows Top Targets in Trump insult Tweets and how Trump insult them "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.core.display import HTML\nHTML('''<div class=\"flourish-embed flourish-cards\" data-src=\"visualisation/5123150\"><script src=\"https://public.flourish.studio/resources/embed.js\"></script></div>''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/xTiTnHXbRoaZ1B1Mo8/source.gif\">"},{"metadata":{},"cell_type":"markdown","source":"#  Conclusion \n\n\n\n* Most of The Insult Tweets about Fake News and Democrats \n\n* Trump use  The  sarcastic expressions to insult The others or situations ( Crooked Hillary - Sleepy Joe - Witch Hunt )\n\n* 25% of trump insults tweets have more than 2 insults\n\n* Tweets have more insults when Trump talks about media and newspapers\n\n* Trump use insults as a style in defence of any person or institution\n\n* 85% of Trump insult tweets Length is long \n\n* only 10% of  Trump insult tweets have no media \n\n* Most target people Hillary Clinton, Joe Biden and Adam Schiff\n\n* More insults tweets started from 2017 \n\n* Trump typed more insults tweets in weekends \n\n* Trump typed more insults tweets in the last quarter of the year\n\n* Trump insults newspapers and democratic the most \n\n* 50% of tweets are Negative \n\n* Trump  write more insult negative tweets in 2018  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}