{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction"},{"metadata":{},"cell_type":"markdown","source":"This IBM Sample Dataset has information about Telco customers and if they left the company within the last month (churn). Each row represents a unique costumer, while the columns contains information about customerâ€™s services, account and demographic data. We will be using Python and Seaborn library to plot and analyze the data."},{"metadata":{},"cell_type":"markdown","source":"7043 rows, There are 21 columns with 19 features."},{"metadata":{},"cell_type":"markdown","source":"#### Basic Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import permutation_importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Churn Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df =pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['customerID','Churn'],axis=1)\nY = df['Churn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\ndf.Churn.value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05))\nplt.title('Target Variable \"Churn\"')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"##### Categorical Features"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, (ax) = plt.subplots(8, 2, figsize=(15,60))\n\nfeat = df[['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n       'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod']]\n\nfeat['gender'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05),ax =ax[0][0])\nax[0][0].set_title('Gender')\nfeat['SeniorCitizen'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05),ax =ax[0][1])\nax[0][1].set_title('Senior Citizen')\nfeat['Partner'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05),ax =ax[1][0])\nax[1][0].set_title('Partner')\nfeat['Dependents'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05),ax =ax[1][1])\nax[1][1].set_title('Dependent')\nfeat['PhoneService'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05),ax =ax[2][0])\nax[2][0].set_title('PhoneService')\nfeat['MultipleLines'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[2][1])\nax[2][1].set_title('MultipleLines')\nfeat['InternetService'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[3][0])\nax[3][0].set_title('InternetServices')\nfeat['OnlineSecurity'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[3][1])\nax[3][1].set_title('OnlineSecurity')\nfeat['OnlineBackup'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[4][0])\nax[4][0].set_title('OnlineBackup')\nfeat['DeviceProtection'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[4][1])\nax[4][1].set_title('DeviceProtection')\nfeat['TechSupport'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[5][0])\nax[5][0].set_title('TechSupport')\nfeat['StreamingTV'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[5][1])\nax[5][1].set_title('StreamingTV')\nfeat['StreamingMovies'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[6][0])\nax[6][0].set_title('StremingMovie')\nfeat['Contract'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05),ax =ax[6][1])\nax[6][1].set_title('Contract')\nfeat['PaperlessBilling'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05),ax =ax[7][0])\nax[7][0].set_title('PaperBilling')\nfeat['PaymentMethod'].value_counts().plot(kind='pie',autopct='%1.1f%%',explode=(0.05,0.05,0.05,0.05),ax =ax[7][1])\nax[7][1].set_title('PaymentMethod')\n\nplt.suptitle(\n    \"Categorical Features\", fontweight=\"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat = feat.astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = pd.get_dummies(feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.tenure = df.tenure.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.TotalCharges = pd.to_numeric(X.TotalCharges, errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = X[['tenure','MonthlyCharges','TotalCharges']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Tenure\nsns.violinplot(x=X.tenure, y=Y)\nplt.title('Tenure Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Monthly Charges\nsns.violinplot(x=X.MonthlyCharges, y=Y)\nplt.title('Monthly Charges Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Total Charges\nsns.violinplot(x=X.TotalCharges, y=Y)\nplt.title('Total Charges Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ReCreating Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dat = categorical_features\nfor col in numerical_features:\n    dat[col] = numerical_features[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"labelencoder = LabelEncoder()\ndaf = dat\ndaf['Y'] = Y\ndaf.Y = labelencoder.fit_transform(daf.Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(40,35))\nplt.title('Correlation', y=1.025, size=30)\nsns.heatmap(daf.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.BrBG\nplt.figure(figsize=(14,12))\nplt.title('Correlation', y=1.025, size=20)\nsns.heatmap(daf[['OnlineSecurity_No',\n       'OnlineSecurity_No internet service', 'OnlineSecurity_Yes',\n       'OnlineBackup_No', 'OnlineBackup_No internet service',\n       'OnlineBackup_Yes', 'DeviceProtection_No',\n       'DeviceProtection_No internet service', 'DeviceProtection_Yes',\n       'TechSupport_No', 'TechSupport_No internet service', 'TechSupport_Yes',\n       'StreamingTV_No', 'StreamingTV_No internet service', 'StreamingTV_Yes',\n       'StreamingMovies_No', 'StreamingMovies_No internet service',\n       'StreamingMovies_Yes', 'Contract_Month-to-month', 'Contract_One year',\n       'Contract_Two year', 'PaperlessBilling_No', 'PaperlessBilling_Yes','Y']].astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Outlier Detetction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monthly Charges\nsns.boxplot(x=X.MonthlyCharges, y=Y)\nplt.title('Monthly Charges Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tenure\nsns.boxplot(x=X.tenure, y=Y)\nplt.title('Tenure Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat.tenure[dat.tenure > 70].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Charges\nsns.boxplot(x=X.TotalCharges, y=Y)\nplt.title('Total Charges Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sorting Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X = dat.drop(['Y'],axis=1) \ndata_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_Y = dat.Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"total = data_X.isnull().sum().sort_values(ascending=False) # missing values analysis\npercent = (data_X.isnull().sum()/data_X.isnull().count()).sort_values(ascending=False) # percentage of missing values \nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X.TotalCharges.fillna(data_X.TotalCharges.median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X.TotalCharges.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size = 0.2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX__train = scaler.fit_transform(X_train)\nX__test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PFI (Permutation Feature Importance)"},{"metadata":{},"cell_type":"markdown","source":"##### KNN total feature PFI"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nmodel = KNeighborsClassifier()\nmodel.fit(X__train, Y_train)\nresults = permutation_importance(model, X__train, Y_train, scoring='accuracy')\nimportance = results.importances_mean\n\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### KNN Numerical Feature PFI"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier()\nmodel.fit(data_X[['tenure','MonthlyCharges','TotalCharges',]], data_Y)\nresults = permutation_importance(model, data_X[['tenure','MonthlyCharges','TotalCharges']], data_Y, scoring='accuracy')\nimportance = results.importances_mean\n\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### KNN Label Encoded Feature PFI"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_le = feat\nfor cols in feat:\n    data_le[cols] = labelencoder.fit_transform(feat[cols])\n    \ndata_le['tenure'] = data_X['tenure']\ndata_le['MonthlyCharges'] = data_X['MonthlyCharges']\ndata_le['TotalCharges'] = data_X['TotalCharges']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(18,16))\nplt.title('Correlation', y=1.025, size=30)\nsns.heatmap(data_le.astype(float).corr(),linewidths=0.1,vmax=0.5, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr, X_t, Y_tr, Y_t = train_test_split(data_le, data_Y, test_size = 0.2)\n\nX__tr = scaler.fit_transform(X_tr)\nX__t = scaler.transform(X_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nmodel = KNeighborsClassifier()\nmodel.fit(X__tr, Y_tr)\nresults = permutation_importance(model, X__tr, Y_tr, scoring='accuracy')\nimportance = results.importances_mean\n\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### GradientBoost Label ENcoded Feature PFI"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n\nclassifier = GradientBoostingClassifier()\nclassifier.fit(X__tr, Y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Multi Layer Perceptron Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nMLP = MLPClassifier(hidden_layer_sizes=(20,6))\nMLP.fit(X__tr, Y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nresults = permutation_importance(MLP, X__tr, Y_tr, scoring='accuracy')\nimportance = results.importances_mean\n\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlsst = []\nfor i in range(20):    \n    minimum,maximum = data_le[:,i].min(), data_le[:,i].max()  # minimum and maximum value of tenure\n    arr = np.arange(minimum, maximum, (maximum - minimum)/100) # randomized array between min and max of 100 values\n\n    lst = []\n    for variation in arr:\n        val = list(data_le.median().values) # saving all rows median values in a list\n        val[i] = variation\n        lst.append(val)  \n    lst = np.array(lst)\n    lst = scaler.transform(lst)\n    MLP.predict(lst)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHAP Explainer"},{"metadata":{},"cell_type":"markdown","source":"###### Variable Importance Plot â€” Global Interpretability"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier(max_depth=8, random_state=19)\nRFC.fit(X__tr, Y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = shap.TreeExplainer(RFC).shap_values(X__tr)\nshap.summary_plot(shap_values, X__tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### XGBoost SHAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\n\n# train XGBoost model\n \nmodel_ = xgboost.XGBClassifier().fit(X__tr, Y_tr)\n\n# compute SHAP values\nshap_values = shap.TreeExplainer(model_).shap_values(X__tr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X__tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### SHAP Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_for_prediction = pd.DataFrame(X__tr).iloc[35]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nprint(model_.predict_proba(data_for_prediction_array))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(model_)\nshap_values = explainer.shap_values(X__tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0], data_for_prediction_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[2], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[3], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[4], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[5], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[6], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Visualize the training set predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values, X__tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Kernel Explainer"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_explainer = shap.KernelExplainer(model_.predict_proba, X__tr)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### SHAP Feature Dependence plot "},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for cols in X_tr[['MonthlyCharges',\"TotalCharges\",\"tenure\"]]:\n    shap.dependence_plot(cols, shap_values, X_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### SHAP Interaction Value Summary Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_interaction_values = shap.TreeExplainer(model_).shap_interaction_values(pd.DataFrame(X_tr).iloc[:2000,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Hidden Patterns Analysis"},{"metadata":{},"cell_type":"markdown","source":"###### tenure"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax) = plt.subplots(2, 2, figsize=(16,12))\nshap.dependence_plot(\n    ('tenure', 'SeniorCitizen'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[0][0]\n)\nshap.dependence_plot(\n    ('tenure', 'Partner'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[0][1]\n)\nshap.dependence_plot(\n    ('tenure', 'Contract'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[1][0]\n)\nshap.dependence_plot(\n    ('tenure','TotalCharges'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[1][1]\n)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Monthly Charges"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax) = plt.subplots(2, 2, figsize=(16,12))\nshap.dependence_plot(\n    ('MonthlyCharges', 'Contract'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[0][0]\n)\nshap.dependence_plot(\n    ('MonthlyCharges', 'StreamingTV'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[0][1]\n)\nshap.dependence_plot(\n    ('MonthlyCharges', 'Dependents'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[1][0]\n)\nshap.dependence_plot(\n    ('MonthlyCharges','Partner'),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[1][1]\n)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Total Charges"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, (ax) = plt.subplots(2, 2, figsize=(16,12))\nshap.dependence_plot(\n    ('TotalCharges', \"DeviceProtection\"),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[0][0]\n)\nshap.dependence_plot(\n    ('TotalCharges', \"OnlineBackup\"),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[0][1]\n)\nshap.dependence_plot(\n    ('TotalCharges', \"Contract\"),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[1][0]\n)\nshap.dependence_plot(\n    ('TotalCharges',\"MultipleLines\"),\n    shap_interaction_values, pd.DataFrame(X_tr).iloc[:2000,:],\n    display_features= X_tr.iloc[:2000,:],\n    show = False,\n    ax = ax[1][1]\n)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Feature Desicion Explainer first 100 entries"},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(model_)\nexpected_value = explainer.expected_value\n\nshap.decision_plot(expected_value, shap_values[0:100,], pd.DataFrame(X_tr).iloc[0:100,])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Desicion Explainer  100-200 entries"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values[0:1999,], pd.DataFrame(X_tr).iloc[0:1999,])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Feature Desicion Explainer 2000-2500 entries"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values[2000:2500,], pd.DataFrame(X_tr).iloc[2000:2500,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values[0:1999,], pd.DataFrame(X_tr).iloc[0:1999,],  feature_order='hclust', return_objects=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values[2000:3999,], pd.DataFrame(X_tr).iloc[2000:3999,],  feature_order='hclust', return_objects=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values[4000:5999,], pd.DataFrame(X_tr).iloc[4000:5999,],  feature_order='hclust', return_objects=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_s = data_le\ndf_y = labelencoder.fit_transform(df['Churn'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### SKewness"},{"metadata":{},"cell_type":"markdown","source":"###### Cloned/Taken from https://github.com/datamadness/Automatic-skewness-transformation-for-Pandas-DataFrame/blob/master/TEST_skew_autotransform.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport math\nimport scipy.stats as ss\nimport matplotlib.pyplot as plt\n\ndef skew_autotransform(DF, include = None, exclude = None, plot = False, threshold = 1, exp = False):\n    \n    #Get list of column names that should be processed based on input parameters\n    if include is None and exclude is None:\n        colnames = DF.columns.values\n    elif include is not None:\n        colnames = include\n    elif exclude is not None:\n        colnames = [item for item in list(DF.columns.values) if item not in exclude]\n    else:\n        print('No columns to process!')\n    \n    #Helper function that checks if all values are positive\n    def make_positive(series):\n        minimum = np.amin(series)\n        #If minimum is negative, offset all values by a constant to move all values to positive teritory\n        if minimum <= 0:\n            series = series + abs(minimum) + 0.01\n        return series\n    \n    \n    #Go throug desired columns in DataFrame\n    for col in colnames:\n        #Get column skewness\n        skew = DF[col].skew()\n        transformed = True\n        \n        if plot:\n            #Prep the plot of original data\n            sns.set_style(\"darkgrid\")\n            sns.set_palette(\"Blues_r\")\n            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n            ax1 = sns.distplot(DF[col], ax=axes[0])\n            ax1.set(xlabel='Original ' + col)\n        \n        #If skewness is larger than threshold and positively skewed; If yes, apply appropriate transformation\n        if abs(skew) > threshold and skew > 0:\n            skewType = 'positive'\n            #Make sure all values are positive\n            DF[col] = make_positive(DF[col])\n            \n            if exp:\n               #Apply log transformation \n               DF[col] = DF[col].apply(math.log)\n            else:\n                #Apply boxcox transformation\n                DF[col] = ss.boxcox(DF[col])[0]\n            skew_new = DF[col].skew()\n         \n        elif abs(skew) > threshold and skew < 0:\n            skewType = 'negative'\n            #Make sure all values are positive\n            DF[col] = make_positive(DF[col])\n            \n            if exp:\n               #Apply exp transformation \n               DF[col] = DF[col].pow(10)\n            else:\n                #Apply boxcox transformation\n                DF[col] = ss.boxcox(DF[col])[0]\n            skew_new = DF[col].skew()\n        \n        else:\n            #Flag if no transformation was performed\n            transformed = False\n            skew_new = skew\n        \n        #Compare before and after if plot is True\n        if plot:\n            print('\\n ------------------------------------------------------')     \n            if transformed:\n                print('\\n %r had %r skewness of %2.2f' %(col, skewType, skew))\n                print('\\n Transformation yielded skewness of %2.2f' %(skew_new))\n                sns.set_palette(\"Paired\")\n                ax2 = sns.distplot(DF[col], ax=axes[1], color = 'r')\n                ax2.set(xlabel='Transformed ' + col)\n                plt.show()\n            else:\n                print('\\n NO TRANSFORMATION APPLIED FOR %r . Skewness = %2.2f' %(col, skew))\n                ax2 = sns.distplot(DF[col], ax=axes[1])\n                ax2.set(xlabel='NO TRANSFORM ' + col)\n                plt.show()\n                \n\n    return DF\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"transformedDF = df_s\ntransformedDF['MonthlyCharges'] = skew_autotransform(df_s[['MonthlyCharges']].copy(deep=True), plot = True, \n                                   exp = True, threshold = 0.7, exclude = ['B','LSTAT'])\n\ntransformedDF['TotalCharges'] = skew_autotransform(df_s[['TotalCharges']].copy(deep=True), plot = True, \n                                   exp = True, threshold = 0.7, exclude = ['B','LSTAT'])\n\ntransformedDF['tenure'] = skew_autotransform(df_s[['tenure']].copy(deep=True), plot = True, \n                                   exp = True, threshold = 0.7, exclude = ['B','LSTAT'])\n\n\n\nprint('Average skewness after transformation is %2.2f' %(np.mean(abs(transformedDF.skew()))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the dataset Oversampling to be exact\noversample = SMOTE()\nx_, y_ = oversample.fit_resample(transformedDF, df_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_, y_, test_size = 0.2) \nx__train = scaler.fit_transform(x_train)\nx__test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nSVD = SVC()\nKNC = KNeighborsClassifier()\nP = Perceptron()\nDTC = DecisionTreeClassifier()\nXGB = xgboost.XGBClassifier()\nRF = RandomForestClassifier()\nML = MLPClassifier()\nGBC = GradientBoostingClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_LR = {'C':[0.001,0.01,0.1,1,10,100]}\n\nclf1= GridSearchCV(LR, param_LR, cv=5)\nclf1.fit(x__train, y_train)\npred=clf1.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\nAcc_LR = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_SVD =  {\n    'C' : [1, 2, 3, 4],\n    'kernel' : ['linear', 'rbf', 'sigmoid']    \n}\n\nclf2= GridSearchCV(SVD, param_LR, cv=5)\nclf2.fit(x__train, y_train)\npred=clf2.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\n\nAcc_SVD = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### K Neighbour Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_KNC = {\n    'n_neighbors':np.arange(1,50),\n    'leaf_size' : [30,20,40] \n}\n\nclf3= GridSearchCV(KNC, param_KNC, cv=5, n_jobs= -1)\nclf3.fit(x__train, y_train)\npred=clf3.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\nAcc_KNC = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"perceptron = Perceptron()\nperceptron.fit(x__train, y_train)\npred = perceptron.predict(x__test)\nacc = accuracy_score(y_test,pred)\nprint('accuracy_score',acc)\nAcc_P = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_DTC = {\n    'criterion' : ['gini' ,'entropy'],\n    'max_depth' : [ 3, 5, 7, 9, 11, 13, 15, 17],\n    \"min_samples_leaf\": [ 1, 3, 5, 7, 9, 13, 11]\n}\n\nclf4= GridSearchCV(DTC, param_DTC, cv=5, n_jobs= -1)\nclf4.fit(x__train, y_train)\npred=clf4.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\n\nAcc_DTC = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n##### XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_XGB = {\n     'max_depth':range(3,10,2),\n     'min_child_weight':range(1,10,2),\n     'max_depth':[4,5,6,7,8],\n     'min_child_weight':[4,5,6,7,8],\n     'learning_rate' : [0.1, 0.001],\n     'gamma':[i/10.0 for i in range(0,5)]\n}\n\nclf5= GridSearchCV(XGB, param_XGB, cv=5, n_jobs= -1)\nclf5.fit(x__train, y_train)\npred=clf5.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\n\nAcc_XGB = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_RFC = {\n    'max_depth' : [4, 6, 8, 10, 12],\n    'max_leaf_nodes' :  [3, 6, 9, 12, 14],\n    'min_samples_leaf': [1, 3, 5, 7],\n    'n_estimators' : [80, 100, 120, 140]\n}\nclf6= GridSearchCV(RF, param_RFC, cv=5, n_jobs= -1)\nclf6.fit(x__train, y_train)\npred=clf6.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\n\nAcc_RFC = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### MLP Classifier"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"param_ML = {\n    'hidden_layer_sizes': [(10,30,10),(20,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive'],\n}\n\nclf7= GridSearchCV(ML, param_ML, cv=3, n_jobs=-1)\nclf7.fit(x__train, y_train)\npred=clf7.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\nAcc_ML = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Gradient Boosting Classfier"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_GBC = {\n    'n_estimators':range(20,81,10),\n    'max_depth':range(5,16,2), \n    'min_samples_split':range(200,1001,200),\n    'min_samples_leaf':range(30,71,10)\n}\n\nclf8= GridSearchCV(GBC, param_GBC, cv=5, n_jobs= -1)\nclf8.fit(x__train, y_train)\npred=clf8.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\nAcc_GBC = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Ada Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nparam_AC = {\n    \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n    \"base_estimator__splitter\" :   [\"best\", \"random\"],\n    \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n    \"n_estimators\" :[10, 100, 200, 250],\n    \"learning_rate\":  [0.05, 0.5, 1.5, 2.5]\n}\n\nAC = AdaBoostClassifier(\n    DecisionTreeClassifier())\n\nclf9= GridSearchCV(AC, param_AC, cv=3, n_jobs= -1)\nclf9.fit(x__train, y_train)\npred=clf9.predict(x__test)\nacc = accuracy_score(y_test,pred)\n\nprint('accuracy_score',acc)\nAcc_AC = acc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Models Accuracy Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame( {\n'Models' : ['Logistic Regression', 'Support Vector Machine', \n         'KNeighbours Classfier', 'Perceptron',\n         'Decision Tree Classifier', 'XGBoost Classifier',\n         'Random Forest Classifier', 'Multi-Layer Perceptron',\n         'Gradient Boosting Classifier', 'Ada Boost Classifier'],\n'Score' : [Acc_LR*100, Acc_SVD*100, Acc_KNC*100,\n        Acc_P*100, Acc_DTC*100, Acc_XGB*100,\n        Acc_RFC*100, Acc_ML*100, Acc_GBC*100, Acc_AC*100]\n})\n\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models.plot(kind='line',x='Score',y='Models',figsize=(14,12))\nplt.figure(figsize=(14,6))\nsns.lineplot(data=models, x=\"Models\", y=\"Score\",ci=None, marker='o')\nplt.xticks(rotation = 45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Best Model Accuracy In-Depth Analysis (ADA Boost Classifier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ny_test_pred = clf5.predict(x__test)\ny_train_pred = clf5.predict(x__train)\n\nprint(\"TRAINIG RESULTS: \\n=========================================================\")\nclf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\nprint(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\nprint(\"\\n\")\nprint(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\nprint(\"\\n=========================================================\")\n\nprint(\"TESTING RESULTS: \\n=========================================================\")\nclf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\nprint(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\nprint(\"\\n\")\nprint(f\"CLASSIFICATION REPORT:\\n{clf_report}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, roc_curve\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.title(\"Precision/Recall Tradeoff\")\n    \n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    \n    \nprecisions, recalls, thresholds = precision_recall_curve(y_test, clf5.predict(x__test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, clf5.predict(x__test))\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Area Under Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\ny_pred_proba = clf5.predict_proba(x__test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"Area Under Curve=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}