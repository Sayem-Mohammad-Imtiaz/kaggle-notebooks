{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Image](https://intermountainhealthcare.org/-/media/images/modules/blog/posts/2020/02/how-to-know-if-youre-having-a-heart-attack.jpg)\n\n# Heart Attack Prediction with Pipelines and Column Trans\n\nAbout this dataset\n* Age : Age of the patient\n\n* Sex : Sex of the patient\n\n* exang : exercise induced angina (1 = yes; 0 = no)\n\n* ca : number of major vessels (0-3)\n\n* cp : Chest Pain type chest pain type\n\n     Value 1: typical angina\n     \n     Value 2: atypical angina\n     \n     Value 3: non-anginal pain\n     \n     Value 4: asymptomatic\n\n* trtbps : resting blood pressure (in mm Hg)\n\n* chol : cholestoral in mg/dl fetched via BMI sensor\n\n* fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\n* rest_ecg : resting electrocardiographic results\n\n     Value 0: normal\n     \n     Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or        depression of > 0.05 mV)\n     \n     Value 2: showing probable or definite left ventricular hypertrophy by Estes'            criteria\n\n* thalach : maximum heart rate achieved\n\n* target : 0= less chance of heart attack 1= more chance of heart attack\n\n\nContent: \n\n* [Importing Analysis Libraries](#1)\n* [Reading Data and Overview](#2)\n* [Basic Data Visualization](#3)\n* [Missing Values](#4)\n* [Unique Values](#5)\n* [Handle Outliers](#6)\n* [Import Machine Learning Libraries](#7)\n* [Train-Test Split](#8)\n* [Column Trans](#9)\n* [ML Models Testing with Pipeline](#10)\n* [Importing Models into Pipeline](#11)\n* [Setting Parameters](#12)\n* [Select Best Model and Parameters](#13)\n* [Final Model](#14)\n* [Summary](#15)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a><br>\n## Importing Analysis Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:00.205233Z","iopub.execute_input":"2021-09-08T10:18:00.205632Z","iopub.status.idle":"2021-09-08T10:18:01.20983Z","shell.execute_reply.started":"2021-09-08T10:18:00.205542Z","shell.execute_reply":"2021-09-08T10:18:01.208833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a><br>\n## Reading Data and Overview","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:01.211418Z","iopub.execute_input":"2021-09-08T10:18:01.211724Z","iopub.status.idle":"2021-09-08T10:18:01.231598Z","shell.execute_reply.started":"2021-09-08T10:18:01.211685Z","shell.execute_reply":"2021-09-08T10:18:01.230575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:01.233482Z","iopub.execute_input":"2021-09-08T10:18:01.233789Z","iopub.status.idle":"2021-09-08T10:18:01.265165Z","shell.execute_reply.started":"2021-09-08T10:18:01.233759Z","shell.execute_reply":"2021-09-08T10:18:01.264123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:01.266991Z","iopub.execute_input":"2021-09-08T10:18:01.267435Z","iopub.status.idle":"2021-09-08T10:18:01.326442Z","shell.execute_reply.started":"2021-09-08T10:18:01.267392Z","shell.execute_reply":"2021-09-08T10:18:01.325372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:01.327937Z","iopub.execute_input":"2021-09-08T10:18:01.328369Z","iopub.status.idle":"2021-09-08T10:18:01.348806Z","shell.execute_reply.started":"2021-09-08T10:18:01.328327Z","shell.execute_reply":"2021-09-08T10:18:01.347834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a><br>\n## Basic Data Visualization","metadata":{}},{"cell_type":"code","source":"numeric_list = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ncategorical_list = [\"sex\", \"cp\",\"fbs\",\"restecg\",\"exng\",\"slp\",\"caa\",\"thall\",\"output\"]\ndf_categoric= df.loc[:,categorical_list]\nfor i in categorical_list:\n    plt.figure()\n    sns.countplot(x = i, data = df_categoric, hue = \"output\")\n    plt.title(i)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:01.350256Z","iopub.execute_input":"2021-09-08T10:18:01.350667Z","iopub.status.idle":"2021-09-08T10:18:03.095196Z","shell.execute_reply.started":"2021-09-08T10:18:01.350625Z","shell.execute_reply":"2021-09-08T10:18:03.094279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a><br>\n## Missing Values","metadata":{}},{"cell_type":"markdown","source":"As you can see, our dataset does not have any missing values. I almost cried.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.096395Z","iopub.execute_input":"2021-09-08T10:18:03.096659Z","iopub.status.idle":"2021-09-08T10:18:03.105353Z","shell.execute_reply.started":"2021-09-08T10:18:03.096633Z","shell.execute_reply":"2021-09-08T10:18:03.104138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a><br>\n## Unique Values","metadata":{}},{"cell_type":"code","source":"for i in list(df.columns):\n    print(\"{} -- {}\".format(i, df[i].value_counts().shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.108053Z","iopub.execute_input":"2021-09-08T10:18:03.108438Z","iopub.status.idle":"2021-09-08T10:18:03.124095Z","shell.execute_reply.started":"2021-09-08T10:18:03.108405Z","shell.execute_reply":"2021-09-08T10:18:03.123136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a><br>\n## Handle Outliers","metadata":{}},{"cell_type":"markdown","source":"Following approaches can be used to deal with outliers once we’ve defined the boundaries for them:\n\nRemove the observations and imputation\n\n\n1.Remove the Observations\n\nWe may explicitly delete outlier observation entries from our data so that they don’t influence the training of our models. When dealing with a small dataset, however, eliminating the observations is not a good idea.\n\n2.Imputation\n\nTo impute the outliers, we can use a variety of imputation values, ensuring that no data is lost.\nAs impute values, we can choose between the mean, median, mode, and boundary values.\n\nI chose to remove the outliers here.","metadata":{}},{"cell_type":"code","source":"for i in numeric_list:\n    \n    # IQR\n    Q1 = np.percentile(df.loc[:, i],25)\n    Q3 = np.percentile(df.loc[:, i],75)\n    \n    IQR = Q3 - Q1\n    \n    print(\"Old shape: \", df.loc[:, i].shape)\n    \n    # upper bound\n    upper = np.where(df.loc[:, i] >= (Q3 +2.5*IQR))\n    \n    # lower bound\n    lower = np.where(df.loc[:, i] <= (Q1 - 2.5*IQR))\n    \n    print(\"{} -- {}\".format(upper, lower))\n    \n    try:\n        df.drop(upper[0], inplace = True)\n    except: print(\"KeyError: {} not found in axis\".format(upper[0]))\n    \n    try:\n        df.drop(lower[0], inplace = True)\n    except:  print(\"KeyError: {} not found in axis\".format(lower[0]))\n        \n    print(\"New shape: \", df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.125757Z","iopub.execute_input":"2021-09-08T10:18:03.126062Z","iopub.status.idle":"2021-09-08T10:18:03.15215Z","shell.execute_reply.started":"2021-09-08T10:18:03.126006Z","shell.execute_reply":"2021-09-08T10:18:03.151162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a><br>\n## Import Machine Learning Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.153412Z","iopub.execute_input":"2021-09-08T10:18:03.153679Z","iopub.status.idle":"2021-09-08T10:18:03.559336Z","shell.execute_reply.started":"2021-09-08T10:18:03.153653Z","shell.execute_reply":"2021-09-08T10:18:03.558393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a><br>\n## Train-Test Split","metadata":{}},{"cell_type":"code","source":"df_copy = df.copy()\nx = df_copy.drop(\"output\", axis= 1)\ny = df_copy[\"output\"]\nx_train, x_test, y_train , y_test = train_test_split(x, y, test_size=0.2, random_state = 69)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.560608Z","iopub.execute_input":"2021-09-08T10:18:03.560908Z","iopub.status.idle":"2021-09-08T10:18:03.569666Z","shell.execute_reply.started":"2021-09-08T10:18:03.560879Z","shell.execute_reply":"2021-09-08T10:18:03.56868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a><br>\n## Column Trans","metadata":{}},{"cell_type":"markdown","source":"The column_trans method is a useful sklearn method that allows it to apply different operations to different columns in the dataset.\n\nFor numeric features, I choose MinMaxScaler method as scaler process because of MinMaxScaler preserves the shape of the original distribution. It doesn't meaningfully change the information embedded in the original data. Note that MinMaxScaler doesn't reduce the importance of outliers.\n\nFor categorical features, OneHotEncoder speeds up the process considerably and increases the accuracy rate. What one hot encoding does is, it takes a column which has categorical data, which has been label encoded and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value. ","metadata":{}},{"cell_type":"code","source":"column_trans = make_column_transformer(\n               \n               (MinMaxScaler(), numeric_list),\n               \n               (OneHotEncoder(sparse = False, handle_unknown=\"ignore\"), categorical_list[:-1]),\n                remainder = \"passthrough\")","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.570992Z","iopub.execute_input":"2021-09-08T10:18:03.571725Z","iopub.status.idle":"2021-09-08T10:18:03.576572Z","shell.execute_reply.started":"2021-09-08T10:18:03.571686Z","shell.execute_reply":"2021-09-08T10:18:03.575863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a><br>\n## ML Models Testing with Pipeline","metadata":{}},{"cell_type":"markdown","source":"I think the make_pipeline method has a much easier syntax than the pipeline method. That's why I prefer make_pipeline.","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression() \n\nknn = KNeighborsClassifier()\n\nrfc = RandomForestClassifier()\n\nsvc = SVC()\n\nmodels = [logreg, knn, rfc, svc]\n\nfor i in models:\n    \n    pipeline = make_pipeline(column_trans , i )\n    \n    print(\"{} = {}\" .format(i,cross_val_score(pipeline, x_train, y_train, cv=5, scoring=\"accuracy\").mean()))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:03.577799Z","iopub.execute_input":"2021-09-08T10:18:03.578364Z","iopub.status.idle":"2021-09-08T10:18:05.232178Z","shell.execute_reply.started":"2021-09-08T10:18:03.578321Z","shell.execute_reply":"2021-09-08T10:18:05.231135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a><br>\n## Importing Models into Pipeline","metadata":{}},{"cell_type":"code","source":"pipelines = []\n\nfor i in models:\n    \n    pipelines.append(make_pipeline(column_trans, i))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:05.233419Z","iopub.execute_input":"2021-09-08T10:18:05.23375Z","iopub.status.idle":"2021-09-08T10:18:05.238616Z","shell.execute_reply.started":"2021-09-08T10:18:05.233711Z","shell.execute_reply":"2021-09-08T10:18:05.237645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a><br>\n## Setting Parameters","metadata":{}},{"cell_type":"markdown","source":"When tuning a model in the pipeline, we must write the parameter names together with the name defined in the pipeline. for example we should write \"logisticregression__C\" instead of \"C\".","metadata":{}},{"cell_type":"code","source":"\ngrid_logreg = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100], \"logisticregression__penalty\": [\"l1\",\"l2\"], \"logisticregression__solver\": [\"saga\", 'liblinear']}\n\ngrid_knn = {\"kneighborsclassifier__n_neighbors\": np.arange(0,11), \"kneighborsclassifier__weights\": [\"uniform\",\"distance\"], \"kneighborsclassifier__metric\": [\"euclidean\", \"manhattan\"]}\n\ngrid_rfc = { 'randomforestclassifier__n_estimators': [200, 300, 400, 500], 'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'], 'randomforestclassifier__max_depth' : np.arange(1,11), 'randomforestclassifier__criterion' : ['gini', 'entropy']}\n\ngrid_svc = {'svc__C': [0.1,1, 10, 100], 'svc__gamma': [1,0.1,0.01,0.001],'svc__kernel': ['rbf', 'poly', 'sigmoid']}\n\nparameters = [grid_logreg, grid_knn, grid_rfc, grid_svc]","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:05.239819Z","iopub.execute_input":"2021-09-08T10:18:05.240119Z","iopub.status.idle":"2021-09-08T10:18:05.253368Z","shell.execute_reply.started":"2021-09-08T10:18:05.240091Z","shell.execute_reply":"2021-09-08T10:18:05.252272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a><br>\n## Select Best Model and Parameters","metadata":{}},{"cell_type":"markdown","source":"There are two methods in sklearn for hypermeter: GridSearchCV and RandomizedSearchCV.\n\nGrid search is a technique which tends to find the right set of hyperparameters for the particular model. In this tuning technique, we simply build a model for every combination of various hyperparameters and evaluate each model. The model which gives the highest accuracy wins. The pattern followed here is similar to the grid, where all the values are placed in the form of a matrix. Each set of parameters is taken into consideration and the accuracy is noted. Once all the combinations are evaluated, the model with the set of parameters which give the top accuracy is considered to be the best. \n\nRandom search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. The drawback of random search is that it yields high variance during computing. Since the selection of parameters is completely random; and since no intelligence is used to sample these combinations, luck plays its part.\n\nIf the n_jobs parameter is given -1, the computer will spend all its power on the process and the time will be shortened.","metadata":{}},{"cell_type":"code","source":"for i, a in zip(pipelines, parameters):\n    grid_search = GridSearchCV(i, a, cv= 5, n_jobs= -1, scoring = \"accuracy\")\n    grid_search.fit(x_train, y_train)\n    \n    grid_random = RandomizedSearchCV(i, a, cv= 5, n_jobs= -1, scoring = \"accuracy\")\n    grid_random.fit(x_train, y_train)\n    \n    print(\"best grid parameters for {} = {}\".format(i, grid_search.best_params_))\n    print(\"best grid score for {} = {}\".format(i, grid_search.best_score_))\n    print(\"best random parameters for {} = {}\".format(i, grid_random.best_params_))\n    print(\"best random score for {} = {} \\n\\n\" .format(i, grid_random.best_score_))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:18:05.25469Z","iopub.execute_input":"2021-09-08T10:18:05.255074Z","iopub.status.idle":"2021-09-08T10:24:05.140524Z","shell.execute_reply.started":"2021-09-08T10:18:05.255005Z","shell.execute_reply":"2021-09-08T10:24:05.135586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It may seem a bit complicated, but it's not hard to find the accuracy and the best parameters. now i will apply the best parameters i found to the final model.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"14\"></a><br>\n## Final Model","metadata":{}},{"cell_type":"code","source":"model =  RandomForestClassifier( n_estimators = 300, max_features = \"log2\" , max_depth = 2, criterion = \"gini\" )\nfinal_model = make_pipeline(column_trans , model )\nfinal_model.fit(x_train, y_train)\npred = final_model.predict(x_test)\nprint(\"Final Model Accuracy = {}\".format(accuracy_score(y_test, pred)))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:31:05.943601Z","iopub.execute_input":"2021-09-08T10:31:05.943938Z","iopub.status.idle":"2021-09-08T10:31:06.559734Z","shell.execute_reply.started":"2021-09-08T10:31:05.943909Z","shell.execute_reply":"2021-09-08T10:31:06.558604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a><br>\n# Summary","metadata":{}},{"cell_type":"markdown","source":"As you can see, you can do the same job much faster with less code by using the pipelines and column trans methods. If you like my notebok, please don't forget to upvote. I would be happy to see your thoughts and suggestions about my notebook in the comments section. I hope everything goes the way you want. Have a nice work!!","metadata":{}}]}