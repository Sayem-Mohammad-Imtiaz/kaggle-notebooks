{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Objective**\n\n1. Techniques to Feature Engineering\n\n2. Thorough Exploratory Data Analysis (EDA)\n\n**Introduction**\n\nThis is my 2nd Kernel so still learning! Do help by commenting any improvements you feel i could use! This is based on the Give Me Some Credit : 2011 Competition Data (Classification problem type). I'm mainly self-taught so don't mind the basic or 'greenhorn' approaches used. \n\n> Do note that this Kernel focuses on Feature Engineering & EDA. Hence, the explanations are emphasized more on these 2 aspects (Chapters 3 to 8). In terms of explanations, I've also included the logic for each feature engineered & pattern evaluations during EDA. \n\n>Now lets Dive in!\n\n\n**Chapter Outline**\n\n0.Dataset Dictionary (For Reference only)\n\n1.Open Dataset\n\n2.Preliminary Overview\n\n    2.1 Shape\n    2.2 Head raw preview\n    2.3 Sample raw preview\n    2.4 Brief Info\n    2.5 Describe (Max, Min, Quartiles)\n    2.6 Own Quick-Stats Function\n\n3.Preliminary Data Cleaning [C1-Correct, C2-Complete]\n\n    3.1 Age\n    3.2 Monthly Income\n    3.3 Number of Dependents\n\n4.Brief EDA\n\n    4.1 Target Variable\n    4.2 Other Features\n    4.3 Notes for Feature Engineering\n\n5.Deeper Data Cleaning/Feature Engineering [C3-Create, C4-Convert]\n\n    5.1 Group_A (Combine Feature)\n    5.2 Group_B (New Feature Net Worth)\n    5.3 Group_C (Combine Feature)\n    5.4 New Feature (Debt Payments)\n    5.5 New Feature (Age Category)\n    5.6 New Feature (Social Economic Status SES Category)\n    5.7 Checks again before Charting for EDA\n\n6.Deeper EDA\n\n    6.1 Quick Data Prep\n    6.2 Uni-variate\n    6.3 Bi-variate\n\n7.EDA Commentary\n\n    7.1 Summative\n    7.2 Heat-Map Reference\n\n8.Final Feature Engineering Commentary\n\n9.Prepare Data - A\n\n    9.1 Establish Features Decided on (Keep/Discard)\n    9.2 Check Heat-Map Again\n    9.3 Replicate for Test-set (DataCleaning & FeaturesDecided)\n\n10.Quick Feature Engineering Validation\n\n    10.1 Accuracy Score (Without Preliminary Engineering)\n    10.2 Accuracy Score (With Preliminary Engineering)\n\n11.Prepare Data - B\n\n    11.1 Train/Test Split\n    11.2 Feature Comparison Function\n    11.3 LASSO L1 Regularized\n    11.4 Ridge L2 Regularized\n    11.5 Logisitc Regression Balanced by sample weight\n    11.6 XGBoost Classifier\n    11.7 Random Forest Classifier\n\n12.Features (Side To Side Comparison)\n\n    12.1 (Coefficient Values) Quick Easy Method\n    12.2 (Coefficient Values) Neat DataFrame Method\n    12.3 (Plotting) Quick Method\n    12.4 (Plotting) Sorted Neat Method\n    12.5 Quick Commentary Result (Feature Engineering VS Importance)\n\n13.ROC AUC (Side To Side Comparison)\n\n14.Cross Validation (Side To Side Comparison)\n"},{"metadata":{"_uuid":"673bcdf4c1cf243756825290b1e820ae86271a83"},"cell_type":"markdown","source":"# Summative of EDA\n\n**Dataset:**\n\n    - Features has extreme frequencyâ€™s concentrated within sub-categories\n    - High Kurtosis & Skewness of some features\n    - Huge income disparity\n    - Predominant retired (>63yrs old) dataset\n    - Unbalanced target datases\n\n**Trends:**\n    \n    - Those who have had experience financial distress (target variable)\n    >Made lesser loans or exceed deadlines\n    >Tend to have lesser dependents & debt ratio & net worth\n    >As expected are of lower-tier income, But lower debt ratio\n\n    - Ignoring mortality and time value of money (i.e.Annuities)\n    >Debt ratio & Net worth shows gaussian distribution against age\n\n    - Those who had acts of debt delinquency (Made loans or exceed deadlines)\n    >Tend to be from the higher-tier income or Retired\n\n    -Others\n    >The higher the income, the higher the debt ratio\n    >The higher the income, the lower the dependents\n"},{"metadata":{"_uuid":"22e5ade0d9a20255a50c5eb3c8b069156d103216"},"cell_type":"markdown","source":"# **Some Considerations :**\n \n    -Feature Engineering\n    A.Consider context of problem\n    B.Consier dispersion/balance of dataset\n    C.Avoid disrupting the inherent distribution\n    D.Use intuitive logical reasoning when imporvising benchmarks (e.g.Minimum Age, Income to age dispersion)\n    E.Go beyong context (E.g.General Net Worth formula used by broad pension-type valuations)\n    -EDA\n    F.Have alternative charts for high Kurtosis & Skewed data.\n    G.Use multiple references when discarding engineered featurs (e.g.Distribution charts & HeatMap correlations )\n"},{"metadata":{"_uuid":"47c71aef8e12cd1470131755910441c1a2d172ac"},"cell_type":"markdown","source":"# 0.Dataset Dictionary (For Reference only)\n\nFeature Name: Description {Type}\n\n* SeriousDlqin2yrs: Person experienced 90 days past due delinquency or worse {Y/N}\n\n* RevolvingUtilizationOfUnsecuredLines: Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits {percentage}\n                                        \n* age: Age of borrower in years {integer}\n\n* NumberOfTime30-59DaysPastDueNotWorse: Number of times borrower has been 30-59 days past due but no worse in the last 2 years. {integer}\n                                        \n* DebtRatio: Monthly debt payments, alimony,living costs divided by monthly gross income {percentage}\n                                        \n* MonthlyIncome: Monthly income {real}\n\n* NumberOfOpenCreditLinesAndLoans: Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards) {integer}\n                                        \n* NumberOfTimes90DaysLate: Number of times borrower has been 90 days or more past due.\t{integer}\n\n* NumberRealEstateLoansOrLines: Number of mortgage and real estate loans including home equity lines of credit {integer}\n                                        \n* NumberOfTime60-89DaysPastDueNotWorse: Number of times borrower has been 60-89 days past due but no worse in the last 2 years. {integer}\n                                        \n* NumberOfDependents: Number of dependents in family excluding themselves (spouse, children etc.) {integer}"},{"metadata":{"_uuid":"b8090c4f964b99308a5d9a9af4b4be2beaa20e76"},"cell_type":"markdown","source":"# 1.Open Dataset\n\nLets first open the dataset!!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3289083156069427b32bd0ecf5c3873d98e83097"},"cell_type":"code","source":"# Import Modules\n\n# Foundational Packages\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.options.display.max_columns = 100","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ed89d557ec73f9997c97c38b9df09eb3790b435","collapsed":true},"cell_type":"code","source":"\"\"\"Read & Open CSV files\"\"\"\n# Open Train & Test files\ntrain_raw = pd.read_csv('../input/cs-training.csv', na_values=-1) #FYI na_values are defined in the original data page\ntest_raw = pd.read_csv('../input/cs-test.csv', na_values=-1)\n# Copy Train file for workings\ntrain_raw_copy = train_raw.copy(deep=True)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"c9b2d5e70ba1b7ff41eaa978e1c786b2eea254d0"},"cell_type":"markdown","source":"# **2.Preliminary Overview**\n\nNow to do some quick overview. In short the shape for the structure which simply is the size dimensions (length of rows and width of columns) of the data-set.\n\nHead & Samples are to give us a authentic visual of the dataset.\n\nInfo & Describes gives us the computational datatypes & basic statistics respectively."},{"metadata":{"_uuid":"9697fc932005b39d39b3e59f6dea93dc2e4255ed"},"cell_type":"markdown","source":"# **2.1** Shape"},{"metadata":{"trusted":true,"_uuid":"f045228a8e6cc17a8eb3a30f11b12bffc7dfbc54","collapsed":true},"cell_type":"code","source":"\"\"\"Shape\"\"\"\nprint('Train Shape: ', train_raw_copy.shape)\nprint('Test Shape: ', test_raw.shape)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"ced0f52d5725fd85aa7d75774fc1c89912374e25"},"cell_type":"markdown","source":"# **2.2** Head"},{"metadata":{"_uuid":"cf83e9ebcd876c000c09c7fa0e8e94587fed32ff"},"cell_type":"markdown","source":"*** Train set**"},{"metadata":{"trusted":true,"_uuid":"ab298dab6147948190aba69b13e8d43591f8b6e1","collapsed":true},"cell_type":"code","source":"display(train_raw_copy.head())","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"d13ca1beb4333bd9d355201ce2719b95a8cf45e7"},"cell_type":"markdown","source":"*** Test set**"},{"metadata":{"trusted":true,"_uuid":"521e3f116293db75b12efb8f98af4309758edd76","collapsed":true},"cell_type":"code","source":"display(test_raw.head())","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"533e1723f6d6ba1a905c489abb6eb250b1b7805d"},"cell_type":"markdown","source":"# **2.3** Sample\n\nThe difference between head & sample is that sample will show you a random quantity. while head takes the first defaulted 5 rows of data."},{"metadata":{"_uuid":"0fbff4cb2a633303e7957ccfbe580683521b44f9"},"cell_type":"markdown","source":"*** Train set**"},{"metadata":{"trusted":true,"_uuid":"99706247356b1187555f5769a56b7cbe327fdcb1","collapsed":true},"cell_type":"code","source":"SampleN = 10\ndisplay(train_raw_copy.sample(SampleN))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"94781bbfef6fb060a1ed2e07f6fb0f1dc8cf46cc"},"cell_type":"markdown","source":"*** Test set**"},{"metadata":{"trusted":true,"_uuid":"a2a2987a01c4816ed13c89b65785d4884772fe2b","collapsed":true},"cell_type":"code","source":"SampleN = 10\ndisplay(test_raw.sample(SampleN))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"9899f54a1dcfe7302895234831fe51551cf6fbbb"},"cell_type":"markdown","source":"# **2.4** Brief Info\n\nNote that I have hid the outputs here as my function at **2.6** is a better inbuilt replacement function instead."},{"metadata":{"_uuid":"e19e7ed4c05ce755f49e043324210aee7fc1126b"},"cell_type":"markdown","source":"*** Train set** *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"3148d666f4c54a99e9a431b9f06547a0f09d288c","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"print(train_raw_copy.info())","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"9224d63cb0589b109aea3b75c36c07fd4d7cead0"},"cell_type":"markdown","source":" * ** Test set ** *Unhide to view output"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"eec1f0ff438a66ad38159745d00e7d1380b34cde","collapsed":true},"cell_type":"code","source":"print(test_raw.info())","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"c8cb68352a5148001fa1b93f63aed067242ac88a"},"cell_type":"markdown","source":"# **2.5** Describe (Max Min Quartiles)"},{"metadata":{"_uuid":"97a2b14075b3ea4902599020b9391bfe5c7a02ec"},"cell_type":"markdown","source":"* ** Train set**"},{"metadata":{"trusted":true,"_uuid":"df88d1488b53dc70e7ced06bd36d891f29dd9615","collapsed":true},"cell_type":"code","source":"display(train_raw_copy.describe())","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"441edb1bee6df1a2909db188d79bc4b8fb7b8b56"},"cell_type":"markdown","source":"* ** Test set**"},{"metadata":{"trusted":true,"_uuid":"69b97c6d32edd1d450ede842364dd861478525ec","collapsed":true},"cell_type":"code","source":"display(test_raw.describe())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"32e49760feab62708331bf86662c53b9b7230dbf"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"5cd133a1b8de22dba6120f748092e1f566ee015d"},"cell_type":"markdown","source":"# **2.6** Own Quick-Stats Check Function\n\nNow for a further stats on the actual components of each respective feature.\n\nGoing forward, I will always use this function as a \"Check\" measure everytime I conduct cleaning/feature engineer on the dataset."},{"metadata":{"trusted":true,"_uuid":"c1c43d270c5df8b0c1b6916c313cc20a21cd6e9c","collapsed":true},"cell_type":"code","source":"\"\"\"Quick Stats: Data Types\"\"\"\n# Function to output missing values & UniqueCounts & DataTypes\ndef basic_details(df):\n    details = pd.DataFrame()\n    details['Missing value'] = df.isnull().sum()\n    details['N unique value'] = df.nunique()\n    details['dtype'] = df.dtypes\n    display(details)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"7aa7242a8f633ae92ed9c00cacd572dbe43f5dea"},"cell_type":"markdown","source":"* ** Train set**"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"829d451d12772864917378c2834f26bdab49e349","collapsed":true},"cell_type":"code","source":"basic_details(train_raw_copy)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"345bd338cf4e874734e61331890b6b30526e8774"},"cell_type":"markdown","source":"* ** Test set**"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"c7f90fb3626549659aa4a20413acd647ed96c3f7","collapsed":true},"cell_type":"code","source":"basic_details(test_raw)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"39efbf8b97d534226c501b1f2212d7817897b2fa"},"cell_type":"markdown","source":"Quick Commentary:\n\n**GREAT!!!** Fortunately, we only need to clean up 3x features (age & MonthlyIncome & NumberOfDependents)\n\n**Reasoning:**\n\nAge: Not sensible to have a Minimum age of 0.\n\nMonthlyIncome: 20103 Missing values\n\nNumberOfDependents: 2626 Missing values\n"},{"metadata":{"_uuid":"9d7fd13afedc1dda4b6ea90caa4ad793f570b344"},"cell_type":"markdown","source":"# **3.Preliminary Data Cleaning**\n\nNow we know what data-set â€˜â€™Hasâ€™â€™ and â€˜â€™Lacksâ€™â€™, we proceed to tidy these imperfections.. Here we will mainly execute C1-Correct & C2-Complete.\n\n**C1-Correct**\n\nWhere we will correct the dataset by removing any uneccesary segments of the data. The objective is to remove unnecessary noise (prediction errors) and computational costs (run time of code sequence) to the analysis and modelling process as we proceed.\n\n**C2-Complete**\n\nWhere we will complete the dataset by filling up any missing values. The most common approach is to replace it with either the mode/mean/median.\n\nI will explain further as we proceed on..."},{"metadata":{"_uuid":"d46a39fdea87e0da55b443b3ba438ca77849f5ab"},"cell_type":"markdown","source":"# 3.1 Age\n\nI've pasted the basic statistics found. Reason for this is that we will try our best to align any \"Data Cleaning\" approaches to the current age dispersion. Thereby not disrupting the natural state (distribution) of the age range in the dataset.\n\nI've assumed the legal age of 18 since this is in a financial context. In addition, since it was only 1 case I've decided to replace that single 0 age occurance with the median.\n\nSubsequently, \"brute-forcing\" by converting the datatype into integer to reduce computational cost & charting use later on. "},{"metadata":{"_uuid":"09e8907d76b14dfa909cab21423d44a0ebe15884"},"cell_type":"markdown","source":"* ** Check those below threshold**"},{"metadata":{"trusted":true,"_uuid":"7875a2e704ffd8a28d3820565d76a4107a2a2a53","collapsed":true},"cell_type":"code","source":"\"\"\"Age\"\"\"\n# Reason due to Age having 0 as minimum which is not sensible\n# Decided to input Median\n\n# From above info we found these Age Stats:\n# Train                 Test\n# Mean - 52.30          Mean - 52.41\n# Max - 109             Max - 104\n# 75% - 63              75% - 63\n# 50% - 52              50% - 52\n# 25% - 41              25% - 41\n# Min - 0               Min - 21\n\n# Find those less than assumed legal age 18yrs old\nLegalAge = 18\nLessLegalAgeCount = len(train_raw_copy.loc[train_raw_copy[\"age\"] < 18, \"age\"])\nprint(\"Total number of less than assumed legal age {} is {}\".format(LegalAge, LessLegalAgeCount))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"659f6f295c128e5b9be4d24e3b38eabc46782f80"},"cell_type":"markdown","source":"Ok we only have 1 . Now lets replace it & ensure it aligns to the correct computational data-type"},{"metadata":{"_uuid":"29ebda22fbada468ae4cc8933a7e5a956e3c89ce"},"cell_type":"markdown","source":"* ** Replace**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a435543214de0d14d9cff6142dd70b65db5f2aa0"},"cell_type":"code","source":"# Replace with Median\ntrain_raw_copy.loc[train_raw_copy[\"age\"] == 0, \"age\"] = train_raw_copy.age.median()\n\n# Convert data-type\ntrain_raw_copy[\"age\"] = train_raw_copy[\"age\"].astype('int64')","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"57e86a498bcaa1242d881dc484547ceadc8c4937"},"cell_type":"markdown","source":"* ** Check again**"},{"metadata":{"_uuid":"977b40c74fe8077429584ed39280f199c1d4f587"},"cell_type":"markdown","source":"Now lets check it again"},{"metadata":{"trusted":true,"_uuid":"b309b0e653e871728693d28f2cfab23cdd353528","collapsed":true},"cell_type":"code","source":"\"\"\"Check Again\"\"\"\nprint(train_raw_copy[\"age\"].describe())","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"918a55c16f5bb07df1cdc7b3a5541770a56bb8aa"},"cell_type":"markdown","source":"# 3.2 **Monthly Income**\n\nSince we are in a financial context, realistically speaking income does vary proportionately along with age groups. Unless you are MarkZuckerberg or BillGates...\n\nBased on the basic stats derived earlier during the Preliminary Analysis of each Quartile 25% 50% 75% (**2.5 Describe (Max Min Quartiles)**), I've decided on 3x categories to hopefully not disrupt the age dispersion too much. In addition, having been in a pensions consultancy placement at the moment actuaries often assume retirement ages of between 60 to 65.\n\nAge_Range_1: Between ages 18 inclusive to 41 excluding (Working)\nAge_Range_2: Between ages 41 inclusive to 63 excluding (Senior Working)\nAge_Range_3: Above 63 including (Retired)"},{"metadata":{"_uuid":"39abf43788f870574f4f1cff15f231fe600527ff"},"cell_type":"markdown","source":"* ** We first derive the respective range mean incomes as mentioned above**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"34173f4280a30fd4b553daf17b4b12c59b244b83"},"cell_type":"code","source":"# Reason due to Monthly Income having 29731 Missing Values\n# Decided to input Median per quartile Age range\n\n# Determine/Set rows that fulfil these quartile range conditions\nAge_Range_1 = train_raw_copy.loc[(train_raw_copy[\"age\"] >= 18) & (train_raw_copy[\"age\"] < 41)]\nAge_Range_2 = train_raw_copy.loc[(train_raw_copy[\"age\"] >= 41) & (train_raw_copy[\"age\"] < 63)]\nAge_Range_3 = train_raw_copy.loc[(train_raw_copy[\"age\"] >= 63)]\n\n# Per Determine/Set rows, Find that range mean MonthlyIncome\nAge_R1_MonthlyIncome_impute = Age_Range_1.MonthlyIncome.mean()\nAge_R2_MonthlyIncome_impute = Age_Range_2.MonthlyIncome.mean()\nAge_R3_MonthlyIncome_impute = Age_Range_3.MonthlyIncome.mean()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"b9dcbb09dce998a9c07ff106a2b673bbe997e534"},"cell_type":"markdown","source":"* ** Now to fill them**"},{"metadata":{"trusted":true,"_uuid":"79e8f2f42d85d2b709bf0cdea167cc3622644603","collapsed":true},"cell_type":"code","source":"# Fill Missing MonthlyIncome with 99999 for easy reference\ntrain_raw_copy[\"MonthlyIncome\"] = train_raw_copy[\"MonthlyIncome\"].fillna(99999)\n# Convert into integer dtype\ntrain_raw_copy[\"MonthlyIncome\"] = train_raw_copy[\"MonthlyIncome\"].astype('int64')\n\n# Now to fill them\ntrain_raw_copy.loc[(train_raw_copy[\"age\"] >= 18) & (train_raw_copy[\"age\"] < 41) & (train_raw_copy[\"age\"] == 99999)] = Age_R1_MonthlyIncome_impute\n\ntrain_raw_copy.loc[(train_raw_copy[\"age\"] >= 41) & (train_raw_copy[\"age\"] < 63) & (train_raw_copy[\"age\"] == 99999)] = Age_R2_MonthlyIncome_impute\n\ntrain_raw_copy.loc[(train_raw_copy[\"age\"] >= 63) & (train_raw_copy[\"age\"] == 99999)] = Age_R3_MonthlyIncome_impute","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"433f0a3d9df0c1be32a0cbc75e8a357b01d4a5bf"},"cell_type":"markdown","source":"* **Check again**"},{"metadata":{"trusted":true,"_uuid":"d3f1decbe592fba8515b86e98200e8dc95d29781","collapsed":true},"cell_type":"code","source":"\"\"\"Check Again\"\"\"\nbasic_details(train_raw_copy)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"64bc910cc42ac5cf53dd64c46cb8fe4dcb18a473"},"cell_type":"markdown","source":"# 3.3 **Number of Dependents**\n\n"},{"metadata":{"_uuid":"c3b247910fb0ff598850edd9e7131cde75e140cf"},"cell_type":"markdown","source":"This part is pretty straightforward...As intuitive as it is I've decided to input 0, since it makes sense to not have any spouse or family"},{"metadata":{"_uuid":"6767f1956dca33ad97d28745ea9039b3522f41c8"},"cell_type":"markdown","source":"* **Fill**"},{"metadata":{"trusted":true,"_uuid":"b43791348ca6dead6110971bf81432d1c18270ef","collapsed":true},"cell_type":"code","source":"# Reason due to NumberOfDependents having 3924 Missing Values\n\n# Fill missing with zero's\ntrain_raw_copy[\"NumberOfDependents\"] = train_raw_copy[\"NumberOfDependents\"].fillna(0)\n# Convert into integer dtype\ntrain_raw_copy[\"NumberOfDependents\"] = train_raw_copy[\"NumberOfDependents\"].astype('int64')\n\n# Check counts per category of 'NumberOfDependents'\n#print(train_raw_copy.NumberOfDependents.value_counts())","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"0fef8934cb50fe27ba2329256572caec14310042"},"cell_type":"markdown","source":"* **Check again**"},{"metadata":{"trusted":true,"_uuid":"48dd96be442f641db5374b11f05ab281455840d0","collapsed":true},"cell_type":"code","source":"\"\"\"Check Again\"\"\"\nbasic_details(train_raw_copy)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"6fd0090f62557b364e2475a959d4aeee531f75ff"},"cell_type":"markdown","source":"**Data Cleaning Done!!!**"},{"metadata":{"_uuid":"69b4542d70f3a729602b8e6f310c08d3bde0c66c"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"b730078ef7ac58c5d0969f758f2a0bf32f6d275c"},"cell_type":"markdown","source":"# **4.Brief EDA**"},{"metadata":{"_uuid":"179b9606313f162cb1581d73e54c17271949cf4e"},"cell_type":"markdown","source":"Now to get a quick EDA on the dataset before we perform further 'Data Cleaning'"},{"metadata":{"_uuid":"5f08bf44f054b43eab77869bce87d57e44c2d64a"},"cell_type":"markdown","source":"# 4.1 Target Variable\n\n**SeriousDlqin2yrs (i.e.Target Variable)**\nLets first see the balance of the dataset"},{"metadata":{"_uuid":"628a46842d5349ec9c82a5fa84ddcb1cdfadfe1c"},"cell_type":"markdown","source":"* **Proportions**"},{"metadata":{"trusted":true,"_uuid":"457910ee96a977d44d6d5d33aca245a52ea5b13b","collapsed":true},"cell_type":"code","source":"\"\"\"SeriousDlqin2yrs (i.e.Target Variable)\"\"\"\nprint(\"Exploring SeriousDlqin2yrs (i.e.Target Variable)...\")\n\n# List Comprehension\nclass_0 = [c for c in train_raw_copy['SeriousDlqin2yrs'] if c == 0]\nclass_1 = [c for c in train_raw_copy['SeriousDlqin2yrs'] if c == 1]\n# # Alternative Mask Method\n# class_0 = train_raw_copy.SeriousDlqin2yrs.value_counts()[0]\n# class_1 = train_raw_copy.SeriousDlqin2yrs.value_counts()[1]\n\nclass_0_count = len(class_0)\nclass_1_count = len(class_1)\n\nprint(\"Target Variable Balance...\")\nprint(\"Total number of class_0: {}\".format(class_0_count))\nprint(\"Total number of class_1: {}\".format(class_1_count))\nprint(\"Occurance event rate: {} %\".format(round(class_1_count/(class_0_count+class_1_count) * 100, 3)))   # round 3.dp","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"f56600488253302208664e466f0419e3283cdf49"},"cell_type":"markdown","source":"* **Plotting**"},{"metadata":{"trusted":true,"_uuid":"7d143baf53e0a4b2304d0cf5aca59d9d576196de","collapsed":true},"cell_type":"code","source":"# Plot\nsns.countplot(\"SeriousDlqin2yrs\", data=train_raw_copy)\nplt.show()","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"918343b6edb9239ab75906e031c24ce76b2f661b"},"cell_type":"markdown","source":"Right...we have a unbalanced dataset here. In short, since we have a higher proportion of the dataset not having experienced financial distress (SeriousDlqin2yrs=0) our model may overfit easily to this rather than accurately predicting those having experienced financial distress. Of which is our model objective....Consequently, this would also likely cause an inaccurate portray of the evaluation metrics itself (ROC AUC)...\n\nAt the mean time, we will just proceed with this until we see the actual model accuracy results. Do note that I will also be doing a subsequent Kernel focusing more on \"Data Cleaning\" to curb issues similar to this. This is my 2nd Kernel and I'm still learning! So apologies in advance!"},{"metadata":{"_uuid":"1867e312c786866fe8b90758f33770e1d918e77b"},"cell_type":"markdown","source":"# 4.2 Other Features \n\n* **HeatMap**\n\nWe will now use the HeatMap to see which features we can discard or merge.\n\nThe **Objective** here is to remove those features bearing high correlations (Remove Multi-Colinearity which causes over-fitting) & engineer more features to optimize the predictive model"},{"metadata":{"trusted":true,"_uuid":"283723b923e02ed1407807805ede5e8735c498f2","collapsed":true},"cell_type":"code","source":"\"\"\"Correlation Heat-Map\"\"\"\ncor = train_raw_copy.corr()\nplt.figure(figsize=(10, 8))\n# sns.set(font_scale=0.7)\nsns.heatmap(cor, annot=True, cmap='YlGn')\nplt.show()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"e7a28e8c22b1c49b738905d5bcc4449b98cf1fb8"},"cell_type":"markdown","source":"# 4.3 Notes for Feature Engineering\n\n* **Group_A**: Bears high inter-correlations (0.98)\n\nNumberOfTime30-59DaysPastDueNotWorse,  \n\nNumberOfTimes90DaysLate, \n\nNumberOfTime60-89DaysPastDueNotWorse\n\n* **Group_B**: Suprisingly low correaltion to Target  Variable  (-0.026)\n\nMonthlyIncome\n\n* **Group_C**: Bears high inter-correlations (0.43)\n\nNumberOfOpenCreditLinesAndLoans, \n\nNumberRealEstateLoansOrLines"},{"metadata":{"_uuid":"f4ebc9faa5b96c4edb0d7dabec9935f3b0f759ed"},"cell_type":"markdown","source":"# **5.Deeper Data Cleaning/Feature Engineering [C3-Create, C4-Convert]**\n\nNow with a good reference point we can further execute feature engineering"},{"metadata":{"_uuid":"a8bd3feba81864bb052851022b6e89744adfbc0c"},"cell_type":"markdown","source":"# 5.1 Group_A\n\n* **Quantity of Defaults**\n\nNumberOfTime30-59DaysPastDueNotWorse, \n\nNumberOfTimes90DaysLate, \n\nNumberOfTime60-89DaysPastDueNotWorse\n\nEvidently, we can see they all bear high correlations 0.98. We will try to merge them to avoid collinearity which may overfit the model by distorting or giving higher and incorrect feature weightings.\n\n* **General Idea**\n\nCreate a new dataframe column of a Binary statistical datatype indicating if a default was made before or not. (1/0) (Yes/No)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d89c0ab6007a0078663ff7b01f94b928deefe2fd"},"cell_type":"code","source":"\"\"\"\nClean Data - 3 - Creating\n\"\"\"\n\n\"\"\"Joining No# Times past due: CombinedDefault\"\"\"\n# Reason huge strong correlations i.e.0.98 & 0.99\n# Decided to Sum all & Change into Binary Feature (1/0) (Y/0)\n# Create Dummy Reference df\ntrain_raw_copy['CD'] = (train_raw_copy['NumberOfTime30-59DaysPastDueNotWorse']\n                                     + train_raw_copy['NumberOfTimes90DaysLate']\n                                     + train_raw_copy['NumberOfTime60-89DaysPastDueNotWorse'])\n\n# Set '1' for those more than zero indicating Yes there was a default before\ntrain_raw_copy['CombinedDefault'] = 1\ntrain_raw_copy.loc[(train_raw_copy['CD'] == 0), 'CombinedDefault'] = 0\n# Remove Dummy Reference df\ndel train_raw_copy['CD']","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"2a0f57da5bb82e0cacd0be38e9d4d69c2be423c9"},"cell_type":"markdown","source":"# 5.2 Group_B (New Feature Net Worth)\n\n* **NEW FEATURE: Net Worth**\n\nMonthly Income seems to have little correlation (-0.026). However, intutively speaking this should have a strong correlation to financial distress.\n\n* **General Idea**\n\nWe will now use a very generic networth formula. In short, incorporate Age & Monthly Income to derive Net Worth\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a8f9b0cef5ac2989051d0bdebce79b82de1e71a"},"cell_type":"code","source":"\"\"\"New Feature: Net Worth\"\"\"\n# Decided on general formula NetWorth = (MonthlyIncome x Age) / 10\n# https://www.bogleheads.org/forum/viewtopic.php?t=195357\nNetWorthDivisor = 10\ntrain_raw_copy['NetWorth'] = train_raw_copy['MonthlyIncome'] * train_raw_copy['age'] / NetWorthDivisor","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"5ac2b248c6021444926757077351acd0b400b921"},"cell_type":"markdown","source":"# 5.3 Group_C\n\n* **Quantity of Loans made**\n\nNumberOfOpenCreditLinesAndLoans, \n\nNumberRealEstateLoansOrLines\n\nSimilar to quantity of defaults, we can see they too high correlations 0.43. We will try to merge them now.\n\n* **General Idea**\n\nSame as **Quantity of Defaults** we will do a Binary Yes or No.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d43bfb8acc395dd3ff253334d92242544a07c673"},"cell_type":"code","source":"\"\"\"Join No# Loans: CombinedLoan\"\"\"\n# Reason huge strong correlations i.e.0.43\n# Decided to Sum all & WITH BUFFER of 5times\nLoanLinesBuffer = 5\n# Create Dummy Reference df\ntrain_raw_copy['CL'] = train_raw_copy['NumberOfOpenCreditLinesAndLoans'] + train_raw_copy['NumberRealEstateLoansOrLines']\n\ntrain_raw_copy['CombinedLoan'] = 1\ntrain_raw_copy.loc[train_raw_copy['CL'] >= LoanLinesBuffer, 'CombinedLoan'] = 1\ntrain_raw_copy.loc[train_raw_copy['CL'] < LoanLinesBuffer, 'CombinedLoan'] = 0\n# Remove Dummy Reference df\ndel train_raw_copy['CL']","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"7f4f14d85ee3f74cced8055db9d8d9c432e17ce6"},"cell_type":"markdown","source":"# 5.4 New Feature (Debt Payments)\n\n* **NEW FEATURE: Debt Payments**\n\nSince we are given components of the ratio, why not? It's always better to do extra than not!\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"65457b104bbe0643cb4ac7f81c71d894720ba091"},"cell_type":"code","source":"\"\"\"New Feature: Monthly debt payments\"\"\"\n# Derivative formula MonthlyDebtPayments = (DebtRatio) x (MonthlyIncome)\ntrain_raw_copy['MonthlyDebtPayments'] = train_raw_copy['DebtRatio'] * train_raw_copy['MonthlyIncome']\ntrain_raw_copy['MonthlyDebtPayments'] = train_raw_copy['MonthlyDebtPayments'].astype('int64')","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"637602a08cadddc60db99fc29c746335b0d3231d"},"cell_type":"markdown","source":"# 5.5 New Feature (Age Category)\n\n* **NEW FEATURE: Age Category**\n\nSince we used this approach to execute C2-Complete missing MonthlyIncome values, we shall continue and now include this categorical feature in.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5ac0c1b931bd520f4ece6476f7739c009b3a95d"},"cell_type":"code","source":"\"\"\"New Feature: Age Category\"\"\"\ntrain_raw_copy[\"Age_Map\"] = train_raw_copy[\"age\"]\ntrain_raw_copy.loc[(train_raw_copy[\"age\"] >= 18) & (train_raw_copy[\"age\"] < 41), \"Age_Map\"] = 1\ntrain_raw_copy.loc[(train_raw_copy[\"age\"] >= 41) & (train_raw_copy[\"age\"] < 63), \"Age_Map\"] = 2\ntrain_raw_copy.loc[(train_raw_copy[\"age\"] >= 63), \"Age_Map\"] = 3\n\n# replacing those numbers to categorical features then get the dummy variables\ntrain_raw_copy[\"Age_Map\"] = train_raw_copy[\"Age_Map\"].replace(1, \"Working\")\ntrain_raw_copy[\"Age_Map\"] = train_raw_copy[\"Age_Map\"].replace(2, \"Senior\")\ntrain_raw_copy[\"Age_Map\"] = train_raw_copy[\"Age_Map\"].replace(3, \"Retired\")\n\ntrain_raw_copy = pd.concat([train_raw_copy, pd.get_dummies(train_raw_copy.Age_Map, prefix='is')], axis=1)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"00fc84671dda878d5928890c037042692dc47a97"},"cell_type":"markdown","source":"# 5.6 New Feature (SES Category)\n\n* **NEW FEATURE: Age Category**\n\nReason being we will use this to gain a deeper insight during EDA. \n\nRather than just knowing the general trend of age, we also want to know which category of age bears the trend too. In other words, the trend for each sub-SES category\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b079f949461161233a97989efa25cf2124141d8f"},"cell_type":"code","source":"\"\"\"New Feature: Income Category\"\"\"\ntrain_raw_copy[\"Income_Map\"] = train_raw_copy[\"MonthlyIncome\"]\ntrain_raw_copy.loc[(train_raw_copy[\"MonthlyIncome\"] <= 3400), \"Income_Map\"] = 1\ntrain_raw_copy.loc[(train_raw_copy[\"MonthlyIncome\"] > 3400) & (train_raw_copy[\"MonthlyIncome\"] <= 8200), \"Income_Map\"] = 2\ntrain_raw_copy.loc[(train_raw_copy[\"MonthlyIncome\"] > 8200), \"Income_Map\"] = 3\n\n# replacing those numbers to categorical features then get the dummy variables\ntrain_raw_copy[\"Income_Map\"] = train_raw_copy[\"Income_Map\"].replace(1, \"LowY\")\ntrain_raw_copy[\"Income_Map\"] = train_raw_copy[\"Income_Map\"].replace(2, \"MidY\")\ntrain_raw_copy[\"Income_Map\"] = train_raw_copy[\"Income_Map\"].replace(3, \"HighY\")\n\ntrain_raw_copy = pd.concat([train_raw_copy, pd.get_dummies(train_raw_copy.Income_Map, prefix='is')], axis=1)\n","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"fdbaad10370c6888bb54fea49ded42e4a4b51fea"},"cell_type":"markdown","source":"# 5.7 **Checks again before Charting for EDA**\n\nFinal checks before doing any deeper EDA!"},{"metadata":{"_uuid":"b56c41c1a734d9de10fdb22a0b42f02e35f3880b"},"cell_type":"markdown","source":"* Checks"},{"metadata":{"trusted":true,"_uuid":"f5c5ca9773347408466edbfac1e0e89607c12793","collapsed":true},"cell_type":"code","source":"\"\"\"Check Again\"\"\"\nbasic_details(train_raw_copy)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"dc3cec0d7bb6c97142f0b94dd83ef871499e6f13"},"cell_type":"markdown","source":"* Just a raw truncated peek into what we have engineered thus far"},{"metadata":{"trusted":true,"_uuid":"1dcd5592932a52c174a65e2bd02b6104c09aae44","collapsed":true},"cell_type":"code","source":"SampleN = 15\ndisplay(train_raw_copy.sample(SampleN))","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"478739b3dd213ab5803e8ef80b5af7af7187a5e9"},"cell_type":"markdown","source":"# **6.Deeper EDA**\n\nNow with a cleaned up dataset we can start doing EDA for gain a deep insight into what data we are modelling. \n\nSpecifically, EDA helps us to get a clearer idea of what relationships or abnormal relationships we have from the features. These may include outliers, a skewed data, reasonableness checks, feature selection etc."},{"metadata":{"_uuid":"64a494437b4673c988eb87d62e4baaaeff81bbb2"},"cell_type":"markdown","source":"# 6.1 **Quick Data Prep**\n\nLets set our variables for easy referencing! *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"93ad22971301d0373c00d5c2584f213142ce5c95","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nPrepare Data for Charting\n\"\"\"\nFeatures_Preselect_All = train_raw_copy.columns\nFeatures_Preselect_Original = ['SeriousDlqin2yrs', 'RevolvingUtilizationOfUnsecuredLines', 'age',\n                               'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n                               'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n                               'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n                               'NumberOfDependents']\nFeatures_Preselect_Engineered = ['CombinedDefault', 'NetWorth', 'CombinedLoan', 'MonthlyDebtPayments', 'is_Retired',\n                                 'is_Senior', 'is_Working', 'is_HighY', 'is_LowY', 'is_MidY']\n\n# Binary_bin\nBinary = ['CombinedLoan', 'Age_Map', 'Income_Map']\nprint(\"Binary\", \"\\n\", Binary)\n\n# Integer_'int'_Ordinal\nInteger = ['age', 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n           'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents',\n           'CombinedDefault']\nprint(\"Integer_Ordinal\", \"\\n\", Integer)\n\n# Real_'float'_Interval\nReal = ['DebtRatio', 'MonthlyIncome', 'NetWorth', 'MonthlyDebtPayments']\nprint(\"Real_float\", \"\\n\", Real)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"aeca59e9d0b529023cf77fb559ed27c427135d4e"},"cell_type":"markdown","source":"**Now for the actual EDA**\n\nI will deal in the order of statistical data types by Binary, Integer, Float."},{"metadata":{"_uuid":"44c0c5d1101af0d151f5fe8d517268c8a972e728"},"cell_type":"markdown","source":"# 6.2  **UNI-VARITE**\n"},{"metadata":{"_uuid":"9857c3b071cf80e8fa6411a4a88139136305f042"},"cell_type":"markdown","source":"* **BINARY**"},{"metadata":{"trusted":true,"_uuid":"13b37f271ff820661ed905136ce5c32b3f0c3ebc","collapsed":true},"cell_type":"code","source":"\"\"\"Binary\"\"\"\nprint(\"Plotting Bar Plot...for Binary\")\n# https://stackoverflow.com/questions/35692781/python-plotting-percentage-in-seaborn-bar-plot\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in Binary:\n    sns.barplot(x=train_raw_copy[col].value_counts().index, y=train_raw_copy[col].value_counts())\n    plt.show()","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"6798e595940f86b1dfe25169a0f6b0d3d29554ab"},"cell_type":"markdown","source":"**Brief Observations**\n\n-'CombinedLoan': Very lob-sided where 3/4 have made loans\n\n-'Age_Map': Data-set contains mostly ages 41 to 63\n\n-'Income_Map': Data-set dominated by High & Mid income earners"},{"metadata":{"_uuid":"9faf17f3fe3b3aecf2fa9bdd234105c937a923fb"},"cell_type":"markdown","source":"* **Integer**\n"},{"metadata":{"trusted":true,"_uuid":"4a229e5eb7839cca13c6560cf036881cbcca24d6","collapsed":true},"cell_type":"code","source":"\"\"\"Integer\"\"\"\nprint(\"Plotting Density Plot...for Integer\")\n# Used as opposed to histogram since this doesnt need bins parameter\ni = 0\nt1 = train_raw_copy.loc[train_raw_copy['SeriousDlqin2yrs'] != 0]\nt0 = train_raw_copy.loc[train_raw_copy['SeriousDlqin2yrs'] == 0]\n\nsns.set_style('whitegrid')\n# plt.figure()\nfig, ax = plt.subplots(2, 4, figsize=(15, 10))\n\nfor feature in Integer:\n    i += 1\n    plt.subplot(2, 4, i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"SeriousDlqin2yrs = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"SeriousDlqin2yrs = 0\")\n    plt.ylabel('Density plot', fontsize=10)\n    plt.xlabel(feature, fontsize=10)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=10)\nplt.show()","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"f80bec5e7098a0b219367f5f1c7371507166669a"},"cell_type":"markdown","source":"**Brief Observations**\n\n-'age': Same trend as seen from Age_Map in Binary EDA. Majority of data-set are between ages 41 to 63\n\n-'NumberOfTime30-59DaysPastDueNotWorse': Very High Kurtosis & right-skewed\n\n-'NumberOfOpenCreditLinesAndLoans': High Kurtosis & Right-skewed\n\n-'NumberOfTimes90DaysLate': Very High Kurtosis & Very Right-skewed\n\n-'NumberRealEstateLoansOrLines': High Kurtosis & Right-skewed\n\n-'NumberOfTime60-89DaysPastDueNotWorse': Very High Kurtosis & Very Right-skewed\n\n-'NumberOfDependents': High Kurtosis & Right-skewed\n\n-'CombinedDefault': Gaussian distribution shape\n\nReasonable data-set as those who experience financial distress (SeriousDlqin2yrs=1) have left skewed (Median>Mean).\n\nIn other words, data population who experience financial distress have a greater proportion of defaults\n\n**We will switch to a scatter plot for a better visual since, most of the charts have very high kurtosis due to outliers**"},{"metadata":{"trusted":true,"_uuid":"3e6a9ba002b883dfdd8856a32026bdf44cd5caad","collapsed":true},"cell_type":"code","source":"print(\"Plotting Scatter Plot...for Integer\")\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in Integer:\n    sns.lmplot(y=col, x=\"Unnamed: 0\", data=train_raw_copy, fit_reg=False, hue='SeriousDlqin2yrs', legend=True,\n               size=5, aspect=1)\n    plt.show()","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"6bef6b84131fb7966f923d487526e6ceec8c49d9"},"cell_type":"markdown","source":"**Integer Scatter Plot For a Clearer Visual**\n\n**Brief Observations (Improved)**\n\n-'age': Same pattern seen. Concentrated on ages 41 to 63\n\n-'NumberOfTime30-59DaysPastDueNotWorse': Interesting disparity! In other words, we have extreme frequency's.\n\nIt's a either or case of Borrowers exceeding the 30-59days\n\n1. very often above 90 times\n\n2. very few times below 15 times\n\n-'NumberOfOpenCreditLinesAndLoans': Evidently, those who have had financial distress (SeriousDlqin2yrs=1) have lower Loans given their poor credit history\n\n-'NumberOfTimes90DaysLate''NumberRealEstateLoansOrLines': Same pattern as 'NumberOfTime30-59DaysPastDueNotWorse'\n\n-'NumberOfTime60-89DaysPastDueNotWorse': Same pattern as 'NumberOfTime30-59DaysPastDueNotWorse'\n\n-'NumberOfDependents': Interesting, those who have had financial distress (SeriousDlqin2yrs=1) tend to have lesser dependents than those who have had financial distress\n\n-'CombinedDefault': n/a"},{"metadata":{"_uuid":"4061f498ad270efbc597f1776cf55c51de730d3a"},"cell_type":"markdown","source":"* **Real**\n\n**Similarly, we will switch to a scatter plot**"},{"metadata":{"trusted":true,"_uuid":"2fcdfcc676383e0abbfcd00b3b08ce8e9c3189ac","collapsed":true},"cell_type":"code","source":"\"\"\"Real\"\"\"\nprint(\"Plotting Density Plot...for Binary\")\n# Used as opposed to histogram since this doesnt need bins parameter\ni = 0\nt1 = train_raw_copy.loc[train_raw_copy['SeriousDlqin2yrs'] != 0]\nt0 = train_raw_copy.loc[train_raw_copy['SeriousDlqin2yrs'] == 0]\n\nsns.set_style('whitegrid')\nfig, ax = plt.subplots(2, 2, figsize=(8, 6))\n\nfor feature in Real:\n    i += 1\n    plt.subplot(2, 2, i)\n    sns.kdeplot(t1[feature], bw=0.5, label=\"SeriousDlqin2yrs = 1\")\n    sns.kdeplot(t0[feature], bw=0.5, label=\"SeriousDlqin2yrs = 0\")\n    plt.ylabel('Density plot', fontsize=10)\n    plt.xlabel(feature, fontsize=10)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=10)\nplt.show()","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"d75dac59e5ae605fa971c8288f2dcabbd3c71040"},"cell_type":"markdown","source":"All very high Kurtosis & Right_skewed. As before, We will switch to a scatter plot for a better visual"},{"metadata":{"trusted":true,"_uuid":"e7723479c89e1e3519aac0f44aa02e1d42ae8140","collapsed":true},"cell_type":"code","source":"print(\"Plotting Scatter Plot...for Real\")\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in Real:\n    sns.lmplot(y=col, x=\"Unnamed: 0\", data=train_raw_copy, fit_reg=False, hue='SeriousDlqin2yrs', legend=True,\n               size=5, aspect=1)\n    plt.show()","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"e14a36f69e8500a43760d32ca89708b6eb78ef4c"},"cell_type":"markdown","source":"**Real Scatter Plot For a Clearer Visual**\n\n**Brief Observations (Improved)**\n\n-'DebtRatio': Contrastingly, those who have had financial distress (SeriousDlqin2yrs=1) posses lower DebtRatio\n\n-'MonthlyIncome': Interesting! Data-set actually has a huge income disparity for those who have had financial distress (SeriousDlqin2yrs=1).\n\nEvidently, from our Preliminary Overview it has a StandardDeviation of 3.650860e+04. \n\nDisparity roughly is between the Less than 33,000 & capping at =100,000. \n\n(SeriousDlqin2yrs=0) instead is more evened out.\n\nNOTE: Further emphasizes need to implement under sampling to even out the data sample\n\n-'NetWorth': Reasonable sense, since those who have had financial distress (SeriousDlqin2yrs=1) have lower net worth\n\n-'MonthlyDebtPayments': Similar to the pattern found in 'DebtRatio' those who have had financial distress (SeriousDlqin2yrs=1) are paying out lesser existing debts\n"},{"metadata":{"_uuid":"f564f71f72da5aa705b67829a7f01031b287387d"},"cell_type":"markdown","source":"# 6.3 **BI-VARIATE**"},{"metadata":{"_uuid":"17cf54562babd6a44229711daa8fd0aa95fcd3d6"},"cell_type":"markdown","source":"* **Pair Plots**\n\nNote that I have truncated the sample to only 800 to avoid excessive computational costs."},{"metadata":{"trusted":true,"_uuid":"f9a37cd802bb5d308156f546c8f10e1b6ee11d37","collapsed":true},"cell_type":"code","source":"\"\"\"Pair-plot\"\"\"     \nprint(\"Plotting Pair-Plots\")\nPairPlot = Binary + Integer + Real\n# # sample = train_raw_copy.sample(frac=0.5)\nsample_SIZE = 800\nsample = train_raw_copy.sample(sample_SIZE)\nPairPlot.extend(['SeriousDlqin2yrs'])  # Add 'target' into list\nvar = PairPlot\nsample = sample[var]\ng = sns.pairplot(sample,  hue='SeriousDlqin2yrs', palette='Set1', diag_kind='kde', plot_kws={\"s\": 8})\nplt.show()\nPairPlot.remove('SeriousDlqin2yrs')","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"14e2d9cd54a73eafe089a049e8a550535dcde782"},"cell_type":"markdown","source":"Okay we have too many features to fit this space....\n\nNo matter, let's individually zoom in by manually splitting them up then!"},{"metadata":{"_uuid":"d7d6b60f5b56d93d68c7bb6481d5e54c7661410c"},"cell_type":"markdown","source":"* **Age_Map Centralized**"},{"metadata":{"trusted":true,"_uuid":"5771bda3de10c37657a377e89729fb4c98aa2181","collapsed":true},"cell_type":"code","source":"\"\"\"Age_Map\"\"\"\nprint(\"Plotting Scatter Plot...for Bi-variate Focus_set_1\")\nEDA_BiVariate_1 = ['CombinedLoan',\n                   'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfDependents', 'CombinedDefault',\n                   'DebtRatio', 'MonthlyIncome',  'NetWorth']\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in EDA_BiVariate_1:\n    sns.lmplot(y=col, x='age', data=train_raw_copy, fit_reg=False, hue='Age_Map', legend=True,\n               size=5, aspect=1)\n    plt.show()","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"65bf6312291f3769f9593e961e0d089316a4a228"},"cell_type":"markdown","source":"**Quick Commentary:**\n\n-'CombinedLoan': For both category's of those who made & did not make a loans, they are dominated by 'Retired' by more than 2x\n\n-'NumberOfTime30-59DaysPastDueNotWorse':\n\nGeneric Interpretation>\n\nOn the high extreme end, dominated by 'Working' & 'Senior'.\n\nOn the low extreme end, dominated by 'Retired'\n\nSpecific Interpretation>\n\nOn the high extreme end, of exceeding over 95times they are split evenly between 'Working' & 'Senior' category's. But rarely 'Retired'\n\nOn the low extreme end, of exceeding over but below 15times they are split evenly between between 'Working' & 'Senior' category's. But this time dominated by 'Retired'\n\n-'NumberOfDependents': 'Working' & 'Senior' category's tend to have higher number of dependents. Evidently, mortality means as we grow older we see more deaths..\n\n-'CombinedDefault': Same pattern as 'CombinedLoan'\n\n-'DebtRatio': Evidently, as 'age' increases 'DebtRatio' increases. But begins falling upon retirement at age 63\n\n-'MonthlyIncome': Similar to before, in Uni-variate EDA we spotted a disparity in 'MonthlyIncome'. But this time we can clearly see a gaussian shape appearing. which is also similar to 'DebtRatio' pattern\n\nEvidently, as 'age' increases 'MonthlyIncome' increases. But begins falling upon retirement at age 63\n\n-'NetWorth': This emphasizes the pattern. Evidently, as 'age' increases 'NetWorth' increases. But begins falling upon retirement at age 63.\n\nHOWEVER, the positive gradient also highlights the flaw in our 'NetWorth' Derivation. Since we are essentially assuming constant income growth & ignoring depreciation considerations for the time value of money (i.e.From my placement year in a Pension's Consultancy firm, we need to include annuities which accounts for both interest rates and mortality."},{"metadata":{"_uuid":"6189c93aaebaf35b7d921eb5e978073b80162c7e"},"cell_type":"markdown","source":"* **Income_Map Centralized**"},{"metadata":{"trusted":true,"_uuid":"3474a2b35f574c64f7eab262567075eb14017bc8","collapsed":true},"cell_type":"code","source":"\"\"\"Income_Map\"\"\"\nprint(\"Plotting Scatter Plot...for Bi-variate Focus_set_2\")\nEDA_BiVariate_2 = ['CombinedLoan',\n                   'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfDependents', 'CombinedDefault',\n                   'DebtRatio', 'MonthlyIncome',  'NetWorth']\nsns.set_style(\"whitegrid\")  # Chosen\nfor col in EDA_BiVariate_1:\n    sns.lmplot(y=col, x='MonthlyIncome', data=train_raw_copy, fit_reg=False, hue='Income_Map', legend=True,\n               size=5, aspect=1)\n    plt.show()","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"9ae3852ef3cccaac77b02e201e409693e400824e"},"cell_type":"markdown","source":"**Quick Commentary:**\n\n-'CombinedLoan': Clearly, those who have made & did not make Loans are dominated by the higher tier income earners\n\n-'NumberOfTime30-59DaysPastDueNotWorse': From a relative perspective, those who exceed the 30-59Days deadline are dominated by higher tier income earners\n\n-'NumberOfDependents': The higher the 'MonthlyIncome', the lower the Dependents\n\n-'CombinedDefault': Same pattern as 'CombinedLoan'\n\n-'DebtRatio': The higher the 'MonthlyIncome', the higher the 'DebtRatio'\n\n-'MonthlyIncome': n/a\n\n-'NetWorth': Obvious of higher 'MonthlyIncome' equates to higher 'NetWorth'"},{"metadata":{"_uuid":"b2cb3dd40242ef59b0cebb78d0f45d8d0da52d6a"},"cell_type":"markdown","source":"* **Others_A Centralized**"},{"metadata":{"trusted":true,"_uuid":"a0f8645bcb95eeb777bd832bdc980f973a1a8e65","collapsed":true},"cell_type":"code","source":"\"\"\"Others_A\"\"\"\nprint(\"Plotting Scatter Plot...for Bi-variate Focus_set_3\")\nsns.set_style(\"whitegrid\")  # Chosen\nsns.lmplot(y='DebtRatio', x='MonthlyIncome', data=train_raw_copy, fit_reg=False, hue='SeriousDlqin2yrs', legend=True,\n           size=5, aspect=1)\nplt.show()","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"cbd086407e99156d1d3b2ee8e6078b12fe58c7fd"},"cell_type":"markdown","source":"**Quick Commentary:**\n\nThose who have had financial distress (SeriousDlqin2yrs=1) clearly have lower 'DebtRatio' & 'MonthlyIncome'"},{"metadata":{"_uuid":"9960e090a7f4dae4f11b786966ac6984b820f015"},"cell_type":"markdown","source":"* **Others_B Centralized**\n"},{"metadata":{"trusted":true,"_uuid":"ae63f2a12301e744a9b857b606ceea46e0533e95","collapsed":true},"cell_type":"code","source":"\"\"\"Others_B\"\"\"\nsns.lmplot(y='NumberOfTime30-59DaysPastDueNotWorse', x='MonthlyIncome', data=train_raw_copy, fit_reg=False,\n           hue='SeriousDlqin2yrs', legend=True, size=5, aspect=1)\nplt.show()","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"d86fa07abdc5e792474e9b2d35002ce6685c6d54"},"cell_type":"markdown","source":"**Quick Commentary:**\n\nContrastingly, often those who have had financial distress (SeriousDlqin2yrs=1) have exceeded the 30-59Days deadline only a few times"},{"metadata":{"_uuid":"307edc6eccd27166430a6897215b21e9e6cd4dc0"},"cell_type":"markdown","source":"* **Others_C Centralized**\n\n"},{"metadata":{"trusted":true,"_uuid":"95eb3556fc9ea7bd243ec00e24baa8f1310650a8","collapsed":true},"cell_type":"code","source":"\"\"\"Others_C\"\"\"\nsns.lmplot(y='NumberOfOpenCreditLinesAndLoans', x='MonthlyIncome', data=train_raw_copy, fit_reg=False,\n           hue='SeriousDlqin2yrs', legend=True, size=5, aspect=1)\nplt.show()","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"100c0ec3ee9679755feba4556cb9858e8b70dba6"},"cell_type":"markdown","source":"**Quick Commentary:**\n\nSimilarly, those who have had financial distress (SeriousDlqin2yrs=1) actually open lesser loans than those of without financial distress (SeriousDlqin2yrs=0)"},{"metadata":{"_uuid":"edd873b198dd31a7db3cb713ae3de75a49856766"},"cell_type":"markdown","source":"* **Others_D Centralized**\n"},{"metadata":{"trusted":true,"_uuid":"fd19b8e42a35e1ac69d0203373b2a386051fec27","collapsed":true},"cell_type":"code","source":"\"\"\"Others_D\"\"\"\nsns.lmplot(y='NumberRealEstateLoansOrLines', x='MonthlyIncome', data=train_raw_copy, fit_reg=False,\n           hue='SeriousDlqin2yrs', legend=True, size=5, aspect=1)\nplt.show()","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"b228806e95b0efa38ccd076420af1bdc9ef3ee93"},"cell_type":"markdown","source":"**Quick Commentary:**\n\nSame pattern as **Others_C_Centralized** !!"},{"metadata":{"_uuid":"50f3726a27e3a2bcd19600b7f0ab16865ce609a3"},"cell_type":"markdown","source":"* **Others_E Centralized**"},{"metadata":{"trusted":true,"_uuid":"0de6543f7dcf72746ac9e7bead27e528e16fdb9a","collapsed":true},"cell_type":"code","source":"\"\"\"Others_E\"\"\"\nsns.lmplot(y='DebtRatio', x='NumberRealEstateLoansOrLines', data=train_raw_copy, fit_reg=False, hue='SeriousDlqin2yrs',\n           legend=True, size=5, aspect=1)\nplt.show()","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"539f3ecffdb03a63368690d9e11ec35d413646f1"},"cell_type":"markdown","source":"**Quick Commentary:**\n\nSimilarly, those who have had financial distress (SeriousDlqin2yrs=1) actually a lower 'DebtRatio' than those of without financial distress (SeriousDlqin2yrs=0)"},{"metadata":{"_uuid":"f53b425eb3062cd6bcb208d5942459dbc9b3e461"},"cell_type":"markdown","source":"* **Others_F Centralized**"},{"metadata":{"trusted":true,"_uuid":"92f81652e3c91d2ea7b9c88d0da90bf6ddf04851","collapsed":true},"cell_type":"code","source":"\"\"\"Others_F\"\"\"\nsns.lmplot(y='NumberOfOpenCreditLinesAndLoans', x='NumberRealEstateLoansOrLines', data=train_raw_copy, fit_reg=False,\n           hue='SeriousDlqin2yrs', legend=True, size=5, aspect=1)\nplt.show()","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"a749b7c6f10972fbbe363b085eda6b5f3a04a00b"},"cell_type":"markdown","source":"**Quick Commentary:**\n\nA realistic scenario here.\nAs the RealEstateLoans increases, borrowers tendency to open other lines and loans (Credit loans) decreases."},{"metadata":{"_uuid":"b9c388d982e92e6e9c5dfdb450656dedd9f92706"},"cell_type":"markdown","source":"# **7.EDA Commentary**\n\n# 7.1 Summative\n\nThis summarizes the thorough EDA commentary above.\n\n**Focus_set_1**\n\n-Disobedient acts (making excessive loans & defaults or exceed deadlines) often made by 'Retired'\n\n-Realistic data-set, income plateau and falls while proceeding with age\n\n**Focus_set_2**\n\n-Disobedient acts often made by Higher-tier income\n\n-Credit balloon; since as Higher-tier income exhibit higher extremes of making loans & experiencing default\n\n-Higher-tier income tend to have lesser dependents. Self-centred data-set?\n\n**Focus_set_3**\n\n-Contrasting relationships. When experiencing financial distress (SeriousDlqin2yrs=1), borrows actually have a \"apt\" financial circumstance (lower 'DebtRatio' & exceeding deadlines & making loans or having defaults) but only have low 'MonthlyIncome'\n\nAlternative POV, low 'MonthlyIncome' main driver for financial distress, while 'DebtRatio' & exceeding deadlines & making loans or having defaults play less significant effect.\n"},{"metadata":{"_uuid":"592f341bc5482adcf8daaebedc289178fea7219a"},"cell_type":"markdown","source":"# 7.2 **Heat-Map Reference**\n\nLet's let the math tell us more using the Heat-Map"},{"metadata":{"trusted":true,"_uuid":"4a81a89a3b94fa7daad9d9311eeb40072682d14d","collapsed":true},"cell_type":"code","source":"cor = train_raw_copy.corr()\nplt.figure(figsize=(17, 10))\n# sns.set(font_scale=0.7)\nsns.heatmap(cor, annot=True, cmap='YlGn')\nplt.show()","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"9262bb535ccf2f136948b0c01e22c04458473272"},"cell_type":"markdown","source":"# **8.Final Feature Engineering Commentary**\n\nNow with the added support of the HeatMap correlations to our EDA analysis, we can further justify and decide on which features to discard or keep. \n\n**Joining No# Times past due: CombinedDefault**\n\n-Keep CombinedDefault since it outperforms in correlation\n\n-Keep 'NumberOfTime30-59DaysPastDueNotWorse' since it has the highest 'target variable' correlation amonst the original 3. But still bears low multi-collinearity with CombinedDefault\n\n-Drop NumberOfTime60-89DaysPastDueNotWorse & NumberOfTimes90DaysLate\n\n**New Feature: Net Worth**\n\n-Keep NetWorth since it outperforms in correlation\n\n-Drop MonthlyIncome; Since NetWorth has higher correlation. NetWorth as a proxy for 'MonthlyIncome' given the formula.\n\n**Join No# Loans: CombinedLoans**\n\n-Keep CombinedLoans. Used as proxy for original features\n\n-Drop NumberOfOpenCreditLinesAndLoans & NumberRealEstateLoansOrLines to avoid multi-collinearity. \n\n**New Feature: Monthly debt payments**\n\n-Drop MonthlyDebtPayments since its correlation is still lower than 'DebtRatio'\n\n**New Feature: Age Map (is_Retired & is_Senior & is_Working**\n\n-Drop Age_Map & is_Retired & is_Senior & is_Working, since original 'age' outperforms\n\n**New Feature: Income Map (is_LowY & is_MidY & is_HighY**\n\n-Drop Income_Map & is_LowY & is_MidY & is_HighY, since original 'MonthlyIncome'' outperforms"},{"metadata":{"_uuid":"dd7667aa03c90eb9402535cf2bf361a57ade71cf"},"cell_type":"markdown","source":"\n\n# ***NOTE: This Kernel primarily focuses on Feature Engineering & EDA (Chapters 3 to 8). Hence, from here forth it is for brief comparative & closure of this Kernel purposes.***\n"},{"metadata":{"_uuid":"712950d6692bbabbc092b752b7e454cbfddfa02a"},"cell_type":"markdown","source":"# **9.Prepare Data A**\n"},{"metadata":{"_uuid":"5fab8997803efa7000f90eb34b2ab2780ab42918"},"cell_type":"markdown","source":"# 9.1 Establish Features Decided on (Keep/Discard)\n\nNow to set the features to keep for modelling!"},{"metadata":{"_uuid":"bd3cb5ad94bd21884d07ca97dbd52780ce9153d6"},"cell_type":"markdown","source":"* Keep or Discard respective features"},{"metadata":{"trusted":true,"_uuid":"89588eaed8198f8fdb569572f8fbeabfddf91de3","collapsed":true},"cell_type":"code","source":"\"\"\"Prepare Data\"\"\"\n\n\"\"\"Dummy set to be used by Model to Validate above drop\"\"\"\ntrain_raw_copy_DropValidate = train_raw_copy\n\n\"\"\"Dropping\"\"\"\nColumnsToDrop = ['Unnamed: 0', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate',\n                 'MonthlyIncome',\n                 'NumberOfOpenCreditLinesAndLoans', 'NumberRealEstateLoansOrLines',\n                 'MonthlyDebtPayments',\n                 'Age_Map', 'is_Retired', 'is_Senior', 'is_Working',\n                 'Income_Map', 'is_LowY', 'is_MidY', 'is_HighY']\n\ntrain_raw_copy.drop(columns=ColumnsToDrop, inplace=True)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"96e476426227609bfa3541246ab356e23ed46eab"},"cell_type":"markdown","source":"* Check Again"},{"metadata":{"trusted":true,"_uuid":"26e95eaf8ad2153254129bdcd5780f47d8c13d03","collapsed":true},"cell_type":"code","source":"\"\"\"Check Again\"\"\"\nbasic_details(train_raw_copy)","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"4f1b2b0ee6acdf8cce9bbe1ac3157a636bb797cd"},"cell_type":"markdown","source":"# 9.2 **Check Heat-Map Again**\n\nGreat, we only have a maximum multi-collinearity correlation of 0.31"},{"metadata":{"trusted":true,"_uuid":"481ce01b15d58bf5fc386edea4380f1483db7ccc","collapsed":true},"cell_type":"code","source":"\"\"\"Correlation Heat-Map\"\"\"\ncor = train_raw_copy.corr()\nplt.figure(figsize=(10, 8))\n# sns.set(font_scale=0.7)\nsns.heatmap(cor, annot=True, cmap='YlGn')\nplt.show()","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"3491af02c26b712ee9cf542e7bfbad29776a5b19"},"cell_type":"markdown","source":"All good! Maximum of 0.31 correlations are acceptable"},{"metadata":{"_uuid":"04bf996c8b6c86b6809e539175242963a1e4ff56"},"cell_type":"markdown","source":"# 9.3 Replicate for Test set\n\nNow to replicate all that we have done for the Test_Set \n\nAnd also checking the shape!"},{"metadata":{"trusted":true,"_uuid":"cedf19e3bff50d78c9cc2e19bb7d527097193379","collapsed":true},"cell_type":"code","source":"\"\"\"Repeat Feature Engineering for Test Set\"\"\"\n\n\ndef clean_dataset(dataset):\n\n    \"\"\"CombinedDefault\"\"\"\n    dataset['CD'] = (dataset['NumberOfTime30-59DaysPastDueNotWorse']\n                     + dataset['NumberOfTimes90DaysLate']\n                     + dataset['NumberOfTime60-89DaysPastDueNotWorse'])\n    dataset['CombinedDefault'] = 1\n    dataset.loc[(dataset['CD'] == 0), 'CombinedDefault'] = 0\n    del dataset['CD']\n\n    \"\"\"NetWorth\"\"\"\n    dataset['NetWorth'] = dataset['MonthlyIncome'] * dataset['age'] / NetWorthDivisor\n\n    \"\"\"CombinedLoans\"\"\"\n    dataset['CL'] = (dataset['NumberOfOpenCreditLinesAndLoans']\n                     + dataset['NumberRealEstateLoansOrLines'])\n    dataset['CombinedLoan'] = 1\n    dataset.loc[dataset['CL'] >= LoanLinesBuffer, 'CombinedLoan'] = 1\n    dataset.loc[dataset['CL'] < LoanLinesBuffer, 'CombinedLoan'] = 0\n    del dataset['CL']\n\n    \"\"\"Dropping\"\"\"\n    to_drop = ['Unnamed: 0', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate',\n               'MonthlyIncome',\n               'NumberOfOpenCreditLinesAndLoans', 'NumberRealEstateLoansOrLines']\n\n    dataset.drop(columns=to_drop, inplace=True)\n\n\nclean_dataset(test_raw)\n\n\n\"\"\"Check Columns\"\"\"\nprint(train_raw_copy.columns)\nprint(test_raw.columns)\n\"\"\"Check Shape\"\"\"\nprint(train_raw_copy.shape)\nprint(test_raw.shape)","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"63c9f6166a4ca4ae5c8567651cf375a9a67a0473"},"cell_type":"markdown","source":"# **10.Quick Feature Engineering Validation**"},{"metadata":{"_uuid":"ca222292bb78a86661d2e5a8222324767e30502f"},"cell_type":"markdown","source":"Now just a precautionary measure here. We are going to use L1 regularized LASSO to do a quick rough validation of the features that we have chosen to exclude.\n\nWe are simply comparing the LASSO accuracy scores before VS after the drop. \n\nIdeally, we should see that we have no difference in accuracy scores. Hence, indicating that the drop had no effect on the model accuracy."},{"metadata":{"_uuid":"f39e56d2cef28c54527c34257e24988880ac054d"},"cell_type":"markdown","source":"# 10.1 Accuracy Score (Without Preliminary Engineering)"},{"metadata":{"trusted":true,"_uuid":"8f53e70011d9cd9fe576b2827b0be0f78c63c77d","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n############# PRE DROPPING FEATURES\ntrain_x1 = train_raw_copy_DropValidate.drop(\"SeriousDlqin2yrs\", axis=1).copy()\n#Y1 = train_raw_copy_DropValidate.SeriousDlqin2yrs\nY1 = train_raw['SeriousDlqin2yrs'].values\n\n# Preparing train/test split of dataset            \nX_train, X_validation, y_train, y_validation = train_test_split(train_x1, Y1, train_size=0.9, random_state=1234)\n\n##### Instantiate Logistic Regression \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Transform data for LogRef fitting\"\"\"\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(X_train.values)\n\n# Establish Model\nRandomState=42\nmodel_LogRegLASSO1 = LogisticRegression(penalty='l1', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegLASSO1.fit(std_data, y_train)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef616e3db3bd036dfb99f3cb759bf893e6449459","collapsed":true},"cell_type":"code","source":"# Run Accuracy score without any dropping of features\nprint(\"PRE DROPPING FEATURES: Running LASSO Accuracy Score without features drop...\")\n# make predictions for test data and evaluate\ny_pred = model_LogRegLASSO1.predict(X_validation)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_validation, predictions)\nprint(\"PRE Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"bc43881d8244fde76f208d96a79f79001dab6a48"},"cell_type":"markdown","source":"# 10.2 Accuracy Score (With Preliminary Engineering)"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"f22b3f9f040a00c30a9e3f9bc1c0606564e7c207","collapsed":true},"cell_type":"code","source":"############# POST DROPPING FEATURES\ntrain_x2 = train_raw_copy.drop(\"SeriousDlqin2yrs\", axis=1).copy()   \nY2 = train_raw_copy['SeriousDlqin2yrs'].values  \n\n# Preparing train/test split of dataset            \nX_train, X_validation, y_train, y_validation = train_test_split(train_x2, Y2, train_size=0.9, random_state=1234)\n\n##### Instantiate Logistic Regression \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Transform data for LogRef fitting\"\"\"\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(X_train.values)\n\n# Establish Model\n\nmodel_LogRegLASSO1 = LogisticRegression(penalty='l1', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegLASSO1.fit(std_data, y_train)","execution_count":56,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f405ed36d00e2ee88589e8d2cd42f0759f40996","collapsed":true},"cell_type":"code","source":"# Run Accuracy score without any dropping of features\nprint(\"POST DROPPING FEATURES: Running LASSO Accuracy Score with features dropped...\")\n# make predictions for test data and evaluate\ny_pred = model_LogRegLASSO1.predict(X_validation)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_validation, predictions)\nprint(\"POST Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"782f598d5e62ffbf53b9220a25397104feca2bbc"},"cell_type":"markdown","source":"**Great both 93.03% and henceno difference in accuracy scores!**\n\n**Let's proceed with the actual ensemble modelling then!**"},{"metadata":{"_uuid":"8a7130caa9688d263a9f94871d4b99b7fea737e0"},"cell_type":"markdown","source":"# **11.Prepare Data - B**"},{"metadata":{"_uuid":"cea57951ddca62dcd4f0dd6be471f6fcfc952a85"},"cell_type":"markdown","source":"# 11.1**Train/Test Split**"},{"metadata":{"trusted":true,"_uuid":"508acf958dc1b4b52e2a28266f75dbdc61215976","collapsed":true},"cell_type":"code","source":"\"\"\"\nPrepare Data\n\"\"\"\n# Split our predictors and the target variable in our data-sets\n\"\"\"Train set\"\"\"\nX = train_raw_copy.drop(\"SeriousDlqin2yrs\", axis=1).copy()\ny = train_raw_copy.SeriousDlqin2yrs\nprint(X.shape, '\\n', y.shape)\n\n\"\"\"Test set\"\"\"\nX_test = test_raw.drop(\"SeriousDlqin2yrs\", axis=1).copy()\ny_test = test_raw.SeriousDlqin2yrs\nprint(X_test.shape, '\\n', y_test.shape)\n\n\n\"\"\"Train/test split of data-set\"\"\"\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, random_state=1234)","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"b53510aafbe521616710c369cf37f3b4fbaca0e1"},"cell_type":"markdown","source":"More or less same shapes, lets proceed!"},{"metadata":{"_uuid":"96f6bb918ef5ddba2822c24b6f490ae2176f77e6"},"cell_type":"markdown","source":"# **Models**"},{"metadata":{"_uuid":"e0d7006d3a03fb38c0cf539faf6ff12ac83fee13"},"cell_type":"markdown","source":"Here I am going to use:\n\nWhite-Box Models (LASSO, Ridge, Logit Balanced)\nBlack-Box Models (XGB Classifier, Random Forest Classifier)\n\nWe will first define a general function to help us normalize all feature scorings. \n\nAs white box models coefficients, white black box models use GINI importance"},{"metadata":{"_uuid":"a5a3b9367b8cbfe090ebcbe07d5af75ef75df238"},"cell_type":"markdown","source":"# 11.2 Feature Comparison Function"},{"metadata":{"trusted":true,"_uuid":"f0507bc18ee2bd758ffaba562ca8b52dd9a9b1a5","collapsed":true},"cell_type":"code","source":"\"\"\"\nModel\n\"\"\"\nRandomState = 42\n\n\"\"\"Preparing Side to Side Comparative Function\"\"\"\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef rank_to_dict(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x, 2), ranks)\n    return dict(zip(names, ranks))\n\n\nnames = X.columns\nranks = {}\nprint('Prep done...')","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"5197bd1e41f657e10d6063b82f8544c0db7e8f67"},"cell_type":"markdown","source":"# 11.3 **LASSO L1 Regularized**\n\n"},{"metadata":{"_uuid":"2fddea291f945c1369e9f191316d9975ce6811c3"},"cell_type":"markdown","source":"* Establish Model *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"24b254ac8ad4932ac62295f1767c01e68599792f","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nLASSO via LogisticRegression l1 penalty - Feature Importance - PART 1\n\"\"\"\nprint('Running LASSO via LogisticRegression l1 penalty...')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\"\"\"\"Transform data for LogRef fitting\"\"\"\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(X_train.values)\n\n\"\"\"Establish Model\"\"\"\nmodel_LogRegLASSO = LogisticRegression(penalty='l1', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegLASSO.fit(std_data, y_train)\n\n\"\"\"For Side To Side\"\"\"\nranks[\"LogRegLASSO\"] = rank_to_dict(list(map(float, model_LogRegLASSO.coef_.reshape(len(names), -1))), names, order=1)\nprint(ranks[\"LogRegLASSO\"])","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"6faa20b070c949de33bf2835df4b8617425f58e8"},"cell_type":"markdown","source":"* Plot Feature Rankings"},{"metadata":{"trusted":true,"_uuid":"1696ce1adae3bbd9c47152fff0d279733328bd7b","collapsed":true},"cell_type":"code","source":"\"\"\"Plotting\"\"\"\nimport operator\nlistsLASSO = sorted(ranks[\"LogRegLASSO\"].items(), key=operator.itemgetter(1))\n# convert list>array>dataframe\ndfLASSO = pd.DataFrame(np.array(listsLASSO).reshape(len(listsLASSO), 2),\n                       columns=['Features', 'Ranks']).sort_values('Ranks')\ndfLASSO['Ranks'] = dfLASSO['Ranks'].astype(float)\n\ndfLASSO.plot.bar(x='Features', y='Ranks', color='blue')\nplt.xticks(rotation=90)\nplt.show()","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"b9f879ea8146641411cbceb483f0bdd5080f6208"},"cell_type":"markdown","source":"# 11.4 **Ridge L2 Regularized**\n\n"},{"metadata":{"_uuid":"6fcf96889c8879f9333478d2609a363c2acfe0e0"},"cell_type":"markdown","source":"* Establish Model *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"d694bb70da179519d2afb00957eac9649dfa7509","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nRidge via LogisticRegression l2 penalty - Feature Importance - PART 1\n\"\"\"\n\nprint('Running Ridge via LogisticRegression l2 penalty...')\n\"\"\"Establish Model\"\"\"\nmodel_LogRegRidge = LogisticRegression(penalty='l2', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegRidge.fit(std_data, y_train)\n\n\"\"\"For Side To Side\"\"\"\nranks[\"LogRegRidge\"] = rank_to_dict(list(map(float, model_LogRegRidge.coef_.reshape(len(names), -1))),\n                                    names, order=1)\nprint(ranks[\"LogRegRidge\"])","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"f6cea2739727f00e04436e48961e94a9c3343ccb"},"cell_type":"markdown","source":"* Plot Feature Rankings"},{"metadata":{"trusted":true,"_uuid":"313b742f9384380bc357b875ade407835e0bbdd4","collapsed":true},"cell_type":"code","source":"\"\"\"Plotting\"\"\"\nimport operator\nlistsRidge = sorted(ranks[\"LogRegRidge\"].items(), key=operator.itemgetter(1))\ndfRidge = pd.DataFrame(np.array(listsRidge).reshape(len(listsRidge), 2),\n                       columns=['Features', 'Ranks']).sort_values('Ranks')\ndfRidge['Ranks'] = dfRidge['Ranks'].astype(float)\n\ndfRidge.plot.bar(x='Features', y='Ranks', color='blue')\nplt.xticks(rotation=90)\nplt.show()","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"529538888c887d3cd22ee4c82158ea68098276a3"},"cell_type":"markdown","source":"# 11.5 **Logit Balanced by sample weight**\n\n"},{"metadata":{"_uuid":"9f742873444a6f359fa61b8f3bc074e0bdf2b044"},"cell_type":"markdown","source":"* Establish Model *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"67985b7a095673b588c659cce6fa60bf3c1e02e3","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nLogisticRegression Balanced - Feature Importance - PART 1\n\"\"\"\nprint('RunningLogisticRegression Balanced...')\n\"\"\"Establish Model\"\"\"\nmodel_LogRegBalance = LogisticRegression(class_weight='balanced', C=0.1, random_state=RandomState, solver='liblinear', n_jobs=1)\nmodel_LogRegBalance.fit(std_data, y_train)\n\n\"\"\"For Side To Side\"\"\"\nranks[\"LogRegBalance\"] = rank_to_dict(list(map(float, model_LogRegBalance.coef_.reshape(len(names), -1))),\n                                      names, order=1)\nprint(ranks[\"LogRegBalance\"])","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"f08f155ca78c330896ebfe24802d03d6b0cf2f78"},"cell_type":"markdown","source":"* Plot Feature Rankings"},{"metadata":{"trusted":true,"_uuid":"2edd7ec72a6a1ea81b0efd5534e9a4dfe4d084fb","collapsed":true},"cell_type":"code","source":"\"\"\"Plotting\"\"\"\nimport operator\nlistsBal = sorted(ranks[\"LogRegBalance\"].items(), key=operator.itemgetter(1))\ndfBal = pd.DataFrame(np.array(listsBal).reshape(len(listsBal), 2),\n                     columns=['Features', 'Ranks']).sort_values('Ranks')\ndfBal['Ranks'] = dfBal['Ranks'].astype(float)\n\ndfBal.plot.bar(x='Features', y='Ranks', color='blue')\nplt.xticks(rotation=90)\nplt.show()","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"8c6954c9b0926f93014240767762fdb84409ef7c"},"cell_type":"markdown","source":"# 11.6 **XGB Classifier**"},{"metadata":{"_uuid":"db2141a0088ed5f2ac5ec0c1b7b416dbf7ad6170"},"cell_type":"markdown","source":"* Establish Model *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"70e21abb9f921158441c2936e427811b76add41c","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nXGBClassifier - Feature Importance - PART 1\n\"\"\"\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance\nprint(\"Running XGBClassifier Feature Importance Part 1...\")\nmodel_XGBC = XGBClassifier(objective='binary:logistic',\n                           max_depth=7, min_child_weight=5,\n                           gamma=0,\n                           learning_rate=0.1, n_estimators=100,)\n# model_XGBC.fit(X_train, y_train)\nmodel_XGBC.fit(std_data, y_train)\nprint(\"XGBClassifier Fitted\")\n\n\"\"\"Side To Side\"\"\"\nprint(\"Ranking Features with XGBClassifier...\")\nranks[\"XGBC\"] = rank_to_dict(model_XGBC.feature_importances_, names)\nprint(ranks[\"XGBC\"])","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"aae2e1016dde8ac29925549a14217207761fee77"},"cell_type":"markdown","source":"* Plot Feature Rankings"},{"metadata":{"trusted":true,"_uuid":"de0d2facea69693c505ba412352b586a80b6db4c","collapsed":true},"cell_type":"code","source":"\"\"\"Plotting\"\"\"\n# plot feature importance for feature selection using default inbuild function\nplot_importance(model_XGBC)\nplt.show()","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"3be369947c034780ee5e2628e3c2c7bd332b365f"},"cell_type":"markdown","source":"# 11.7 **Random Forest Classifier**"},{"metadata":{"_uuid":"b39c3ba35ec0572a9a98bbcea4c929484b2f595f"},"cell_type":"markdown","source":"* Establish Model *Unhide to view output"},{"metadata":{"trusted":true,"_uuid":"46aac14ad24d38aa11dbbf818330a02c35a4dd9d","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"\"\"\"\nRandom Forest Classifier - Feature Importance - PART 1\n\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel_RFC = RandomForestClassifier(bootstrap=True, max_depth=80,\n                                   criterion='entropy',\n                                   min_samples_leaf=3, min_samples_split=10, n_estimators=100)\n# model_RFC.fit(X_train, y_train)\nmodel_RFC.fit(std_data, y_train)\nprint(\"Random Forest Classifier Fitted\")\n\n\"\"\"Side To Side\"\"\"\nprint(\"Ranking Features with RFClassifier...\")\nranks[\"RFC\"] = rank_to_dict(model_RFC.feature_importances_, names)\nprint(ranks[\"RFC\"])","execution_count":68,"outputs":[]},{"metadata":{"_uuid":"d4b7b13a26965c161b4b7dec0cae30041f4da575"},"cell_type":"markdown","source":"* Plot Feature Rankings"},{"metadata":{"trusted":true,"_uuid":"498777571e4d230f1a177c50c535d0247147c11d","collapsed":true},"cell_type":"code","source":"\"\"\"Plotting\"\"\"\n# For Chart\nimportance = pd.DataFrame({'feature': X_train.columns, 'importance': np.round(model_RFC.feature_importances_, 3)})\nimportance_sorted = importance.sort_values('importance', ascending=False).set_index('feature')\n# plot feature importance for feature selection using default inbuild function\n#print(importance_sorted)\nimportance_sorted.plot.bar()\nplt.show()","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"889dc369fb4ad32d9d140b79a2ba4bae4ef1d2da"},"cell_type":"markdown","source":"# **12.Features (Side To Side Comparison)**\n\nHere i present 2 different approaches"},{"metadata":{"_uuid":"27fd71cd9726ba833865bde849a034ef62089d4e"},"cell_type":"markdown","source":"# 12.1 (Coefficient Values) Quick Easy Method\n\nFirst quick approach allows for quick printing if you are short of time."},{"metadata":{"trusted":true,"_uuid":"27bf0be4919acde548a26098ac38ca72d2efedbb","collapsed":true},"cell_type":"code","source":"\"\"\"\nCollate Side to Side df - Feature Importance\n\"\"\"\n\n\"\"\"Quick Print Method\"\"\"\n# Create empty dictionary to store the mean value calculated across all the scores\nr = {}\nfor name in names:\n    # This is the alternative rounding method from the earlier map & lambda combination\n    r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\n\nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n\nprint(\"\\t%s\" % \"\\t\".join(methods))\nfor name in names:\n    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, [ranks[method][name] for method in methods]))))","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"2ebfe30149ee94c3e57f9bbd711df28611f0c623"},"cell_type":"markdown","source":"As you easily see this is quick but alignments are off....Now for a neater and more presentable method"},{"metadata":{"_uuid":"b60a43acd753b59b3da960255e955caaf31ad391"},"cell_type":"markdown","source":"# 12.2 (Coefficient Values) Neat DataFrame Method\n\nSecond approach you could loop through and procure a dataframe for further use going forward."},{"metadata":{"trusted":true,"_uuid":"602b4d541080f6ce3f9d4fc3961e1d33f10b1ca7","collapsed":true},"cell_type":"code","source":"\"\"\"Alternatively, Set into a df\"\"\"\n# Loop through dictionary of scores to append into a dataframe\nrow_index = 0\nAllFeatures_columns = ['Feature', 'Scores']\nAllFeats = pd.DataFrame(columns=AllFeatures_columns)\nfor name in names:\n    AllFeats.loc[row_index, 'Feature'] = name\n    AllFeats.loc[row_index, 'Scores'] = [ranks[method][name] for method in methods]\n\n    row_index += 1\n\n# Here the dataframe scores are a list in a list.\n# To split them, we convert the 'Scores' column from a dataframe into a list & back into a dataframe again\nAllFeatures_only = pd.DataFrame(AllFeats.Scores.tolist(), )\n# Now to rename the column headers\nAllFeatures_only.rename(columns={0: 'LogRegBalance', 1: 'LogRegLASSO', 2: 'LogRegRidge',\n                                 3: 'Random ForestClassifier', 4: 'XGB Classifier', 5: 'Mean'}, inplace=True)\nAllFeatures_only = AllFeatures_only[['LogRegBalance', 'LogRegLASSO', 'LogRegRidge',\n                                     'Random ForestClassifier', 'XGB Classifier', 'Mean']]\n# Now to join both dataframes\nAllFeatures_compare = AllFeats.join(AllFeatures_only).drop(['Scores'], axis=1)\ndisplay(AllFeatures_compare)","execution_count":71,"outputs":[]},{"metadata":{"_uuid":"e7ff35e79689dda95f169590b1364064885e77b9"},"cell_type":"markdown","source":"# **Plotting**\n\nNow likewise plotting I've given 2 methods"},{"metadata":{"_uuid":"a1f497cc3a47ed3f57bd82b6a731d733408195ad"},"cell_type":"markdown","source":"# 12.3 (Plotting) Quick Method\n\nA quick plot to see the rough trend."},{"metadata":{"trusted":true,"_uuid":"6ed131cfbeb29174cb17bd7ef13837dc513a214b","collapsed":true},"cell_type":"code","source":"\"\"\"Quick Plotting\"\"\"\n# Plotting\ndf = AllFeatures_compare.melt('Feature', var_name='cols',  value_name='vals')\ng = sns.factorplot(x=\"Feature\", y=\"vals\", hue='cols', data=df, size=10, aspect=2)\n\nplt.xticks(rotation=90)\nplt.show()","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"9c6c2f269ef79b208b566cf424b95df1187fea97"},"cell_type":"markdown","source":"# 12.4 (Plotting) Sorted Neat Method\n\nA better plot IMO could be used, where we sort by the mean first. Similar, to what the Excel Pivot Chart does."},{"metadata":{"trusted":true,"_uuid":"ffea4e733d4051294ef5ddc3dbcdb4f9f8a4d6d0","collapsed":true},"cell_type":"code","source":"\"\"\"Sorted Plotting\"\"\"\nAllFeatures_compare_sort = AllFeatures_compare.sort_values(by=['Mean'], ascending=True)\norder_ascending = AllFeatures_compare_sort['Feature']\n#Plotting\ndf2 = AllFeatures_compare_sort.melt('Feature', var_name='cols',  value_name='vals')\n# ONLY Difference is that now we use row_order to sort based on the above ascending Ascending Mean Features\n# g2 = sns.factorplot(x=\"Feature\", y=\"vals\", hue='cols', data=df2, size=10, aspect=2, row_order=order_ascending)\ng2 = sns.factorplot(x=\"Feature\", y=\"vals\", hue='cols', data=df2, size=10, aspect=2, row_order=order_ascending)\nplt.xticks(rotation=60)\nplt.show()","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"22a48ba4227f70f238e3455da5853fd03a2ca0e6"},"cell_type":"markdown","source":"# 12.5 Quick Commentary Result (Feature Engineering VS Importance)\n\n* **Group_A (Combine Feature)**\n\n-Quantity of Defaults\n\nWow, it was ranked highest!! Even outdid its component \"NumberOfTime30-59DaysPastDueNotWorse\", But on the downside ranked the lowest by XGB.\n\nNote we discarded its other 2 components: \"NumberOfTimes90DaysLate\" & \"NumberOfTime60-89DaysPastDueNotWorse\"\n\n* **Group_B (New Feature Net Worth)**\n\n-Net Worth\n\nWoho!! it was ranked in the middle. Good call on this one :)\n\n* **Group_C (Combine Feature)**\n\n-Combined Loan\n\nLooks like our this new feature we created ranked last....Even it's component \"RevolvingUtilizationOfUnsecuredLines\" outdid it...Bad call hahaha\n\n* **New Feature (Debt Payments)**\n\nNote we discarded this\n\n* **New Feature (Age Category)**\n\nNote we discarded this\n\n* **New Feature (Social Economic Status SES Category)**\n\nNote we discarded this\n"},{"metadata":{"_uuid":"ac3506f9cfac040579a41f1835913e5af267cd03"},"cell_type":"markdown","source":"# **13.ROC AUC (Side To Side Comparison)**\n\nNow as before we will collate the ROC & AUC for each model & plot all of them for comparison"},{"metadata":{"trusted":true,"_uuid":"7808e9d96fc81201df82b4df5267b2f4f8604e82","collapsed":true},"cell_type":"code","source":"\"\"\"\nCollate Side to Side df - ROC AUC\n\"\"\"\n# Ensemble Comparison of ROC AUC\nfrom sklearn import model_selection\nimport matplotlib.pyplot as plt\n\n\nprint(\"Charting ROC AUC for Ensembles...\")\nfrom sklearn.metrics import roc_curve, auc\n\n# Establish Models\nmodels = [\n    {\n        'label': 'LASSO',\n        'model': model_LogRegLASSO,\n    },\n    {\n        'label': 'Ridge',\n        'model': model_LogRegRidge,\n    },\n    {\n        'label': 'LogReg Balance',\n        'model': model_LogRegBalance,\n    },\n    {\n        'label': 'XGBoost Classifier',\n        'model': model_XGBC,\n    },\n    {\n        'label': 'Random Forest Classifier',\n        'model': model_RFC,\n    }\n]\n\n# Models Plot-loop\nfor m in models:\n    #fpr, tpr, thresholds = roc_curve(y_validation, m['model'].predict_proba(X_validation).T[1])\n    scaler = StandardScaler()\n    std_data2 = scaler.fit_transform(X_validation.values)\n    fpr, tpr, thresholds = roc_curve(y_validation, m['model'].predict_proba(std_data2).T[1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], roc_auc))\n\n# Set Plotting attributes\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=0, fontsize='small')\nplt.show()","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"6f4f7b69658b8b709bd5252a6f71816e89b09672"},"cell_type":"markdown","source":"# **14.Cross Validation (Side To Side Comparison)**\n\nNow as before we will collate the relevant 'Cross Validated' (CV) accuracy scores for each model & compare all of them."},{"metadata":{"trusted":true,"_uuid":"eb43cc4fe7c1e08726e8fe6e8baf325e0af2beee","collapsed":true},"cell_type":"code","source":"\"\"\"Collate Side to Side df - Accuracy Scores\"\"\"\n# run model 10x with 60/30 split, but intentionally leaving out 10% avoiding overfitting\ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=.3, train_size=.6, random_state=0)\n\n# Ensemble Comparison of Accuracy Scores\n# Set dataframe for appending``\n# pd.options.display.max_columns = 100\nACCScores_columns = ['Model Name', 'Train Accuracy Mean', 'Test Accuracy Mean']\nACCScores_compare = pd.DataFrame(columns=ACCScores_columns)\n\n# Models CV-loop\nrow_index = 0\nfor m in models:\n    # Name of Model\n    ACCScores_compare.loc[row_index, 'Model Name'] = m['label']\n\n    # Execute Cross Validation (CV)\n    cv_results = model_selection.cross_validate(m['model'], X_train, y_train, cv=cv_split)\n    # Model Train Accuracy\n    ACCScores_compare.loc[row_index, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n    # Model Test Accuracy\n    ACCScores_compare.loc[row_index, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n\n    row_index += 1\n\ndisplay(ACCScores_compare)","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"d3b5a1e6bd9a3529f0fe97e85e674bf71f8e7cf1"},"cell_type":"markdown","source":"Thank you for reading till the end! If you like more commentary on this tail chapters, do refer to my other Kernel  \"Ensemble Models Comparison Techniques\".\n\nHope you now have a better understanding of EDA & Feature Engineering!!\n\nCheers"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}