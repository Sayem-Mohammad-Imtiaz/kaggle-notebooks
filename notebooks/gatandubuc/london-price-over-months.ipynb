{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,15)\nimport seaborn as sns\nimport scipy.stats as st\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Table 1: year variables\n# Table 2: Month variables\nlondon_table = pd.read_csv(r'/kaggle/input/housing-in-london/housing_in_london_monthly_variables.csv', sep=',')\nlondon_table2 = pd.read_csv(r'/kaggle/input/housing-in-london/housing_in_london_yearly_variables.csv', sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dates\nlondon_table['date'] = pd.to_datetime(london_table['date'], errors = 'coerce')\nlondon_table = london_table.assign(day=london_table.date.dt.day.astype('uint16'),\n                           year=london_table.date.dt.year.astype('uint16'),\n                           month=london_table.date.dt.month.astype('uint16'),\n                           )\n\nlondon_table2['date'] = pd.to_datetime(london_table2['date'], errors = 'coerce')\nlondon_table2 = london_table2.assign(day=london_table2.date.dt.day.astype('uint16'),\n                           year=london_table2.date.dt.year.astype('uint16'),\n                           month=london_table2.date.dt.month.astype('uint16'),\n                           )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"london_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"london_table2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creation of joined key between the two previous tables\nlondon_table[\"code_year\"] = london_table[\"code\"] + \"_\" + london_table[\"year\"].astype('str')\nlondon_table2[\"code_year\"] = london_table2[\"code\"] + \"_\" + london_table2[\"year\"].astype('str')\nlondon_table2[\"code_year_month\"] = london_table2[\"code\"] + \"_\" + london_table2[\"year\"].astype('str') + \"_\" + london_table2[\"month\"].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge\ntable = london_table2.merge(london_table.drop(['day','year','month','borough_flag','area','code', 'date'], axis=1), how=\"inner\", on=\"code_year\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Take into account borough and not regions\ntable_borough = table[table['borough_flag']==1]\ntable_borough.replace([0,'#','na'],np.nan, inplace=True) # Bad data\ntable_borough.mean_salary = table_borough.mean_salary.astype('float64')\ntable_borough.recycling_pct = table_borough.recycling_pct.astype('float64')\nprint(table_borough.info()) # Verifying if no bad data like '-'\n\n# Nan analysis\n# Nan according each code -> E0900001 don't have data or few for No_of_crimes or life_satisfaction\n# We can replaced them with a copy of an other borough closer than E0900001.\ntable_borough.groupby(by='code').agg({'no_of_crimes':lambda x : x.isna().sum(),\n                                     'median_salary':lambda x : x.isna().sum(),\n                                     'life_satisfaction':lambda x : x.isna().sum(),\n                                     'recycling_pct':lambda x : x.isna().sum(),\n                                     'population_size':lambda x : x.isna().sum(),\n                                     'number_of_jobs':lambda x : x.isna().sum(),\n                                     'area_size':lambda x : x.isna().sum(),\n                                     'no_of_houses':lambda x : x.isna().sum()}).plot(kind='bar', width=0.9)\n\n# Nan according each years -> 1999 and 2019 have no data, that's mean lot of borough have no data for these columns.\n# We can replace them by regression over years.\ntable_borough.groupby(by='year').agg({'no_of_crimes':lambda x : x.isna().sum(),\n                                     'median_salary':lambda x : x.isna().sum(),\n                                     'life_satisfaction':lambda x : x.isna().sum(),\n                                     'recycling_pct':lambda x : x.isna().sum(),\n                                     'population_size':lambda x : x.isna().sum(),\n                                     'number_of_jobs':lambda x : x.isna().sum(),\n                                     'area_size':lambda x : x.isna().sum(),\n                                     'no_of_houses':lambda x : x.isna().sum()}).plot(kind='bar', width=0.9)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Take into account regions\ntable_region = table[table['borough_flag']==0]\n\n# Replace bad data\ntable_region.replace([0,'-'],np.nan, inplace=True)\ntable_region.mean_salary = table_region.mean_salary.astype('float64')\ntable_region.recycling_pct = table_region.recycling_pct.astype('float64')\n\n# link with table_borough\ntable_region['area_year_month'] = table_region.area + '_' + table_region.year.astype('str') + '_' + table_region.month.astype('str')\nprint(table_region.info()) # Verifying if there is no bad data\n\ntable_region.groupby(by='code').agg({'median_salary':lambda x : x.isna().sum(),\n                                     'life_satisfaction':lambda x : x.isna().sum(),\n                                     'recycling_pct':lambda x : x.isna().sum(),\n                                     'population_size':lambda x : x.isna().sum(),\n                                     'number_of_jobs':lambda x : x.isna().sum(),\n                                     'area_size':lambda x : x.isna().sum(),\n                                     'no_of_houses':lambda x : x.isna().sum()}).plot(kind='bar', width=0.9)\ntable_region.groupby(by='year').agg({'median_salary':lambda x : x.isna().sum(),\n                                     'life_satisfaction':lambda x : x.isna().sum(),\n                                     'recycling_pct':lambda x : x.isna().sum(),\n                                     'population_size':lambda x : x.isna().sum(),\n                                     'number_of_jobs':lambda x : x.isna().sum(),\n                                     'area_size':lambda x : x.isna().sum(),\n                                     'no_of_houses':lambda x : x.isna().sum()}).plot(kind='bar', width=0.9)\n\n# No no_of_crimesand drop dupllicates data from table_borough and table_region\ntry:\n    table_region.drop(['no_of_crimes','borough_flag','day','code_year','code','year','month','area'], axis=1, inplace=True)\nexcept:\n    pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_borough","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_region","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We divide each borough into two categories of regions \nif 'london' not in table_borough.columns:\n    table_borough = table_borough.merge(pd.Series([clef for clef in {\n    'city of london':'inner london',\n    'barking and dagenham':'outer london',\n    'barnet':'outer london',\n    'bexley':'outer london',\n    'brent':'outer london',\n    'bromley':'outer london',\n    'croydon':'outer london',\n    'ealing':'outer london',\n    'enfield':'outer london',\n    'haringey':'outer london',\n    'harrow':'outer london',\n    'havering':'outer london',\n    'hillingdon':'outer london',\n    'hounslow':'outer london',\n    'kingston upon thames':'outer london',\n    'merton':'outer london',\n    'newham':'outer london',\n    'redbridge':'outer london',\n    'richmond upon thames':'outer london',\n    'sutton':'outer london',\n    'waltham forest':'outer london',\n    'camden':'inner london',\n    'greenwich':'inner london',\n    'hackney':'inner london',\n    'hammersmith and fulham':'inner london',\n    'islington':'inner london',\n    'kensington and chelsea':'inner london',\n    'lambeth':'inner london',\n    'lewisham':'inner london',\n    'southwark':'inner london',\n    'tower hamlets':'inner london',\n    'wandsworth':'inner london',\n    'westminster':'inner london'}.values()], index = [value for value in {\n    'city of london':'inner london',\n    'barking and dagenham':'outer london',\n    'barnet':'outer london',\n    'bexley':'outer london',\n    'brent':'outer london',\n    'bromley':'outer london',\n    'croydon':'outer london',\n    'ealing':'outer london',\n    'enfield':'outer london',\n    'haringey':'outer london',\n    'harrow':'outer london',\n    'havering':'outer london',\n    'hillingdon':'outer london',\n    'hounslow':'outer london',\n    'kingston upon thames':'outer london',\n    'merton':'outer london',\n    'newham':'outer london',\n    'redbridge':'outer london',\n    'richmond upon thames':'outer london',\n    'sutton':'outer london',\n    'waltham forest':'outer london',\n    'camden':'inner london',\n    'greenwich':'inner london',\n    'hackney':'inner london',\n    'hammersmith and fulham':'inner london',\n    'islington':'inner london',\n    'kensington and chelsea':'inner london',\n    'lambeth':'inner london',\n    'lewisham':'inner london',\n    'southwark':'inner london',\n    'tower hamlets':'inner london',\n    'wandsworth':'inner london',\n    'westminster':'inner london'\n    }.keys()], name='london'), how='left', left_on='area', right_index=True)\n    \n    table_borough['london'] = table_borough.london + '_' + table_borough.year.astype('str') + '_' + table_borough.month.astype('str')\n\ntable_b_r = table_borough.merge(table_region.drop(['date','code_year_month'], axis=1), how='left', left_on='london', right_on='area_year_month')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\n\n# Encoding\ntry:\n    cat_features = ['area', 'code', 'code_year', 'code_year_month', 'london','area_year_month']\n    num_features = table_b_r.drop(cat_features, axis=1).drop(['average_price_x'], axis=1).columns\nexcept NameError:\n    print('n')\n\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    table_b_r[col] = label_encoder.fit_transform(table_b_r[col])\n\ntable_b_r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inputs and outputs detection\n# flag nan columns \ndef detection_nan(dataframe, append_col=True):\n    col_nan = []\n    for col in dataframe:\n        if dataframe[col].isna().any():\n            if append_col:\n                dataframe[col+'_'+'nan'] = [1 if math.isnan(i) else 0 for i in dataframe[col]]\n            col_nan.append(col)\n    return dataframe, col_nan\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    dataframe = dataframe.sort_values(by='date')\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test\n\ntable_b_r = detection_nan(table_b_r)[0]\ntrain, valid, test = get_data_splits(table_b_r)\nX_test = test.drop('average_price_x', axis=1)\ny_test = test['average_price_x']\nX_valid = valid.drop('average_price_x', axis=1)\ny_valid = valid['average_price_x']\nX_train_filt = train.drop('average_price_x', axis=1)\ny_train_filt = train['average_price_x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing numeric data\n# outliers are deleted\n# useless to delete outliers\n\"\"\"def outliers(clf, y_train=y_train.copy(), X_train=X_train.copy(), features_x=['area_size_x','population_size_x'], y=True):\n    \n    if y:\n        y_train = y_train.to_frame()\n        X = y_train.values.reshape(-1,1)\n        clf = clf.fit(X)\n        y_train['res'] = clf.predict(X)\n    X_train = X_train.loc[(y_train['res']==1),:]\n    y_train = y_train.query('res==1').iloc[:,0]\n    \n    # Inutile de supprimer les outliers des autres colonnes\n    for col in features_x:\n        X = X_train[col].replace(np.nan, np.mean(X)).values.reshape(-1,1)\n        clf = clf.fit(X)\n        X_train[col+'_res'] = clf.predict(X)\n    X_train.replace({col:-1 for col in features_x}, np.nan, inplace=True)\n    X_train.dropna(subset=features_x, inplace=True)\n    X_train = X_train.loc[:, [col for col in X_train.columns if 'res' not in col]]\n    y_train = y_train.loc[X_train.index]\n    \n    return X_train, y_train\n\nout = outliers(OneClassSVM(gamma='auto'))\nX_train_filt = out[0]\ny_train_filt = out[1]\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Fill nan by regression\ndef fill_nan(dataframe=X_train_filt.copy(), col_nan=detection_nan(X_train_filt)[1]):\n    for code in dataframe.code.unique():\n        data = dataframe[dataframe['code']==code]\n        for col in col_nan:\n            try:\n                if data[col].isna().any():\n                    data_regr = data.dropna(subset=[col])\n                    regr = st.linregress(data_regr['code_year_month'], data_regr[col])\n                    data.loc[data[col].isna().loc[data[col].isna()==True].index, col] = regr[0]*data.loc[data[col].isna().loc[data[col].isna()==True].index, 'code_year_month']+ regr[1]\n                    dataframe.loc[dataframe['code']==code, col] = data[col].values\n            except ValueError:\n                print(code, col)\n    return dataframe\n                \nX_train_filt = fill_nan()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Copying data for columns no_of_crimes and life_satisfaction from the closest borough for code E0900001 (code==0)\n\nfor idx, price in y_train_filt.loc[X_train_filt[X_train_filt.code==0].index].iteritems():\n    idx_closest = (y_train_filt.loc[X_train_filt[X_train_filt.code!=0].index]-price).abs().sort_values().index[0]\n    X_train_filt.loc[idx,'no_of_crimes'] = X_train_filt.loc[idx_closest,'no_of_crimes']\n    X_train_filt.loc[idx,'life_satisfaction_x'] = X_train_filt.loc[idx_closest,'life_satisfaction_x']\nX_train_filt.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We could also replace columns 23, 25 and 27 by regressions but i prefer to drop them.\nX_train_filt = X_train_filt.drop(['date','borough_flag','life_satisfaction_y','recycling_pct_y','number_of_jobs_y','life_satisfaction_y_nan','recycling_pct_y_nan','number_of_jobs_y_nan'], axis=1)\nX_test = X_test.drop(['date','borough_flag','life_satisfaction_y','recycling_pct_y','number_of_jobs_y','life_satisfaction_y_nan','recycling_pct_y_nan','number_of_jobs_y_nan'], axis=1)\nnum_features = num_features.drop(['life_satisfaction_y','recycling_pct_y','number_of_jobs_y','date','borough_flag','day','year','month'])\nX_valid = X_valid.drop(['date','borough_flag','life_satisfaction_y','recycling_pct_y','number_of_jobs_y','life_satisfaction_y_nan','recycling_pct_y_nan','number_of_jobs_y_nan'], axis=1)\nX_valid.dropna(how='any', inplace=True)\n\ny_valid = y_valid.loc[X_valid.index]\nX_test_filt = X_test.dropna(how='any')\ny_test_filt = y_test.loc[X_test_filt.index]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Correlation analysis\n# We calculate mean correlation.\ncorr_table = X_train_filt[X_train_filt['code']==X_train_filt['code'].unique()[0]].corr().fillna(0)\nfor code in X_train_filt['code'].unique()[1:]:\n    corr_table = corr_table + X_train_filt[X_train_filt['code']==code].corr().fillna(0)\ncorr_table = corr_table / len(X_train_filt['code'].unique())\ncorr_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepocessing # useless\n\"\"\"from sklearn.preprocessing import Normalizer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', PowerTransformer(), num_features)\n    ])\n\nX_train_filt.loc[:,num_features] = preprocessor.fit_transform(X_train_filt)\ny_train_filt_tf = np.log(y_train_filt)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data augmentation\nX_train_filt['test'] = X_train_filt['average_price_y']*X_train_filt.area_year_month\nX_valid['test'] = X_valid['average_price_y']*X_valid.area_year_month\nX_test_filt['test'] = X_test_filt['average_price_y']*X_test_filt.area_year_month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import SGDRegressor, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmodels = [LinearRegression(),\n          GradientBoostingRegressor(),\n          SGDRegressor(),\n          RandomForestRegressor(),\n          DecisionTreeRegressor()]\n\nfor model in models:\n    # pipeline\n    my_pipeline = Pipeline(steps=[\n                                  ('model', model)\n                                 ])\n\n    # fit le modele \n    my_pipeline.fit(X_train_filt, y_train_filt)\n\n    # prédiction\n    preds = my_pipeline.predict(X_valid)\n    print(model, 'MAE:', mean_absolute_error(y_valid, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz\n\nmodel = DecisionTreeRegressor(random_state=0).fit(pd.concat([X_train_filt,X_valid]), pd.concat([y_train_filt,y_valid]))\n\n # prédiction\npreds = model.predict(X_test_filt)\nprint('MAE', mean_absolute_error(y_test_filt, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAE per borough\nresults = model.predict(X_test_filt) - y_test_filt\nanalyse_results = pd.DataFrame({'code': X_test_filt.code, 'results': results})\nanalyse_result_gb = analyse_results.groupby(by='code').mean()\ncity = table_borough.area.drop_duplicates()\ncity.index = range(1,34)\nanalyse_result_gb['city'] = city.loc[analyse_result_gb.index]\nanalyse_result_gb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# graph of results\n\nplt.scatter(y=y_test_filt, x= range(len(preds)), label='Prices', c='b')\nplt.scatter(y=preds, x=range(len(preds)), label='Predicted prices', c='r')\n\nplt.title('graph of the price vs predicted prices')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Most important features\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=0).fit(X_valid, y_valid)\ntable_w = eli5.explain_weights_dfs(perm, top=None, feature_names = X_valid.columns.tolist())['feature_importances']\ncol_unused = table_w.iloc[[41,42],0].values\nnew_X_train = X_train_filt.drop(col_unused, axis=1)\nnew_X_valid = X_valid.drop(col_unused, axis=1)\ntable_w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How the features influence the outputs\nfrom pdpbox import pdp, get_dataset, info_plots\n\ndef isolate(feat):\n    feature_names = [i for i in X_train_filt.columns if X_train_filt[i].dtype in [np.int64]]\n    # Create the data that we will plot\n    pdp_goals = pdp.pdp_isolate(model=model, dataset=X_valid, model_features=X_valid.columns, feature=feat)\n\n    # plot it\n    pdp.pdp_plot(pdp_goals, feat)\n    plt.show()\n\nisolate('area_size_x')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isolate('recycling_pct_x')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isolate('number_of_jobs_x')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}