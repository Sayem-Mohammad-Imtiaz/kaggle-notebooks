{"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","version":"3.6.3","file_extension":".py","nbconvert_exporter":"python"}},"cells":[{"cell_type":"code","source":"%reset -f\nimport time                   # To time processes\nimport warnings               # To suppress warnings\n\nimport numpy as np            # Data manipulation\nimport pandas as pd           # Dataframe manipulation\nimport matplotlib.pyplot as plt                   # For graphics\n\nfrom sklearn import cluster, mixture              # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset\n\nimport os                     # For os related operations\nimport sys                    # For data size\n\nX= pd.read_csv(\"../input/2017.csv\", header=0)\nX.columns.values\nX.shape\nX = X.iloc[:, 2:]\n# Ignore Country and Happiness_Rank columns\nX.head(2)\nX.dtypes\nX.info\n#1. Normalize dataset\nss = StandardScaler()\nss.fit_transform(X)\n\n#2.1 Begin Clustering \nn_clusters = 2\n\n#2.2 KMeans\nkm = cluster.KMeans(n_clusters =n_clusters )\nkm_result = km.fit_predict(X)\nplt.subplot(4, 2, 1)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=km_result)\n\n#2.3 Mean Shift\nbandwidth = 0.1\nms = cluster.MeanShift(bandwidth=bandwidth)\nms_result = ms.fit_predict(X)\nplt.subplot(4, 2, 2)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=ms_result)\n\n#2.4 Mini Batch K-Means\ntwo_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\ntwo_means_result = two_means.fit_predict(X)\nplt.subplot(4, 2, 3)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c= two_means_result)\n\n#2.5 Spectral clustering\nspectral = cluster.SpectralClustering(n_clusters=n_clusters)\nsp_result= spectral.fit_predict(X)\nplt.subplot(4, 2, 4)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=sp_result)\n\n#3. DBSCAN\n\neps = 0.3\ndbscan = cluster.DBSCAN(eps=eps)\ndb_result= dbscan.fit_predict(X)\nplt.subplot(4, 2, 5)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5], c=db_result)\n\n# 4. Affinity Propagation\ndamping = 0.9\npreference = -200\naffinity_propagation = cluster.AffinityPropagation(damping=damping, preference=preference)\naffinity_propagation.fit(X)\nap_result = affinity_propagation .predict(X)\nplt.subplot(4, 2, 6)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=ap_result)\n\n#5. Birch\nbirch = cluster.Birch(n_clusters=n_clusters)\nbirch_result = birch.fit_predict(X)\nplt.subplot(4, 2, 7)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=birch_result)\n\n#6. Gaussian Mixture modeling\ngmm = mixture.GaussianMixture( n_components=n_clusters, covariance_type='full')\ngmm.fit(X)\ngmm_result = gmm.predict(X)\nplt.subplot(4, 2, 8)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=gmm_result)\n","execution_count":null,"metadata":{"_cell_guid":"129f712e-52d0-42fd-a955-90d3bb3d9fbf","_uuid":"24ab8ad7f83276aaf858fb5df4893263134f57ae","scrolled":true},"outputs":[]}],"nbformat":4}