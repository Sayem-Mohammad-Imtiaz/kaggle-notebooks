{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"* Hello! We will practice about Linear Regression for myself-improvement. I hope this notebook will be useful to you. Lets get start :)","metadata":{}},{"cell_type":"markdown","source":"# Contents\n1. [Load and Check Data](#1)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nfrom collections import Counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n## Load and Check Data","metadata":{}},{"cell_type":"code","source":"y_2018 = pd.read_csv(\"../input/world-happiness/2018.csv\")\ny_2019 = pd.read_csv(\"../input/world-happiness/2019.csv\")\n\ndata = pd.concat([y_2018,y_2019], sort = False)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Variable Description","metadata":{}},{"cell_type":"markdown","source":"1. Overall rank: Overall rank: Ranking of countries by happiness level\n1. Country or region: Country or region names\n1. Score: Happiness scores\n1. GDP per capita: Value representing the country's income and expense levels\n1. Social support\n1. Healthy life expectancy\n1. Freedom to make life choices\n1. Generosity\n1. Perceptions of corruption ","metadata":{}},{"cell_type":"code","source":"data.describe().T #statistical information about the data set ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Lets change the column names for convenience.","metadata":{}},{"cell_type":"code","source":"data.rename(columns={\n    \"Overall rank\": \"rank\",\n    \"Country or region\": \"country\",\n    \"Score\": \"score\",\n    \"GDP per capita\": \"gdp\",\n    \"Social support\": \"social\",\n    \"Healthy life expectancy\": \"healthy\",\n    \"Freedom to make life choices\": \"freedom\",\n    \"Generosity\": \"generosity\",\n    \"Perceptions of corruption\": \"corruption\"\n},inplace = True)\ndel data[\"rank\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Value","metadata":{}},{"cell_type":"code","source":"data.columns[data.isnull().any()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data[\"corruption\"].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_data_corruption = data[data[\"score\"] > 6.774].mean().corruption\ndata.loc[data[\"corruption\"].isnull(),[\"corruption\"]] = avg_data_corruption\ndata[data[\"corruption\"].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation","metadata":{}},{"cell_type":"code","source":"df = data.copy()\ndf = df.select_dtypes(include=[\"float64\",\"int64\"])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]\nsns.heatmap(df[column_list].corr(), annot = True, fmt = \".2f\") #annot=True dersek minik karelerin içinde coorelation skorlarını da görmüş oluruz (daha kolay anlayabilmek için)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.factorplot(x = \"score\", y = \"gdp\", data = df, kind = \"bar\", size = 5)\ng.set_ylabels(\"GDP per capita\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in column_list:\n    sns.boxplot(x = df[col])\n    plt.xlabel(col)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We observed outlier detection with boxplot in corruption and social features. But we can observed this features with outlier detection.  ","metadata":{}},{"cell_type":"code","source":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"lower bound is\" + str(lower_bound))\nprint(\"upper bound is\" + str(upper_bound))\nprint(\"Q1: \", Q1)\nprint(\"Q3: \", Q3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[detect_outliers(df,[\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for corruption\ndf_table = df[\"corruption\"]\n\nQ1 = df_table.quantile(0.25)\nQ3 = df_table.quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"lower bound is \" + str(lower_bound))\nprint(\"upper bound is \" + str(upper_bound))\nprint(\"Q1: \", Q1)\nprint(\"Q3: \", Q3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_vector = (df_table < (lower_bound)) | (df_table > (upper_bound))\noutliers_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_vector = df_table[outliers_vector]\noutliers_vector.index.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_table = data.copy()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values] = df_table[\"corruption\"].mean()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_table","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### score -- gdp","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = \"gdp\", y = \"score\", data = df_table, kind = \"reg\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nX = data[[\"gdp\"]]\nX.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data[[\"score\"]]\ny.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg = LinearRegression()\nmodel = reg.fit(X,y)\nprint(\"intercept: \", model.intercept_)\nprint(\"coef: \", model.coef_)\nprint(\"rcore. \", model.score(X,y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* gdp feature used here describes 63% of the data.","metadata":{}},{"cell_type":"code","source":"# prediction\nplt.figure(figsize = (10,8))\ng = sns.regplot(x = data[\"gdp\"], y = data[\"score\"], ci = None, scatter_kws = {'color':'r','s':9})\ng.set_title(\"Model Equation\")\ng.set_xlabel(\"gdp\")\ng.set_ylabel(\"score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* If gdp score is 1.50 , happines score is 6.74","metadata":{}},{"cell_type":"code","source":"model.predict([[1.50]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdb_list = [[0.25],[0.50],[0.75],[1.00],[1.25],[1.50]]\nmodel.predict(gdb_list)\nfor g in gdb_list:\n    print(\"The happiness value of the country with a gdp value of \",g,\": \",model.predict([g]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### score -- social","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = \"social\", y = \"score\", data = df_table, kind = \"reg\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's create a class and make the job easier.","metadata":{}},{"cell_type":"code","source":"def linear_reg(col,text,prdctn):\n    \n    sns.jointplot(x=col,y=\"score\",data=df_table,kind=\"reg\")\n    plt.show()\n    \n    X = data[[col]]\n    y = data[[\"score\"]]\n    reg = LinearRegression()\n    model = reg.fit(X,y)\n    \n    # prediction\n    plt.figure(figsize=(12,6))\n    g = sns.regplot(x=data[col],y=data[\"score\"],ci=None,scatter_kws = {'color':'r','s':9})\n    g.set_title(\"Model Equation\")\n    g.set_ylabel(\"score\")\n    g.set_xlabel(col)\n    plt.show()\n    \n    print(text,\": \", model.predict([[prdctn]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_reg(\"social\",\"The happiness value of the country whose sociability value is 2:\",2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### score -- healthy","metadata":{}},{"cell_type":"code","source":"linear_reg(\"healthy\",\"The happiness value of the country whose healthiest value is 1.20:\",1.20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### score -- freedom","metadata":{}},{"cell_type":"code","source":"linear_reg(\"freedom\",\"The happiness value of the country whose freedom value is 1.20:\",1.20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiple Linear Regression","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sms\n\nX = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\n\n# OLS (dependent,independent)\nlm = sms.OLS(y,X)\nmodel = lm.fit()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* R-squared   :   Percentages of independent variables that explain the change in dependent variables.\n* F-statistic :   Expresses the significance of the model.\n* Coef        :   Refers to coefficients.\n* Std Err     :   Standard errors.","metadata":{}},{"cell_type":"markdown","source":"#### Create model with sckit learn","metadata":{}},{"cell_type":"code","source":"lm = LinearRegression()\nmodel = lm.fit(X,y)\nprint(\"constant: \", model.intercept_)\nprint(\"coefficient: \", model.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTION\n# Score = 0.929921*gdp + 1.06504217*social + 0.94321492*healthy + 1.40426054*freedom + 0.52070628*generosity + 0.88114008*corruption\n\nnew_data = [[1],[2],[1.25],[1.75],[1.50],[0.75]]\nnew_data = pd.DataFrame(new_data).T\nnew_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(new_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating the amount of error\n\nfrom sklearn.metrics import mean_squared_error\n\nMSE = mean_squared_error(y,model.predict(X))\nRMSE = np.sqrt(MSE)\n\nprint(\"MSE: \", MSE)\nprint(\"RMSE: \", RMSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Linear & Multiple Linear Regression - Model Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(\"Training error: \", np.sqrt(mean_squared_error(y_train, model.predict(X_train))))\nprint(\"Test Error: \", np.sqrt(mean_squared_error(y_test, model.predict(X_test))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Every time we change the random_state value we defined at first, a different result is returned. We need to find out which of these returns the best result. For this we need to do the following.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvs_avg_mse = np.mean(-cross_val_score(model, X_train, y_train, cv = 20, scoring = \"neg_mean_squared_error\"))\ncvs_avg_rmse = np.sqrt(cvs_avg_mse)\n\nprint(\"Cross Val Score MSE : \",cvs_avg_mse)\nprint(\"Cross Val Score RMSE : \",cvs_avg_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge Regression\n<br>\n* The aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n<br>\n* It is resistant to over learning.\n* It is biased but its variance is low.\n* It is better than OLS when there are too many parameters.\n* Builds a model with all variables. It does not exclude the unrelated variables from the model, it approximates its coefficients to zero.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nridge_model = Ridge(alpha = 0.1).fit(X_train, y_train)\nridge_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model.intercept_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lambdas = 10**np.linspace(10,-2,100)*0.5 # Creates random numbers\nridge_model =  Ridge()\ncoefs = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefs.append(ridge_model.coef_)\n    \nax = plt.gca()\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge Regression - Prediction","metadata":{}},{"cell_type":"code","source":"ridge_model = Ridge().fit(X_train,y_train)\n\ny_pred = ridge_model.predict(X_train)\n\nprint(\"predict: \", y_pred[0:10])\nprint(\"real: \", y_train[0:10].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RMSE = np.mean(mean_squared_error(y_train,y_pred))\nprint(\"train error: \", RMSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Verified_RMSE = np.sqrt(np.mean(-cross_val_score(ridge_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nprint(\"Verified_RMSE: \", Verified_RMSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test error\ny_pred = ridge_model.predict(X_test)\nRMSE = np.mean(mean_squared_error(y_test, y_pred))\nprint(\"test error: \", RMSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge Model -- Model Tuning","metadata":{}},{"cell_type":"code","source":"ridge_model = Ridge(10).fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model = Ridge(30).fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model = Ridge(90).fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can find out which value will work better by trial and error. But with the method we will use below, we can find the most appropriate value more easily and quickly.","metadata":{}},{"cell_type":"code","source":"lambdas1 = 10**np.linspace(10,-2,100)\nlambdas2 = np.random.randint(0,1000,100)\n\nridgeCV = RidgeCV(alphas = lambdas1,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridgeCV.alpha_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final model\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train, y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for lambdas2\nridgeCV = RidgeCV(alphas = lambdas2, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nridgeCV.fit(X_train, y_train)\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train, y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lasso Regression","metadata":{}},{"cell_type":"markdown","source":"# Lasso Regression -- Model","metadata":{}},{"cell_type":"code","source":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\nlasso_model = Lasso().fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"intercept: \", lasso_model.intercept_)\nprint(\"coef: \", lasso_model.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coefficients for different lambda values\n\nalphas = np.random.randint(0,10000,10)\nlasso = Lasso()\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train,y_train)\n    coefs.append(lasso.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.gca()\nax.plot(alphas,coefs)\nax.set_xscale(\"log\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression - Prediction ","metadata":{}},{"cell_type":"code","source":"lasso_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_model.predict(X_train)[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_model.predict(X_test)[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = lasso_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression - Model Tuning","metadata":{}},{"cell_type":"code","source":"lasso_cv_model = LassoCV(cv = 10, max_iter = 100000).fit(X_train, y_train)\nlasso_cv_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_cv_model.alpha_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_tuned = Lasso().set_params(alpha= lasso_cv_model.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ElasticNet Regression","metadata":{}},{"cell_type":"code","source":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nenet_model = ElasticNet().fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_model.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_model.intercept_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prediction\nenet_model.predict(X_train)[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_model.predict(X_test)[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = enet_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression Model Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_cv_model = ElasticNetCV(cv = 10, random_state = 0).fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_cv_model.alpha_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_cv_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enet_tuned = ElasticNet(alpha = enet_cv_model.alpha_).fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = enet_tuned.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}