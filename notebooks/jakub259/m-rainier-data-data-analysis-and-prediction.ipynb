{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mount Rainier data analysis and Success Rate prediction"},{"metadata":{},"cell_type":"markdown","source":"Our goal is to find some patterns in the data and then to try different ML models and experiment a little bit with PCA and TSNE reduction"},{"metadata":{},"cell_type":"markdown","source":"# Sections:\n1. Understanding data, data visualisation and cleaning\n2. Preprocessing data\n3. ML algorithms comparison\n4. Final Evaluation and conclution"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I really hate these warnings about changes in future versions of skicit-learn so we will take care of them\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's import useful packages for start\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading datasets\n# Weather\ndf_weather = pd.read_csv(\"../input/mount-rainier-weather-and-climbing-data/Rainier_Weather.csv\")\ndf_weather.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Climbing\ndf_climbing = pd.read_csv(\"../input/mount-rainier-weather-and-climbing-data/climbing_statistics.csv\")\ndf_climbing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's merge these datasets\ndf = df_climbing.merge(df_weather, on=\"Date\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look for null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Great, no null values\n# Let's make feature called Class, which tells us if the overall attempt was a success or not (0 for unsuccessful, 1 for successful)\ndf[\"Class\"] = 0\n\nfor index in df.index:\n    if df.iloc[index, df.columns.get_loc(\"Succeeded\")] != 0:\n        df[\"Class\"].loc[index] = 1\n    else:\n        df[\"Class\"].loc[index] = 0\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look on a class plot\nf, ax = plt.subplots(1, 1, figsize=(10, 6))\nsns.countplot(\"Class\", data=df, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It's clear that we have more successful attempts then unsuccessful\n# Let's which routes are more prefered\nf, ax = plt.subplots(1, 1, figsize=(10, 6))\ndf.Route.value_counts().plot(kind=\"bar\", ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's see some correlations between weather features\nf, ax = plt.subplots(1, 1, figsize=(10, 8))\n\ncorr = df_weather.corr()\nsns.heatmap(corr, cmap=\"coolwarm_r\", annot=True, ax=ax)\nax.set_ylim(len(df_weather.columns)-1, 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that Temperature is high positive correlated to Solare Radiation. That only makes sense.\n# Let's now plot correlation of everything to success rate\nf, ax = plt.subplots(1, 1, figsize=(10, 8))\n\ncorr_succ = df.corr()\nsns.heatmap(corr_succ, cmap=\"coolwarm_r\", annot=True, ax=ax)\n\nax.set_ylim(len(df.columns)-2, 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The only positive effect from all features on success percentage has temperature and Solare Radiation\n# On the other hand there is only Wind Speed worth noticing (negative correlation).\n# We have a lot of dates so let's group them into months\n\nimport calendar\n\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf[\"Month\"] = 0\ndf[\"Month\"] = df[\"Date\"].dt.month\ndf.Month = df.Month.apply(lambda x: calendar.month_abbr[x])\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unfortunatlly i didn't succeed in placing months into right order, but it doesn't matter that much\n\n# Let's see what temperature has to do with success rate\nf, ax = plt.subplots(1, 1, figsize=(10, 8))\n\ndf[\"Success Percentage\"] = df[\"Success Percentage\"] * 100\n\nsns.lineplot(x=\"Month\", y=\"Temperature AVG\", data=df, ax=ax)\nsns.lineplot(x=df[\"Month\"], y=\"Success Percentage\", data=df, ax=ax)\n\nax.set_ylabel(\"\")\nf.legend(labels=[\"Success Rate\", \"Temperature\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can see that almost everytime when temperature is high the success rate is also high and vica versa\n# December is usually the coldest so the success rate should be lower there, because the cold is making the mountain harder to climb\n# On the other hand the Jul and Jun are the warmest months so you get the idea","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next plot should be success rate vs solare radiation within months\nf, ax= plt.subplots(1, 1, figsize=(10, 8))\nsns.lineplot(x=\"Month\", y=\"Solare Radiation AVG\", data=df, ax=ax)\nsns.lineplot(x=\"Month\", y=\"Success Percentage\", data=df, ax=ax)\nf.legend(labels=[\"Solare Radiation\", \"Success Rate\"])\nax.set_label(\"\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can see that it's almost the same as the other plot. It's because in the weather section solare rad. and temperature are highly positively correlated\n\n#Let's plot humidity as well, because it's the feature with the highest negative correlation\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\nsns.lineplot(x=\"Month\", y=\"Wind Speed Daily AVG\", data=df, ax=ax)\nsns.lineplot(x=\"Month\", y=\"Success Percentage\", data=df, ax=ax)\nax.set_ylabel(\"\")\nfig.legend(labels=[\"Wind Speed\", \"Success Rate\"])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It's clear that wind speed curve is going against success rate curve\n\n# Well, we have now better insight into this problem\n# We see a lot of other correlations between weather itself, but it's not necessary to plot that as well\n# We are only interested in Success Percentage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"Success Percentage\"] / 100\ndf.drop([\"Class\", \"Date\", \"Success Percentage\"], axis=1, inplace=True)\nX = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"4\">Data **Scaling**, **Encoding** and **Normalization**</font> \n  :\n  \nFirst we have to encode labels in Route and Month features, because ML algorithm is unable to process Categorical features without encoding.\nWe will use LabelEncoder first to encode labels into numbers. Then we will use OneHotEncoder to create a binary columns for each category (for each category\nwe will need as many columns as we have unique values in these categories). Then scaling etc.\nWe will be following this order:\n    1. LabelEncoder\n    2. OneHotEncoder\n    3. RobustScaler (scaling)\n    4. PCA and TSNE reduction (just for model comparison, we wont use it later)\n    5. Model comparison with non-normalized data (we will choose the best one)\n    6. Zscore normalization (There will be an improvement in final accuracy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nX[\"Route_encoded\"] = label_encoder.fit_transform(X[\"Route\"])\nX[\"Month_encoded\"] = label_encoder.fit_transform(X[\"Month\"])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nhot_encoder = OneHotEncoder()\n\nX_route = hot_encoder.fit_transform(X[\"Route_encoded\"].values.reshape(-1, 1)).toarray()\nX_month = hot_encoder.fit_transform(X[\"Month_encoded\"].values.reshape(-1, 1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have to create columns for onehot encoded values\ndf_route = pd.DataFrame(X_route, columns=[\"Route_\"+str(int(i)) for i in range(X_route.shape[1])])\ndf_month = pd.DataFrame(X_month, columns=[\"Month_\"+str(int(i)) for i in range(X_month.shape[1])])\n\ndf_list = [\n    df_route,\n    df_month,\n    df,\n]\n\nX = pd.concat(df_list, axis=1)\n\nX.drop([\"Route\", \"Month\", \"Route_encoded\", \"Month_encoded\"], axis=1, inplace=True)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have our encoded data, the next step is scaling\nfrom sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nfor column in X.columns[X.columns.get_loc(\"Attempted\"):]:\n    X[column] = scaler.fit_transform(X[column].values.reshape(-1, 1))\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Float to int (categorical values only)\nfor column in X.columns[:X.columns.get_loc(\"Attempted\")]:\n    X[column] = X[column].astype(int)\n        \nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA and TSNE reduction\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\npca = PCA(n_components=2, random_state=42)\ntsne = TSNE(n_components=2, random_state=42)\n\nX_reduced_pca = pca.fit_transform(X.values)\nX_reduced_tsne = tsne.fit_transform(X.values)\nX_reduced_pca.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing models\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n# Testing classifiers one after another\nclassifiers = {\n    \"Linear Regression:\": LinearRegression(),\n    \"Random Forest:\": RandomForestRegressor(),\n    \"Support Vector Regressor:\": SVR(),\n    \"Extra Trees Regressor\": ExtraTreesRegressor(),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training with X_pca data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_reduced_pca, y, test_size=0.2, random_state=42)\n\nsummary = []\nresult = []\nfor key, classifier in classifiers.items():\n    mse_score= cross_val_score(classifier, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_score = np.sqrt(-mse_score)\n    summary += [classifier.__class__.__name__]\n    result += [round(rmse_score.mean(), 4)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Results represents success RMSE(root mean squared error) of success rate feature (values from 0 to 1)\nprint(\"PCA REDUCTION\")\nfor index in range(4):\n    print(\"Classifier: {} has score: {}\".format(summary[index], result[index]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that our data are kind of complex (they have a lot of features) so I tried PCA and TSNE reduction. We can see that Linear model is the best, but when it comes to data without any\nreduction, LinearRegressor will go crazy"},{"metadata":{"trusted":true},"cell_type":"code","source":"#training with X_tsne data\nX_train, X_test, y_train, y_test = train_test_split(X_reduced_tsne, y, test_size=0.2, random_state=42)\n\nsummary = []\nresult = []\nfor key, classifier in classifiers.items():\n    mse_score= cross_val_score(classifier, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_score = np.sqrt(-mse_score)\n    summary += [classifier.__class__.__name__]\n    result += [round(rmse_score.mean(), 4)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TSNE REDUCTION\")\nfor index in range(4):\n    print(\"Classifier: {} has score: {}\".format(summary[index], result[index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training with normal data with out reducion\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsummary = []\nresult = []\nfor key, classifier in classifiers.items():\n    mse_score= cross_val_score(classifier, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_score = np.sqrt(-mse_score)\n    summary += [classifier.__class__.__name__]\n    result += [round(rmse_score.mean(), 4)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No Reduction:\")\nfor index in range(4):\n    print(\"Classifier: {} has score: {}\".format(summary[index], result[index]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yep, this is what I was talking about. LinearRegression is now accurate as hell! Error 6310034155200% No big deal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ZScore scaling \nfrom scipy.stats import zscore\n\nX_zscore = X.apply(zscore)\nX_zscore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting zscore normalized data\nX_train, X_test, y_train, y_test = train_test_split(X_zscore, y, test_size=0.2, random_state=42)\n\n# Let's how is our model good in predicting training data\n\ntraining_score = cross_val_score(ExtraTreesRegressor(), X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)\nrmse = np.sqrt(-training_score)\nprint(\"Our RandomForestRegressor has the final RMSE(root mean squared error):  \", round(rmse.mean(), 6)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wow that's huge error, that means our model is overfitting. It will get better in the future.\n# I would like to see learning curves of RandomForestRegressor and ExtraTreesRegressor to see which is better in terms of not overfitting\nfrom sklearn.model_selection import learning_curve, ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest learning curve\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(RandomForestRegressor(), \"Random Forest Regressor\", X_train, y_train, cv=cv, n_jobs=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra Trees learning curve\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(ExtraTreesRegressor(), \"Extra Trees Regressor\", X_train, y_train, cv=cv, n_jobs=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that both models are overfitting (huge gap between training and validation score)\n# We will prefer RandomForestRegressor, because it overfits less than Extra Trees\n\n# I tried to tune the model, but the best performace i could reach was with model's default hyper parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final valuation on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nforest = RandomForestRegressor()\nforest.fit(X_train, y_train)\n\nfinal_pred = forest.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_pred)\nfinal_rmse = np.sqrt(final_mse)\nprint(\"Our RandomForestRegressor has the final RMSE(root mean squared error):  \", round(final_rmse, 6)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nMost Success Rate values are between 0 % to 66.6 % so error roughtly 6 % is kind of huge, but we can do nothing about it as we have small dataset. I didn't optimized my RandomForestRegressor, because the best score i can achieve is with forest model's default hyper parameters.\n\nI will be greatful for every feedback from you guys. Thank you!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}