{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine Quality Analysis and Model parameter tuning."},{"metadata":{},"cell_type":"markdown","source":"#### This work includes, basic data visualization, Modeling with XGB and also parameter tuning. After this we will come to know which set of parameters will give best accuracy after tuning. Here we have used Grid Search, Randomized search and Bayes Optimization techniques to tune the parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clear ipython memory\n%reset -f\n\n#Data manipulation and plotting modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport seaborn as sns\n\n#Call Standarad Scaler\nfrom sklearn.preprocessing import StandardScaler as ss\n\n#Cal PCA for Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\n#Data splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom bayes_opt import BayesianOptimization\n\n#Call Modeling module, we will be using XBG for modeling.\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import precision_recall_fscore_support\n\n#Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n\n#Model evaluation metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n# This is needed for Bayes optimization takes an estimator, performs cross-validation\n# and gives out average score\nfrom sklearn.model_selection import cross_val_score\n\n#Find feature importance of ANY BLACK BOX estimator\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n\n#Misc\nimport time\nimport os\nimport gc\nimport random\nfrom scipy.stats import uniform\n\n#Set option to dislay many rows\npd.set_option('display.max_columns', 100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set working directory and set the filename\nprint(os.listdir(\"../input\"))\nos.listdir()\ntr_f = \"../input/winequalityN.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total number of rows. Here I am reading 99% rows, just wanted to show we can also read part of data.\ntotal_lines = 6498\nnum_lines = 0.99 * total_lines    # 99% of data\nnum_lines\np = num_lines/total_lines\ndf = pd.read_csv(tr_f,header=0, # First row is header-row\n         skiprows=lambda i: (i>0) and (random.random() > p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Below is the code to check and remove the any NaN values present in the rows.\nprint (df.isnull().sum())\ndata=df.dropna()\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic data visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns.values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes.value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Draw some graphs to understand the data more."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot to show quality and fixed acidity\nsns.boxplot(x='quality',y='fixed acidity', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot to show how many white and red wine types are there.\nsns.countplot(x='type',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bar plot to show the quality and fixed acidity\nsns.barplot(x = 'quality', y = 'fixed acidity', data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heat map to show the correlation between the feature columns.\nsns.heatmap(data.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scatter plot to show the qulity and fixed acidity\nsns.scatterplot(\"quality\",\"fixed acidity\",data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplot to show quality and volatile acidty with type as hue parameter.\nsns.boxplot(x=\"quality\", y=\"volatile acidity\",hue='type',data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling and parameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the feature and target columns\nX = data.iloc[ :, 1:13]\nprint (X.head(2))\ny = data.iloc[ : , 0]\ny.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Map the target data to '1' and '0'\ny = y.map({'white':1, 'red' : 0})\ny.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset into train and validation parts\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.35,\n                                                    shuffle = True\n                                                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (X_train.shape)        \nprint (X_test.shape)         \nprint (y_train.shape)        \nprint (y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pipelining\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        \n            )\n            ]\npipe_xg = Pipeline(steps_xg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'xg__learning_rate':  [0, 1],\n              'xg__n_estimators':   [50,  100],  \n              'xg__max_depth':      [3,5],\n              'pca__n_components' : [5,7] }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =2 ,             # No of folds\n                   verbose =2,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters?\n                                       # Those which maximise auc\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nclf.fit(X_train, y_train)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best score: {clf.best_score_} \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best parameter set {clf.best_params_}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy: {accuracy * 100.0}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Randomized Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,100),\n              'xg__max_depth':      range(3,5),\n              'pca__n_components' : range(5,7)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          \n                                            \n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          \n                        cv = 2              \n                                             \n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run random search for 25 iterations. \nstart = time.time()\nrs.fit(X_train, y_train)\nend = time.time()\n(end - start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best score: {rs.best_score_} \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"Best parameter set: {rs.best_params_} \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rs.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy: {accuracy * 100.0}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayes Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"para_set = {\n           'learning_rate':  (0, 1),                 \n           'n_estimators':   (50,100),               \n           'max_depth':      (3,5),               \n           'n_components' :  (5,7)                \n            }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    pipe_xg1 = make_pipeline (ss(),                        \n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # take the average of all results\n\n\n    return cv_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nxgBO.maximize(init_points=5,    \n               n_iter=25,        \n               )\nend = time.time()\n(end-start)/60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO.res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgBO.max","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model fitting using best parameters from above tuning techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search\nxg_gs = XGBClassifier(learning_rate = clf.best_params_['xg__learning_rate'],\n                    max_depth = clf.best_params_['xg__max_depth'],\n                    n_estimators=clf.best_params_['xg__n_estimators'])\n#Randomized search\nxg_rs = XGBClassifier(learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__n_estimators'])\n#Bayes Optimization\nxg_bo = XGBClassifier(learning_rate = xgBO.max['params']['learning_rate'],\n                    max_depth = int(xgBO.max['params']['max_depth']),\n                    n_estimators= int(xgBO.max['params']['n_estimators']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the data using X_Train and y_train\nxg_gs1 = xg_gs.fit(X_train,y_train)\nxg_rs1 = xg_rs.fit(X_train,y_train)\nxg_bo1 = xg_bo.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xg_gs = xg_gs1.predict(X_test)\ny_pred_xg_rs = xg_rs1.predict(X_test)\ny_pred_xg_bo = xg_bo1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xg_gs_prob = xg_gs1.predict_proba(X_test)\ny_pred_xg_rs_prob = xg_rs1.predict_proba(X_test)\ny_pred_xg_bo_prob = xg_bo1.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (accuracy_score(y_test,y_pred_xg_gs))\nprint (accuracy_score(y_test,y_pred_xg_rs))\nprint (accuracy_score(y_test,y_pred_xg_bo))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_pred_xg_gs)\nconfusion_matrix(y_test,y_pred_xg_rs)\nconfusion_matrix(y_test,y_pred_xg_bo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr_xg_gs, tpr_xg_gs, thresholds = roc_curve(y_test,\n                                 y_pred_xg_gs_prob[: , 1],\n                                 pos_label= 1\n                                 )\nfpr_xg_rs, tpr_xg_rs, thresholds = roc_curve(y_test,\n                                 y_pred_xg_rs_prob[: , 1],\n                                 pos_label= 1\n                                 )\nfpr_xg_bo, tpr_xg_bo, thresholds = roc_curve(y_test,\n                                 y_pred_xg_bo_prob[: , 1],\n                                 pos_label= 1\n                                 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate the Precision, Recall and F1 Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"p_xg_gs,r_xg_gs,f_xg_gs,_ = precision_recall_fscore_support(y_test,y_pred_xg_gs)\np_xg_rs,r_xg_rs,f_xg_rs,_ = precision_recall_fscore_support(y_test,y_pred_xg_rs)\np_xg_bo,r_xg_bo,f_xg_bo,_ = precision_recall_fscore_support(y_test,y_pred_xg_bo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate the AUC(Area Under the ROC Curve)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print (auc(fpr_xg_gs,tpr_xg_gs))\nprint (auc(fpr_xg_rs,tpr_xg_rs))\nprint (auc(fpr_xg_bo,tpr_xg_bo))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below is the plotting the ROC curve for all the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))          # Create window frame\nax = fig.add_subplot(111)   # Create axes\n# 9.2 Also connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")   # Dashed diagonal line\n# 9.3 Labels etc\nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for multiple tuning methods')\n# 9.4 Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n# 9.5 Plot each graph now\nax.plot(fpr_xg_gs, tpr_xg_gs, label = \"xg_gs\")\nax.plot(fpr_xg_rs, tpr_xg_rs, label = \"xg_rs\")\nax.plot(fpr_xg_bo, tpr_xg_bo, label = \"xg_bo\")\n# 9.6 Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}