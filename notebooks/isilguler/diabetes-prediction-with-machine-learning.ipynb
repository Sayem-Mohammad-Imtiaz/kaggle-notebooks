{"cells":[{"metadata":{},"cell_type":"markdown","source":"## WHAT IS DIABETES ?\nDiabetes mellitus, commonly known as diabetes, is a metabolic disease that causes high blood sugar. The hormone insulin moves sugar from the blood into your cells to be stored or used for energy. With diabetes, your body either doesn’t make enough insulin or can’t effectively use the insulin it does make. And also, untreated high blood sugar from diabetes can damage your nerves, eyes, kidneys, and other organs."},{"metadata":{},"cell_type":"markdown","source":"## CONTEXT\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nThis dataset consists several variables : \n- **Pregnancies**: Number of times pregnant\n- **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n- **BloodPressure**: Diastolic blood pressure (mm Hg)\n- **SkinThickness**: Triceps skin fold thickness (mm)\n- **Insulin**: 2-Hour serum insulin (mu U/ml)\n- **BMI**: Body mass index (weight in kg/(height in m)^2)\n- **DiabetesPedigreeFunction**: Diabetes pedigree function\n- **Age**: Age (years)\n- **Outcome**: Class variable (0 or 1)"},{"metadata":{},"cell_type":"markdown","source":"***In this dataset, \"Outcome\" variable is the target variable of the dataset. Our object here is to predict whether patients with certain values have diabetes by creating a machine learning model. Also, if the class value is 1, diabetes indicates that the test result is positive.***"},{"metadata":{},"cell_type":"markdown","source":"## 1) EXPLORATORY DATA ANALYSIS"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Required libraries\nimport numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nimport warnings\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\nwarnings.simplefilter(action=\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the dataset and displaying top 5 observation unit\ndf = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe([0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Outcome.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Outcome\"].value_counts()*100/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Outcome.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The histograms of the all variables\ndf.hist(figsize = (15,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix\nk = 9\ncols = df.corr().nlargest(k, 'Outcome')['Outcome'].index\ncm = df[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'magma')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Outcome\").agg({\"Pregnancies\":\"mean\",\"Age\":\"mean\",\"Insulin\":\"mean\",\"Glucose\":\"mean\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) DATA PREPROCESSING"},{"metadata":{},"cell_type":"markdown","source":"- MISSING VALUE ANALYSIS"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def median_target(var):   \n    temp = df[df[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp\n\ncolumns = df.columns\ncolumns = columns.drop(\"Outcome\")\nfor i in columns:\n    median_target(i)\n    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NaN values according to target variable were filled with median values. And so, we got rid of missing values.\nimport missingno as msno\nmsno.bar(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- OUTLIER ANALYSIS"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.25)\n    quartile3 = dataframe[variable].quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_outliers(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    if dataframe[(dataframe[variable] < low_limit) | (dataframe[variable] > up_limit)].any(axis=None):\n        print(variable, \"yes\")\n    else: \n        print(variable, \"no\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_with_thresholds(dataframe, numeric_columns):\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns: \n    has_outliers(df,col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_with_thresholds(df, df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    has_outliers(df, col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) FEATURE ENGINEERING\n\nIn this section, we aim to create new variables based on some independent variables in the data set. We aim to increase the prediction success of the model with the created variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"NewBMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\n\ndf[\"NewBMI\"] = NewBMI\n\ndf.loc[df[\"BMI\"] < 18.5, \"NewBMI\"] = NewBMI[0]\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] <= 24.9), \"NewBMI\"] = NewBMI[1]\ndf.loc[(df[\"BMI\"] > 24.9) & (df[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[2]\ndf.loc[(df[\"BMI\"] > 29.9) & (df[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[3]\ndf.loc[(df[\"BMI\"] > 34.9) & (df[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[4]\ndf.loc[df[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df.Outcome == 1) & (df.Insulin <= 100) , \"Insulin\"] = 70\ndf.loc[(df.Outcome == 0) & (df.Insulin >= 200 ) , \"Insulin\"] = 200\ndf.loc[(df.Outcome == 0) & (df.Glucose >= 175 ) , \"Glucose\"] = 175\ndf.loc[(df.Outcome == 1) & (df.Glucose <= 80 ) , \"Glucose\"] = 80\ndf.loc[(df.Outcome == 0) & (df.Pregnancies >= 13 ) , \"Pregnancies\"] = 13\ndf.loc[(df.Outcome == 0) & (df.DiabetesPedigreeFunction >= 1.3 ) , \"DiabetesPedigreeFunction\"] = 1.3\ndf.loc[(df.Outcome == 0) & (df.BMI >= 50 ) , \"BMI\"] = 50\ndf[\"NEW_g_p\"] = (df.Glucose * df.Pregnancies) \ndf[\"NEW_i_g\"] = (df.Glucose * df.Insulin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['New_Glucose'] = pd.cut(x=df['Glucose'], bins=[0,74,99,139,200],labels = [\"Low\",\"Normal\",\"Overweight\",\"High\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.assign(NewInsulinScore=df.apply(set_insulin, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) ONE-HOT ENCODING\n\nCategorical variables in the data set should be converted into numerical values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=True)\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n    return dataframe, new_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = [col for col in df.columns\n                           if len(df[col].unique()) <= 10\n                      and col != \"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[categorical_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df, new_cols_ohe = one_hot_encoder(df,categorical_columns)\nnew_cols_ohe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\",'NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','New_Glucose_Normal', 'New_Glucose_Overweight', 'New_Glucose_High'], axis = 1)\ncols = X.columns\nindex = X.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nStandardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. \n\n**Robust Scaler** removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Median and interquartile range are then stored to be used on later data using the transform method."},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = RobustScaler().fit(X)\nX = transformer.transform(X)\nX = pd.DataFrame(X, columns = cols, index = index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','New_Glucose_Normal', 'New_Glucose_Overweight', 'New_Glucose_High']]\nX = pd.concat([X,categorical_df], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(\"Outcome\",axis=1)\ny = df[\"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [('LR', LogisticRegression()),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier()),\n          ('RF', RandomForestClassifier()),\n          ('SVR', SVC(gamma='auto')),\n          ('XGBM', XGBClassifier()),\n          ('GBM',GradientBoostingClassifier()),\n          (\"LightGBM\", LGBMClassifier())]\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplot for comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) MODEL TUNING"},{"metadata":{},"cell_type":"markdown","source":"- GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm_model = GradientBoostingClassifier()\n# Model Tuning\ngbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n               \"max_depth\": [3,5, 8, 10],\n               \"n_estimators\": [200, 500, 1000],\n               \"subsample\": [1, 0.5, 0.8]}\ngbm_cv_model = GridSearchCV(gbm_model,\n                            gbm_params,\n                            cv=10,\n                            n_jobs=-1,\n                            verbose=2).fit(X, y)\ngbm_cv_model.best_params_\n# Final Model Installation\ngbm_tuned = GradientBoostingClassifier(**gbm_cv_model.best_params_).fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(gbm_tuned, X, y, cv = 10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = LGBMClassifier()\n# Model Tuning\nlgbm_params = lgbm_params = {\"learning_rate\": [0.01, 0.5, 1],\n                             \"n_estimators\": [200, 500, 1000],\n                             \"max_depth\": [6, 8, 10],\n                             \"colsample_bytree\": [1, 0.5, 0.4 ,0.3 , 0.2]}\nlgbm_cv_model = GridSearchCV(lgb_model,\n                             lgbm_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X, y)\nlgbm_cv_model.best_params_\n# Final Model Installation\nlgbm_tuned = LGBMClassifier(**lgbm_cv_model.best_params_).fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(lgbm_tuned, X, y, cv = 10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Barplots for models"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7) COMPARISON OF FINAL MODELS"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('RF', RandomForestClassifier(random_state = 12345, max_depth = 8, max_features = 7, min_samples_split = 2, n_estimators = 500)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state = 12345, learning_rate = 0.1, max_depth = 5, min_samples_split = 0.1, n_estimators = 100, subsample = 1.0)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345, learning_rate = 0.01,  max_depth = 3, n_estimators = 1000)))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 12345)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8) CONCLUSION"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression, KNN, CART, RF, SVC, XGBM, GB, LightGBM classification machine learning models were used to predict the onset of diabetes within five years in women with Pima Indian heritage who were given medical details about their bodies. The 3 classification models ***(Random Forests, GBM, LightGBM)*** with the highest prediction success were selected and these models were compared again among themselves. As a result of comparing the established models; It was observed that the model with the highest result was ***GBM (0.93)***."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}