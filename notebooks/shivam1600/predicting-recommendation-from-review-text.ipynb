{"cells":[{"metadata":{"id":"Au7Y1ZaGwdOJ","colab_type":"text","_uuid":"c31c4729d35dabb474a989530af3445a93ceb964"},"cell_type":"markdown","source":"# Predicting Recommendation from Review Text : LSA vs Autoencoder vs NN"},{"metadata":{"_uuid":"5f95e212b851fbbb7df4d6f1b53599884b3731be"},"cell_type":"markdown","source":"## Table of Content <a id=\"toc\"></a>\n* [Global Variables](#gv)\n* [1. Data Preprocessing](#data_preprocessing)\n    * [1.1 Importing Data and Separating Data of Our Interest](#1.1)\n    * [1.2 Creating Preprocessing Function and Applying it on Our Data](#1.2)\n    * [1.3 Creating TF-IDF Matrix](#1.3)\n* [2. Apply SVD to TF-IDF Matrix](#apply_svd)\n    * [2.1 Create Term and Document Representation](#2.1)\n    * [2.2 Visulize Those Representation](#2.2)\n* [3 Information Retreival Using LSA](#ir_lsa)\n* [4. Create Model to Predict Recommendation](#4)\n* [5. Train Autoencoder on TF-IDF Matrix](#5)\n* [6. Information Retrieval Using Autoencoder](#6)\n* [7. Predict Recommendation using Encoding of Autoencoder](#7)\n* [8. Use simple NN to predict Recommendation](#8)"},{"metadata":{"trusted":true,"_uuid":"fdcda8e32cccecfea55139961ade8691757b9566"},"cell_type":"code","source":"# Global Variables \nK = 2 # number of components\nquery = 'nice good price'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d09beab03f60b5751a78aac9bbd0d1dbf69a7305"},"cell_type":"markdown","source":"##  1. Data Preprocessing <a id=\"data_preprocessing\"></a>"},{"metadata":{"_uuid":"4a6517b047dad67861a5c67e317d5cb67d0cd237"},"cell_type":"markdown","source":"### 1.1 Importing Data and Separating Data of Our Interest <a id=\"1.1\"></a>"},{"metadata":{"trusted":true,"_uuid":"5139af95b1e8a2e95d6c932feeb1450bb06dd805"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Data filename\ndataset_filename = \"../input/Womens Clothing E-Commerce Reviews.csv\"\n\n# Loading dataset\ndata = pd.read_csv(dataset_filename, index_col=0)\n\n\n# We are reducing the size of our dataset to decrease the running time of code\nlist_of_clothing_id = data['Clothing ID'].value_counts()[:10].index\ny1 = [x == 0 for x in data['Recommended IND']]\ny2 = [x in list_of_clothing_id for x in data['Clothing ID']]\ny3 = [a or b for a,b in zip(y1,y2)]\n\ndatax = data.loc[y3, :]\n\n\n# Delete missing observations for variables that we will be working with\nfor x in [\"Recommended IND\",\"Review Text\"]:\n    datax = datax[datax[x].notnull()]\n\n# Keeping only those features that we will explore\ndatax = datax[[\"Recommended IND\",\"Review Text\"]]\n\n# Resetting the index\ndatax.index = pd.Series(list(range(datax.shape[0])))\n    \nprint('Shape : ',datax.shape)\ndatax.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5ee3c8b515e0fe16b782c06200bfec562df5bc9"},"cell_type":"markdown","source":"### 1.2 Creating Preprocessing Function and Applying it on Our Data <a id=\"1.2\"></a>"},{"metadata":{"trusted":true,"_uuid":"2c50f748bf9ecb996c35fce87eb492b552c13854"},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nwordnet_lemmatizer = WordNetLemmatizer()\ntokenizer = RegexpTokenizer(r'[a-z]+')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(document):\n    document = document.lower() # Convert to lowercase\n    words = tokenizer.tokenize(document) # Tokenize\n    words = [w for w in words if not w in stop_words] # Removing stopwords\n    # Lemmatizing\n    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n    return \" \".join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d00a4e0511807d8a760781112e28d4be98e4567a"},"cell_type":"code","source":"datax['Processed Review'] = datax['Review Text'].apply(preprocess)\n\ndatax.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c19525e27d26e6a8b699a71cfd6982eb57f8ddb0"},"cell_type":"markdown","source":"### 1.3 Creating TF-IDF Matrix <a id=\"1.3\"></a>"},{"metadata":{"trusted":true,"_uuid":"b7c41047518ae454b4959f3255769bc02dd4ec67"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nTF_IDF_matrix = vectorizer.fit_transform(datax['Processed Review'])\nTF_IDF_matrix = TF_IDF_matrix.T\n\nprint('Vocabulary Size : ', len(vectorizer.get_feature_names()))\nprint('Shape of Matrix : ', TF_IDF_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6f6eb1290e1f92bb6104f2ba70127e277abbf0b"},"cell_type":"markdown","source":"## 2. Apply SVD to TF-IDF Matrix <a id=\"apply_svd\"></a>"},{"metadata":{"_uuid":"6921c2e1e184f0c22bf10ecca6855a3556ddb48a"},"cell_type":"markdown","source":"### 2.1 Create Term and Document Representation  <a id=\"2.1\"></a>"},{"metadata":{"trusted":true,"_uuid":"0913100c9c12d69aee347955c82a48aedb722fe4"},"cell_type":"code","source":"# import numpy as np\n\n# # Applying SVD\n# U, s, VT = np.linalg.svd(TF_IDF_matrix.toarray()) # .T is used to take transpose and .toarray() is used to convert sparse matrix to normal matrix\n\n# TF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# # Getting document and term representation\n# terms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\n# docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"253525c72aae14f385490896d6e14f9abcda1c17"},"cell_type":"code","source":"import numpy as np\nfrom scipy.sparse.linalg import svds\n\n# Applying SVD\nU, s, VT = svds(TF_IDF_matrix) # .T is used to take transpose and .toarray() is used to convert sparse matrix to normal matrix\n\nTF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# Getting document and term representation\nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\ndocs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix ","execution_count":null,"outputs":[]},{"metadata":{"id":"FseHPcSsGRTG","colab_type":"text","_uuid":"e2dfc63325fbd4ccec389e5bdecd6453e1ccd0eb"},"cell_type":"markdown","source":"### 2.2 Visulize Those Representation <a id=\"2.2\"></a>"},{"metadata":{"id":"g-oPbgCeFfws","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"5d0e997e-4cac-4780-f9ea-7940bd34adf1","trusted":true,"_uuid":"bd9f19a7bc124f0d7a9f6780f566fde7bf716622"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=datax['Recommended IND'])\nplt.title(\"Document Representation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yT-NgwjaA6nG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"ff01cfc6-2a6c-4f2d-e73d-b0a2fa08a1d2","trusted":true,"_uuid":"f7f86d0720ea6257c7cfda1e35d9cf0dcc04f33e"},"cell_type":"code","source":"plt.scatter(terms_rep[:,0], terms_rep[:,1])\nplt.title(\"Term Representation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7adc9fbcac6f39738f6200bac7d99ee6e2d0b6f"},"cell_type":"markdown","source":"## 3 Information Retreival Using LSA <a id=\"ir_lsa\"></a>"},{"metadata":{"trusted":true,"_uuid":"90c3cf2db0659576729ba052dfe9acda60c9aea4"},"cell_type":"code","source":"# This is a function to generate query_rep\n\ndef lsa_query_rep(query):\n    query_rep = [vectorizer.vocabulary_[x] for x in preprocess(query).split()]\n    query_rep = np.mean(terms_rep[query_rep],axis=0)\n    return query_rep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b91c1d053574b84cb6d8dff76dd45de9f9bf8019"},"cell_type":"code","source":"from scipy.spatial.distance import cosine\n\nquery_rep = lsa_query_rep(query)\n\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nprint_count = 0\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index],' Review : ', datax['Review Text'][sort_index])\n    if print_count == 4 :\n        break\n    else:\n        print_count += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b9eb714fde26a0f7be97619994cb758a20b95b0"},"cell_type":"markdown","source":"## 4. Create Model to Predict Recommendation <a id=\"4\"></a>"},{"metadata":{"trusted":true,"_uuid":"df78ab0b5360a23601362b8546beceb1c693a580"},"cell_type":"code","source":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\n\ndef create_logistic_model(X,y):\n    \n    # Splitting data for training and validation\n    x_train, x_test, y_train, y_test = train_test_split(pd.DataFrame(X),y,test_size=0.1, random_state=1)\n    \n    # Getting the input dimension\n    input_dim = X.shape[1]\n    \n    # this is our input placeholder\n    input_doc = Input(shape=(input_dim,))\n    # This is dense layer\n    dense_layer = Dense(1, activation='sigmoid')(input_doc)\n    # Our final model\n    model = Model(input_doc, dense_layer)\n    \n    # Compiling model\n    model.compile(optimizer='sgd', loss='binary_crossentropy',metrics=['accuracy'])\n    \n    \n    # Training model\n    history = model.fit(x_train, y_train,\n                epochs=5,\n                batch_size=100,\n                shuffle=True,\n                validation_data=(x_test, y_test),\n                verbose=0)\n    # Printing Accuracy\n    print('Accuracy on Training Data : ', history.history['acc'][-1])\n    print('Accuracy on Validation Data : ', history.history['val_acc'][-1])\n    \n    # Returning model\n    return model, history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a22b36427615737d4e6b9d49c3b06f8d072ff5"},"cell_type":"code","source":"model_using_lsa, history = create_logistic_model(docs_rep, datax['Recommended IND'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd09d51f2f4a24e8b5ce68ac866f302a4b3ec70e"},"cell_type":"code","source":"datax['Recommended IND'].value_counts() / datax['Recommended IND'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"356d94dda486c4c7f67293a53888b5a742a91a7b"},"cell_type":"code","source":"print(np.sum(model_using_lsa.predict(docs_rep) > .5))\nprint(docs_rep.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"776e0364734edd3a1b7367730d2509bdd7b83f55"},"cell_type":"markdown","source":"## 5. IR using Autoencoder with TF-IDF matrix <a id=\"5\"></a>"},{"metadata":{"trusted":true,"_uuid":"352f511befdd36c4667c880c29dbc22294756c7b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\nencoding_dim = 2 # Size of encoding\ninput_dim = TF_IDF_matrix.shape[0] # Size of docs\n\n# Splitting Data for training and validation\ndf = pd.DataFrame(TF_IDF_matrix.T.toarray())\nx_train, x_val, y_train, y_val = train_test_split(df, df[0], test_size=0.1, random_state=1)\n\n# Encoder and Decoder\n# this is our input placeholder\ninput_docs = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='tanh')(input_docs)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='relu')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_docs, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_docs, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\nhistory = autoencoder.fit(x_train, x_train,\n                epochs=100,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=(x_val, x_val))\n\n# encode and decode some data points\nprint('Original Data : ', x_val[:5])\nencoded_datapoints = encoder.predict(x_val[:5])\nprint('Encodings : ', encoded_datapoints)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\nprint('Reconstructed Data : ', decoded_datapoints)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1762b40ab9821334ccc103863d6ca8d636a07e95"},"cell_type":"code","source":"docs_rep_autoencoder = encoder.predict(TF_IDF_matrix.T)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep_autoencoder[:,0], docs_rep_autoencoder[:,1], c=datax['Recommended IND'])\nplt.title(\"Document Representation\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d978c5f08368075c968d9b2361b2366582932cb"},"cell_type":"markdown","source":"## 6 Information Retreival Using Autoencoder <a id=\"6\"></a>"},{"metadata":{"trusted":true,"_uuid":"b686660aa0394a5a858231c28519fd7141187e90"},"cell_type":"code","source":"# This is a function to generate query_rep\ndef autoencoder_query_rep(query):\n    query_rep = vectorizer.transform([query])\n    query_rep = encoder.predict(query_rep)\n    return query_rep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f5b125e41e11e226497f005fec460f9b12387b7"},"cell_type":"code","source":"from scipy.spatial.distance import cosine\n\nquery_rep = autoencoder_query_rep(query)\n\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nprint_count = 0\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index],' Review : ', datax['Review Text'][sort_index])\n    if print_count == 4 :\n        break\n    else:\n        print_count += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b527fb47e775164c425b6ab3d7ce1d01755928e1"},"cell_type":"markdown","source":"## 7. Predict Recommendation using Encoding of Autoencoder <a id=\"7\"></a>"},{"metadata":{"trusted":true,"_uuid":"6e408d123485319055d834b2c7536989be81b860"},"cell_type":"code","source":"model_using_autoencoder, history = create_logistic_model(docs_rep_autoencoder, datax['Recommended IND'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf29ff45eb2890f6e6bfde0034343f498eec365d"},"cell_type":"code","source":"datax['Recommended IND'].value_counts() / datax['Recommended IND'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeb58fe1938cd9032dd9fe0a09bdde615492e696"},"cell_type":"code","source":"print(np.sum(model_using_lsa.predict(docs_rep) > .5))\nprint(docs_rep.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d76748270c7b78cf288671271439d3c990b4fa14"},"cell_type":"markdown","source":"## 8. Use simple NN to predict Recommendation <a id=\"8\"></a>"},{"metadata":{"trusted":true,"_uuid":"d9d60ea3b3b7152168fc491f7ca0d487fce5cb98"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\ninput_dim = TF_IDF_matrix.shape[0] # Size of docs\n\n# Splitting Data for training and validation\ndf = pd.DataFrame(TF_IDF_matrix.T.toarray())\nx_train, x_val, y_train, y_val = train_test_split(df, datax['Recommended IND'], test_size=0.1, random_state=1)\n\n# this is our input placeholder\ninput_docs = Input(shape=(input_dim,))\nlayer1 = Dense(100, activation='relu')(input_docs)\nlayer2 = Dense(10, activation='relu')(layer1)\nlayer3 = Dense(2, activation='relu')(layer2)\nlayer4 = Dense(1, activation='sigmoid')(layer3)\n\n# Get encoding\nencoder = Model(input_docs, layer3)\n\n# Final Model\nmodel = Model(input_docs, layer4)\n\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                epochs=125,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=(x_val, y_val))\n\n# Printing Accuracy\nprint('Accuracy on Training Data : ', history.history['acc'][-1])\nprint('Accuracy on Validation Data : ', history.history['val_acc'][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"947e7ce1ac7d941f64de087181af1f5c2ced0ce5"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c8a45998f1d6a3675e7f11458e92c3b2f7d685e"},"cell_type":"markdown","source":"# Displaying all plot of encodings and saving these for report"},{"metadata":{"trusted":true,"_uuid":"dda64560035310ad14e6343472bcf11668a6f7ac"},"cell_type":"code","source":"docs_rep_nn = encoder.predict(TF_IDF_matrix.T)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep_nn[:,0], docs_rep_nn[:,1], c=datax['Recommended IND'])\nplt.show()\n\nplt.savefig('doc_rep_plot_nn.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92aae9bf8654f1bdff0848bc42ecc9bbb76062ce"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=datax['Recommended IND'])\nplt.show()\n\nplt.savefig('doc_rep_plot_lsa.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd85aa530fb63f69914069c902dc4f9ea7cb3128"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep_autoencoder[:,0], docs_rep_autoencoder[:,1], c=datax['Recommended IND'])\nplt.show()\n\nplt.savefig('doc_rep_plot_ae.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4526dfc481f958a7b82c7f3d58556439c112f1e6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"AI - Project - 3","version":"0.3.2","provenance":[],"collapsed_sections":["3U9Bp1h1yg6H","Siy0qO2sBk3A","AcmwI7ycBu36","F9ulboWcyoDF","WAAD_hlyE_nQ","JKfAiRKTMwSg","D9SJcwIKRors","aeYMp5Lch2hq","Z1GfGn6DZhcp","J68QTlqAoWU_","bfg_V2ZjEB6K","FseHPcSsGRTG","erlg5cOSGWt-"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU","language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}