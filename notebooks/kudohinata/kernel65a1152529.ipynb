{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#課題1\nprint ('helllllloooooooouuuu')\narr = np.array([1,2,3,4,5])\nprint (arr+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題2\nw = np.array([1,2,3,4])\nx = np.array([6,7,8,9])\n\nprint (w.dot(x)+4)\nsum((w*x))+4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題3\narr1 = np.array([[1,2,3],[6,50,400],[5,10,100]])\nprint(np.average(arr1[:,0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr100=np.array([[1, 2], [3, 4]])\nprint('arr100:\\n',arr100)\nprint('--------------')\nprint('arr100[0]', arr100[0])\nprint('--------------')\nprint('arr100:avg:',np.average(arr100))\nprint('--------------')\narr101=arr100[1]\nprint('arr101:',arr101)\nprint('--------------')\nprint(np.average(arr101))\nprint('--------------')\narr102=arr100.T\nprint('arr102:\\n',arr102)\nprint('--------------')\narr104=arr100[0:2,0]\nprint('arr104:',arr104)\nprint('arr100:avg:\\n',np.average(arr100, axis=0)[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nprint('arr:\\n', arr)\nprint('---------------')\narr2 = arr[:2, 0:4]\nprint('arr2:\\n', arr2)\nprint('---------------')\narr3 = arr[0:4, 2:]\nprint('arr3:\\n', arr3)\nprint('---------------')\narr4 = arr[:, 0]\nprint('arr4:\\n', arr4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題4\nimport pandas as pd\n\nindex = [\"Taro\", \"Jiro\", \"Saburo\", \"Hanako\", \"Yoshiko\"]\ndata = [90, 100, 70, 80, 100]\nseries = pd.Series(data, index = index)\nseries = series[series == 100]\nprint(series)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題5\ndata = pd.read_csv(\"../input/bostonhoustingmlnd/housing.csv\", delimiter=\",\")\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題6\nimport pandas as pd\n\ndata = pd.read_csv(\"../input/pandasdata2/pandasdata2.csv\", delimiter=\",\")\ndata = data.drop([\"id\",\"name\"], axis=1)\ndata = data.drop(range(0,10))\ndata = data.drop(data.columns[[0]],axis=1)\nprint(data)\n\ndata.to_csv(\"pandasdata3.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題7\n\ndf = pd.read_csv(\"../input/titanic/train.csv\", delimiter=\",\")\n#print(df.loc[df[\"Embarked\"].isnull()])\n\n# 欠損データ対応：Embarked\n# → 'Embarked'列にNaNを持つデータ行を削除\n#df = df.dropna(subset=['Embarked'])\n\ndf = df.drop([\"Name\",\"Ticket\",\"Cabin\"], axis=1)\nprint(df)\ndf.to_csv(\"train2.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#課題8\n\ndf = pd.read_csv(\"./train2.csv\", delimiter=\",\")\n\n#df = df.drop([\"Name\",\"Ticket\",\"Cabin\"], axis=1)\n#print(df)\n#df.to_csv(\"train2.csv\")\n\nmale_bin = (df[\"Sex\"] == \"male\").astype(int)\nfemale_bin = (df[\"Sex\"] == \"female\").astype(int)\ndf[\"male\"] = male_bin\ndf[\"female\"] = female_bin\ndf = df.drop([\"Sex\"], axis=1)\n\ndf[\"embarked_c\"] = (df[\"Embarked\"] == \"C\").astype(int)\ndf[\"embarked_q\"] = (df[\"Embarked\"] == \"Q\").astype(int)\ndf[\"embarked_s\"] = (df[\"Embarked\"] == \"S\").astype(int)\ndf = df.drop([\"Embarked\"], axis=1)\n\nprint(df)\ndf.to_csv(\"train3.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n#パーセプトロン\nclass Perceptron(object):\n\n    # 重みを設定する\n    def __init__(self, w):\n        self.w = w\n\n    # 入力値と重みから予測する\n    def net_input(self, X):\n        return np.dot(X, self.w[1:]) + self.w[0]\n\n    # 単位ステップ関数による判別\n    def predict(self, X):\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\ndata:image/pjpeg;base64,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\nw_and = np.array([-0.0337565463634, 0.0238824358635, 0.0147182824774])\nw_or = np.array([-0.00375654636337, 0.0038824358635, 0.00471828247737])\nw_nand = np.array([0.0162434536366, -0.0161175641365, -0.00528171752263])\nw_nor = np.array([0.00624345363663, -0.0161175641365, -0.0152817175226])\nw_xor = np.array([-0.00375654636337, 0.0038824358635, -0.00528171752263])\n\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n\nY_AND = [0, 0, 0, 1]\nY_OR = [0, 1, 1, 1]\nY_NAND = [1, 1, 1, 0]\nY_NOR = [1, 0, 0, 0]\nY_XOR = [1, 0, 0, 1]\n\ndef print_perceptron(perceptron, X, Y):\n    for i in range(len(X)):\n        print(\"X:\", X[i], \"Y:\", Y[i], \"predict:\", perceptron.predict(X[i]))\nprint(\"AND回路\")\nprint_perceptron(Perceptron(w_and), X, Y_AND)\nprint(\"OR回路\")\nprint_perceptron(Perceptron(w_or), X, Y_NOR)\nprint(\"NAND回路\")\nprint_perceptron(Perceptron(w_nand), X, Y_NAND)\nprint(\"NOR回路\")\nprint_perceptron(Perceptron(w_nor), X, Y_NOR)\nprint(\"XOR回路\")\nprint_perceptron(Perceptron(w_xor), X, Y_XOR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Adaline(object):\n#    w = [] # 重み\n#    errors_ = [] # 誤分類回数\n\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        # 学習率:eta\n        # 学習回数:n_iter\n        # 乱数シード: random_state\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def net_input(self, X):\n        return np.dot(X, self.w[1:]) + self.w[0]\n\n    def predict(self, X):\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n\n    def show_w(self):\n        for i in range(len(self.w)):\n            print(\"w[\", i, \"]:\", self.w[i])\n\n    def fit(self, X, Y):\n        # 重みの初期化（バイアス＋重み（入力値の種類数））\n        rgen = np.random.RandomState(self.random_state)\n        # 正規分布に従った乱数生成\n        # 平均:loc = 0\n        # 標準偏差:scale = 0.01\n        # 出力配列サイズ: 入力データ種類数 + 1(バイアス分)\n        self.w = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n        self.errors_=[]\n        \n        for i in range(self.n_iter):\n            errors = 0\n            # 重み更新式はw:=w-ηΔw\n            # ΔｗはコストJ(w)をwで微分した値なので、結果として\n            # 「-(ラベル-予測)*入力値」となる（合成関数の微分）\n            for xi, target in zip(X, Y):\n                update = self.eta * (target - self.predict(xi))\n                self.w[1:] += update * xi\n                self.w[0] += update\n                # 誤差があった回数だけ＋１する\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n\n            \nand_ada  = Adaline()\n\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\nY_AND = [0,0,0,1]\nand_ada.fit(X,Y_AND)\nfor i in range(len(X)):\n    print(and_ada.predict(X[i]))\n\nprint(\"----------\")\nor_ada  = Adaline()\n\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\nY_OR = [0,1,1,1]\nor_ada.fit(X,Y_OR)\nfor i in range(len(X)):\n    print(or_ada.predict(X[i]))\n    \nprint(\"----------\")\nnand_ada  = Adaline()\n\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\nY_NAND = [1,1,1,0]\nnand_ada.fit(X,Y_NAND)\nfor i in range(len(X)):\n    print(nand_ada.predict(X[i]))\n    \nprint(\"----------\")\nnor_ada  = Adaline()\n\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\nY_NOR = [1,0,0,0]\nnor_ada.fit(X,Y_OR)\nfor i in range(len(X)):\n    print(nor_ada.predict(X[i]))\n# AND\n# OR\n# NAND\n# NOR\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/iris/Iris.csv')\ndf = (df.loc[:, ['SepalLengthCm', 'PetalLengthCm', 'Species']])\ndf = df[df['Species'].isin(['Iris-setosa', 'Iris-versicolor'])]\ndf = df.replace({'Species': {\"Iris-setosa\": 0, \"Iris-versicolor\": 1}})\na_df = (df.loc[:, ['SepalLengthCm', 'PetalLengthCm']])\na_df = a_df.values\nl_df = (df.loc[:, 'Species'])\nl_df = l_df.values.tolist()\n\n# print(df)\n# print(\"-----------------\")\n# print(a_df)\n# print(type(a_df))\n# print(\"-----------------\")\n# print(l_df)\n# print(type(l_df))\n\nada  = Adaline()\nada.fit(a_df,l_df)\nfor i in range(len(a_df)):\n    print(ada.predict(a_df[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_iris\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\n\niris = load_iris()\nfrom sklearn.model_selection import train_test_split\ndata_X = iris.data\ndata_y = np_utils.to_categorical(iris.target)\n\nX_train, X_test, Y_train, Y_test = train_test_split(data_X, data_y, test_size=0.3)\n\nmodel = Sequential()\nmodel.add(Dense(64, input_shape=(4, )))\nmodel.add(Activation('relu'))\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n\n# 計算打ち切り処理\nfrom keras.callbacks import EarlyStopping\nearly_stopping =  EarlyStopping(monitor='val_loss',min_delta=0.0,patience=5,)\n\nresult = model.fit(X_train, Y_train,batch_size=20, epochs=400,verbose=1, validation_split=0.2)\n\n\ntrain_score = model.evaluate(X_train, Y_train)\ntest_score = model.evaluate(X_test, Y_test)\nprint(\"\\ntrain-loss:\", train_score[0])\nprint('train-accuracy:', train_score[1])\nprint(\"\\ntest-loss:\", test_score[0])\nprint('test-accuracy:', test_score[1])\n                     \nimport matplotlib.pyplot as plt\nplt.figure()\nplt.title('Accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.plot(result.history['accuracy'], label='train')\nplt.plot(result.history['val_accuracy'], label='test')\nplt.legend()\n\nplt.figure()\nplt.title('categorical_crossentropy Loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(result.history['loss'], label='train')\nplt.plot(result.history['val_loss'], label='test')\nplt.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[0])\nprint(Y_train[0])\nprint(model.predict(X_train[0:1]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import np_utils\nfrom keras.layers import Conv2D, AveragePooling2D\nfrom keras.layers import Flatten\nfrom keras.optimizers import Adadelta\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\n# 不確定箇所に-1を入れる（推測）\nX_train = X_train.reshape(-1, 28, 28, 1) \nX_test = X_test.reshape(-1, 28, 28, 1)\n\n# one-hotエンコーディング\nY_train = np_utils.to_categorical(Y_train)\nY_test = np_utils.to_categorical(Y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(6, kernel_size=(3, 3), padding='same', activation='sigmoid', input_shape=(28, 28, 1)))\nmodel.add(AveragePooling2D((2, 2)))\nmodel.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='sigmoid'))\nmodel.add(AveragePooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(120, activation='sigmoid'))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=Adadelta(),\n    metrics=['accuracy']\n)\n\nprint(model.summary())\n\nresult = model.fit(X_train, Y_train, epochs=20, batch_size=128, validation_split=0.2)\nscore = model.evaluate(X_test, Y_test, verbose=1)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1])\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}