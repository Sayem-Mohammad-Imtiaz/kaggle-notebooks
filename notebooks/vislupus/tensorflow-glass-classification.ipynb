{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(f'Tensorflow version: {tf.__version__}')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the Data","metadata":{}},{"cell_type":"code","source":"glass_data = pd.read_csv('../input/glass/glass.csv', parse_dates=True, encoding = \"cp1252\")\nglass_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glass_data.groupby('Type').count().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glass_data['Type'].replace(to_replace={1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.countplot(data=glass_data, x='Type')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test for correlation","metadata":{}},{"cell_type":"code","source":"corr = glass_data.corr(method = \"pearson\")\n# corr = glass_data.corr(method = \"spearman\")\n# corr = glass_data.corr(method = \"kendall\")\n\nf, ax = plt.subplots(figsize=(10, 10))\n\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split up the data to training set and test set","metadata":{}},{"cell_type":"code","source":"X = glass_data[['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']]\n\ny = glass_data['Type']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization of the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(units=150, input_shape=(X_train.shape[1],), activation='relu'),\n  tf.keras.layers.Dense(units=300, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=750, activation='relu'),\n  tf.keras.layers.Dropout(0.7),\n  tf.keras.layers.Dense(units=1350, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=250, activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(units=350, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=1500, activation='relu'),\n  tf.keras.layers.Dropout(0.7),\n  tf.keras.layers.Dense(units=1300, activation='relu'),\n  tf.keras.layers.Dropout(0.7),\n  tf.keras.layers.Dense(units=750, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=250, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=7, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cl = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,5))\n\nplt.plot(cl.history['accuracy'], label='accuracy')\nplt.plot(cl.history['val_accuracy'], label='val_accuracy', linestyle='--')\nplt.plot(cl.history['loss'], label='loss')\nplt.plot(cl.history['val_loss'], label='val_loss', linestyle='--')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ModelLoss, ModelAccuracy = model.evaluate(X_test, y_test)\n\nprint(f'Test Loss is {ModelLoss}')\nprint(f'Test Accuracy is {ModelAccuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_test_list=list(y_test)\ntotal=len(y_test_list)\ncorrect=0\n\n# for i in range(len(y_test_list)):\n#   print(f'{i+1} - {y_pred[4][i]:.3f} - {y_test_list[4]}')\n#   if np.argmax(y_pred[i])+1==y_test_list[i]:\n#     print(f'{i+1} - {np.argmax(y_pred[i])} - {y_test_list[i]}')\n\nfor i in range(total):\n  # print(f'{np.argmax(y_pred[i])} - {np.amax(y_pred[i])} - {y_test_list[i]}')\n  if(np.argmax(y_pred[i])==y_test_list[i]):\n    correct+=1\n    \nprint(f'{correct}/{total}')\nprint(correct/total)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confustion matrix","metadata":{}},{"cell_type":"code","source":"p_test = model.predict(X_test).argmax(axis=1)\ncm = tf.math.confusion_matrix(y_test, p_test)\n\nf, ax = plt.subplots(figsize=(7, 5))\nsns.heatmap(cm, annot=True, cmap='Blues', square=True, linewidths=0.01, linecolor='grey')\nplt.title('Confustion matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodels=[(\"Logistic Regression\",LogisticRegression()),\n        (\"Linear Discriminant Analysis\",LinearDiscriminantAnalysis()),\n        (\"Decision Tree\",DecisionTreeClassifier()),\n        (\"Random Forest\",RandomForestClassifier()),\n        (\"Extra Trees\",ExtraTreesClassifier()),\n        (\"Gradient Boostin\",GradientBoostingClassifier()),\n        (\"KNeighbors\",KNeighborsClassifier()),\n        (\"SVM\",SVC()),\n        (\"Gaussian Naive Bayes\",GaussianNB()),\n        (\"Ada Boost\",AdaBoostClassifier())]\n\n    \nfor name, model in models:\n    results = cross_val_score(model, X_train, y_train.values.ravel(), cv=10, scoring='accuracy')\n    print(f\"\\x1b[94m{name}\\x1b[0m: \\x1b[95m{results.mean():.4f}\\x1b[0m Â± {results.std():.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}