{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\")\ndata = data[['v1', 'v2']]\ndata = data.rename(columns = {'v1': 'label', 'v2': 'text'})\ndata.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.countplot(data=data, x='label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create feature for text message length","metadata":{}},{"cell_type":"code","source":"data['length'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\n# data[\"length\"] = data[\"text\"].apply(len)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.displot(data=data, x='length', hue='label', bins=np.linspace(0, 200, 20), fill=True, aspect=1.5, alpha=0.5)\n\nplt.figure(figsize=(10, 5))\nbins = np.linspace(0, 200, 40)\nplt.hist(data[data['label']=='ham']['length'], bins, alpha=0.5, label='ham')\nplt.hist(data[data['label']=='spam']['length'], bins, alpha=0.5, label='spam')\nplt.legend(loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create feature for % of text that is punctuation","metadata":{}},{"cell_type":"code","source":"import string\n\ndef count_punct(text):\n#     count = sum([1 for char in text if char in string.punctuation])\n    \n    count=0\n    for char in text:\n        if char in string.punctuation:\n            count+=1\n    \n    return round(count/(len(text) - text.count(\" \")), 3)*100\n\ndata['punct%'] = data['text'].apply(lambda x: count_punct(x))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nbins = np.linspace(0, 50, 40)\nplt.hist(data[data['label']=='ham']['punct%'], bins, alpha=0.5, label='ham')\nplt.hist(data[data['label']=='spam']['punct%'], bins, alpha=0.5, label='spam')\nplt.legend(loc='upper right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create feature for % of uppercase letters","metadata":{}},{"cell_type":"code","source":"def count_uppercase(text):  \n    count=0\n    for char in text:\n        if char.isupper():\n            count+=1\n    \n    return round(count/(len(text) - text.count(\" \")), 3)*100\n\ndata['upper%'] = data['text'].apply(lambda x: count_uppercase(x))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create feature count the exclamation marks","metadata":{}},{"cell_type":"code","source":"data['exclamation_marks'] = data['text'].apply(lambda x: x.count(\"!\"))\n\ndata.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data[data['Unnamed: 3'].notnull()]\n# data[data['Unnamed: 4'].notnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence Tokenization","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\ndata['text_clean'] = data['text'].apply(lambda x: sent_tokenize(x))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove punctuation","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punct(text):\n    #     text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n    message_not_punc = []\n    for char in text:\n        if char not in string.punctuation:\n            message_not_punc.append(char)\n            \n    text_nopunct = \"\".join(message_not_punc)\n            \n    return text_nopunct\n\ndata['text_clean'] = data['text'].apply(lambda x: remove_punct(x.lower()))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Tokenization","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndata['text_clean'] = data['text_clean'].apply(lambda x: word_tokenize(x))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stopwords","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\n\ndef remove_stopwords(text):\n    message = []\n    \n    for word in text:\n        if word not in stop_words:\n            message.append(word)\n            \n    return message\n\ndata['text_clean'] = data['text_clean'].apply(lambda x: remove_stopwords(x))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lexicon Normalization","metadata":{}},{"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef lemmatizing(text):\n    message = []\n    \n    for word in text:\n        message.append(wnl.lemmatize(word))\n            \n    return message\n\ndata['text_clean'] = data['text_clean'].apply(lambda x: lemmatizing(x))\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the text","metadata":{}},{"cell_type":"code","source":"!pip install contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contractions\nimport string\nfrom nltk.corpus import wordnet\nfrom nltk.tag import pos_tag\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef get_wordnet_pos(word):\n    tag = pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef remove_punct(text):\n    message=[]\n    for word in text:\n        message_not_punc = []\n        \n        if word not in stop_words:\n            for char in word:\n                if char not in string.punctuation:\n                    message_not_punc.append(char)\n\n            text_nopunct = \"\".join(message_not_punc)\n            if text_nopunct!=\"\":\n                message.append(text_nopunct)\n                \n    return message\n\ndef lemmatizing(text):\n    text=text.replace(\"/\",\" \")\n    text=contractions.fix(text)\n    text=word_tokenize(text)\n    \n    message = []\n    \n    for word in text:\n        message.append(wnl.lemmatize(word, get_wordnet_pos(word)))\n    \n    message = remove_punct(message)\n    message = \" \".join(message)\n    \n    return message\n\ndata['text_clean'] = data['text'].apply(lambda x: lemmatizing(x.lower()))\n\ndata.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CountVectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer()\nX_counts = count_vect.fit_transform(data['text_clean'])\nprint(X_counts.shape)\n# print(count_vect.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df.columns = count_vect.get_feature_names()\nX_counts_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CountVectorizer (w/ N-Grams)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nngram_vect = CountVectorizer(ngram_range=(2,2))\nX_counts = ngram_vect.fit_transform(data['text_clean'])\nprint(X_counts.shape)\n# print(ngram_vect.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df.columns = ngram_vect.get_feature_names()\nX_counts_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer()\nX_tfidf = tfidf_vect.fit_transform(data['text_clean'])\nprint(X_tfidf.shape)\n# print(tfidf_vect.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\nX_tfidf_df.columns = tfidf_vect.get_feature_names()\nX_tfidf_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_features = pd.concat([data['length'], data['punct%'], data['upper%'], data['exclamation_marks'], pd.DataFrame(X_tfidf.toarray())], axis=1)\nX_features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\n\nrf = RandomForestClassifier(n_jobs=-1)\nk_fold = KFold(n_splits=5)\ncross_val_score(rf, X_features, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(n_jobs=-1)\nrf_model = rf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = rf_model.predict(X_test)\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((y_pred==y_test).sum() / len(y_pred),3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]\n\nfeature_importance = rf_model.feature_importances_[:10]\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Feature Importance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}