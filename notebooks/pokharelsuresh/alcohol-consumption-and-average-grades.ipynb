{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"64a6eb28-ef1e-efe7-cf41-f7e78a559bfa"},"source":"Alcohol Consumption and Average Grades\n--------------------------------------\n\n\n----------\n\n\nThis notebook examines whether alcohol consumption has any predictive power over student average grades. I am also interested in learning what other features may be important predictors of student grades. Just a word of warning, I am not going to attempt to predict student grade evolution over marking periods since all the features in the dataset (other than grades) remain constant over marking periods and are general descriptors of student backgrounds. My results indicate that while alcohol consumption on weekdays and weekends are not the strongest predictor of student average grades, they are in the top 10 (out of 29). The two strongest predictors of student average grade are the willingness to pursue higher education (higher) and (guess what?) mother's education (Medu).\n\n\n----------\n\n\nThe notebook is organized as follows. I first conduct some EDA to see if the two tables (with math and Portuguese grades) can be combined. After combining the two tables, I obtain student average grades over marking periods and estimate 3 models: linear, regression tree, and random forest. Some quick cross-validation indicates that the random forest is the best performing model out of the three. Finally, I use the best-performing random forest to rank features by their importance."},{"cell_type":"markdown","metadata":{"_cell_guid":"2db47aa6-c8b6-9841-35e6-3c29aa2e2046"},"source":"First, I would ideally like to combine both tables into one. But before that I would like to get a feeling if math and Portuguese grades are comparable. There is a number of students who are repeated in both tables, so I would use their records to examine if math and Portuguese grades are comparable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d884da07-c5d4-136e-5dc9-315a865f8702"},"outputs":[],"source":"library(ggplot2)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(gridExtra)\nlibrary(alluvial)\nlibrary(extrafont)\n\nd1=read.table(\"../input/student-mat.csv\",sep=\",\",header=TRUE)\nd2=read.table(\"../input/student-por.csv\",sep=\",\",header=TRUE)\n\n#Following the suggestion of Carlo Ventrella, one of the attributes, \"paid,\" is course specific \n#rather than student specific, so I am eliminating it from the list of attributes by which student\n# are matched matched\ndata.source=merge(d1,d2,by=c(\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\n                            \"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\",\n                            \"guardian\",\"guardian\",\"traveltime\",\"studytime\",\"failures\",\n                            \"schoolsup\",\"famsup\",\"activities\",\"higher\",\"romantic\",\n                            \"famrel\",\"freetime\",\"goout\",\"Dalc\",\"Walc\",\"health\",\"absences\"))\nprint(nrow(data.source)) # 85 students"},{"cell_type":"markdown","metadata":{"_cell_guid":"d9409c45-a6b9-2de8-1a9d-8b5b9097f871"},"source":"There are 85 students who belong to both tables, and I am going to examine their average math and Portuguese grades a test case."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1128b2a4-e2ee-f66a-5d01-e70ff7e87cee"},"outputs":[],"source":"data.source$mathgrades=rowMeans(cbind(data.source$G1.x,data.source$G2.x,data.source$G3.x))\ndata.source$portgrades=rowMeans(cbind(data.source$G1.y,data.source$G2.y,data.source$G3.y))\n\ndata.source$Dalc <- as.factor(data.source$Dalc)      \ndata.source$Dalc <- mapvalues(data.source$Dalc, \n                              from = 1:5, \n                              to = c(\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"))\n\nstr1=ggplot(data.source, aes(x=mathgrades, y=portgrades)) +\n geom_point(aes(colour=factor(Dalc)))+ scale_colour_hue(l=25,c=150)+\ngeom_smooth(method = \"lm\", se = FALSE)\n\ndata.source$Walc <- as.factor(data.source$Walc)      \ndata.source$Walc <- mapvalues(data.source$Walc, \n                              from = 1:5, \n                              to = c(\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"))\n\nstr2=ggplot(data.source, aes(x=mathgrades, y=portgrades))+\ngeom_point(aes(colour=factor(Walc)))+ scale_colour_hue(l=25,c=150)+\ngeom_smooth(method = \"lm\", se = FALSE)\n\ngrid.arrange(str1,str2,nrow=2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3cc6648a-5a31-a59a-971f-4b3c407cf0f8"},"source":"The two scatter plots have few implications. First, among the 85 students, no one consumed high or very high levels of alcohol on daily basis. Second, almost all of those who earned relatively high scores consumed very low levels of alcohol on weekdays. Third, math and Portuguese grades seem to correlate highly with each other. When I regress Portuguese grades on math grades, the adjusted R-squared is 0.55. This means that the correlation coefficient between math and Portuguese grades is about 0.74 and that about 55% of the variation in Portuguese grades can be explained by the variation in math grades. In my view, this is an indication that I can go ahead and combine the two tables together without worrying much about the subject matter, average grades in math or Portuguese reflect general student aptitude.  That's what I will do next"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a66ea9a9-7f1a-ec8b-e7f6-93fec8baafc7"},"outputs":[],"source":"d3<-rbind(d1,d2) #combine the two datasets\n# and eliminate the repeats:\nd3norepeats<-d3 %>% distinct(school,sex,age,address,famsize,Pstatus,\n                Medu,Fedu,Mjob,Fjob,reason,\n                guardian,traveltime,studytime,failures,\n                schoolsup, famsup,activities,nursery,higher,internet,\n                romantic,famrel,freetime,goout,Dalc,Walc,health,absences, .keep_all = TRUE)\n#add a column with average grades (math or Portuguese, whichever is available)\nd3norepeats$avggrades=rowMeans(cbind(d3norepeats$G1,d3norepeats$G2,d3norepeats$G3))\n# and drop grades in 3 marking periods.\nd3norepeats<-d3norepeats[,-(31:33)]"},{"cell_type":"markdown","metadata":{"_cell_guid":"79200866-7bd7-c696-c958-fc4773cc68b6"},"source":"Now a basic boxplot of average subject grades grouped by the levels of daily alcohol consumption. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c2ba43d-a804-f40d-2274-ad9d90ed6963"},"outputs":[],"source":"ggplot(d3norepeats, aes(x=Dalc, y=avggrades, group=Dalc))+\n  geom_boxplot()+\n  theme(legend.position=\"none\")+\n  scale_fill_manual(values=waffle.col)+\n  xlab(\"Daily Alcohol consumption\")+\n  ylab(\"Average Grades\")+\n  ggtitle(\"Average Grade\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"67d1301e-67d9-207f-bf87-4452b3244894"},"source":"The median average grade is visually higher among those students who had very low levels of daily alcohol consumption.  However, the median grade of the students with medium, high, and very high levels of daily  alcohol consumption doesn't seem to be very different.  As my first stab at the predictability of average grades using all other variables, I am going to 1) run a multiple linear regression and 2) build a regression tree of average grades on all other variables."},{"cell_type":"markdown","metadata":{"_cell_guid":"568c9b19-3e1c-2e8f-e5d1-605454e9529d"},"source":"Variable \"failures: is closely related to my target variable, avggrades. Since past failures and avggrades represent the same general student aptitude (thus it is rather a target rather than a feature), I am inclined to remove variable \"failures\" from the dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6cd5c61a-a2c8-a7ba-48bc-29487fa432e3"},"outputs":[],"source":"failureind<-which(names(d3norepeats)==\"failures\")\nd3norepeats<-d3norepeats[,-failureind]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1423906-438b-7432-0196-0f6c67f12013"},"outputs":[],"source":"# 1) multiple regression \nlm2<-lm(avggrades~., data=d3norepeats[,1:30])\nsummary(lm2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"39a1839e-df65-67fb-591d-a383e585242a"},"source":"Adjusted R-squared in the above regression is only 0.17, which is quite low. It implies that only 17% of the variation in the average grades is explained by the variation in everything else. The variables that have statistically significant impact on average grade are studytime (duh...), schoolsup, paid, and higher. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08ba8ac8-59d6-e368-408b-c3ec3d59821e"},"outputs":[],"source":"#2) Regression tree: \nlibrary(rpart)\nlibrary(DMwR)# I will be relying heavily on the DMwR library that comes with Torgo, L. (2011) Data Mining with R. \nrt2<-rpart(avggrades~., data=d3norepeats[,1:30])\nprettyTree(rt2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc27a802-20c4-d1df-35f1-bdf7f4dfd4bf"},"source":"According to the regression tree analysis, the variable that seems to be important in \"higher\" that indicates whether the student  wants to pursue higher education. The overwhelming majority of surveyed students would like to pursue higher education and their average grade (11.4/20) is significantly higher than the average grade of those who don't (8.47/20). Regression tree analysis reveals that mother's education is another important feature (interestingly, this feature did not come up as important in the linear regression model). Students whose mothers had at least secondary education had significantly higher grade (12.2) than the students whose mothers do not (their average grade was 11). I would like to take the first stab at  evaluating the relative predictive performance of the two models. Without getting into cross-validation, I'm at first interested in the normalized mean squared error of the two models. The following code does it. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1769f0c9-1f75-8367-15f9-55ef62a0290b"},"outputs":[],"source":"#predictions\nlm.predictions<-predict(lm2,d3norepeats)\nrt.predictions<-predict(rt2,d3norepeats)\nnmse.lm<-mean((lm.predictions-d3norepeats[,\"avggrades\"])^2)/mean((mean(d3norepeats$avggrades)-d3norepeats[,\"avggrades\"])^2)\nnmse.rt<-mean((rt.predictions-d3norepeats[,\"avggrades\"])^2)/mean((mean(d3norepeats$avggrades)-d3norepeats[,\"avggrades\"])^2)\nprint(nmse.lm) #0.79\nprint(nmse.rt) #0.85"},{"cell_type":"markdown","metadata":{"_cell_guid":"e23d68fd-182e-6990-602f-719a234ab9b6"},"source":"It seems that the linear model performs better than the regression tree.  The following model code shows the error scatter plots."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"072450a2-dcad-5684-df2f-712e0152881e"},"outputs":[],"source":"lmpltdata1=data.frame(cbind(lm.predictions,d3norepeats[,\"avggrades\"]))\ncolnames(lmpltdata1)<-c(\"lm.predictions\",\"avggrades\")\nrtpltdata1=data.frame(cbind(rt.predictions,d3norepeats[,\"avggrades\"]))\ncolnames(rtpltdata1)<-c(\"rt.predictions\",\"avggrades\")\n\nd3norepeats$Dalc<-as.factor(d3norepeats$Dalc)\n\nerrplt.lt1=ggplot(lmpltdata1,aes(lm.predictions,avggrades))+\n                  geom_point(aes(color=d3norepeats[,\"Dalc\"]))+\n                  xlab(\"Predicted Grades (Linear Model)\")+\n                  ylab(\"Actual Grades\")+\n                  geom_abline(intercept=0,slope=1,color=\"#0066CC\",size=1)+\n                  #geom_smooth(method = \"lm\", se = FALSE)+\n                  scale_colour_brewer(palette = \"Set1\",name = \"Daily Alcohol \\nConsumption\")\n\nerrplt.rt1=ggplot(rtpltdata1,aes(rt.predictions,avggrades))+\n  geom_point(aes(color=d3norepeats[,\"Dalc\"]))+\n  xlab(\"Predicted Grades (Regression Tree)\")+\n  ylab(\"Actual Grades\")+\n  geom_abline(intercept=0,slope=1,color=\"#0066CC\",size=1)+\n  #geom_smooth(method = \"lm\", se = FALSE)+\n  scale_colour_brewer(palette = \"Set1\",name = \"Daily Alcohol \\nConsumption\")\n\ngrid.arrange(errplt.lt1,errplt.rt1,nrow=2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3de55501-0ec9-fe1c-2bf2-7d7274cc91d5"},"source":"In the above graphs, horizontal axes represent predicted grades while the vertical axes represent true grades. If the model is accurate in predicting actual grades then predicted grades must be equal to actual grades and thus the scatter points should line up along the 45 degree (blue) line. Unfortunately, as the NMSEs and error plots indicate, neither of the two models seems to do a decent job in predicting student average grades. Unsatisfied with how linear regression and regression tree models perform, I am going to take it up a notch and give a random forest a try."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2eecd76-fb3e-c1cd-8067-b5b28e780d47"},"outputs":[],"source":"library(randomForest)\nset.seed(4543)\nrf2<-randomForest(avggrades~., data=d3norepeats[,1:30], ntree=500, importance=T)\nrf.predictions<-predict(rf2,d3norepeats)\nnmse.rf<-mean((rf.predictions-d3norepeats[,\"avggrades\"])^2)/mean((mean(d3norepeats$avggrades)-d3norepeats[,\"avggrades\"])^2)\nprint(nmse.rf) #0.2"},{"cell_type":"markdown","metadata":{"_cell_guid":"bfefde90-34b1-cc62-640a-4a90f5e8bc57"},"source":"NMSE of the random forest implementation is 0.2 and it is much, much lower than that of the linear and regression tree models. As a validation, I am going to obtain the error plot of the random forest and compare it with the error plots of the linear and regression tree models obtained above."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11a4397e-2789-178f-277f-963b62ca2dcf"},"outputs":[],"source":"#first combine the rf predictions and actual scores in a single data frame\nrfpltdata1=data.frame(cbind(rf.predictions,d3norepeats[,\"avggrades\"]))\ncolnames(rfpltdata1)<-c(\"rf.predictions\",\"avggrades\")\n\n# then create the error plot.\nerrplt.rf1<-ggplot(rfpltdata1,aes(rf.predictions,avggrades))+\n  geom_point(aes(color=d3norepeats[,\"Dalc\"]))+\n  xlab(\"Predicted Grades (Random Forest with 500 Trees)\")+\n  ylab(\"Actual Grades\")+\n  geom_abline(intercept=0,slope=1,color=\"#0066CC\",size=1)+\n  #geom_smooth(method = \"lm\", se = FALSE)+\n  scale_colour_brewer(palette = \"Set1\",name = \"Daily Alcohol \\nConsumption\")\n#finally, plot the error plot from the random forest with the error plots of the linear and regression tree models.\ngrid.arrange(errplt.rf1, errplt.lt1,errplt.rt1,nrow=3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1c62071-f314-be56-6cda-c438178cffbd"},"source":"Even though, random forest seems to systematically underpredict the grades of low grade earners and overpredict the grades of high grade earners, overall, random forest seems to be a much better predictor of average grades than either the linear regression or regression tree model. 10 x 5 fold cross validation that I run on my local machine confirms that out of the three models that I executed here (LM, RT, & RF), the RF model with 500 trees is indeed the best predictor of average student grades (somehow Kaggle's notebook environment keeps on reconnecting and is unable to run the cross-validation code in the notebook environment).  I plot the relative importance of all the features in the dataset as measured by the Random Forest with 500 trees."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35fb55c3-213e-0e83-02b8-3379959da2e0"},"outputs":[],"source":"varImpPlot(rf2,type=1) # this metric is obtained by measuring the %increase in MSE of the model if the variable is removed"},{"cell_type":"markdown","metadata":{"_cell_guid":"6d894008-a89e-0940-0124-e354ded64dcf"},"source":"The top 10 most important variables that impact student average grades are:\n\n -  **higher**- wants to take higher education (binary: yes or no)\n - **Medu** - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade,        3 - secondary education or 4 – higher education)\n - **studytime** - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n - **schoolsup**  - extra educational support (binary: yes or no)\n - **Dalc** - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n - **goout**  - going out with friends (numeric: from 1 - very low to 5 - very high)\n - **Walc** - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n - **reason** - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n - **Fedu** - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n - **sex** - student's sex (binary: 'F' - female or 'M' - male)\n\nThese results imply that both weekday and weekend alcohol consumption are important predictors of student average grades. Removing either of these two variables will increase the MSE of predictions by between 10-20%. \n\nWhat's interesting about these results is that some features that would be conventionally thought as important did not end up in the top ten list (I would speculate that the variables such as Pstatus, famsupport, famrel, & absences are among those) while some other variables (such as higher and Medu) turned out to be very important. \n\nIn what follows, I am going to produce a partial dependence plot for each feature in the dataset (ranked by importance). Partial dependence plots give a graphical depiction of the marginal effect of a feature on the response. In this case, partial dependencies are produced by the best performing Random Forest model with 500 trees.  As the plots indicate, shifting from \"very low\" levels to just \"low\" levels of alcohol consumption on weekdays, will reduce the expected average grade from 11.22 to 10.88. Similarly, moving from \"very low\" levels to just \"low\" levels of alcohol consumption on weekends will reduce the average predicted grade from 11.19 to 11.17. \n\nAs a conclusion, the impact of the most important 2 variables, \"higher\" and \"Medu,\" on average student grade is as follows: \n\n - **higher**: willingness to pursue higher education increases average predicted grade from 9.42 to 11.25. Thus, motivate your kids to pursue higher education is the best thing you can do to improve their grades in school!\n - **Medu**: increase in mother's education from none to more than secondary education increases predicted average grade from 10.8 to 11.5. Thus, advice to future parents (especially fathers): If you want your future children do well in school marry someone educated."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d4ad76a-750a-7d0c-93ee-839f7fcc6f0d"},"outputs":[],"source":"imp <- importance(rf2)\nimpvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]\nop <- par(mfrow=c(2, 3))\nfor (i in seq_along(impvar)) {\n  partialPlot(rf2, d3norepeats[,1:30], impvar[i], ,rug=TRUE, xlab=impvar[i],\n              main=paste(\"Partial Dependence on\", impvar[i]))\n  abline(h=mean(d3norepeats$avggrades),col=\"red\")\n}\npar(op)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}