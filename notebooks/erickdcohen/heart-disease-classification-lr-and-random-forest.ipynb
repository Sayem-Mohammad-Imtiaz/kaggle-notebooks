{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Classification\n\nThis notebook explores the stroke-prediction-dataset that creates a model to predict heart disease. The analysis starts with simple exploration, then goes into a simple logistic regression model, and finally trains a random forest model acheving a 95% accuracy! ","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport os \nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = '../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv'\nstroke_raw = pd.read_csv(filepath)\nstroke_raw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring the data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(9, 5))\nsns.histplot(x=\"bmi\", data=stroke_raw).set_title(\"BMI Distribution\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# value count plots\nsns.set(font_scale=2)\nplt.figure(figsize=(9, 5))\nsns.countplot(x=\"heart_disease\", data=stroke_raw).set_title(\"Heart Disease\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because the amount of patients with heart disease is so small, we need to either downsample or create more data of individuals with heart disease to train the model. We use the `SMOTE` algorithm to create a more balanced dataset. We also preprocess the data for training.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score \nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\nX = stroke_raw.drop([\"heart_disease\", \"id\"], axis=1).fillna(stroke_raw.median())\nle = LabelEncoder()\nX[\"gender\"] = le.fit_transform(X[\"gender\"])\nX[\"ever_married\"] = le.fit_transform(X[\"ever_married\"])\nX[\"work_type\"] = le.fit_transform(X[\"work_type\"])\nX[\"Residence_type\"] = le.fit_transform(X[\"Residence_type\"])\nX[\"smoking_status\"] = le.fit_transform(X[\"smoking_status\"])\n\nsclr = StandardScaler()\nX = sclr.fit_transform(X)\n\ny = stroke_raw[\"heart_disease\"]\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=44, stratify=y)\n\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts() # ensuring even samples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(train_X, train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(model, train_X, train_y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\n\npreds = model.predict(test_X)\nconfusion_matrix(test_y, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perhaps we can look at a different model to acheive a higher accuracy\n\n## Random Forest\n\nFor the random forest model we do a randomized grid search to get close to the optimal hyperparameters. We will test the accuracy. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making a Random Forest Classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=33, n_jobs=-1)\n# rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n# rf_clf.fit(train_X, train_y)\n\nrf_random_search = RandomizedSearchCV(\n    estimator=rf_clf, \n    param_distributions=random_grid, \n    n_iter = 50, \n    cv=3, \n    verbose=2, \n    random_state=32, \n    n_jobs = -1)\n# Fit the random search model\nrf_random_search.fit(train_X, train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rf_random_search.best_params_)\n\ncv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=21)\n\nscores = cross_val_score(rf_random_search, train_X, train_y, scoring='accuracy', cv=5, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = rf_random_search.predict(test_X)\nconfusion_matrix(test_y, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = accuracy_score(test_y, preds)\nprint(f'Test set accuracy score was: {acc:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nAnd there we go! Our random forest model acheived 95% accuracy and does not appear to overfit the training set.","metadata":{}}]}