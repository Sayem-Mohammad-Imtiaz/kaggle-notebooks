{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\nThis bankrupcy dataset is great for practice since much of the data cleaning and scaling is done, and we can focus primarily on building the model itself. We will build a model to predict whether or not a company will file for bankrupcy. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n!pip install pyjanitor\nimport janitor\n\ncompany_bankrupcy = pd.read_csv('../input/company-bankruptcy-prediction/data.csv').clean_names()\ncompany_bankrupcy.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = company_bankrupcy['bankrupt_']\nX = company_bankrupcy.drop('bankrupt_', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have almost seven thousand observations and 95 predictor variables\n\n## Splitting and setting up our model\n\nWe will split the data into a testing set and a training set, and from the training set, we will split that into a training and validation set to evaluate our model both during training and on new data (the testing data) it hadn't seen.\n\nThe model will also adopt an early-stopping call back to halt training if we see we are not lowering our loss.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow import keras \nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Split training and testing sets \nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13, stratify=y)\n\n# split training and validation set\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n                                                  test_size=0.25, random_state=10,\n                                                 stratify=y_train) \n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=15, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n#     layers.Dense(95, activation='relu'),\n    layers.Dense(190, activation='relu', input_shape=[95]),\n    layers.Dense(190, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A wider model appears to work best. This is assuming many of the predictor variables have a linear relationship to our target variable `y`. \n\nWe will fir the model now and take a look at the corresponding accuracy and loss curves to ensure we are not overfitting our model to the training set. ","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    X_train, \n    y_train, \n    validation_data=(X_val, y_val),\n    epochs=500, #150 \n    batch_size=100, #50 \n    callbacks=[early_stopping]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_df = pd.DataFrame(history.history)\n# history_df.plot();\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(f\"Minimum validation loss: {history_df['val_loss'].min()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our loss curves do not seem to indicate we are overfitting the training data, although the validation loss is slightly worse. ","metadata":{}},{"cell_type":"code","source":"history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot();\nprint(f\"Max validation Accuracy: {history_df['val_binary_accuracy'].max()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final test set ","metadata":{}},{"cell_type":"code","source":"preds = model.predict(X_test)\nscores = model.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok! Our highest validation accuracy during training was around 96.5%, and our predictions on the new testing data was 95.8%. This suggests that the model was not overfit.","metadata":{}},{"cell_type":"code","source":"preds.round()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, preds.round())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nBecause this bankrupcy dataset was clean, we could focus on building the deep learning model. The confusion matrix above shows how the model performed on the new data.","metadata":{}}]}