{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Modelling Random Forest Classifier and ANN"},{"metadata":{},"cell_type":"markdown","source":"Import libary and dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata = pd.read_csv('../input/credit-risk/original.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check data structure:\n* age has null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* age has values < 0. Assuming this is due to fat finger (enter negative accidentally), change the values back to positive"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['age'] <0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data['age'] < 0, 'age'] = data['age']*-1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  To prevent loss of data, replace the age with null values with the mean of the age"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.age.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_mean = data[data.age.isnull() == False]['age'].mean()\ndata['age'] = data['age'].fillna(age_mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  Check the data again. There is no null values. We can proceed to examine the variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Examine the distribution of the variables**\n\n* Check the Income data. Income data appears to be evenly distributed across 20k to 70k and there is no clear difference between distribution of income for defaulters and non-defaulters."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n\nsns.distplot( data[\"income\"], bins=50,ax=axes[0]).set_title(\"Histogram of Income\")\nviz_1=sns.violinplot(data=data, x='default', y='income', ax=axes[1])\nviz_1.set_title('Density and distribution of income for default')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Check the Age data. Income data appears to be evenly distributed around 20 to 60. The range of defaulters' age is lower and concentrates around 30, while range of non-defaulters is higher and concentrates around 50 to 60. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n\nsns.distplot( data[\"age\"], bins=20, ax=axes[0]).set_title(\"Histogram of Age\")\nviz_2=sns.violinplot(data=data, x='default', y='age', ax=axes[1])\nviz_2.set_title('Density and distribution of age for default')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Check the Loan data. Loan data appears to be right-skewed and concentrates around 0 to 2.5k. The range of defaulters' loan amount is higher and concentrates around 6k to 8k, while range of non-defaulters is lower and concentrates around 2k. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n\nsns.distplot( data[\"loan\"], bins=20, ax=axes[0] ).set_title(\"Histogram of Loan\")\nviz_3=sns.violinplot(data=data, x='default', y='loan', ax=axes[1] )\nviz_3.set_title('Density and distribution of loan for default')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The default in the data seems to around 250 out of 2000 (12.5%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped1 = pd.DataFrame(data.groupby(['default'])['clientid'].count()).reset_index()\n\nlabel = list(grouped1['clientid'])\nplt.bar(grouped1['default'], grouped1['clientid'])\nfor i in range(len(grouped1)):\n    plt.text(x = grouped1['default'][i]-0.1 , y = grouped1['clientid'][i]+0.3, s = label[i], size = 10)\n\nplt.xticks(np.arange(0, 2, 1))\nplt.title('Count of defaut')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model to predict the default**\n* Split data into training and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data.drop(columns=['clientid'])\n\n\nX = dataset.iloc[:, 0:-1].values\ny = dataset.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Validation set\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Create a Random Forest Classifier Model with number of estimators = 50, although the number of estimator is high, it does not seems to have create a overfitting issues as the out of sample prediction is high"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Random Forest Classifier to the dataset\n\nfrom sklearn.ensemble import RandomForestClassifier\nregressor = RandomForestClassifier(n_estimators = 50, random_state = 0)\nregressor.fit(X_train, y_train)\n# Predicting result for training set and validation set\npredict_train_rf = regressor.predict(X_train)\npredict_val_rf = regressor.predict(X_val)\n\n# Model Performance \nfrom sklearn.metrics import accuracy_score \nprint(\"Train Score : \", accuracy_score(y_train, predict_train_rf) *  100) \nprint(\"Val Score : \", accuracy_score(y_val, predict_val_rf) *  100) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Create an ANN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_val = sc.transform(X_val)\n\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 3, kernel_initializer = 'uniform', activation = 'relu', input_dim = 3))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting result for training set and validation set\npredict_train_ann = classifier.predict(X_train)\npredict_val_ann = classifier.predict(X_val)\npredict_train_ann = predict_train_ann.flatten()\npredict_val_ann = predict_val_ann.flatten()\ntrain_df = pd.DataFrame({'y_train': y_train, 'predict_train_ann': predict_train_ann})\nval_df = pd.DataFrame({'y_val': y_val, 'predict_val_ann': predict_val_ann})\ntrain_df['predict_train_binary_ann'] = train_df['predict_train_ann'].apply(lambda x: 1 if x >= 0.5 else 0)\nval_df['predict_val_binary_ann'] = val_df['predict_val_ann'].apply(lambda x: 1 if x >= 0.5 else 0)\n\n\n# Model Performance \nfrom sklearn.metrics import accuracy_score \nprint(\"Train Score : \", accuracy_score(y_train, train_df['predict_train_binary_ann']) *  100) \nprint(\"Val Score : \", accuracy_score(y_val, val_df['predict_val_binary_ann']) *  100) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2-steps ANN model provides a better prediction compared to Random Forest Classifier. The Val Score is different every time it is generated but is consistently above 99%"},{"metadata":{},"cell_type":"markdown","source":"* Plot the â€˜Cumulative Accuracy Profile' (CAP) of the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Set\ntotal = len(y_train) \n  \n# Counting '1' labels in test data \none_count = np.sum(y_train) \n  \n# counting '0' lables in test data  \nzero_count = total - one_count \n  \nplt.figure(figsize = (10, 6)) \n  \n# x-axis ranges from 0 to total number of data\n# y-axis ranges from 0 to the total defaulters. \n  \nplt.plot([0, total], [0, one_count], c = 'b',  \n         linestyle = '--', label = 'Random Model') \n\n\nplt.plot([0, one_count, total], [0, one_count, one_count], \n         c = 'grey', linewidth = 2, label = 'Perfect Model') \n\nlm = [y for _, y in sorted(zip(predict_train_ann, y_train), reverse = True)] \nx = np.arange(0, total + 1) \ny = np.append([0], np.cumsum(lm)) \nplt.plot(x, y, c = 'b', label = 'ANN', linewidth = 2) \n\nlm = [y for _, y in sorted(zip(predict_train_rf, y_train), reverse = True)] \nx = np.arange(0, total + 1) \ny = np.append([0], np.cumsum(lm)) \nplt.plot(x, y, c = 'red', label = 'Random Forest', linewidth = 2) \n\nplt.legend() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validation Set\ntotal = len(y_val) \n  \n# Counting '1' labels in test data \none_count = np.sum(y_val) \n  \n# counting '0' lables in test data  \nzero_count = total - one_count \n  \nplt.figure(figsize = (10, 6)) \n  \n# x-axis ranges from 0 to total number of data\n# y-axis ranges from 0 to the total defaulters. \n  \nplt.plot([0, total], [0, one_count], c = 'b',  \n         linestyle = '--', label = 'Random Model') \n\n\nplt.plot([0, one_count, total], [0, one_count, one_count], \n         c = 'grey', linewidth = 2, label = 'Perfect Model') \n\nlm = [y for _, y in sorted(zip(predict_val_ann, y_val), reverse = True)] \nx = np.arange(0, total + 1) \ny = np.append([0], np.cumsum(lm)) \nplt.plot(x, y, c = 'b', label = 'ANN', linewidth = 2) \nlm = [y for _, y in sorted(zip(predict_val_rf, y_val), reverse = True)] \nx = np.arange(0, total + 1) \ny = np.append([0], np.cumsum(lm)) \nplt.plot(x, y, c = 'red', label = 'Random Forest', linewidth = 2) \n\nplt.legend() ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}