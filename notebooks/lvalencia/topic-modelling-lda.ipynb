{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation # LDA\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read csv\ntweets = pd.read_csv(\"../input/pfizer-vaccine-tweets/vaccination_tweets.csv\")\n\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"stops = stopwords.words(\"english\")             \n\ntweets['text'] = tweets.text.str.replace(\"[^\\w\\s]\", \"\").str.lower() # Lowercase\ntweets['text'] = tweets['text'].apply(lambda x: \n                              ' '.join([item for item in x.split() if item not in stops])) # Remove english stopwords\ntweets['text'] = tweets.text.str.replace(\"[^\\w\\s]\", \"\") # Remove punctuation\ntweets['text'] = tweets['text'].str.replace('\\d+', '') # Remove digits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['text'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create the document term matrix\n\nWe need to create vocabulary of all the words in our data by using the CountVectorizer class from the sklearn.feature_extraction.text module to create a document-term matrix. We specify to only include those words that appear in less than 80% of the document and appear in at least 5 tweets. We also remove all the stop words as they do not really contribute to topic modeling."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"count_vect = CountVectorizer(max_df=0.75, min_df=10, stop_words='english')\ndoc_term_matrix = count_vect.fit_transform(tweets['text'].values.astype('U'))\n\n# Print the document text matrix\ndoc_term_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LDA with GridSearchCV\n\nTopic modeling involves counting words and grouping similar word patterns to describe topics within the data. If the model knows the word frequency, and which words often appear in the same document, it will discover patterns that can group different words together.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Search Params to check which combination of parameters are the best\nsearch_params = {'n_components': [5, 6, 7, 8, 10], 'learning_decay': [.2, .5, .7, .9]}\ncv = 2\n\n# Initialize LDA to perform LDA on our document-term matrix\n# n_components specifies the number of categories, or topics, that we want our text to be divided into. \nLDA = LatentDirichletAllocation(random_state=0, evaluate_every=-1, n_jobs=-1)\n\n# Init Grid Search Class\nmodel = GridSearchCV(LDA, param_grid=search_params, cv=cv, verbose=5, n_jobs=-1)\n\n# Fit transform the feature matrix\n# This step might take a while a GridSearchCV() can indeed take a huge amount of CPU-time / CPU-poolOfRESOURCEs\nmodel.fit(doc_term_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best Model\nbest_lda_model = model.best_estimator_\nprint(\"Best Model's Params: \", model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model's perplexity and log-likelihood"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print log-likelihood\nprint(\"Log likelihood: \", model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print perplexity\nprint(\"Perplexity: \", best_lda_model.perplexity(doc_term_matrix))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare LDA Models Performance Scores"},{"metadata":{},"cell_type":"markdown","source":"Plotting the log-likelihood scores against num_topics, clearly shows number of topics = 5 has better scores. And learning_decay doesn't seem to make much of a difference."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get Log Likelyhoods from Grid Search Output\nn_topics = [5, 6, 7, 8, 10] # same as in n_components in search_param variable\n\nmeans = model.cv_results_['mean_test_score']\n\nparams = model.cv_results_['params']\n\nlog_likelyhoods_2 = [round(mean) \n                     for mean, params in zip(means, params) if params['learning_decay']==0.2]\nlog_likelyhoods_5 = [round(mean) \n                     for mean, params in zip(means, params) if params['learning_decay']==0.5]\nlog_likelyhoods_7 = [round(mean) \n                     for mean, params in zip(means, params) if params['learning_decay']==0.7]\nlog_likelyhoods_9 = [round(mean)  \n                    for mean, params in zip(means, params) if params['learning_decay']==0.9]\n\n# Show graph\nplt.figure(figsize=(12, 8))\nplt.plot(n_topics, log_likelyhoods_2, label='0.2')\nplt.plot(n_topics, log_likelyhoods_5, label='0.5')\nplt.plot(n_topics, log_likelyhoods_7, label='0.7')\nplt.plot(n_topics, log_likelyhoods_9, label='0.9')\nplt.title(\"Choosing The best LDA Model\")\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Log Likelyhood Scores\")\nplt.legend(title='Learning decay', loc='best')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot LDA best model\n\nTopic modeling is useful, but itâ€™s difficult to understand it just by looking at a combination of words and numbers like above.\n\nOne of the most effective ways to understand data is through visualization. For that purpose I am going to use [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html). The pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data.\n\nClick in the image to interact with the visualization yourself. Here you have some tips to help you understand the plot:\n\n- Each bubble represents a topic. The larger the bubble, the higher percentage of the number of tweets in the corpus is about that topic. \n- Blue bars represent the overall frequency of each word in the corpus. If no topic is selected, the blue bars of the most frequently used words will be displayed. \n- Red bars give the estimated number of times a given term was generated by a given topic. \n\n\nThe further the bubbles are away from each other, the more different they are. A good topic model will have big and non-overlapping bubbles scattered throughout the chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyLDAvis.sklearn\n\npanel = pyLDAvis.sklearn.prepare(best_lda_model, doc_term_matrix, count_vect, mds='tsne')\npyLDAvis.display(panel)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}