{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom os import path\nfrom PIL import Image\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\n# Importing TextBlob\nfrom textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read csv\ntweets = pd.read_csv(\"../input/pfizer-vaccine-tweets/vaccination_tweets.csv\")\n\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the length of the tweets\nseq_length = [len(i) for i in tweets['text']]\n\npd.Series(seq_length).hist(bins = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A bit of cleaning\n\n# remove special characters from text column\ntweets.text = tweets.text.str.replace('[#,@,&]', '')\n#Remove twitter handlers\ntweets.text = tweets.text.str.replace('@[^\\s]+','')\n#Remove digits\ntweets.text = tweets.text.str.replace(' \\d+ ','')\n# remove multiple spaces with single space\ntweets.text = tweets.text.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ntweets.text = tweets.text.str.replace('\\s+', ' ')\n#remove all single characters\ntweets.text = tweets.text.str.replace(r'\\s+[a-zA-Z]\\s+', '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get stopwords\n# Define nltk stopwords in english\nstop_words = stopwords.words('english')\nstop_words.extend(['ha', 'wa', '-'])\n\n# Get a string of tweets \ntweet_text = \",\".join(review.lower() for review in tweets.text if 'covid' not in review)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud(max_font_size=50, \n                      max_words=100, \n                      stopwords=stop_words,\n                      scale=5,\n                      background_color=\"white\").generate(tweet_text)\n\n# Display the generated image:\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Most repeated words in tweets',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word frequency lemmatized"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lemmatize text column by using a lemmatize function\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text.lower())]\n\n\n# Initialize the Lemmatizer and Whitespace Tokenizer\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\n# Lemmatize words\ntweets['lemmatized'] = tweets.text.apply(lemmatize_text)\ntweets['lemmatized'] = tweets['lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n\n# use explode to expand the lists into separate rows\nwf_tweets = tweets.lemmatized.explode().to_frame().reset_index(drop=True)\n\n# plot dfe\nsns.countplot(x='lemmatized', data=wf_tweets, order=wf_tweets.lemmatized.value_counts().iloc[:10].index)\nplt.xlabel('Most common used words')\nplt.ylabel('Frequency [%]')\nplt.xticks(rotation=70)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check top 5 most used hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"MostUsedTweets = tweets.hashtags.value_counts().sort_values(ascending=False)[:5]\ncolors = ['lightcoral', 'lightskyblue', 'yellowgreen', 'grey', 'orange']\nexplode = (0.1, 0.2, 0.1, 0.1, 0.1) \n\n# Wedge properties \nwp = { 'linewidth' : 0.5, 'edgecolor' : \"red\" }\n\n# Creating autocpt arguments \ndef func(pct, allvalues): \n    absolute = int(pct / 100.*np.sum(allvalues)) \n    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute) \n  \n# Creating the plot \nfig, ax = plt.subplots(figsize =(10, 7)) \nwedges, texts, autotexts = ax.pie(MostUsedTweets,  \n                                  autopct = lambda pct: func(pct, MostUsedTweets), \n                                  explode = explode,  \n                                  labels = MostUsedTweets.keys(), \n                                  shadow = True, \n                                  colors = colors, \n                                  startangle = 90, \n                                  wedgeprops = wp, \n                                  textprops = dict(color =\"black\")) \n  \n# Adding legend \nax.legend(wedges, MostUsedTweets.keys(), \n          title =\"Most used tweets\", \n          loc =\"center left\", \n          bbox_to_anchor =(1, 0, 0.5, 1)) \n\n\nplt.setp(autotexts, size=9, weight=\"bold\") \nax.set_title(\"Most used tweets\") \nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tweets source"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncmap = cm.get_cmap('Spectral') \n\ncountries=tweets['source'].value_counts().sort_values(ascending=False)[:5].plot(\n    kind = 'bar', \n    cmap=cmap, \n    edgecolor='None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top 10 tweet posts countries "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get countries which post more tweets\ncmap = cm.get_cmap('Spectral') \n\ncountries=tweets['user_location'].value_counts().sort_values(ascending=False)[:5].plot(\n    kind = 'barh', \n    cmap=cmap, \n    edgecolor='None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopandas\ncities = geopandas.read_file(geopandas.datasets.get_path('naturalearth_cities'))\n\nax = cities.plot()\n\nfor x, y, label in zip(cities.geometry.x, cities.geometry.y, cities.name):\n    ax.annotate(label, xy=(x, y), xytext=(3, 3), textcoords=\"offset points\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment analysis with TextBlob\n\n[TextBlob](https://textblob.readthedocs.io/en/dev/) is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['polarity'] = tweets.text.apply(lambda x: TextBlob(x).polarity)\ntweets['subjectivity'] = tweets.text.apply(lambda x: TextBlob(x).subjectivity)\n\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In TextBlob, based on the polarity and subjectivity, you determine whether it is a positive text or negative or neutral. For TextBlob, if  polarity is > 0, it is considered positive, if polarity < 0 is considered negative and if polarity == 0 is considered as neutral."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['sentiment'] = np.where(tweets.polarity > 0, 'positive', \n                                 np.where(tweets.polarity < 0, 'negative', 'neutral'))\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shows the top 5 tweets with highest polarity scores\ntweets.nlargest(5,'polarity')['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shows the top 5 tweets with highest polarity and subjectivity scores\ntweets.nlargest(5, ['polarity', 'subjectivity'])['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shows the top 5 tweets with lowest polarity scores\ntweets.nsmallest(5,'polarity')['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shows the top 5 tweets with lowest polarity and subjectivity scores\ntweets.nsmallest(5, ['polarity', 'subjectivity'])['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA of Sentiment analysis result"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n# LabelBinarize sentiment column and merge with tweets DF\nlb = LabelBinarizer()\n\nsentbinarized = lb.fit_transform(tweets['sentiment']).tolist()\n#lb.classes_ # Classes of the LabelBinarizer\n\ndfbinarized = pd.DataFrame(sentbinarized, columns=lb.classes_)\ntweets[dfbinarized.columns] = dfbinarized\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.title('Distribution Of Sentiments Across Our Tweets',fontsize=12,fontweight='bold')\nsns.kdeplot(tweets['polarity'], label='Polarity', lw=2.5)\nsns.kdeplot(tweets['subjectivity'], label='Subjectivity', lw=2.5)\nplt.xlabel('Polarity|subjetivity Value', fontsize=10)\nplt.ylabel('Density', fontsize=10)\n# Display the generated image:\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=12, fontweight='bold')\nsns.kdeplot(tweets['polarity'],cumulative=True, label='Polarity',lw=2.5)\nsns.kdeplot(tweets['subjectivity'],cumulative=True, label='Subjectivity',lw=2.5)\nplt.xlabel('Polarity Value', fontsize=10)\nplt.ylabel('Density', fontsize=10)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sorting and feature engineering dates\ntweets = tweets.sort_values(by='date')\ntweets=tweets.copy()\ntweets['date'] = pd.to_datetime(tweets['date']).dt.date\n\ntweets['year']         = pd.DatetimeIndex(tweets['date']).year\ntweets['month']        = pd.DatetimeIndex(tweets['date']).month\ntweets['day']          = pd.DatetimeIndex(tweets['date']).day\ntweets['day_of_year']  = pd.DatetimeIndex(tweets['date']).dayofyear\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}