{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport scipy.stats as ss\nimport sklearn.preprocessing as skpe\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm\nimport sklearn.linear_model as lm\nimport sklearn.ensemble as sken\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Description\n\"\"\"The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\n    CRIM - per capita crime rate by town\n    ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n    INDUS - proportion of non-retail business acres per town.\n    CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n    NOX - nitric oxides concentration (parts per 10 million)\n    RM - average number of rooms per dwelling\n    AGE - proportion of owner-occupied units built prior to 1940\n    DIS - weighted distances to five Boston employment centres\n    RAD - index of accessibility to radial highways\n    TAX - full-value property-tax rate per $10,000\n    PTRATIO - pupil-teacher ratio by town\n    B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    LSTAT - % lower status of the population\n    MEDV - Median value of owner-occupied homes in $1000's\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path=\"../input/boston-housing-dataset/HousingData.csv\"\ndf=pd.read_csv(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variable Identification\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Univariate Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continuous Varaible\ndf.describe()    # CHAS seems to be a categorical variable from the description and describe method","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical Variable\n(df['CHAS'].value_counts()/len(df['CHAS'])*100).plot.bar()   # Frequency Chart","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bivariate Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking correlation bw different variables\nplt.figure(figsize=(18,18))\nsns.heatmap(df.corr(),vmax=.7,cbar=True,annot=True)   # From this heatmap we can conclude that INDUS, NOX AND AGE are highly correlated with DIS.Also the target variable MEDV is having a correlation score > 0.4 with following features:LSTAT, PTRATIO, RM, TAX, INDUS and NOX  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking the type of relationship different features share with the target varaiable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = skpe.MinMaxScaler()\ncolumn_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\nx = df.loc[:,column_sels]\ny = df['MEDV']\nx = pd.DataFrame(data=scaler.fit_transform(x), columns=column_sels)\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.scatterplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relationship with other features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='LSTAT',y='INDUS',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='LSTAT',y='RM',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='LSTAT',y='AGE',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='TAX',y='INDUS',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='TAX',y='NOX',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='TAX',y='LSTAT',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='NOX',y='INDUS',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution of features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box Plots to detect outliers\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.items():\n    sns.boxplot(y=k, data=df, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)  # Columns like CRIM, ZN, RM and B seems to have outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df.drop(['CHAS'],axis=1).items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)   # We have dropped CHAS for this analysis as it is a discrete variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Variable Transformation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['AGE'].skew())\nsns.distplot(np.log1p(df['AGE']),color='Black')\nprint(np.log1p(df['AGE']).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['B'].skew())\nsns.distplot(np.power(df['B'],3),color='Black')\nprint(np.power(df['B'],3).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['PTRATIO'].skew())\nsns.distplot(np.sqrt(df['PTRATIO']),color='Black')\nprint(np.sqrt(df['PTRATIO']).skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing skewness of these features\ndf['LSTAT']=np.log1p(df['LSTAT'])\ndf['AGE']=np.log1p(df['AGE'])\ndf['PTRATIO']=np.log1p(df['PTRATIO'])\ndf['B']=np.power(df['B'],3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Treating Missing Values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing missing values with the Mean\navg_mean_cr = df['CRIM'].astype('float').mean(axis=0)\ndf['CRIM'].replace(np.nan,avg_mean_cr,inplace=True)\n\navg_mean_zn = df['ZN'].astype('float').mean(axis=0)\ndf['ZN'].replace(np.nan,avg_mean_zn,inplace=True)\n\navg_mean_in = df['INDUS'].astype('float').mean(axis=0)\ndf['INDUS'].replace(np.nan,avg_mean_in,inplace=True)\n\navg_mean_ch = df['CHAS'].astype('float').mean(axis=0)\ndf['CHAS'].replace(np.nan,avg_mean_ch,inplace=True)\n\navg_mean_ls = df['LSTAT'].astype('float').mean(axis=0)\ndf['LSTAT'].replace(np.nan,avg_mean_ls,inplace=True)\n\navg_mean_age = df['AGE'].astype('float').mean(axis=0)\ndf['AGE'].replace(np.nan,avg_mean_age,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Building**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Segregating features and labels and dropping unnecessary features\nx=df.drop(['MEDV'],axis=1)\ny=df['MEDV']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling values using MinMaxScaler\nscaler=skpe.MinMaxScaler()\nx_scaled=scaler.fit_transform(x)\nx=pd.DataFrame(x_scaled,columns=x.columns)\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting dataset into train and test sets\ntrain_x,test_x,train_y,test_y=ms.train_test_split(x,y,test_size=0.2,random_state=115)\ntrain_x.shape,test_x.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=lm.LinearRegression(normalize=True)\nlr.fit(train_x,train_y)\nlr.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performance Metrics**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nmodel=sm.OLS(train_y,train_x)\nresult=model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Verifying assumtions to keep in mind while building a linear regression model :-**\n\n**1. There should be a linear relationship between dependent and independent variable. If it's not you can use variable transformation **\n**2. Correlation of error terms,i.e. no particular pattern should be observed **\n**3. Constant variance of error,i.e. no particular pattern should be observed in a plot containing residuals and fitted values **\n**4. Normal Distribution of Errors (Use a Q-Q plot) **\n**5. Minimize Multi-Collinearity by eliminating on of the features which are having multi-collinearity (Use VIF technique)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the probabality distribution of the two quantiles\n\n# Q-Q Normal plot\ndef resid_qq(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(test_y, y_score)\n    ## now make the residual plots\n    ss.probplot(resids, plot = plt)\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Quantiles of standard Normal distribution')\n    plt.ylabel('Quantiles of residuals')\n    \ny_score=lr.predict(test_x)\nresid_qq(test_y, y_score)         # This is perfectly fine even though it has a few outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A common misconception is that the features or label of a linear regression model must have Normal distributions. This is not the case! Rather, the residuals (errors) of the model should be Normally distributed\n\ndef hist_resids(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(test_y, y_score)\n    ## now make the residual plots\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(test_y, y_score)        # This is almost good exvept for the reason that it is slightly right-skewed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dataframe of residuals,test values and predicted values\nresiduals = pd.DataFrame({\n    'fitted values' : test_y,\n    'predicted values' : y_score\n})\n\nresiduals['residuals'] = residuals['fitted values'] - residuals['predicted values']\nresiduals.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Residual scatterplot\n# Remember,there should be no pattern observed in this plot;the residuals should be randomly distributed across the plot\nf = range(0,102)\nk = [0 for i in range(0,102)]\nplt.scatter( f, residuals.residuals[:], label = 'residuals')\nplt.plot( f, k , color = 'red', label = 'regression line' )\nplt.xlabel('fitted points ')\nplt.ylabel('residuals')\nplt.title('Residual plot')\nplt.ylim(-4,4)\nplt.legend()      # This is good","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Error terms\nplt.hist(residuals.residuals, bins = 150)\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.title('Distribution of Error Terms')\nplt.show()        # It seems fine but there are a few outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Again seperating features and labels to check each feature's weightage**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Segregating features and labels\nx=df.drop(['MEDV'],axis=1)\ny=df['MEDV']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Coefficients = pd.DataFrame({\n    'Variable'    : x.columns ,\n    'coefficient' : lr.coef_\n})\nCoefficients.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Normalized Coefficient plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}