{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### import data, using Pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature manipulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xcheck=np.array(dftrain.iloc[:,1:])\nycheck=np.array(dftrain.iloc[:,0])\n\n# scale the X vector, dividing for the maximum value (i.e. 255)\nX=Xcheck/255\n\n# convert y in dummy variable. This is necessary as output of the neural network\ny=np.array(pd.get_dummies(ycheck))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### image visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape of the vector, to visualize the image\n\ndef numimg2(t,X):\n    vecreshape=np.reshape(X[t,:],[28,28])\n    return vecreshape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visuale a number\n\nprint(ycheck[5016])\n\nplt.imshow(numimg2(5016,X),cmap='gray')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network: building and training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# beginning of deep learning\n\nncols=X.shape[1]       # number of elements for the input layer\nnumutput=10            # number of elements for output layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I use train_test_split function to split in 80% training data and 20% test data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\nfrom keras.layers.core import Dense, Dropout, Activation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sequential Neural Network, with 2 hidden layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = Sequential()\n\n# input and 1th hidden layer with 100 neurons. \nmodel.add(Dense(500, input_shape=(ncols,)))\nmodel.add(Activation('relu'))                            \nmodel.add(Dropout(0.2))   # setting 20% of neurons to 0 (randomly). This should help in avoiding overfitting\n\n# 2th hidden layer with 200 neurons. \nmodel.add(Dense(500))\nmodel.add(Activation('relu'))                            \nmodel.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\n# Here I compile the model\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# I set 100 epochs as default, with an Early Stop in case of no improvement after 5 epochs\nearly_stopping_monitor = EarlyStopping(patience=5)\n\nhistory = model.fit(X_train,y_train,\n                    batch_size=128, epochs=100,\n                    validation_data=(X_test,y_test),callbacks=[early_stopping_monitor])\n\nprint(\"Loss function: \" + model.loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network: performance on the validation test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the validation dest\ndftest = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\n\nX_val_test=np.array(dftest.iloc[:,1:])/255\ny_val_test=np.array(dftest.iloc[:,0])\n\n#prediction of the model (probability)\nresult=np.round(model.predict(X_val_test),1)\n\n#prediction of the model (I select the number with the maximum probability)\nprediction=np.argmax(result,axis=1)\n\n# build the confusion matrix\npd.DataFrame(confusion_matrix(y_val_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy=round(np.sum(y_val_test==prediction)/len(prediction)*100,1)\n\nprint('The accuracy of the model on the validation data is: '+str(accuracy)+'%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}