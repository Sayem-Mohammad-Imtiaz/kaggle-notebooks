{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current \n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nimport sklearn.metrics as met\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\n\nhappy = pd.read_csv('/kaggle/input/world-happiness/2019.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### You may run our dataset part by part,or model by model, since we may use the same table name. \n\n                   Introduction\nThe data set name: World Happiness Rank Report 2019\n\n#Our Hypothesis is Health and GDP can determine people's happniess.\n\nWorld Happiness Report in 2019, according to the data set retrieved from Kaggle which is based on the report published by United Nations Sustainable Development Solutions Network.\n\nWe conclude that by analyzing these data we will pay more attention to the things in life that make us the happiest, and we'd like to know the components that make people happy. We would implement the following criteria: GDP per Capita, Family, Life Expectancy, Freedom, Confidence (Government Corruption) across 156 countries across the world. \n"},{"metadata":{},"cell_type":"markdown","source":"    Data Understanding\n\nThe World Happiness Report is published annually by the United Nations Sustainable Development Solutions Network. It includes papers and rankings of national satisfaction based on the answer scores of their own lives, which are often associated with different life conditions.\nThe happiness scores were calculated through different variables such as  Economy(GDP per capita), Social support, Health(healthy life expectancy), Freedom(to make life choices), Generosity and Government Trust (Perception of Corruption). Compared to the values of GDP and Social support, the values of Generosity and Government Trust are rather small, so we think it might be  the least influential factor. The first variable of  overall rank is numeric and ranks happiness from 156 countries or regions. The second variable is “Country or region” which is the nominal variable. It contains 156 countries or regions names. From the third column all variables are numeric and are based on a happiness scale from one to ten. \n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n###        Data cleaning\n#  First, we check how much of our data is missing, and found there are no missing values.\n\n#  Second, renamed the column names to make it easier to work with the data. Typing out 'GDP Per Capita' is \n#cumbersome at best so 'GDC' is far preferable.  made all the names one word long.\nhappy.rename(columns={ 'GDP per capita' : 'GDP',  'Social support' : 'Social', \n 'Healthy life expectancy' : 'Health',  \n 'Freedom to make life choices' : 'Freedom',  'Generosity': 'Generosity', \n 'Perceptions of corruption' : 'Corruption'}, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardize the variables to see if there are any outliers within the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Third, we find that the variables of the  data are all between 0 to 2. And the response value is between 0-10.  \n#  Standardize the variables to see if there are any outliers within the data.\nhappy['zGDP'] = stats.zscore(happy['GDP'])\nhappy['zSocial'] = stats.zscore(happy['Social'])\nhappy['zHealth'] = stats.zscore(happy['Health'])\nhappy['zFreedom'] = stats.zscore(happy['Freedom'])\nhappy['zGenerosity'] = stats.zscore(happy['Generosity'])\nhappy['zCorruption'] = stats.zscore(happy['Corruption'])\nhappy.query('zGDP > 3 | zGDP < -3')\n# No outliers for zGDP\nhappy.query('zSocial > 3 | zSocial < -3')\n# Central Africa Republic has zSocial score of 4.05\noSocial = happy.query('zSocial > 3 | zSocial < -3')\n# I'm saving this outlier\nhappy.query('zHealth > 3 |zHealth  < -3')\n# Swaziland has a zHealth score of -3.005\noHealth = happy.query('zHealth > 3 |zHealth  < -3')\n# I'm saving this outlier\nhappy.query('zFreedom > 3 | zFreedom  < -3')\n# No outlier for zFreedom\nhappy.query('zGenerosity > 3 |zGenerosity  < -3')\n# Myanmar has a zGenerosity score of 4.01 \n# Indonesia has a zGenerosity score of 3.298 \noGenerosity = happy.query('zGenerosity > 3 |zGenerosity  < -3')\nhappy.query('zCorruption > 3 | zCorruption < -3')\n# Singapore has a zCorruption score of 3.63\n# Rwanda has a zCorruption score of 3.19\n# Denmark has a zCorruption score of 3.18\noCorruption = happy.query('zCorruption > 3 | zCorruption < -3')\n# No outlier for zFreedom\n# Outliers are standardized numbers above 3 or below -3. 7 total countries across four categories fall into the outlier designation.\nhappy_outliers = oSocial, oHealth, oGenerosity, oCorruption\n# This stores all of the outliers in one place if we want to see them\n# Because our numbers deal with people's perceptions, we won't remove the outliers.\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Exploratory Data Analytics\n\nWe would like to dig into the most important variables that can have more impact on the happiness of a person. We will analyze the dataset from different angles to validate our hypothesis.\n\nThe Visualization shown as below:\nMatplot \nHistograms \nCorrelation between variable\nPolar Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n#           Exploratory Data Analytics:\n\n# Matplot / Visualization   \nplt.plot(range(len(happy['Score'])),happy['Score'])\nplt.ylabel(\"Happiness Score\")\nplt.title(\"Happiness Score Matplot\")\nplt.show()\nplt.plot(range(len(happy['GDP'])),happy['GDP'])\nplt.ylabel(\"GDP per Capita\")\nplt.title(\"GDP Matplot\")\nplt.show()\n\nplt.plot(range(len(happy['Social'])),happy['Social'])\nplt.ylabel(\"Social Support\")\nplt.title(\"Social Support Matplot\")\nplt.show()\n\nplt.plot(range(len(happy['Health'])),happy['Health'])\nplt.ylabel(\"Life Expectancy\")\nplt.title(\"Healthy Life Expectancy\")\nplt.show()\n\nplt.plot(range(len(happy['Freedom'])),happy['Social'])\nplt.ylabel(\"Freedom to make life choices\")\nplt.title(\"Freedom to make life choices\")\nplt.show()\n\nplt.plot(range(len(happy['Generosity'])),happy['Social'])\nplt.ylabel(\"Generosity\")\nplt.title(\"Generosity\")\nplt.show()\n\nplt.plot(range(len(happy['Corruption'])),happy['Social'])\nplt.ylabel(\"Perceptions of corruption\")\nplt.title(\"Perceptions of corruption\")\nplt.show()\n# the matplots have the same trends with histograms and not \n# easy to read. Not show in presentation. \n# Histograms Visualization                         \n#All variables\nplt.hist(happy['Score'],color='green',alpha=0.35)\nplt.xlabel('Happiness Score')\nplt.ylabel('Frequency')\nplt.title('Happiness Summary')\nplt.show\n##------------------------------------------------------------------\n\nplt.hist(happy['GDP'],edgecolor='k',alpha=0.35)\nplt.xlabel('GDP Value per capita')\nplt.ylabel('Frequency')\nplt.title('GDP per Capita')\nplt.show\n##------------------------------------------------------------------\n\nplt.hist(happy['Social'],color='r',alpha=0.35)\nplt.xlabel('Social Support')\nplt.ylabel('Frequency')\nplt.title('Social Support')\nplt.show\n##------------------------------------------------------------------\n\nplt.hist(happy['Health'],color='orange',alpha=0.35)\nplt.xlabel('Life Expectancy')\nplt.ylabel('Frequency')\nplt.title('Healthy Life Expectancy')\nplt.show\n##------------------------------------------------------------------\n\nplt.hist(happy['Freedom'],edgecolor='k',alpha=0.35)\nplt.xlabel('Freedom to make life choices')\nplt.ylabel('Frequency')\nplt.title('Freedom to make life choices')\nplt.show\n##------------------------------------------------------------------\n\nplt.hist(happy['Generosity'],color='darkblue',alpha=0.35)\nplt.xlabel('Generosity')\nplt.ylabel('Frequency')\nplt.title('Generosity')\nplt.show\n##------------------------------------------------------------------\n\nplt.hist(happy['Corruption'],color='purple',alpha=0.35)\nplt.xlabel('Perceptions of corruption')\nplt.ylabel('Frequency')\nplt.title('Perceptions of corruption')\nplt.show\n#Most of the respondents in the world happiness report feel their life\n# at an average level of happiness which is in the range between 4.5 and 6.5.\n# Corresponding the GDP level is similar. Most of the people do not take care about\n# the perception of corruption while freedom to make life choices is extremely important for them.\n#------------------------------------------------------\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histograms Visualization\n\nNext step is to observe the value distribution of variables. We chose creating histograms for each variable which can be clearly shown. From Figure 4 we can see the happiness score summarized how people feel about their life. Luckily, most of the respondents in the world happiness report feel their life at an average level of happiness which is in the range between 4.5 and 6.5. Corresponding the GDP level is similar. Most of the people do not take care about the perception of corruption while freedom to make life choices is extremely important for them although the scores are not good.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histograms Visualization with overlay and four groups    \n#there are totally 156 countries or regions in the rank list. I divided them into\n#4 groups according to the ranking.\n# Rename the Column name of Overall rank and Country or region\nhappy_df=happy\nhappy_df.rename(columns={ 'Overall rank' : 'Rank',  'Country or region' : 'Area'}, inplace=True)\n\nhappy_1=happy_df[happy_df.Rank<40]\nhappy_2=happy_df[(happy_df.Rank>39)&(happy_df.Rank<79)]\nhappy_3=happy_df[(happy_df.Rank>78)&(happy_df.Rank<118)]\nhappy_4=happy_df[happy_df.Rank>117]\nhappy_1.shape\nhappy_2.shape\nhappy_3.shape\nhappy_4.shape\n\nhappyset1=happy_1['Score']\nhappyset2=happy_2['Score']\nhappyset3=happy_3['Score']\nhappyset4=happy_4['Score']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('Happiness Score');plt.ylabel('Frequency')\nplt.title('Histogram of Happiness Score')\nplt.show()\n\nhappyset1=happy_1['GDP']\nhappyset2=happy_2['GDP']\nhappyset3=happy_3['GDP']\nhappyset4=happy_4['GDP']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('GDP');plt.ylabel('Frequency')\nplt.title('Histogram of GDP per capita')\nplt.show()\n\nhappyset1=happy_1['Social']\nhappyset2=happy_2['Social']\nhappyset3=happy_3['Social']\nhappyset4=happy_4['Social']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('Social Support');plt.ylabel('Frequency')\nplt.title('Histogram of Social Support')\nplt.show()\n\nhappyset1=happy_1['Health']\nhappyset2=happy_2['Health']\nhappyset3=happy_3['Health']\nhappyset4=happy_4['Health']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('Healthy Life Expectancy');plt.ylabel('Frequency')\nplt.title('Histogram of Healthy Life Expectancy')\nplt.show()\n\n\nhappyset1=happy_1['Freedom']\nhappyset2=happy_2['Freedom']\nhappyset3=happy_3['Freedom']\nhappyset4=happy_4['Freedom']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('Freedom to make life choices');plt.ylabel('Frequency')\nplt.title('Histogram of Freedom to make life choices')\nplt.show()\n\nhappyset1=happy_1['Generosity']\nhappyset2=happy_2['Generosity']\nhappyset3=happy_3['Generosity']\nhappyset4=happy_4['Generosity']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('Generosity');plt.ylabel('Frequency')\nplt.title('Histogram of Generosity')\nplt.show()\n\nhappyset1=happy_1['Corruption']\nhappyset2=happy_2['Corruption']\nhappyset3=happy_3['Corruption']\nhappyset4=happy_4['Corruption']\n(n_ha,bins,patches)=plt.hist([happyset1,happyset2,happyset3,happyset4],bins=4,stacked=True)\nplt.legend(['Rank1-39','Rank40-78','Rank79-117','Rank118-156'])\nplt.xlabel('Perceptions of corruption');plt.ylabel('Frequency')\nplt.title('Histogram of Perceptions of corruption')\nplt.show()\n# The histogram can not show any rules among the groups. \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between variable\n\nSince we have not analysed the dataset deeply, we created heat maps to look through the correlations between variables. Three correlations heat maps were created, variables for all countries, variables for top 10 and last 10 countries or regions, respectively.\nThe heat map for all countries shows the most important factors of happiness are economic level (GDP per capita) and Health(healthy life expectancy). However, the trend of the top 10 and last 10 countries and regions are distinct-different. People in the top 10 areas do not think happiness is related to economic level, while they are more focusing on social support and freedom to make life decisions. By contrast, people in the bottom 10 areas are still caring about the economy and health which are the most important to decide their life happiness.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################################\n#Correlation between variable ：\n#Option1:\nhappy.corr()\n#It is simple to create a correlation table. However, it is hard to distinguish\n#from table the weighfactors at the first glance. Therefore, a heat map was created below.\n#Option2: heat map for correlation between varialbes.\ncorr_matrix1,ax=plt.subplots(figsize=(10, 10))\nsns.heatmap(happy.corr(), ax=ax, annot=True, linewidth=0.05, fmt='.2f', cmap='magma')\nplt.title(\"Correlations Overall Areas\")\nplt.show()\n#In this Heat Map we can see that Happiness Score is very highly correlated with GDP, \n#Social support, and Life expectancy and somewhat related with Freedom also, \n#but has a very low relation with Generosity and Perceptions of corruption in average case.\n\n# Created a new dataframe of top10 happiness countries and add a new index to category\ntop10=happy.iloc[0:10, 0:]\ntop10['category']='Top10'\ntop10.ax=plt.subplots(figsize=(10,10))\ntop10_d=top10.loc[lambda top10: top10['category']=='Top10']\ntop10_matrix=np.triu(top10_d.corr())\nsns.heatmap(top10_d.corr(),cmap='ocean', annot=True)\nplt.title(\"Correlations of Top 10 Areas\")\nplt.show()\n#The Heat Map particularly for Top 10 areas has one more thing to add apart from Family Satisfaction, Freedom, Economy, Generosity, It is also highly correlated with Trust in Government.\n\n # Created a new dataframe of last10 happiest countries and add a new index to category\nbottom10=happy.iloc[146:156, 0:]\nbottom10['category']='Bottom10'\nplt.rcParams['figure.figsize']=(10, 10)\nbottom10_d=bottom10.loc[lambda bottom10: bottom10['category']=='Bottom10']\nsns.heatmap(bottom10_d.corr(), cmap='Wistia', annot=True)\nplt.title(\"Correlations of Last 10 Areas\")\nplt.show()\n\n#Correlation between variable ：\n#In this Heat Map we can see that Happiness Score is very highly correlated with GDP, \n#Social support, and Life expectancy and somewhat related with Freedom also, \n#but has a very low relation with Generosity and Perceptions of corruption in average case.\nplt.rcParams['figure.figsize'] = (20, 15)\nsns.heatmap(happy.corr(), cmap = 'copper', annot = True)\nplt.show()\n#the trend of the top 10 and last 10 countries and regions are distinct-different.\n# People in the top 10 areas do not think happiness is related to economic level,\n# while they are more focusing on social support and freedom to make life decisions.\n# By contrast, people in the bottom 10 areas are still caring about the economy \n#and health which are the most important to decide their life happiness.\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Polar Chart\nPolar charts make it possible to research the features of one or more entities with a variety of numerical variables. Demonstrate as many charts as the number of people, making it easier to compare the outline of each chart.\nWe chose the top 5 and bottom 5 countries or regions and set up two polar charts to compare the influential factors of happiness. It is obvious that the top 5 countries have the similar influential factors while the bottom countries have quite different factors.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################################\n#Construct Polar Chart for top 5 and bottom 5 for observation of trends and relationships\n\n#top5 areas dataset to array\nhappy001=happy.iloc[0,3:9]\nhappy1=np.array(happy001)\nhappy1=happy1.tolist()\nhappy1=np.concatenate((happy1,[happy1[0]]))\n\nhappy002=happy.iloc[1,3:9]\nhappy2=np.array(happy002)\nhappy2=happy2.tolist()\nhappy2=np.concatenate((happy2,[happy2[0]]))\n\nhappy003=happy.iloc[2,3:9]\nhappy3=np.array(happy003)\nhappy3=happy3.tolist()\nhappy3=np.concatenate((happy3,[happy3[0]]))\n\nhappy004=happy.iloc[3,3:9]\nhappy4=np.array(happy004)\nhappy4=happy4.tolist()\nhappy4=np.concatenate((happy4,[happy4[0]]))\n\nhappy005=happy.iloc[4,3:9]\nhappy5=np.array(happy005)\nhappy5=happy5.tolist()\nhappy5=np.concatenate((happy5,[happy5[0]]))\nprint(happy1);print(happy2);print(happy3);print(happy4);print(happy5)\n###################################\n#Bottom 5 areas dataset to array\nhappy0152=happy.iloc[151,3:9]\nhappy152=np.array(happy0152)\nhappy152=happy152.tolist()\nhappy152=np.concatenate((happy152,[happy152[0]]))\n\nhappy0153=happy.iloc[152,3:9]\nhappy153=np.array(happy0153)\nhappy153=happy153.tolist()\nhappy153=np.concatenate((happy153,[happy153[0]]))\n\nhappy0154=happy.iloc[153,3:9]\nhappy154=np.array(happy0154)\nhappy154=happy154.tolist()\nhappy154=np.concatenate((happy154,[happy154[0]]))\n\nhappy0155=happy.iloc[154,3:9]\nhappy155=np.array(happy0155)\nhappy155=happy155.tolist()\nhappy155=np.concatenate((happy155,[happy155[0]]))\n\nhappy0156=happy.iloc[155,3:9]\nhappy156=np.array(happy0156)\nhappy156=happy156.tolist()\nhappy156=np.concatenate((happy156,[happy156[0]]))\nprint(happy152);print(happy153);print(happy154);print(happy155);print(happy156)\n#############################\n\n#set polar chart shapes and labels(same for two charts)\nlabels=np.array(['GDP per capita','Social Support','Healthy Life Expectancy',\n                 'Freedom to make life choices', 'Generosity','Perceptions of corruption'])\nhappyLength=6\nangles=np.linspace(0,2*np.pi, happyLength, endpoint=False)\nangles=np.concatenate((angles,[angles[0]]))\n\n#Polar chart1 for top5 areas\nfig1=plt.figure()\nax1=fig1.add_subplot(111,polar=True)\nax1.plot(angles,happy1,'ro-',label='Finland',color='r',linewidth=3)\nax1.plot(angles,happy2,'ro-',label='Denmark',color='y',linewidth=3)\nax1.plot(angles,happy3,'ro-',label='Norway',color='orange',linewidth=3)\nax1.plot(angles,happy4,'ro-',label='Iceland',color='pink',linewidth=3)\nax1.plot(angles,happy5,'ro-',label='Netherlands',color='black',linewidth=3)\n\nax1.set_thetagrids(angles*180/np.pi,labels,fontproperties=\"fontprop\")\nax1.set_title(\"Top5 Country or Region\",va='bottom',fontproperties=\"Times New Roman\")\nax1.grid(True)\nplt.legend()\nplt.show()\n\n#set polar chart shapes and labels(same for two charts)\nlabels=np.array(['GDP per capita','Social Support','Healthy Life Expectancy',\n                 'Freedom to make life choices', 'Generosity','Perceptions of corruption'])\nhappyLength=6\n\nangles=np.linspace(0,2*np.pi, happyLength, endpoint=False)\n\nangles=np.concatenate((angles,[angles[0]]))\n\n#Polar chart2 for bottom5 areas\nfig2=plt.figure()\nax2=fig2.add_subplot(111,polar=True)\nax2.plot(angles,happy152,'ro-',label='Rwanda',color='r',linewidth=3)\nax2.plot(angles,happy153,'ro-',label='Tanzania',color='y',linewidth=3)\nax2.plot(angles,happy154,'ro-',label='Afghanistan',color='orange',linewidth=3)\nax2.plot(angles,happy155,'ro-',label='Central African Republic',color='pink',linewidth=3)\nax2.plot(angles,happy156,'ro-',label='South Sudan',color='blue',linewidth=3)\n\nax2.set_thetagrids(angles*180/np.pi,labels,fontproperties=\"fontprop\")\nax2.set_title(\"Bottom5 Country or Region\",va='bottom',fontproperties=\"Times New Roman\")\nax2.grid(True)\nplt.legend()\nplt.show()\n# the top 5 countries have the similar influential factors while the bottom countries\n# have quite different factors\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Perpare modeling : \n                    Partition/ Validation/ Methodology\n     \n"},{"metadata":{},"cell_type":"markdown","source":"The Y value of our dataset is continuous variables, so here are the models we can do if we use supervised learning:#Regression, Linear Regression, Decision Tree, RandomForest, GBT, AFT Survival Regression, Isotonic Regression.\nAt the same time, when testing and plotting those grafts we found that these insightful variables definitely had a linear relationship.\n\n\n#We chose three of them and made a comparison which is Random Forest Regression, Multiple Regression and DecisionTrees.\n    \nAfter the Horizontal and vertical contrast we found 78%with 22% is the best split. Since it has the least error of each model.\n\nWe split the dataset to train 78% & test 22% which is(121, 35) in order to improve the performance of our small size data training. Because When we did a lot of tests of MAE in each model, with different partitions, for example 80% with 20%, 78% with 22%, 75%with 25%, 70% with 30%, 67% with 32%, and 65% with 35%. After the Horizontal and vertical contrast we found 78%with 22% is the best split. Since it has the least error of each model.  After calculating the MAEbaseline,  and comparing it with the models: Multiple Regression MAE is 0.5004 ,  the Random Forest Regressor is 0.436, the DecisionTrees is 0.4804 and the MAEbaseline is  0.8949.  We noticed that the best performance model is Random forest regressor, which is the highest accuracy of our model. Also, because our data is linear, it is also very important to evaluate the linear model. \n\nSo we are focused on these two models, and use them to do the prediction. \n\n\n \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"##############################################################################\n##############################################################################\n#           Partition/ Validation/ Methodology\n# we split our data set to train 78% & test 22% which is(121：35),in order to improve the performance of our small size data trainning.\nhappy_train2, happy_test2 = train_test_split(happy, test_size = 0.22, random_state = 7)\n\n# The Y value of our dataset is continuous variables, so here are the models we can do if we use supervised learning:\n    #Regression, Linear Regression, Decision Tree, RandomForest, GBT, AFT Survival Regression, Isotonic Regression.\n# We chose three of them and made a comparison which is Random Forest Regression, Multiple Regression and DecisionTrees.\n\n# We did a lot of tests of MAE in each model, with different partitions, \n    #for example 80% with 20%, 78% with 22%, 75%with 25%, 70% with 30%, 67% with 32%, and 65% with 35%.\n#After the Horizontal and vertical contrast we found 78%with 22% is the best split. Since it has the least error of each model.\n\n#############################################################################\n#          Using three model to test/validate our Partition and Methodology\n\n#           Random Forest Regressor\n#Again, Random ForestClassifier and 'gini' are used for int data. Because Score out data is floating point (there are always decimals), we're using Random Forest Regressor\nhappy_train2, happy_test2 = train_test_split(happy, test_size = 0.22, random_state = 7)\n#### For overall \ncols = ['GDP', 'Health', 'Freedom','Social', 'Corruption', 'Generosity']\nXtrain_2 = happy_train2[cols]\nytrain_2 = happy_train2['Score']\n# Test the max_depth with 1,2,3,4,5. and 4 performance the best result.( which is less error)\nrf01 = RandomForestRegressor(max_depth=4, random_state = 0)\nrf01.fit(Xtrain_2, ytrain_2)\n\n#Create Test data set:        \nXtest_2 = happy_test2[cols]\nytest_2= happy_test2['Score']\n\n# Use the test data to make predictions\nrf26_pred = rf01.predict(Xtest_2)\nMSE = np.sum(np.power(np.array(ytest_2) - np.array(rf26_pred), 2)) / len(ytest_2)\n#MSE:0.3564\nMAE_test = met.mean_absolute_error(ytest_2, rf26_pred)\n# MAE is obtained by subtracting the predict data from the actual data : 0.4360\n\n#Random Forest Regressor performance Visualization\nplt.figure()\nplt.scatter(rf26_pred, ytest_2)\nplt.plot([3, 7], [3, 7], 'k')\nplt.title('Random Forest Regressor with overall')\nplt.xlabel('Predict Happiness Score') \nplt.ylabel('Actual Happiness Score')\n\n\n############################################\n#          Multiple Regression model\n# Since we want to compare the Multiple Regression model with others models, so we did this overall model\n# Partitioned data use same partition with Random Forest Regressor\n#Training data set\nhappy_train2, happy_test2 = train_test_split(happy, test_size = 0.22, random_state = 7)\nx1 = pd.DataFrame(happy_train2[['GDP', 'Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nx1 = sm.add_constant(x1)\ny1 = pd.DataFrame(happy_train2[['Score']])\nmodel1 = sm.OLS(y1, x1).fit()\nmodel1.summary()\n\n# Test data set\nx2 = pd.DataFrame(happy_test2[['GDP', 'Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nx2 = sm.add_constant(x2)\ny2 = pd.DataFrame(happy_test2[['Score']])\nmodel2 = sm.OLS(y2, x2).fit()\nmodel2.summary()\n\n\ny2pred = model1.predict(x2)\nMAEregression_test = met.mean_absolute_error(y2, y2pred)\n#0.5129530374043557\n\n#Multiple Regression model performance Visualization\nplt.figure()\nplt.scatter(y2pred, y2)\nplt.plot([3, 7], [3, 7], 'k')\nplt.title('Multiple Regression model & Overall')\nplt.xlabel('Predict Happiness Score') \nplt.ylabel('Actual Happiness Score')\n\n###########################################\n#                    DECISION TREEs\n# Since we want to compare the Decision Tree model performance with others models, so we did this overall model\nhappy_train2, happy_test2 = train_test_split(happy, test_size = 0.22, random_state = 7)\n#Train data set\nx1 = pd.DataFrame(happy_train2[['GDP', 'Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nx1 = sm.add_constant(x1)\ny1 = pd.DataFrame(happy_train2[['Score']])\ncart01 = DecisionTreeRegressor(criterion = 'mse', max_leaf_nodes = 8).fit(x1,y1)\n#Test data set\nx2 = pd.DataFrame(happy_test2[['GDP', 'Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nx2 = sm.add_constant(x2)\ny2 = pd.DataFrame(happy_test2[['Score']])\n# Use the test data to make predictions\ndt_pred = cart01.predict(x2)\n#Ther MAE is 0.5613471638655461\nMAE_test = met.mean_absolute_error(y2, dt_pred)\n\n#Decision Tree model performance Visualization\nplt.figure()\nplt.scatter(dt_pred, y2)\nplt.plot([3, 7], [3, 7], 'k')\nplt.title('Decision Tree & Overall')\nplt.xlabel('Predict Happiness Score') \nplt.ylabel('Actual Happiness Score')\n\n##############################################################################\n#MAEbaseline = 0.8949\nMAEbaseline = np.mean(y1)\ny2.shape\nMAEbaseline_list = ([5.444917]*35)\nMAE_test = met.mean_absolute_error(y2, MAEbaseline_list)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modeling and Predictive Analytics:\n\nDecisionTrees \nDecisionTreeClassifier and 'gini' are used for int data. Because Score out data is floating point (there are always decimals), we're using DecisionTreeRegressor and 'mse'.\nHere’s a Decision Tree regression model with the five strongest predictors from our six variable dataset.  The first split occurs along the Social rating. Countries that have a less than or equal to a 1.206 go to the left, those that are higher go right. Following the True branch down, we see further splits using GDP (less than or equal to 0.4491), and Freedom (less than or equal to .024).\nAlong the False (or higher Social value) branch, we see splits along GDP ( less than or equal to 1.266), Health (less than or equal to 0.857) and Social (less than or equal to 1.442) before the final split at Freedom (less than or equal to 0.405). There are eight boxes that represent an ending for one of the branching paths. Here they are in order of highest Happiness rating to lowest. The short explanation is that higher numbers in any of these categories equals higher happiness. The summary shows that all independent variables have a significant impact. Ultimately, Corruption and Generosity have the least pull on a country’s overall Happiness level. \n\n\nRandom Forest Regression \nAgain, we use Random Forest Regressor, because the data Y value is continuous variables. We want to know the correlation of variables, so we do the test by taking out one variable and observing whether the error is increasing or decreasing. Here is the result of  each time:\n#Take out GDP, the MAE is: 0.4697 > overall MAE 0.4360\n#Take out Health, the MAE is:0.4810  >overall MAE 0.436\n#Take out Freedom, the MAE is:0.4386 > overall MAE 0.436\n#Take out Social, the MAE is:0.4682 >overall MAE 0.436\n#Take out Corruption, the MAE is:0.4387 >overall MAE 0.436\n#Take out Generosity, the MAE is:0.4297< overall MAE 0.436\nBy this comparison, we can see that the two most relevant are GDP and Health , for example when we take out Health, the MAE increases lots compared to others, so We believe that health is the biggest influencing factor affecting the entire data set. And number two is GDP , number three is Social support, which confirmed the previous analysis when exploring data through the visualization.\n\n\nMultiple Regression \nHere we use the same logic as Random Forest Regressor. To know the correlation of variables we do the test by taking out one variable and observing whether the error is increasing or decreasing. Here is the result of  each time:\n#Take out GDP, the MAE is 0.445  : > overall MAE 0.5004\n#Take out Health, the MAE  is 0.501 : > overall MAE 0.5004\n#Take out Freedom, the MAE is: 0.461 > overall MAE 0.5004\n#Take out Social, the MAE is: 0.463 > overall MAE 0.5004\n#Take out Corruption, the MAE is: 0.446 > overall MAE 0.5004\n#Take out Generosity, the MAE is: 0.447 > overall MAE 0.5004\nBy this comparison, we can see that the two most relevant are Health and Social, for example when we take out Health, the MAE increases lots compared to others, so We believe that health is the biggest influencing factor affecting the entire data set. And number two is Social , number three is Freedom, which confirmed the previous analysis when exploring data through the data visualization.\n \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################################\n     #           Modeling and Predictive Analytics\n############################################################################\n#DECISION TREE\n# DecisionTreeClassifier and 'gini' are used for int data. Because Score out data is floating point (there are always decimals), we're using DecisionTreeRegressor and 'mse'\n# For Overall X\ny = happy[['Score']]\nx = happy[['GDP', 'Health','Social', 'Freedom', 'Corruption', 'Generosity']]\nx_names = ['GDP', 'Health','Social', 'Freedom', 'Corruption', 'Generosity']\ncart01 = DecisionTreeRegressor(criterion = 'mse', max_leaf_nodes = 8).fit(x,y)\n#cannot show from here, but can show on your computer\n#export_graphviz(cart01, out_file = \"/Users/zli/Downloads/894_813759_bundle_archive/happy_cartall.dot\", feature_names=x_names)\n# Here’s a decision tree regression model with the five strongest predictors from our six variable dataset.  The first split occurs along the Social rating. Countries that have a less than or equal to a 1.206 go to the left, those that are higher go right.\n\n# Following the True branch down, we see further splits using GDP (less than or equal to 0.4491), and Freedom (less than or equal to .024).\n\n# Along the False (or higher Social value) branch, we see splits along GDP ( less than or equal to 1.266), Health (less than or equal to 0.857) and Social (less than or equal to 1.442) before the final split at Freedom (less than or equal to 0.405).\n\n# There are eight boxes that represent an ending for one of the branching paths. Here they are in order of highest Happiness rating to lowest.\n\n# Happy Rating: 7.079 (Social greater than 1.442, GDP greater than 1.266)\n# Happy Rating: 6.161 (Social less than or equal to 1.442, GDP greater than 1.266)\n# Happy Rating: 6.109 (Social greater than 1.206, GDP less than or equal to 1.266, Health greater than 0.857)\n# Happy Rating: 5.729 (Social greater than 1.206, GDP less than or equal to 1.266, Health less than or equal to 0.857), Freedom greater than 0.0405)\n# Happy Rating: 5.287 (Social greater than 1.206, GDP less than or equal to 1.266, Health less than or equal to 0.857), Freedom less than or equal to 0.0405)\n# Happiness Rating 4.716 (Social less than or equal to 1.206, GDP greater than 0.491)\n# Happy Rating: 4.206 (Social less than or equal to 1.206, GPD less than or equal to 0.491, Freedom greater than 0.24)\n# Happy Rating: 3.572 (Social less than or equal to 1.206, GPD less than or equal to 0.491, Freedom less than or equal to 0.24)\n\n# That’s a lot of numbers! The short explanation is that higher numbers in any of these categories equals higher happiness. \n\ny = happy[['Score']]\nx = happy[['GDP', 'Health']]\nx_names = ['GDP', 'Health']\ncart01 = DecisionTreeRegressor(criterion = 'mse', max_leaf_nodes = 5).fit(x,y)\n#cannot show from here, but can show on your computer\n#export_graphviz(cart01, out_file = \"/Users/zli/Downloads/894_813759_bundle_archive/2019happy_cart01.dot\", feature_names=x_names)\n# The top box shows that Health is going to be the first branching point. Countries with a health rating that's less than or equal to .65 go to the left, those who are greater than .66 go to the right. That means the healthiest people (or those with the longest life expectancy) go to the right and the unhealthiest people (or those who have a low life expectancy) go the left. \n\n# 51 countries fall into the less healthy category whereas 105 fall in the healthier category.\n\n# The next split is based on GDP. One interesting thing to note is the dividing point between the two boxes. The left (less healthy) box splits at less than or equal to .501. The right (or healthier) box splits at less than or equal to 1.266. Those that life longer have significantly more money.\n\n# The poorer, less health section doesn't split anymore. 29 of the 51 samples have a GDP score of less than or equal to .501 whereas 22 samples are above that number. The average happiness rating of the poorest, least healthy people is 4.009 while the second poorest, least healthy is above that at 4.577. The poorer, less healthy people are also less happy.\n\n# One of the boxes in the healthier, richer section does split again so we'll focus on the one that doesn't split first. The richest people who also live the longest terminate on the third level. 33 of the 105 samples from the first split made it into this box. They have an average happiness score of 6.829.\n\n# The other box (the second most healthy and rich) has 72 of the 105 from the first split and splits between according to GDP again. Those that have less than or equal to 1.003 GDP go to the left, the rest go to the right. Those with less money on this split number 35 from the 72 in the box above. They have a happinest average of 5.333. The slightly richer box numbers 37 and has a happiness score of 5.799.\n\ny = happy[['Score']]\na = happy[['Social', 'Freedom']]\na_names = ['Social', 'Freedom']\ncart02 = DecisionTreeRegressor(criterion = 'mse', max_leaf_nodes = 5).fit(a,y)\n#cannot show from here, but can show on your computer\n#export_graphviz(cart02, out_file = \"/Users/zli/Downloads/894_813759_bundle_archive/2019happy_cart02.dot\", feature_names=a_names)\n\n# will show the tree:\n# The top box shows that Health is going to be the first branching point. Countries with a health rating that's less than or equal to .65 go to the left, those who are greater than .66 go to the right. That means the healthiest people (or those with the longest life expectancy) go to the right and the unhealthiest people (or those who have a low life expectancy) go the left. \n\n# 51 countries fall into the less healthy category whereas 105 fall in the healthier category.\n\n# The next split is based on GDP. One interesting thing to note is the dividing point between the two boxes. The left (less healthy) box splits at less than or equal to .501. The right (or healthier) box splits at less than or equal to 1.266. Those that life longer have significantly more money.\n\n# The poorer, less health section doesn't split anymore. 29 of the 51 samples have a GDP score of less than or equal to .501 whereas 22 samples are above that number. The average happiness rating of the poorest, least healthy people is 4.009 while the second poorest, least healthy is above that at 4.577. The poorer, less healthy people are also less happy.\n\n# One of the boxes in the healthier, richer section does split again so we'll focus on the one that doesn't split first. The richest people who also live the longest terminate on the third level. 33 of the 105 samples from the first split made it into this box. They have an average happiness score of 6.829.\n\n# The other box (the second most healthy and rich) has 72 of the 105 from the first split and splits between according to GDP again. Those that have less than or equal to 1.003 GDP go to the left, the rest go to the right. Those with less money on this split number 35 from the 72 in the box above. They have a happinest average of 5.333. The slightly richer box numbers 37 and has a happiness score of 5.799.\n\ny = happy[['Score']]\na = happy[['Social', 'Freedom']]\na_names = ['Social', 'Freedom']\ncart02 = DecisionTreeRegressor(criterion = 'mse', max_leaf_nodes = 5).fit(a,y)\n#cannot show from here, but can show on your computer\n#export_graphviz(cart02, out_file = \"/Users/zli/Downloads/894_813759_bundle_archive/2019happy_cart02.dot\", feature_names=a_names)\n# The first box uses Social as a branching point. Those who score less than or equal to 1.206 go left, those that are above that number go right.\n\n# There are two boxes on the second row, 62 on the left and 94 on the right. The one with a lower Social rating has an average Happiness score of 4.385, the countries with the higher Social ranking have an average Happiness score of 6.081\n\n# 62 of the 156 values go left. The box with the lower Social ranking branches once more, again using Social as a metric. Countries with a Social ranking less than or equal to 0.765 go left, the rest go right. \n\n# On the third line, we get the final Happiness scores for the lower Social countries. The least social have an average Happiness score of 3.776 while the slightly more social box is also happier: 4.563.\n\n# The box on the second line with the higher social score also splits again, this time using Freedom as a metric. Countries with a Freedom rating of less than 0.515 go left, those that are higher go right.\n\n# There are 67 countries with a happiness score above 1.206 and a Freedom score below 0.515. These countries have an average Happiness score of 5.791.\n\n# The box on the third row that has both higher Social and Freedom rankings splits once cmore, this time using Social rating again. Those that have a Social score less than or equal to 1.47 go left, the rest go right.\n\n# 11 samples go left. These have the second highest social score and the highest Freedom score. Their happiness rating is 6.245. 16 countries have the highest Social and Freedom score and their rating for Happiness is 7.182.\n\n# So, the higher the Social and Freedom rankings, the happier people are. This doesn't disprove our theory that Wealth and Health make a person happy, but it does show other factors also directly contribute to a person's well being. \ny = happy[['Score']]\nc = happy[[ 'Generosity', 'Corruption']]\nc_names = happy[['Generosity', 'Corruption']]\ncart03 = DecisionTreeRegressor(criterion = 'mse', max_leaf_nodes = 5).fit(c,y)\n#cannot show from here, but can show on your computer\n#export_graphviz(cart03, out_file = \"/Users/zli/Downloads/894_813759_bundle_archive/2019happy_cart03.dot\")\n# Here's a decision tree with the final two variables: Corruption and Generosity. The trajectory is a little different. If you follow the divergent paths from top to bottom, there are five boxes that serve as ending points. All told, there are five different ending Corruption ratings and two different ending Generosity levels. \n\n# Here are the five ending states and their corresponding Happiness rating:\n\n# A Corruption >0.288 has a Happiness rating of 7.378\n# B Corruption <= 0.288 has a Happiness rating of 6.31\n# C: Corruption <= 0.181, Generosity <=0.146 has a Happiness rating of 5.45\n# D: Corruption <= 0.181, Generosity >0.146 has a Happiness rating of 4.992\n# E: Corruption >0.411 has a Happiness rating of 4.798\n\n# The most corrupt is the least happy.\n\n#The summary shows that all independent variables have a significant impact\n\n# When the entire dataset is used, only Generosity (.327) and Corruption (.075) are above the .05 cutoff.\n\n# Ultimately, Corruption and Generosity have the least pull on a country’s overall Happiness level. \n\n\n#############################################################################\n#MAEbaseline = 0.8949\nMAEbaseline = np.mean(y1)\ny2.shape\nMAEbaseline_list = ([5.444917]*35)\nMAE_test = met.mean_absolute_error(y2, MAEbaseline_list)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Regression \n\nAgain, we use Random Forest Regressor, because the data Y value is continuous variables. We want to know the correlation of variables, so we do the test by taking out one variable and observing whether the error is increasing or decreasing. Here is the result of each time:\n\n#Take out GDP, the MAE is: 0.4697 > overall MAE 0.4360\n\n#Take out Health, the MAE is:0.4810 >overall MAE 0.436\n\n#Take out Freedom, the MAE is:0.4386 > overall MAE 0.436\n\n#Take out Social, the MAE is:0.4682 >overall MAE 0.436\n\n#Take out Corruption, the MAE is:0.4387 >overall MAE 0.436\n\n#Take out Generosity, the MAE is:0.4297< overall MAE 0.436 By this comparison, we can see that the two most relevant are GDP and Health , for example when we take out Health, the MAE increases lots compared to others, so We believe that health is the biggest influencing factor affecting the entire data set. And number two is GDP , number three is Social support, which confirmed the previous analysis when exploring data through the visualization.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#############################################\n# Random Forest Regressor\n\n#Again, Random ForestClassifier and 'gini' are used for int data. Because Score out data is floating point (there are always decimals), we're using Random Forest Regressor\n# This time we split our data set to train 78% & test 22% which is(121：35),in order to improve the performance of our small size data trainning.\n\nhappy_train2, happy_test2 = train_test_split(happy, test_size = 0.22, random_state = 7)\n#### For overall \ncols = ['GDP', 'Health', 'Freedom','Social', 'Corruption', 'Generosity']\nXtrain_2 = happy_train2[cols]\nytrain_2 = happy_train2['Score']\n# Test the max_depth with 1,2,3,4,5. and 4 performance the best result.( which is less error)\nrf01 = RandomForestRegressor(max_depth=4, random_state = 0)\nrf01.fit(Xtrain_2, ytrain_2)\n\n#Create Test data set:        \nXtest_2 = happy_test2[cols]\nytest_2= happy_test2['Score']\n\n# Use the test data to make predictions\nrf26_pred = rf01.predict(Xtest_2)\n\nMSE = np.sum(np.power(np.array(ytest_2) - np.array(rf26_pred), 2)) / len(ytest_2)\n#MSE:0.3564\n\nMAE_test = met.mean_absolute_error(ytest_2, rf26_pred)\n# MAE is obtained by subtracting the predict data from the actual data : 0.4360\n\n#Random Forest Regressor performance Visualization\nplt.figure()\nplt.scatter(rf26_pred, ytest_2)\nplt.plot([3, 7], [3, 7], 'k')\nplt.title('Random Forest Regressor& Overall')\nplt.xlabel('Predict Happiness Score') \nplt.ylabel('Actual Happiness Score')\n\n\n# We want to know the correlation of variables, so we do the test by taking out a variable, and observing whether the error is increasing or decreasing.\n\n# Take out GDP, the MAE is: 0.4697 > overall MAE 0.4360\n# Take out Health, the MAE is:0.4810  >overall MAE 0.436\n# Take out Freedom, the MAE is:0.4386 > overall MAE 0.436\n# Take out Social, the MAE is:0.4682 >overall MAE 0.436\n# Take out Corruption, the MAE is:0.4387 >overall MAE 0.436\n# Take out Generosity, the MAE is:0.4297< overall MAE 0.436\n\n#By this comparison, the most relevant element should be the Health and GDP , after is Social & Freedom, and last is Corruption & Generosity.\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Multiple Regression Here we use the same logic as Random Forest Regressor. To know the correlation of variables we do the test by taking out one variable and observing whether the error is increasing or decreasing. Here is the result of each time:\n\n#Take out GDP, the MAE is 0.445 : > overall MAE 0.5004\n\n#Take out Health, the MAE is 0.501 : > overall MAE 0.5004\n\n#Take out Freedom, the MAE is: 0.461 > overall MAE 0.5004\n\n#Take out Social, the MAE is: 0.463 > overall MAE 0.5004\n\n#Take out Corruption, the MAE is: 0.446 > overall MAE 0.5004\n\n#Take out Generosity, the MAE is: 0.447 > overall MAE 0.5004 By this comparison, we can see that the two most relevant are Health and Social, for example when we take out Health, the MAE increases lots compared to others, so We believe that health is the biggest influencing factor affecting the entire data set. And number two is Social , number three is Freedom, which confirmed the previous analysis when exploring data through the data visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################################################\n#  Multiple Regression model \n# Partitioned data use same partition with Random Forest Regressor\n#Training data set\nhappy_train, happy_test = train_test_split(happy, test_size = 0.22, random_state = 7)\n# No GDP\n# Train data set\nxtrain = pd.DataFrame(happy_train[['Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nxtrain = sm.add_constant(xtrain)\nytrain = pd.DataFrame(happy_train[['Score']])\nmodel1 = sm.OLS(ytrain, xtrain).fit()\nmodel1.summary()\n\n# Test data set\nxtest = pd.DataFrame(happy_test[['Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nxtest = sm.add_constant(xtest)\nytest = pd.DataFrame(happy_test[['Score']])\nmodel2 = sm.OLS(ytest, xtest).fit()\nmodel2.summary()\n\n# Use the test data to make predictions\nypred = model1.predict(xtest)\nMAEregression_test = met.mean_absolute_error(ytest, ypred)\n# 0.4445515888853238\n\n# No Health\n# Train data set\nxtrain2 = pd.DataFrame(happy_train[['GDP', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nxtrain2 = sm.add_constant(xtrain2)\nytrain2 = pd.DataFrame(happy_train[['Score']])\nmodel3 = sm.OLS(ytrain2, xtrain2).fit()\nmodel3.summary()\n# test2 data set\nxtest2 = pd.DataFrame(happy_test[['GDP', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nxtest2 = sm.add_constant(xtest2)\nytest2 = pd.DataFrame(happy_test[['Score']])\nmodel4 = sm.OLS(ytest2, xtest2).fit()\nmodel4.summary()\n\n# Use the test2 data to make predictions\nypred2 = model3.predict(xtest2)\nMAEregression_test2 = met.mean_absolute_error(ytest2, ypred2)\n# 0.5010302403540907\n\n# No Freedom\n\nxtrain3 = pd.DataFrame(happy_train[['GDP', 'Health', 'Social', 'Corruption', 'Generosity']])\nxtrain3 = sm.add_constant(xtrain3)\nytrain3 = pd.DataFrame(happy_train[['Score']])\nmodel5 = sm.OLS(ytrain3, xtrain3).fit()\nmodel5.summary()\n\n# test3 data set\nxtest3 = pd.DataFrame(happy_test[['GDP', 'Health', 'Social', 'Corruption', 'Generosity']])\nxtest3 = sm.add_constant(xtest3)\nytest3 = pd.DataFrame(happy_test[['Score']])\nmodel6 = sm.OLS(ytest3, xtest3).fit()\nmodel6.summary()\n\n# Use the test3 data to make predictions\nypred3 = model5.predict(xtest3)\nMAEregression_test3 = met.mean_absolute_error(ytest3, ypred3)\n# 0.461498861801825\n\n# No Social\n\nxtrain4 = pd.DataFrame(happy_train[['GDP', 'Health', 'Freedom', 'Corruption', 'Generosity']])\nxtrain4 = sm.add_constant(xtrain4)\nytrain4 = pd.DataFrame(happy_train[['Score']])\nmodel7 = sm.OLS(ytrain4, xtrain4).fit()\nmodel7.summary()\n\n# test4 data set\nxtest4 = pd.DataFrame(happy_test[['GDP', 'Health', 'Freedom', 'Corruption', 'Generosity']])\nxtest4 = sm.add_constant(xtest4)\nytest4 = pd.DataFrame(happy_test[['Score']])\nmodel8 = sm.OLS(ytest4, xtest4).fit()\nmodel8.summary()\n\n# Use the test4 data to make predictions\nypred4 = model7.predict(xtest4)\nMAEregression_test4 = met.mean_absolute_error(ytest4, ypred4)\n# 0.4626210775277544\n\n# No Corruption \n\nxtrain5 = pd.DataFrame(happy_train[['GDP', 'Health', 'Freedom', 'Social', 'Generosity']])\nxtrain5 = sm.add_constant(xtrain5)\nytrain5 = pd.DataFrame(happy_train[['Score']])\nmodel9 = sm.OLS(ytrain5, xtrain5).fit()\nmodel9.summary()\n\n# test5 data set\nxtest5 = pd.DataFrame(happy_test[['GDP', 'Health', 'Freedom', 'Social', 'Generosity']])\nxtest5 = sm.add_constant(xtest5)\nytest5 = pd.DataFrame(happy_test[['Score']])\nmodel10 = sm.OLS(ytest5, xtest5).fit()\nmodel10.summary()\n\n# Use the test5 data to make predictions\nypred5 = model9.predict(xtest5)\nMAEregression_test5 = met.mean_absolute_error(ytest5, ypred5)\n# 0.4462581703777429\n\n# No Generosity \nxtrain6 = pd.DataFrame(happy_train[['GDP', 'Health', 'Freedom', 'Social', 'Corruption']])\nxtrain6 = sm.add_constant(xtrain6)\nytrain6 = pd.DataFrame(happy_train[['Score']])\nmodel11 = sm.OLS(ytrain6, xtrain6).fit()\nmodel11.summary()\n\n# test6 data set\nxtest6 = pd.DataFrame(happy_test[['GDP', 'Health', 'Freedom', 'Social', 'Corruption']])\nxtest6 = sm.add_constant(xtest6)\nytest6 = pd.DataFrame(happy_test[['Score']])\nmodel12 = sm.OLS(ytest6, xtest6).fit()\nmodel12.summary()\n\n# Use the test6 data to make predictions\nypred6 = model11.predict(xtest6)\nMAEregression_test6 = met.mean_absolute_error(ytest6, ypred6)\n0.44653839713541876\n\nxtrain7 = pd.DataFrame(happy_train[['GDP', 'Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nxtrain7 = sm.add_constant(xtrain7)\nytrain7 = pd.DataFrame(happy_train[['Score']])\nmodel13 = sm.OLS(ytrain7, xtrain7).fit()\nmodel13.summary()\n\n# test7 data set\nxtest7 = pd.DataFrame(happy_test[['GDP', 'Health', 'Freedom', 'Social', 'Corruption', 'Generosity']])\nxtest7 = sm.add_constant(xtest7)\nytest7 = pd.DataFrame(happy_test[['Score']])\nmodel14 = sm.OLS(ytest7, xtest7).fit()\nmodel14.summary()\n\n# Use the test7 data to make predictions\nypred7 = model13.predict(xtest7)\nMAEregression_test7 = met.mean_absolute_error(ytest7, ypred7)\n0.4433334554668311\n\n#Multiple Regression model performance Visualization\nplt.figure()\nplt.scatter(y2, y2pred)\nplt.plot([3, 7], [3, 7], 'k')\nplt.title('Multiple Regression model & Overall')\nplt.xlabel('Predict Happiness Score') \nplt.ylabel('Actual Happiness Score')\n\n\n#So our predicted values are Health and Social by Multiple Regression model\n#The predicted values from Random Forest Regressor are Health and GDP\n\n#Two model have a same result which is Health is No.1 reason of happy\n# Our Hypothesis is Health and GDP, which is close.\n\n#############################################################################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion\n\nRandom Forest Regressor : Predict Result is GDP & Health\nMultiple Regression Model:Predict Result is Health & Social\n\nAfter analysing the dataset of Global Happiness scores in the world, we were able to discover the impact of each different factor in determining “happiness.” We had also found that among the different factors, “Health” tends to have the greatest happiness with economic “GDP” and “Social”  support following by. Our hypothesis that a rich life and healthy body are the biggest influencing factors affecting happiness.  However, the data in our analysis is only 2019, which does not fully explain the factors that affect people's happiness. Our figures will change with immigration and pandemics, or other technological revolutions. We are able to understand what makes countries and their people happy by reviewing these data analysis, thereby enabling us to concentrate on prioritizing and optimizing these facets of the country. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}