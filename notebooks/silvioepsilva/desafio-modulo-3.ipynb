{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1 - Importando e visualizando os dados","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando dados de treino\ndf_train = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')\ntrain_instancias, train_atributos = df_train.shape\nprint('O dataset de treino possui {} instâncias e {} atributos.'.format(train_instancias,train_atributos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando dados de test\ndf_test  = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')\ntest_instancias, test_atributos = df_test.shape\nprint('O dataset de treino possui {} instâncias e {} atributos.'.format(test_instancias,test_atributos))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renomeando colunas\ndf_train.columns = ['pixel'+ str(i) for i in range(0, 785)]\ndf_train.rename(columns={'pixel0':'label'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecionando 5 imagens aleatórias para visualizá-las\nrandom_indexes = random.sample(range(df_train.shape[0]),5)\noriginal_images = [np.array(df_train.iloc[element,1:]).reshape(28,28) for element in random_indexes]\narray_representation = [np.array(df_train.iloc[element,1:]) for element in random_indexes]\n\n# Visualizando\nfig, axes = plt.subplots(nrows=1, ncols=5)\n\ni=0\nfor ax in axes:\n    ax.imshow(original_images[i], cmap ='gist_gray')\n    i +=1\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando algumas funções\n\n# Normalização\ndef feat_normalize(X):\n    \n    M = X.shape[1]\n    for i in range(M):\n        if np.any(X[:,i]) != 0:\n            min_ = X[:,i].min()\n            max_ = X[:,i].max()\n            X[:,i] =(2*X[:,i]-min_-max_)/(max_-min_)\n# -----------------------------------------------------            \n            \ndef append_ones(X):\n    \n    s = X.shape[0]\n    ones = np.ones(shape=(s,1))\n    return np.concatenate((ones, X), axis=1)\n\n# -----------------------------------------------------\n            \n# funções para calcular precisão, recall e f1_score\ndef prec_rec_F1(class_rep):\n    \n    precision = []\n    recall = []\n    F1 = []\n\n    for i in range(10):\n        temp = np.zeros(shape=(2,2))\n        temp[0,0] = class_rep.iloc[i,i]\n        temp[0,1] = sum(class_rep.iloc[i,:i]) + sum(class_rep.iloc[i,i+1:])\n        temp[1,0] = sum(class_rep.iloc[:i,i]) + sum(class_rep.iloc[i+1:, i])\n        temp[1,1] = sum(np.diag(class_rep))- class_rep.iloc[i,i]\n    \n        ptemp = temp[0,0]/(temp[0,0]+ temp[0,1])\n        precision.append([i,ptemp])\n        rectemp = temp[0,0]/(temp[0,0]+ temp[1,0])\n        recall.append([i,rectemp])\n        F1.append([i,2 * ptemp * rectemp /(ptemp+rectemp)])\n    \n    return [precision, recall, F1]\n\n# -----------------------------------------------------\n\ndef create_class_rep(prediction, y_test):\n    \n    class_rep =np.zeros(shape=(10,10))\n    \n    for i in range(len(y_test)):\n        x = prediction[i]\n        y = y_test[i]\n        class_rep[x,y] +=1\n        \n    class_rep = pd.DataFrame(class_rep)\n    return class_rep.applymap(int)\n\n# -----------------------------------------------------\n\n# Sigmoid function\ndef sigmoid(x):\n    \n    return 1/(1 + np.exp(-x))\n\n# -----------------------------------------------------\n\n# Cost function of the logistic regression for binary classification, s_i = {0,1} \ndef cost(X, y , theta):\n    \n    dim = X.shape[0]\n    s = sigmoid(np.dot(X,theta))\n    tot = -(np.log(s)*y +np.log(1-s)*(1-y))\n    return 1/dim *sum(tot)[0]\n\n# -----------------------------------------------------\n\n# Gradient of the cost function with respect to the parameters theta. To be used in gradient descent below\ndef grad_cost(X, y, theta):\n    \n    dim = X.shape[0]\n    pred = sigmoid(np.dot(X,theta))\n    c1 = 1/dim * np.transpose(pred-y)\n    return np.transpose(np.dot(c1,X))\n\n# -----------------------------------------------------\n\n# Gradient descent to get the parameter theta\ndef grad_descent(X, y, theta, learning_par, num_iter):\n\n    for i in range(num_iter):\n        #print cost(X,y,theta) to check the cost is monotonically decreasing at each iteration\n        theta = theta - learning_par*grad_cost(X,y,theta)\n        \n    return theta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 - Regressão logística","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing the training set in train and test set\n\ny = df_train.iloc[:,0]\nX = df_train.iloc[:,1:]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 101)\n\n#Normalizing the train and test sets\nX_train = np.array(X_train)\nfeat_normalize(X_train)\nX_train = append_ones(X_train)\n\n#Appending the bias column to the train and test matrices\nX_test = np.array(X_test)\nfeat_normalize(X_test)\nX_test = append_ones(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the vector of target lables for each digit 0-9\ny_target = []\nfor i in range(10):\n    y_target.append(y_train.apply(lambda x: 1 if x == i else 0))\n    \n#Initialize the list of training parameters (784+1 (bias) for each digit)\ntheta=[]\n\n#Gradient descent to train the model\nfor i in range(10):\n    ytemp = np.array(y_target[i])\n    ytemp = ytemp.reshape(y_train.shape[0],1)\n\n    thetatemp = np.zeros(shape=(X_train.shape[1],1))\n\n    alpha = 0.03\n    n_iter = 100\n\n    thetatemp = grad_descent(X_train,ytemp,thetatemp,alpha,n_iter)\n    theta.append(thetatemp)\n    print('{}: done!'.format(i))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando os dados classificados\n\nplt.imshow(theta[0][1:].reshape(28,28), cmap='gist_gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predição\nresult = [sigmoid(np.dot(X_test,theta[i])) for i in range(10)]\nresult = np.transpose(np.array(result)).reshape(X_test.shape[0],10)\n\nprediction = (np.array([element.argmax() for element in result])).reshape(X_test.shape[0],1)\n\n#testing accuracy of the prediction\ny_test = np.array(y_test)\ny_test = y_test.reshape(y_test.shape[0],1)\n\naccuracy = sum(prediction == y_test)[0]/(y_test.shape[0])\nprint('Accuracy is: {}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matrix de classificação\nclass_rep = create_class_rep(prediction,y_test)\nclass_rep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision, recall, F1 = prec_rec_F1(class_rep)\n\nplt.figure(figsize=(8,8))\n\nplt.xticks(range(10))\nplt.yticks(1/10*np.array(range(10)))\n\nplt.bar(np.transpose(precision)[0],np.transpose(precision)[1], align='edge', width =-0.25)\nplt.bar(np.transpose(recall)[0],np.transpose(recall)[1],align='center',width = 0.25)\nplt.bar(np.transpose(F1)[0],np.transpose(F1)[1],align='edge',width =0.25)\nplt.legend(labels = ('Precision','Recall','F1'))\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 - Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n#Create a forest with n=100 trees and fot to the model\nforest = RandomForestClassifier(n_estimators=100)\nforest.fit(X_train, y_train)\n\n#Predicting new results\nprediction = forest.predict(X_test)\nprediction = prediction.reshape(prediction.shape[0],1)\n\n#Accuracy\naccuracy = sum(prediction == y_test)[0]/(y_test.shape[0])\nprint('Accuracy is: {}'.format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report\nclass_rep = create_class_rep(prediction,y_test)\nclass_rep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And precision, recall, F1\n\nprecision, recall, F1 = prec_rec_F1(class_rep)\n\nplt.xticks(range(10))\nplt.yticks(1/10*np.array(range(10)))\n\nplt.bar(np.transpose(precision)[0],np.transpose(precision)[1], align='edge', width =-0.25)\nplt.bar(np.transpose(recall)[0],np.transpose(recall)[1],align='center',width = 0.25)\nplt.bar(np.transpose(F1)[0],np.transpose(F1)[1],align='edge',width =0.25)\nplt.legend(labels = ('Precision','Recall','F1'))\n\nplt.tight_layout()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}