{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sales Prediction based on Social Media Ads","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## HImanshu Sharma (DSOCT03)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n> Using the fictional dataset of Gender, Age, Salary, Purchased (Target variable), the company wants to know whether a customer will buy its product or not.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Approach - Supervised Machine Learning (Classification)\n\n### Supervised Machine Learning:\n\n- The algorithms which learns from labeled data. After learning from the the data, the algorithm determines which label should be given to new data by their associating patterns to the unlabeled new data.\n\n- It can be divided into two categories: classification and regression. \n\n### Classification:\n\n- Classification is a technique which is useful to determining the class based on one or more independent variables.\n\n- I am going through the following classification algorithms in this notebook:\n\n> k-nearest neighbors\n\n> Logistic Regression\n\n> Decision Tree\n\n> Random Forest\n\n>Naive Bayes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Steps\n- Data wrangling, which consists of:\n    - Gathering data\n    - Assessing data\n    - Cleaning data\n- Storing, analyzing, and visualizing our wrangled data\n- Making Report on\n    - Aboout data wrangling efforts and\n    - About data analyses and visualizations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import Useful Library\nimport pandas as pd\nimport numpy as np\n\n#for making graph\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#for warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/social-network-ads/Social_Network_Ads.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Purchased.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Perform Basic EDA\n### a. Boxplot","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(y='Age', x='Purchased', data=df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(y='EstimatedSalary', x='Purchased', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b. Histogram – Distribution of Target Variable","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.hist(x=\"Purchased\", data=df);\nplt.title('Distribution of Purchase');\nplt.ylabel('Count');\nplt.xlabel('Purchase');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- By this histogram we have clear idea that by the social media ads most of the are not purchased the product. In this notebook We will deep dive into the data and try to find the reason and explore it further. ","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nbins_size = np.arange(15000,150000+10000,1000)\nplt.hist(x=\"EstimatedSalary\", data=df, bins= bins_size,rwidth=0.9);\nplt.title('Distribution of Salary');\nplt.ylabel('Count');\nplt.xlabel('Salary');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The chart is better explain the Distribution of Each Income level, Interesting there are customers in the mall with a very much comparable frequency with their Annual Income ranging from approx 75k US Dollars and 80K US Dollars. The average salary of the customers is 69742.5.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.EstimatedSalary.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nbins_size = np.arange(18,65,2)\nplt.hist(x=\"Age\", data=df, bins= bins_size,rwidth=0.9);\nplt.title('Distribution of Age');\nplt.ylabel('Count');\nplt.xlabel('Age');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- By looking at the above graph-, It can be seen that the Ages from 27 to 42 are very much frequent but there is no clear pattern, we can only find some group wise patterns such as the the older age groups are lesser frequent in comparison of youngsters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### c. Distribution Plot – Target Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.Purchased);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d. Aggregation for all numerical Columns","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### e. Unique Values across all columns","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.Age.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Age.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.EstimatedSalary.unique()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.EstimatedSalary.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Purchased.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### f. Duplicate values across all columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### g. Correlation – Heatmap","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(8,8))\nax= sns.heatmap(df.corr(),square = True, annot = True,cmap= 'Spectral' )\nax.set_ylim(4.0, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The Above Graph for Showing the linear correlation between the different attributes of the Mall Customer Segementation Dataset, This Heat map reflects the most correlated features with Blue Color and least correlated features with Red color.We can clearly see that only age has linearly related to the purchased. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### h. Regression Plot","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ax = sns.regplot(x=\"EstimatedSalary\", y=\"Purchased\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ax = sns.regplot(x=\"Age\", y=\"Purchased\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### i. Bar Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = sns.color_palette()[0]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"Purchased\", y=\"EstimatedSalary\", data=df, color=col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- By this bar plot we can take the idea that the person who purchased the product have more salary then the average.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"Purchased\", y=\"EstimatedSalary\",hue='Gender', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- females have higher average salary.\n- females purchased more","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### j. Pair plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, vars=[\"Age\", \"EstimatedSalary\",\"Purchased\"])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.pairplot(df, vars=[\"Age\", \"EstimatedSalary\",\"Purchased\"], hue = \"Gender\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Drop all duplicate rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Drop all non-essential features","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.drop(columns=['User ID'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Replace outliers with Nulls (if you find it essential) and replace all the nulls with respective approach of central tendencies (Mean/Median/Mode).","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.loc[((df.Age >58) & (df.Purchased==0)), 'Age'] = np.nan\ndf.fillna(53,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df.EstimatedSalary>120000) & (df.Purchased==0), 'EstimatedSalary'] = np.nan\ndf.fillna(120000,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Calculate Z score to validate whether outliers are still present or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nz = np.abs(stats.zscore(df['EstimatedSalary']))\nthreshold = 3\nprint(np.where(z > 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = np.abs(stats.zscore(df['Age']))\nprint(np.where(z > 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Clean the data with formatting issues if any. (converting datatypes, replacing dollars, etc.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Age = df.Age.astype(\"int64\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.EstimatedSalary = df.EstimatedSalary.astype(\"int64\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Add your view of EDA to enhance understanding of data. i.e., Grouping data and observing the way data is distributed. Try to add as many layers of EDA as possible.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"a = df.groupby(['Gender', 'Age'])\na.first()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"a = df.groupby(['Purchased','EstimatedSalary'])\na.first()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.scatterplot(y=\"EstimatedSalary\", x=\"Purchased\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nsns.scatterplot(y=\"EstimatedSalary\", x=\"Age\", data=df, hue = 'Purchased')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.catplot(y=\"EstimatedSalary\", x=\"Purchased\", data=df, hue = 'Gender')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(y=\"Age\", x=\"Purchased\", data=df, hue = 'Gender')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Build a model of choice – Classification problem statement, hence build a classification model first and calculate Confusion Matrix, AUC, F1 Score, Precision, Recall and Accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Gender.replace({'Male':1,\n                   'Female':0}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardize the Variables\n- Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, [1, 2]]\ny = df.iloc[:, 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions and Evaluations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,knn_pred))\nprint(classification_report(y_test,knn_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choosing a K Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that that after arouns K>4 the error rate is decreasing and if we take those values that maked overfitting so we take k = 4. So Let's retrain the model with that and check the classification report!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=4)\n\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\n\nprint('WITH K=4')\nprint('\\n')\nprint(confusion_matrix(y_test,knn_pred))\nprint('\\n')\nprint(classification_report(y_test,knn_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint ('accuracy_score : ', accuracy_score(y_test,knn_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K fold Cross Validation\n\n> This technique is useful to evaluate bias and variance more accurately. It splits the training set into k groups, and in each iteration, the algorithm chooses different test fold (individual section) for testing. This allows every part of the training set to be used for testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nknn_accuracy = cross_val_score(knn,X,y, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_accuracy.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nknn_fpr, knn_tpr, threshold = roc_curve(y_test, knn_pred)\nauc_knn = auc(knn_fpr, knn_tpr)\n\nplt.figure(figsize=(5, 5), dpi=100)\nplt.plot(knn_fpr, knn_tpr, marker='.', label='Knn (auc = %0.3f)' % auc_knn)\n\nplt.xlabel('False Positive Rate -->')\nplt.ylabel('True Positive Rate -->')\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Build at least a minimum of 4 different Classification models. All the models should use K-Fold cross Validation to train the model with at least 5-fold cross validation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\nlog_pred = log_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,log_pred))\nprint('\\n')\nprint(classification_report(y_test,log_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K- Fold CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"log_accuracy = cross_val_score(log_reg,X,y, cv = 5)\nprint(log_accuracy)\nprint(\"mean value of accuracy\",log_accuracy.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'rbf', random_state = 0)\nsvc_classifier.fit(X_train, y_train)\nsvc_pred = svc_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,svc_pred))\nprint('\\n')\nprint(classification_report(y_test,svc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K- Fold CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_accuracy = cross_val_score(svc_classifier,X,y, cv = 5)\nprint(svc_accuracy)\nprint(\"mean value of accuracy\",svc_accuracy.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\ndt_classifier.fit(X_train, y_train)\ndt_pred = dt_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K- Fold CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_accuracy = cross_val_score(dt_classifier,X,y, cv = 5)\nprint(dt_accuracy)\nprint(\"mean value of accuracy\",dt_accuracy.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf_classifier.fit(X_train, y_train)\nrf_pred = rf_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint(classification_report(y_test,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K- Fold CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_accuracy = cross_val_score(rf_classifier,X,y, cv = 5)\nprint(rf_accuracy)\nprint(\"mean value of accuracy\",rf_accuracy.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Compare the error and pick the ideal one with least errors.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> By seeing the confusion matrix and accuracy score `Random Forest classifier` perform batter than others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"For Random Forest Classifier::\")\nprint(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint(classification_report(y_test,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. Run hyperparameter tuning on all the models and pick the best parameters (A minimum of 2 Parameters should be tuned) and picked.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## SVM","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Applying grid search\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{\"C\": [1, 10, 100, 1000], \"kernel\": ['linear']}, \n              {\"C\": [1, 10, 100, 1000], \"kernel\": ['rbf'], 'gamma': [0.5, 0.1, 0.01, 0.001]}]\n\n#Use this list to train\ngrid_search = GridSearchCV(estimator = svc_classifier, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\n\n#Use attributes of grid_search to get the results\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint(\"Best accuracy: \",best_accuracy)\nprint(best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'rbf', random_state = 0, C =10, gamma=0.1)\nsvc_classifier.fit(X_train, y_train)\nsvc_pred = svc_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"For SVM Classifier::\")\nprint(confusion_matrix(y_test,svc_pred))\nprint('\\n')\nprint(classification_report(y_test,svc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classification","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"n_estimators = [100, 300, 500]\nmax_depth = [5, 8, 15]\nmin_samples_leaf = [1, 2] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(rf_classifier, hyperF, cv = 5, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestF.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestF.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0, max_depth=5,min_samples_leaf=1)\nrf_classifier.fit(X_train, y_train)\nrf_pred = rf_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,rf_pred))\nprint('\\n')\nprint(classification_report(y_test,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = ['gini', 'entropy']\nmax_depth = [4,6,8,12]\n\nparameters = dict(criterion=criterion,max_depth=max_depth)\n\n  \nclf = GridSearchCV(rf_classifier, hyperF, cv = 5, verbose = 1, n_jobs = -1)\n\n# Fit the grid search\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=5)\ndt_classifier.fit(X_train, y_train)\ndt_pred = dt_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [    \n    {'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\nclf = GridSearchCV(log_reg, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\nbest_clf = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = best_clf.best_estimator_\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(C=1.0, solver='lbfgs',max_iter=100 )\nlog_reg.fit(X_train,y_train)\nlog_pred = log_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,log_pred))\nprint('\\n')\nprint(classification_report(y_test,log_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,10))\nn_neighbors = list(range(1,10))\np=[1,2]\n\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n\n#Use GridSearch\nclf = GridSearchCV(knn, hyperparameters, cv=10, verbose=True, n_jobs=-1)\n#Fit the model\nbest_model = clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=9, p = 1, leaf_size=1)\n\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\n\nprint(confusion_matrix(y_test,knn_pred))\nprint('\\n')\nprint(classification_report(y_test,knn_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 12. Now, compare the models and pick the ideal one.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### By the hyperparameter tuning `Decision Tree` prefome best in above model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 13. Try to Predict the target with maximum independent features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = df.iloc[:,0: 3]\ny1 = df.iloc[:, 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.30, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=5)\ndt_classifier.fit(X_train, y_train)\ndt_pred = dt_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,dt_pred))\nprint('\\n')\nprint(classification_report(y_test,dt_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}