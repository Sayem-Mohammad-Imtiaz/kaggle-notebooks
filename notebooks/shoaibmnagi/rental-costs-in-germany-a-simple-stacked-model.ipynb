{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # dataviz\nimport seaborn as sns # dataviz\n\ndf = pd.read_csv(\"../input/apartment-rental-offers-in-germany/immo_data.csv\")\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = df.drop(['telekomHybridUploadSpeed', 'noParkSpaces', 'houseNumber', \\\n              'heatingCosts', 'energyEfficiencyClass', 'lastRefurbish', \\\n              'telekomTvOffer', 'telekomUploadSpeed', 'streetPlain', \\\n              'numberOfFloors', 'floor', 'firingTypes', 'serviceCharge', \\\n              'description', 'facilities', 'scoutId', 'pricetrend',\n              'baseRentRange', 'noRoomsRange', 'livingSpaceRange', \\\n              'yearConstructedRange'], axis=1) #drop columns that aren't needed\n\ndf = df[df.baseRent.between(100,10000, inclusive=True)] #drop extreme rent values\ndf = df[df.livingSpace.between(10, 500, inclusive=True)] #drop extreme and wrongly coded values\ndf = df[df.noRooms.between(0,15, inclusive=True)] #drop extreme and probably wrongly coded value\ndf = df[np.isfinite(df['totalRent'])] #drop observations where totalRent isn't available\ndf = df[df.totalRent.between(100,10000, inclusive=True)] #drop extreme totalRent value\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the dataset through some visualizations. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['baseRent'].hist(bins=30, range=(100,4000), grid=False, color='#86bf91')\nplt.title('Distribution of Base Rents')\nplt.xlabel('Base Rent')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's further explore these values according to States.\n\nWe can see that a lot of these rental offers are concentrated in Nordrhein-Westfalen and Sachsen. The mode in most states is towards the lower side (within the first 3 bars). However, for Bayern, Baden-Wurttemberg and Hessen, the mode is a bit higher. This offers us some insight into rental price variations in different states. The Southern states tend to be more expensive than the Northern or the Eastern ones. \n\nIf we observe the mean Base Rent by state, we can get a confirmation on this trend. In addition to the three states already mentioned, we can also see that the mean rent in Hamburg and Berlin is relatively high. This makes sense as these states entirely consist of metropolises and big metropolises do generally have higher rents. Unlike other states, there are no rural rental offers that dampen the mean. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['regio1'].value_counts()\n\ng = sns.FacetGrid(df, col='regio1', col_wrap=4)\ng = g.map(plt.hist, 'baseRent', bins=20, range=(100,4000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['regio1'])['baseRent'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x='yearConstructed', y='baseRent', data=df)\nplt.title('Price by Year of Construction')\nplt.xlabel('Year of Construction')\nplt.ylabel('Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a simple scatter plot of Price against Year of Construction, we can see that the vast majority of rental ads are for properties constructed recently and generally, older properties tend to be cheaper. This might probably be because of their condition, lack of modern facilities etc. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x='livingSpace', y='baseRent', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we visualize Base Rent against Living Space, we can see a general trend. Bigger apartments tend to be more expensive. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#FEATURE ENGINEERING\n\n#make a single binary variable to indicate if the apartment is refurbished/new\ndf['refurbished'] = (df.condition == 'refurbished') | (df.condition == 'first_time_use') | \\\n                    (df.condition == 'mint_condition') | (df.condition == 'fully_renovated') | \\\n                    (df.condition == 'first_time_use_after_refurbishment')\n\n#make a binary variable to indicate if the property is located in a 'rich' state i.e. states where GDP/capita is over 40,000 + Berlin, since it is a metropolis\ndf['richstates'] = (df.regio1 == 'Bayern') | (df.regio1 == 'Hamburg') | \\\n                    (df.regio1 == 'Baden_Württemberg') | (df.regio1 == 'Hessen') | \\\n                    (df.regio1 == 'Bremen') | (df.regio1 == 'Berlin')\n\n#make a binary variable to indicate if the property is located in a poor state where property prices are low (the poorest five states of Germany)\ndf['poorstates'] = (df.regio1 == 'Mecklenburg_Vorpommern') | (df.regio1 == 'Sachsen_Anhalt') | \\\n                    (df.regio1 == 'Thüringen') | (df.regio1 == 'Brandenburg') | (df.regio1 == 'Sachsen')\n\n#make a binary variable to indicate if the rental property has good interior\ndf['greatInterior'] = (df.interiorQual == 'sophisticated') | (df.interiorQual == 'luxury')\n\n#make a binary variable to indicated if the rental property has good heating\ndf['goodHeating'] = (df.heatingType == 'central_heating') | (df.heatingType == 'floor_heating') | \\\n                    (df.heatingType == 'self_contained_central_heating')\n\n#make a binary variable to identify rental ads from last year to factor in any inflationary effects.\ndf['2018_ads'] = (df.date == 'Sep18')\n\n#transform totalRent into log(totalRent) to get a better distribution + better interpretive quality\ndf['logRent'] = np.log(df['totalRent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_var = ['logRent']\nX_var = ['balcony', 'hasKitchen', 'cellar', 'livingSpace', 'noRooms', 'garden',\n         'refurbished', 'richstates', 'poorstates', 'greatInterior', 'newlyConst',\n         '2018_ads', 'lift']\ny = df[y_var].values\nX = df[X_var].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, \n                                                    random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to build the model and to find appropriate hyperparameters for Random Forest and GBM, I conducted Random Search on an IDE on my computer. I am not copying my code for the Random Search over here since it takes a long time and Kaggle kernels (at least, for me) sometimes disconnect automatically. \n\nI will be running a Linear Regression model, a Random Forest Regressor Model and a Gradient Boosting Regressor Model. After I'm done with all three, I will be making a simple stacked model out of it and see if it improves predictions. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LINEAR REGRESSION\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\ndef linearregression(xtrain, ytrain, xtest, ytest):\n    linreg = LinearRegression()\n    linreg.fit(xtrain, ytrain)\n    y_pred = linreg.predict(xtest)\n    print('MAE:', metrics.mean_absolute_error(ytest, y_pred))\n    print('MSE:', metrics.mean_squared_error(ytest, y_pred))\n\nlinearregression(X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RANDOM FOREST\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n#Best hyperparamters from the Random Search:\n#minsamleaf: 30, maxfeat: 11, maxdepth: 24 \n\ndef randomforestreg(msl, mf, md, xtrain, ytrain, xtest, ytest):\n    rfr_best = RandomForestRegressor(n_estimators=70, random_state=1111,\n                                     max_depth=md, max_features=mf, min_samples_leaf=msl)\n    rfr_best.fit(xtrain,ytrain)\n    y_pred_rfr = rfr_best.predict(xtest)\n    print('MAE:', metrics.mean_absolute_error(ytest, y_pred_rfr))\n    print('MSE:', metrics.mean_squared_error(ytest, y_pred_rfr))\n    \nrandomforestreg(30, 11, 24, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GRADIENT BOOSTING\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#Best hyperparameters from Random Search:\n#maxdepth: 16, minsamleaf: 117, n: 73, maxfeat: 10, lr: 0.07\ndef gradientboostingmachine(md, msl, n, mf, lr, xtrain, ytrain, xtest, ytest):\n    gbm_best = GradientBoostingRegressor(n_estimators=n, random_state=1111,\n                                         max_depth=md, max_features=mf, \n                                         min_samples_leaf=msl, learning_rate=lr\n                                         )\n    gbm_best.fit(xtrain, ytrain)\n    y_pred_gbm = gbm_best.predict(xtest)\n    print('MAE:', metrics.mean_absolute_error(ytest, y_pred_gbm))\n    print('MSE:', metrics.mean_squared_error(ytest, y_pred_gbm))\n    \ngradientboostingmachine(16, 117, 73, 10, 0.07, X_train, y_train, X_test, y_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to build a stacked model, I have just used a simple Linear Regression to stack up the individual predictions of the model. However, if we use Lasso, Ridge or Elastic Net methods, we might end up getting a better result. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stackedmodel(xtrain, ytrain, xtest, ytest):\n    x_training, x_valid, y_training, y_valid = train_test_split(xtrain, ytrain,\n                                                                test_size=0.5,\n                                                                random_state=42)\n    model1 = LinearRegression()\n    model2 = RandomForestRegressor(n_estimators=70, random_state=1111,\n                                   max_depth=24, max_features=11, \n                                   min_samples_leaf=24)\n    model3 = GradientBoostingRegressor(n_estimators=73, random_state=1111,\n                                       max_depth=16, max_features=10, \n                                       min_samples_leaf=117, learning_rate=0.07)\n    \n    model1.fit(x_training, y_training)\n    model2.fit(x_training, y_training)\n    model3.fit(x_training, y_training)\n    \n    preds1 = model1.predict(x_valid)\n    preds2 = model2.predict(x_valid)\n    preds3 = model3.predict(x_valid)\n    \n    testpreds1 = model1.predict(xtest)\n    testpreds2 = model2.predict(xtest)\n    testpreds3 = model3.predict(xtest)\n    \n    stackedpredictions = np.column_stack((preds1, preds2, preds3))\n    stackedtestpredictions = np.column_stack((testpreds1, testpreds2,\n                                              testpreds3))\n    \n    metamodel = LinearRegression()\n    metamodel.fit(stackedpredictions, y_valid)\n    final_predictions = metamodel.predict(stackedtestpredictions)\n    print('MAE:', metrics.mean_absolute_error(ytest, final_predictions))\n    print('MSE:', metrics.mean_squared_error(ytest, final_predictions))\n\nstackedmodel(X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our results from the stacked model give us an Mean Absolute Error of 17%. Since we used Log(TotalRent) as a dependent variable, we can perceive MAP as Mean Absolute Percentage Error. This means that our stacked model predicts our rent with an average absolute error of 17%, which isn't a bad prediction. However, we can see that the individual predictions from Gradient Boosting Regressor were slightly better than the stacked model. \n\nI haven't tested alternative stacking methods for the stacked model but there is a chance that they might give us a better overall error rate than a simple Linear Regression metamodel. I think that any kind of a shrinkage model in lieu of the Linear Regression stacker might allow us to improve upon our predictions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It goes without saying that this is an extremely simple model. I think we can improve upon our predictions quite a lot with some heavier feature engineering.\n\nThe dataset provides us with more detailed locational variables. Clever locational engineering can allow us to better capture the differences in rents across different locations (urban vs. rural), (metro vs. small city) etc. Moreover, I have not included several variables in the analysis because of the high number of missing values. If we cleverly impute these values, these additional variables might further allow us to improve upon our model.\n\nAnother interesting thing can be to add Neural Networks to our stacked model. Neural Networks might be able to predict our prices even better and the only reason I didn't include them here was due to the time constraint. \n\nI encourage those who are interested to try playinig around with this model by inluding more variables.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}