{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mushroom Classification\n\n### Goal\nThe goal of this project is to be able to classifiy mushrooms into either safe to eat or poisionous based on certain physical features. The dataset was originally contributed to the UCI Machine Learning Library.    \nIn doing so, I attempt to answer the following questions:\n- What model works best to classify this data?\n- What features are imporatant when trying to discern between a mushroom thats poisionous and safe to eat?\n\n### Description\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\n\nDataset and information about the dataset can be found here: https://www.kaggle.com/uciml/mushroom-classification","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:36.700483Z","iopub.execute_input":"2021-06-10T10:53:36.701076Z","iopub.status.idle":"2021-06-10T10:53:36.705451Z","shell.execute_reply.started":"2021-06-10T10:53:36.701042Z","shell.execute_reply":"2021-06-10T10:53:36.704745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:36.706987Z","iopub.execute_input":"2021-06-10T10:53:36.707244Z","iopub.status.idle":"2021-06-10T10:53:36.757234Z","shell.execute_reply.started":"2021-06-10T10:53:36.707212Z","shell.execute_reply":"2021-06-10T10:53:36.756353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"#Data about information in each column in dataset\nfor col in data.columns:\n    print(col, ':', data[col].unique())","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:36.759061Z","iopub.execute_input":"2021-06-10T10:53:36.759338Z","iopub.status.idle":"2021-06-10T10:53:36.792344Z","shell.execute_reply.started":"2021-06-10T10:53:36.759308Z","shell.execute_reply":"2021-06-10T10:53:36.791392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding missing values\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:36.793649Z","iopub.execute_input":"2021-06-10T10:53:36.793932Z","iopub.status.idle":"2021-06-10T10:53:36.808487Z","shell.execute_reply.started":"2021-06-10T10:53:36.793904Z","shell.execute_reply":"2021-06-10T10:53:36.807474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------------------------------------------------\n\nEvery column in the dataset consists of catagorical variables. \n\nThere are no columns with numerical data. Additionally, there are no missing values in the dataset at all. \n\nThis makes the preprocessing stage a breeze. \n\nHowever, do note that the column 'veil-type' contains only a single variable. This makes the column useless to the dataset because it adds no insight into the data. Therefore, we shall drop this column.\n\nAll we need to do is to biforcate the columns into two lists. The first will contain all the columns that need to be label encoded, while the second will contain those that need to be onehot encoded.\n\nBefore that, however, I label encode the entire dataset in order to better visualise the trends in the data.\n\n-------------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"#Creating labeled dataframe in order to visualize trends in the data\ntry:\n    data.drop(columns = 'veil-type', inplace = true)\n    print('veil-type dropped')\nexcept:\n    pass\n\nlenc = LabelEncoder()\nlabeled_data = pd.DataFrame()\nfor col in data.columns:\n    labeled_data[col] = lenc.fit_transform(data[col]).astype('int64')\n    print(col, 'done')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:36.810123Z","iopub.execute_input":"2021-06-10T10:53:36.810414Z","iopub.status.idle":"2021-06-10T10:53:36.872748Z","shell.execute_reply.started":"2021-06-10T10:53:36.810386Z","shell.execute_reply":"2021-06-10T10:53:36.871868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution of data in the dataframe in the form of histograms.\nfig = plt.figure(figsize = (30,30))\nax = fig.gca()\nlabeled_data.hist(ax=ax)\nfig.suptitle('Distributions of each Feature', size = 30)\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:36.873838Z","iopub.execute_input":"2021-06-10T10:53:36.874112Z","iopub.status.idle":"2021-06-10T10:53:40.57401Z","shell.execute_reply.started":"2021-06-10T10:53:36.874088Z","shell.execute_reply":"2021-06-10T10:53:40.573299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting features vs class data to uncover any strong trends between features and target class\nfig, ax = plt.subplots(6,4, figsize = (28,40))\ncolumns = list(labeled_data.columns)\nn = 1\nfor i in range(6):\n    for j in range(4):\n        sns.barplot(x = labeled_data[['class',columns[n]]].groupby(columns[n])['class'].sum().keys(),\n                    y = labeled_data[['class',columns[n]]].groupby(columns[n])['class'].sum().values,\n                   palette = 'viridis',\n                   ax = ax[i,j]\n                   ).set_title(columns[n], size = 20)\n        ax[i,j].set_xlabel(\" \")\n        n += 1\n        if n == len(columns):\n            break\nfig.suptitle('Poisonous Mushrooms in each category', size = 30)\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nax[5,1].set_visible(False)\nax[5,2].set_visible(False)\nax[5,3].set_visible(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:40.57514Z","iopub.execute_input":"2021-06-10T10:53:40.575561Z","iopub.status.idle":"2021-06-10T10:53:43.694379Z","shell.execute_reply.started":"2021-06-10T10:53:40.575523Z","shell.execute_reply":"2021-06-10T10:53:43.69342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------------------------------------------\nThe barcharts above display the number of poisonous mushrooms for each category in each feature. \n\nWe notice that some of the data like 'ring-number' and 'gill-attachment' is heavily skewed. This is because they are highly correlated with our taget class, which would allow us to make good predictions based on those classes alone. This means that for a feature like 'ring-number', if the 'ring-number' is 1, then there is a high probability that the mushroom in consideration is poisonous. \n\n\n-------------------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#Creating lists of colums to be label encoded and onehot encoded. \n## Columns to be label encoded have only 2 categories while those that need to be onehot encoded have more than 2 categories. \nlabel_encoding_columns = [col for col in data.columns if data[col].nunique() == 2]\nonehot_encoding_columns = [col for col in data.columns if col not in label_encoding_columns]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.696416Z","iopub.execute_input":"2021-06-10T10:53:43.696715Z","iopub.status.idle":"2021-06-10T10:53:43.724729Z","shell.execute_reply.started":"2021-06-10T10:53:43.696686Z","shell.execute_reply":"2021-06-10T10:53:43.723741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Label Encoding and Onehot Encoding\nlabel_encoder = LabelEncoder()\nonehot_encoder = OneHotEncoder(handle_unknown='ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.726075Z","iopub.execute_input":"2021-06-10T10:53:43.726341Z","iopub.status.idle":"2021-06-10T10:53:43.73075Z","shell.execute_reply.started":"2021-06-10T10:53:43.726313Z","shell.execute_reply":"2021-06-10T10:53:43.729736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a Transformer to smoothly transform columns\npreprocessor = ColumnTransformer(\n    transformers=[\n    ('label', label_encoder, label_encoding_columns),\n    ('onehot', onehot_encoder, onehot_encoding_columns)\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.731768Z","iopub.execute_input":"2021-06-10T10:53:43.732014Z","iopub.status.idle":"2021-06-10T10:53:43.742722Z","shell.execute_reply.started":"2021-06-10T10:53:43.731989Z","shell.execute_reply":"2021-06-10T10:53:43.741649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a dataframe with labeled data\ndata_label = pd.DataFrame()\nfor col in label_encoding_columns:\n    data_label[col] = label_encoder.fit_transform(data[col]).astype('int64')\n    print(col, 'label encoded')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.744051Z","iopub.execute_input":"2021-06-10T10:53:43.744427Z","iopub.status.idle":"2021-06-10T10:53:43.770699Z","shell.execute_reply.started":"2021-06-10T10:53:43.744387Z","shell.execute_reply":"2021-06-10T10:53:43.769708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create dataframe with onehot encoded (dummy) data\ndata_dummies = pd.get_dummies(data[onehot_encoding_columns], columns = onehot_encoding_columns, drop_first = False, dtype= 'int64')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.771712Z","iopub.execute_input":"2021-06-10T10:53:43.771942Z","iopub.status.idle":"2021-06-10T10:53:43.805721Z","shell.execute_reply.started":"2021-06-10T10:53:43.771919Z","shell.execute_reply":"2021-06-10T10:53:43.804605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Concatinate both labeled and onehot dataframes to create dataframe with all data which has been iether label encoded or onehot encoded\ndata_encoded = pd.DataFrame()\ndata_encoded = pd.concat([data_label ,data_dummies], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.807196Z","iopub.execute_input":"2021-06-10T10:53:43.807591Z","iopub.status.idle":"2021-06-10T10:53:43.819867Z","shell.execute_reply.started":"2021-06-10T10:53:43.807549Z","shell.execute_reply":"2021-06-10T10:53:43.818876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_label.shape, data_dummies.shape, data_encoded.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.821348Z","iopub.execute_input":"2021-06-10T10:53:43.821627Z","iopub.status.idle":"2021-06-10T10:53:43.832051Z","shell.execute_reply.started":"2021-06-10T10:53:43.821596Z","shell.execute_reply":"2021-06-10T10:53:43.830808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"#Splitting data into features X and target y\nX = data_encoded.drop('class', axis = 1)\ny = data_encoded['class']","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.833527Z","iopub.execute_input":"2021-06-10T10:53:43.833922Z","iopub.status.idle":"2021-06-10T10:53:43.850927Z","shell.execute_reply.started":"2021-06-10T10:53:43.833892Z","shell.execute_reply":"2021-06-10T10:53:43.85006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting data into train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.852061Z","iopub.execute_input":"2021-06-10T10:53:43.85231Z","iopub.status.idle":"2021-06-10T10:53:43.864891Z","shell.execute_reply.started":"2021-06-10T10:53:43.852287Z","shell.execute_reply":"2021-06-10T10:53:43.863868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\nscore = accuracy_score(y_pred, y_test)\n\nprint('The accuracy score for Random Forest Classifier is %.1f'%(score))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T10:53:43.866295Z","iopub.execute_input":"2021-06-10T10:53:43.866702Z","iopub.status.idle":"2021-06-10T10:53:44.203807Z","shell.execute_reply.started":"2021-06-10T10:53:43.866644Z","shell.execute_reply":"2021-06-10T10:53:44.202961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n\nIn conclusion, we obtained a perfect accuracy score on our testing set. Normally, a perfect score would mean the model has overfit the dataset, however, in this case, because of extremely skewed features and high correlation with our target, our model was able to obtained a perfect accuracy with minimal effort and no hyper parameter tuning. This result atypical and should not be expected to be replicated for other datasets. \n\nThank you for reading! Please comment down below if you have any questions!\n\n---","metadata":{}}]}