{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing Required Python libaries.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport tkinter as tk\nfrom tkinter import filedialog\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/diamonds/diamonds.csv')\nprint(df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 . What is structure of the dataset .\n# we are finding columns , rows and shape (rows and columns) and information of dataset using info() function\n\n# columns\nprint(\"\\n*** Columns ***\")\nprint(df.columns)\n\n#shape\nprint(\"\\n*** Shape of Dataframe ***\")\nprint(df.shape)\n\n# info\nprint(\"\\n*** Structure ***\")\nprint(df.info())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2 . What are the data type of each columns?\n# dtypes function will give data types of each column. \n\n# Datatype\nprint(\"\\n*** Datatype of each column ***\")\nprint(df.dtypes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3 . What is the length of alphanumeric columns ?\n# This loop will return length of each alphanumeric column in the given dataset.\ndef lenght_of_alphanumeric_columns(data):\n    for col in (data.columns):\n        if data[col].dtypes=='object':\n           df[data[col].str.isalnum() == True]\n           print(col,':', len(data[col]))\nlenght_of_alphanumeric_columns(df)   \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4 . What are precision & scale of numeric columns?\n\n\ndef precision_and_scale(x):\n    max_digits = 14\n    int_part = int(abs(x))\n    magnitude = 1 if int_part == 0 else int(math.log10(int_part)) + 1\n    if magnitude >= max_digits:\n        return (magnitude, 0)\n    frac_part = abs(x) - int_part\n    multiplier = 10 ** (max_digits - magnitude)\n    frac_digits = multiplier + int(multiplier * frac_part + 0.5)\n    while frac_digits % 10 == 0:\n        frac_digits /= 10\n    scale = int(math.log10(frac_digits))\n    return (magnitude + scale, scale)\nfor column in list(df.select_dtypes(include='float64').columns):\n    print(column)\n    new_list = []\n    for x in range(len(df[column])-1):\n        if not math.isnan(df[column][x]):\n            new_list.append(precision_and_scale(df[column][x]))\n    print(set(new_list))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5 . What are the significant columns?\n''' For diffrent dataset there will be diffrent significant columns, we can drop insignificant column \n    using below program.\n    '''\ninsignificant = list(map(int, input(\"Enter the column index which you want to drop: \").split()))\nfor column in insignificant:\n    df.drop(df.columns[column], axis = 1, inplace = True)\nprint(df.columns)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6 . Find out for each  column\n# ▪ Number of Null values\n# ▪ Number of zeros\n\ndef null_Zero_count(data):\n    for column in data.columns.to_list():\n        print('*****',column,'*****')\n        print(\" Number of Null values : \",data[column].isnull().sum())\n        print(\" Number of zeros : \",(data[column]==0).sum(axis=0))\nnull_Zero_count(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7 . For each significant column\n# ▪ Provide the obvious errors\n'''\nDifferent dataset will have diffrent obvious errors,\nby using below function we can determine diffrent unique values present\nin the dataset based upon that we can find obvious errors.\n''' \ndef obvious_errors(dataset):\n    \"\"\"The functions prints the uniques values for each column.\"\"\"\n    pd.options.display.float_format = '{:,.2f}'.format\n    for colName in df.columns:\n        if  df[colName].dtype == 'object':\n            print(\"Unique values in\",colName,\"\\n\",df[colName].unique()) \nobvious_errors(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8 . For each numeric column\n# ▪ Replace null values & zeros with median value of the column .\n\ndef null_zero_replace_median(data):\n    for column in list(data.select_dtypes(include='float64' or 'int64').columns):\n        print('*****',column,'*****')\n        print(\"*Original Count*\")\n        print(\"Zero Values: \", (data[column]==0).sum())\n        print(\"Null Values: \", data[column].isnull().sum())\n        data[column] = np.where(data[column]==0, data[column].median(), data[column])\n        data[column] = data[column].fillna(data[column].median)\n        print(\"*Cleaned Count*\")\n        print(\"Zero Values: \", (data[column]==0).sum())\n        print(\"Null Values: \", data[column].isnull().sum())\n    return data\nnull_zero_replace_median(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#9. For each numeric column\n#  ▪ Provide the quartile summary along with the cout , mean & sum\n\nprint(\"********| Quartile summary along with the count , mean |*********\")\nprint(df.describe())\nprint(\"********|  SUM  |*********\")\nprint(df.select_dtypes(include='float64' or 'int64').sum())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#10. For each significant column\n# ▪ Provide the range, variance and standard deviation.\n\nprint(\"****| Range |****\")\nprint(df.select_dtypes(include='float64' or 'int64').max() -\n      df.select_dtypes(include='float64' or 'int64').min())\n\n#variance\nprint(\"****| Variance |****\")\nprint(df.var())\n\n#standard deviation\nprint(\"****| Standard Deviation |****\")\nprint(df.std())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 11. For each significant column\n# ▪ Provide the count of outliers and their value\n\n\ndef outliercount(data):\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    print(\"Outlier Count in each column\")\n    print(((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum())\n\noutliercount(df)\n#to get outlier value\ndef OutlierValues(data):\n    print(\"outlier values in each column\")\n    colNames = data.columns\n    for colName in colNames:\n        if data[colName].dtype == \"object\": \n            continue\n        colValues = data[colName].values\n        print('Column: ', colName)\n        quartile_1, quartile_3 = np.percentile(colValues, [25, 75])\n        iqr = quartile_3 - quartile_1\n        lower_bound = quartile_1 - (iqr * 1.5)\n        upper_bound = quartile_3 + (iqr * 1.5)\n        ndOutData = np.where((colValues > upper_bound) | (colValues < lower_bound))\n        ndOutData = np.array(colValues[ndOutData])\n        print(ndOutData)\n        print(\" \")\nOutlierValues(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#12. Are there any class or categoric variables? If yes ,\n#   ▪ provide frequency distribution table & chart for the same\n'''\nDifferent data set will have diffrent class or categoric variables so we can give user input\n and let user decide which are columns have class variables .\n\nIn Diamond data set there are 4 class variables ie cut, color and clarity.\n''' \n\nclass_variable = list(map(str, input(\"Enter columns names which have class variables : \").split()))\n\nfor colName in class_variable:\n    print(\"\\n*\"+colName+\"*\")\n    print(df.groupby(colName).size())\n    print(\"\")\n    plt.figure(figsize=(10,6))\n    sns.countplot(df[colName],label=\"Count\")\n    plt.title(colName)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#13. For all numeric columns\n#  ▪ Provide histogram\n                      \nprint('\\n*** | Histogram | ***')\ncolNames = df.columns.tolist()\nfor column in colNames:\n    if df[column].dtype == \"object\": \n        continue\n    df[column].hist()\n    plt.title(column)\n    plt.xlabel(column)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 14. For all numeric variables\n# ▪ Provide box & whisker plot\n\nprint('\\n***| Boxplot |***')\ncolNames = df.columns.tolist()\nfor column in colNames:\n    if df[column].dtype == \"object\": \n        continue\n    plt.figure(figsize=(6,6))\n    sns.boxplot(y=df[column], color='b')\n    plt.title(column)\n    plt.ylabel(column)\n    plt.xlabel('Bins')\t\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 15. For all numeric variables\n# ▪ Provide correlation table & graph\n#Below functions returns the correlation and graph for all numeric columns\n \nprint(\"***| Correlation Table |***\")\npd.options.display.float_format = '{:,.2f}'.format\nprint(df.corr())\n \nprint(\"***| Heat Map |***\")\nplt.figure(figsize=(8,8))\nax = sns.heatmap(df.corr(), annot=True, cmap=\"PiYG\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom+0.5, top-0.5)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#16. Prepare relationship chart showing relation of each numeric\n#column with all other numeric columns .\n#Below given function returns the Relationship chart for all numerical values.\n\nplt.figure()\nsns.set_style(\"whitegrid\");\nsns.pairplot(df, kind=\"scatter\")\nplt.title(\"Scatter Plot\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 17. Find out the difference between the Actual Depth & Ideal Depth .\n\ndf['ideal depth']=df['z']/((df['x']+df['y'])/2)\ndf['actual depth']=df['depth']/(df['z']*100)\ndf['depth diffrence']=abs(df['ideal depth']-df['actual depth'])\nprint(df.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}