{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Setup & EDA"},{"metadata":{},"cell_type":"markdown","source":"Initial dataset contains 2 labels: spam/ ham(no-spam).\n\n**Goal:** Build classifier to predict spam/ham text class.\n\n**Metrics:** ROC AUC, spam F1 score"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport lightgbm as lgbm\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report, accuracy_score, make_scorer, roc_auc_score, roc_curve, precision_recall_fscore_support, f1_score\nfrom sklearn.utils import class_weight\nfrom eli5 import show_weights\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nimport re\n\n%config InlineBackend.figure_format = 'retina'\nsns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n\nfrom IPython.core.display import display, HTML, clear_output\n\ndef print_df(df, index=True):\n    display(HTML(df.to_html(index=index)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\ndf = df.drop(df.columns.difference(['v1','v2']), 1).rename({'v1': 'label', 'v2': 'text'}, axis=1)\n\ndf.groupby(by='label').describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: *dataset is imbalanced (ham/spam ration 6.5/1). Spam is less unique - 87% оf sms is unique, in ham - 93.5%*"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 EDA - Text length"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_labels_df(df=df):\n    return df[df.label=='spam'], df[df.label=='ham']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sms_length'] = df.text.map(lambda x: len(x))\n\nfig1, axes = plt.subplots(1,2, figsize=(20 ,5))\n\nspam, ham = get_labels_df()\n\nsns.distplot(ham.sms_length, hist=False, rug=False, label='ham', ax=axes[0])\nsns.distplot(spam.sms_length, hist=False, rug=False, label='spam', ax=axes[0])\n\nsns.boxplot(x='label', y='sms_length', data=df, ax=axes[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** spam length usually greater than ham length. Also ham distribution is wider, while spam is thinner. Ham contains a lot outliers with lenght > Q3 , while spam outliers usually < Q1"},{"metadata":{},"cell_type":"markdown","source":"## 1.2 EDA - Sentiment and subjectivity"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_blob = df['text'].map(lambda x: TextBlob(x))\ndf['sentiment'] = text_blob.map(lambda x: x.sentiment.polarity)\ndf['subjectivity'] = text_blob.map(lambda x: x.sentiment.subjectivity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1, axes = plt.subplots(1,2, figsize=(20 ,5))\nspam, ham = get_labels_df(df)\n\nsns.distplot(ham.sentiment, hist=False, rug=False, label='ham', ax=axes[0])\nsns.distplot(spam.sentiment, hist=False, rug=False, label='spam', ax=axes[0])\n\nsns.distplot(ham.subjectivity, hist=False, rug=False, label='ham', ax=axes[1])\nsns.distplot(spam.subjectivity, hist=False, rug=False, label='spam', ax=axes[1])\n\nplt.show()\n\nprint('spam mean sentiment: {0:.2}, abs mean: {1:.2}'.format(spam.sentiment.mean(), spam.sentiment.abs().mean()))\nprint('ham mean abs sentiment: {0:.2}, abs mean: {1:.2}'.format(ham.sentiment.mean(), ham.sentiment.abs().mean()))\n\nprint('\\n'+'spam mean subjectivity: {0:.2}'.format(spam.subjectivity.mean()))\nprint('ham mean abs subjectivity: {0:.2}'.format(ham.subjectivity.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** spam have little bit greater sentiment polarity and greater sentiment subjectivity"},{"metadata":{},"cell_type":"markdown","source":"## 1.3 EDA - Tf-idf values, words and specific symbols analyzing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef get_tf_ids(text, top_N):\n    vect = TfidfVectorizer()\n    matrix = vect.fit_transform(text)\n    freqs = zip(vect.get_feature_names(),matrix.sum(axis=0).tolist()[0])\n   \n    return sorted(freqs, key=lambda x: -x[1])[:top_N]\n\nN = 50\ntf_idf_df = pd.DataFrame()\ntf_idf_df['top_spam'] = get_tf_ids(spam.text, N)\ntf_idf_df['top_ham'] = get_tf_ids(ham.text, N)\n\ntf_idf_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**View text samples, try to find text specific (for future feature engineering)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_df(spam.sample(100)[['text']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes about spam text specific:**\n1. links (http, www, .com)\n2. a lot of numbers\n3. currencies (£, $, €)\n4. ALL_CAPITAL_LETTERS like this\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"print_df(ham.sample(100)[['text']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes about ham text specific:**\n1. A lot of slangs usage (try to use spelling check for finding slangs?)\n2. Smiles! Normal people use smiles ^) ( ':)' , ':(', '=D', ':-)', ':-(', \n3. '...' usage\n4. Not all text ends with punctuation marks ( !, ?, .)\n "},{"metadata":{},"cell_type":"markdown","source":"# 2. Classification"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Simple BoW aproach (baseline model test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify_text(df, clf, max_features):\n    text_feature_name = 'text'\n    target = 'label'\n    df = df[(df[text_feature_name].notna()) & (df[target].notna())]\n    \n    vect = TfidfVectorizer(max_features=max_features)\n    text_clf_pipe = Pipeline([\n        ('vect', vect),\n        ('clf', clf)\n    ])\n    \n    # Fit on splited data & print classification report\n    train_df, test_df = train_test_split(df, test_size=0.20, stratify=df[target])\n    text_clf_pipe.fit(train_df[text_feature_name], train_df[target].values)\n    \n    test_predicted_proba = text_clf_pipe.predict_proba(test_df[text_feature_name])[:,1]\n    test_predicted = text_clf_pipe.predict(test_df[text_feature_name])\n    \n    auc_score = roc_auc_score(test_df[target], test_predicted_proba)\n    print('ROC AUC: {0:.4f}'.format(auc_score))\n    \n    print(classification_report(test_df[target].values, test_predicted)) \n    \n    columns = vect.get_feature_names()\n    return clf, columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MultinomialNB baseline\nclf, columns = classify_text(df, clf = MultinomialNB(), max_features=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: low spam F1 score with MultinomialNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGBMClassifier baseline\nclf, columns = classify_text(df, clf = lgbm.LGBMClassifier(n_jobs=-1, importance_type='gain'), max_features=10000)\n\nfi_percent = (100*clf.feature_importances_ / clf.feature_importances_.sum()).round(2)\nfi =  pd.DataFrame(fi_percent, \n                   index = columns, \n                   columns=['feature_importance'])\n\nfi.sort_values(by='feature_importance', inplace=True, ascending=False)\nprint_df(fi.head(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Results:** simple BoW aproach with lightgbm ROC AUC up to 0.98, spam F1 0.9..9.5, spam recall 0.8...0.9"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Feature engineering & use combined dataset (tf-ids + generated features)"},{"metadata":{},"cell_type":"markdown","source":"In this section i will try use some generated features from section \"words and specific symbols analyze\", also add text_lenght and sentiment features and compare to simple BoW results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text_df(df, text_feature_name, target, max_features=None, regex=None):\n    vect = TfidfVectorizer(max_features=max_features)\n    matrix = vect.fit_transform(df.text)\n\n    text_df = pd.DataFrame(matrix.toarray(), columns=vect.get_feature_names())\n    text_df[target] = df[target]\n    text_df[text_feature_name] = df[text_feature_name]\n    return text_df, vect\n\n\n\ntext_feature_name = 'text'\ntarget = 'label'\n\nINF_COLUMNS = [text_feature_name, target]\n\ndef get_X_df(df):\n    return df[df.columns.difference(INF_COLUMNS)]\n\ndef get_X_columns(df):\n    return df.columns.difference(INF_COLUMNS)\n\n\ncustom_df, _ = get_text_df(df,text_feature_name, target, max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all numbers from text features, instead - use one feature \"numbers_count\"\nregex = r\"(?u)[0-9]+\"\ncustom_df = custom_df[custom_df.columns.drop(list(custom_df.filter(regex=r\"[0-9]+|[^\\x00-\\x7f]\")))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#custom_df['__grammar_error_count'] = text_blob.map(lambda x: len([w for w in x.words if len(w.spellcheck())>0 ]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_df['__sms_length'] = df['sms_length']\ncustom_df['__sentiment'] = df['sentiment']\ncustom_df['__subjectivity'] = df['subjectivity']\n\n# SPAM specific\n#1. links (http, www, .com)\ncustom_df['__url_symbols_count'] = df['text'].str.count(pat='http|www|.com')\n#2. a lot of numbers\ncustom_df['__numbers_count'] = df['text'].str.count(pat=\"(?u)[0-9]+\")\n\n#3. currencies (£, $, €)\ncustom_df['__currencies_count'] = df['text'].str.count(pat=\"£|€\")\n\n#4. ALL_CAPITAL_LETTERS like this\ncustom_df['__all_capital_words'] = df['text'].astype(str).map(lambda x: len([y for y in x.split(' ') if y.isupper()]))\n\n\n# HAM specific\n#1. A lot of slangs usage (try to use spelling check for finding slangs?)\n#custom_df['__grammar_error_count'] = text_blob.map(lambda x: len([w for w in x.words if len(w.spellcheck())>0 ]))\n\n#2. Smiles! Normal people use smiles ^) ( ':)' , ':(', '=D', ':-)', ':-(', \ncustom_df['__smiles_count'] = df['text'].str.count(pat=\"=D|:-\\)|:-\\(|:\\)|:\\(\")\n\n#3. '...' usage\ncustom_df['__ellipsis_count'] = df['text'].str.count(pat=\"\\.\\.\\.\")\n\n#4. Not all text ends with punctuation marks ( !, ?, .)\n# pandas endswith not accept regex\ncustom_df['__ends_with_punctuation'] = df['text'].str.endswith(pat=\"?\") | df['text'].str.endswith(pat=\"!\") | df['text'].str.endswith(pat=\".\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1, axes = plt.subplots(2,2, figsize=(20 ,10))\nspam, ham = get_labels_df(custom_df)\n\nsns.distplot(ham.__numbers_count, hist=True, kde=False, label='ham', ax=axes[0, 0])\nsns.distplot(spam.__numbers_count, hist=True, kde=False, label='spam', ax=axes[0, 0])\n\nsns.distplot(ham.__ellipsis_count, hist=True, kde=False, label='ham', ax=axes[1, 0])\nsns.distplot(spam.__ellipsis_count, hist=True, kde=False, label='spam', ax=axes[1, 0])\n\nsns.distplot(ham.__currencies_count, hist=True, kde=False, label='ham', ax=axes[0, 1])\nsns.distplot(spam.__currencies_count, hist=True, kde=False, label='spam', ax=axes[0, 1])\n\nsns.distplot(ham.__all_capital_words, hist=True, kde=False, label='ham', ax=axes[1, 1])\nsns.distplot(spam.__all_capital_words, hist=True, kde=False, label='spam', ax=axes[1, 1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Test classifier with custom dataset (tf-ids + custom features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(custom_df, test_size=0.20, stratify=custom_df[target])\n\n\nclf = lgbm.LGBMClassifier(n_jobs=-1, importance_type='gain')\n\nX_train = get_X_df(train_df)\nclf.fit(X_train, train_df[target].values)\n    \nX_test = get_X_df(test_df)\ntest_predicted_proba = clf.predict_proba(X_test)[:,1]\ntest_predicted = clf.predict(X_test)\n    \nauc_score = roc_auc_score(test_df[target], test_predicted_proba)\nprint('ROC AUC: {0:.4f}'.format(auc_score))\n    \nprint(classification_report(test_df[target].values, test_predicted)) \n\nfi_percent = (100*clf.feature_importances_ / clf.feature_importances_.sum()).round(2)\nfi =  pd.DataFrame(fi_percent, \n                   index = X_test.columns, \n                   columns=['feature_importance'])\n\nfi.sort_values(by='feature_importance', inplace=True, ascending=False)\nprint_df(fi.head(30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** A lot of computed features is valuable, spam F1 and recall improved (compared to simple BoW aproach)"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Dataset improvements (try to balance dataset, duplicates removing, outliers, etc)"},{"metadata":{},"cell_type":"markdown","source":"**class_weight=\"balanced\" vs class_weight as is**\n\nDataset is imbalanced, try to use auto balancer"},{"metadata":{},"cell_type":"markdown","source":"class_weight=None (as is)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\n\nlb = preprocessing.LabelBinarizer()\n\ndef score_classifier(df, clf, preprocess_objects=None):\n    X = get_X_df(df)\n    y = df[target]\n  \n    #clf = clf if preprocess_objects is None else make_pipeline(preprocess_objects + [clf])\n\n    roc_auc_scores = cross_val_score(clf, X=X, y=y, scoring='roc_auc', cv=5)\n    print('ROC AUC = {0} +- {1}'.format(roc_auc_scores.mean(), roc_auc_scores.std()))\n\n    f1_scores =  cross_val_score(clf, X=X, y=y, scoring=make_scorer(f1_score, pos_label='spam'), cv=5)\n    print('spam F1 = {0} +- {1}'.format(f1_scores.mean(), f1_scores.std()))\n    \n    return roc_auc_scores, f1_scores\n\n\n_,_ = score_classifier(clf = lgbm.LGBMClassifier(n_jobs=-1, importance_type='gain'), df=custom_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"class_weight=\"balanced\""},{"metadata":{"trusted":true},"cell_type":"code","source":"_,_ = score_classifier(clf = lgbm.LGBMClassifier(n_jobs=-1, importance_type='gain', class_weight=\"balanced\"), df=custom_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** with balanced class_weight spam_f1_std is decreased\n\n**Next steps**: try different undersampling strategies (https://imbalanced-learn.readthedocs.io/en/stable/index.html)"},{"metadata":{},"cell_type":"markdown","source":"**Try different undersampling strategies (imbalanced-learn)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for undersampler in \nfrom imblearn.under_sampling import *\nfrom imblearn.pipeline import Pipeline as PipelineImblearn\nfor undersampler in [\n                     #CondensedNearestNeighbour(), \n                     #EditedNearestNeighbours(), \n                     #AllKNN(), \n                     #InstanceHardnessThreshold(), \n                     #NeighbourhoodCleaningRule(), \n                     OneSidedSelection(),\n                     RandomUnderSampler(),\n                     TomekLinks()]:\n    \n    print(type(undersampler))\n    clf = lgbm.LGBMClassifier(n_jobs=-1, importance_type='gain')\n    pipeline = PipelineImblearn([('undersampler', undersampler), ('clf', clf)])\n    _,_ = score_classifier(clf = pipeline,\n                     df = custom_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** TomekLinks improve ROC AUC and F1"},{"metadata":{},"cell_type":"markdown","source":"**Find and remove duplicates**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_df(df.groupby(by='text')\n        .agg(['count', 'max'])\n        .label\n        .sort_values(by='count', ascending=False)\n        .head(100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.pipeline import Pipeline as PipelineImblearn\nfrom imblearn import FunctionSampler\n\ndef remove_duplicates(X, y):\n    _, unique_indexes = np.unique(X.astype('float'), return_index=True, axis=0)\n\n    return X[unique_indexes], y[unique_indexes]\n\nduplicates_remover = FunctionSampler(func=remove_duplicates)\nclf = lgbm.LGBMClassifier(n_jobs=-1, importance_type='gain', class_weight=\"balanced\")\n\npipeline = PipelineImblearn([('remove_duplicates', duplicates_remover), ('undersampler', TomekLinks()), ('clf', clf)])\n\n_,_ = score_classifier(clf = pipeline,\n                 df = custom_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** removing duplicates does not increase spam F1"},{"metadata":{},"cell_type":"markdown","source":"**View wrong predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = get_X_df(custom_df)\ny = df[target]\n  \npipeline = PipelineImblearn([('undersampler', TomekLinks()), ('clf', clf)])\n\npredicted = cross_val_predict(clf, X=X, y=y, cv=5)\n\nwrong_prediction = custom_df[predicted != y][['text', 'label']]\n#wrong_prediction['predicted_label'] = predicted\nprint_df(wrong_prediction)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}