{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T07:11:50.057523Z","iopub.execute_input":"2021-08-30T07:11:50.057963Z","iopub.status.idle":"2021-08-30T07:11:50.067134Z","shell.execute_reply.started":"2021-08-30T07:11:50.057929Z","shell.execute_reply":"2021-08-30T07:11:50.06627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install pyspark and findspark","metadata":{}},{"cell_type":"code","source":"!pip install pyspark\n\n\n!pip install findspark","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:11:50.07621Z","iopub.execute_input":"2021-08-30T07:11:50.076879Z","iopub.status.idle":"2021-08-30T07:12:04.947699Z","shell.execute_reply.started":"2021-08-30T07:11:50.076823Z","shell.execute_reply":"2021-08-30T07:12:04.946788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Declare library","metadata":{}},{"cell_type":"code","source":"import findspark\nfindspark.init\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:04.950452Z","iopub.execute_input":"2021-08-30T07:12:04.95078Z","iopub.status.idle":"2021-08-30T07:12:04.957778Z","shell.execute_reply.started":"2021-08-30T07:12:04.950741Z","shell.execute_reply":"2021-08-30T07:12:04.956535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark import SparkConf\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import explode\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import *\nfrom IPython.display import display\nfrom IPython.display import FileLink, FileLinks\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\npd.options.display.max_columns = None\nimport numpy as np\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom pyspark.sql.types import ArrayType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\n\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\n\n#from mmlspark.lightgbm import LightGBMClassifier ,  LightGBMClassificationModel\n#from mmlspark.vw import VowpalWabbitFeaturizer, VowpalWabbitClassifier\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.stat import Summarizer #used to summarize vectors\nfrom pyspark.ml.feature import VectorAssembler , VectorSlicer , HashingTF, IDF, Tokenizer , StopWordsRemover,CountVectorizer\nfrom pyspark.ml.linalg import Vectors , DenseVector\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import RandomForestClassifier , LogisticRegression\n\n\nspark = SparkSession.builder.appName(\"Pyspark_Machine_Learning\").getOrCreate()\n\nsc= spark.sparkContext\nsqlContext= SQLContext(sc)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:04.959894Z","iopub.execute_input":"2021-08-30T07:12:04.960234Z","iopub.status.idle":"2021-08-30T07:12:04.979947Z","shell.execute_reply.started":"2021-08-30T07:12:04.960202Z","shell.execute_reply":"2021-08-30T07:12:04.978844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data in csv format and show some of its info","metadata":{}},{"cell_type":"code","source":"df=spark.read\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.csv('../input/water-potability/water_potability.csv')\n\ndf.show(10)\ndf.count()\ndf.dtypes\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:04.981515Z","iopub.execute_input":"2021-08-30T07:12:04.981876Z","iopub.status.idle":"2021-08-30T07:12:05.516683Z","shell.execute_reply.started":"2021-08-30T07:12:04.981842Z","shell.execute_reply":"2021-08-30T07:12:05.515722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df=df.dropna()\n#df.count()\ndf.select('Potability').describe().show()","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:05.518312Z","iopub.execute_input":"2021-08-30T07:12:05.518997Z","iopub.status.idle":"2021-08-30T07:12:05.814722Z","shell.execute_reply.started":"2021-08-30T07:12:05.518947Z","shell.execute_reply":"2021-08-30T07:12:05.813578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Impute the dataset with mean (to prevent missing valuesï¼‰","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import Imputer\nimputeCols = [\"ph\", \"Hardness\", \"Solids\", \"Chloramines\", \"Sulfate\",\"Conductivity\",\"Organic_carbon\",\"Trihalomethanes\",\"Turbidity\"]\nimputer = Imputer(strategy=\"mean\", inputCols=imputeCols, outputCols=imputeCols)\nimputerModel = imputer.fit(df)\nimputedDF = imputerModel.transform(df)\n\nimputedDF.show(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:05.816184Z","iopub.execute_input":"2021-08-30T07:12:05.816616Z","iopub.status.idle":"2021-08-30T07:12:06.411435Z","shell.execute_reply.started":"2021-08-30T07:12:05.816565Z","shell.execute_reply":"2021-08-30T07:12:06.410495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Put all feature columns into vector","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nfeatureCols = [\"ph\", \"Hardness\", \"Solids\", \"Chloramines\", \"Sulfate\",\"Conductivity\",\"Organic_carbon\",\"Trihalomethanes\",\"Turbidity\"]\nassembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\ndf_trans = assembler.transform(imputedDF)\n\ndf_trans.select('features','Potability').show(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:06.412729Z","iopub.execute_input":"2021-08-30T07:12:06.413041Z","iopub.status.idle":"2021-08-30T07:12:06.604019Z","shell.execute_reply.started":"2021-08-30T07:12:06.413011Z","shell.execute_reply":"2021-08-30T07:12:06.601288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trans.select('features','Potability').filter(df_trans.Potability==1).show(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:06.608074Z","iopub.execute_input":"2021-08-30T07:12:06.608497Z","iopub.status.idle":"2021-08-30T07:12:06.861119Z","shell.execute_reply.started":"2021-08-30T07:12:06.608455Z","shell.execute_reply":"2021-08-30T07:12:06.85968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import logistic regression of pyspark ml, since this is a classification problem","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"Potability\", featuresCol=\"features\")\nlrModel = lr.fit(df_trans)\n\ndf_predict = lrModel.transform(df_trans)\ndf_predict.show(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:06.863183Z","iopub.execute_input":"2021-08-30T07:12:06.86374Z","iopub.status.idle":"2021-08-30T07:12:10.022177Z","shell.execute_reply.started":"2021-08-30T07:12:06.863688Z","shell.execute_reply":"2021-08-30T07:12:10.020818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check which metrics the logistic model have, then choose one to have a look","metadata":{}},{"cell_type":"code","source":"[attr for attr in dir(lrModel.summary) if attr[0] != \"_\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:10.026248Z","iopub.execute_input":"2021-08-30T07:12:10.026609Z","iopub.status.idle":"2021-08-30T07:12:10.041005Z","shell.execute_reply.started":"2021-08-30T07:12:10.026578Z","shell.execute_reply":"2021-08-30T07:12:10.039726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrModel.summary.truePositiveRateByLabel","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:10.042618Z","iopub.execute_input":"2021-08-30T07:12:10.043011Z","iopub.status.idle":"2021-08-30T07:12:10.225094Z","shell.execute_reply.started":"2021-08-30T07:12:10.042978Z","shell.execute_reply":"2021-08-30T07:12:10.224007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the model (roc and pr curve)","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Let's use the run-of-the-mill evaluator\nevaluator = BinaryClassificationEvaluator(labelCol='Potability')\n\n# We have only two choices: area under ROC and PR curves :-(\nauroc = evaluator.evaluate(df_predict, {evaluator.metricName: \"areaUnderROC\"})\nauprc = evaluator.evaluate(df_predict, {evaluator.metricName: \"areaUnderPR\"})\nprint(\"Area under ROC Curve: {:.4f}\".format(auroc))\nprint(\"Area under PR Curve: {:.4f}\".format(auprc))","metadata":{"execution":{"iopub.status.busy":"2021-08-30T07:12:10.226813Z","iopub.execute_input":"2021-08-30T07:12:10.227528Z","iopub.status.idle":"2021-08-30T07:12:11.253828Z","shell.execute_reply.started":"2021-08-30T07:12:10.227476Z","shell.execute_reply":"2021-08-30T07:12:11.252716Z"},"trusted":true},"execution_count":null,"outputs":[]}]}