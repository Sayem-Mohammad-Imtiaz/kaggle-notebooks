{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Lung Segmentation using pretrained U-net**\n\n* For this notebook, UNet architecture with pre-trained ResNet34 as an encoder is used, I've used [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) library which has many inbuilt segmentation architectures with different backbones.\n\n* The purpose of this notebook is to identify \"Pneumothorax\" or a collapsed lung from chest x-rays. Pneumothorax is a condition that is responsible for making people suddenly gasp for air, and feel helplessly breathless for no apparent reason. So ultimately, we want to develop a model to identify and segment pneumothorax from a set of chest radiographic images.","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\n\nfrom PIL import Image,ImageFile\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection\n# import segmentation_models_pytorch as smp\n\nimport torch\nfrom torch import nn,optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n# device=\"cpu\"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:31.413735Z","iopub.execute_input":"2021-06-10T07:46:31.414147Z","iopub.status.idle":"2021-06-10T07:46:34.632584Z","shell.execute_reply.started":"2021-06-10T07:46:31.414035Z","shell.execute_reply":"2021-06-10T07:46:34.631617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Utility Functions**\n\n* Most Segmentation problems like this should have two images: input and mask. In this case of multiple objects, there will be multiple masks. In this dataset, we are provided with **RLE** instead. **RLE** stands for **Run Length Encoding** and is a way to represent binary mask to save space.\n\n* Here is the utility function which is used to create mask from this RLE data.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n    component = np.zeros((height, width), np.float32)\n    component = component.reshape(-1)\n    rle = np.array([int(s) for s in rle.strip().split(' ')])\n    rle = rle.reshape(-1, 2)\n    start = 0\n    for index, length in rle:\n        start = start+index\n        end = start+length\n        component[start: end] = fill_value\n        start = end\n    component = component.reshape(width, height).T\n    return component","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.634399Z","iopub.execute_input":"2021-06-10T07:46:34.634725Z","iopub.status.idle":"2021-06-10T07:46:34.64249Z","shell.execute_reply.started":"2021-06-10T07:46:34.634689Z","shell.execute_reply":"2021-06-10T07:46:34.641255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None):\n\n    if input_space == \"BGR\":\n        x = x[..., ::-1].copy()\n\n    if input_range is not None:\n        if x.max() > 1 and input_range[1] == 1:\n            x = x / 255.0\n\n    if mean is not None:\n        mean = np.array(mean)\n        x = x - mean\n\n    if std is not None:\n        std = np.array(std)\n        x = x / std\n\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.644218Z","iopub.execute_input":"2021-06-10T07:46:34.644821Z","iopub.status.idle":"2021-06-10T07:46:34.652148Z","shell.execute_reply.started":"2021-06-10T07:46:34.644782Z","shell.execute_reply":"2021-06-10T07:46:34.651312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create Dataset Class**\n\n* In this cell, the traditional Dataset class has been created. Please note that, this class is created in such way that it can applied to almost any segmentation problem.\n* Here training dataset is a CSV file consisting only ImageIds which are also filenames and other column contains RLE data. So, in the **init()** function we fatch the image ids and initialize some other parameters as well which are then used during **getitem()** method.\n* In the **getitem()** method Augmentation and preprocessing has been done. This method returns dictionary contains image and mask.","metadata":{}},{"cell_type":"code","source":"class SIIMDataset(Dataset):\n    \n    def __init__(self, df, data_dir, transform=None, preprocessing_fun=None, channel_first=True):\n        self.data_dir = data_dir\n        self.transform = transform                       # for augmentations\n        self.preprocessing_fun = preprocessing_fun       # preprocessing_fun to normalize images\n        self.channel_first = channel_first               # set channels as first dimension\n        self.image_ids = df.ImageId.values\n        self.group_by = df.groupby('ImageId')\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        df = self.group_by.get_group(img_id)\n        annotations = df[' EncodedPixels'].tolist()\n        \n        img_path = os.path.join(self.data_dir, img_id + \".png\")\n        img = Image.open(img_path).convert('RGB')\n        img = np.array(img)\n\n        mask = np.zeros(shape=(1024,1024))\n        if annotations[0] != ' -1':\n            for rle in annotations:\n                mask += run_length_decode(rle)\n        mask = (mask >= 1).astype('float32')\n        mask = np.expand_dims(mask, axis=-1)\n        \n        # apply augmentation\n        if self.transform:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n\n        if self.preprocessing_fun:\n            img = self.preprocessing_fun(img,\n                                         input_range=[0, 1],\n                                         mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        \n        # convert shape from (width, height, channel) ----> (channel, width, height) \n        if self.channel_first:\n            img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n            mask = np.transpose(mask, (2, 0, 1)).astype(np.float32)\n\n        return {\n            'image': torch.Tensor(img),\n            'mask': torch.Tensor(mask)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.654063Z","iopub.execute_input":"2021-06-10T07:46:34.654381Z","iopub.status.idle":"2021-06-10T07:46:34.669307Z","shell.execute_reply.started":"2021-06-10T07:46:34.654349Z","shell.execute_reply":"2021-06-10T07:46:34.668496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create Training and Evaluation Function**\n\n* In this section, Training and Evaluation function is created for one epoch. The functions takes model, dataloader, criterion(loss function), optimizer and returns average loss for one epoch.","metadata":{}},{"cell_type":"code","source":"# Train model for one epoch\n\ndef train(data_loader, model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    for data in tqdm(data_loader):\n        inputs = data['image']\n        labels = data['mask']\n\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    return train_loss/len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.672574Z","iopub.execute_input":"2021-06-10T07:46:34.67291Z","iopub.status.idle":"2021-06-10T07:46:34.680583Z","shell.execute_reply.started":"2021-06-10T07:46:34.672877Z","shell.execute_reply":"2021-06-10T07:46:34.6798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\n\ndef evaluate(data_loader, model, criterion):\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for data in tqdm(data_loader):\n            inputs = data['image']\n            labels = data['mask']\n\n            inputs = inputs.to(device, dtype=torch.float)\n            labels = labels.to(device, dtype=torch.float)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n\n    return eval_loss/len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.683411Z","iopub.execute_input":"2021-06-10T07:46:34.683695Z","iopub.status.idle":"2021-06-10T07:46:34.693376Z","shell.execute_reply.started":"2021-06-10T07:46:34.683664Z","shell.execute_reply":"2021-06-10T07:46:34.69268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **U-Net**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models, datasets, transforms\nfrom torch.nn import functional as F\n\n\ndef get_backbone(name, pretrained=True):\n\n    \"\"\" Loading backbone, defining names for skip-connections and encoder output. \"\"\"\n\n    # TODO: More backbones\n\n    # loading backbone model\n    if name == 'resnet18':\n        backbone = models.resnet18(pretrained=pretrained)\n    elif name == 'resnet34':\n        backbone = models.resnet34(pretrained=pretrained)\n    elif name == 'resnet50':\n        backbone = models.resnet50(pretrained=pretrained)\n    elif name == 'resnet101':\n        backbone = models.resnet101(pretrained=pretrained)\n    elif name == 'resnet152':\n        backbone = models.resnet152(pretrained=pretrained)\n    elif name == 'vgg16':\n        backbone = models.vgg16_bn(pretrained=pretrained).features\n    elif name == 'vgg19':\n        backbone = models.vgg19_bn(pretrained=pretrained).features\n    # elif name == 'inception_v3':\n    #     backbone = models.inception_v3(pretrained=pretrained, aux_logits=False)\n    elif name == 'densenet121':\n        backbone = models.densenet121(pretrained=True).features\n    elif name == 'densenet161':\n        backbone = models.densenet161(pretrained=True).features\n    elif name == 'densenet169':\n        backbone = models.densenet169(pretrained=True).features\n    elif name == 'densenet201':\n        backbone = models.densenet201(pretrained=True).features\n    elif name == 'unet_encoder':\n        from unet_backbone import UnetEncoder\n        backbone = UnetEncoder(3)\n    else:\n        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n\n    # specifying skip feature and output names\n    if name.startswith('resnet'):\n        feature_names = [None, 'relu', 'layer1', 'layer2', 'layer3']\n        backbone_output = 'layer4'\n    elif name == 'vgg16':\n        # TODO: consider using a 'bridge' for VGG models, there is just a MaxPool between last skip and backbone output\n        feature_names = ['5', '12', '22', '32', '42']\n        backbone_output = '43'\n    elif name == 'vgg19':\n        feature_names = ['5', '12', '25', '38', '51']\n        backbone_output = '52'\n    # elif name == 'inception_v3':\n    #     feature_names = [None, 'Mixed_5d', 'Mixed_6e']\n    #     backbone_output = 'Mixed_7c'\n    elif name.startswith('densenet'):\n        feature_names = [None, 'relu0', 'denseblock1', 'denseblock2', 'denseblock3']\n        backbone_output = 'denseblock4'\n    elif name == 'unet_encoder':\n        feature_names = ['module1', 'module2', 'module3', 'module4']\n        backbone_output = 'module5'\n    else:\n        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n\n    return backbone, feature_names, backbone_output\n\n\nclass UpsampleBlock(nn.Module):\n\n    # TODO: separate parametric and non-parametric classes?\n    # TODO: skip connection concatenated OR added\n\n    def __init__(self, ch_in, ch_out=None, skip_in=0, use_bn=True, parametric=False):\n        super(UpsampleBlock, self).__init__()\n\n        self.parametric = parametric\n        ch_out = ch_in/2 if ch_out is None else ch_out\n\n        # first convolution: either transposed conv, or conv following the skip connection\n        if parametric:\n            # versions: kernel=4 padding=1, kernel=2 padding=0\n            self.up = nn.ConvTranspose2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(4, 4),\n                                         stride=2, padding=1, output_padding=0, bias=(not use_bn))\n            self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n        else:\n            self.up = None\n            ch_in = ch_in + skip_in\n            self.conv1 = nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(3, 3),\n                                   stride=1, padding=1, bias=(not use_bn))\n            self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # second convolution\n        conv2_in = ch_out if not parametric else ch_out + skip_in\n        self.conv2 = nn.Conv2d(in_channels=conv2_in, out_channels=ch_out, kernel_size=(3, 3),\n                               stride=1, padding=1, bias=(not use_bn))\n        self.bn2 = nn.BatchNorm2d(ch_out) if use_bn else None\n\n    def forward(self, x, skip_connection=None):\n\n        x = self.up(x) if self.parametric else F.interpolate(x, size=None, scale_factor=2, mode='bilinear',\n                                                             align_corners=None)\n        if self.parametric:\n            x = self.bn1(x) if self.bn1 is not None else x\n            x = self.relu(x)\n\n        if skip_connection is not None:\n            x = torch.cat([x, skip_connection], dim=1)\n\n        if not self.parametric:\n            x = self.conv1(x)\n            x = self.bn1(x) if self.bn1 is not None else x\n            x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x) if self.bn2 is not None else x\n        x = self.relu(x)\n\n        return x\n\n\nclass Unet(nn.Module):\n\n    \"\"\" U-Net (https://arxiv.org/pdf/1505.04597.pdf) implementation with pre-trained torchvision backbones.\"\"\"\n\n    def __init__(self,\n                 backbone_name='resnet50',\n                 pretrained=True,\n                 encoder_freeze=False,\n                 classes=21,\n                 decoder_filters=(256, 128, 64, 32, 16),\n                 parametric_upsampling=True,\n                 shortcut_features='default',\n                 decoder_use_batchnorm=True):\n        super(Unet, self).__init__()\n\n        self.backbone_name = backbone_name\n\n        self.backbone, self.shortcut_features, self.bb_out_name = get_backbone(backbone_name, pretrained=pretrained)\n        shortcut_chs, bb_out_chs = self.infer_skip_channels()\n        if shortcut_features != 'default':\n            self.shortcut_features = shortcut_features\n\n        # build decoder part\n        self.upsample_blocks = nn.ModuleList()\n        decoder_filters = decoder_filters[:len(self.shortcut_features)]  # avoiding having more blocks than skip connections\n        decoder_filters_in = [bb_out_chs] + list(decoder_filters[:-1])\n        num_blocks = len(self.shortcut_features)\n        for i, [filters_in, filters_out] in enumerate(zip(decoder_filters_in, decoder_filters)):\n            print('upsample_blocks[{}] in: {}   out: {}'.format(i, filters_in, filters_out))\n            self.upsample_blocks.append(UpsampleBlock(filters_in, filters_out,\n                                                      skip_in=shortcut_chs[num_blocks-i-1],\n                                                      parametric=parametric_upsampling,\n                                                      use_bn=decoder_use_batchnorm))\n\n        self.final_conv = nn.Conv2d(decoder_filters[-1], classes, kernel_size=(1, 1))\n\n        if encoder_freeze:\n            self.freeze_encoder()\n\n        self.replaced_conv1 = False  # for accommodating  inputs with different number of channels later\n\n    def freeze_encoder(self):\n\n        \"\"\" Freezing encoder parameters, the newly initialized decoder parameters are remaining trainable. \"\"\"\n\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n\n    def forward(self, *input):\n\n        \"\"\" Forward propagation in U-Net. \"\"\"\n\n        x, features = self.forward_backbone(*input)\n\n        for skip_name, upsample_block in zip(self.shortcut_features[::-1], self.upsample_blocks):\n            skip_features = features[skip_name]\n            x = upsample_block(x, skip_features)\n\n        x = self.final_conv(x)\n        return x\n\n    def forward_backbone(self, x):\n\n        \"\"\" Forward propagation in backbone encoder network.  \"\"\"\n\n        features = {None: None} if None in self.shortcut_features else dict()\n        for name, child in self.backbone.named_children():\n            x = child(x)\n            if name in self.shortcut_features:\n                features[name] = x\n            if name == self.bb_out_name:\n                break\n\n        return x, features\n\n    def infer_skip_channels(self):\n\n        \"\"\" Getting the number of channels at skip connections and at the output of the encoder. \"\"\"\n\n        x = torch.zeros(1, 3, 224, 224)\n        has_fullres_features = self.backbone_name.startswith('vgg') or self.backbone_name == 'unet_encoder'\n        channels = [] if has_fullres_features else [0]  # only VGG has features at full resolution\n\n        # forward run in backbone to count channels (dirty solution but works for *any* Module)\n        for name, child in self.backbone.named_children():\n            x = child(x)\n            if name in self.shortcut_features:\n                channels.append(x.shape[1])\n            if name == self.bb_out_name:\n                out_channels = x.shape[1]\n                break\n        return channels, out_channels\n\n    def get_pretrained_parameters(self):\n        for name, param in self.backbone.named_parameters():\n            if not (self.replaced_conv1 and name == 'conv1.weight'):\n                yield param\n\n    def get_random_initialized_parameters(self):\n        pretrained_param_names = set()\n        for name, param in self.backbone.named_parameters():\n            if not (self.replaced_conv1 and name == 'conv1.weight'):\n                pretrained_param_names.add('backbone.{}'.format(name))\n\n        for name, param in self.named_parameters():\n            if name not in pretrained_param_names:\n                yield param\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-10T07:46:34.694763Z","iopub.execute_input":"2021-06-10T07:46:34.69531Z","iopub.status.idle":"2021-06-10T07:46:34.947124Z","shell.execute_reply.started":"2021-06-10T07:46:34.695276Z","shell.execute_reply":"2021-06-10T07:46:34.946387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# U-Net Backbone","metadata":{}},{"cell_type":"code","source":"class UnetDownModule(nn.Module):\n\n    \"\"\" U-Net downsampling block. \"\"\"\n\n    def __init__(self, in_channels, out_channels, downsample=True):\n        super(UnetDownModule, self).__init__()\n\n        # layers: optional downsampling, 2 x (conv + bn + relu)\n        self.maxpool = nn.MaxPool2d((2,2)) if downsample else None\n        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                               kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels,\n                               kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if self.maxpool is not None:\n            x = self.maxpool(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        return x\n\n\nclass UnetEncoder(nn.Module):\n\n    \"\"\" U-Net encoder. https://arxiv.org/pdf/1505.04597.pdf \"\"\"\n\n    def __init__(self, num_channels):\n        super(UnetEncoder, self,).__init__()\n        self.module1 = UnetDownModule(num_channels, 64, downsample=False)\n        self.module2 = UnetDownModule(64, 128)\n        self.module3 = UnetDownModule(128, 256)\n        self.module4 = UnetDownModule(256, 512)\n        self.module5 = UnetDownModule(512, 1024)\n\n    def forward(self, x):\n        x = self.module1(x)\n        x = self.module2(x)\n        x = self.module3(x)\n        x = self.module4(x)\n        x = self.module5(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.948499Z","iopub.execute_input":"2021-06-10T07:46:34.948941Z","iopub.status.idle":"2021-06-10T07:46:34.961246Z","shell.execute_reply.started":"2021-06-10T07:46:34.948907Z","shell.execute_reply":"2021-06-10T07:46:34.96041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Define train-val Dataset and Create DataLoader**","metadata":{}},{"cell_type":"code","source":"# Intialize some useful variables\n\nDATA_DIR = '../input/siim-png-images/train_png'\ndata_csv = '../input/siim-acr-pneumothorax-segmentation-data/train-rle.csv'\nbatch_size = 16\n\nEncoder = 'resnet34'\nWeights = 'imagenet'","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.962459Z","iopub.execute_input":"2021-06-10T07:46:34.962839Z","iopub.status.idle":"2021-06-10T07:46:34.972919Z","shell.execute_reply.started":"2021-06-10T07:46:34.962804Z","shell.execute_reply":"2021-06-10T07:46:34.972133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define augmentation and preprocessing-function(according to Encoder)\n\ntransform = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5),\n    A.OneOf([A.RandomGamma(gamma_limit=(90,110)),\n             A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)], p=0.5),\n    A.Resize(width=224, height=224)\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.974254Z","iopub.execute_input":"2021-06-10T07:46:34.974612Z","iopub.status.idle":"2021-06-10T07:46:34.982233Z","shell.execute_reply.started":"2021-06-10T07:46:34.974577Z","shell.execute_reply":"2021-06-10T07:46:34.981562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation\n\ndf = pd.read_csv(data_csv)\ndf_train, df_val = model_selection.train_test_split(df, test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:34.983463Z","iopub.execute_input":"2021-06-10T07:46:34.983833Z","iopub.status.idle":"2021-06-10T07:46:35.127388Z","shell.execute_reply.started":"2021-06-10T07:46:34.983805Z","shell.execute_reply":"2021-06-10T07:46:35.126507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Dataset\ntrain_dataset = SIIMDataset(df_train,\n                            DATA_DIR,\n                            transform = transform,\n                            preprocessing_fun = preprocess_input)\n\nval_dataset = SIIMDataset(df_val,\n                          DATA_DIR,\n                          transform = transform,\n                          preprocessing_fun = preprocess_input)\n\n# Create DataLoader\ntrain_loader = DataLoader(train_dataset,\n                          batch_size = batch_size,\n                          shuffle = True,\n                          num_workers = 8)\n\nval_loader = DataLoader(val_dataset,\n                        batch_size = batch_size,\n                        num_workers = 4)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:35.128738Z","iopub.execute_input":"2021-06-10T07:46:35.129107Z","iopub.status.idle":"2021-06-10T07:46:35.13637Z","shell.execute_reply.started":"2021-06-10T07:46:35.129058Z","shell.execute_reply":"2021-06-10T07:46:35.135509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore DataLoader\n\nprint('Training data Info:')\ndataiter = iter(train_loader)\ndata = dataiter.next()\nimages,labels = data['image'],data['mask']\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))\n\nprint('\\nValidation data Info:')\ndataiter = iter(val_loader)\ndata = dataiter.next()\nimages,labels = data['image'],data['mask']\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:35.138125Z","iopub.execute_input":"2021-06-10T07:46:35.138494Z","iopub.status.idle":"2021-06-10T07:46:43.164612Z","shell.execute_reply.started":"2021-06-10T07:46:35.138458Z","shell.execute_reply":"2021-06-10T07:46:43.16315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize Images**","metadata":{}},{"cell_type":"code","source":"def denoramlize(img):\n    img = img.permute(1,2,0)            # change shape ---> (width, height, channel)\n    mean = torch.FloatTensor([0.485, 0.456, 0.406])\n    std = torch.FloatTensor([0.229, 0.224, 0.225])\n    img = img*std + mean\n    img = np.clip(img,0,1)              # convert the pixel values range(min=0, max=1)\n    return img\n\ndef imshow(img, mask):\n    fig = plt.figure(figsize=(15, 10))\n    a = fig.add_subplot(1, 3, 1)\n    plt.imshow(denoramlize(img), cmap='bone')\n    a.set_title(\"Original x-ray image\")\n    plt.grid(False)\n    plt.axis(\"off\")\n\n    a = fig.add_subplot(1, 3, 2)\n    imgplot = plt.imshow(torch.squeeze(mask, dim=1).permute(1,2,0), cmap='binary')\n    a.set_title(\"The mask\")\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    a = fig.add_subplot(1, 3, 3)\n    plt.imshow(denoramlize(img), cmap='bone')\n    plt.imshow(torch.squeeze(mask, dim=1).permute(1,2,0), cmap='binary', alpha=0.3)\n    a.set_title(\"Mask on the X-ray image\")\n\n    plt.axis(\"off\")\n    plt.grid(False)\n\n\ndef show_batch_image(dataloader, num_images):\n    data = next(iter(dataloader))\n    image,mask = data['image'],data['mask']\n    img_idx = torch.randint(0, dataloader.batch_size, (num_images,))\n    for i in img_idx:\n        imshow(image[i], mask[i])\n\nshow_batch_image(train_loader, 5)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:43.169451Z","iopub.execute_input":"2021-06-10T07:46:43.181202Z","iopub.status.idle":"2021-06-10T07:46:53.04455Z","shell.execute_reply.started":"2021-06-10T07:46:43.181146Z","shell.execute_reply":"2021-06-10T07:46:53.043455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create Loss Class**\n\n* Here, Dice Loss is define as well as Focal Loss is also created to get better results.\n* For this notebook, I have used the combination of two loss: diceloss and focalloss","metadata":{}},{"cell_type":"code","source":"def dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\nclass MixedLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=None):\n        super().__init__()\n#         self.alpha = alpha\n#         self.focal = FocalLoss(gamma)\n\n    def forward(self, input, target):\n#         loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        loss = -torch.log(dice_loss(input, target))\n\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:53.04646Z","iopub.execute_input":"2021-06-10T07:46:53.046825Z","iopub.status.idle":"2021-06-10T07:46:53.0584Z","shell.execute_reply.started":"2021-06-10T07:46:53.046785Z","shell.execute_reply":"2021-06-10T07:46:53.05714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train The Model**","metadata":{}},{"cell_type":"code","source":"# Create Model\nmodel = Unet(backbone_name = Encoder, classes=1, encoder_freeze=True) # load pre-trained weights \nmodel_tfs =  Unet(backbone_name = Encoder, classes=1, encoder_freeze=False, pretrained=False)\nmodel.to(device)\nmodel_tfs.to(device);","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:46:53.05983Z","iopub.execute_input":"2021-06-10T07:46:53.060296Z","iopub.status.idle":"2021-06-10T07:47:08.130484Z","shell.execute_reply.started":"2021-06-10T07:46:53.06026Z","shell.execute_reply":"2021-06-10T07:47:08.129693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define loss function and Set Optimizer\n\ncriterion = MixedLoss()\n\noptimizer = optim.Adam(model.parameters(), lr = 0.0001)\n\n# Loop over all Epochs\n# CUDA_LAUNCH_BLOCKING=1\nepochs = 15\ntrain_loss_pretrained = []\nval_loss_pretrained = []\n\nfor epoch in range(epochs):\n\n    train_loss = train(train_loader,\n                       model,\n                       criterion,\n                       optimizer)\n    train_loss_pretrained.append(train_loss)\n\n    val_loss = evaluate(val_loader,\n                        model,\n                        criterion)\n    val_loss_pretrained.append(val_loss)\n\n\n    print(f'Epoch: {epoch+1}')\n    print(f'Training Loss: {train_loss}, \\t Validation Loss: {val_loss}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T07:55:54.198644Z","iopub.execute_input":"2021-06-10T07:55:54.198987Z","iopub.status.idle":"2021-06-10T09:14:11.985766Z","shell.execute_reply.started":"2021-06-10T07:55:54.198953Z","shell.execute_reply":"2021-06-10T09:14:11.983463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train from scratch ","metadata":{}},{"cell_type":"code","source":"# Loop over all Epochs\n# CUDA_LAUNCH_BLOCKING=1\nepochs = 15\ntrain_loss_tfs = []\nval_loss_tfs = []\n\nfor epoch in range(epochs):\n\n    train_loss = train(train_loader,\n                       model_tfs,\n                       criterion,\n                       optimizer)\n    train_loss_tfs.append(train_loss)\n\n    val_loss = evaluate(val_loader,\n                        model_tfs,\n                        criterion)\n    val_loss_tfs.append(val_loss)\n\n\n    print(f'Epoch: {epoch+1}')\n    print(f'Training Loss: {train_loss}, \\t Validation Loss: {val_loss}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T09:49:07.552821Z","iopub.execute_input":"2021-06-10T09:49:07.553219Z","iopub.status.idle":"2021-06-10T11:09:01.414597Z","shell.execute_reply.started":"2021-06-10T09:49:07.55318Z","shell.execute_reply":"2021-06-10T11:09:01.412251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Val Loss Curves","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,7))\nfig.add_subplot(1,2,1)\nplt.plot(train_loss_pretrained, label='Imagenet Pretrained')\nplt.plot(train_loss_tfs, label='Train from sratch')\nplt.title('Train Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Dice Loss')\nplt.legend()\n\nfig.add_subplot(1,2,2)\nplt.plot(val_loss_pretrained, label='Imagenet Pretrained')\nplt.plot(val_loss_tfs, label='Train from sratch')\nplt.title('Val Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Dice Loss');","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:09:01.416672Z","iopub.execute_input":"2021-06-10T11:09:01.417048Z","iopub.status.idle":"2021-06-10T11:09:01.737885Z","shell.execute_reply.started":"2021-06-10T11:09:01.417002Z","shell.execute_reply":"2021-06-10T11:09:01.736932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}