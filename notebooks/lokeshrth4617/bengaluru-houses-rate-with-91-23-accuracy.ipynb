{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib \nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nimport sklearn\nimport xgboost\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"home = pd.read_csv(\"/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv\")\nhome.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"## finding null values in %form\n\nround(100*(home.isnull().sum()/len(home.index)),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing NaN values from the dataset\nhome.dropna(inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home = home.drop(columns='society')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home.reset_index(drop= True, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home['bhk'] = home['size'].str.split().str[0]\nhome['bhk'].dropna(inplace = True)\nhome['bhk'] = home['bhk'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(home['total_sqft'].iloc[[17]])\n\n## fucntion to remove 2100 - 2850 by taking there average\ndef convert_sqft_to_num(x):\n    tokens = x.split('-')\n    if len(tokens) == 2:\n        return (float(tokens[0])+float(tokens[1]))/2\n    try:\n        return float(x)\n    except:\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## applying the fucntion to the column: - 'total_sqft'\nhome.total_sqft = home.total_sqft.apply(convert_sqft_to_num)\n# Taking only the Numeric values from the data and storing it in 'home'\nhome = home[home.total_sqft.notnull()]\n# display the first 2 columns from the dataset\nhome.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##removing invalid data entry\n## Example: The total sqft divided by the number of bhk should always be more than 300\n\nhome = home[~(home.total_sqft/home.bhk<200)]\nhome.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## dividing the dataset into Continous and Categorical variables:\ncont_ = home.select_dtypes(exclude = 'object')\ncat_ = home.select_dtypes(include  = 'object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## displaying only the continous variables from the dataset\n## to determine the variables which have outliers and those which needs to be removed\nfig = plt.figure(figsize = (10,8))\nfor index,col in enumerate(cont_):\n    plt.subplot(3,2,index+1)\n    sns.boxplot(y = cont_.loc[:,col])\nfig.tight_layout(pad = 1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home = home.drop(home[home['bath']>6].index)\nhome = home.drop(home[home['bhk']>7.0].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature Engineering step\nhome['price_per_sqft'] = home['price']*100000/home['total_sqft']\nhome.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home['price_per_sqft'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## taking only the values with 1st Standard devaition values.\n## as per Normal Distribution, 95% of our data lies within 1st Standard Deviation as per the location\n\ndef remove_pps_outliers(df):\n    df_out = pd.DataFrame()\n    for key, subdf in df.groupby('location'):\n        m = np.mean(subdf.price_per_sqft)\n        st = np.std(subdf.price_per_sqft)\n        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out\nhome = remove_pps_outliers(home)\nhome.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## finding correlation values within the dataset\n## we remove features which are highly related to each other as they do not provide\n## any significance value to our Model\n\ncorr = home.corr()\nplt.figure(figsize = (10,8))\nsns.heatmap(corr,mask = corr<0.8 ,annot= True,cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home.drop(columns=['availability','size','area_type'],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking the dataset with highest location data provided\n## because havind values for a location less than 10 wont give us good information on the dataset\n\nhome.location = home.location.str.strip()\nlocation_stats = home['location'].value_counts(ascending=False)\nlocation_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## cretaing a Series of all the location having less than 10 entries against its  \nlocation_stats_less_than_10 = location_stats[location_stats<=10]\nlocation_stats_less_than_10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## using lambda function to naming 'location_stats_less_than_10' as 'other' and then removing it\n\nhome.location = home.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)\n\nhome = home[home.location != 'other']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Keeping in mind that the number of Bathroom shouldn't be more than BHK+2\n## Example for a 3 bhk, the number of bathrooms shouldn't be more than 5\n\nhome = home[home.bath<home.bhk+2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## representing Numerical Data and Visualizing the same usin Distplot to gain further info\n\nnum_ = home.select_dtypes(exclude = 'object')\nfig = plt.figure(figsize =(10,8))\nfor index, col in enumerate(num_):\n    plt.subplot(3,2,index+1)\n    sns.distplot(num_.loc[:,col],kde = False)\nfig.tight_layout(pad = 1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## performing One hot encoding on the Categorical values\n## 1st step. create dummies\ndummies = pd.get_dummies(home.location)\ndummies.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## adding the dummies dataframe to our main DataFrame\n\nhome = pd.concat([home,dummies],axis='columns')\n\n## removing 'location' as we have already created the dummies\nhome1 = home.drop('location',axis = 1)\n\n## removing columns which will not be required by our model\nhome1 = home1.drop(columns=['balcony','price_per_sqft'])\nhome1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"home1.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dividing our dataset to Independent and Dependent Variables\n\nX = home1.drop('price',axis = 1).values ## Independent Variables\ny = home1.price.values ## Dependent Variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## adding a new axis\ny = y[:,np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardize features by removing the mean and scaling to unit variance\n\n#### The standard score of a sample x is calculated as:\n\n**z = (x - u) / s**\n\n- where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## preprocessing the data values to StandardScaler\nsc = preprocessing.StandardScaler()\nX1 = sc.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Standardize a dataset along any axis\n\n## Center to the mean and component wise scale to unit variance.\n\nStd_x1 = preprocessing.scale(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Part"},{"metadata":{"trusted":true},"cell_type":"code","source":"## importing the required libraries for Machine Learning\n\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nfrom sklearn.model_selection import cross_validate as CV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## using Cross Validation of 5 andscoring of Negative mean sqaured error\n\ncross1 = cross_val_score(lr,Std_x1,y,cv=5,scoring='neg_mean_squared_error')\nprint(cross1.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.SCORERS.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from the model selection module import train_test_split for the ML training and testing.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1,y,test_size=0.3,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nacc = mean_squared_error(y_pred,y_test)\nrscore = r2_score(y_pred,y_test)\nprint(rscore)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}