{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Topic Modelling on NeurIPS Papers\n\nIn this notebook, we will explore a dataset containing more than 9,000 documents, which are papers from <br>\nThe Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS). <br>\nIt is a machine learning and computational neuroscience conference. <br>\n<br>\nWe will conduct LDA topic modelling on these papers, and explore the groups in an interative manner.\n\nTable of Content\n* Environment Setup\n* Load and Preprocess Data\n* Word Cloud\n* LDA Topic Modelling\n* Result Visualisation\n* Further Study"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#section-1\"></a>\n# Environment Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#section-2\"></a>\n# Load and Preprocess Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/nips-papers-1987-2019-updated/papers.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick look on missing data. <br>\nIn this notebook we will focus on topic-modelling on `full_text`[](http://) column only, which has no missing data. <br>\nGood to go."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.heatmap(df.isna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess the `full_text` column with a series of functions. <br>\nThe operations are quite obvious from their function names so not to be repeated here. <br>\nLemmatization instead of Port Stemmer is used to preserve more meaningful full words from the documents. <br>\nNoun is used as the part of speech in lemmatization."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 2min 30s\nimport nltk\nfrom gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, preprocess_string\n\nlemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n\ndf['full_text'] = df['full_text'].astype('str')\ndf['full_text_tokenized'] = df['full_text'].apply(lambda text: preprocess_string(text, [\n    strip_tags, \n    strip_punctuation, \n    strip_multiple_whitespaces, \n    strip_numeric, \n    remove_stopwords, \n    strip_short, \n    lemmatizer.lemmatize, \n    lambda x: x.lower()\n]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['full_text_tokenized'].sample(n=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick look on tokenized documents' lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\ndf['full_text_tokenized_len'] = df['full_text_tokenized'].apply(lambda text: len(text))\nsns.histplot(df[df['full_text_tokenized_len'] > 0]['full_text_tokenized_len'], log_scale=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#section-3\"></a>\n# Word Cloud\nNext, the typical word cloud. <br>\nNot surpisingly, machine learning, processing system, neural network, information processing, reinforcement learning, loss function are some common terms."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 2min 50s\nfrom wordcloud import WordCloud\n\nlong_string = ' '.join([' '.join(words) for words in df['full_text_tokenized'].values])\nwordcloud = WordCloud(width=800, height=400)\nwordcloud.generate(long_string)\nwordcloud.to_image()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#section-4\"></a>\n# LDA Topic Modelling\n\nWe will go through the following steps:\n* Create a \"dictionary\" containing all unique words in all documents\n* Create a \"corpus\". Each document will be converted into a bag of words, e.g. [(0, 1), (1, 1), (4, 2), ...]. <br> \nEach tuple means (word index, word occurrence in the document)\n* Train the LDA model. Tune hyper-parameter `passes` and `iterations` until most documents are \"converged\"\n* Visualise the result, exploring different groups of documents"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 30s\nimport gensim\n\ndictionary = gensim.corpora.Dictionary(df['full_text_tokenized'].values)\ndictionary.filter_extremes(no_below=20, no_above=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 13.2s\ncorpus = [dictionary.doc2bow(doc) for doc in df['full_text_tokenized'].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of unique tokens: {len(dictionary):,}')\nprint(f'Number of documents: {len(corpus):,}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n# 13mins\nimport logging\nfrom gensim.models.ldamulticore import LdaMulticore\n\n# Take too much time for kaggle save version. Set it to True during development.\nenable_debug = False\n\nif enable_debug:\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n\nmodel = LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=40, iterations=100)\n\nif enable_debug:\n    logger.setLevel(logging.WARNING)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`passess` and `iterations` are tuned to be high enough so that most of the documents are converged at the last pass.\nBelow are some logs when running the cell in DEBUG logging mode:\n\n> 2021-03-15 15:28:37,074 : DEBUG : 1666/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:40,471 : DEBUG : 1509/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:42,532 : DEBUG : 1454/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:46,568 : DEBUG : 1489/2000 documents converged within 100 iterations <br>\n2021-03-15 15:28:49,296 : DEBUG : 1265/1680 documents converged within 100 iterations <br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#section-5\"></a>\n# Result Visualisation\n\nHere comes the fruit. <br>\nWe will use gensim model's `print_topics` function to see popular terms in each group. <br>\nAlso, pyLDAvis will be used to see the groups in a graph. <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.print_topics(num_topics=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nprep_display = pyLDAvis.gensim.prepare(model, corpus, dictionary)\npyLDAvis.display(prep_display)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"#section-6\"></a>\n# Further Study\n\nHere are some possible directions to study the dataset further:\n* Change `num_topics` from 10 to 20, 50, etc to explore more detailed papers groupings\n* Compare and contrast the resulted groups using other simlarity algorithms such as TF-IDF, LSA"},{"metadata":{},"cell_type":"markdown","source":"# Thank you for reading\n\nLet me know your thoughts in the comments below :D "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}