{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Code cell just to show what all data is there\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy\n# import gensim\nimport nltk\n# import seaborn\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data into DataFrames","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"summary = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='iso-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw = pd.read_csv('/kaggle/input/news-summary/news_summary_more.csv', encoding='iso-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.head() #how does raw DataFrame look like","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.iloc[0,0], raw.iloc[0,1] #viewing the contents inside raw's 0th element","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making a word_count column in the DataFrame\nraw['word_count'] = raw['text'].apply(lambda x: len(str(x).split(\" \")))\nraw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw.word_count.describe() #describes what all data is there and other stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency counted from all rows/texts (so has all the various kinds of news)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(raw['text']).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq.plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stopwords are all the commonly used english words which don't contribute to keywords such as 'as', 'are' etc\nstop_words = set(stopwords.words(\"english\"))\n# Creating a customized stopword list from data shown below after several iterations\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\",\n             \"one\", \"two\", \"new\", \"previously\", \"shown\", \"year\", \"old\", \"said\", \"reportedly\",\n             \"added\", \"u\", \"day\", \"time\"]\nstop_words = stop_words.union(new_words) #customised stopwords added to previous stopword","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new list of texts called corpus where the following things are removed\ncorpus = []\nfor i in range(0, 3847):\n    # Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', raw['text'][i])\n    # Convert to lowercase\n    text = text.lower()\n    # Remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    # Remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    # Convert to list from string\n    text = text.split()\n    # Stemming\n    ps=PorterStemmer()\n    # Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The original text\nraw.iloc[0,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After removing stopwords, punctions and normalizing to root words\ncorpus[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word cloud which is pretty good for representation\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n# This is just to represent such representations are also possible\n# Here representation only done for the first text\nnum_text = 1\nfor i in range(num_text):\n    wordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(corpus[i]))\n    fig = plt.figure(1)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    fig.savefig(\"word1.png\", dpi=900)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tells us the max keywords used without including stopwords in the whole corpus and we add such words to new stop words\nfreq = pd.Series(' '.join(corpus).split()).value_counts()[:20]\nfreq ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Only done for a single corpus text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Corpus cell number chosen(arbritarily)\ncorpn = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizes and builds a vocabulary\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(stop_words=stop_words, max_features=10, ngram_range=(1,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this can be put inside a loop to get key words for all articles\ncorpi = [corpus[corpn]] #changing the number here will give us the key words for that specific article\nX=cv.fit_transform(corpi)\nlist(cv.vocabulary_.keys())[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realtext = raw.iloc[corpn,1]\nrealtext","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Term Frequency: This summarizes how often a given word appears within a document.\nInverse Document Frequency: This downscales words that appear a lot across documents.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n# get feature names\nfeature_names=cv.get_feature_names()\n \n# fetch document for which keywords needs to be extracted\ndoc=corpus[corpn]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf_idf sorting in descending order\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\nAbstract:\")\nprint(doc)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}