{"cells":[{"metadata":{"_uuid":"324153cfe110504b39f5933c7fa268f416655379"},"cell_type":"markdown","source":"In this notebook, I predict the average price of avocado using time-series modeling \ntechniques"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# The usual suspects\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pylab as plt\nfrom sklearn.cross_validation import cross_val_score, time\nfrom sklearn.model_selection import TimeSeriesSplit\n# Ignore warnings (this isn't a good practice usually)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a18fd8a0adf9cd7d06e21eb8b0fe2123ab0558bf"},"cell_type":"code","source":"# Avocados are green. :) \nGREEN_COLORMAP = sns.color_palette(\"Greens\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69c707c2c54b22d6689d3c2c21dc88bf07fbe670"},"cell_type":"markdown","source":"Let's start by loading the data and doing some time-series exploration."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Loading data and EDA"},{"metadata":{"trusted":true,"_uuid":"348be436e2405ef8518571d5facab3c1a4dda1c8"},"cell_type":"code","source":"DATA_PATH  = \"../input/avocado.csv\"\ndf = pd.read_csv(DATA_PATH, parse_dates=['Date'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"869740a1b17a55640600907897f018eb88e69b58"},"cell_type":"markdown","source":"First, let's plot the avocado's average price over time."},{"metadata":{"trusted":true,"_uuid":"2edfc505f30f97a81131e4605675d1fa63dacc39","scrolled":false},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\ndf.set_index('Date').plot(y='AveragePrice', ax=ax, color=GREEN_COLORMAP[2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b78bfd411eeb45a05c9cdbd3b731853fc69263fd"},"cell_type":"markdown","source":"Not bad for a first graph. However, things are a little bit packed. Possible solution: let's make \na time-series plot per year."},{"metadata":{"trusted":true,"_uuid":"e34367f869b9bb20df6a6438b7597b7ecca9da52","scrolled":false},"cell_type":"code","source":"# Get the number of years and create one plot per year.\n# Notice that 2018 has less samples than the previous ones.\nyears = df.year.unique()\nnumber_years = len(years)\nfig, axes = plt.subplots(number_years, 1, figsize=(12, 8))\nfor i, year in enumerate(years):\n    # One green shade per year :)\n    # Also, no line connecting the points and marker set to a dot\n    # for enhanced readability.\n    (df.set_index('Date')\n       .loc[lambda df: df.year == year]\n       .plot(y='AveragePrice', ax=axes[i], color=GREEN_COLORMAP[i],\n             marker=\"o\", linestyle=\"\"))\n    axes[i].legend_.remove()\n\nfig.set_tight_layout(\"tight\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c9795b9535b09861bc5d05227e3d34f06db43f"},"cell_type":"markdown","source":"It appears there are multiple points per day. How many exactly?"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e13b0ac3fdbea64338fcea09ada364ace7da5078"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\ndf.groupby('Date').size().plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f6990fe99c3d2811703bec4392fadd067473b203"},"cell_type":"code","source":"df.Date.diff().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e6edd81fc459f43481f07f286ff67ce0a04a1a"},"cell_type":"markdown","source":"We also notice that most observations are made once a week (thus the delta of -7  days). \nWhy are there 108 observations per week though? Hint: look at the `region` and `type` columns."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7afd4b1a58071b15bce5e4a364859d049412a1bc"},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(12, 8))\ndf.groupby('Date')['region'].nunique().plot(ax=axes[0])\ndf.groupby('Date')['type'].nunique().plot(ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"071eff3fbf6f59102861364ded35d46dbd1d2c8b"},"cell_type":"code","source":"(\"Bingo, that's it: there are {} unique regions and {} unique\" \n\" types of Avocado ({})\").format(df['region'].nunique(),\n                                 df['type'].nunique(),\n                                ' and '.join(df['type'].unique())) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b215a73b388fcf9af915ad67a2c9811156a18f1"},"cell_type":"markdown","source":"To wrap this short EDA, let's check these regions and types."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"de9040a0ab823e9e8567034dd9302d5947beb486"},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\ndf['region'].value_counts().plot(kind='bar', ax=axes[0], \n                                 color=GREEN_COLORMAP)\ndf['type'].value_counts().plot(kind='bar', ax=axes[1], color=GREEN_COLORMAP)\nfig.set_tight_layout(\"tight\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"679022b6c89b19095c18d99c9890407a81b90dac"},"cell_type":"markdown","source":"# Time-series processing"},{"metadata":{"_uuid":"9e1e46f3d2b79233034f57153ab0dcea40ff8a26"},"cell_type":"markdown","source":"Next, we will compute the \"real\" average avocado's price (over the different regions and types) and only keep\nthis column (in addition to the `Date` obviously). "},{"metadata":{"trusted":true,"_uuid":"891948f7a1d93ab3fc8ef48e83a5088b60d4d97a"},"cell_type":"code","source":"ts = df.groupby('Date')['AveragePrice'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"455f1554fe7427d07ccea8e36b37ace65235e5e7"},"cell_type":"code","source":"ts.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05007ca59d2d353aebf34fa0eb4aaa97e7f98c76"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\nts.set_index('Date').plot(ax=ax, marker=\"o\", linestyle=\"-\", color=GREEN_COLORMAP[2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"905a0c7baa57fe4403b6f034832cd4345efd77a4"},"cell_type":"markdown","source":"Let's see how this time-series looks like when resampled to a monthly frequency."},{"metadata":{"trusted":true,"_uuid":"e7fb36c77959c99153a240977be975b54444d649"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n(ts.set_index('Date')\n   .resample('1M')\n   .mean()\n   .plot(ax=ax, marker=\"o\", linestyle=\"-\", color=GREEN_COLORMAP[2]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6a92cbfb9f2240f88bb8daf64ac1de6307ad414"},"cell_type":"markdown","source":"Some observations: \n    \n* There are seasonal variations: prices are higher from July to October (roughly) since demand is higher during these months.\n* There are also yearly variations: an upward trend probably due to a higher demand?\n* As mentionned earlier, 2018 data stops in March. \n\nTo finish this section, let's plot some simple statistics about the average monthly price: mean, standard deviation, median, min, and max values. "},{"metadata":{"trusted":true,"_uuid":"19e2df67b3651735d76d35b9781d2a1daa1184c5"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n(ts.set_index('Date')\n   .assign(month=lambda df: df.index.month)\n   .groupby('month')['AveragePrice'].agg([\"mean\", \"std\", \"median\", \"min\", \"max\"])\n   .plot(ax=ax, marker=\"o\"))\nax.set_xlabel('Month')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca0fb9ad97c5f2b15dd9461eaf589a35c0de4954"},"cell_type":"markdown","source":"# Temporal train/test split"},{"metadata":{"_uuid":"e8cfb27db99064cedffbae39c5d9a23452fbef8a"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"335395f362cb9aa085adedf962c2cf096dc27933"},"cell_type":"markdown","source":"As in any ML task, will start by dividing the dataset into train and test data. \nWe won't look at the test dataset until the end. \nAlso, since this is a time-series ML problem, we will use a timestamp to perform the split (can't use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn): \ndata before 2018 belongs to the train dataset. After, it belongs to the test dataset. "},{"metadata":{"trusted":true,"_uuid":"0ee3d909d6e6aed41b4fc5a6e99d028096e95cc4"},"cell_type":"code","source":"# Renaming the ts DataFrame's columns (you will see why soon) before temporal split\nrenamed_ts = ts.rename(columns={\"Date\": \"ds\", \"AveragePrice\": \"y\"})\ntrain_ts = renamed_ts.loc[lambda df: df['ds'].dt.year < 2018, :]\ntest_ts = renamed_ts.loc[lambda df: df['ds'].dt.year == 2018, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e734baeed05acba53d9f707997aefb7302815ab"},"cell_type":"code","source":"train_ts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7a8d4b2d27d3b3d68643918eea760dc3e1c701e4"},"cell_type":"code","source":"train_ts.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2d99b014eda5b5b5c31e83c3c3ac6f909fd97a0"},"cell_type":"markdown","source":"To assess the quality of the model's predictions, will be using a the [**mean absolute error**](https://en.wikipedia.org/wiki/Mean_absolute_error). The lower is this error, the better the model. \n\nAlright, time to kickstart the modeling! Let's begin with traditional models, i.e. statistical time-series models. "},{"metadata":{"_uuid":"f5dae2eec8b453d10c6dc8c97b02f9cc7583146c"},"cell_type":"markdown","source":"# Statistical models"},{"metadata":{"_uuid":"8e250591c49d5c1a99a090f12b7508405d269d6b"},"cell_type":"markdown","source":"For that, let's use [`Prophet`](https://facebook.github.io/prophet/docs/quick_start.html) (this is why we renamed the `Date` and `AveragePrice` columns), an open-source time-series (unveiled a year and half ago) analysis library developed at Facebook. For more details, I recommend checking the [announcement](https://research.fb.com/prophet-forecasting-at-scale/) blog post."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"5b27b517bc0a7acd2658c7f559e917e0fc325565"},"cell_type":"code","source":"from fbprophet import Prophet\nfrom fbprophet.diagnostics import cross_validation, performance_metrics\n\n\n# TODO: Add some comments\nHORIZON = \"90days\"\nPERIOD = \"7days\"\n\nprophet_model = Prophet()\nprophet_model.fit(train_ts)\nprophet_cv_df = cross_validation(prophet_model, horizon=HORIZON, \n                                 period=PERIOD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0dfb110fe6613f75396e39331c07228a96f7ae1f"},"cell_type":"code","source":"prophet_cv_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e1c2e742b458f982efe98021acf5922069ef1139"},"cell_type":"code","source":"prophet_perf_df = performance_metrics(prophet_cv_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1dc218c842374856a5c22d1bd95d4c44722ffafe"},"cell_type":"code","source":"prophet_perf_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"856fe48eeef9273cb045c59282271e32995a8486"},"cell_type":"code","source":"from fbprophet.plot import plot_cross_validation_metric\nplot_cross_validation_metric(prophet_cv_df, metric='mae');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ba236bf48ec3a0db7a2b108226a64cd1a72ea50"},"cell_type":"markdown","source":"What is the CV MAPE evolution when varying the horizon (i.e. the number of days in the future to predict)? "},{"metadata":{"trusted":true,"_uuid":"54203c55aee58c960b069cf71266511f5e51dadb"},"cell_type":"code","source":"fig ,ax = plt.subplots(1, 1, figsize=(12, 8))\n(prophet_perf_df.groupby('horizon')['mae']\n                .mean()\n                .plot(ax=ax, marker=\"o\", colors=GREEN_COLORMAP[2]))\nax.set_ylabel('MAE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8169a92e5dadd97ab02ca1dde90432785bb69c62"},"cell_type":"code","source":"future_prophet_df = prophet_model.make_future_dataframe(periods=365)\npredicted_prophet_df = prophet_model.predict(future_prophet_df)\nprophet_model.plot(predicted_prophet_df);\nprophet_model.plot_components(predicted_prophet_df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c75d2d3f467e5997447fcb6ed80d2a27e4ccdd5b"},"cell_type":"markdown","source":"## ML models"},{"metadata":{"_uuid":"c9c898a9162e0135fd5c6d4d0e413484dda0729a"},"cell_type":"markdown","source":"Before starting this section, we will need to extract calendar features from the `ds` column. \nWill also add average rolling mean prices (yearly and monthly). Notice that I approximate the last year using\nthe 52 previous points and the last months by using the 4 previous points. Finally, I backfill missing data. "},{"metadata":{"trusted":true,"_uuid":"1943758418528f5b2109d7fed566ffd5d16bf184"},"cell_type":"code","source":"def add_calendar_features(df):\n    # TODO: Add some comments\n    return (df.assign(month=lambda df: df['ds'].dt.month, \n                                     week=lambda df: df['ds'].dt.week,\n                                     year=lambda df: df['ds'].dt.year,\n                                     past_month_mean_y=lambda df: \n                                      (df['y'].rolling(window=4)\n                                              .mean()\n                                              .fillna(method='bfill')),\n                                     past_year_mean_y=lambda df: \n                                      (df['y'].rolling(window=52)\n                                              .mean())\n                                              .fillna(method='bfill'))\n                              )\n\n\n\naugmented_ts = add_calendar_features(renamed_ts)\naugmented_train_ts = augmented_ts.loc[lambda df: df['ds'].dt.year < 2018, :].drop('ds', axis=1)\naugmented_test_ts = augmented_ts.loc[lambda df: df['ds'].dt.year == 2018, :].drop('ds', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f46e2882783bbcd00abf31f638ba476acee77cb8"},"cell_type":"code","source":"augmented_train_ts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598492f6252a90b4733bff4a4024f711682e91fb"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\naugmented_train_ts.plot(y='past_month_mean_y', ax=ax)\naugmented_train_ts.plot(y='past_year_mean_y', ax=ax)\naugmented_train_ts.plot(y='y', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9631a9b2458a8bfd78c9bf8eb4770cc8ea8e7cd9"},"cell_type":"markdown","source":"ALright, now we need to define a time-series compatible CV. For that, we will \nuse `TimeSeriesSplit` from `sklearn`."},{"metadata":{"trusted":true,"_uuid":"0418deef8c73f37eb760cfafbed3d934f7ea418b"},"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a39769274ddb2814a95fd4d5e8abb544da75f376"},"cell_type":"markdown","source":"If you are unfamilar with CV for time-series, I highly recommend checking this blog post: https://robjhyndman.com/hyndsight/tscv/. "},{"metadata":{"_uuid":"033894e68fa8aa8eb144f65fad2a6647733adab7"},"cell_type":"markdown","source":"As a model, let's try the tpot auto-ml tool and see what it gets. Notice that I use the negative MAE since sklearn needs a score (the higher the better) to optimize in the CV method."},{"metadata":{"trusted":true,"_uuid":"c1ea1ed0ae6c4f5b5588b8b085f3af49f28a2365"},"cell_type":"code","source":"from tpot import TPOTRegressor\n\n# TODO: Try more generations and a bigger population size. \n# Be careful not to run out of time!\n\ntpot_model = TPOTRegressor(generations=20, population_size=100, cv=tscv, \n                           scoring=\"neg_mean_absolute_error\", \n                           n_jobs=2, verbosity=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e5e94346f501cc80303566e7676c08e1457cd890"},"cell_type":"code","source":"tpot_model.fit(augmented_train_ts.drop('y', axis=1), \n               augmented_train_ts['y'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2abd5f772c89b8d7446eb1a94932851b023704e6"},"cell_type":"markdown","source":"Based on the CV score, tpot is the winner!\nLet's see if this is true on the test dataset."},{"metadata":{"_uuid":"20746eebb08bb26bcbd47d581f07b490934e9a90"},"cell_type":"markdown","source":"# Test evaluation"},{"metadata":{"_uuid":"ecd75e044f9655fad2aa352e243f291c1a927473"},"cell_type":"markdown","source":"Let's plot the predictions for each model (alongside the true values). For that, we need to prepare\nthe predictions DataFrame. Also, we will compute the MAE for each model."},{"metadata":{"trusted":true,"_uuid":"ccb8b89cc3f1a1c1f46af349c4d1d35f60eb6e7b"},"cell_type":"code","source":"test_timestamps = test_ts.ds.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c497a1f9cd7a0f86fa165f58c24c2331949f9e74"},"cell_type":"code","source":"predicted_prophet_s = predicted_prophet_df.loc[lambda df: df['ds']\n                                               .isin(test_timestamps), \"yhat\"]\npredicted_tpot_s = tpot_model.predict(augmented_test_ts.drop(\"y\", axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"364dc3eb15843dc09ae2f7f46f72dbd0cdda0ecc"},"cell_type":"code","source":"assert predicted_tpot_s.shape == predicted_prophet_s.shape\nassert predicted_tpot_s.shape == test_ts[\"y\"].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d850e544d90b5328b0e19f9f3ddbc5f96f1a421"},"cell_type":"code","source":"predictions_df = pd.DataFrame({'tpot': predicted_tpot_s, \n                              'prophet': predicted_prophet_s,\n                              'true': test_ts['y'].values,\n                              'Date': test_ts['ds'].values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"872e90f27249291aabaa3447953cb8d17fdc3fa5"},"cell_type":"code","source":"print(\"MAE for tpot on the test dataset is: {}\".format(\n    (predictions_df['tpot'] - predictions_df['true']).abs().mean(axis=0)))\nprint(\"MAE for prophet on the test dataset is: {}\".format(\n    (predictions_df['prophet'] - predictions_df['true']).abs().mean(axis=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"45f2b70a4ca0f56975eb40ad1b0f06f8a04767b4"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 10))\npredictions_df.set_index('Date').plot(marker='o', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a97c7e45eae5a62dbad6202177eb61ab2f0e225"},"cell_type":"markdown","source":"That's it for now. I hope you have enjoyed exploring this dataset and some of the time-series modeling techniques.\n\nTo be continued, stay tuned!\n\nOther ideas: \n\n*  Better explanation and investigation of CV for Prophet model.\n* Tuning hyperparamters for Prophet model.\n* RNN models. \n* More generation for TPOT\n\nAlso, since I am not in expert in time-series modeling, let me know if there is any mistake or data leakage.\nAs usual, enjoy!"},{"metadata":{"_uuid":"caf22013e75eeee2f68ca5d0b2bf19987f7a9419"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}