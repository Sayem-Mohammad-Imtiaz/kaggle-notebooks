{"cells":[{"metadata":{"_uuid":"1a5443f687f1ddce7ae6f0ad713cb37bd008d848"},"cell_type":"markdown","source":"In the following notebook, I will try to predict the **major type** (`Type 1` column in this dataset) \nof Pokemons given various features (more about this in what follows).  \n\nBefore you start reading this notebook, I highly recommend checking a previous [EDA notebook](https://www.kaggle.com/yassinealouini/pokemon-eda) where I explore more in details the dataset. \n\nEnjoy!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# There are a lot of warnings about CV not having enough data for each fold.\n# TODO: Find a better way to deal with the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Same old imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport pandas_profiling as pdp\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pylab as plt\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nfrom tqdm import tqdm\nimport itertools\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom tpot import TPOTClassifier\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d8b2e0cc07670c4059be2d8542df14751209d8"},"cell_type":"markdown","source":"# Load the data and quick exploration"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Some constants\nDATA_PATH = \"../input/Pokemon.csv\"\nTARGET_COL = \"Type 1\"\nENCODED_TARGET_COL = \"encoded_type_1\"\nTO_DROP_COLS = [\"#\", \"Name\"]\n# The dataset is small\nTEST_RATIO = 0.1\n# For reproducibility\nSEED = 31415\nRUN_HP_OPTIMIZATION = False\n# Reduce this if needed! (resources are scarce here!)\nMAX_EVALS = 200\nHP_SPACE = {\n    # Trying to reduce class imbalance\n    'max_delta_step': 2, \n    # To avoid overfitting\n    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(1)), \n    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(1)), \n    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n    'gamma': hp.loguniform('gamma', np.log(0.01), np.log(1)),\n}\n# Optimal hp from previous run\nOPTIMAL_HP = {'colsample_bytree': 0.7316836664311229, 'gamma': 0.04744535212276833, \n              'learning_rate': 0.02478735341127185, 'max_depth': 5.0, 'n_estimators': 349.0, \n              'reg_alpha': 0.03216806358838591, 'reg_lambda': 0.019055394071559602}\n# Tpot conf values: increase these for more runs (and hopefully better results)\nTPOT_GENERATION = 20\nTPOT_POPULATION_SIZE = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1628b03b519a3925b33f5f6a6946637ae572789"},"cell_type":"code","source":"# Some useful functions \n\n\n# Inspired from here: http://scikit-learn.org/stable/auto_examples/model_selection/\n# plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_title(title)\n    fig.colorbar(im)\n    tick_marks = np.arange(len(classes))\n    ax.set_xticks(tick_marks)\n    ax.set_xticklabels(classes, rotation=45)\n    ax.set_yticks(tick_marks)\n    ax.set_yticklabels(classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    fig.tight_layout()\n    ax.set_ylabel('True Type 1')\n    ax.set_xlabel('Predicted Type 1')\n    ax.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c1c66bb1de213068b79b25f4914668a276d4de8","scrolled":true},"cell_type":"code","source":"pokemon_df = pd.read_csv(DATA_PATH)\npokemon_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87eda816cc2dd0956a1ac43e89398117269dbf7b"},"cell_type":"markdown","source":"Notice that the `#` and `Name` columns aren't useful for predicting the major type so will be dropped (these are the `TO_DROP_COLS`).  "},{"metadata":{"trusted":true,"_uuid":"4fa9884dc018e0760254e3aa29b831eacd7566c6"},"cell_type":"code","source":"pokemon_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae0c9f2669ef184efadd19f06bb826d3874b2d9f"},"cell_type":"code","source":"pdp.ProfileReport(pokemon_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60bb4e5c09473a5f0c671429ae6e8f6c01979b0c"},"cell_type":"markdown","source":"As mentionned in the beginning, I will predict the major type (this is the `Type 1` column). \nLet's explore the target to start. "},{"metadata":{"trusted":true,"_uuid":"cc4d115016818579a1a616ae18761954db1f42a4"},"cell_type":"code","source":"target_s = pokemon_df['Type 1']\n\"There are {} unique major types\".format(target_s.nunique())\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\ntarget_s.value_counts().plot(kind='bar', ax=ax)\nax.set_ylabel('Number')\nax.set_xlabel(\"Pokemons' Type 1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"545ef657fbbe678733f02e631fc942469197763a"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\ntarget_s.value_counts(normalize=True).mul(100).plot(kind='bar', ax=ax)\nax.set_ylabel('%')\nax.set_xlabel(\"Pokemons' Type 1\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbbf94312eb05fd7d5b775183ea229c9a59ca229"},"cell_type":"markdown","source":"Based on the target's historgrams: \n\n1. This is a **multi-class** (**18** major types) **classification** (categorical target) problem\n2. This is an **unblanaced** problem. Indeed, some types (fairy and flying) are much less common than the other ones."},{"metadata":{"_uuid":"2efaaf0d6c0f17503b8d2226a450f555bbd197e5"},"cell_type":"markdown","source":"Notice that some major types (check the EDA notebook) aren't present for all the generations: flying, dark, and steel types aren't available for the six generations. \n\nThus some **features engineering** based on the `Generation` column might be useful. "},{"metadata":{"_uuid":"bd8556dc84fc0799dc9234318c56e7353bdd97e4"},"cell_type":"markdown","source":"Let's **dummify** (i.e. transform categorical columns into boolean ones) the target, the `Type 2` and `Generation` columns. \n\nFor that I use pandas [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function. Also, since not every Pokemon has a `Type 2`, I have filled the missing values with the \"missing\" type before dummifying. Notice also that I have used a `LabelEncoder` for the target col (since the target contains strings). \n\nFinally, I drop the `TARGET_COL` and `TO_DROP_COLS` (i.e. `Name` and `#`) columns from the features. "},{"metadata":{"trusted":true,"_uuid":"1e2213ddc99a2ceb6192051c450cdb33e9ac6ae4"},"cell_type":"code","source":"le = LabelEncoder()\nencoded_target_s = pd.Series(le.fit_transform(target_s), name=ENCODED_TARGET_COL)\ndummified_target_s = pd.get_dummies(target_s)\ndummified_features_df = (pokemon_df.drop(TO_DROP_COLS + [TARGET_COL], axis=1)\n                                   .assign(Generation=lambda df: df.Generation.astype(str))\n                                   .assign(**{\"Legendary\": lambda df: df[\"Legendary\"].astype(int), \n                                              \"Type 2\": lambda df: df[\"Type 2\"].fillna(\"missing\")})\n                                   .pipe(pd.get_dummies))\nfeatures_and_targets_df = pd.concat([encoded_target_s, dummified_features_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"17c58c3b8a23e06288ee45d20969c8a50daa05c9"},"cell_type":"code","source":"encoded_target_s.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a83cbf8012c83be086f1d10faec98825078539be"},"cell_type":"code","source":"le.inverse_transform(encoded_target_s.sample(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"116924147ade1a17f59e7a9f515f514a57e468b1"},"cell_type":"code","source":"dummified_target_s.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a462231115c3aa3578733b28f69a5a20826e50de"},"cell_type":"code","source":"dummified_features_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5037a19312ec2e0e66f44b4fb861dfdeba441653"},"cell_type":"markdown","source":"To end this preparation phase, let's see if there are any **correlations**"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"507a8265cd5501d634c4c43773283ad04f8acef2"},"cell_type":"code","source":"# Inspired from this: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\ncorr_df = pd.concat([dummified_features_df, dummified_target_s], axis=1).corr()\n\n\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac2d171b267136e532f76487782a925e5acebbb3"},"cell_type":"markdown","source":"Some correlations: \n\n* **Steel** major types tend to be positivelt correlated with **Defense** and **Psychic** with ** Special Attack**. \n* **Ghost** major types tend to be positvely correlated with a Type 2 of **Grass** and **Grass** with **Poison**. \n* **Fairy** major types tend to be positvely correlated with the **Generation** 6. \n* **Dragon** major types tend to be positvely correlated with **Attack** and **Total**. \n\nThese observations aren't surprising to any true Pokemon connoisseur but are, nonethless, reassuring to find using the data. "},{"metadata":{"_uuid":"2971d7cfa37c5ac3b84221af40d2630b0c14bec0"},"cell_type":"markdown","source":"## Train and test split"},{"metadata":{"_uuid":"eca912f3c86b42eac9d64e7010c584019747ea1e"},"cell_type":"markdown","source":"I will split the features and targets into train and test datasets. \n\nThe test dataset will only be used at the end to evaluate the various trained models (you should do this as well whenever you train an ML model). Next, I will use cross validation to train and evaluate the model using the train dataset. "},{"metadata":{"trusted":true,"_uuid":"9fa5ca88c8c5bfa909e52bdd04d30527573f26f9"},"cell_type":"code","source":"train_df, test_df = train_test_split(features_and_targets_df, \n                                     stratify=encoded_target_s, \n                                     test_size=TEST_RATIO, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e888fbb519b196cbd7b254174d96831d7636359"},"cell_type":"code","source":"train_df.head(1).T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"676ac4546bea51af6d5a1720ef8b7ef39145a31d"},"cell_type":"markdown","source":"# Evaluation metric"},{"metadata":{"_uuid":"73961d91b1249adaf37131ecfb879e0f126389da"},"cell_type":"markdown","source":"Alright, now that the features have been prepared and split, it is time to pick an evaluation metric. \n\nSince this an **nbalanced multi-class classification** problem, I will be using the [**F1 score**](https://en.wikipedia.org/wiki/F1_score) with **weighted** average: the F1 score is computed for each class then we take the weighted average using the true classes count.\n\nCheck the sklearn documentation for more details [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). In what follows, I have provied two examples of usage of the `f1_score`(unblanaced and balanced classes). "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3033e21098b74dc0574da218ad5295fadba786aa"},"cell_type":"code","source":"# The three variations of the F1 score for unbalanced classes are different for unblanaced classes\n\ntrue_classes = [\"a\", \"b\", \"c\", \"a\", \"c\", \"c\"]\npredicted_classes = [\"a\", \"b\", \"c\", \"c\", \"c\", \"c\"]\n\nprint(\"Unbalanced: \")\nprint(\"Weighted F1 score:\", f1_score(true_classes, predicted_classes, average=\"weighted\"))\nprint(\"Micro F1 score:\", f1_score(true_classes, predicted_classes, average=\"micro\"))\nprint(\"Macro F1 score:\", f1_score(true_classes, predicted_classes, average=\"macro\"))\n\n# The three variations of the F1 score for balanced classes are the same\n\ntrue_classes = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\npredicted_classes = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\nprint(32 * \"-\")\n\nprint(\"Balanaced: \")\nprint(\"Weighted F1 score:\", f1_score(true_classes, predicted_classes, average=\"weighted\"))\nprint(\"Micro F1 score:\", f1_score(true_classes, predicted_classes, average=\"micro\"))\nprint(\"Macro F1 score:\", f1_score(true_classes, predicted_classes, average=\"macro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcc84ce1a7e08c053705d911bac47fa15b505bee"},"cell_type":"markdown","source":"# Baseline model"},{"metadata":{"_uuid":"f3817262de6d9f901828150c54eafa65269b5746"},"cell_type":"markdown","source":"As with any ML problem, one usually starts by establishing a baseline, i.e. a score/error that one aims at improving. \nWhy is that important? Well, without a baseline, it is hard to tell if one is making progress or not. Moreover, some problems are much easier than others: a very high accuracy might look impressive\nbut is less impressive one compared to a high accuracy obtained with a very simple model. \n\nAs a baseline, let's use a linear regression model.\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e7ab6572925b79ef1329621bf118d95c9b206f4b"},"cell_type":"code","source":"train_features_df = train_df.drop(ENCODED_TARGET_COL, axis=1)\ntrain_target_s = train_df[ENCODED_TARGET_COL]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21bb266298cea18b17425736d93e3061c39d7bee"},"cell_type":"code","source":"def improvement_in_percent(model_score, baseline_score):\n    return (100 * (model_score - baseline_score)  / baseline_score).round(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec0fa69042d0fa92d39c84f1ddd0884ef8092753","scrolled":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=SEED)\nlr_scores = cross_val_score(lr, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Logistic regression mean and std scores are: ({}, {})\".format(lr_scores.mean(), lr_scores.std()))\nlr.fit(train_features_df, train_target_s)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25acae48963a893d7d8aea927cc951dba7dfdedc"},"cell_type":"markdown","source":"# Simple XGBoost"},{"metadata":{"_uuid":"c3572879b5172e250ca84e7334b940c5cc9775b8"},"cell_type":"markdown","source":"Now that a baseline score has been found, let's try to improve it. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0ad3ce271714829ccaf5e56717f1e791cbac46d4"},"cell_type":"code","source":"xgb_clf = XGBClassifier(random_state=SEED)\nxgb_clf_scores = cross_val_score(xgb_clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\n\"Simple XGBoost classification mean and std scores are: ({}, {})\".format(xgb_clf_scores.mean(), xgb_clf_scores.std())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f31c30145b2df1ae267770306c8664a68211d9bd"},"cell_type":"markdown","source":"A \"simple\" (no hyperparameters tuning) XGBoost classifier does better than the baseline. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d8eda4e37f5cc43dc3d6973097cc6e48fb00fb4f"},"cell_type":"code","source":"\"This is a {} % improvement\".format(improvement_in_percent(xgb_clf_scores.mean(), lr_scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64399b6a932f1224866d219d4bfe61704ec1fc76"},"cell_type":"markdown","source":"Could we do better?"},{"metadata":{"_uuid":"6609edbac0dae06cecdea7654b9e17ee9fedf9aa"},"cell_type":"markdown","source":"## Tuning the XGBoost classifier"},{"metadata":{"_uuid":"039e795d90cfaabda73155e285c319604bf9a194"},"cell_type":"markdown","source":"Let's try to vary the hyperparamters for the XGBoost classifier and see what we get."},{"metadata":{"trusted":true,"_uuid":"19dac67f0b45c4dfc7d5a5cc9db854dc6aae1cb3"},"cell_type":"code","source":"# More trees\nclf = XGBClassifier(random_state=SEED, n_estimators=1000)\nclf_scores = cross_val_score(clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Alternative XGBoost classification mean and std scores are: ({}, {})\".format(clf_scores.mean(), clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(clf_scores.mean(), lr_scores.mean())))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e37e3a4095c3e2e05f34e26c5c6722c812c6029","trusted":true,"scrolled":true},"cell_type":"code","source":"# Smaller learning rate\nclf = XGBClassifier(random_state=SEED, learning_rate=0.01)\nclf_scores = cross_val_score(clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Alternative XGBoost classification mean and std scores are: ({}, {})\".format(clf_scores.mean(), clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(clf_scores.mean(), lr_scores.mean())))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eeb308e641be3da53112364156e200719a159d4"},"cell_type":"markdown","source":"As you can see, trying different hyperparamters values manually would be tedious. Is there a better way?\nFortunately, there is (at least) one method: using an automatic hyperparameter optimizaton tool. \n    \nOne of these is [**hyperopt**](http://https://github.com/hyperopt/hyperopt)."},{"metadata":{"_uuid":"ea733dcd5bcf9e2228dd8df15d46d1af14f7e474"},"cell_type":"markdown","source":"# Hyperopt + XGboost"},{"metadata":{"trusted":true,"_uuid":"7205b04c04a1cf8fd267cfc530df374957b24c8a","scrolled":false},"cell_type":"code","source":"class HPOptimizer(object):\n\n    def __init__(self):\n        # A progress bar to monitor the hyperopt optimization process\n        self.pbar = tqdm(total=MAX_EVALS, desc=\"Hyperopt\")\n        self.trials = Trials()\n\n    def objective(self, hyperparameters):\n        hyperparameters = {\n            \"max_delta_step\": hyperparameters[\"max_delta_step\"],\n            \"reg_alpha\": '{:.3f}'.format(hyperparameters[\"reg_alpha\"]), \n            \"reg_lambda\": '{:.3f}'.format(hyperparameters[\"reg_lambda\"]), \n            \"n_estimators\": int(hyperparameters[\"n_estimators\"]), \n            \"max_depth\": int(hyperparameters[\"max_depth\"]),\n            \"learning_rate\": '{:.3f}'.format(hyperparameters[\"learning_rate\"]), \n            \"colsample_bytree\": '{:.3f}'.format(hyperparameters['colsample_bytree']),\n            \"gamma\": \"{:.3f}\".format(hyperparameters['gamma']),\n        }\n        print(\"The current hyperparamters are: {}\".format(hyperparameters))\n\n        clf = XGBClassifier(\n            n_jobs=4,\n            **hyperparameters\n        )\n\n        scores = cross_val_score(clf, X=train_features_df, y=train_target_s, cv=5, \n                                 scoring=\"f1_weighted\")\n        print(\"Mean and std CV scores are: ({}, {})\".format(scores.mean(), scores.std()))\n        # Update the progress bar after each iteration\n        self.pbar.update()\n        # Since we are minimizing the objective => return -1 * mean(scores) (this is a loss)\n        return -scores.mean()\n\n    def run(self):\n        if RUN_HP_OPTIMIZATION:\n            optimal_hp = fmin(fn=objective,\n                              space=HP_SPACE,\n                              algo=tpe.suggest,\n                              trials= trials,\n                              max_evals=MAX_EVALS)\n        else:\n            optimal_hp = OPTIMAL_HP\n        self.optimal_hp = optimal_hp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4053be8e3414c2397701c615cca710571c8f0f61"},"cell_type":"code","source":"hp_optimizer = HPOptimizer()\nhp_optimizer.run()\noptimal_hp = hp_optimizer.optimal_hp\nprint(\"The optimal hyperparamters are: {}\".format(optimal_hp))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b376aaba189ddfbfaa108c2da04c7c09b4319d6"},"cell_type":"markdown","source":"# Exploring the hyperopt trials"},{"metadata":{"_uuid":"902b6f829937a70422982692df7d3ac4a238be46"},"cell_type":"markdown","source":"Let's explore the saved trials (these are handy to store hyperopt runs)."},{"metadata":{"trusted":true,"_uuid":"62e8a51186a145236239545adca06df9293c9803"},"cell_type":"code","source":"if RUN_HP_OPTIMIZATION:\n    hyperaramters_df = pd.DataFrame(trials.idxs_vals[1])\n    losses_df = pd.DataFrame(trials.results)\n    hyperopt_trials_df = pd.concat([losses_df, hyperaramters_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaabfa52140208b7166da0d398ac8ace908e76f0","scrolled":false},"cell_type":"code","source":"if RUN_HP_OPTIMIZATION:\n    # Check that the argmin of the hyperopt_trials_df DataFrame is the same as the optimal_hp \n    min_loss_index = losses_df['loss'].argmin()\n    assert (hyperaramters_df.loc[min_loss_index, :].to_dict() == optimal_hp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b52bb58b2a4021c5a647c0614373b189ee16b65"},"cell_type":"code","source":"def hp_vs_loss_scatterplot(hyperparameter):\n\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    hyperopt_trials_df.plot(x=hyperparameter, y='loss', kind='scatter', ax=ax)\n    best_coordinates = hyperopt_trials_df.loc[min_loss_index, [hyperparameter, \"loss\"]].values\n    ax.annotate(\"Best {}: {}\".format(hyperparameter, round(best_coordinates[0], 3)), \n                xy=best_coordinates, \n                color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bb0fc91b05905e1f1612a5a49772ccd8236a28b"},"cell_type":"code","source":"if RUN_HP_OPTIMIZATION:\n    # Remove the \"max_delta_step\" since it is fixed for now\n    HP_SPACE.pop(\"max_delta_step\")\n    for hyperparmeter in HP_SPACE.keys():\n        hp_vs_loss_scatterplot(hyperparmeter)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ba18ee507ce1caedfe3eef2e02c4b7184d24808"},"cell_type":"markdown","source":"# Train tuned XGBoost classifier model on train data and evaluate on test"},{"metadata":{"_uuid":"ea2073274305a8681b7802fe0857983d8064266c"},"cell_type":"markdown","source":"Let's train our best XGBoost classifier (using the optimal hyperparamters) on the train dataet then evaluate it on the test dataset."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"67d86f91492a73bacbb14b1c947edf98c71bcb74"},"cell_type":"code","source":"parsed_optimal_hp = {\n    \"n_estimators\": int(optimal_hp[\"n_estimators\"]), \n    \"max_depth\": int(optimal_hp[\"max_depth\"]),\n    \"learning_rate\": optimal_hp[\"learning_rate\"], \n    \"colsample_bytree\": '{:.3f}'.format(optimal_hp['colsample_bytree']),\n    \"gamma\": \"{:.3f}\".format(optimal_hp['gamma']),\n}\n\nbest_xgb_clf =  XGBClassifier(random_state=SEED, **parsed_optimal_hp)\nbest_xgb_clf.fit(train_features_df, train_target_s)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"754576dc9d9b89105b6c063e1cc7818aed55e331"},"cell_type":"markdown","source":"# Random Forests"},{"metadata":{"_uuid":"be0cc98081182f72a9cf54d1f91348f1aa45e01d"},"cell_type":"markdown","source":"Let's try other models starting with a random forests classifier"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"201a9e508665b8e0047caed002364120d3f58668"},"cell_type":"code","source":"rf_clf = RandomForestClassifier(random_state=SEED)\nrf_clf_scores = cross_val_score(rf_clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Simple random forests classification mean and std scores are: ({}, {})\".format(rf_clf_scores.mean(), rf_clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(rf_clf_scores.mean(), lr_scores.mean())))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abd62b0c9be0b7999e984edd6249ed0f39d1ff12"},"cell_type":"markdown","source":"This isn't very promising for a start. Probably will neeed some hp tuning..."},{"metadata":{"_uuid":"ea3356ceced00b4ccdbc59cb6a8eff864cfe9171"},"cell_type":"markdown","source":"# Neural network"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"788064e32d03c5e8af252960f7eb38f707007bc1"},"cell_type":"code","source":"nn_clf = MLPClassifier(random_state=SEED)\nnn_clf_scores = cross_val_score(nn_clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Simple classification neural network mean and std scores are: ({}, {})\".format(nn_clf_scores.mean(), nn_clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(nn_clf_scores.mean(), lr_scores.mean())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"042256bbdcd08f989775a0395a74a949ab970525"},"cell_type":"code","source":"That's a better start. Let's see if one can improve things. ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c12c4ff8a7718aa2f92e0b7ae9ced592a02165c1"},"cell_type":"markdown","source":"# TPOT"},{"metadata":{"trusted":true,"_uuid":"bb032d2c81c3774da88b6fc519674d8fb79078f6"},"cell_type":"code","source":"TPOTClassifier?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caba862aef6fc1ab92cd406953886bb54cb1c191","scrolled":false},"cell_type":"code","source":"# Previous values: TPOT_GENERATION=15 and TPOT_POPULATION_SIZE=80\nTPOT_GENERATION = 20\nTPOT_POPULATION_SIZE = 100\n# TPOT will have TPOT_POPULATION_SIZE + offspring_size * TPOT_GENERATION runs in total. \n# The offspring_size is set to 100 by default.\n\n\ntpot_clf = TPOTClassifier(generations=TPOT_GENERATION, \n                          population_size=TPOT_POPULATION_SIZE,\n                          random_state=SEED, cv=5, \n                          n_jobs=-1, memory='auto', \n                          early_stop = 10,\n                          verbosity=2, scoring=\"f1_weighted\")\ntpot_clf.fit(train_features_df, train_target_s)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb67f1366754edce4813d5e5941869cb993f168"},"cell_type":"markdown","source":"# Test evaluation"},{"metadata":{"trusted":true,"_uuid":"a97e9a4e99365da798a16f003f3a68efb4e811f8"},"cell_type":"code","source":"test_features_df = test_df.drop(ENCODED_TARGET_COL, axis=1)\nencoded_test_targets_s = test_df[ENCODED_TARGET_COL]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7b1f4c399db0fd7df2532f4b0c69a3d0d5edcec"},"cell_type":"code","source":"def test_evaluation(clf):\n    \"\"\"\n    Evaluate a classifier on the test dataset. Returns a confusion matrix and F1 score. \n    \"\"\"\n    encoded_test_predictions_s = clf.predict(test_features_df)\n    test_predictions_s = pd.Series(le.inverse_transform(encoded_test_predictions_s), \n                                   name=\"predicted_type_1\")\n    test_targets_s = pd.Series(le.inverse_transform(encoded_test_targets_s), \n                               name=\"true_type_1\")\n    test_cm = confusion_matrix(test_targets_s, test_predictions_s)\n    test_f1_score = f1_score(test_targets_s, test_predictions_s, average='weighted').round(3)\n    return test_cm, test_f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c60328fe9059234e77e7567d14a2f29cfb2efb4a"},"cell_type":"code","source":"test_cm_tpot, test_f1_score_tpot = test_evaluation(tpot_clf)\ntest_cm_best_xgb, test_f1_score_best_xgb = test_evaluation(best_xgb_clf)\ntest_cm_lr, test_f1_score_lr = test_evaluation(lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cf849579b28fa1874d4ae5e64481338bdcf5924b"},"cell_type":"code","source":"print(\"Tpot test F1 weighted score is {}\".format(test_f1_score_tpot))\nprint(\"Best XGBoost test F1 weighted score is {}\".format(test_f1_score_best_xgb))\nprint(\"Logistic regression test F1 weighted score is {}\".format(test_f1_score_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"8da751bcde0f3e2a85241733e802b1a872daf854"},"cell_type":"code","source":"# Confusion matrix for Tpot\nplot_confusion_matrix(test_cm_tpot, classes=target_s.unique())\nplot_confusion_matrix(test_cm_tpot, classes=target_s.unique(), normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"439e2cc5bb38d9103b280fd0e4010f778e1afb27"},"cell_type":"markdown","source":"That's impressive. Tpot is by far the winner!"},{"metadata":{"_uuid":"a683daf1bd805d4e6fc29d1121556e4fd23cee29"},"cell_type":"markdown","source":"# Stacking "},{"metadata":{"_uuid":"da7ca649a5d48daf4e6b49bc03557390fb44c0a3"},"cell_type":"markdown","source":"Alright. Let's stack our best models and use an XGBoost as a second-level model. \nTo be continued..."},{"metadata":{"_uuid":"6288353315109845f7526c05fc1f8f421d9e1307"},"cell_type":"markdown","source":"# To wrap up"},{"metadata":{"_uuid":"c2e689e0e888de663a427b851f9504f1c1ddd92f"},"cell_type":"markdown","source":"Some ideas to test: \n\n* More hyperopt iterations and other hypreparamters to optimize. This had the effect of improving the test F1 weighted score. Add more regularization?\n* Use 3 folds CV instead of 5 folds CV. \n* Change the objective to optimize (try something that accounts for the classes' imbalance).\n* Try a neural network.\n* Try random forests.\n* Try stacking.\n* Try TPOT => done\n* Try TPOT with more generations and bigger poupulation size (for now: generations=10, population_size=40)\n\nI hope you have enjoyed this notebook. Stay tuned for updates!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}