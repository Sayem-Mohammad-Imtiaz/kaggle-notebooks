{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Red Wine Quality - Model Comparison","metadata":{}},{"cell_type":"markdown","source":"## Dataset Load","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import log_loss\nfrom math import log\n\n\n\nwine = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\nwine.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T02:06:06.573435Z","iopub.execute_input":"2021-08-20T02:06:06.573955Z","iopub.status.idle":"2021-08-20T02:06:06.607658Z","shell.execute_reply.started":"2021-08-20T02:06:06.573923Z","shell.execute_reply":"2021-08-20T02:06:06.606961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analyisis","metadata":{}},{"cell_type":"code","source":"wine.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.608745Z","iopub.execute_input":"2021-08-20T02:06:06.609137Z","iopub.status.idle":"2021-08-20T02:06:06.616357Z","shell.execute_reply.started":"2021-08-20T02:06:06.609108Z","shell.execute_reply":"2021-08-20T02:06:06.615706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.617518Z","iopub.execute_input":"2021-08-20T02:06:06.617909Z","iopub.status.idle":"2021-08-20T02:06:06.629648Z","shell.execute_reply.started":"2021-08-20T02:06:06.61788Z","shell.execute_reply":"2021-08-20T02:06:06.62899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.646104Z","iopub.execute_input":"2021-08-20T02:06:06.646577Z","iopub.status.idle":"2021-08-20T02:06:06.692386Z","shell.execute_reply.started":"2021-08-20T02:06:06.646545Z","shell.execute_reply":"2021-08-20T02:06:06.691347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(\n    rows = 4,\n    cols = 3,\n    subplot_titles = wine.columns\n)\n\ni = 1\nj = 1\n\nfor col in wine.columns:\n    fig.append_trace(go.Box(y=wine[:][col]), i, j)\n    if j < 3:\n        j += 1\n    else:\n        j = 1\n        i += 1\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=2000,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.69428Z","iopub.execute_input":"2021-08-20T02:06:06.694712Z","iopub.status.idle":"2021-08-20T02:06:06.892919Z","shell.execute_reply.started":"2021-08-20T02:06:06.694666Z","shell.execute_reply":"2021-08-20T02:06:06.891905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = wine.corr()\ncorr.style.background_gradient(cmap='plasma').set_precision(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.895184Z","iopub.execute_input":"2021-08-20T02:06:06.895584Z","iopub.status.idle":"2021-08-20T02:06:06.949979Z","shell.execute_reply.started":"2021-08-20T02:06:06.895545Z","shell.execute_reply":"2021-08-20T02:06:06.949123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.abs(corr[\"quality\"]).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.951409Z","iopub.execute_input":"2021-08-20T02:06:06.951686Z","iopub.status.idle":"2021-08-20T02:06:06.96017Z","shell.execute_reply.started":"2021-08-20T02:06:06.95166Z","shell.execute_reply":"2021-08-20T02:06:06.959233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling","metadata":{}},{"cell_type":"code","source":"# List of columns that need to be scaled:\nneed_scaling = [\"fixed acidity\", \"residual sugar\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"alcohol\"]\nscaler = RobustScaler()\nwine[need_scaling] = scaler.fit_transform(wine[need_scaling], wine[\"quality\"]);","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.961754Z","iopub.execute_input":"2021-08-20T02:06:06.962187Z","iopub.status.idle":"2021-08-20T02:06:06.983599Z","shell.execute_reply.started":"2021-08-20T02:06:06.962139Z","shell.execute_reply":"2021-08-20T02:06:06.982481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outlier removal","metadata":{}},{"cell_type":"code","source":"z_scores = zscore(wine)\nabs_z_scores = np.abs(z_scores)\nwine = wine[(abs_z_scores <= 1.5).all(axis=1)]\n\"\"\"\nwine[\"residual sugar\"] = wine[\"residual sugar\"].drop(wine[\n    (wine[\"residual sugar\"] > 2)\n].index)\nwine[\"chlorides\"] = wine[\"chlorides\"].drop(wine[\n    (wine[\"chlorides\"] > 0.12)\n].index)\n\"\"\"\nwine.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:06.984833Z","iopub.execute_input":"2021-08-20T02:06:06.985107Z","iopub.status.idle":"2021-08-20T02:06:07.03492Z","shell.execute_reply.started":"2021-08-20T02:06:06.985081Z","shell.execute_reply":"2021-08-20T02:06:07.034103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(\n    rows = 4,\n    cols = 3,\n    subplot_titles = wine.columns\n)\n\ni = 1\nj = 1\n\nfor col in wine.columns:\n    fig.append_trace(go.Box(y=wine[:][col]), i, j)\n    if j < 3:\n        j += 1\n    else:\n        j = 1\n        i += 1\nfig.update_layout(\n    autosize=False,\n    width=1200,\n    height=2000,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:07.036751Z","iopub.execute_input":"2021-08-20T02:06:07.03714Z","iopub.status.idle":"2021-08-20T02:06:07.2148Z","shell.execute_reply.started":"2021-08-20T02:06:07.037112Z","shell.execute_reply":"2021-08-20T02:06:07.214078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histograms","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=(wine.shape[1]//3)+1, cols=3)\n\nfor i, col in enumerate(wine.columns):\n    if col == 'quality':\n        fig.add_trace(go.Histogram(x=wine[col], name=col, nbinsx=3),row=(i//3)+1, col=(i%3)+1)\n    else:\n        fig.add_trace(go.Histogram(x=wine[col], name=col), row=(i//3)+1, col=(i%3)+1)\n    \nfig.update_layout(height=1500,)\n    \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:07.216146Z","iopub.execute_input":"2021-08-20T02:06:07.216555Z","iopub.status.idle":"2021-08-20T02:06:07.453207Z","shell.execute_reply.started":"2021-08-20T02:06:07.216525Z","shell.execute_reply":"2021-08-20T02:06:07.452235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 8))\n\nax1.scatter(wine[\"alcohol\"], wine[\"quality\"], marker=\".\")\nax1.set_title(\"Acohol vs Quality\")\nax2.scatter(wine[\"volatile acidity\"], wine[\"quality\"], marker=\".\")\nax2.set_title(\"Volatile Acidity vs Quality\")\nax3.scatter(wine[\"sulphates\"], wine[\"quality\"], marker=\".\")\nax3.set_title(\"Sulphates vs Quality\")\nax4.scatter(wine[\"citric acid\"], wine[\"quality\"], marker=\".\")\nax4.set_title(\"Citric Acid vs Quality\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:07.454368Z","iopub.execute_input":"2021-08-20T02:06:07.454689Z","iopub.status.idle":"2021-08-20T02:06:08.144977Z","shell.execute_reply.started":"2021-08-20T02:06:07.45466Z","shell.execute_reply":"2021-08-20T02:06:08.143993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Alcohol vs Quality Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### Train/Test Dataset Split","metadata":{}},{"cell_type":"code","source":"X = wine[['alcohol']]\nY = wine['quality']\nwine_sample = wine.sample(500, random_state=0)\nwine_sample = wine_sample.sort_values(by='alcohol')\nX_val = wine_sample[['alcohol']]\nY_val = wine_sample['quality']","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.146555Z","iopub.execute_input":"2021-08-20T02:06:08.146998Z","iopub.status.idle":"2021-08-20T02:06:08.157151Z","shell.execute_reply.started":"2021-08-20T02:06:08.146953Z","shell.execute_reply":"2021-08-20T02:06:08.156149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42)\n\nmodel = LinearRegression().fit(X, Y)\n\n\nmodel_l1 = Lasso(alpha=1).fit(X, Y)\n\nmodel_l2 = Ridge(alpha=1).fit(X, Y)\n\n\nerror_metrics = pd.DataFrame(columns=['CONFIG','RMSE', 'MAE'])\n\n# model without regularization\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'train',\n    'RMSE': mean_squared_error(Y, model.predict(X), squared=True),\n    'MAE': mean_absolute_error(Y, model.predict(X))\n}, ignore_index=True)\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'validation',\n    'RMSE': mean_squared_error(Y_val, model.predict(X_val), squared=True),\n    'MAE': mean_absolute_error(Y_val, model.predict(X_val))\n}, ignore_index=True)\n\n# model with regularization L1\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'train (L1)',\n    'RMSE': mean_squared_error(Y, model_l1.predict(X), squared=True),\n    'MAE': mean_absolute_error(Y, model_l1.predict(X))\n}, ignore_index=True)\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'validation (L1)',\n    'RMSE': mean_squared_error(Y_val, model_l1.predict(X_val), squared=True),\n    'MAE': mean_absolute_error(Y_val, model_l1.predict(X_val))\n}, ignore_index=True)\n\n# model with regularization L2\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'train (L2)',\n    'RMSE': mean_squared_error(Y, model_l2.predict(X), squared=True),\n    'MAE': mean_absolute_error(Y, model_l2.predict(X))\n}, ignore_index=True)\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'validation (L2)',\n    'RMSE': mean_squared_error(Y_val, model_l2.predict(X_val), squared=True),\n    'MAE': mean_absolute_error(Y_val, model_l2.predict(X_val))\n}, ignore_index=True)\n\nerror_metrics\n\nfig = make_subplots(rows=1, cols=2)\n\n# draw models\nfig.add_trace(go.Scatter(x=X.iloc[:,0], y=Y, mode='markers', name='train'), row=1, col=1)\nfig.add_trace(go.Scatter(x=X_val.iloc[:,0], y=Y_val, mode='markers', name='validation'), row=1, col=1)\n\nmodel_names = [\"model\", \"model L1\", \"model L2\"]\nfor i, model in enumerate([model, model_l1, model_l2]):\n    fig.add_trace(go.Scatter(\n        x=X.iloc[:,0], y=model.predict(X), mode='lines', name=model_names[i], line_shape='spline'\n    ), row=1, col=1)\n\nfig.update_yaxes(title_text=\"quality\", row=1, col=1)\nfig.update_xaxes(title_text=\"alcohol\", row=1, col=1)\n\n# draw errors\n\nfor index, row in error_metrics.iterrows():\n    fig.add_trace(go.Bar(name=row['CONFIG'], x=[\"RMSE\", \"MAE\"], y=row[['RMSE','MAE']]), row=1, col=2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.158475Z","iopub.execute_input":"2021-08-20T02:06:08.158864Z","iopub.status.idle":"2021-08-20T02:06:08.288329Z","shell.execute_reply.started":"2021-08-20T02:06:08.158833Z","shell.execute_reply":"2021-08-20T02:06:08.287602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BIC and AIC for Linear Regression","metadata":{}},{"cell_type":"code","source":"from math import log\n\ndef calculate_aic(n, mse, num_params):\n    '''calculate aic for linear regression'''\n    aic = n * log(mse) + 2 * num_params\n    return aic# ****# # # AIC/BIC LINEAR REGRESSION\n\nnum_params = len(model.coef_) + 1\nprint('Number of parameters: %d' % (num_params))\n\nyhat = model.predict(X)\n\nmse = mean_squared_error(Y, model.predict(X))\nprint('MSE: %.3f' % mse)\n\naic = calculate_aic(len(Y), mse, num_params)\nprint('AIC: %.3f' % aic)\n\n\ndef calculate_bic(n, logloss, num_params):\n    '''calculate aic for logistic regression'''\n    bic = -2 * log(logloss) + log(n) * num_params\n    return bic\n\nbic = calculate_bic(len(Y), mse, num_params)\nprint('BIC: %.3f' % bic)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.289268Z","iopub.execute_input":"2021-08-20T02:06:08.289548Z","iopub.status.idle":"2021-08-20T02:06:08.304072Z","shell.execute_reply.started":"2021-08-20T02:06:08.289521Z","shell.execute_reply":"2021-08-20T02:06:08.303015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multiple Linear Regression","metadata":{}},{"cell_type":"code","source":"X = wine[['alcohol', 'volatile acidity', 'sulphates']].copy()\nY = wine['quality']\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42)\n\nlrmodel_l1 = Lasso(alpha=1, max_iter=10000, tol=1e-5).fit(X_train, y_train)\n\nlogreg_model = LogisticRegression(\n    class_weight='balanced',\n    solver='newton-cg',\n    max_iter=1000,\n    C=0.001\n)\nlogreg_model.fit(X_train, y_train)\n\nerror_metrics = pd.DataFrame(columns=['CONFIG','RMSE', 'MAE'])\n\n# model without regularization\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'train',\n    'RMSE': mean_squared_error(Y, logreg_model.predict(X), squared=True),\n    'MAE': mean_absolute_error(Y, logreg_model.predict(X))\n}, ignore_index=True)\n\n\n# model with regularization L1\n\nerror_metrics = error_metrics.append({\n    'CONFIG': 'train (L1)',\n    'RMSE': mean_squared_error(Y, lrmodel_l1.predict(X), squared=True),\n    'MAE': mean_absolute_error(Y, lrmodel_l1.predict(X))\n}, ignore_index=True)\n\n\nerror_metrics\n\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'is_3d': True}, {'is_3d': False}]])\n\n# draw models\nfig.add_trace(go.Scatter3d(\n    x=X.iloc[:,0],\n    y=X.iloc[:,1],\n    z=Y,\n    mode='markers',\n    \n), row=1, col=1)\n\n\n# prediction surface\n\nX1 = np.array([X.iloc[:,0].min(), X.iloc[:,0].min(), X.iloc[:,0].max(), X.iloc[:,0].max()])\nX2 = np.array([X.iloc[:,1].min(), X.iloc[:,1].max(), X.iloc[:,1].min(), X.iloc[:,1].max()])\nX3 = np.array([X.iloc[:,2].mean(), X.iloc[:,2].mean(), X.iloc[:,2].mean(), X.iloc[:,2].mean()])\nZ = logreg_model.predict(np.column_stack([X1, X2, X3]))\n\n\nfig.add_trace(go.Mesh3d(\n    x=X1,\n    y=X2,\n    z=Z,\n    opacity=0.8,\n    name=\"model\"\n),row=1, col=1)\n\n# draw errors\n\nfor index, row in error_metrics.iterrows():\n    fig.add_trace(go.Bar(name=row['CONFIG'], x=[\"RMSE\", \"MAE\"], y=row[['RMSE','MAE']]), row=1, col=2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.305486Z","iopub.execute_input":"2021-08-20T02:06:08.305799Z","iopub.status.idle":"2021-08-20T02:06:08.378738Z","shell.execute_reply.started":"2021-08-20T02:06:08.305773Z","shell.execute_reply":"2021-08-20T02:06:08.377744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AIC and BIC for Multiple Linear Regression","metadata":{}},{"cell_type":"code","source":"def calculate_aic(n, logloss, num_params):\n    '''calculate aic for logistic regression'''\n    aic = (-2/n) * log(logloss) + (2 * (num_params/n))\n    return aic\n\n# numero de parametros\nnum_params = len(logreg_model.coef_) + 1\nprint('Number of parameters: %d' % (num_params))\n\n# predicciones\nyhat = logreg_model.predict_proba(X)\n\n# calcular el mean squared error\nlogloss = log_loss(Y, yhat)\nprint('log_loss: %.3f' % logloss)\n\n# calcular el AIC\naic = calculate_aic(len(Y), logloss, num_params)\nprint('AIC: %.3f' % aic)\n\n# calcular el mean squared error\nmse = mean_squared_error(Y, logreg_model.predict(X))\nprint('MSE: %.3f' % mse)\n\n# calcular el BIC\n\nfrom math import log\n\ndef calculate_bic(n, logloss, num_params):\n    '''calculate aic for logistic regression'''\n    bic = -2 * log(logloss) + log(n) * num_params\n    return bic\nbic = calculate_bic(len(Y), mse, num_params)\nprint('BIC: %.3f' % bic)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.379946Z","iopub.execute_input":"2021-08-20T02:06:08.380216Z","iopub.status.idle":"2021-08-20T02:06:08.398567Z","shell.execute_reply.started":"2021-08-20T02:06:08.38019Z","shell.execute_reply":"2021-08-20T02:06:08.397559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"wine.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.400715Z","iopub.execute_input":"2021-08-20T02:06:08.401193Z","iopub.status.idle":"2021-08-20T02:06:08.428148Z","shell.execute_reply.started":"2021-08-20T02:06:08.40115Z","shell.execute_reply":"2021-08-20T02:06:08.427133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train\nY = y_train","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.430754Z","iopub.execute_input":"2021-08-20T02:06:08.43119Z","iopub.status.idle":"2021-08-20T02:06:08.441279Z","shell.execute_reply.started":"2021-08-20T02:06:08.431144Z","shell.execute_reply":"2021-08-20T02:06:08.440339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:08.443665Z","iopub.execute_input":"2021-08-20T02:06:08.444161Z","iopub.status.idle":"2021-08-20T02:06:08.459562Z","shell.execute_reply.started":"2021-08-20T02:06:08.444113Z","shell.execute_reply":"2021-08-20T02:06:08.458554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X_train\nY = y_train\n\nlogrmodel = LogisticRegression(random_state=0,solver='liblinear').fit(X, Y)\n\nlogrmodel_l1 = LogisticRegression(random_state=0, solver='liblinear', penalty='l1', C=1).fit(X, Y)\n\nlogrmodel_l2 = LogisticRegression(random_state=0,solver='liblinear', penalty='l2', C=1).fit(X, Y)\n\nfig = make_subplots(rows=1, cols=2)\n\n# draw models\nfig.add_trace(go.Scatter(x=X['alcohol'], y=Y, mode='markers', name='train'), row=1, col=1)\n#fig.add_trace(go.Scatter(x=X_val[:,0], y=Y_val, mode='markers', name='validation'), row=1, col=1)\n\nmodel_names = [\"model\", \"model L1\", \"model L2\"]\nfor i, model in enumerate([logrmodel, logrmodel_l1, logrmodel_l2]):\n    fig.add_trace(go.Scatter(\n        x=X['alcohol'], y=model.predict(X), mode='lines', name=model_names[i], line_shape='spline'\n    ), row=1, col=1)\n\nfig.update_yaxes(title_text=\"Quality\", row=1, col=1)\nfig.update_xaxes(title_text=\"Alcohol\", row=1, col=1)\n\n# draw errors\n\n\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:06:38.231818Z","iopub.execute_input":"2021-08-20T02:06:38.232245Z","iopub.status.idle":"2021-08-20T02:06:38.305513Z","shell.execute_reply.started":"2021-08-20T02:06:38.232214Z","shell.execute_reply":"2021-08-20T02:06:38.30434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AIC and BIC for Logistic Regression","metadata":{}},{"cell_type":"code","source":"# numero de parametros\nnum_params = len(logrmodel.coef_) + 1\nprint('Number of parameters: %d' % (num_params))\n\n# predicciones\nyhat = logrmodel.predict_proba(X)\n\n# calcular el mean squared error\nlogloss = log_loss(Y, yhat)\nprint('log_loss: %.3f' % logloss)\n\n# calcular el AIC\naic = calculate_aic(len(Y), logloss, num_params)\nprint('AIC: %.3f' % aic)\n\n# calcular el BIC\ndef calculate_bic(n, logloss, num_params):\n    '''calculate aic for logistic regression'''\n    bic = -2 * log(logloss) + log(n) * num_params\n    return bic\nbic = calculate_bic(len(Y), logloss, num_params)\nprint('BIC: %.3f' % bic)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:08:09.118201Z","iopub.execute_input":"2021-08-20T02:08:09.118636Z","iopub.status.idle":"2021-08-20T02:08:09.131883Z","shell.execute_reply.started":"2021-08-20T02:08:09.118593Z","shell.execute_reply":"2021-08-20T02:08:09.131068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"acc_train = logreg_model.score(X_train, y_train)\nacc_test = logreg_model.score(X_test, y_test)\n\nprint(f\"accuracy: {acc_train}, accuracy (test): {acc_test}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T02:09:04.745916Z","iopub.execute_input":"2021-08-20T02:09:04.746312Z","iopub.status.idle":"2021-08-20T02:09:04.758118Z","shell.execute_reply.started":"2021-08-20T02:09:04.746281Z","shell.execute_reply":"2021-08-20T02:09:04.757071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"**The best model is simple Linear Regression because it has the lowest AIC score.**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}