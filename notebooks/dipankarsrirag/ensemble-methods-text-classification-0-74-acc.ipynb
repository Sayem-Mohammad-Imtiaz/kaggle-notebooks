{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Headline Classification using Stacking Ensemble method"},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries needed"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#------------------------------------------Libraries---------------------------------------------------------------#\n####################################################################################################################\n#-------------------------------------Boiler Plate Imports---------------------------------------------------------#\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#---------------------------------------Text Processing------------------------------------------------------------#\nimport regex\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\n#------------------------------------Metrics and Validation---------------------------------------------------------#\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n#-------------------------------------Models to be trained----------------------------------------------------------#\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost\n#####################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/ireland-historical-news/ireland-news-headlines.csv')\ntest = pd.read_csv('/kaggle/input/ireland-historical-news/w3-latnigrin-tokens.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Peek Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.headline_category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We could clearly make out that for every category there are multiple sub-categories which only increases the complexity of our predictions and increases variance as the sub-categories are less in number. \n\n* One way to tackle this, is to merge the sub-categories into their respective 'super class'. This wouldn't effect the predictions much as the sub-categories are clearly out-numbered."},{"metadata":{"trusted":true},"cell_type":"code","source":"year = [] \nmonth = [] \nday = [] \n\ndates = train.publish_date.values\n\nfor date in dates:\n    str_date = list(str(date))\n    year.append(int(\"\".join(str_date[0:4]))) \n    month.append(int(\"\".join(str_date[4:6])))\n    day.append(int(\"\".join(str_date[6:8])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = year\ntrain['month'] = month\ntrain['day'] = day\n\ntrain.drop(['publish_date'] , axis=1,inplace=True) \ntrain = train[train['headline_category'] != 'removed']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Unique Headlines Categories: {}'.format(len(train.headline_category.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set([category for category in train.headline_category if \".\" not in category] ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['headline_category'] = train.headline_category.apply(lambda x: x.split(\".\")[0]) \ntrain = train.loc[train.headline_category != 'removed']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean the textual data\n\n* Normalize the data by converting to lower case.\n* Remove punctuations, stopwords and special characters.\n* Tokenize the text, i.e segment the text into sentences and further into words.\n* Extract the root word by 'Lemmatizing' each word. Ex: Lemmatize('swimming') = 'swim'"},{"metadata":{},"cell_type":"markdown","source":"**Helper Function For Data Preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordnet_lemmatizer = WordNetLemmatizer()\n\nstop = stopwords.words('english')\n\nfor punct in punctuation:\n    stop.append(punct)\n\ndef filter_text(text, stop_words):\n    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha()]\n    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n    return \" \".join(filtered_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"filtered_text\"] = train.headline_text.apply(lambda x : filter_text(x, stop)) \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Training Data"},{"metadata":{},"cell_type":"markdown","source":"### Basic Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.countplot(train.headline_category, palette = sns.color_palette(\"mako\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=train.year.value_counts().index.values,y=train.year.value_counts().values, color = 'seagreen')\nax = plt.title('Number of Published News by Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=train.month.value_counts().index.values,y=train.month.value_counts().values, color = 'seagreen')\nax = plt.title('Number of Published News by Month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.lineplot(x=train.day.value_counts().index.values,y=train.day.value_counts().values, color = 'seagreen')\nax = plt.title('Number of Published News by Day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Clouds\n\n**Helper Function for word clouds**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_wordcloud(words,title):\n    cloud = WordCloud(width=1920, height=1080,max_font_size=200, max_words=300, background_color=\"white\").generate(words)\n    plt.figure(figsize=(20,20))\n    plt.imshow(cloud, interpolation=\"gaussian\")\n    plt.axis(\"off\") \n    plt.title(title, fontsize=60)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = \" \".join(train[train.headline_category == \"news\"].filtered_text) \nmake_wordcloud(all_text, \"News\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = \" \".join(train[train.headline_category == \"culture\"].filtered_text) \nmake_wordcloud(all_text, \"Culture\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = \" \".join(train[train.headline_category == \"opinion\"].filtered_text) \nmake_wordcloud(all_text, \"Opinion\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = \" \".join(train[train.headline_category == \"business\"].filtered_text) \nmake_wordcloud(all_text, \"Business\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = \" \".join(train[train.headline_category == \"sport\"].filtered_text) \nmake_wordcloud(all_text, \"Sport\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = \" \".join(train[train.headline_category == \"lifestyle\"].filtered_text) \nmake_wordcloud(all_text, \"Lifestyle\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorizing Cleaned Text For Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(lowercase=False)\ntrain_vec = tfidf.fit_transform(train['filtered_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vec.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['classification'] = train['headline_category'].replace(['news','culture','opinion','business','sport','lifestyle'],[0,1,2,3,4,5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(train_vec,train['classification'], stratify=train['classification'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\n\n### Base Models\n\n* Logistic Regression with RandomizedSearchCV.\n* Multinomial Naive Bayes with RandomizedSearchCV.\n* SDGClassifier with Squared Hinge Loss -> SVM.\n\n### Ensemble Methods\n\n* Hard Voting Classifier.\n* Stacking Classifier with XGBClassifier as the \"final estimator\" or \"meta learner\"."},{"metadata":{},"cell_type":"markdown","source":"## Base Models\n\n### Logistic Regression with RandomizedSearchCV\n\nCross-Validation takes a lot of time, typically 1hr 30min (due to the size of dataset) on an 8 core CPU for Logistic Regression.<br>\nThe parameters used were obtained by already performing Cross Validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#C = np.arange(0, 1, 0.001)\n#l1_ratio = np.ratio(0, 1, 0.01)\n#max_iter = range(100, 500)\n#warm_start = [True, False]\n#solver = ['lbfgs', 'newton-cg']\n#penalty = ['l2', 'l1']\n\n#params = {\n#    'C' : C,\n#    'l1_ratio' : l1_ratio,\n#    'max_iter' : max_iter,\n#    'warm_start' : warm_start,\n#    'solver' : solver,\n#    'penalty' : penalty\n#}\n#\n#random_search = RandomizedSearchCV(\n#    estimator = LogisticRegression(),\n#    param_distributions = params,\n#    n_iter = 100,\n#    cv = 3,\n#    n_jobs = -1,\n#    random_state = 1\n#).fit(x_train, y_train)\n#\n#random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr = LogisticRegression(\n    C=0.98, \n    l1_ratio=0.23, \n    max_iter=430, \n    random_state=1,\n    warm_start=True\n).fit(x_train, y_train)\n\nmodel_lr.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model_lr.predict(x_val)\n\nlr_acc = accuracy_score(y_val,predicted)\nlr_cop = cohen_kappa_score(y_val,predicted)\nlr = pd.DataFrame([lr_acc, lr_cop], columns = ['Logistic Regression with RandomizedSearchCV'])\n\nprint(\"Test score: {:.2f}\".format(lr_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(lr_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multinomial Naive Bayes with RandomizedSearchCV\nCross-Validation takes typically 10min (due to the size of dataset) on an 8 core CPU for Multinomial Naive Bayes.<br>\nThe parameters used were obtained by already performing Cross Validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#alpha = np.arange(0, 1, 0.001)\n#fit_prior = [True, False]\n\n#params = {\n#    'alpha' : alpha,\n#    'fit_prior' : fit_prior\n#}\n#\n#random_search = RandomizedSearchCV(\n#    estimator = MultinomialNB(),\n#    param_distributions = params,\n#    n_iter = 100,\n#    cv = 3,\n#    n_jobs = -1,\n#    random_state = 1\n#).fit(x_train, y_train)\n#\n#random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_mnb = MultinomialNB(alpha=1.9000000000000001, fit_prior=False).fit(x_train, y_train)\n\nmodel_mnb.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model_mnb.predict(x_val)\n\nmnb_acc = accuracy_score(y_val,predicted)\nmnb_cop = cohen_kappa_score(y_val,predicted)\nmnb = pd.DataFrame([mnb_acc, mnb_cop], columns = ['MultinomialNB with RandomizedSearchCV'])\n\nprint(\"Test score: {:.2f}\".format(mnb_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(mnb_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGDClassifier with Squared Hinge Loss\n* Support Vector Classifiers can be implemented by using the SGDClassifier with **'hinge'** as the loss function.<br><br>\n* Since the **'squared hinge loss'** removes the possibility of negative class in the range, makes it the optimal choice for the loss function.<br><br>\n* Cross Validation **failed** to yeild better results than the base model defined, hence skipped."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sgd_hinge = SGDClassifier(\n    loss='squared_hinge',\n    penalty='l2',\n    alpha=0.0001,\n    l1_ratio=0.15,\n    fit_intercept=True,\n    max_iter=1000,\n    tol=0.001,\n    shuffle=True,\n    verbose=0,\n    epsilon=0.1,\n    n_jobs=-1,\n    random_state=1,\n    learning_rate='optimal',\n    eta0=0.0,\n    power_t=0.5,\n    early_stopping=False,\n    validation_fraction=0.1,\n    n_iter_no_change=5,\n    class_weight=None,\n    warm_start=False,\n    average=False).fit(x_train, y_train)\n\nmodel_sgd_hinge.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model_sgd_hinge.predict(x_val)\n\nsgd_hinge_acc = accuracy_score(y_val,predicted)\nsgd_hinge_cop = cohen_kappa_score(y_val,predicted)\nsgd_hinge = pd.DataFrame([sgd_hinge_acc, sgd_hinge_cop], columns = ['SGDClassifier with Squared Hinge Loss'])\n\nprint(\"Test score: {:.2f}\".format(sgd_hinge_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(sgd_hinge_cop))\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [\n    ('svm', model_sgd_hinge),\n    ('mnb', model_mnb),\n    ('lr', model_lr)\n]\n\nestimators","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hard Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_voting = VotingClassifier(\n    estimators = estimators,\n    voting='hard', \n    n_jobs=-1,\n    flatten_transform=True, \n    verbose=1).fit(x_train, y_train)\n\nmodel_voting.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model_voting.predict(x_val)\n\nvoting_acc = accuracy_score(y_val,predicted)\nvoting_cop = cohen_kappa_score(y_val,predicted)\nvoting = pd.DataFrame([voting_acc, voting_cop], columns = ['Hard Voting Classifier'])\n\nprint(\"Test score: {:.2f}\".format(voting_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(voting_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgc = xgboost.XGBClassifier()\n\nmodel_stack = StackingClassifier(\n    estimators=estimators,\n    final_estimator=xgc,\n    n_jobs = -1,\n    verbose = 1\n)\n\nmodel_stack.fit(x_train, y_train)\n\nmodel_stack.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model_stack.predict(x_val)\n\nstack_acc = accuracy_score(y_val,predicted)\nstack_cop = cohen_kappa_score(y_val,predicted)\nstack = pd.DataFrame([stack_acc, stack_cop], columns = ['Stacking Classifier'])\n\nprint(\"Test score: {:.2f}\".format(stack_acc))\nprint(\"Cohen Kappa score: {:.2f}\".format(stack_cop))\n\nplt.figure(figsize=(15,10))\nax = sns.heatmap(confusion_matrix(y_val,predicted),annot=True)\nax = ax.set(xlabel='Predicted',ylabel='True',title='Confusion Matrix',\n            xticklabels=(['news','culture','opinion','business','sport','lifestyle']),\n            yticklabels=(['news','culture','opinion','business','sport','lifestyle']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Comparision"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_comp = pd.concat([lr, mnb, sgd_hinge, voting, stack], axis = 1)\nmodel_comp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict the Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vec = tfidf.transform(test.iloc[:, -1])\ntest_vec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = ['news','culture','opinion','business','sport','lifestyle']\ncode = [0,1,2,3,4,5]\ndic = dict([(code[x], cat[x])for x in range(6)])\ndic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model_stack.predict(test_vec)\n\npredictions = []\nfor i in pred:\n    predictions.append(dic[i])\ntest['Category'] = predictions\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.countplot(test.Category, palette = sns.color_palette(\"mako\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inferences and Future Works\n\n* **Though Naive Bayes based models are considered to be the go-to models for Text Classification, they couldn't perform well as the linear model because of the size of the dataset. Same goes for SVM, these models are suitable for small to medium sized datasets.**\n\n* **Tree Based Models such as Decision Trees and other ensemble methods such as GradientBoost and AdaBoost could also be used but require much more computational power and proper hyperparameter tuning.**\n\n* **For a huge dataset like this, traditional ML models can prove short. Hence artificial neural network architectures using LSTMs and also CNNs can be proved as a step up. They would also require an extra computational capacity with GPUs.**"},{"metadata":{},"cell_type":"markdown","source":"## Credits : \n\n* [EDA and Data Preparation](https://www.kaggle.com/darkrubiks/classifying-irish-times-headlines-categories)\n* [SVC implemented with SGDClassifier](https://www.kaggle.com/flubber/text-classification-irish-times-news-dataset)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}