{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center><span style = 'color:red'>Sentiment Analysis IMDB 50k Movie Dataset</span></center>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filepath = os.path.join(dirname, filename) \n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # <center><span style = 'color:red'>Sentiment Analysis IMDB 50k Movie Dataset</span></center>"},{"metadata":{},"cell_type":"markdown","source":"### Since the competition is over, I am creating this notebook to help the begineers to learn from this solution and also the experts can provide me suggestions to further improve my solution. Let's start the project by exploring the data. \n\nI have stored the filename and path in 'filepath'. Let's load the data as a pandas dataframe and play with it. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(filepath)\ndf.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset contains 50k rows indexed 0 to 49,999 and two columns. One column contains the review and other contains sentiment. In total there are 25k +ive and 25k -ive sentiments. Let's check if the reviews are randomly distributed or are in some order. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using df.head(), df.tail() we can observe that reviews are randomly distributed thus we don't need to redistribute the data to randomize it. We also observe that data contains (review column): lower case and upper case letters, some html tags within \"< > \", punctuations, apostrophies e.t.c. It would be prudent to clean the data before proceeding but just to show you the difference we will do this project with and without data pre-processing.   "},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF \n\nSince we have reviews as text and we want to run a mathematical model we need a method to convert the text to numbers. We will use TF-IDF method for this, inorder to know the details about TF-IDF you may read about it __[here](https://towardsdatascience.com/sentiment-analysis-introduction-to-naive-bayes-algorithm-96831d77ac91)__. Note that it contains 50k rows so its a mid-sized database so we expect the TF-IDF based model to work better than deep learning models.  \n\nWe will use pre-built TF-IDF vectorizer from sklearn library. Let us create a document term matrix (DTM) using TF-IDF. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\ntext_count_matrix = tfidf.fit_transform(df.review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the complete dataset in test and training dataset:\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(text_count_matrix, df.sentiment, test_size=0.30, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the sentiments (positive and negatives) to 1 and 0. \ny_train = (y_train.replace({'positive': 1, 'negative': 0})).values\ny_test = (y_test.replace({'positive': 1, 'negative': 0})).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's use Naive Bayes classifier and fit our model:\nfrom sklearn.naive_bayes import MultinomialNB \nMNB = MultinomialNB()\nMNB.fit(x_train, y_train)\n#4. Evaluating the model\nfrom sklearn import metrics\naccuracy_score = metrics.accuracy_score(MNB.predict(x_test), y_test)\nprint(\"accuracy_score without data pre-processing = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## With Data Pre-Processing\n\nI am using some steps of data pre-processing, for ex: I am not using lemmatization you can use it and see how the result varies. You may add some more steps to it for example if there is any happy smiley replace it with a positive word like good and a sad smiley with negative word like bad. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's investigate what kind of special characters and language is used by the reviewers to review the content. \n#we can observe some html tags\n#use of parenthesis\n#punctuation (apostrophy, '' e.t.c)\n\nimport re\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words('english'))\nlemmatizer = nltk.stem.WordNetLemmatizer()\n#print(df.review[4])\n\nprocessed_review = []\nsingle_review = \"string to iniialize <br /> my email id is charilie@waoow.com. You can also reach to me at charlie's \"\nreviews = df.review\nfor review in range(0,50000):\n    single_review = df.loc[review,'review']\n    \n    #start processing the single_review \n    \n    #removing html tags:\n    single_review = re.sub('<.*?>',' ',single_review)\n    #removing special characters (punctuation) '@,!' e.t.c.\n    single_review = re.sub('\\W',' ',single_review)\n    #removing single characters\n    single_review = re.sub('\\s+[a-zA-Z]\\s+',' ', single_review)\n    #substituting multiple spaces with single space\n    single_review = re.sub('\\s+',' ', single_review)\n   \n    #removing stop words\n    #word_tokens = []\n    word_tokens = word_tokenize(single_review)\n    #lemmatization\n    #lemmatized_sentence = \" \".join(lemmatizer.lemmatize(token) for token in word_tokens if token not in stop_words)\n    filtered_sentence = []\n    #filtered_sentence.append([w for w in word_tokens if w not in stop_words])\n    filtered_sentence2 = \" \".join([w for w in word_tokens if w not in stop_words])\n    \n    \n    #compile all the sentences to make a complete dictionary of processed reviews\n    processed_review.append(filtered_sentence2)\n    \nprint(processed_review[10])\n#print(filtered_sentence2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_count_matrix2 = tfidf.fit_transform(processed_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(text_count_matrix2, df.sentiment, test_size=0.30, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = (Y_train.replace({'positive': 1, 'negative': 0})).values\nY_test = (Y_test.replace({'positive': 1, 'negative': 0})).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MNB.fit(X_train, Y_train)\n#4. Evaluating the model\naccuracy_score = metrics.accuracy_score(MNB.predict(X_test), Y_test)\nprint(str('{:04.2f}'.format(accuracy_score*100))+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"Classification Report: \\n\", classification_report(Y_test, MNB.predict(X_test),target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, MNB.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Using Linear-SVC with pre-processing and SGDC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSVC = LinearSVC()\nLSVC.fit(X_train, Y_train)\naccuracy_score = metrics.accuracy_score(LSVC.predict(X_test), Y_test)\nprint(\"Linear SVC accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\nprint(\"Classification Report: \\n\", classification_report(Y_test, LSVC.predict(X_test),target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, LSVC.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SGDC = SGDClassifier()\nSGDC.fit(X_train, Y_train)\npredict = SGDC.predict(X_test)\naccuracy_score = metrics.accuracy_score(predict, Y_test)\nprint(\"Stocastic Gradient Classifier accuracy = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\nprint(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(X_train, Y_train)\npredict = LR.predict(X_test)\naccuracy_score = metrics.accuracy_score(predict, Y_test)\nprint(\"LR = \" + str('{:04.2f}'.format(accuracy_score*100))+\" %\")\nprint(\"Classification Report: \\n\", classification_report(Y_test, predict,target_names=['Negative','Positive']))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(Y_test, predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing Different Models\n\nHere we have seen that LSVM with limited data pre-processing brings the best result to us with the accuracy of over 91.10% . This is a significant improvement over NB model. \n\nI am open for your suggestions and feedback. Kindly let me know if you want any elaborate explanantion on any of the steps or have ideas to further improve the model.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}