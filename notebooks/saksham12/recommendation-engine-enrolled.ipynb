{"metadata":{"language_info":{"file_extension":".py","name":"python","version":"3.5.2","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{},"source":"# Movie Recommendation Engine\n## Recommendation engine are Basically of three types\n        Popularity Based-These recommend the most popular items to user and these have applicationssuch as in  News Websites\n        Content Based - These are Based on the Description of the product \n        Collabrative Filtering-These can be of two types User Based and Item Based\n                                     User Based-This is based out of  Similarity between users\n                                     Item Based-This takes into account the  Similarity between items\n                                   ","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.neighbors import NearestNeighbors\nfrom fuzzywuzzy import fuzz"},{"metadata":{},"source":"### Here we change new columns to match with the IMDB dataset ","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"import json\nimport pandas as pd\n#___________________________\ndef load_tmdb_movies(path):\n    df = pd.read_csv(path)\n    df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n    json_columns = ['genres', 'keywords', 'production_countries',\n                    'production_companies', 'spoken_languages']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n#___________________________\ndef load_tmdb_credits(path):\n    df = pd.read_csv(path)\n    json_columns = ['cast', 'crew']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n#___________________\nLOST_COLUMNS = [\n    'actor_1_facebook_likes',\n    'actor_2_facebook_likes',\n    'actor_3_facebook_likes',\n    'aspect_ratio',\n    'cast_total_facebook_likes',\n    'color',\n    'content_rating',\n    'director_facebook_likes',\n    'facenumber_in_poster',\n    'movie_facebook_likes',\n    'movie_imdb_link',\n    'num_critic_for_reviews',\n    'num_user_for_reviews']\n#____________________________________\nTMDB_TO_IMDB_SIMPLE_EQUIVALENCIES = {\n    'budget': 'budget',\n    'genres': 'genres',\n    'revenue': 'gross',\n    'title': 'movie_title',\n    'runtime': 'duration',\n    'original_language': 'language',\n    'keywords': 'plot_keywords',\n    'vote_count': 'num_voted_users'}\n#_____________________________________________________\nIMDB_COLUMNS_TO_REMAP = {'imdb_score': 'vote_average'}\n#_____________________________________________________\ndef safe_access(container, index_values):\n    # return missing value rather than an error upon indexing/key failure\n    result = container\n    try:\n        for idx in index_values:\n            result = result[idx]\n        return result\n    except IndexError or KeyError:\n        return pd.np.nan\n#_____________________________________________________\ndef get_director(crew_data):\n    directors = [x['name'] for x in crew_data if x['job'] == 'Director']\n    return safe_access(directors, [0])\n#_____________________________________________________\ndef pipe_flatten_names(keywords):\n    return '|'.join([x['name'] for x in keywords])\n#_____________________________________________________\ndef convert_to_original_format(movies, credits):\n    tmdb_movies = movies.copy()\n    tmdb_movies.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES, inplace=True)\n    tmdb_movies['title_year'] = pd.to_datetime(tmdb_movies['release_date']).apply(lambda x: x.year)\n    # I'm assuming that the first production country is equivalent, but have not been able to validate this\n    tmdb_movies['country'] = tmdb_movies['production_countries'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['language'] = tmdb_movies['spoken_languages'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['director_name'] = credits['crew'].apply(get_director)\n    tmdb_movies['actor_1_name'] = credits['cast'].apply(lambda x: safe_access(x, [1, 'name']))\n    tmdb_movies['actor_2_name'] = credits['cast'].apply(lambda x: safe_access(x, [2, 'name']))\n    tmdb_movies['actor_3_name'] = credits['cast'].apply(lambda x: safe_access(x, [3, 'name']))\n    tmdb_movies['genres'] = tmdb_movies['genres'].apply(pipe_flatten_names)\n    tmdb_movies['plot_keywords'] = tmdb_movies['plot_keywords'].apply(pipe_flatten_names)\n    return tmdb_movies\n"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"movies=load_tmdb_movies('../input/tmdb_5000_movies.csv')\ncredits=load_tmdb_credits('../input/tmdb_5000_credits.csv')\ndf_initial = convert_to_original_format(movies, credits)"},{"metadata":{},"source":"### Let's Investigate the Null values in Various columns","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column_type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().apply(lambda x:len([w for w in x if w==True]))).T.rename({0:'null_values'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().apply(lambda x:len([w for w in x if w==True])/len(x)*100)).T.rename({0:'%null_values'}))"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"tab_info\n"},{"metadata":{},"source":"###  Keywords are one the most Important Keywords of our Recommendation engine so put in some time in cleaning it.\nWe start By Finding out a list of unique keywords along with their frequency ","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"temp=\"\"\nfor i in df_initial['plot_keywords']:\n    temp=temp+'|'+i\n    \nkeywords=set(temp.split('|'))\nkeywords.remove('')\nlist_keywords=list(keywords)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## function for counting the frequency of unique words in a column\ndef word_count(df,col):\n    temp=\"\"\n    for i in df[col]:\n        temp=temp+'|'+i\n    \n    keywords=set(temp.split('|'))\n    if '' in list(keywords):\n        keywords.remove('')\n    liste=list(keywords)\n    dict_key={}\n    keywords=[]\n    for i in liste:\n        dict_key[i]=[]\n    lt=temp.split(\"|\")\n    for i in liste:\n        j=0\n        for w in lt:\n            if w==i:\n                j=j+1\n        dict_key[i].append(j)\n    for i in liste:\n        keywords.append([i,dict_key[i][0]])\n    keywords=sorted(keywords,key=lambda x:x[1],reverse=True)\n    \n    \n \n    return keywords\n            \n        \n        \n        "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"keyword_occurences=word_count(df_initial,'plot_keywords')"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"len(keyword_occurences)"},{"metadata":{},"source":"# Different Keywords with thier Frequencies which has a total  of 9813 unique Keywords","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"keyword_occurences[:10]"},{"metadata":{},"source":"## Word Cloud and Histogram Representation of Famous Keywords","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"top=keyword_occurences[:50]\ndict_key={}\nfor i in keyword_occurences[:50]:\n    dict_key[i[0]]=i[1]\nplt.figure(figsize=(12,13))\nplt.subplot(211)\nwordcloud = WordCloud()\nwordcloud.generate_from_frequencies(frequencies=dict_key)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Word Cloud',size=40,bbox={'facecolor':'k','pad':5},color='w')\nplt.axis('off')\n\nplt.subplot(212)\nx=[i[0] for i in top]\ny=[i[1] for i in top]\nsns.barplot(x=x,y=y,color='green')\nplt.xticks(fontsize='x-large',rotation='vertical')\nplt.title('Histogram',size=40,bbox={'facecolor':'k','pad':5},color='w')\n\n\n\nplt.show()"},{"metadata":{},"source":"## Along with Keywords we also need Genres so let's use the above function to get the unique genres along with their frequency.","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"temp=\"\"\nfor i in df_initial['genres']:\n    temp=temp+'|'+i\n    \ngenres=set(temp.split('|'))\ngenres.remove('')\nlist_genres=list(genres)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"genre_freq=word_count(df_initial,'genres')"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"len(genre_freq)"},{"metadata":{},"source":"There are a total of 20 genres so we don't hvae to clean this","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"genre_freq[:10]"},{"metadata":{},"source":"### Now there are keywords which share the same rootword so let's combine them together ","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"\nporter= nltk.PorterStemmer()\ndef word_converter(df,col):\n    temp=\"\"\n    for i in df[col]:\n        temp=temp+'|'+i\n    \n    genres=set(temp.split('|'))\n    if '' in list(genres):\n        genres.remove('')\n    liste=list(genres)\n    keyword_roots={}\n    keyword_select={}\n    for s in liste:\n        root=porter.stem(s)\n        if root not in keyword_roots.keys():\n            keyword_roots[root]=[s]\n        else:\n            keyword_roots[root].append(s)\n    for s in keyword_roots.keys():\n                   \n        if len(keyword_roots[s])>1:\n            min_length=10000\n            for k in keyword_roots[s]:\n                if len(k)<min_length:\n                    lth=k\n                    min_length=len(k)\n            keyword_select[s]=lth\n        else:\n            keyword_select[s]=keyword_roots[s][0]\n     \n       \n    return keyword_roots,keyword_select\n        "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"keyword_roots,keyword_select=word_converter(df_initial,'plot_keywords')"},{"metadata":{},"source":"## Let's peak into the different set of root keywords available with us and words associated with them.","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"keys=keyword_roots.keys()\nl=0\nfor key in keys:\n    print([key,keyword_roots[key]])\n    l+=1\n    if l>10:\n        break\n\n"},{"metadata":{},"source":"Now for  the roots which have Multiple words associated with it, we choose one word which has the shortest length and assign all  other words of the same pool to that word.Hence increasing the frequency of the selected word and eliminating the others.","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"keys=keyword_select.keys()\nl=0\nfor key in keys:\n    print([key,keyword_select[key]])\n    l+=1\n    if l>10:\n        break\n    "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## Now that we have cleaned keywords let's replace them by the selected (as discussed above) keywords in the original Data Frame\ndef cleaned_dataframe(df,keyword_select,roots=True):\n    df_cleaned=df.copy()\n    for index,row in df_cleaned.iterrows():\n        liste=[]\n        t=row['plot_keywords']\n        if pd.isnull(t):continue\n        keys=t.split('|')\n        for s in keys:\n            root=porter.stem(s) if roots else s\n            if root in keyword_select.keys():\n                liste.append(keyword_select[root])\n            else:\n                liste.append(s)\n\n        df_cleaned.set_value(index,'plot_keywords',\"|\".join(liste))\n                   \n    return df_cleaned\n        "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_cleaned=cleaned_dataframe(df_initial,keyword_select)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"keyword_occurences_filtered=word_count(df_cleaned,'plot_keywords')"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"key_filter=keyword_occurences_filtered\nlen(key_filter)"},{"metadata":{},"source":"## Here we see that the new Dataframe has a length of 9473 keywords .","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"keyword_occurences_filtered[:10]"},{"metadata":{},"source":"### Now further working on the cleaning of the keywords we replace the words having  Frequency less than 5 with words that share the same meaning but have higher frequency.","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## functions for getting a synonym of a word and herewe are considering only Nouns.\ndef get_synonyms(word):\n    sys=wn.synsets(word)\n    synonyms=set()\n    if not len(sys)==0:\n        for w in sys:\n            if w.name().split('.')[1]=='n':\n                for i in w.lemmas():\n                    synonyms.add(i.name())\n    return list(synonyms)\n    \n    "},{"metadata":{},"source":"This function replaces the words with low Frequency with their synonyms of higher fequency and give a list of equal_synoyms which have same low frequency ","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"\ndef synonym_replace_lessthan(liste,alpha):\n    dict_liste={}\n    for w in liste:\n        dict_liste[w[0]]=w[1]\n    replace={}\n    replacement=[]\n    equal_synonyms=[]\n    freq=[w[0] for w in liste if w[1]<alpha]\n    for w in freq:\n        col=[]\n        con=[]\n        syn=get_synonyms(w)\n      \n        for s in syn:\n            if s in dict_liste.keys():\n                    col.append([s,dict_liste[s]])\n        d=sorted(col,key=lambda x:x[1],reverse=True)\n   \n        if len(col)>1:\n            if dict_liste[w]<dict_liste[d[0][0]]:\n                replace[w]=d[0][0]  \n                replacement.append([w,dict_liste[w],d[0][0],dict_liste[d[0][0]]])\n            for s in d:\n                if dict_liste[s[0]]==dict_liste[w]:\n                    con.append(True)\n                else:\n                    con.append(False)\n            if all(con):\n                    equal_synonyms.append(d)\n        \n    return replace,freq,replacement,equal_synonyms\n    \n    \n    "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"replace_keys,freq,replacement,equal_synonyms=synonym_replace_lessthan(key_filter,5)"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"equal_synonyms[:10]\n"},{"metadata":{},"source":"## Treating the equal_synonyms:-\n* First we try find a synonym of higher frequency from replace_keys and map  all words of equal_synonyms with that.\n* Else we replace all the words with the first word. ","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"\ndef connect(equal_synonyms,replace_keys,dict_liste):  \n    f=replace_keys.copy()\n    \n    for s in equal_synonyms:\n        col=[]\n        for w in [i[0] for i in s]:\n            if w in replace_keys.keys():\n                col.append([replace_keys[w],dict_liste[replace_keys[w]]])\n    \n        d=sorted(col,key=lambda x:x[1],reverse=True)\n        if not len(d)==0: \n               for w in [i[0] for i in s]:\n                    f[w]=d[0][0]\n        if len(d)==0:\n            \n            for i in range(1,len(s)):\n                if s[i][0] not  in f.keys() and s[i][0] not in f.values():\n                    f[s[i][0]]=s[0][0]\n    return f\n        \n            \n        \n    "},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"dict_keywords={}\nfor w in key_filter:\n    dict_keywords[w[0]]=w[1]\n    \nsecond_replace=connect(equal_synonyms,replace_keys,dict_keywords)\nkeys=second_replace\nl=0\nfor key in keys:\n    print([key,second_replace[key]])\n    l+=1\n    if l>10:\n        break"},{"metadata":{},"source":"## Here we come across a situation where\n* I is being mapped into J \n* J is bing mapped into K\n> So J occurs in both key and value of Second_replace list.To encounter this we map,\n* I to k since it has a greater frequenct than J","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"key_value=list(set(list(second_replace.keys())).intersection(list(second_replace.values())))\nkey_value[:10]"},{"metadata":{"scrolled":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"Intersections=[[replace_keys[w],second_replace[w]] for w in list(set(second_replace.keys()).intersection(replace_keys.keys()))]\nIntersections[:10]"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"key_value=list(set(list(second_replace.keys())).intersection(list(second_replace.values())))\nwhile(len(key_value)>0):\n    key_value=list(set(list(second_replace.keys())).intersection(list(second_replace.values())))\n    i=list(second_replace.values())\n    for s in list(second_replace.keys()):\n        if s in key_value:\n            second_replace[list(second_replace.keys())[i.index(s)]]=second_replace[s]\n        \n        \n    \n    "},{"metadata":{},"source":"###  Now we replace the words with their synonyms and create a new list of unique keywords of length","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def replace_synonyms(replace_dict,liste):\n    dict_liste={}\n    for w in liste:\n        dict_liste[w[0]]=w[1]\n    filtered={}\n    filter_list=[]\n    for w in dict_liste.keys():\n        if w not in replace_dict.keys():\n            filtered[w]=dict_liste[w]\n    for w in replace_dict.keys():\n        filtered[replace_dict[w]]= filtered[replace_dict[w]]+dict_liste[w]\n    for w in filtered.keys():\n        filter_list.append([w,filtered[w]])\n    filter_list=sorted(filter_list,key=lambda x:x[1],reverse=True)\n        \n        \n    return filtered,filter_list\n            "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"filter_dict,filter_list=replace_synonyms(second_replace,key_filter)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_cleaned2=cleaned_dataframe(df_cleaned,second_replace,roots=False)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"processed_keywords=word_count(df_cleaned2,'plot_keywords')"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"len(processed_keywords)"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"processed_keywords[:10]"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## function for removing words with frequency lower than certain threshold\ndef frequency_lessthan(liste,alpha):\n    new_list=[]\n    for w in liste:\n        if w[1]>alpha:\n            new_list.append(w)\n    return new_list\n            \n        "},{"metadata":{},"source":"### Futher Processing Keywords we drop the keywords with frequency lower than 3","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"new_keyword_occurences=frequency_lessthan(processed_keywords,3)\nnew_keyword_occurences[:10]"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## Creating A new dataframe with supressed Keywords\ndef supress_keywords(df,supress_keywords):\n    new_df=df.copy()\n    dict_supress={}\n    for i in supress_keywords:\n        dict_supress[i[0]]=i[1]\n    for index,row in new_df.iterrows():\n        col=[i for i in row['plot_keywords'].split(\"|\") if i in dict_supress.keys()]\n        new_df.set_value(index,'plot_keywords',\"|\".join(col))\n        \n    return new_df\n        \n    "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_cleaned3=supress_keywords(df_cleaned2,new_keyword_occurences)\n"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"keyword_list=word_count(df_cleaned3,'plot_keywords')"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"len(keyword_list)"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"plt.figure(figsize=(25,8))\nx=list(range(1,len(keyword_occurences)+1))\ny=[i[1] for i in keyword_occurences]\nnew_xaxis=list(range(1,len(new_keyword_occurences)+1))\nnew_yaxis=[i[1] for i in new_keyword_occurences]\nplt.plot(x,y,'r--',label='before cleaning',linewidth=3.0)\nplt.plot(new_xaxis,new_yaxis,'green',label='after cleaning',linewidth=3.0)\nplt.xlabel('keywords index',size=30,weight='bold')\nplt.ylabel('No. of occurences',size=30,weight='bold')\nplt.axhline(y=3.5,linewidth=2.0,color='black')\nplt.text(3000,3.5,'threshold for keyword deletion',fontsize=30)\nplt.xticks(size=20)\nplt.yticks(size=20)\nplt.legend(loc='upper right',fontsize=30)\nplt.ylim(0,25)## to zoom in the plot\nplt.show()"},{"metadata":{},"source":"### Cleaning other columns of the DataFrame","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_var_cleaned=df_cleaned3.copy()\ns=df_var_cleaned.isnull().sum(axis=0).reset_index()\ns.columns=['column_name','missing_values']\ns.sort_values('missing_values',ascending=False,inplace=True)\ns['filling_factor']=(df_var_cleaned.shape[0]-s['missing_values']) / df_var_cleaned.shape[0] * 100\ns=s.reset_index(drop=True)\nmissing_df=s.copy()\nmissing_df[:10]"},{"metadata":{},"source":"# Recommendation Engine","cell_type":"markdown"},{"metadata":{},"source":"## Basic functioning of the engine\n\norder to build the recommendation engine, I will basically proceed in two steps:\n\n1. determine  N films with a content similar to the entry provided by the user\n2. select the 5 most popular films among these  N  films\n\n### **Similarity**\n\nWhen builing the engine, the first step thus consists in defining a criteria that would tell us how close two films are. To do so, I start from the description of the film that was selected by the user: from it, I get the director name, the names of the actors and a few keywords. I then build a matrix where each row corresponds to a film of the database and where the columns correspond to the previous quantities (director + actors + keywords) plus the k genres that were describe above.\n \n Now that we have our first N movies we need to recommend 5 movies to the user.\n According to similarities between entries, we get a list of  NN  films. At this stage, I select 5 films from this list and, to do so, I give a score for every entry. I decide de compute the score according to 3 criteria:\n\n 1. the IMDB score\n 2. the number of votes the entry received\n 3. the year of release\n \nThe two first criteria will be a direct measure of the popularity of the various entries in IMDB. For the third criterium, I introduce the release year since the database spans films from the early  XXthXXth  century up to now. \nI assume that people's favorite films will be most of the time from the same epoch.\n$score=IMDB^2 X ϕ_{σ1,c1} X ϕ_{σ2,c2}$\n\n$ϕ_{σ,c}(x)∝exp(−(x−c)^2/2σ^2)$","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## This functuion retrievs the labels of the movie entered by the user \ndef add_entry(df,id_entry):\n    col_labels=[]\n    index=list(df['id']).index(id_entry)\n    columns=['director_name','actor_1_name','actor_2_name','actor_3_name','plot_keywords','genres']\n    for s in columns:\n        if pd.isnull(df[s].iloc[index]):continue\n        a=df[s].iloc[index].split(\"|\")\n        for i in a:\n            col_labels.append(i)\n    return col_labels\n"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_initial.head(5)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## This cretaes a datframe including all the  variables given by the add_entry as columns\ndef new_dataframe(df,ref_var):\n    for s in ref_var:df[s]=pd.Series([0 for i in range(len(df_initial))])\n    columns=['director_name','actor_1_name','actor_2_name','actor_3_name','plot_keywords','genres']\n    for col in columns:\n        for index,row in df.iterrows():\n            if pd.isnull(row[col]):continue\n            t=row[col].split('|')\n            for s in t:\n                if s in ref_var:df.set_value(index,s,1)\n                \n    return df\n    \n    "},{"metadata":{},"source":" This functions returns a list of 30 films which were nearest to the movie entered by the user . It uses **Euclidean distance** for this Calculation\n $Distance=\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2+..}$","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def make_recommendation(df,id_entry):\n    index=list(df['id']).index(id_entry)\n    ref_var=add_entry(df,id_entry)\n    new_df=new_dataframe(df,ref_var)\n    X=new_df.as_matrix(ref_var)\n    nearest=NearestNeighbors(n_neighbors=31,algorithm='auto',metric='euclidean').fit(X)\n    xtest=new_df.iloc[index,:].as_matrix(ref_var)\n    xtest=xtest.reshape(1,-1)\n    distance,indices=nearest.kneighbors(xtest)\n    indices=indices.ravel()\n    return indices   "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## Once we have the list of 30 films we extract the variables as stataed above to calculate the score for each movie\ndef feature_extractor(df,film_indices,id_entry):\n    index=list(df['id']).index(id_entry)\n    parametre_list=[]\n    scores=[]\n    col=['vote_average','title_year','num_voted_users','original_title']\n    for s in film_indices:\n        parametre_list.append(df.iloc[s,:][col].values)\n    maximum=0\n    for s in parametre_list:\n        if s[2]>maximum:\n            maximum=s[2]\n    title_year_ref=df.iloc[index,:]['title_year']\n    for s in parametre_list:\n        sc=criterion(s[0],s[1],s[2],s[3],maximum,title_year_ref)\n        scores.append([s[3],sc])\n    scores.sort(key=lambda x:x[1],reverse=False)\n    return parametre_list\n    "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"gaussian_filter = lambda x,mu,sigma: 1./(np.sqrt(2.*np.pi)*sigma)*np.exp(-np.power((x - mu)/sigma, 2.)/2)"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def criterion(vote_average,title_year,num_voted_users,original_title,maximum,title_year_ref):\n    if pd.notnull(vote_average):\n        feature1=vote_average\n    else:\n        feature1=0\n    if pd.notnull(title_year):\n        feature2=gaussian_filter(title_year,title_year_ref,20)\n    else:\n        feature2=0\n    \n    if pd.notnull(num_voted_users):\n        feature3=num_voted_users**2\n    else:\n        feature3=0\n    score=feature1**2*feature2*feature3\n    return score\n    \n    "},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def get_id_entry(title):\n    \n    index=list(df_var_cleaned['original_title']).index(title)\n    return df_var_cleaned['id'].iloc[index]"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"## This is the final functions which recommends the movie\ndef recommend_movies(df,title,sequal=False):\n    selected=[]\n    id_entry=get_id_entry(title)\n    \n    col_labels=add_entry(df,id_entry)\n    film_indices=make_recommendation(df,id_entry)\n    parametre_list=feature_extractor(df,film_indices,id_entry)\n    for s in parametre_list[:5]:\n        selected.append(s[3])\n    if sequal==False:selected=remove_sequal(df,parametre_list,id_entry)\n    \n    \n    return selected"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"def check_sequal(title,title_ref):\n    if (fuzz.ratio(title,title_ref) or fuzz.token_sort_ratio(title,title_ref))>60:\n        return True\n    else:\n        return False"},{"metadata":{},"source":"Many a times recommending a sequal of the movie entered by the user might come of as dumb recommendation, so we remove alll sequals and just give out the most popular sequal","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"\ndef remove_sequal(df,parametre_list,id_entry):\n    index=list(df['id']).index(id_entry)\n    title_ref=df['original_title'].iloc[index]\n    sequal_list=[]\n    film_list=[]\n    selected=[]\n    a=[]\n    for i in parametre_list:\n        if check_sequal(i[3],title_ref):\n            index=list(df['original_title']).index(i[3])\n            sequal_list.append([i[3],df['popularity'].iloc[index]])\n    c=0\n\n    names,score=zip(*sequal_list)\n    for i in parametre_list:\n        if c<5:\n            if  not i[3] in names:\n                film_list.append(i[3])\n                a.append(i[3])\n                c+=1\n    if len(sequal_list)>0:\n        sequal_list.sort(key=lambda x:x[1],reverse=True)\n        selected.append(sequal_list[0][0])\n        for i in film_list[:4]:\n            selected.append(i)\n        return selected\n    else:\n        return film_list         "},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"df_var_cleaned.head(5)"},{"metadata":{},"source":"## Let's Use of Our Recommendation Engine to make some recommendations","cell_type":"markdown"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"recommend_movies(df_var_cleaned,\"Avatar\",sequal=True)"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"recommend_movies(df_var_cleaned,\"Pirates of the Caribbean: At World's End\")"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"recommend_movies(df_var_cleaned,\"Pirates of the Caribbean: At World's End\",sequal=True)"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"recommend_movies(df_var_cleaned,\"Spectre\")"},{"metadata":{},"outputs":[],"execution_count":null,"cell_type":"code","source":"recommend_movies(df_var_cleaned,\"Bound\")"},{"metadata":{},"source":"# Future Work\n* Here I have used euclidean distance we can Other Distances as well such as jacradian distance.\n* One can use other methods to Calculate the score.\n* We can consider production country as well Because user might want movie from the same country to be recommended and even language.\n\n\n","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":""}]}