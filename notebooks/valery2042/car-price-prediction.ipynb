{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\n* The objective of this kernel is to predict the car's price and understand how various factors would influence it. The website www.cardekho.com provided us with all necessary information on a wide variety of cars, including their selling prices. \n\n* I plan to use this information to find an optimal set of parameters that would allow us to predict the Selling Price of the car as accurately as possible. This is a regression problem, thus I would apply various regression models for the assessment of the relationship between independent variables and the dependent variable (Selling_Price) in this dataset.\n\n* I have compared the most common regression models and found that KNN regressor (k-nearest neighbors) resulted in the highest r2 score of 0.83 \n\n* Please do UpVote this Notebook if you find it helpful!"},{"metadata":{},"cell_type":"markdown","source":"**Uploading data and libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/vehicle-dataset-from-cardekho/car data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a></a>\n# Table of contents\n\n\n1. [Quick glance at Data](#Quick-glance-at-the-Data)\n\n2. [My Hypothesis](#Hypothesis)\n\n3. [Exploratory Data Analysis](#EDA)\n\n4. [Feature Engineering](#Feature_Engineering)\n\n5. [Feature Selection](#Feature_Selection)\n\n6. [Model Building](#Model_Building)\n\n7. [Hyperparameters Optimization](#Hyperparameters_Optimization)\n\n8. [Model Evaluation](#Model_Evaluation)\n\n9. [Deployment](#Deployment)\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Quick-glance-at-the-Data'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Quick glance at Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape and data types of the data\n\nprint(df.dtypes)\n\nprint()\n# select numeric columns\nprint('Numerical columns:')\ndf_numeric = df.select_dtypes(include=[np.number])\nnumeric_cols = df_numeric.columns.values\nprint(numeric_cols)\n\n# select non numeric columns\nprint('Categorical columns:')\ndf_cat = df.select_dtypes(exclude=[np.number])\ncat = df_cat.columns.values\nprint(cat)\n\nprint()\nprint('Shape of this df is:')\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the null values\nround(df.isnull().sum()/df.shape[0]*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no null values found in this dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just a minor Feature Engeneering to get the Car's age:\ndf[\"Car_Age\"] = 2021 - df['Year']\ndf.drop('Year',axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Hypothesis'></a>"},{"metadata":{},"cell_type":"markdown","source":"# My Hypothesis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is a good idea to have some ideas about the project before doing any exploratory data analysis:\n\n**My hypothesis was that the car's Age (column Year) would influence the Selling Price at most**"},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Exploratory-Data-Analysis'></a>"},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'EDA'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Car_Name column is not very usefull\ndf.drop('Car_Name', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency of each category separated by label\nplt.figure(figsize=[15,18])\nfeafures = df.columns[1:]\n#take all columns except the target Selling_Price\nn=1\nfor f in feafures:\n    plt.subplot(5,2,n)\n    sns.barplot(x=f,  y= df['Selling_Price'], data=df, palette='twilight')\n    plt.title(\"Countplot of {}  by Selling_Price\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary of EDA analysis**: how the independent features influence the dependent feature (Selling_Price) \n\n1. Present_Price: as Present_Price increases, Selling_Price increases as wel;, demonstrating a steep increase at the higher Present_Price values even though overall it shows a linear correlation with Selling_Price.\n2. Kms_Driven: no correlation with Selling_Price was found.\n3. Fuel_Type: cars with diesel is 3 times more expensive than those with Petrol and CNG.\n4. Seller_Type: cars sold by dealer have higher Selling_Price than the individual sellers by 700% \n5. Transmission: automatic transmissions cost up to 2 times more than Manual ones!\n6. Owner: used cars cost less than bran new, which is kind of common sense. \n6. Car_Age: unsurprisingly, old cars cost less than brand new ones."},{"metadata":{},"cell_type":"markdown","source":"Let's look at the corellation plot of all variables\n\nTo do that let's change the categorical variables to the one-hot-encoded, so we can calculate the correlation matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the one hot encode variables\ndf = pd.get_dummies(data = df,drop_first=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the correlation matrix\nplt.figure(figsize=(8, 10))\nsns.heatmap(df.corr()[['Selling_Price']].sort_values(by='Selling_Price'), \n                    annot=True, \n                    cmap='coolwarm', \n                    vmin=-1,\n                    vmax=1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I have used the absolute cutoff value of 0.5 as a general approach to classify the variables importance based on the correlation matrix.**\n\n1. Fuel_Type_Diesel and Present_Price (with more than +0.5) have considerable positive correlation with Selling_Price.\n\n2. Seller_Type_Individual and Petrol_Type_Diesel (with less than -0.5) have significant negative correlation with Selling_Price.\n\n3. Other features such as Transmission_Manual and Car_Age having correlation coefficient in the range  0 - 0.5 showed lesser correlation with Selling_Price and, thus will be dropped.\n"},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Feature_Selection'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection "},{"metadata":{},"cell_type":"markdown","source":"For now, we have a rough idea of variables that are positively and negatively correlated with the target feature. To make final decision, which variables from highly correlated we will use for our model, we need to access the strength of the \"true\" correlation between predictors by using the Variance Inflation Factor (VIF), which can give us a cleart idea of how these features explain and discribe the \"inflated\" variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that there is a nulticollinearity between the selected independent variables\n#We need to check the inflation factor to make sure we can use the features\n\n#Lets drop the target dependent feature\nX = df.drop('Selling_Price', axis = 1)\n\n#Lets scale the features\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_st = sc.fit_transform(X)\n#X_test_std = sc_x.transform(X_test)\n\n\n# Checking for the VIF values of the variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\n# Creating a dataframe that will contain the names of all the feature variables and their VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train_st, i) for i in range(X_train_st.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VIF must of each variable must be less than 5, or ideally less than 2, for the safety of predictions, [ref_here](https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf). Thus we have to drop the most \"Inflated\" variable (Fuel_Type_Diesel) and assess the set of predictors (independent features) again."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(columns = ['Fuel_Type_Diesel', 'Selling_Price'], axis = 1)\n\n\n#Lets scale the features\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_st = sc.fit_transform(X)\n#X_test_std = sc_x.transform(X_test)\n\n\n# Checking for the VIF values of the variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\n# Creating a dataframe that will contain the names of all the feature variables and their VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train_st, i) for i in range(X_train_st.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! all features have VIF of less than 2. For now, we can drop the uncorrelated columns as Kms_Driven and those that have less than +0.5 and more than -0.5 according to the correlation matrix: \n\nI do not want to use many variables for the model because I want to train it to generalize better and not to overfit the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.drop(columns = ['Kms_Driven','Owner', 'Car_Age',  'Fuel_Type_Diesel', 'Transmission_Manual'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, now we have only three major predictors: Seller_Type, Fuel_Type, and Present_Price.\nLet's check the distribution of these, so we can check for the outliers that might affect our model, for Selling_Price and Present_Price, as others are just frequency features."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['Selling_Price','Present_Price']\n\nfor i in range(2):\n    fig = plt.figure(figsize=[15,4])\n\n    sns.boxplot(x=num_cols[i], data=df)\n\n  \n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<a class = 'anchor' id = 'Feature_Engineering'></a>\n"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"We see that data is rightly skewed, thus we need to correct for the outliers on the right side of the boxplot. There are some general strategies for dealing with the outliers: \n1. Dropping (I do not like it because we lose some information that night be important). You can drop them if that makes perfect sense: like Age of a person that is more than 200 years old!\n2. Imputing outliers to the right end of the distribution (That is what we will do)\n3. Grouping the outliers in a separate dataset or creating a feature from the outliers that can be added to the dataset along with the other independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the Interquartile range of Selling_Price\nIQR=df1.Selling_Price.quantile(0.75)-df1.Selling_Price.quantile(0.25)\n\n#calculating the borders of the normal distribution of Selling_Price\nlower_bridge=df1['Selling_Price'].quantile(0.25)-(IQR*1.5)\nupper_bridge=df1['Selling_Price'].quantile(0.75)+(IQR*1.5)\n\n#Inpute the outliers with the max values that are present for the normal distribution of Selling_Price\ndf1.loc[df1['Selling_Price']>=upper_bridge,'Selling_Price']=upper_bridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#calculating the Interquartile range of Present_Price\nIQR=df1.Present_Price.quantile(0.75)-df.Present_Price.quantile(0.25)\n\n#calculating the borders of the normal distribution of Present_Price\nlower_bridge=df1['Present_Price'].quantile(0.25)-(IQR*1.5)\nupper_bridge=df1['Present_Price'].quantile(0.75)+(IQR*1.5)\n\n#Inpute the outliers with the max values that are present for the normal distribution of Present_Price\ndf1.loc[df1['Present_Price']>=upper_bridge,'Present_Price']=upper_bridge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check again whether we got the outliers out of our dist or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['Selling_Price','Present_Price']\n\nfor i in range(2):\n    fig = plt.figure(figsize=[15,4])\n\n    sns.boxplot(x=num_cols[i], data=df1)\n\n  \n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our outliers are now at the right border of the upper quartile: around 6 for Selling_Price and  around 10 for Present_Price respectively.\n\n\nLets start the model builing with our dataset!"},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Model_Building'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Model Building "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This reduced dataset(df1) we will use for the Selling_Price prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating target variable and its features\ny = df1['Selling_Price']\nX = df1.drop('Selling_Price',axis=1)\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to scale features because some of the regression algorithms as Linear, Lasso, Ridge, KNN require scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#uploading Standard Scaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_st = sc.fit_transform(X_train)\nX_test_st = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#uploading all necessary models that we will apply for this dataset \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost import  XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#creating a dictionary of the models(estimators) \nestimators = {\n    'Linear Regression': [LinearRegression()],\n    'Lasso' :[Lasso()],\n    'Ridge' :[Ridge()],\n    'KNN' :[KNeighborsRegressor()],\n    'Decision Tree' :[DecisionTreeRegressor()],\n    'Ransom Forest' :[RandomForestRegressor()],\n    'XG Boost': [XGBRegressor()],\n}\n\n#writing a function to fit models above to the train dataset\ndef mfit(estimators, X_train_st, y_train):\n    for m in estimators:\n        estimators[m][0].fit(X_train_st, y_train)\n        print(m+' fitted')\n\nmfit(estimators, X_train_st, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\n\n#applying the fitted models to the test dataset\ndef mpredict(estimators, X_test_st, y_test):\n    outcome = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test_st)\n        outcome[m] = [round(metrics.r2_score(y_test, y_pred), 2), \n                      metrics.mean_absolute_error(y_test, y_pred),\n                     \n                     metrics.mean_squared_error(y_test, y_pred),\n                     np.sqrt(metrics.mean_squared_error(y_test, y_pred))]\n    return outcome\n\noutcome = mpredict(estimators, X_test_st, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the regression errors as metrics for the model evaluation\nfor m in outcome:\n    print('------------------------'+m+'------------------------')\n    print('R2 score', round(outcome[m][0],2))\n    print('MAE', round(outcome[m][1],2))\n    print('MSE', round(outcome[m][2],2))\n    print('RMSE', round(outcome[m][3],2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among all model KNN method score not only the highest r2 values, but at the same time the lowest\n[MAE, MSE, RMSE](https://towardsdatascience.com/a-common-mans-guide-to-mae-and-rmse-d5efcd238221)\n\n\nNow , lets do the hyperparaemter tuning for this method"},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Hyperparameters_Optimization'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nmodel = KNeighborsRegressor()\n\n# Select an algorithm\nalgorithm = KNeighborsRegressor()\n\n# Create 3 folds. We will use 3 fold cross validation\nseed = 13\nkfold = KFold(n_splits=3, shuffle=True, random_state=seed)\n\n\n# Define our candidate hyperparameters\nhp_candidates = [{'n_neighbors': [2,3,4,5,6,7,8,9,10], 'weights': ['uniform','distance']}]\n\n\n# Search for best hyperparameters\ngrid = GridSearchCV(estimator=algorithm, param_grid=hp_candidates, cv=kfold, scoring='r2')\ngrid.fit(X_train_st, y_train)\n\n\n# Get the results\nprint(grid.best_estimator_)\nprint(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the best hyperparameters we get are {'n_neighbors': 8, 'weights': 'uniform'}. Thus,lets \nuse these exact parameters for our KNN model and see how that would affect MAE, MSE, RMSE, and R2 score."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nKNN = KNeighborsRegressor(n_neighbors = 8, weights = 'uniform')\nKNN.fit(X_train_st, y_train)\n\npred = KNN.predict(X_test_st)\n\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint(\"R2 score =\", round(metrics.r2_score(y_test, pred), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes! We not only improved the R2 score from 0.79 to 0.83, but at the same time further minimized the errors"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lest check how the predicted values(pred) are correlated with the sample data(y_test)\n\nsns.distplot(y_test-pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the distribution of predictors strongly resembles the actual one of the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.style.use('default')\nplt.style.use('ggplot')\n\nfig, ax = plt.subplots(figsize=(7, 3.5))\n\nax.scatter(X_test_st[:,0], y_test, color='k', label='Regression model')\nax.scatter(X_train_st[:,0], y_train, edgecolor='k', facecolor='red', alpha=0.7, label='Sample data')\nax.set_ylabel('Selling_price', fontsize=14)\nax.set_xlabel('Factors = f(Present_Price, Fuel_Type_Petrol, Seller_Type_Individual)', fontsize=14)\nax.legend(facecolor='white', fontsize=11)\n\nfig.tight_layout()\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Model_Evaluation'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Model_Evaluation"},{"metadata":{},"cell_type":"markdown","source":"\n**There are four key assumptions that need to be tested for a linear regression model:**\n\n1. **Linearity**: The expected value of the dependent variable is a linear function of each independent variable, holding the others fixed (note this does not restrict you to use a nonlinear transformation of the independent variables i.e. you can still model f(x) = ax² + bx + c, using both x² and x as predicting variables.\n\n2. **Independence**: The errors (residuals of the fitted model) are independent of each other.\n\n3. **Homoscedasticity** (constant variance): The variance of the errors is constant with respect to the predicting variables or the response.\n\n4. **Normality**: The errors are generated from a Normal distribution (of unknown mean and variance, which can be estimated from the data). Note, this is not a necessary condition to perform linear regression unlike the top three above. However, without this assumption being satisfied, you cannot calculate the so-called ‘confidence’ or ‘prediction’ intervals easily as the well-known analytical expressions corresponding to Gaussian distribution cannot be used. [Ref](https://towardsdatascience.com/how-do-you-check-the-quality-of-your-regression-model-in-python-fa61759ff685)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first, from the final dataset (df1) we need to create a linear model and assess it with the statistical tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as sm\nmodel=sm.ols(formula = 'Selling_Price ~ Present_Price + Fuel_Type_Petrol + Seller_Type_Individual', \n             data=df1)\nfitted = model.fit()\nprint(fitted.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe an R-squared value of 0.798 demonstrating a decent correlation of Present_Price, Fuel_Type_Petrol, Seller_Type_Individual with the Sellling_Price. The most important part of this table are p-values that show the importance of the individual predictors for the overall model corresponding to the significance level providing that 0,000 < 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for the linearity \nfor c in df1.columns[1:]:\n    plt.figure(figsize=(8,5))\n    plt.title(\"{} vs. \\nSelling_Price\".format(c),fontsize=16)\n    plt.scatter(x=df1[c],y=df1['Selling_Price'],color='blue',edgecolor='k')\n    plt.grid(True)\n    plt.xlabel(c,fontsize=14)\n    plt.ylabel('Selling_Price',fontsize=14)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model shows a linear relationship for the Present_Price and Selling_Price especially for the lowest prices in the range of 0-5 (Present_Price). As Present_Price increases, the variance of Selling_Price peakes dramatically especially at the highest Present_Price decreasing the quality of our linear model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for Normality\nfrom statsmodels.graphics.gofplots import qqplot\nfig=qqplot(fitted.resid_pearson,line='45',fit = 'True')\nplt.xlabel('Theoretical quantiles')\nplt.ylabel('Sample quantiles')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Checking for Normality\nfrom scipy.stats import shapiro\n_,p=shapiro(fitted.resid)\nif p>0.05:\n    print(\"The residuals seem to come from Gaussian process\")\nelse:\n    print(\"The normality assumption may not hold\")\n    \nplt.figure(figsize=(8,5))\nplt.hist(fitted.resid_pearson,bins=20,edgecolor='k')\nplt.ylabel('Count',fontsize=15)\nplt.xlabel('Normalized residuals',fontsize=15)\nplt.title(\"Histogram of normalized residuals\",fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Normality assumption is violated as the left side of the Residual distribution does not follow the normal distribution, thus we can anticipate that the model might not predict well the higher values of Selling_Price."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for Homoscedasticity\nplt.figure(figsize=(8,5))\np=plt.scatter(x=fitted.fittedvalues,y=fitted.resid,edgecolor='k')\nxmin=min(fitted.fittedvalues)\nxmax = max(fitted.fittedvalues)\nplt.hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color='red',linestyle='--',lw=3)\nplt.xlabel(\"Fitted values\",fontsize=15)\nplt.ylabel(\"Residuals\",fontsize=15)\nplt.title(\"Fitted vs. residuals plot\",fontsize=18)\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good model displays the constant variance of the residuals vs their fitted values. In our case we have there major clusters of the residuals as displayed on the X-axis: 1st cluster (0-2), 2nd cluster (3-6), and 3rd cluster (6-12) meaning that the variance of Selling_Price is increasing with the values of predictors meaning that our model might not work well at the higher values of Selling_Price."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Autocorrelation\nfrom statsmodels.stats.stattools import durbin_watson\ndurbinWatson = durbin_watson(fitted.resid)\nprint(durbinWatson)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The autocorrelation assumption is violated. The Durbin-Watson value must be outside of the 0-2 range\nValues from 0 to 2 show positive autocorrelation (common in time series data).\n"},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n\nCars Present_price is the most important factor for predicting the cars Selling_Price."},{"metadata":{},"cell_type":"markdown","source":"<a class = 'anchor' id = 'Deployment'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Deployment"},{"metadata":{},"cell_type":"markdown","source":"[Deployment website](https://carpriceprediction1-api.herokuapp.com/)"},{"metadata":{},"cell_type":"markdown","source":"All files necessary for the deployment can be found [here](https://github.com/valera2042/Cars-price-prediction).\nGenerally, to predict the car's Selling_Price you could convert your Present Price to the scale of this dataset's Present_Price (0.32 - 92.6) then follow the deployment link and recalculate the Selling_Price. Finally, convert Selling_Price (0.1 - 35) back to your Selling Price.\n"},{"metadata":{},"cell_type":"markdown","source":"Dear Reader, thank you for your attention!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}