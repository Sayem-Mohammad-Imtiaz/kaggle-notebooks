{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing Libraries and Data\nimport numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"../input/craigslist-carstrucks-data/vehicles.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Table of Content\n## 1) Exploratory Date Analysis\n### a) Understanding data & cleaning dataset\n### b) Visualizing variables and relationships\n## 2) Data Modelling\n## 3) Feature Importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1) Exploratory Data Analysis\n\n## a) Understanding data & cleaning dataset\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Get a quick glimpse of what I'm working with\nprint(df.shape)\nprint(df.columns)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Want to better understand my numerical variables, specifically the min and max (range)\ndf.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Want to better understand categorical data\ndf.nunique(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns with more than 40% missing values\nNA_val = df.isna().sum()\n\ndef na_filter(na, threshold = .4): #only select variables that passees the threshold\n    col_pass = []\n    for i in na.keys():\n        if na[i]/df.shape[0]<threshold:\n            col_pass.append(i)\n    return col_pass\n\ndf_cleaned = df[na_filter(NA_val)]\ndf_cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Getting rid of outliers for dependent variable ###\ndf_cleaned = df_cleaned[df_cleaned['price'].between(999.99, 250000)] # need to first get rid of unrealistic points to compute IQR more accurately\ndf_cleaned.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing IQR\nQ1 = df_cleaned['price'].quantile(0.25)\nQ3 = df_cleaned['price'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Filtering Values between Q1-1.5IQR and Q3+1.5IQR\ndf_filtered = df_cleaned.query('(@Q1 - 1.5 * @IQR) <= price <= (@Q3 + 1.5 * @IQR)')\ndf_filtered.boxplot('price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking values again\ndf_filtered.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing unrealistic outliers for independent variables\n\ndf_filtered = df_filtered[df_filtered['year'].between(1900, 2020)] # cant be newer than 2020\ndf_filtered = df_filtered[df_filtered['odometer'].between(0, 271431.5)] # = 140000 + 1.5 * (140000-52379)\nprint(df_filtered.shape)\nprint(df_filtered.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary of NA values present\ndf_filtered.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping last few columns\n\ndf_final = df_filtered.copy().drop(['id','url','region_url','image_url','region','description','model','state','paint_color'], axis=1) #removing region since lat/long mean same thing\ndf_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping rows with null values\ndf_final = df_final.dropna(axis=0)\ndf_final.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b) Visualizing variables and relationships","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\nimport seaborn as sns\n\n# calculate correlation matrix\ncorr = df_final.corr()\n# plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.pairplot(df_final)\n\n### Can also use the following if I want to narrow on specific variables ###\n\n# histogram: df_cleaned['price'].plot(kind='hist', bins=50, figsize=(12,6), facecolor='grey',edgecolor='black')\n# boxplot: df_cleaned.boxplot('odometer')\n# scatterplot: df_cleaned.plot(kind='scatter', x='year', y='price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final['manufacturer'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned['type'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) Data Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting categorical variables into dummy variables\ndf_final = pd.get_dummies(df_final, drop_first=True)\nprint(df_final.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nX_head = df_final.iloc[:, df_final.columns != 'price']\n\nX = df_final.loc[:, df_final.columns != 'price']\ny = df_final['price']\nX = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error as mae\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\nmodel = RandomForestRegressor(random_state=1)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking accuracy of model\nprint(mae(y_test, pred))\nprint(df_final['price'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3) Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(model.feature_importances_, index=X_head.columns)\nfeat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}