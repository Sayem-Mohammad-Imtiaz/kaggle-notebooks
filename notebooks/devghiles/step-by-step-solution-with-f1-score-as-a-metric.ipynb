{"cells":[{"metadata":{"_uuid":"ad6fb4ec569aa955a3b38776dca78910a555f182"},"cell_type":"markdown","source":"# SMS Spam Classification","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1a3a1f7d7beb407213ec2062bab23c50de555222"},"cell_type":"markdown","source":"## Imports","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"af36e4352f55fa1fd81fa619e04f394a7f972a24"},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2938956532496794515db378b82aad84c80e13ad"},"cell_type":"markdown","source":"## Read and clean data","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"8ad1934787b09e4582d221d238cc46936a69b6f6"},"cell_type":"code","source":"data = pd.read_csv('../input/spam.csv', encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4094cd65d1ef5f009f779af7657db2e4e9386f18"},"cell_type":"markdown","source":"We see that there are 3 useless columns, we're going to delete them.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"42ee33f8694a510f7a7c7480fd54ab0957c2a6b5"},"cell_type":"code","source":"data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84767b6322732f9aeb9f435efd67bf9f29ffac97"},"cell_type":"markdown","source":"We're going to rename 'v1' and 'v2' as 'label' and 'message' respectively, because that's more meaningful.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"b8ceac2c5fd2d2ad91ce50b95c3e8474f3e974b9"},"cell_type":"code","source":"data.rename(columns={'v1': 'label', 'v2': 'message'}, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6ff2d4a46961965518431377f4f2a5275693235"},"cell_type":"markdown","source":"We check if there are any missing data.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"4f36211a809887a6cb7897950b8c9e50e3182388"},"cell_type":"code","source":"data.count()  # check if the two columns have the same number of values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9f4d03109a94e5e24576a760c26240ec5ec6be39"},"cell_type":"code","source":"data['label'].apply(len).min()  # check for empty labels","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"8f91812e979c4d1deddfb35dd60a480692e03667"},"cell_type":"code","source":"data['message'].apply(len).min()  # check for empty messages","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"204c472866a60d772e0af8c1a487321a2d04ec2e"},"cell_type":"code","source":"set(data['label'])  # check if there are labels other than 'ham' and 'spam'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d736df9eb261b9678f0265c99611c7d49834636f"},"cell_type":"markdown","source":"Apparently, there are no missing data. Before finishing this part, we'll transform the values of the 'label' column from 'ham' and 'spam' to 0 and 1 respectively.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"c49771d6e479d8b3aabe3fa3f7ad04be065104a2"},"cell_type":"code","source":"data['label'] = data['label'].apply(lambda label: 0 if label == 'ham' else 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7943431956c6fe0357e2afee8fb2ee9b0a2fd3c"},"cell_type":"markdown","source":"## Exploratory Data Analysis","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"8f087218f79a176868609055ba2ffb04046c430b"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"25e2e24766c35b72e2c89c8cb95b21e0b2500ef2"},"cell_type":"code","source":"# Empirical distribution of the labels\nprint('Percentage of spams: {0}%'.format(round(100 * data['label'].sum() / len(data['label']), 2)))\nplt.hist(data['label'], bins=3, weights=np.ones(len(data['label'])) / len(data['label']))\nplt.xlabel('label')\nplt.ylabel('Empirical PDF')\nplt.title('Empirical distribution of the labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d79241aa3ba6af72b19eaa7d07a238761cd033a8"},"cell_type":"markdown","source":"We see that the labels are unbalanced. 0 (ham) is highly represented wheras 1 (spam) represents only 13.41% of the data.\n\nSince the classes (labels) are unbalanced, we cannot use accuracy to access the performance of the classifier that we'll build. A classifier which always returns 0 will have an accuracy around 87% (assuming the given data is well sampled). Instead, we'll use the F1-score because it takes into account the precision and recall of the system, which should be the case here.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f4291f3707b6cbdd7ecf294c6b95d256be7575b7"},"cell_type":"markdown","source":"Let's have a look at the messages now.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"07e8b6f4a66406785f44ca8aab7c03ecb2bf03c0"},"cell_type":"code","source":"# extract spams and hams\nspams = data['message'].iloc[(data['label'] == 1).values]\nhams = data['message'].iloc[(data['label'] == 0).values]\nprint(spams[:10])\nprint(hams[:10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c05666128421b9f635c649f2e9f8a212bfa34677"},"cell_type":"markdown","source":"We start seeing some caracteristics of spams, most notably the extensive use of the word 'Free'.","outputs":[],"execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"fb5b36f0405556bc7472018c943534429c62631e"},"cell_type":"code","source":"# message length\ndata['message_length'] = data['message'].apply(lambda message: len(message))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5501c81229f56adc52237b1e2a89f56ca681f841"},"cell_type":"code","source":"plt.hist(data['message_length'], bins=50, weights=np.ones(len(data))/len(data))\nplt.xlabel('Message length')\nplt.ylabel('Empirical PDF')\nplt.title('Messages lengths')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2d0ef161c06b099ea237457d3419c504a0838d0f"},"cell_type":"code","source":"plt.hist(spams.apply(lambda x: len(x)),\n         bins=50,\n         weights=np.ones(len(spams)) / len(spams),\n         facecolor='r',\n         label='Spams')\nplt.hist(hams.apply(lambda x: len(x)),\n         bins=50,\n         weights=np.ones(len(hams)) / len(hams),\n         facecolor='g',\n         alpha=0.8,\n         label='Hams')\nplt.xlabel('Message lenght')\nplt.ylabel('Empirical PDF')\nplt.title('Spam/ham messages lengths')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c87320a73d1469b4ba430003029d874e7666813c"},"cell_type":"markdown","source":"We see that spams tend to be longer than hams. This information might be useful.\n\nNow we'll look at which words are most common in spams and hams.","outputs":[],"execution_count":null},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"c0587a5194166c958a41c4b649c3dc2fff146b36"},"cell_type":"code","source":"# most common words in spam and ham\nspam_tokens = []\nfor spam in spams:\n    spam_tokens += nltk.tokenize.word_tokenize(spam)\nham_tokens = []\nfor ham in hams:\n    ham_tokens += nltk.tokenize.word_tokenize(ham)\nprint(spam_tokens[:10])\nprint(ham_tokens[:10])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ff791e39c1609d358b17e5468caff0e4a653999f"},"cell_type":"code","source":"# remove stop words and puncuation from tokens\nstop_words = ['.', 'to', '!', ',', 'a', '&', \n              'or', 'the', '?', ':', 'is', 'for',\n              'and', 'from', 'on', '...', 'in', ';',\n              'that', 'of']\nfor tokens in [spam_tokens, ham_tokens]:\n    for stop_word in stop_words:\n        try:\n            while True:\n                tokens.remove(stop_word)\n        except ValueError:  # all occurrences of the stop word have been removed\n            pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3e4c8d279ab95a6209de5cf82ec61a36825f0848"},"cell_type":"code","source":"most_common_tokens_in_spams = Counter(spam_tokens).most_common(20)\nmost_common_tokens_in_hams = Counter(ham_tokens).most_common(20)\nprint(most_common_tokens_in_spams)\nprint(most_common_tokens_in_hams)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"704212218e3cf4e13a66c29abf6077b855d21826"},"cell_type":"markdown","source":"Certain words like 'call' and 'free' seem to occurre often in the spams.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1f9839fda9263a2f7cc57c745ba1869ffc987234"},"cell_type":"markdown","source":"## Classification","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5ca99dba8a8b33ce321d37cdf86dc24cfbfa1c85"},"cell_type":"markdown","source":"Since we're maily dealing with text data (the messages), it makes sense to change the representation of the text, using either bag of words representations (tfidfs or word counts or binary BoW) or word embeddings. we'll use BoW representations, starting with tfidfs.\n\nBut before doing anything else, we'll split the data into train data and test data in order to have an accurate estimation of the final classifier's performance.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"9cf56141e586f42c77d08fad7ea0fa965f9d808a"},"cell_type":"code","source":"data, test_data = train_test_split(data, test_size=0.3)\nprint('Train-valid data length: {0}'.format(len(data)))\nprint('Test data length: {0}'.format(len(test_data)))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f090ff8f7dd1b7991f5a782e9a69cc4dc8056adf"},"cell_type":"code","source":"binary_vectorizer = CountVectorizer(binary=True)\ncount_vectorizer = CountVectorizer()\ntfidf_vectorizer = TfidfVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ee285aea85fa95bd22f8dd67c4d35c85d65224d7"},"cell_type":"code","source":"def feature_extraction(df, test=False):\n    if not test:\n        tfidf_vectorizer.fit(df['message'])\n    \n    X = np.array(tfidf_vectorizer.transform(df['message']).todense())\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1bb1d89136cae4aba34cf4b4ec5e9bfd13c75ba4"},"cell_type":"code","source":"train_df, valid_df = train_test_split(data, test_size=0.3)\n\nX_train = feature_extraction(train_df)\ny_train = train_df['label'].values\n\nX_valid = feature_extraction(valid_df, test=True)\ny_valid = valid_df['label'].values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b99eed24750a016cae66d569e961cc9a2619e4aa"},"cell_type":"code","source":"clfs = {\n    'mnb': MultinomialNB(),\n    'gnb': GaussianNB(),\n    'svm1': SVC(kernel='linear'),\n    'svm2': SVC(kernel='rbf'),\n    'svm3': SVC(kernel='sigmoid'),\n    'mlp1': MLPClassifier(),\n    'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),\n    'ada': AdaBoostClassifier(),\n    'dtc': DecisionTreeClassifier(),\n    'rfc': RandomForestClassifier(),\n    'gbc': GradientBoostingClassifier(),\n    'lr': LogisticRegression()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"191330249b490b4ced66be7aba87acb43c1cd553"},"cell_type":"code","source":"f1_scores = dict()\nfor clf_name in clfs:\n    print(clf_name)\n    clf = clfs[clf_name]\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_valid)\n    f1_scores[clf_name] = f1_score(y_pred, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c899530f39031cc5f3b50b0c9bc38022343a33f"},"cell_type":"code","source":"f1_scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b425860a540f034067b5073b20c37c19cd83e96"},"cell_type":"markdown","source":"Multinomial naive bayes seem to give the best results. Before doing hyperparameter optimization, we'll see how other BoW representations perform. We'll reduce the number of classifier to same some computation time.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5f600fdd495f9a2240047abbb44f6b1a8b10038b"},"cell_type":"code","source":"clfs = {\n    'mnb': MultinomialNB(),\n    'gnb': GaussianNB(),\n    'svm': SVC(kernel='linear'),\n    'mlp': MLPClassifier(),\n    'ada': AdaBoostClassifier(),\n    'dtc': DecisionTreeClassifier(),\n    'rfc': RandomForestClassifier(),\n    'gbc': GradientBoostingClassifier(),\n    'lr': LogisticRegression()\n}","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f583262594bb84af83f76e5b123478bee8de4471"},"cell_type":"code","source":"def feature_extraction(df, test=False):\n    if not test:\n        count_vectorizer.fit(df['message'])\n    \n    X = np.array(count_vectorizer.transform(df['message']).todense())\n    return X","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"462c4b65910e888a89b00f0340cfce6b71103f06"},"cell_type":"code","source":"X_train = feature_extraction(train_df)\nX_valid = feature_extraction(valid_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f988e8def232f5b066a32dc23a678a6e0e4c39d8"},"cell_type":"code","source":"f1_scores = dict()\nfor clf_name in clfs:\n    print(clf_name)\n    clf = clfs[clf_name]\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_valid)\n    f1_scores[clf_name] = f1_score(y_pred, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e00e65749d8ac481d937d6d4dabe661f696f4f66"},"cell_type":"code","source":"f1_scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faa2480ca62f4014b2716101d2ba5b825e1e8816"},"cell_type":"markdown","source":"Multinomial naive bayes is still the best. The f1 score for it is the same.\n\nNow, let's see with BoW.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0408d734043d98c30b990150448e30c06032862d"},"cell_type":"code","source":"def feature_extraction(df, test=False):\n    if not test:\n        binary_vectorizer.fit(df['message'])\n    \n    X = np.array(binary_vectorizer.transform(df['message']).todense())\n    return X","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"02c118b46130078c89d9cc1d8678d45e909d3493"},"cell_type":"code","source":"X_train = feature_extraction(train_df)\nX_valid = feature_extraction(valid_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"76aa4111d7affa0ef549844f9e8dc6eba9fb8f03"},"cell_type":"code","source":"f1_scores = dict()\nfor clf_name in clfs:\n    print(clf_name)\n    clf = clfs[clf_name]\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_valid)\n    f1_scores[clf_name] = f1_score(y_pred, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a7bc4fef3d79f4acf3ac8b8b22448e3283818e32"},"cell_type":"code","source":"f1_scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"631bcd420ffdafb13417b936a4c73a5cf7a9b30f"},"cell_type":"markdown","source":"Same results. I want to see how the binomial naive bayes performes.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"081a4f5f0f375eb312d6b5cd6a2245af29c30aa0"},"cell_type":"code","source":"clf = BernoulliNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_valid)\nf1_score(y_pred, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96b20fe7ad6eacb48716c80bfece346d3dd7b910"},"cell_type":"markdown","source":"Not that good.\n\nConclusion: Multinomial naive bayes is the way to go.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"47239fa4d85ef8058740f5582145241d2649b9ff"},"cell_type":"markdown","source":"Now, we'll see if we can do better by using message length.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"732bbea8061080942e4bb026a358594edcebd06a"},"cell_type":"code","source":"def feature_extraction(df, test=False):\n    if not test:\n        count_vectorizer.fit(df['message'])\n    \n    X = np.array(count_vectorizer.transform(df['message']).todense())\n    X = np.concatenate((X, df['message_length'].values.reshape(-1, 1)), axis=1)\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1f3fb9da6a743fb8d11fa1ebecf2574342537253"},"cell_type":"code","source":"X_train = feature_extraction(train_df)\nX_valid = feature_extraction(valid_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a88ecfadd1503a018da872ccf753305cde1eaf87"},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_valid)\nf1_score(y_pred, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"807bbf09b3b4e893a46d4a443a9eb8afe98e1f94"},"cell_type":"markdown","source":"It went down, we'll forget about it and work only with the word counts.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f4397445dec78f195d59276e8180f4458c7374aa"},"cell_type":"markdown","source":"### Hyperparameter optimization","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ade8001685cfd3410901a42b12028cf37d6422b8"},"cell_type":"markdown","source":"We'll only optimize the hyperparemeter alpha.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cf4fca9d3b7882aca0fc2e0ad00d52b40ff018cc"},"cell_type":"code","source":"def feature_extraction(df, test=False):\n    if not test:\n        tfidf_vectorizer.fit(df['message'])\n    \n    X = np.array(tfidf_vectorizer.transform(df['message']).todense())\n    return X","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d874cebd9d71678e4e720cc3b862fb66800bcf71"},"cell_type":"code","source":"X_train = feature_extraction(train_df)\nX_valid = feature_extraction(valid_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"602af166f9f7e9c2e96996e87ed8bf9259b33d95"},"cell_type":"code","source":"alpha_values = [i * 0.1 for i in range(11)]\nmax_f1_score = float('-inf')\nbest_alpha = None\nfor alpha in alpha_values:\n    clf = MultinomialNB(alpha=alpha)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_valid)\n    current_f1_score = f1_score(y_pred, y_valid)\n    if current_f1_score > max_f1_score:\n        max_f1_score = current_f1_score\n        best_alpha = alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e012f6fa4a2b4718f39d91b7e1db9ffd80a9444a"},"cell_type":"code","source":"print('Best alpha: {0}'.format(best_alpha))\nprint('Best f1-score: {0}'.format(max_f1_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afdd859b379d567957dba5c0cd5c948e15c65fe2"},"cell_type":"markdown","source":"## Final results","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e42376d89b22ecc47e7e37372dba385cac7446ce"},"cell_type":"code","source":"clf = MultinomialNB(alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"921316e00c9366cacb8e6b3acdbc3b486d52679d"},"cell_type":"code","source":"X_train = feature_extraction(data)\ny_train = data['label'].values\n\nX_test = feature_extraction(test_data, test=True)\ny_test = test_data['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3588feafabab1f749f76f7f795c96968eee217f1"},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f16d78d5b7c2c2c548380fedde7ccbfdd04876d8"},"cell_type":"code","source":"y_pred = clf.predict(X_test)\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165dcd693af0824a4aeae9d210c9253e0bd747e1"},"cell_type":"markdown","source":"We obtained an f1-score of 0.92, which is pretty good for the task.","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":1}