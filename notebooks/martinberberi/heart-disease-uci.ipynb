{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_columns', None)\n\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (15,7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape, 'This dataset has', data.shape[0], 'rows &', data.shape[1], 'columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['target'], palette='Set1')\nplt.title('Patients who has and has not disease')\nplt.xlabel('Target')\nplt.ylabel('Number of patients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*We can see that our dataset is balanced distributed: 140 patients has not disease & 163 has disease*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['age'], bins = 20)\nplt.title('Dataset Members Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data['target'], data['age'], palette='Set1')\nplt.title('Patients who has and has not disease depends on Age')\nplt.xlabel('Target')\nplt.ylabel('Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*We can see that patients who has disease varies from 45 years old to almost 60 years old and for people who has not disease are on age from 52 years old to 63 years old. Age is not a good predictor for this disease*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['target'], hue = data['sex'], palette='Set1')\nplt.title('Patients with or without disease depends on Sex')\nplt.xlabel('Target')\nplt.ylabel('Nr of patients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_feature = ['cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n\nfor i in columns_feature:\n    sns.boxplot(data['target'], data[i], palette='Set1')\n    plt.title('Target depends on ' + i)\n    plt.xlabel('Target')\n    plt.ylabel(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.Data Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('target', axis = 1)\ny = data['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = LogisticRegression(random_state=0)\ndt = DecisionTreeClassifier(random_state=1)\nsvc = SVC()\nrandom = RandomForestClassifier(random_state=1)\nxgb = XGBClassifier(random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic.fit(X_train, list(y_train.values))\nlog_pred = logistic.predict(X_test)\nprint('Accuracy: ', logistic.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), log_pred))\nprint('Recall: ', recall_score(list(y_test.values), log_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), log_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), log_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.fit(X_train, list(y_train.values))\ndt_pred = dt.predict(X_test)\nprint('Accuracy: ', dt.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), dt_pred))\nprint('Recall: ', recall_score(list(y_test.values), dt_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), dt_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), dt_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector "},{"metadata":{"trusted":true},"cell_type":"code","source":"svc.fit(X_train, list(y_train.values))\nsvc_pred = svc.predict(X_test)\nprint('Accuracy: ', svc.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), svc_pred))\nprint('Recall: ', recall_score(list(y_test.values), svc_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), svc_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), svc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.fit(X_train, list(y_train.values))\nrandom_pred = random.predict(X_test)\nprint('Accuracy: ', random.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), random_pred))\nprint('Recall: ', recall_score(list(y_test.values), random_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), random_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), random_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(X_train, list(y_train.values))\nxgb_pred = xgb.predict(X_test)\nprint('Accuracy: ', xgb.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), xgb_pred))\nprint('Recall: ', recall_score(list(y_test.values), xgb_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), xgb_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), xgb_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nFor better comparison between the models let's draw the ROC curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score_log = logistic.decision_function(X_test)\ny_score_dt = dt.predict_proba(X_test)[:,1]\ny_score_svc = svc.decision_function(X_test)\ny_score_random = random.predict_proba(X_test)[:,1]\ny_score_xgb = xgb.predict_proba(X_test)[:,1]\n\nfpr_log, tpr_log, _ = roc_curve(y_test.values, y_score_log)\nfpr_dt, tpr_dt, _ = roc_curve(y_test.values, y_score_dt)\nfpr_svc, tpr_svc, _ = roc_curve(y_test.values, y_score_svc)\nfpr_random, tpr_random, _ = roc_curve(y_test.values, y_score_random)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test.values, y_score_xgb)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr_log, tpr_log, label = \"Logistic Regression\")\nplt.plot(fpr_dt, tpr_dt, label = 'Decision Tree')\nplt.plot(fpr_svc, tpr_svc, label = 'Support Vector')\nplt.plot(fpr_random, tpr_random, label = 'Random Forest')\nplt.plot(fpr_xgb, tpr_xgb, label = 'XGBoost')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score_random = random.predict_proba(X_test)[:,1]\ny_score_xgb = xgb.predict_proba(X_test)[:,1]\n\nfpr_random, tpr_random, _ = roc_curve(y_test.values, y_score_random)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test.values, y_score_xgb)\n\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr_random, tpr_random, label = 'Random Forest')\nplt.plot(fpr_xgb, tpr_xgb, label = 'XGBoost')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's rank the features in terms of importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in random.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_test.shape[1]):\n    print(\"%d. feature: %s (%f)\" % (f + 1, data.iloc[:,indices[f]].name, importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['cp', 'ca', 'thalach', 'oldpeak', 'chol', 'age', 'thal', 'trestbps']\n\nX2 = X[labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size = 0.2, random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random2 = RandomForestClassifier(random_state=1)\nrandom2.fit(X_train2, list(y_train2.values))\nrandom2_pred = random2.predict(X_test2)\n\nprint('Accuracy: ', random2.score(X_test2, list(y_test2.values)))\nprint('Precission: ', precision_score(list(y_test2.values), random2_pred))\nprint('Recall: ', recall_score(list(y_test2.values), random2_pred))\nprint('F1 Score: ', f1_score(list(y_test2.values), random2_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test2.values), random2_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sel_model = SelectFromModel(Lasso(alpha = 0.005, random_state = 42))\nfeat_sel_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sel_model.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_feat = X.columns[feat_sel_model.get_support()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X3 = X[selected_feat]\nX3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y, test_size = 0.2, random_state = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb3 = XGBClassifier(random_state = 10)\nxgb3.fit(X_train3, list(y_train3.values))\nxgb_pred3 = xgb3.predict(X_test3)\nprint('Accuracy: ', xgb3.score(X_test3, list(y_test3.values)))\nprint('Precission: ', precision_score(list(y_test3.values), xgb_pred3))\nprint('Recall: ', recall_score(list(y_test3.values), xgb_pred3))\nprint('F1 Score: ', f1_score(list(y_test3.values), xgb_pred3))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test3.values), xgb_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*This is a very small dataset so when we remove columns we loose information that is why the accuracy is decreasing*"},{"metadata":{},"cell_type":"markdown","source":"Random Forest Optimization Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 50)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3,\n                              verbose = 2, random_state = 42, n_jobs = -1)\nrf_random.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random3 = RandomForestClassifier(max_depth=80, max_features='sqrt', min_samples_split=10,\n                       n_estimators=991)\nrandom3.fit(X_train2, list(y_train2.values))\nrandom3_pred = random3.predict(X_test2)\n\nprint('Accuracy: ', random3.score(X_test2, list(y_test2.values)))\nprint('Precission: ', precision_score(y_test2.values, random3_pred))\nprint('Recall: ', recall_score(list(y_test2.values), random3_pred))\nprint('F1 Score: ', f1_score(list(y_test2.values), random3_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test2.values), random3_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost Optimization Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int (x) for x in np.linspace(100, 1500, num = 50)]\nlearning_rate = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25]\ngamma = [0.0, 0.1, 0.2, 0.3, 0.4]\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 10)]\nmin_child_weight = [int(x) for x in np.linspace(1, 5, num = 5)]\nbooster = ['gbtree', 'gblinear']\nbase_score = [0.25, 0.5, 0.75, 1]\n\nhyper_parameter = {\n    'n_estimators': n_estimators,\n    'learning_rate': learning_rate,\n    'gamma': gamma,\n    'max_depth': max_depth,\n    'min_child_weight': min_child_weight,\n    'booster': booster,\n    'base_score': base_score\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_optimize = XGBClassifier()\nxgb_random = RandomizedSearchCV(estimator=xgb_optimize, param_distributions=hyper_parameter, n_iter=100,\n                                cv = 3, verbose=2, random_state=42, n_jobs=-1)\nxgb_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_optimize =XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.25, max_delta_step=0, max_depth=7,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=414, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\nxgb_optimize.fit(X_train, list(y_train.values))\nxgb_optimize_pred = xgb_optimize.predict(X_test)\nprint('Accuracy: ', xgb_optimize.score(X_test, list(y_test.values)))\nprint('Precission: ', precision_score(list(y_test.values), xgb_optimize_pred))\nprint('Recall: ', recall_score(list(y_test.values), xgb_optimize_pred))\nprint('F1 Score: ', f1_score(list(y_test.values), xgb_optimize_pred))\nprint('Confusion matrix: \\n', confusion_matrix(list(y_test.values), xgb_optimize_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest after feature selection is the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score_random = random.predict_proba(X_test)[:,1]\ny_score_random2 = random2.predict_proba(X_test2)[:,1]\ny_score_xgb = xgb.predict_proba(X_test)[:,1]\n\nfpr_random, tpr_random, _ = roc_curve(y_test.values, y_score_random)\nfpr_random2, tpr_random2, _ = roc_curve(y_test2.values, y_score_random2)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test.values, y_score_xgb)\n\nplt.plot(fpr_random, tpr_random, label = 'Random Forest', color = 'orange')\nplt.plot(fpr_random2, tpr_random2, label = 'Random Forest Optimize', color = 'blue')\nplt.plot(fpr_xgb, tpr_xgb, label = 'XGBoost', color = 'green')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}