{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"At the beginning I want to say that this is my first more written analysis so don‚Äôt be too hard on me AND my english is bad üò¢.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/f1uQ0uN.png\" width=\"500\">","metadata":{}},{"cell_type":"markdown","source":"# About\r\n*Liver disease* is a growing problem of our time, and having a good method to identify the patients most at risk could help doctors make a faster diagnosis and treatment. We must remember that statistical methods are intended to HELP diagnosticians, not replace them as unquestionable oracles. \r\n\r\n*Note*: During the analysis, certain variables may be removed for the sake of the model. I will use the knowledge acquired from the books with **Logistic Regression** as the default classification method in my mind. \r\n\r\n\r\n**Major variables:** \r\n+ **Bilirubin** is a bile pigment that comes from the breakdown of red blood cells. An increase in this concentration may cause jaundice. \r\n+ **Alkaline** is en enzyme which can by found in the liver and when liver is damaged Alkaline may leak into the bloodstream. Its high levels in blood can indicate liver disease. \r\n+ **Alamine Aminotransferase**: test result can range from 7 to 55 units per liter. \r\n+ **Aspartate_Aminotransferase**: normal ranges are: 10-40 units/L (males), 9-32 units/L (females). \r\n+ In people with badly damaged livers, **proteins** are not properly processed. \r\n+ Low **albumin** levels can indicate a problem with liver or kidneys. \r\n+ **Globulins** play an important role in liver function, blood clotting, and fighting infection. Low globulin levels can be a sign of liver or kidney disease. High levels may indicate infection, inflammatory disease or immune disorders.\r\n\r\n**Dataset variable:**\r\n+ 1-liver patient\r\n+ 2-non liver patient\r\n\r\nDataset variable will be renamed and shift in values.\r\n","metadata":{}},{"cell_type":"markdown","source":"# ANALYSIS OVERVIEW üê±‚Äçüë§\r\n\r\n1. Loading data and packages\r\n2. First look\r\n3. Missing values\r\n4. Fix dataset\r\n5. Dividing the dataset into categorical and quantitative variables\r\n6. Operation on categorical variables\r\n   1. Value counts for Gender & Liver, Disease among Gender, Barplot\r\n7. Operation on quantitative variables\r\n   1. Descriptive statistics\r\n   2. Coefficient of variation\r\n   3. Kurtosis\r\n   4. Skewness\r\n   5. Normality test\r\n   6. Outleiers\r\n   7. Person correlation coefficients\r\n9.  PCA\r\n10. Logistic Regression\r\n    1.  Splitting data to X & y\r\n    2.  Models\r\n    3.  Comparison of results\r\n11. Conclusions","metadata":{}},{"cell_type":"markdown","source":"# Loading data and packages","metadata":{}},{"cell_type":"code","source":"!pip install factor_analyzer","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-10T16:59:24.916755Z","iopub.execute_input":"2021-09-10T16:59:24.917846Z","iopub.status.idle":"2021-09-10T16:59:31.704289Z","shell.execute_reply.started":"2021-09-10T16:59:24.917763Z","shell.execute_reply":"2021-09-10T16:59:31.703103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# packages\r\n\r\nimport pandas as pd\r\nimport matplotlib as plt\r\nimport seaborn as sns \r\nimport numpy as np\r\nfrom scipy.stats import kurtosis, skew, shapiro, zscore\r\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix, accuracy_score, precision_score, recall_score, balanced_accuracy_score, roc_curve, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.706215Z","iopub.execute_input":"2021-09-10T16:59:31.706478Z","iopub.status.idle":"2021-09-10T16:59:31.712827Z","shell.execute_reply.started":"2021-09-10T16:59:31.706449Z","shell.execute_reply":"2021-09-10T16:59:31.71182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/indian-liver-patient-records/indian_liver_patient.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.714397Z","iopub.execute_input":"2021-09-10T16:59:31.715129Z","iopub.status.idle":"2021-09-10T16:59:31.735696Z","shell.execute_reply.started":"2021-09-10T16:59:31.715086Z","shell.execute_reply":"2021-09-10T16:59:31.735056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First look","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.737485Z","iopub.execute_input":"2021-09-10T16:59:31.738399Z","iopub.status.idle":"2021-09-10T16:59:31.756997Z","shell.execute_reply.started":"2021-09-10T16:59:31.738359Z","shell.execute_reply":"2021-09-10T16:59:31.755804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the Dtype of our variables?\r\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.758679Z","iopub.execute_input":"2021-09-10T16:59:31.759048Z","iopub.status.idle":"2021-09-10T16:59:31.779191Z","shell.execute_reply.started":"2021-09-10T16:59:31.759004Z","shell.execute_reply":"2021-09-10T16:59:31.778172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have one variable of class \"object\", but remember that the variable \"Dataset\" is a quality variable too.","metadata":{}},{"cell_type":"code","source":"# How many rows and columns we have?\r\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.780486Z","iopub.execute_input":"2021-09-10T16:59:31.78083Z","iopub.status.idle":"2021-09-10T16:59:31.786946Z","shell.execute_reply.started":"2021-09-10T16:59:31.780769Z","shell.execute_reply":"2021-09-10T16:59:31.785957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing values\r\n\r\nSometimes for some reason there are gaps in our database. This is quite a complex issue and before we start talking about it, we should check how many such missing values our database contains. <br />\r\nIf it is a small number, the best and the simplest method is to remove all the cases and not bother with theory and validity of imputation methods.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.788353Z","iopub.execute_input":"2021-09-10T16:59:31.788592Z","iopub.status.idle":"2021-09-10T16:59:31.801832Z","shell.execute_reply.started":"2021-09-10T16:59:31.788557Z","shell.execute_reply":"2021-09-10T16:59:31.800841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of the nearly 600 cases, only four have missing data in one column. Let's see how they look...","metadata":{}},{"cell_type":"code","source":"df[df.isna().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.803148Z","iopub.execute_input":"2021-09-10T16:59:31.803385Z","iopub.status.idle":"2021-09-10T16:59:31.824123Z","shell.execute_reply.started":"2021-09-10T16:59:31.803359Z","shell.execute_reply":"2021-09-10T16:59:31.823089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These four cases have *Gender* and *Dataset* equally, so removing them should not harm us. ","metadata":{}},{"cell_type":"code","source":"# Just drop NA \r\ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.825508Z","iopub.execute_input":"2021-09-10T16:59:31.825987Z","iopub.status.idle":"2021-09-10T16:59:31.83715Z","shell.execute_reply.started":"2021-09-10T16:59:31.82595Z","shell.execute_reply":"2021-09-10T16:59:31.836112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fix dataset\r\n\r\nThe current step will be a bit tangled but will prove very helpful in the next sections. <br />\r\nFirst, we will copy the current database and describe it as *df_c*, then replace the Dataset values to healthy, sick respectively. Next, for the *df* database, we will change the Gender column to Male, where as 1 - Yes, 0 - No; similarly, we will do with the Dataset column i.e. rename it to Target: 1 - liver disease, 0 - healthy.","metadata":{}},{"cell_type":"code","source":"# copy \r\ndf_c = df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.840014Z","iopub.execute_input":"2021-09-10T16:59:31.840439Z","iopub.status.idle":"2021-09-10T16:59:31.849373Z","shell.execute_reply.started":"2021-09-10T16:59:31.840388Z","shell.execute_reply":"2021-09-10T16:59:31.84845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For df_c\r\ndf_c[\"Dataset\"] = df[\"Dataset\"].map({1:\"Sick\", 2:\"Healthy\"})\r\n\r\n# For df\r\ndf['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\r\ndf['Dataset'] = df['Dataset'].map({1: 1, 2: 0})\r\ndf.rename(columns={'Gender': 'Male'}, inplace=True)\r\ndf.rename(columns={'Dataset': 'Target'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.85042Z","iopub.execute_input":"2021-09-10T16:59:31.851218Z","iopub.status.idle":"2021-09-10T16:59:31.867334Z","shell.execute_reply.started":"2021-09-10T16:59:31.851184Z","shell.execute_reply":"2021-09-10T16:59:31.866276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dividing the dataset into categorical and quantitative variables\r\n\r\nAs a person closely related to the sciences, I have to say one important and unpleasant thing: you MUST NOT perform certain mathematical operations on qualitative variables, for example, the kind that *describe()* does. <br />\r\nBefore calculations, data should be divided in such a way that some operations can be performed on qualitative variables and others on quantitative variables.\r\n\r\nWe only need *df_c* for one purpose - to perform operations on quality variables, so we can remove all other variables from it.\r\n\r\n*df* is our main database, so for operations on quantitative variables I will create an additional copy of it, containing only quantitative variables.","metadata":{}},{"cell_type":"code","source":"df_c.drop(columns=['Age', 'Total_Bilirubin', 'Direct_Bilirubin',\r\n                   'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\r\n                   'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin',\r\n                   'Albumin_and_Globulin_Ratio'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.868548Z","iopub.execute_input":"2021-09-10T16:59:31.868861Z","iopub.status.idle":"2021-09-10T16:59:31.874444Z","shell.execute_reply.started":"2021-09-10T16:59:31.868825Z","shell.execute_reply":"2021-09-10T16:59:31.873823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_quantitative = df.drop(columns=[\"Male\", \"Target\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.875555Z","iopub.execute_input":"2021-09-10T16:59:31.876395Z","iopub.status.idle":"2021-09-10T16:59:31.891981Z","shell.execute_reply.started":"2021-09-10T16:59:31.876352Z","shell.execute_reply":"2021-09-10T16:59:31.891051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Operation on categorical variables\r\n\r\nThe categorical variables we have in the database are expressed on a nominal scale. This means that we can only perform the following operations on them: \r\n+ counting, \r\n+ calculating fractions, \r\n+ calculating mode.","metadata":{}},{"cell_type":"code","source":"# Fraction for Gender\r\ndf_c.Gender.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.893064Z","iopub.execute_input":"2021-09-10T16:59:31.893283Z","iopub.status.idle":"2021-09-10T16:59:31.909164Z","shell.execute_reply.started":"2021-09-10T16:59:31.893259Z","shell.execute_reply":"2021-09-10T16:59:31.908211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fraction for Dataset\r\ndf_c.Dataset.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.910975Z","iopub.execute_input":"2021-09-10T16:59:31.911654Z","iopub.status.idle":"2021-09-10T16:59:31.922343Z","shell.execute_reply.started":"2021-09-10T16:59:31.911611Z","shell.execute_reply":"2021-09-10T16:59:31.921567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Disease and Gender values\r\ndf_c.groupby(\"Dataset\").Gender.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.923875Z","iopub.execute_input":"2021-09-10T16:59:31.924159Z","iopub.status.idle":"2021-09-10T16:59:31.93727Z","shell.execute_reply.started":"2021-09-10T16:59:31.924132Z","shell.execute_reply":"2021-09-10T16:59:31.936363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fraction of healthy/sick by Gender\r\npd.crosstab(df_c['Gender'], df_c['Dataset']).apply(lambda r: r/r.sum()*100, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.938307Z","iopub.execute_input":"2021-09-10T16:59:31.939126Z","iopub.status.idle":"2021-09-10T16:59:31.96055Z","shell.execute_reply.started":"2021-09-10T16:59:31.939094Z","shell.execute_reply":"2021-09-10T16:59:31.959936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [10, 8]  # for size\r\nsns.countplot(x=\"Gender\", hue=\"Dataset\", data=df_c).set_title(\"Liver dieses among Gender\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:31.96147Z","iopub.execute_input":"2021-09-10T16:59:31.962013Z","iopub.status.idle":"2021-09-10T16:59:32.192671Z","shell.execute_reply.started":"2021-09-10T16:59:31.96198Z","shell.execute_reply":"2021-09-10T16:59:32.192025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The conclusions we can draw from the above are: \r\n+ the database is mainly composed of men (76%) and sick people (72%),\r\n+ despite the large difference in numbers, women are less often ill by only about 9%,\r\n+ our database is heavily unbalanced.","metadata":{}},{"cell_type":"markdown","source":"# Operation on quantitative variables\r\n\r\nOne of the most popular functions for initial review of QUANTITATIVE data is \"describe()\". Most often its result is not discussed, but we are tempted to give a brief comment.","metadata":{}},{"cell_type":"code","source":"df_quantitative.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.193795Z","iopub.execute_input":"2021-09-10T16:59:32.194668Z","iopub.status.idle":"2021-09-10T16:59:32.229369Z","shell.execute_reply.started":"2021-09-10T16:59:32.194625Z","shell.execute_reply":"2021-09-10T16:59:32.228491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mean +/- std:** 4 variables have std greater than the mean which means really high dispersion in our data. Alkaline_P also has a very high std, comparable to its mean. <br />\r\n**Max/Min:** Keeping in mind that the correct range for Alamine is 7-55, Aspartate 10-40(M)/9-32(F) we can feel very anxious to see that the maximum values in the database are expressed in thousands. While the minimums are within the normal range.\r\n\r\nWith the above, I believe the database will contain a great number of outliers.","metadata":{}},{"cell_type":"code","source":"# coefficient of variation\r\ndef cv(x): return np.std(x) / np.mean(x) * 100\r\ndf_quantitative.apply(cv)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.230653Z","iopub.execute_input":"2021-09-10T16:59:32.231566Z","iopub.status.idle":"2021-09-10T16:59:32.241733Z","shell.execute_reply.started":"2021-09-10T16:59:32.231522Z","shell.execute_reply":"2021-09-10T16:59:32.240793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$ V = \\frac{std}{mean}*100 $\r\n\r\nThe coefficient of variation is often used to determine if a variable will be important to the model. In our case, the variable: Protein and Albumin do not have a very high V, compared to the other variables. So if they correlate strongly with other variables in the database they will most likely be excluded.","metadata":{}},{"cell_type":"code","source":"# kurtosis\r\ndf_quantitative.apply(kurtosis, bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.242988Z","iopub.execute_input":"2021-09-10T16:59:32.243326Z","iopub.status.idle":"2021-09-10T16:59:32.261759Z","shell.execute_reply.started":"2021-09-10T16:59:32.243284Z","shell.execute_reply":"2021-09-10T16:59:32.260513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kurtosis is a measure of outliers. The higher its value, the more likely there are outliers in the database. \r\nThe lower the value, the more the results are clustered around the mean.\r\n\r\nIn our case, five variables exceed the safe threshold of $K=|3|$, of which Aspartate and Alamine very strongly.  This means that there will undoubtedly be many outliers in the database.","metadata":{}},{"cell_type":"code","source":"# skewness\r\ndf_quantitative.apply(skew, bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.263283Z","iopub.execute_input":"2021-09-10T16:59:32.264079Z","iopub.status.idle":"2021-09-10T16:59:32.27617Z","shell.execute_reply.started":"2021-09-10T16:59:32.264043Z","shell.execute_reply":"2021-09-10T16:59:32.275554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The skewness for most variables is positive, indicating that the distribution has an extended right arm.\r\n\r\nBased on previous results I believe that almost none of the variables have a normal distribution, but to prove this we will perform a normality test.","metadata":{}},{"cell_type":"code","source":"# alpha = 0.05\r\n# H0 = The sample comes from a normal distribution.\r\n# H1 = The sample is not coming from a normal distribution.\r\n\r\nfor i in df_quantitative:\r\n    print([i])\r\n    a, b = shapiro(df_quantitative[[i]])\r\n    if b < 0.05:\r\n        print(\"H1\")\r\n    else:\r\n        print(\"H0\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.277607Z","iopub.execute_input":"2021-09-10T16:59:32.277903Z","iopub.status.idle":"2021-09-10T16:59:32.294954Z","shell.execute_reply.started":"2021-09-10T16:59:32.277871Z","shell.execute_reply":"2021-09-10T16:59:32.293758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outleiers\r\n\r\nWe have already determined that our database will have a significant amount of outliers so to seal this we will perform a boxplot.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=df_quantitative, orient=\"h\").set_title(\"Plot showing outliers\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.295979Z","iopub.execute_input":"2021-09-10T16:59:32.296477Z","iopub.status.idle":"2021-09-10T16:59:32.646912Z","shell.execute_reply.started":"2021-09-10T16:59:32.296442Z","shell.execute_reply":"2021-09-10T16:59:32.645903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To find outliers we will use the interquartile range $IQR = Q_3 - Q_1$. The outlier observations are below the lower bound defined as $lb = Q_1 - 1.5*IQR$ and above the upper bound defined as $ub = Q_3+1.5*IQR$.","metadata":{}},{"cell_type":"code","source":"def remove_outliers(df_in):\r\n\r\n    Q1 = df_in.quantile(0.25)\r\n    Q3 = df_in.quantile(0.75)\r\n    IQR = Q3 - Q1\r\n    upper_limit = Q3 + 1.5*IQR\r\n    lower_limit = Q1 - 1.5*IQR\r\n\r\n    df_clean = df_in[~((df_in < lower_limit) | (df_in > upper_limit)).any(axis=1)]\r\n    \r\n    return df_clean","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.648145Z","iopub.execute_input":"2021-09-10T16:59:32.648397Z","iopub.status.idle":"2021-09-10T16:59:32.653713Z","shell.execute_reply.started":"2021-09-10T16:59:32.648368Z","shell.execute_reply":"2021-09-10T16:59:32.652892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having a function defined to remove outliers we will apply it once on a data base containing quantitative variables.","metadata":{}},{"cell_type":"code","source":"df_clean = remove_outliers(df_quantitative)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.65493Z","iopub.execute_input":"2021-09-10T16:59:32.655177Z","iopub.status.idle":"2021-09-10T16:59:32.671624Z","shell.execute_reply.started":"2021-09-10T16:59:32.65515Z","shell.execute_reply":"2021-09-10T16:59:32.670484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=df_clean, orient=\"h\").set_title(\r\n    \"Plot showing outliers after the 1st removal of outliers\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:32.672896Z","iopub.execute_input":"2021-09-10T16:59:32.673176Z","iopub.status.idle":"2021-09-10T16:59:33.173184Z","shell.execute_reply.started":"2021-09-10T16:59:32.673147Z","shell.execute_reply":"2021-09-10T16:59:33.172374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our database unfortunately has many outliers. A lot of them had a high value, so a single procedure didn't give a very good result. \r\nTherefore, we will create a loop that will repeat the procedure a certain number of times.\r\n\r\n*Keep in mind that the written function will not make any changes to the database if there are no outliers left in the database.*","metadata":{}},{"cell_type":"code","source":"for i in range(5):\r\n    df_clean = remove_outliers(df_clean)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.176885Z","iopub.execute_input":"2021-09-10T16:59:33.177155Z","iopub.status.idle":"2021-09-10T16:59:33.211471Z","shell.execute_reply.started":"2021-09-10T16:59:33.177125Z","shell.execute_reply":"2021-09-10T16:59:33.210469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=df_clean, orient=\"h\").set_title(\r\n    \"Plot showing outliers after the 6th removal of outliers\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.213115Z","iopub.execute_input":"2021-09-10T16:59:33.213446Z","iopub.status.idle":"2021-09-10T16:59:33.561379Z","shell.execute_reply.started":"2021-09-10T16:59:33.213405Z","shell.execute_reply":"2021-09-10T16:59:33.560436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the plot, we can conclude that there are no more outliers in the database. \r\nWe should now ask the question how many cases we had to remove to reach this state.","metadata":{}},{"cell_type":"code","source":"print(\"Number of cases in df:\", len(df))\r\nprint(\"Number of cases in df_clean:\", len(df_clean))\r\nprint(\"We've removed:\", round(100-(len(df_clean)*100/len(df)),2), \"percent of rows.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.562695Z","iopub.execute_input":"2021-09-10T16:59:33.562995Z","iopub.status.idle":"2021-09-10T16:59:33.570227Z","shell.execute_reply.started":"2021-09-10T16:59:33.562963Z","shell.execute_reply":"2021-09-10T16:59:33.569352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By removing outliers we have erased almost 80% of the entire database. This is very bad and we could suggest another solution to this problem. \r\n\r\n*For example*, we could replace the variables with the largest spread of values with qualitative variables, e.g. below normal, in normal, above normal, based on the ranges given in the study. \r\n\r\nIn this notebook, however, we will not do this. We will continue to work with a reduced database.\r\nUsing the indexes, we will examine how the qualitative variables for the base look after removing outliers.","metadata":{}},{"cell_type":"code","source":"df_c_trimmed = df_c[df_c.index.isin(df_clean.index)]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.57141Z","iopub.execute_input":"2021-09-10T16:59:33.571643Z","iopub.status.idle":"2021-09-10T16:59:33.581403Z","shell.execute_reply.started":"2021-09-10T16:59:33.571619Z","shell.execute_reply":"2021-09-10T16:59:33.580423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c_trimmed.Dataset.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.583036Z","iopub.execute_input":"2021-09-10T16:59:33.583294Z","iopub.status.idle":"2021-09-10T16:59:33.594123Z","shell.execute_reply.started":"2021-09-10T16:59:33.583268Z","shell.execute_reply":"2021-09-10T16:59:33.593214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c_trimmed.Gender.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.595659Z","iopub.execute_input":"2021-09-10T16:59:33.59675Z","iopub.status.idle":"2021-09-10T16:59:33.607387Z","shell.execute_reply.started":"2021-09-10T16:59:33.596689Z","shell.execute_reply":"2021-09-10T16:59:33.606404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see by removing the outliers we have accidentally solved the problem of strongly unbalanced classes.\r\n\r\nAt the very end, all that is left is to trim the main database based on the removed outliers.","metadata":{}},{"cell_type":"code","source":"df_trimmed = df[df.index.isin(df_clean.index)]","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.608549Z","iopub.execute_input":"2021-09-10T16:59:33.608861Z","iopub.status.idle":"2021-09-10T16:59:33.618239Z","shell.execute_reply.started":"2021-09-10T16:59:33.608831Z","shell.execute_reply":"2021-09-10T16:59:33.617509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Person correlation coefficients\r\n\r\n$\\rho_{X, Y}=\\frac{\\operatorname{cov}(X, Y)}{\\sigma_{X} \\sigma_{Y}}$\r\n\r\nThe formula above describes the Pearson linear correlation between two variables. We can use it for our quantitative data before and after removing outliers.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df_quantitative.corr(), annot=True, cmap='coolwarm',\r\n            mask=np.triu(df_quantitative.corr())).set_title(\"Before removing outliers\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:33.619356Z","iopub.execute_input":"2021-09-10T16:59:33.620002Z","iopub.status.idle":"2021-09-10T16:59:34.228785Z","shell.execute_reply.started":"2021-09-10T16:59:33.619954Z","shell.execute_reply":"2021-09-10T16:59:34.227833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df_clean.corr(), annot=True, cmap='coolwarm',\r\n            mask=np.triu(df_clean.corr())).set_title(\"After removing outliers\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.230093Z","iopub.execute_input":"2021-09-10T16:59:34.230345Z","iopub.status.idle":"2021-09-10T16:59:34.785916Z","shell.execute_reply.started":"2021-09-10T16:59:34.230316Z","shell.execute_reply":"2021-09-10T16:59:34.785001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the correlation results above, it can be seen that removing outliers reduced the correlations in the database. This is a positive effect considering the logistic regression model, but a negative effect for PCA.\r\n\r\nThe only highly correlating variable is Albumin therefore we will remove it from the database before building the logistic regression model.","metadata":{}},{"cell_type":"markdown","source":"# PCA\r\n\r\n...is a popular algorithm for dimensionality reduction. It performs a transformation of our current variables into principal components, the first two/three of which should explain a large enough percentage of the total variance to make the graph helpful in, for example, identifying groups. \r\n\r\nOne of the requirements for this algorithm to work properly is that there is a strong correlation between our variables. Our database does not meet this requirement, and to prove this I will use Bartlett's test and KMO criterion.","metadata":{}},{"cell_type":"markdown","source":"## Bartlett \r\n\r\nTest the hypothesis that the correlation matrix is equal to the identity matrix. <br />\r\n*H0*: The matrix of population correlations **is equal** to I. <br />\r\n *H1*: The matrix of population correlations **is not equal** to I.","metadata":{}},{"cell_type":"code","source":"calculate_bartlett_sphericity(df_clean)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.787222Z","iopub.execute_input":"2021-09-10T16:59:34.787606Z","iopub.status.idle":"2021-09-10T16:59:34.797956Z","shell.execute_reply.started":"2021-09-10T16:59:34.787563Z","shell.execute_reply":"2021-09-10T16:59:34.797138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Kaiser-Meyer-Olkin\r\n\r\nCalculate the Kaiser-Meyer-Olkin criterion for items and overall. This statistic represents the degree to which each observed variable is predicted, without error, by the other variables in the dataset. In general, a $KMO < 0.6$ is considered inadequate.","metadata":{}},{"cell_type":"code","source":"kmo_per_variable, kmo_total = calculate_kmo(df_clean)\r\nprint(\"per variable:\", kmo_per_variable, \"total:\", kmo_total)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.799483Z","iopub.execute_input":"2021-09-10T16:59:34.79988Z","iopub.status.idle":"2021-09-10T16:59:34.811187Z","shell.execute_reply.started":"2021-09-10T16:59:34.799847Z","shell.execute_reply":"2021-09-10T16:59:34.809917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on both tests, it is safe to say that PCA would not help us in any way. ","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression\r\n\r\nAll necessary theoretical information can be found at this [link](https://en.wikipedia.org/wiki/Logistic_regression).","metadata":{}},{"cell_type":"markdown","source":"As I wrote previously we remove the highly correlated variable.","metadata":{}},{"cell_type":"code","source":"df_trimmed.drop(columns=\"Albumin\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.812393Z","iopub.execute_input":"2021-09-10T16:59:34.812676Z","iopub.status.idle":"2021-09-10T16:59:34.818734Z","shell.execute_reply.started":"2021-09-10T16:59:34.812648Z","shell.execute_reply":"2021-09-10T16:59:34.818158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting data to X & y","metadata":{}},{"cell_type":"markdown","source":"Since we practically decimated our database I decided to build two models. The first with our trimmed df, the second using the entire database.\r\nThe trimmed df is to small for the purposes of typical machine learning (in my opinion) so we will not split it into a training and test set. ","metadata":{}},{"cell_type":"code","source":"X = df_trimmed.loc[:, df_trimmed.columns!='Target']\r\ny = df_trimmed.loc[:, 'Target']","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.819693Z","iopub.execute_input":"2021-09-10T16:59:34.82058Z","iopub.status.idle":"2021-09-10T16:59:34.830394Z","shell.execute_reply.started":"2021-09-10T16:59:34.820538Z","shell.execute_reply":"2021-09-10T16:59:34.829545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"...but the df is large enough so we will split it.","metadata":{}},{"cell_type":"code","source":"X_all = df.loc[:, df.columns!='Target']\r\ny_all = df.loc[:, 'Target']","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.831571Z","iopub.execute_input":"2021-09-10T16:59:34.832454Z","iopub.status.idle":"2021-09-10T16:59:34.842101Z","shell.execute_reply.started":"2021-09-10T16:59:34.832412Z","shell.execute_reply":"2021-09-10T16:59:34.841432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_all, y_all, test_size = 0.30, random_state = 0, stratify = y_all)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.843113Z","iopub.execute_input":"2021-09-10T16:59:34.843887Z","iopub.status.idle":"2021-09-10T16:59:34.856919Z","shell.execute_reply.started":"2021-09-10T16:59:34.843843Z","shell.execute_reply":"2021-09-10T16:59:34.855813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"I did not perform standardization of the variables therefore I increased the number of iterations for the model.","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(max_iter=1000)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.858617Z","iopub.execute_input":"2021-09-10T16:59:34.858977Z","iopub.status.idle":"2021-09-10T16:59:34.8656Z","shell.execute_reply.started":"2021-09-10T16:59:34.858948Z","shell.execute_reply":"2021-09-10T16:59:34.864637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trimmed df","metadata":{}},{"cell_type":"code","source":"res_1 = model.fit(X, y)\r\ny_predict_1 = model.predict(X)\r\nconfusion_matrix(y_pred=y_predict_1,y_true=y)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.866552Z","iopub.execute_input":"2021-09-10T16:59:34.866761Z","iopub.status.idle":"2021-09-10T16:59:34.897009Z","shell.execute_reply.started":"2021-09-10T16:59:34.866738Z","shell.execute_reply":"2021-09-10T16:59:34.896418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(y, y_predict_1))\r\nprint(\"Precision:\", precision_score(y, y_predict_1))\r\nprint(\"Recall:\", recall_score(y, y_predict_1))\r\nprint(\"Balanced accuracy score:\", balanced_accuracy_score(y, y_predict_1))","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.897831Z","iopub.execute_input":"2021-09-10T16:59:34.898617Z","iopub.status.idle":"2021-09-10T16:59:34.906711Z","shell.execute_reply.started":"2021-09-10T16:59:34.898585Z","shell.execute_reply":"2021-09-10T16:59:34.906149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_roc_auc_1 = roc_auc_score(y, y_predict_1)\r\nfpr_1, tpr_1, thresholds_1 = roc_curve(y, res_1.predict_proba(X)[:, 1])\r\nplt.pyplot.plot(fpr_1, tpr_1, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_1)\r\nplt.pyplot.plot([0, 1], [0, 1], 'r--')\r\nplt.pyplot.xlim([0.0, 1.0])\r\nplt.pyplot.ylim([0.0, 1.05])\r\nplt.pyplot.xlabel('False Positive Rate')\r\nplt.pyplot.ylabel('True Positive Rate')\r\nplt.pyplot.title('Receiver operating characteristic for df')\r\nplt.pyplot.legend(loc=\"lower right\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:34.907561Z","iopub.execute_input":"2021-09-10T16:59:34.908201Z","iopub.status.idle":"2021-09-10T16:59:35.150532Z","shell.execute_reply.started":"2021-09-10T16:59:34.908171Z","shell.execute_reply":"2021-09-10T16:59:35.149859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### df","metadata":{}},{"cell_type":"code","source":"res_2 = model.fit(X_train_all, y_train_all)\r\ny_predict_2 = model.predict(X_test_all)\r\nconfusion_matrix(y_pred=y_predict_2, y_true=y_test_all)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:35.151437Z","iopub.execute_input":"2021-09-10T16:59:35.15216Z","iopub.status.idle":"2021-09-10T16:59:35.345905Z","shell.execute_reply.started":"2021-09-10T16:59:35.152114Z","shell.execute_reply":"2021-09-10T16:59:35.344801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(y_test_all, y_predict_2))\r\nprint(\"Precision:\", precision_score(y_test_all, y_predict_2))\r\nprint(\"Recall:\", recall_score(y_test_all, y_predict_2))\r\nprint(\"Balanced accuracy score:\", balanced_accuracy_score(y_test_all, y_predict_2))","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:35.347564Z","iopub.execute_input":"2021-09-10T16:59:35.347966Z","iopub.status.idle":"2021-09-10T16:59:35.364343Z","shell.execute_reply.started":"2021-09-10T16:59:35.347918Z","shell.execute_reply":"2021-09-10T16:59:35.363267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_roc_auc_2 = roc_auc_score(y_test_all, y_predict_2)\r\nfpr_2, tpr_2, thresholds_2 = roc_curve(y_test_all, res_2.predict_proba(X_test_all)[:, 1])\r\nplt.pyplot.plot(fpr_2, tpr_2, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_2)\r\nplt.pyplot.plot([0, 1], [0, 1], 'r--')\r\nplt.pyplot.xlim([0.0, 1.0])\r\nplt.pyplot.ylim([0.0, 1.05])\r\nplt.pyplot.xlabel('False Positive Rate')\r\nplt.pyplot.ylabel('True Positive Rate')\r\nplt.pyplot.title('Receiver operating characteristic for df_all')\r\nplt.pyplot.legend(loc=\"lower right\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:35.36619Z","iopub.execute_input":"2021-09-10T16:59:35.366519Z","iopub.status.idle":"2021-09-10T16:59:35.625096Z","shell.execute_reply.started":"2021-09-10T16:59:35.36648Z","shell.execute_reply":"2021-09-10T16:59:35.624058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparison of results","metadata":{}},{"cell_type":"code","source":"data = {\"df_trimmed\": [accuracy_score(y, y_predict_1), precision_score(y, y_predict_1), recall_score(y, y_predict_1), balanced_accuracy_score(y, y_predict_1)],\r\n        \"df\": [accuracy_score(y_test_all, y_predict_2), precision_score(y_test_all, y_predict_2), recall_score(y_test_all, y_predict_2), balanced_accuracy_score(y_test_all, y_predict_2)]}\r\n\r\ncomparision = pd.DataFrame(data, index = [\"Accuracy\", \"Precision\", \"Recall\", \"Balanced accuracy\"])\r\nprint(comparision)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:35.626463Z","iopub.execute_input":"2021-09-10T16:59:35.626666Z","iopub.status.idle":"2021-09-10T16:59:35.645607Z","shell.execute_reply.started":"2021-09-10T16:59:35.626643Z","shell.execute_reply":"2021-09-10T16:59:35.644675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Liver patients percentage in df_trimmed:\", df_trimmed.Target.sum()/len(df_trimmed.Target))\r\nprint(\"Liver patients percentage in df:\", df.Target.sum()/len(df.Target))","metadata":{"execution":{"iopub.status.busy":"2021-09-10T16:59:35.646787Z","iopub.execute_input":"2021-09-10T16:59:35.647043Z","iopub.status.idle":"2021-09-10T16:59:35.654722Z","shell.execute_reply.started":"2021-09-10T16:59:35.64701Z","shell.execute_reply":"2021-09-10T16:59:35.653755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\r\n\r\nThe first conclusion we can make is: DELETING almost 80% of our database because of outliers is a bad idea. We really shouldn't be doing this.\r\n\r\nThe models for the two cases are slightly different. I would say about 10% on average. Which means that laboriously checking assumptions, eliminating outliers, removing a highly correlated variable, etc. produced a poor end result.\r\n\r\nOf course, it should be noted that in our decimated database, about 51% of cases had a diseased liver, and our accuracy is 61% in this model. Using this model, we slightly improve our assessment of whether a patient has a diseased liver or not, compared to assuming that all patients have the disease.\r\nFor the entire database, patients with diseased liver make up about 72% of the cases, and our model has a accuracy of 70%, so just looking at this rate, you could say that whether we use the model or assume that everyone has the disease...it doesn't matter.\r\n\r\nThe next step in further analysis should be to convert those variables with a lot of outliers into qualitative variables. Then applying the solution method to the strongly unbalanced classes.","metadata":{}}]}