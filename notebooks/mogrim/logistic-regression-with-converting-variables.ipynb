{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a continuation of my previous notebook about liver patients, so I will not repeat certain elements of the usual data overview here.\r\n\r\n# Contents üê±‚Äçüë§\r\n\r\n1. Initial data manipulation\r\n   1. Loading data\r\n   2. Drop NA\r\n   3. Renaming values and columns\r\n2. Converting a quantitative variable to a qualitative variable\r\n   1. Kurtosis\r\n   2. Boxplots\r\n   3. Converting Aspartate and Alamine \r\n   4. Renaming values\r\n3. A look at new qualitative variables\r\n4. Outliers\r\n5. Pearson's correlation\r\n6. Standardization of quantitative variables\r\n7. Testing databases\r\n8. Logistic regression\r\n9. Performance comparison\r\n10. Conclusions\r\n11. Appendix I: balancing classes\r\n    1.  Random Undersampling\r\n    2.  Random Oversampling\r\n    3.  Logistic regression for undersampling and oversampling\r\n    4.  Conclusions II","metadata":{}},{"cell_type":"markdown","source":"# Initial data manipulation is discussed more fully in this [notebook](https://www.kaggle.com/mogrim/logistic-regression-with-all-outliers-removed).","metadata":{}},{"cell_type":"code","source":"# packages\r\nimport pandas as pd\r\nimport matplotlib as plt\r\nimport seaborn as sns\r\nimport numpy as np\r\nfrom scipy.stats import kurtosis\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score\r\nfrom imblearn.over_sampling import RandomOverSampler\r\nfrom imblearn.under_sampling import RandomUnderSampler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data loading\r\ndf = pd.read_csv(\"../input/indian-liver-patient-records/indian_liver_patient.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna() # simply drop NA\r\ndf_c = df.copy()  # database copy for qualitative data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For df_c\r\ndf_c[\"Dataset\"] = df[\"Dataset\"].map({1: \"Sick\", 2: \"Healthy\"})\r\n\r\n# For df\r\ndf['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\r\ndf['Dataset'] = df['Dataset'].map({1: 1, 2: 0})\r\ndf.rename(columns={'Gender': 'Male'}, inplace=True)\r\ndf.rename(columns={'Dataset': 'Target'}, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.drop(columns=['Age', 'Total_Bilirubin', 'Direct_Bilirubin',\r\n                   'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\r\n                   'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin',\r\n                   'Albumin_and_Globulin_Ratio'], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting a quantitative variable to a qualitative variable","metadata":{}},{"cell_type":"markdown","source":"**Kurtosis** is a measure of outliers. The higher its value, the more likely there are outliers in the database. ","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"Target\", \"Male\"]).kurtosis()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The very high value of kurtosis has two variables therefore, having medical information about normal ranges of their values in blood, we will convert them into qualitative variables.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [10, 8]  # for size\r\nsns.boxplot(data=df.Aspartate_Aminotransferase, orient=\"h\").set_title(\"Aspartate_Aminotransferase\")\r\nprint(\"Min value:\", min(df.Aspartate_Aminotransferase))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=df.Alamine_Aminotransferase, orient=\"h\").set_title(\"Alamine_Aminotransferase\")\r\nprint(\"Min value:\", min(df.Alamine_Aminotransferase))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting Aspartate and Alamine to qualitative varibles\r\n\r\n+ **Alamine Aminotransferase**: test result can range from 7 to 55 units per liter. \r\n+ **Aspartate_Aminotransferase**: normal ranges are: 10-40 units/L (males), 9-32 units/L (females). \r\n\r\nIn both cases, the minimum value of the variable is 10, so neither case is sub-normal. Hence the replacement will be to assign a value to the person:1: above normal, 0: in normal, depending on the reference values given in the medical literature. ","metadata":{}},{"cell_type":"code","source":"def alamine(df):\r\n    if df['Alamine_Aminotransferase'] <= 55: return 0\r\n    else: return 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aspartate(df):\r\n    if df['Male'] == 1 and df['Aspartate_Aminotransferase'] <= 40: return 0\r\n    elif df['Male'] == 0 and df['Aspartate_Aminotransferase'] <= 32: return 0 \r\n    elif df['Male'] == 0 and df['Aspartate_Aminotransferase'] > 32: return 1\r\n    else: return 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't want to make changes to the main database, so I will make copy.","metadata":{}},{"cell_type":"code","source":"df_1 = df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1['Alamine_Aminotransferase'] = df_1.apply(alamine, axis=1)\r\ndf_1['Aspartate_Aminotransferase'] = df_1.apply(aspartate, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For better readability of the quality variables, I will put them in a separate database and convert 1-0 to above normal and normal.","metadata":{}},{"cell_type":"code","source":"to_add = df_1.loc[:,['Alamine_Aminotransferase', 'Aspartate_Aminotransferase']]\r\ndf_c = df_c.join(to_add)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c['Alamine_Aminotransferase'] = df_c['Alamine_Aminotransferase'].map({0: \"Normal\", 1: \"Above_Normal\"})\r\ndf_c['Aspartate_Aminotransferase'] = df_c['Aspartate_Aminotransferase'].map({0: \"Normal\", 1: \"Above_Normal\"})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A look at new qualitative variables","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"Alamine_Aminotransferase\", hue=\"Dataset\", data=df_c).set_title(\r\n    \"Liver dieses among Alamine_Aminotransferase level\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"Aspartate_Aminotransferase\", hue=\"Dataset\", data=df_c).set_title(\r\n    \"Liver dieses among Aspartate_Aminotransferase level\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.groupby(\"Gender\").Aspartate_Aminotransferase.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.groupby(\"Dataset\").Aspartate_Aminotransferase.value_counts()\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.Alamine_Aminotransferase.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_c.Aspartate_Aminotransferase.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outliers\r\n\r\nAs I mentioned in a previous notebook on this issue, **we cannot perform mathematical operations such as counting the mean or quartile on qualitative variables**. Therefore, they must be excluded.","metadata":{}},{"cell_type":"code","source":"# database for quantitative variables\r\ndf_q = df_1.drop(columns=['Male', 'Target', 'Alamine_Aminotransferase', 'Aspartate_Aminotransferase'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=df_q, orient=\"h\").set_title(\"Outliers\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Alkaline variable has quite a few outliers (not as many as the two we converted to qualitative variables) so we will perform the outlier removal procedure once.","metadata":{}},{"cell_type":"code","source":"def remove_outliers(df_in):\r\n\r\n    Q1 = df_in.quantile(0.25)\r\n    Q3 = df_in.quantile(0.75)\r\n    IQR = Q3 - Q1\r\n    upper_limit = Q3 + 1.5*IQR\r\n    lower_limit = Q1 - 1.5*IQR\r\n\r\n    df_clean = df_in[~((df_in < lower_limit) | (df_in > upper_limit)).any(axis=1)]\r\n    \r\n    return df_clean","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_q = remove_outliers(df_q)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of cases in df:\", len(df))\r\nprint(\"Number of cases in df_q:\", len(df_q))\r\nprint(\"We've removed:\", round(100-(len(df_q)*100/len(df)),2), \"percent of rows.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this procedure we removed about 25% of the database which is a very good result compared to the previous 80%.","metadata":{}},{"cell_type":"markdown","source":"# Pearson's correlation","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df_q.corr(), annot=True, cmap='coolwarm',\r\n            mask=np.triu(df_q.corr())).set_title(\"Correlogram\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two variables correlate very strongly, so I will remove them. ","metadata":{}},{"cell_type":"code","source":"df_trimmed = df_1[df_1.index.isin(df_q.index)]\r\ndf_trimmed = df_trimmed.drop(columns=['Albumin', 'Total_Bilirubin'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardization of quantitative variables\r\n\r\n$ z = \\frac{x-u}{s}$\r\n\r\nwhere: x - *sample*, u - *mean*, s - *std* \r\n\r\nStandardization is the process of putting different variables on the same scale; allows you to compare scores between different types of variables.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\r\ndf_scaled = df_trimmed.copy()\r\n# scaling trimmed data\r\ndf_scaled[['Age', 'Direct_Bilirubin', 'Alkaline_Phosphotase', 'Total_Protiens', 'Albumin_and_Globulin_Ratio']] = scaler.fit_transform(\r\n    df_trimmed[['Age', 'Direct_Bilirubin', 'Alkaline_Phosphotase', 'Total_Protiens', 'Albumin_and_Globulin_Ratio']])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaled_all = df.copy()\r\n# scaling complete data\r\ndf_scaled_all[['Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase', 'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin', 'Albumin_and_Globulin_Ratio']] = scaler.fit_transform(\r\n    df[['Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase', 'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin', 'Albumin_and_Globulin_Ratio']])\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing databases\r\n\r\nI will use four databases to perform a logistic regression model to predict whether a patient has a diseased liver or not.","metadata":{}},{"cell_type":"code","source":"# initial database\r\ndf.head() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaled initial database\r\ndf_scaled_all.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trimmed database\r\ndf_trimmed.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaled trimmed database\r\ndf_scaled.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic regression","metadata":{}},{"cell_type":"markdown","source":"## Splitting data to X & y","metadata":{}},{"cell_type":"code","source":"# for df\r\nX_df = df.loc[:, df.columns != 'Target']\r\ny_df = df.loc[:, 'Target']\r\n\r\n# for df_scaled_all\r\nX_df_scaled_all = df_scaled_all.loc[:, df_scaled_all.columns != 'Target']\r\ny_df_scaled_all = df_scaled_all.loc[:, 'Target']\r\n\r\n# for df trimmed\r\nX_df_trimmed = df_trimmed.loc[:, df_trimmed.columns != 'Target']\r\ny_df_trimmed = df_trimmed.loc[:, 'Target']\r\n\r\n# for df trimmed and scaled\r\nX_df_trimmed_scaled = df_scaled.loc[:, df_scaled.columns != 'Target']\r\ny_df_trimmed_scaled = df_scaled.loc[:, 'Target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for df\r\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size = 0.30, random_state = 0, stratify = y_df)\r\n\r\n# for df_scaled_all\r\nX_train_df_scaled_all, X_test_df_scaled_all, y_train_df_scaled_all, y_test_df_scaled_all = train_test_split(X_df_scaled_all, y_df_scaled_all, test_size = 0.30, random_state = 0, stratify = y_df_scaled_all)\r\n\r\n# for df trimmed\r\nX_train_df_trimmed, X_test_df_trimmed, y_train_df_trimmed, y_test_df_trimmed = train_test_split(X_df_trimmed, y_df_trimmed, test_size = 0.30, random_state = 0, stratify = y_df_trimmed)\r\n\r\n# for df trimmed and scaled\r\nX_train_df_trimmed_scaled, X_test_df_trimmed_scaled, y_train_df_trimmed_scaled, y_test_df_trimmed_scaled = train_test_split(X_df_trimmed_scaled, y_df_trimmed_scaled, test_size = 0.30, random_state = 0, stratify = y_df_trimmed_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model_1 = LogisticRegression(max_iter=1000)\r\nmodel_2 = LogisticRegression(max_iter=1000)\r\nmodel_3 = LogisticRegression(max_iter=1000)\r\nmodel_4 = LogisticRegression(max_iter=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_1 = model_1.fit(X_train_df, y_train_df)\r\nres_2 = model_2.fit(X_train_df_scaled_all, y_train_df_scaled_all)\r\nres_3 = model_3.fit(X_train_df_trimmed, y_train_df_trimmed)\r\nres_4 = model_4.fit(X_train_df_trimmed_scaled, y_train_df_trimmed_scaled)\r\n\r\ny_predict_1 = model_1.predict(X_test_df)\r\ny_predict_2 = model_2.predict(X_test_df_scaled_all)\r\ny_predict_3 = model_3.predict(X_test_df_trimmed)\r\ny_predict_4 = model_4.predict(X_test_df_trimmed_scaled)\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance comparison","metadata":{}},{"cell_type":"code","source":"def score_function(y_pred, y_test):\r\n\r\n    Acc = accuracy_score(y_test, y_pred)\r\n    Pre = precision_score(y_test, y_pred)\r\n    Rec = recall_score(y_test, y_pred)\r\n    Bal = balanced_accuracy_score(y_test, y_pred)\r\n\r\n    data = pd.DataFrame()\r\n    names = [\"Accuracy\", \"Precision\", \"Recall\", \"Balanced accuracy\"]\r\n    values = [Acc, Pre, Rec, Bal]\r\n    data[\"Names\"] = names\r\n    data['Scores'] = values\r\n\r\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_1 = score_function(y_predict_1, y_test_df)\r\nscores_2 = score_function(y_predict_2, y_test_df_scaled_all)\r\nscores_3 = score_function(y_predict_3, y_test_df_trimmed)\r\nscores_4 = score_function(y_predict_4, y_test_df_trimmed_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [\"Accuracy\", \"Precision\", \"Recall\", \"Balanced accuracy\"]\r\n\r\nresults = pd.DataFrame({\"Names\": names, \"df\": scores_1['Scores'], \"df_scaled_all\":scores_2['Scores'],\r\n                        \"df_trimmed\": scores_3['Scores'], \"df_trimmed_scaled\": scores_4['Scores']})\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.set_index(\"Names\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\r\n\r\n## Confusion matrix\r\n<img src=\"https://i.imgur.com/Xdtufpb.jpg\" width=\"500\">\r\n\r\n\r\n$\\operatorname{accuracy} = \\frac{t_p+t_n}{t_p+t_n+f_p+f_n}$\r\n\r\n$\\operatorname{precision} = \\frac{t_p}{t_p + f_p}$\r\n\r\n$\\operatorname{recall} = \\frac{t_p}{t_p + f_n}$\r\n\r\n$\\operatorname{balanced-accuracy} = \\frac{1}{2}\\left(\\frac{t_p}{t_p + f_n}+\\frac{t_n}{t_n+f_p}\\right)$\r\n\r\n## Summary\r\n+ Standardization of the variables has barely any effect on the results.\r\n+ For the trimmed dataset, there is a slight decrease in accuracy (about 2%), a slightly larger decrease in precision (about 8%).\r\n+ For the trimmed dataset, there is an increase in recall of approximately 6%.\r\n+ Almost zero change in balanced accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Appendix 1: balancing classes\r\n\r\nStrongly unbalanced classes can affect the performance quality of classification algorithms so we will use two solution methods. <br />\r\nYou can read about both of them and many more [here](https://imbalanced-learn.org/stable/index.html).","metadata":{}},{"cell_type":"code","source":"df.Target.value_counts().plot.pie(autopct='%.2f')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trimmed.Target.value_counts().plot.pie(autopct='%.2f')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"df class 0:\", len(df.Target)-df.Target.sum())\r\nprint(\"df_trimmed class 0:\", len(df_trimmed.Target)-df_trimmed.Target.sum())\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Undersampling","metadata":{}},{"cell_type":"code","source":"rus_1 = RandomUnderSampler(sampling_strategy=1)\r\nrus_2 = RandomUnderSampler(sampling_strategy=1)\r\nrus_3 = RandomUnderSampler(sampling_strategy=1)\r\nrus_4 = RandomUnderSampler(sampling_strategy=1)\r\n\r\nX_rus_df, y_rus_df = rus_1.fit_resample(X_df, y_df)\r\nX_rus_df_scaled_all, y_rus_df_scaled_all = rus_2.fit_resample(X_df_scaled_all, y_df_scaled_all)\r\nX_rus_df_trimmed, y_rus_df_trimmed = rus_3.fit_resample(X_df_trimmed, y_df_trimmed)\r\nX_rus_df_trimmed_scaled, y_rus_df_trimmed_scaled = rus_4.fit_resample(X_df_trimmed_scaled, y_df_trimmed_scaled)\r\n\r\n# for df\r\nX_train_df_rus, X_test_df_rus, y_train_df_rus, y_test_df_rus = train_test_split(\r\n    X_rus_df, y_rus_df, test_size=0.30, random_state=0, stratify=y_rus_df)\r\n\r\n# for df_scaled_all\r\nX_train_df_scaled_all_rus, X_test_df_scaled_all_rus, y_train_df_scaled_all_rus, y_test_df_scaled_all_rus = train_test_split(\r\n    X_rus_df_scaled_all, y_rus_df_scaled_all, test_size=0.30, random_state=0, stratify=y_rus_df_scaled_all)\r\n\r\n# for df trimmed\r\nX_train_df_trimmed_rus, X_test_df_trimmed_rus, y_train_df_trimmed_rus, y_test_df_trimmed_rus = train_test_split(\r\n    X_rus_df_trimmed, y_rus_df_trimmed, test_size=0.30, random_state=0, stratify=y_rus_df_trimmed)\r\n\r\n# for df trimmed and scaled\r\nX_train_df_trimmed_scaled_rus, X_test_df_trimmed_scaled_rus, y_train_df_trimmed_scaled_rus, y_test_df_trimmed_scaled_rus = train_test_split(\r\n    X_rus_df_trimmed_scaled, y_rus_df_trimmed_scaled, test_size=0.30, random_state=0, stratify=y_rus_df_trimmed_scaled)\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Oversampling","metadata":{}},{"cell_type":"code","source":"ros_1 = RandomOverSampler(sampling_strategy=1)\r\nros_2 = RandomOverSampler(sampling_strategy=1)\r\nros_3 = RandomOverSampler(sampling_strategy=1)\r\nros_4 = RandomOverSampler(sampling_strategy=1)\r\n\r\nX_ros_df, y_ros_df = ros_1.fit_resample(X_df, y_df)\r\nX_ros_df_scaled_all, y_ros_df_scaled_all = ros_2.fit_resample(X_df_scaled_all, y_df_scaled_all)\r\nX_ros_df_trimmed, y_ros_df_trimmed = ros_3.fit_resample(X_df_trimmed, y_df_trimmed)\r\nX_ros_df_trimmed_scaled, y_ros_df_trimmed_scaled = ros_4.fit_resample(X_df_trimmed_scaled, y_df_trimmed_scaled)\r\n\r\n# for df\r\nX_train_df_ros, X_test_df_ros, y_train_df_ros, y_test_df_ros = train_test_split(\r\n    X_ros_df, y_ros_df, test_size=0.30, random_state=0, stratify=y_ros_df)\r\n\r\n# for df_scaled_all\r\nX_train_df_scaled_all_ros, X_test_df_scaled_all_ros, y_train_df_scaled_all_ros, y_test_df_scaled_all_ros = train_test_split(\r\n    X_ros_df_scaled_all, y_ros_df_scaled_all, test_size=0.30, random_state=0, stratify=y_ros_df_scaled_all)\r\n\r\n# for df trimmed\r\nX_train_df_trimmed_ros, X_test_df_trimmed_ros, y_train_df_trimmed_ros, y_test_df_trimmed_ros = train_test_split(\r\n    X_ros_df_trimmed, y_ros_df_trimmed, test_size=0.30, random_state=0, stratify=y_ros_df_trimmed)\r\n\r\n# for df trimmed and scaled\r\nX_train_df_trimmed_scaled_ros, X_test_df_trimmed_scaled_ros, y_train_df_trimmed_scaled_ros, y_test_df_trimmed_scaled_ros = train_test_split(\r\n    X_ros_df_trimmed_scaled, y_ros_df_trimmed_scaled, test_size=0.30, random_state=0, stratify=y_ros_df_trimmed_scaled)\r\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression for undersampling and oversampling ","metadata":{}},{"cell_type":"markdown","source":"### For Random Undersampling","metadata":{}},{"cell_type":"code","source":"model_1_rus = LogisticRegression(max_iter=1000)\r\nmodel_2_rus = LogisticRegression(max_iter=1000)\r\nmodel_3_rus = LogisticRegression(max_iter=1000)\r\nmodel_4_rus = LogisticRegression(max_iter=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_1_rus = model_1_rus.fit(X_train_df_rus, y_train_df_rus)\r\nres_2_rus = model_2_rus.fit(X_train_df_scaled_all_rus, y_train_df_scaled_all_rus)\r\nres_3_rus = model_3_rus.fit(X_train_df_trimmed_rus, y_train_df_trimmed_rus)\r\nres_4_rus = model_4_rus.fit(X_train_df_trimmed_scaled_rus, y_train_df_trimmed_scaled_rus)\r\n\r\ny_predict_1_rus = model_1_rus.predict(X_test_df_rus)\r\ny_predict_2_rus = model_2_rus.predict(X_test_df_scaled_all_rus)\r\ny_predict_3_rus = model_3_rus.predict(X_test_df_trimmed_rus)\r\ny_predict_4_rus = model_4_rus.predict(X_test_df_trimmed_scaled_rus)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_1_rus = score_function(y_predict_1_rus, y_test_df_rus)\r\nscores_2_rus = score_function(y_predict_2_rus, y_test_df_scaled_all_rus)\r\nscores_3_rus = score_function(y_predict_3_rus, y_test_df_trimmed_rus)\r\nscores_4_rus = score_function(y_predict_4_rus, y_test_df_trimmed_scaled_rus)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [\"Accuracy\", \"Precision\", \"Recall\", \"Balanced accuracy\"]\r\n\r\nresults_rus = pd.DataFrame({\"Names\": names, \"df_rus\": scores_1_rus['Scores'], \"df_scaled_all_rus\": scores_2_rus['Scores'],\r\n                          \"df_trimmed_rus\": scores_3_rus['Scores'], \"df_trimmed_scaled_rus\": scores_4_rus['Scores']})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_rus.set_index(\"Names\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For Random Oversampling","metadata":{}},{"cell_type":"code","source":"model_1_ros = LogisticRegression(max_iter=1000)\r\nmodel_2_ros = LogisticRegression(max_iter=1000)\r\nmodel_3_ros = LogisticRegression(max_iter=1000)\r\nmodel_4_ros = LogisticRegression(max_iter=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_1_ros = model_1_ros.fit(X_train_df_ros, y_train_df_ros)\r\nres_2_ros = model_2_ros.fit(X_train_df_scaled_all_ros, y_train_df_scaled_all_ros)\r\nres_3_ros = model_3_ros.fit(X_train_df_trimmed_ros, y_train_df_trimmed_ros)\r\nres_4_ros = model_4_ros.fit(X_train_df_trimmed_scaled_ros, y_train_df_trimmed_scaled_ros)\r\n\r\ny_predict_1_ros = model_1_ros.predict(X_test_df_ros)\r\ny_predict_2_ros = model_2_ros.predict(X_test_df_scaled_all_ros)\r\ny_predict_3_ros = model_3_ros.predict(X_test_df_trimmed_ros)\r\ny_predict_4_ros = model_4_ros.predict(X_test_df_trimmed_scaled_ros)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_1_ros = score_function(y_predict_1_ros, y_test_df_ros)\r\nscores_2_ros = score_function(y_predict_2_ros, y_test_df_scaled_all_ros)\r\nscores_3_ros = score_function(y_predict_3_ros, y_test_df_trimmed_ros)\r\nscores_4_ros = score_function(y_predict_4_ros, y_test_df_trimmed_scaled_ros)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [\"Accuracy\", \"Precision\", \"Recall\", \"Balanced accuracy\"]\r\n\r\nresults_ros = pd.DataFrame({\"Names\": names, \"df_ros\": scores_1_ros['Scores'], \"df_scaled_all_ros\": scores_2_ros['Scores'],\r\n                            \"df_trimmed_ros\": scores_3_ros['Scores'], \"df_trimmed_scaled_ros\": scores_4_ros['Scores']})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_ros.set_index(\"Names\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions II\r\n\r\nFor Random Undersampling:\r\n+ Standardizing the variables for the original database (df) has positive effects.\r\n+ Standardizing the variables for the trimmed database (df_trimmed) has negative effects.\r\n+ I do not find the effect of using Undersampling to be satisfying.\r\n\r\nFor Random Oversampling:\r\n+ Standardization of variables in both cases has positive effects.\r\n+ The df has higher Accuracy, Precision and Bananced accuracy values, but lower Recall from the trimmed dataset.\r\n\r\nIn this case, Oversampling is better than Undersampling.<br />\r\nStatistical treatments seem to have little effect on the final performance of the logistic regression classifier.","metadata":{}},{"cell_type":"markdown","source":"# Final Words\r\n\r\nIf anyone has suggestions on how to do something in the loops or improve it - feel free to write.","metadata":{}}]}