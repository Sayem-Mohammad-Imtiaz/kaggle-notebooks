{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"## Imports (code & data)\nimport re\nimport pandas as pd\nimport yake_helper_funcs as yhf\nfrom datetime import datetime, timedelta\nfrom math import sqrt, floor\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\nimport itertools\nfrom matplotlib import pyplot as plt\nimport removing_polite_posts as rpp\nfrom flashtext.keyword import KeywordProcessor\nimport string\nimport nltk\nimport math\n\nforum_posts = pd.read_csv(\"../input/meta-kaggle/ForumMessages.csv\")\n\n# read in pre-tuned vectors\nvectors = pd.read_csv(\"../input/fine-tuning-word2vec-2-0/kaggle_word2vec.model\", \n                      delim_whitespace=True,\n                      skiprows=[0], \n                      header=None\n                     )\n\n# set words as index rather than first column\nvectors.index = vectors[0]\nvectors.drop(0, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"## Utility functions\n\n# get vectors for each word in post\n# TODO: can we vectorize this?\ndef vectors_from_post(post):\n    all_words = [] \n\n    for words in post:\n        all_words.append(words) \n        \n    return(vectors[vectors.index.isin(all_words)])\n\n\n# create document embeddings from post\ndef doc_embed_from_post(post):\n    test_vectors = vectors_from_post(post)\n\n    return(test_vectors.mean())\n\n# explore our posts by cluster\ndef get_keyword_set_by_cluster(number):\n    cluster_index = list(clustering.labels_ == number)\n    return(list(itertools.compress(keyword_sets, cluster_index)))\n\n# get sample post info by #\ndef get_post_info_by_cluster(number, \n                             data,\n                             cluster):\n    return(data[cluster.labels_ == number])\n\n# remove HTML stuff\n# https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\ndef remove_html_tags(text):\n    clean = re.compile('<.*?>')\n    return(re.sub(clean, '', text))\n\n# remove \"good\", \"nice\", \"thanks\", etc\ndef remove_thanks(text):\n    text = text.lower()\n    \n    text = re.sub(\"nice\", \"\", text)\n    text = re.sub(\"thank.*\\s\", \" \", text)\n    text = re.sub(\"good\",\"\", text)\n    text = re.sub(\"hi\", \"\", text)\n    text = re.sub(\"hello\", \"\", text)\n    \n    return(text)\n\ndef polite_post_index(forum_posts):\n    '''Pass in a list of fourm posts, get\n    back the indexes of short, polite ones.'''\n    \n    polite_indexes = []\n    \n    # create  custom stop word list to identify polite forum posts\n    stop_word_list = [\"no problem\", \"thanks\", \"thx\", \"thank\", \"great\",\n                      \"nice\", \"interesting\", \"awesome\", \"perfect\", \n                      \"amazing\", \"well done\", \"good job\"]\n\n    # create a KeywordProcess\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keywords_from_list(stop_word_list)\n\n    # test our keyword processor\n    for i,post in enumerate(forum_posts):\n        post = post.lower().translate(str.maketrans({a:None for a in string.punctuation}))\n        \n        if len(post) < 100:\n            keywords_found = keyword_processor.extract_keywords(post.lower(), span_info=True)\n            if keywords_found:\n                polite_indexes.append(i)\n\n    return(polite_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyperprameters\n\n# number of clusters currently based on the square root of the # of posts\ndays_of_posts = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing posts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For sample posts, get forum title and topic title\n# based on queries from https://www.kaggle.com/pavlofesenko/strategies-to-earn-discussion-medals\ntopics = pd.read_csv('../input/meta-kaggle//ForumTopics.csv').rename(columns={'Title': 'TopicTitle'})\nforums = pd.read_csv('../input/meta-kaggle/Forums.csv').rename(columns={'Title': 'ForumTitle'})\n\ndf1 = pd.merge(forum_posts[['ForumTopicId', 'PostDate', 'Message']], topics[['Id', 'ForumId', 'TopicTitle']], left_on='ForumTopicId', right_on='Id')\ndf1 = df1.drop(['ForumTopicId', 'Id'], axis=1)\n\nforum_posts = pd.merge(df1, forums[['Id', 'ForumTitle']], left_on='ForumId', right_on='Id')\nforum_posts = forum_posts.drop(['ForumId', 'Id'], axis=1)\nforum_posts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parse dates\nforum_posts['Date'] = pd.to_datetime(forum_posts.PostDate, format=\"%m/%d/%Y %H:%M:%S\")\n\n# posts from the last X days\nstart_time = datetime.now() + timedelta(days=-days_of_posts)  \n\n# forum posts from last week (remember to convert to str)\nsample_post_info = forum_posts.loc[forum_posts.Date > start_time]\nsample_posts = sample_post_info.Message.astype(str)\n\n# reindex from 0\nsample_posts.reset_index(drop=True)\nsample_post_info.reset_index(drop=True)\n\n# remove html tags\nsample_post_info.Message = sample_post_info.Message\\\n    .astype(str)\\\n    .apply(remove_html_tags)\nsample_posts = sample_posts.apply(remove_html_tags)\n\n# remove polite posts (make sure you remove HTML tags first)\npolite_posts = sample_posts.index[polite_post_index(sample_posts)]\n# posts aren't being dropped \nsample_posts = sample_posts.drop(polite_posts)\nsample_post_info = sample_post_info.drop(polite_posts)\n\n# number of posts\nnum_of_posts = sample_posts.shape[0]\n\n# Number of clusters is square root of the # of posts (rounded down)\nnumber_clusters = floor(sqrt(num_of_posts))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# extact keywords & tokenize\n#keywords = yhf.keywords_yake(sample_posts, )\nkeywords_tokenized = yhf.tokenizing_after_YAKE(sample_posts)\nkeyword_sets = [set(post) for post in keywords_tokenized]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get word vectors for keywords in post"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create empty array for document embeddings\ndoc_embeddings = np.zeros([num_of_posts, 300])\n\n# get document embeddings for posts\nfor i in range(num_of_posts):\n    embeddings = np.array(doc_embed_from_post(keyword_sets[i]))\n    if np.isnan(embeddings).any():\n        doc_embeddings[i,:] = np.zeros([1,300])\n    else:\n        doc_embeddings[i,:] = embeddings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the default k-means label assignment didn't work well\nclustering = SpectralClustering(n_clusters=number_clusters, \n                                assign_labels=\"discretize\",\n                                n_neighbors=number_clusters).fit(doc_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at distrobution of cluster labels\npd.Series(clustering.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(number_clusters):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, \n                                   data = sample_post_info,\n                                   cluster = clustering))\n    print(\"\\n\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(number_clusters):\n    \n#     print(f\"Cluster {i}:\\n\")\n#     print(get_keyword_set_by_cluster(i))\n#     print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Refining clustering\n\nSteps:\n\n1. Drop empty clusters\n2. Identify large clusters (2 times more than expected)\n3. Recluster those clusters (# clusters = sqrt # posts)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count of posts/cluster\ncluster_counts = pd.Series(clustering.labels_).value_counts()\n\n# get clusters bigger than expected\nmax_cluster_size = number_clusters * 2\nbig_clusters = cluster_counts[cluster_counts > max_cluster_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub-cluster first (biggest) cluster\ncluster_label = big_clusters.index[0]\n\nsub_sample = sample_post_info[clustering.labels_ == cluster_label]\nsub_cluster_embeddings = doc_embeddings[clustering.labels_ == cluster_label]\n\nnumber_sub_clusters = floor(sqrt(sub_sample.shape[0]))\n\nsub_cluster = SpectralClustering(n_clusters=number_sub_clusters, \n                                 assign_labels=\"discretize\", \n                                 n_neighbors=number_sub_clusters).fit(sub_cluster_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see how it looks\nfor i in range(number_sub_clusters):\n\n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, data = sub_sample, \n                                   cluster = sub_cluster))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(sub_cluster.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word clouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import path\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO why do I see thank you?\nposts_as_string = sample_post_info\\\n    .Message\\\n    .to_string(index=False)\n\n# shouldn't have to do this b/c I removed polite posts earlier\nposts_as_string = remove_thanks(posts_as_string)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(posts_as_string)\n\n# Display the generated image:\n# the matplotlib way:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Going forward\n\nBiggest problem: redundent clusters\n\nPossible solutions: \n\n* Remove very short posts\n* Don't include posts on kernels\n* Build filter for removing short \"thanks!\" type posts\n* Start w/ sentiment analys & put all very high sentiment posts in a single bin"},{"metadata":{},"cell_type":"markdown","source":"# Visualization brain storming\n\nSlides on text visualizatoin: https://courses.cs.washington.edu/courses/cse512/15sp/lectures/CSE512-Text.pdf\n\n* Bigram based method, reporting the two terms with the median freuquency\n* term saliency, normalize by freq of most common term log(tf_w) / log(tf_the) (and then some sort of regression?)\n* Termite-based model: Topics as columns, terms as rows and weight visualiation of term distinctivenes as KL divergence p(T|term)/p(T|any_term)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# next week: get saliency measure by day, \n# look at shift between sliency on day & in corpus as whole pick summary words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: \n# make sure to match preprocessing (lower cased)\n# for each cluster, find the nomralized saliencey measure \n# rank words based on difference in normalizd saliency in whole corpus\n\n# edge cases:\n# OOV words, add smoothing or set corpus freq. to 0\n# ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"frequency_table = pd.read_csv(\"../input/kaggle-forum-term-frequency-unstemmed/kaggle_lex_freq.csv\",\n                             error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cluster_saliency_dict(cluster_number):\n    # create corpus from a cluster\n    text = get_post_info_by_cluster(cluster_number, data = sub_sample, cluster = sub_cluster)\\\n        .Message.astype(str).str.cat(sep=' ')\n\n    # tokenize\n    words = nltk.word_tokenize(text)\n\n    # Remove single-character tokens (mostly punctuation)\n    words = [word for word in words if len(word) > 1]\n\n    # Remove numbers\n    words = [word for word in words if not word.isnumeric()]\n\n    # remove non-breaking space\n    words = [word for word in words if word != \"nbsp\"]\n\n    # Lowercase all words (default_stopwords are lowercase too)\n    words = [word.lower() for word in words]\n\n    # Calculate frequency distribution\n    fdist = nltk.FreqDist(words)\n\n    cluster_dict = dict() \n\n    # get saliency measures\n    for word, frequency in fdist.most_common():\n        saliency_measure_smoothed = math.log(frequency + 0.0001)/(math.log(fdist.most_common(1)[0][1] + 0.0001))\n        cluster_dict[word] = saliency_measure_smoothed\n        \n    return(cluster_dict, fdist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_surprising_words(cluster_number, frequency_table):\n    cluster_dict, fdist = get_cluster_saliency_dict(cluster_number)\n    \n    words = []\n    surprisal = []\n\n    for word, freq in fdist.most_common():\n        words.append(word)\n        surprisal_measure = cluster_dict[word] - frequency_table.saliency[frequency_table.word == word]\n        if surprisal_measure.empty:\n            surprisal.append(cluster_dict[word] - .0001)\n        else:\n            surprisal.append(surprisal_measure.values[0])\n\n    cluster_surprisal_measures = pd.DataFrame(list(zip(words, surprisal)), \n                                              columns =['Words', 'Surprisal']) \n\n    suprising_words = cluster_surprisal_measures.Words[cluster_surprisal_measures.Surprisal > 0]\n    \n    return(suprising_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_surprising_words(1, frequency_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_post_info_by_cluster(1, data = sub_sample, cluster = sub_cluster).Message","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_surprising_words(0, frequency_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_post_info_by_cluster(0, data = sub_sample, cluster = sub_cluster).Message","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}