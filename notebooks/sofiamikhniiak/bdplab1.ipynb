{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_15 = pd.DataFrame(pd.read_csv(\"/kaggle/input/world-happiness/2015.csv\"))\nmilitary =  pd.DataFrame(pd.read_csv(\"/kaggle/input/military-expenditure-of-countries-19602019/Military Expenditure.csv\"))\nhdi = pd.DataFrame(pd.read_csv(\"/kaggle/input/human-development-index-hdi/HDI.csv\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"military.columns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"military_15 = military[['Name','2015']].set_index('Name')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"military_15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_15 = happiness_15.set_index(\"Country\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hdi\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hdi = hdi.set_index(\"Country\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"countries = list(set(hdi.index) & set(military_15.index) & set(happiness_15.index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"countries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mil = list(set(military_15.index) - set(countries))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"military_15 = military_15.drop(mil).rename(columns={\"2015\":\"Military Expenditure\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"military_15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hap = list(set(happiness_15.index) - set(countries))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_15 = happiness_15.drop(hap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = list(set(hdi.index) - set(countries))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hdi = hdi.drop(h)[['HDI Rank', 'HDI', 'Mean years of schooling', 'Gross national income (GNI) per capita', 'GNI per capita rank minus HDI rank','Gender Development Index Group',\n       'Human Development Index (HDI) Female',\n       'Human Development Index (HDI) Male', 'Life expectancy at birth Female',\n       'Life expectancy at birth Male', 'Mean years of schooling Female',\n       'Mean years of schooling Male',\n       'Estimated gross national income per capita Female',\n       'Estimated gross national income per capita Male',\n       'Share of seats in parliament (% held by women)', 'Labour force participation rate (% ages 15 and older) Female ',\n       'Total Population (millions) 2015','Population Urban 2015 %', 'Population Under age 5 (millions) 2015',\n       'Population Ages 15–64 (millions) 2015',\n       'Population Ages 65 and older (millions) 2015',\n       'Population Median age (years) 2015',\n       'Dependency Ration Young age (0–14) /(per 100 people ages 15–64)',\n       'Dependency Ratio Old age (65 and older) /(per 100 people ages 15–64)','Infants lacking immunization DTP (% of one-year-olds)',\n       'Infants lacking immunization Measles (% of one-year-olds)','Mortality rates Infant (per 1,000 live births) 2015',\n       'Mortality rates Under-five (per 1,000 live births) 2015','Employment to population ratio (% ages 15 and older) ',\n       'Labour force participation rate (% ages 15 and older)','Total Unemployment (% of labour force) 2015','Mandatory paid maternity leave (days)','Inequality-adjusted HDI (IHDI)',\n       'Inequality-adjusted HDI (IHDI) Over loss(%)',\n       'Difference from HDI rank', 'Coefficient of human inequality', 'Inequality-adjusted life expectancy index',\n       'Inequality in education(%)', 'Inequality-adjusted education index',\n       'Inequality in income (%)', 'Inequality-adjusted income index',]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hdi.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hdi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_happiness = pd.concat([happiness_15, military_15, hdi], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_happiness.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Nan check","metadata":{}},{"cell_type":"code","source":"new_happiness.isna().sum() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"part_one = new_happiness[:len(new_happiness)//2]\npart_two = new_happiness[len(new_happiness)//2 :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(part_one)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(part_two)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"part_two = part_two.fillna(np.mean(part_two))\npart_one = part_one.fillna(np.mean(part_one))\nhappiness = pd.concat([part_one, part_two])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Unique check**","metadata":{}},{"cell_type":"code","source":"len(happiness.index) == len(np.unique(np.asarray(happiness.index)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness = happiness.drop(columns=(\"Region\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(happiness)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(happiness.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_col = sel.fit_transform(happiness)[0]\nold = np.asarray(happiness[:1])[0]\ncols = happiness.columns\nfor i in range(len(cols)):\n    if round(old[i], 2) in new_col:\n        print(\"'\"+str(cols[i])+ \"',\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness2 = happiness[['Happiness Rank',\n'Military Expenditure',\n'HDI Rank',\n'Mean years of schooling',\n'Gross national income (GNI) per capita',\n'GNI per capita rank minus HDI rank',\n'Gender Development Index Group',\n'Life expectancy at birth Female',\n'Life expectancy at birth Male',\n'Mean years of schooling Female',\n'Mean years of schooling Male',\n'Estimated gross national income per capita Female',\n'Estimated gross national income per capita Male',\n'Share of seats in parliament (% held by women)',\n'Labour force participation rate (% ages 15 and older) Female ',\n'Population Urban 2015 %',\n'Population Under age 5 (millions) 2015',\n'Population Ages 65 and older (millions) 2015',\n'Population Median age (years) 2015',\n'Dependency Ration Young age (0–14) /(per 100 people ages 15–64)',\n'Dependency Ratio Old age (65 and older) /(per 100 people ages 15–64)',\n'Infants lacking immunization DTP (% of one-year-olds)',\n'Infants lacking immunization Measles (% of one-year-olds)',\n'Mortality rates Infant (per 1,000 live births) 2015',\n'Mortality rates Under-five (per 1,000 live births) 2015',\n'Employment to population ratio (% ages 15 and older) ',\n'Labour force participation rate (% ages 15 and older)',\n'Total Unemployment (% of labour force) 2015',\n'Mandatory paid maternity leave (days)',\n'Inequality-adjusted HDI (IHDI) Over loss(%)',\n'Difference from HDI rank',\n'Coefficient of human inequality',\n'Inequality in education(%)',\n'Inequality in income (%)']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lab2","metadata":{}},{"cell_type":"markdown","source":"## SVD dimensional reduction","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_transformed = svd.fit_transform(happiness2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness_transformed[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clusterization","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport plotly.express as px\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(happiness_transformed, columns=['Military Expenditure', \"N1\", \"N2\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KMeans(n_clusters=5)\nmodel.fit(df)\n# assign a cluster to each example\nyhat = model.predict(df)\n# retrieve unique clusters\nclusters = np.unique(yhat)\nres = df.copy()\nres[\"Cluster\"] = yhat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_3d(res, x = 'Military Expenditure', \n                    y = 'N1', \n                    z = 'N2',\n                    color=\"Cluster\")\n  \nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"happiness2[happiness2['Military Expenditure'] > 595000000000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = DBSCAN(eps=0.3, min_samples=1).fit(df)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res2 = df.copy()\nres2[\"Cluster\"] = labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_3d(res2, x = 'Military Expenditure', \n                    y = 'N1', \n                    z = 'N2',\n                    color=\"Cluster\")\n  \nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\nclustering = SpectralClustering(n_clusters=10,\n        assign_labels=\"discretize\",\n        random_state=0).fit(df)\nres3 = df.copy()\nres3[\"Cluster\"] = clustering.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_3d(res3, x = 'Military Expenditure', \n                    y = 'N1', \n                    z = 'N2',\n                    color=\"Cluster\")\n  \nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = happiness2[[\n    'HDI Rank',\n'Military Expenditure',\n'Mean years of schooling',\n'Gross national income (GNI) per capita',\n'GNI per capita rank minus HDI rank',\n'Gender Development Index Group',\n'Life expectancy at birth Female',\n'Life expectancy at birth Male',\n'Mean years of schooling Female',\n'Mean years of schooling Male',\n'Estimated gross national income per capita Female',\n'Estimated gross national income per capita Male',\n'Share of seats in parliament (% held by women)',\n'Labour force participation rate (% ages 15 and older) Female ',\n'Population Urban 2015 %',\n'Population Under age 5 (millions) 2015',\n'Population Ages 65 and older (millions) 2015',\n'Population Median age (years) 2015',\n'Dependency Ration Young age (0–14) /(per 100 people ages 15–64)',\n'Dependency Ratio Old age (65 and older) /(per 100 people ages 15–64)',\n'Infants lacking immunization DTP (% of one-year-olds)',\n'Infants lacking immunization Measles (% of one-year-olds)',\n'Mortality rates Infant (per 1,000 live births) 2015',\n'Mortality rates Under-five (per 1,000 live births) 2015',\n'Employment to population ratio (% ages 15 and older) ',\n'Labour force participation rate (% ages 15 and older)',\n'Total Unemployment (% of labour force) 2015',\n'Mandatory paid maternity leave (days)',\n'Inequality-adjusted HDI (IHDI) Over loss(%)',\n'Difference from HDI rank',\n'Coefficient of human inequality',\n'Inequality in education(%)',\n'Inequality in income (%)']][:133]\ny = happiness2['Happiness Rank'][:133]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.4, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nclf = RandomForestRegressor(max_depth=6, random_state=0)\nclf.fit(X, y)\nprint(\"Train score: \", clf.score(X, y))\nprint(\"Test score: \", clf.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(clf.estimators_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# РГР","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbeg = time.perf_counter()\nclf = RandomForestRegressor(max_depth=7, random_state=0, n_jobs=1)\nclf.fit(X, y)\nend = time.perf_counter()\nprint(\"Performance without paralelization: \",end-beg, \"sec\")\nbeg = time.perf_counter()\nclf = RandomForestRegressor(max_depth=7, random_state=0, n_jobs=-1, min_samples_split=3)\nclf.fit(X, y)\nend = time.perf_counter()\nprint(\"Performance with paralelization: \", end-beg, \"sec\")\nprint(\"Train score: \", clf.score(X, y))\nprint(\"Test score: \", clf.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/kepler-labelled-time-series-data/exoTest.csv\")\ntrain = pd.read_csv(\"/kaggle/input/kepler-labelled-time-series-data/exoTrain.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Nan check","metadata":{}},{"cell_type":"code","source":"train.isna().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=(.95))\nres = sel.fit_transform(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = set()\nfor coef, feat in zip(res[0], train.columns): \n    cols.add(feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train.LABEL\ntrain = train.drop(columns=[\"LABEL\"])\ny_test = test.LABEL\ntest = test.drop(columns=[\"LABEL\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beg = time.perf_counter()\nclf = RandomForestClassifier(max_depth=7, random_state=0, n_jobs=None)\nclf.fit(train, y_train)\nend = time.perf_counter()\nprint(\"Performance without parallelization: \",end-beg, \"sec\")\nbeg = time.perf_counter()\nclf = RandomForestClassifier(max_depth=7, random_state=0, n_jobs=-1, min_samples_split=3)\nclf.fit(train, y_train)\nend = time.perf_counter()\nprint(\"Performance with parallelization: \", end-beg, \"sec\")\nprint(\"Train score: \", clf.score(train, y_train))\nprint(\"Test score: \", clf.score(test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install pyspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\ndf = spark.read.csv(\"/kaggle/input/kepler-labelled-time-series-data/exoTrain.csv\", header = True, inferSchema = True)\n#df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tempList = ['label']#Edit01\nfor col in df.columns[1:]:\n    new_name = col.strip()\n    new_name = \"\".join(new_name.split())\n    new_name = new_name.replace('.','') # EDIT\n    tempList.append(new_name) #Edit02\n#print(tempList) #Just for the sake of it #Edit03\n\ndf = df.toDF(*tempList) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, VectorAssembler\n\nstages = []\n\nassembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\nstages += [assembler]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml import Pipeline\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features']\ndf = df.select(selectedCols)\ndf.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nbeg = time.perf_counter()\nrfModel = rf.fit(train)\nend = time.perf_counter()\nprint(\"Performance with pyspark: \", end-beg, \"sec\")\npredictions = rfModel.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator()\nprint(\"Test Log Loss: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"logLoss\"})))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Diff","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.regression import RandomForestRegressor\nfrom multiprocessing.pool import ThreadPool\n\npool = ThreadPool(5)\n\n# hyperparameters to test out (n_trees)\nparameters = [ 10, 20, 50]\n# define a function to train a RF model and return metrics \ndef mllib_random_forest(trees, train, test):\n\n    # train a random forest regressor with the specified number of trees\n    rf = RandomForestRegressor(numTrees = trees, labelCol=\"label\")\n    model = rf.fit(train)\n\n    # make predictions\n    boston_pred = model.transform(test)\n    r = boston_pred.stat.corr(\"features\", \"label\")\n\n    # return the number of trees, and the R value \n    return [trees, r**2]\n  \n# run the tasks \n\npool.map(lambda trees: mllib_random_forest(trees, train, test), parameters)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# load the boston data set\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\n# convert to a Pandas Data Frame\nboston_pd = pd.DataFrame(data= np.c_[boston['data'],boston['target']], \n              columns= np.append(boston['feature_names'], 'target')).sample(frac=1)\nprint(boston_pd.shape)\nboston_pd.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom scipy.stats.stats import pearsonr\n\n# split into data and label arrays \ny = boston_pd['target']\nX = boston_pd.drop(['target'], axis=1)\n\n# create training (~80%) and test data sets\nX_train = X[:400]\nX_test = X[400:]\ny_train = y[:400]\ny_test = y[400:]\n\n# train a classifier \nlr = LinearRegression()\nmodel = lr.fit(X_train, y_train)\n\n# make predictions\ny_pred = model.predict(X_test)\n\n# error metrics\nr = pearsonr(y_pred, y_test)\nmae = sum(abs(y_pred - y_test))/len(y_test)\nprint(\"R-sqaured: \" + str(r[0]**2))\nprint(\"MAE: \" + str(mae))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\n\n# convert to a Spark data frame\nboston_sp = spark.createDataFrame(boston_pd)\ndisplay(boston_sp.take(5))\n\n# split into training and test spark data frames\nboston_train = spark.createDataFrame(boston_pd[:400])\nboston_test = spark.createDataFrame(boston_pd[400:])\n\n# convert to vector representation for MLlib\nassembler = VectorAssembler(inputCols= boston_train.schema.names[:(boston_pd.shape[1] - 1)],  \n                                                                        outputCol=\"features\" )\nboston_train = assembler.transform(boston_train).select('features', 'target') \nboston_test = assembler.transform(boston_test).select('features', 'target') \n\ndisplay(boston_train.take(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\n\n# split into training and test spark data frames\nboston_train = df = spark.read.csv(\"/kaggle/input/kepler-labelled-time-series-data/exoTrain.csv\", header = True, inferSchema = True)\nboston_test = df = spark.read.csv(\"/kaggle/input/kepler-labelled-time-series-data/exoTest.csv\", header = True, inferSchema = True)\n\ntempList = []#Edit01\nfor col in boston_train.columns:\n    new_name = col.strip()\n    new_name = \"\".join(new_name.split())\n    new_name = new_name.replace('.','') # EDIT\n    tempList.append(new_name) #Edit02\n#print(tempList) #Just for the sake of it #Edit03\n\nboston_train = boston_train.toDF(*tempList) \n\ntempList = []#Edit01\nfor col in boston_test.columns:\n    new_name = col.strip()\n    new_name = \"\".join(new_name.split())\n    new_name = new_name.replace('.','') # EDIT\n    tempList.append(new_name) #Edit02\n#print(tempList) #Just for the sake of it #Edit03\n\nboston_test = boston_test.toDF(*tempList) \n\n# convert to vector representation for MLlib\nassembler = VectorAssembler(inputCols= boston_train.schema.names[1:train.shape[1]],  \n                                                                        outputCol=\"features\" )\n\n\nboston_train = assembler.transform(boston_train).select('features', 'LABEL') \nboston_test = assembler.transform(boston_test).select('features', 'LABEL') \n\ndisplay(boston_train.take(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.regression import LinearRegression\n\n# linear regression \nlr = LinearRegression(maxIter=10, regParam=0.1, \n                      elasticNetParam=0.5, labelCol=\"target\")\n\n# Fit the model\nmodel = lr.fit(boston_train)\nboston_pred = model.transform(boston_test)\n\n# calculate results \nr = boston_pred.stat.corr(\"prediction\", \"target\")\nprint(\"R-sqaured: \" + str(r**2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ncrossval = CrossValidator(estimator=LinearRegression(labelCol = \"target\"),  \n                         estimatorParamMaps=ParamGridBuilder().addGrid(\n                           LinearRegression.elasticNetParam, [0, 0.5, 1.0]).build(),\n                         evaluator=RegressionEvaluator(\n                           labelCol = \"target\", metricName = \"r2\"),\n                         numFolds=10)\n\n# cross validate the model and select the best fit\ncvModel = crossval.fit(boston_train) \nmodel = cvModel.bestModel\n\n# calculate results \nboston_pred = model.transform(boston_test)\nr = boston_pred.stat.corr(\"prediction\", \"target\")\nprint(\"R-sqaured: \" + str(r**2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sklearn version \nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom multiprocessing.pool import ThreadPool\n\n# allow up to 5 concurrent threads\npool = ThreadPool(5)\n\n# hyperparameters to test out (n_trees)\nparameters = [ 10, 20, 50]\n\n# define a function to train a RF model and return metrics \ndef sklearn_random_forest(trees, X_train, X_test, y_train, y_test):\n\n    # train a random forest regressor with the specified number of trees\n    rf= RFR(n_estimators = trees)\n    model = rf.fit(X_train, y_train)\n\n    # make predictions\n    y_pred = model.predict(X_test)\n    r = pearsonr(y_pred, y_test)\n\n    # return the number of trees, and the R value \n    return [trees, r[0]**2]  \n\n# run the tasks \npool.map(lambda trees: sklearn_random_forest(trees, X_train,\n                                           X_test, y_train, y_test), parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spark version\nfrom pyspark.ml.regression import RandomForestRegressor\n\n# define a function to train a RF model and return metrics \ndef mllib_random_forest(trees, boston_train, boston_test):\n\n    # train a random forest regressor with the specified number of trees\n    rf = RandomForestRegressor(numTrees = trees, labelCol=\"LABEL\")\n    model = rf.fit(boston_train)\n\n    # make predictions\n    boston_pred = model.transform(boston_test)\n    r = boston_pred.stat.corr(\"prediction\", \"LABEL\")\n\n    # return the number of trees, and the R value \n    return [trees, r**2]\n  \n# run the tasks \npool.map(lambda trees: mllib_random_forest(trees, boston_train, boston_test), parameters)\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql.types import *\n\n# setup the spark data frame as a table\nboston_sp.createOrReplaceTempView(\"boston\")\n\n# add train/test label and expand the data set by 3x (each num trees parameter)\nfull_df = spark.sql(\"\"\"\n  select *\n  from (\n    select *, case when rand() < 0.8 then 1 else 0 end as training \n    from boston\n  ) b\n  cross join (\n      select 11 as trees union all select 20 as trees union all select 50 as trees)\n\"\"\")\n\nschema = StructType([StructField('trees', LongType(), True),\n                     StructField('r_squared', DoubleType(), True)])  \n\n@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\ndef train_RF(boston_pd):\n    trees = boston_pd['trees'].unique()[0]\n\n    # get the train and test groups \n    boston_train = boston_pd[boston_pd['training'] == 1]\n    boston_test = boston_pd[boston_pd['training'] == 0] \n        \n    # create data and label groups \n    y_train = boston_train['target']\n    X_train = boston_train.drop(['target'], axis=1)\n    y_test = boston_test['target']\n    X_test = boston_test.drop(['target'], axis=1)\n   \n    # train a classifier \n    rf= RFR(n_estimators = trees)\n    model = rf.fit(X_train, y_train)\n\n    # make predictions\n    y_pred = model.predict(X_test)\n    r = pearsonr(y_pred, y_test)\n    \n    # return the number of trees, and the R value \n    return pd.DataFrame({'trees': trees, 'r_squared': (r[0]**2)}, index=[0])\n  \n# use the Pandas UDF\nresults = full_df.groupby('trees').apply(train_RF)\n\n# print the results \nprint(results.take(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}