{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/life-expectancy-who/Life Expectancy Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the dimension of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is not too big.\n\nLet's take a look at what the data tells us.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there are 2 features which are categorical in nature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's also see if there are any missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now while this gives the missing values, it's not that good when it comes to readability.\nAnd as Data Scientists, we should also be careful that our code looks clean and readable.\nSo, let's make a function that throws a more clean readable output.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values(df):\n    missing=pd.DataFrame(df.isnull().sum()/len(data))*100\n    missing.columns = ['missing_values(%)']\n    missing['missing_values(numbers)'] = pd.DataFrame(df.isnull().sum())\n    return missing.sort_values(by='missing_values(%)', ascending=False)\nmissing_values(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmmm, looks clean enough now let's take care of these missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's rename the column names, because they have spaces between the words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming some column names as they contain trailing spaces.\ndata.rename(columns={\" BMI \":\"BMI\",\"Life expectancy \":\"Life_Expectancy\",\"Adult Mortality\":\"Adult_Mortality\",\n                   \"infant deaths\":\"Infant_Deaths\",\"percentage expenditure\":\"Percentage_Exp\",\"Hepatitis B\":\"HepatitisB\",\n                  \"Measles \":\"Measles\",\"under-five deaths \":\"Under_Five_Deaths\",\"Diphtheria \":\"Diphtheria\",\n                  \" HIV/AIDS\":\"HIV/AIDS\",\" thinness  1-19 years\":\"thinness_1to19_years\",\" thinness 5-9 years\":\"thinness_5to9_years\",\"Income composition of resources\":\"Income_Comp_Of_Resources\",\n                   \"Total expenditure\":\"Tot_Exp\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now taking care of missing values is always a pickle, because we'd be uncertain of choosing the values that are going to replace missing values. \nNow that depends upon the data.\n\nIf the feature that has missing values also has outliers, it's better to replace missing values with ***`median()`***.\nIf there are no outliers then go ahead and use ***`mean()`*** of the feature to replace missing values.\nNow while this holds good for numerical data, in categorical data we can maybe use ***`mode()`*** of that feature to fill the missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for label,content in data.items():\n    if pd.isnull(content).sum():\n        data[label] = content.fillna(content.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if we have successfully replaced missing values with ***`median()`***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beautiful, all our missing values have been taken care of.\n\nNow let's take care of categorical features.\nWhile there are multipe ways of taking care of categorical features, I've decided to use **`pd.get_dummies()`** function from **`pandas`** library. \nLet's see how that works.\n\nThere are only two categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.get_dummies(data, columns=['Country','Status'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at our data now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, looks good.\nNow let's divide the data into X & y, so that we can split the data into training & test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Life_Expectancy', axis=1)\ny = data['Life_Expectancy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's split our model into training & test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything look's good.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now here comes the dilemma, choosing the right estimator. But lucky for us **scikit-learn** is kind enough to provide us with a map, that'll help us choose the right estimator.\nYou can find it [here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now I'll first try with **`GradientBoostingRegressor`**\nBut first let's import our evaluation metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\ngbr_pred = gbr.predict(X_test)\nprint('R2 score is : {:.2f}'.format(r2_score(y_test, gbr_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad. But wait a minute, we've not yet checked for outliers.\n\nBut first, let's make a copy of our dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\noutliers = pd.DataFrame(((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum())\noutliers1= outliers[:60]\noutliers2 = outliers[60:120]\noutliers3 = outliers[120:180]\noutliers4 = outliers[180:]\noutliers1,outliers2,outliers3,outliers4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow ! Almost all of the features have outliers.\n\nSo instead of taking care of these outliers, let's use `RandomForestRegressor`, since they're really robust and immune to outliers. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred=rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R2 score is : {:.2f}'.format(r2_score(y_test, rf_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This did really better compared to `GradientBoostingRegressor`.\n\nNow let's take a look at important features according to our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Woaahhh ! That looks overwhelming.\n\nLet's try and visualize it, so that we can understand it better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's look at the top 10 features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=10):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_features(X_train.columns, rf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmmm, top 10 features according to our model are : \n                                                    \n`'HIV/AIDS',\n'Income_Comp_Of_Resources',\n'Adult_Mortality',\n'BMI',\n'Under_Five_Deaths',\n'Schooling',\n'thinness_5to9_years',\n'Year',\n'Alcohol',\n'thinness_1to19_years'`.\n                                                    \nSo let's just use these 10 features and see if the model still works good.\n\nBecause as a data scientist, we should always look at ways to cut down computational costs. And that can happen when you reduce the dimension of your data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = data[['HIV/AIDS','Adult_Mortality','Income_Comp_Of_Resources','Schooling',\n      'BMI','thinness_5to9_years','Under_Five_Deaths','Infant_Deaths',\n      'thinness_1to19_years','Year']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(new_data,y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's re-train our model on this new data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred_new = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's evaluate on new data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R2 score is : {:.2f}'.format(r2_score(y_test, rf_pred_new)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Worked like a charm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred_new = pd.DataFrame(rf_pred_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred_new.to_csv('predictions.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}