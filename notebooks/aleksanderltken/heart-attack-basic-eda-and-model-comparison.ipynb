{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"########################## Import libraries ###################################\n\n#loading dataset\nimport pandas as pd\nimport numpy as np\n#visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#EDA\nimport pandas_profiling as pp\n#! pip install -q scikit-plot\nimport scikitplot as skplt\n\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\n# data splitting\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# data modeling\nfrom tpot import TPOTClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve\nfrom sklearn.linear_model import LogisticRegression, ElasticNet\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# data interpretation\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## Load data ##########################################\n\ndf = pd.read_csv('../input/health-care-data-set-on-heart-attack-possibility/heart.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick check\nprint(df.info(), \"\\n\",\"\\n\", \"Head(5): \", \"\\n\", df.head(), \"\\n\",\"\\n\", \"Describe: \", \"\\n\", df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for NaN values\ndf.isna().sum()\n# Another option: df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## EDA ################################################\n\n# Pandas profiling report\npp.ProfileReport(df).to_file('heart_attack.html')\n# Gives an easy interpretation of the df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countplot for imbalance in target\nsns.countplot(df['target'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countplot for imbalance in gender\nsns.countplot(df['sex'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age distribution\nfig, ax = plt.subplots(figsize=(10,5))\nsns.distplot(df['age'])\nplt.xlim([0,80])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Chol distribution\nfig, ax = plt.subplots(figsize=(10,5))\nsns.distplot(df['chol'])\nplt.xlim([0,600])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trestbps distribution\nfig, ax = plt.subplots(figsize=(10,5))\nsns.distplot(df['trestbps'])\nplt.xlim([0,250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot\nsns.pairplot(data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pearson Correlation Heatmap\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################## Prepare data #######################################\n\n# Seperating the target from the features\nX = df.copy(deep = True)\ny = X.pop('target')\n\n# Train, test, split the data with test sze at 20% and random state = 0\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \n                                                    random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is there imbalance in the training dataset? No\nprint('Original dataset shape %s' % Counter(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################### Machine Learning models################################\n\n#1\nm1 = 'Logistic Regression'\nLogReg = LogisticRegression()\nmodel = LogReg.fit(X_train, y_train)\nLogReg_predict = LogReg.predict(X_test)\nLogReg_conf_matrix = confusion_matrix(y_test, LogReg_predict)\nLogReg_acc_score = accuracy_score(y_test, LogReg_predict)\nprint(\"confussion matrix\", \"\\n\", LogReg_conf_matrix, \"\\n\", \n      \"Accuracy of Logistic Regression:\",LogReg_acc_score*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2\nm2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\", \"\\n\", nb_conf_matrix, \"\\n\",\n      \"Accuracy of Naive Bayes:\",nb_acc_score*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3\nm3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=500, random_state=12, max_depth=10)\nrf.fit(X_train,y_train)\nrf_predict = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predict)\nrf_acc_score = accuracy_score(y_test, rf_predict)\nprint(\"confussion matrix\", \"\\n\", rf_conf_matrix,\"\\n\", \n      \"Accuracy of Random Forest:\",rf_acc_score*100)\n\n\n###\nimport scikitplot as skplt\nfeature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n                    'restecg', 'thalach', 'exang', 'oldpeak',\n                    'slope', 'ca', 'thal']\nskplt.estimators.plot_feature_importances(rf, feature_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4\nm4 = 'AdaBoostClassifier'\nparam_grid2 = [{'n_estimators': list(range(1,20)), 'learning_rate': list(np.arange(0.1,2,0.1))}] # params to try in the grid search\nclfAB = AdaBoostClassifier()\nad = GridSearchCV(clfAB, param_grid2, cv=5, verbose=1, return_train_score = True, n_jobs = -1)\nad.fit(X_train,y_train)\nad_predict = ad.predict(X_test)\nad_conf_matrix = confusion_matrix(y_test, ad_predict)\nad_acc_score = accuracy_score(y_test, ad_predict)\nprint(\"confussion matrix\", \"\\n\", ad_conf_matrix,\"\\n\", \n      \"Accuracy of AdaBoost:\",ad_acc_score*100)\n\n###\nprint(ad.best_params_)\n#how should we expect this to do based on the validation scores?\nprint('''best score = {:.2f}'''.format(ad.best_score_))\n###","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#5\nm5 = 'DecisionTreeClassifier'\nparam_grid1 = [{'max_depth': list(range(1,9))}] # params to try in the grid search\nclfDT = DecisionTreeClassifier()\ndt = GridSearchCV(clfDT, param_grid1, cv=5, verbose=1, return_train_score = True)\ndt.fit(X_train,y_train)\ndt_predict = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predict)\ndt_acc_score = accuracy_score(y_test, dt_predict)\nprint(\"confussion matrix\", \"\\n\", dt_conf_matrix,\"\\n\", \n      \"Accuracy of Decision Tree:\",dt_acc_score*100)\n\n###\nprint(dt.best_params_)\n#how should we expect this to do based on the validation scores?\nprint('''best score = {:.2f}'''.format(dt.best_score_))\n###\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#6\nm6 = 'TPOT'\nTPOT = TPOTClassifier(generations=7, population_size=60, verbosity=2, n_jobs = -1)\nTPOT.fit(X_train,y_train)\nTPOT_predict = TPOT.predict(X_test)\nTPOT_conf_matrix = confusion_matrix(y_test, TPOT_predict)\nTPOT_acc_score = accuracy_score(y_test, TPOT_predict)\nprint(\"confusion matrix\", \"\\n\", TPOT_conf_matrix,\"\\n\", \n      \"Accuracy of TPOT:\",TPOT_acc_score*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#7\nm7 = 'SupportVectorMachineClassifier'\nparam_grid3 = [{'kernel': ['rbf','sigmoid'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}]\nsvc = GridSearchCV(SVC(), param_grid3, cv=5,return_train_score = True)\nsvc.fit(X_train,y_train)\nsvc_predict = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predict)\nsvc_acc_score = accuracy_score(y_test, svc_predict)\nprint(\"confusion matrix\", \"\\n\", svc_conf_matrix,\"\\n\", \n      \"Accuracy of Support Vector Machine:\",svc_acc_score*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#8\nm8 = 'ElasticNet'\nen_alpha = 0.1\nen_l1ratio = 0.5\nen = ElasticNet(alpha = en_alpha, l1_ratio = en_l1ratio)\nen.fit(X_train,y_train)\nen_predict = en.predict(X_test)\nen_predict = np.where(en_predict>0.5,1,0)\nen_conf_matrix = confusion_matrix(y_test, en_predict)\nen_acc_score = accuracy_score(y_test, en_predict)\nprint(\"confusion matrix\", \"\\n\", en_conf_matrix,\"\\n\", \n      \"Accuracy of Elastic Net:\",en_acc_score*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################### Model results #################################\n\n# Barplot of the accuracy score\ncolors = [\"orange\", \"green\", \"magenta\", \"red\", \"blue\" , \"grey\", \"yellow\", \"purple\"]\nacc = [LogReg_acc_score,nb_acc_score,rf_acc_score,ad_acc_score,dt_acc_score,\n       TPOT_acc_score,svc_acc_score,en_acc_score]\nm = [m1,m2,m3,m4,m5,m6,m7,m8]\nplt.figure(figsize=(18,4))\nplt.yticks(np.arange(0,100,10))\nplt.title(\"barplot Represent Accuracy of different models\")\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot( y=acc,x=m, palette=colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy score for the models\nmodelscore = [[\"Accuracy of Logistic Regression:\",LogReg_acc_score*100],\n              [\"Accuracy of Naive Bayes:\",nb_acc_score*100],\n              [\"Accuracy of Random Forest:\",rf_acc_score*100],\n              [\"Accuracy of AdaBoost:\",ad_acc_score*100],\n              [\"Accuracy of Decision Tree:\",dt_acc_score*100],\n              [\"Accuracy of TPOT:\",TPOT_acc_score*100],\n              [\"Accuracy of Support Vector Machine:\",svc_acc_score*100],\n              [\"Accuracy of Elastic Net:\",en_acc_score*100]]\nfor m,ms in modelscore:\n    print(m,ms)\n\n#The model with the best accuracy score in our case turned out to be the\n# adaboost model with a score of 86,88%. Lets have a closer look at the\n# confusion matrixes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for the models\nconfusion_matrix = [[\"Confusion matrix for Logistic Regression:\", \"\\n\",LogReg_conf_matrix],\n                    [\"Confusion matrix for Naive Bayes:\", \"\\n\",nb_conf_matrix],\n                    [\"Confusion matrix for Random Forest:\", \"\\n\",rf_conf_matrix],\n                    [\"Confusion matrix for AdaBoost:\", \"\\n\",ad_conf_matrix],\n                    [\"Confusion matrix for Decision Tree:\", \"\\n\",dt_conf_matrix],\n                    [\"Confusion matrix for TPOT:\", \"\\n\",TPOT_conf_matrix],\n                    [\"Confusion matrix for Support Vector Machine:\", \"\\n\",svc_conf_matrix],\n                    [\"Confusion matrix for Elastic Net:\", \"\\n\", en_conf_matrix]]\nfor m,tab,cm in confusion_matrix:\n    print(m,tab,cm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# My interpretation of the models:\n# I want my model to be make as many correct predicions as possible. It is also\n# important to be aware of where the model fails. For the Adaboost model (w/high accuracy):\n# It had 30 TP and 23 TN. There were 4 instances in the FP. This is not too bad\n# as we predicted heart attack and it turns out they dont got it. For FN,there \n# is 4 instances. This means we predicted no chance for heart attack but\n# they still got it. We want to minize this as much as possible.\n\n\n# 1. One could make changes to the threshold, and test if it would make any \n# difference on a valuation dataset.\n\n# 2. Choose another model: We could for example choose the support vector\n# machine model which had a lower accuracy score, but fewer FN (more FP).\n\n\n# Visualization of confusion matrix\ny_pred_model = [[ad_predict, 'AdaBoost'], [svc_predict, 'Support Vector Machine']]\nfor y_pred_i, m_name in y_pred_model:\n    skplt.metrics.plot_confusion_matrix(y_test,y_pred_i, figsize=(4,4), title=(m_name))\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One could also try to balance out the small imbalance in the dataset to see if\n# it would make any improvement. An option could be using SMOTE,which will use a\n# syntethic upsampling method. We could also use class_weight as a way to deal\n# with imbalance.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}