{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Firstly, import and extract to get important data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/videogamesales/vgsales.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization data to get information\n#Import library to plotting data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot 10 publisher that most contribute to global sales. The best one is Electronic Art publisher.\ncontri=data.groupby('Publisher')['Global_Sales'].count().sort_values(ascending=False).head(10)\nmy_range=range(0,len(contri.index))\nplt.hlines(y=contri.index, xmin=0, xmax=contri.values, color='skyblue')\nplt.plot(contri.values, my_range, \"o\")\nplt.xlabel('Global Sales')\nplt.title('Top 10 Publisher Contribute Global Sales', pad=20, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot top 10 games that most selled in global.\ngames=data.groupby('Name')['Global_Sales'].count().sort_values(ascending = False).head(10)\nsns.barplot(x=games.values,y=games.index, palette='YlGn_r')\nplt.xlabel('Global Sales')\nplt.ylabel('Game Names')\nplt.title('Top 10 Games Contribute Global Sales', pad=20, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot genre games selled in global\nimport matplotlib\ngames=data.groupby('Genre')['Global_Sales'].count().head(10)\ncmap = matplotlib.cm.PuBu\nmini=min(games)\nmaxi=max(games)\nnorm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\ncolors = [cmap(norm(value)) for value in games]\nplt.figure(figsize=(9,7))\nplt.pie(games, labels=games.index, colors = colors, autopct='%.1f%%')\nplt.legend(loc = \"upper right\", bbox_to_anchor=(0.60, 0.5, 0.60, 0.5))\nplt.title('Top 10 Game Genres Contribute Global Sales', pad=20, fontsize=15)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find correlation between feature. In this, we get NA_Sales, EU_Sales, JP_Sales, and Other_Sales get much correlation with global sales\n\ncor = data.corr()\ncor = pd.DataFrame(cor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cor, vmin=-0.5, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since region sales high correlate to global sales, we plot it to know where highest region correlation.\ndata[[\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\"]].sum().plot(x=\"Global_Sales\", kind=\"barh\", colormap='gnuplot')\nplt.xlabel('Global Sales', fontsize=10)\nplt.ylabel('Region Sales', fontsize=10)\nplt.title('Region of Global Sales', pad=20, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Prediction Global Sales**"},{"metadata":{},"cell_type":"markdown","source":"Prediction global sales based on regional data (Uni Eropa, North America, Japan, and other countries sales) using regression model because target data in number format and we don't use simple linear regression since have 4 feature are affecting global sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make cluster data\nX = data.iloc[:, 7:11].values\ny = data.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=['LR','SVR','DT','RFR']\ntr=[]\nts=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hierarchical Clustering Model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr = lr.fit(X_train, y_train)\ny_predlr = lr.predict(X_test)\ntr.append(lr.score(X_train, y_train))\nts.append(lr.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", lr.score(X_train, y_train))\nprint(\"Testing Accuracy :\", lr.score(X_test, y_test))\nprint(r2_score(y_test,y_predlr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 4)\nX_poly = poly_reg.fit_transform(X)\nlr_2 = LinearRegression()\nlr_2.fit(X_poly, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy :\", lr_2.score(X_poly, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nsvr = SVR(kernel = 'rbf')\nsvr.fit(X_train, y_train)\ny_predsvr = svr.predict(X_test)\ntr.append(svr.score(X_train, y_train))\nts.append(svr.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", svr.score(X_train, y_train))\nprint(\"Testing Accuracy :\", svr.score(X_test, y_test))\nprint(r2_score(y_test,y_predsvr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state = 0)\ndtr.fit(X_train, y_train)\ny_preddtr = dtr.predict(X_test)\ntr.append(dtr.score(X_train, y_train))\nts.append(dtr.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", dtr.score(X_train, y_train))\nprint(\"Testing Accuracy :\", dtr.score(X_test, y_test))\nprint(r2_score(y_test,y_preddtr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\nrfr.fit(X_train, y_train)\ny_predrfr = rfr.predict(X_test)\ntr.append(rfr.score(X_train, y_train))\nts.append(rfr.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", rfr.score(X_train, y_train))\nprint(\"Testing Accuracy :\", rfr.score(X_test, y_test))\nprint(r2_score(y_test,y_predrfr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nr = Ridge()\nr = r.fit(X_train,y_train)\ny_predr = r.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", r.score(X_train, y_train))\nprint(\"Testing Accuracy :\", r.score(X_test, y_test))\nprint(r2_score(y_test,y_predr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nl = Lasso()\nl = l.fit(X_train,y_train)\ny_predl = l.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", l.score(X_train, y_train))\nprint(\"Testing Accuracy :\", l.score(X_test, y_test))\nprint(r2_score(y_test,y_predl))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nenet = ElasticNet()\nenet.fit(X_train,y_train)\ny_predEnet = enet.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Accuracy :\", enet.score(X_train, y_train))\nprint(\"Testing Accuracy :\", enet.score(X_test, y_test))\nprint(r2_score(y_test,y_predEnet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Train ve Test')\nplt.plot(clf,tr,color='magenta',label='train data')\nplt.plot(clf,ts,color='darkolivegreen',label='test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# From testing, Elastic Net is the best model for predict. It's accuracy score is 0.88 and the others have overfitting and underfitting."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}