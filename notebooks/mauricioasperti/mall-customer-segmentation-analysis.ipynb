{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction: Business Goal & Problem Definition\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nThe goal of this project is to identify, study and analyze a Mall´s clients clusters, so the business can have a better understanding of its customers segmentations and adapt different marketing strategies to each of them, increasing the commerce´s revenue. For that we´ll use the Mall Customer Segmentation dataset available in Kaggle, containing 200 customers. Each customer has the following attributes:\n\n* Gender\n* Age\tAnnual\n* Income (k$)\n* Spending Score (1-100)"},{"metadata":{},"cell_type":"markdown","source":"# 2. Importing Basic Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"mall_ds = pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv', encoding='latin1', sep=\",\")\n\nmall_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preliminary Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nmall_ds.sample(n=10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking dataset info by feature\n\nmall_ds.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of zeros in rows\n\n(mall_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(mall_ds==0).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of duplicated rows\n\nmall_ds.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking basic statistical data by feature\n\nmall_ds.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Cleaning\n\n    We´ll perform the following:\n\n    1. Create a calculated column (Spending Score / Annual Income) that could be potentially important to the model\n\n    2. Convert categorical variables (Gender) to dummies\n    \n    * No missing, zero or invalid values to treat\n    * No duplications found\n    * No outliers found"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\n\nmall_ds[\"spending_score_to_annual_score_ratio\"] = mall_ds[\"Spending Score (1-100)\"] / mall_ds[\"Annual Income (k$)\"] #feature engineering\n\n#2\n\nmall_ds = pd.concat([mall_ds, pd.get_dummies(mall_ds[\"Gender\"])], axis=1) #genre dummy coding\n\nmall_ds.to_excel(\"mall_ds_clean.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nfig.suptitle(\"Gender Frequency\", fontsize=15)\nmall_ds[\"Gender\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmall_ds[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Age Distribution\", fontsize=15)\nsns.distplot(mall_ds[\"Age\"], ax=ax[0])\nsns.boxplot(mall_ds[\"Age\"], ax=ax[1])\nsns.violinplot(mall_ds[\"Age\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Annual Income (k$) Distribution\", fontsize=15)\nsns.distplot(mall_ds[\"Annual Income (k$)\"], ax=ax[0])\nsns.boxplot(mall_ds[\"Annual Income (k$)\"], ax=ax[1])\nsns.violinplot(mall_ds[\"Annual Income (k$)\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Spending Score (1-100) Distribution\", fontsize=15)\nsns.distplot(mall_ds[\"Spending Score (1-100)\"], ax=ax[0])\nsns.boxplot(mall_ds[\"Spending Score (1-100)\"], ax=ax[1])\nsns.violinplot(mall_ds[\"Spending Score (1-100)\"], ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Correlations Analysis & Features Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting not relevant and original categorical columns\n\nmall_ds2 = mall_ds.drop([\"CustomerID\", \"Gender\"], axis=1)\n\n#Plotting a Heatmap\n\nfig, ax = plt.subplots(1, figsize=(25,25))\nsns.heatmap(mall_ds2.corr(), annot=True, fmt=\",.2f\")\nplt.title(\"Heatmap Correlation\", fontsize=20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n#Plotting a Pairplot\n\nsns.pairplot(mall_ds2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Data Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining Xs\n\nX_orig = mall_ds\nX = mall_ds2\n\n#Scaling all features\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Machine Learning Algorithms Implementation & Assessment"},{"metadata":{},"cell_type":"markdown","source":"# 9.1.1 K-means"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a K-means model and checking its Metrics\n\nfrom sklearn.cluster import KMeans\n\n#Applying the Elbow Method to calculate distortion for a range of number of cluster\n\ndistortions = []\nfor i in range(1, 21):\n    km = KMeans(n_clusters=i, init=\"random\", n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    km.fit(X_scaled)\n    distortions.append(km.inertia_)\n\n#Plotting\n\nplt.plot(range(1, 21), distortions, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Distortion\")\nplt.show()\n\n#Applying the Silhouette Method to interpret and validate of consistency within clusters of data\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_coefficients = []\nfor j in range(2, 21):\n    km = KMeans(n_clusters=j, init=\"random\", n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    km.fit(X_scaled)\n    score = silhouette_score(X_scaled, km.labels_)\n    silhouette_coefficients.append(score)\n\n#Plotting\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(2, 21), silhouette_coefficients)\nplt.xticks(range(2, 21))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n\n#Choosing number of clusters\n\nn_clusters = 3\nprint('Estimated number of clusters: %d' % n_clusters)\nkm = KMeans(n_clusters=n_clusters)\nkm.fit(X_scaled)\nprint(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X_scaled, km.fit(X_scaled).labels_))\n\n#Plotting chosen number of clusters\n\nfrom yellowbrick.cluster import silhouette_visualizer\nsilhouette_visualizer(KMeans(n_clusters=n_clusters, random_state=0), X_scaled)\n\n#Visualizing clusters in the dataset\nX_orig = pd.DataFrame(X_orig)\nX_orig[\"cluster\"] = km.labels_\nX_orig.to_excel(\"model_km.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.1.2 Clusters exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cluster 0\")\nX_orig.query(\"cluster == 0\").describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cluster 1\")\nX_orig.query(\"cluster == 1\").describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cluster 2\")\nX_orig.query(\"cluster == 2\").describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, len(X_orig[\"cluster\"].unique()))\nfig.suptitle(\"Gender Frequency\", fontsize=15)\nX_orig.query(\"cluster == 0\")[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[0])\nX_orig.query(\"cluster == 1\")[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nX_orig.query(\"cluster == 2\")[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[2])\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1, len(X_orig[\"cluster\"].unique()))\nfig.suptitle(\"Age Distribution\", fontsize=15)\nsns.distplot(X_orig.query(\"cluster == 0\")[\"Age\"], label = \"Cluster 0\", ax=ax[0])\nsns.distplot(X_orig.query(\"cluster == 1\")[\"Age\"], label = \"Cluster 1\", ax=ax[1])\nsns.distplot(X_orig.query(\"cluster == 2\")[\"Age\"], label = \"Cluster 2\",ax=ax[2])\n\nfig, ax = plt.subplots(1, len(X_orig[\"cluster\"].unique()))\nfig.suptitle(\"Annual Income (k$) Distribution\", fontsize=15)\nsns.distplot(X_orig.query(\"cluster == 0\")[\"Annual Income (k$)\"], label = \"Cluster 0\", ax=ax[0])\nsns.distplot(X_orig.query(\"cluster == 1\")[\"Annual Income (k$)\"], label = \"Cluster 1\", ax=ax[1])\nsns.distplot(X_orig.query(\"cluster == 2\")[\"Annual Income (k$)\"], label = \"Cluster 2\",ax=ax[2])\n\nfig, ax = plt.subplots(1, len(X_orig[\"cluster\"].unique()))\nfig.suptitle(\"Spending Score (1-100) Distribution\", fontsize=15)\nsns.distplot(X_orig.query(\"cluster == 0\")[\"Spending Score (1-100)\"], label = \"Cluster 0\", ax=ax[0])\nsns.distplot(X_orig.query(\"cluster == 1\")[\"Spending Score (1-100)\"], label = \"Cluster 1\", ax=ax[1])\nsns.distplot(X_orig.query(\"cluster == 2\")[\"Spending Score (1-100)\"], label = \"Cluster 2\",ax=ax[2])\n\nfig, ax = plt.subplots(1, len(X_orig[\"cluster\"].unique()))\nfig.suptitle(\"Spending score ratio Distribution\", fontsize=15)\nsns.distplot(X_orig.query(\"cluster == 0\")[\"spending_score_to_annual_score_ratio\"], label = \"Cluster 0\", ax=ax[0])\nsns.distplot(X_orig.query(\"cluster == 1\")[\"spending_score_to_annual_score_ratio\"], label = \"Cluster 1\", ax=ax[1])\nsns.distplot(X_orig.query(\"cluster == 2\")[\"spending_score_to_annual_score_ratio\"], label = \"Cluster 2\",ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Plotting scatter graph per pair features\n\n# #Mapping every individual cluster to a color\n\n# colors = ['goldenrod', 'olive', 'navy']\n\n# vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n\n# #Plotting\n\n# for i in range(0, X_scaled.shape[1]):\n#     for j in range(1, X_scaled.shape[1]):\n#         plt.scatter(X_scaled.iloc[:,i], X_scaled.iloc[:,j])\n#         plt.xlabel(X.columns[i])\n#         plt.ylabel(X.columns[j])\n#         plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.2 DBSCAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a DBSCAN model and checking its Metrics\n#OBS: we´re exploring DBSCAN only as a study exercise in this project - we´ll adopt K-Means\n\nfrom sklearn.neighbors import NearestNeighbors\n\n#We can calculate the distance from each point to its closest neighbour using the NearestNeighbors. The point itself is included in n_neighbors. The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points\n\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(X_scaled)\ndistances, indices = nbrs.kneighbors(X_scaled)\n\n#Soring and plotting results\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.xlabel(\"Distances to the closest n_neighbors\")\nplt.ylabel(\"eps\")\nplt.show()\n\nfrom sklearn.cluster import DBSCAN\n\n#Selecting the best eps (the optimal value for epsilon will be found at the point of maximum curvature)\n\ndbs = DBSCAN(eps=0.8)\ndbs.fit(X_scaled)\n\n#The labels_ property contains the list of clusters and their respective points\n\nclusters = dbs.labels_\n\nfrom sklearn import metrics\n\n#Number of clusters in labels, ignoring noise (outlier) (-1) if present\n\nn_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\nn_noise_ = list(clusters).count(-1)\nprint('Estimated number of clusters: %d' % n_clusters)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X_scaled, clusters))\n\n#Visualizing clusters in the dataset\nX_orig = pd.DataFrame(X_orig)\nX_orig[\"cluster\"] = dbs.labels_\nX_orig.to_excel(\"model_dbs.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this exercise we went through all the process from collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting a clustering model, indicating groups of customers with similarities to explored, as explained below, so the Mall can have a better understanding of its customers segmentations according and adapt different marketing strategies to each of them, bringing more revenue and market share to the business.\n\nFirst group of clients:\nthe first group is formed by men, average of 42 years old, with the highest annual income and the lowest spending score of all groups. This group has the biggest potential of all to grow so the Mall should adopt specific strategies to adapt this group´s profile and explore its huge probability on growing in more sophisticated items for men.\n\nSecond group of clients:\nthe second group if formed by women, average of 40 years old, with a considerable annual income and also a low spending score ratio (meaning a low spending when comparing to the income). This is the group with the second highest potential to grow, so also here the Mall should invest in strategies to offer more sophisticated items for women.\n\nThird group of clients:\nthe third group os formed by a mix of men and women, much younger with an average of 24 years old, and also a much lower annual income, but with the highest spending score of all (4.5x of group 1 and 4.25x of group 2), meaning they represent the most meaningful part of our business today. This is a group with lower opportunities to grow but it needs to be kept as crucial for the business continuity,"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}