{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the Dataset","metadata":{}},{"cell_type":"code","source":"path=\"../input/sms-spam-collection-dataset/spam.csv\"\ndata=pd.read_csv(path,encoding='latin-1')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Information about data","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df=data[[\"v1\",'v2']].copy()\ndf.rename(columns={'v1':'Class','v2':'sms'},inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=df.Class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The dataset is imbalanced ","metadata":{}},{"cell_type":"markdown","source":"# Text preprocessing","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp=spacy.load('en')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"df[\"tokens\"]=df.sms.apply(lambda x: nlp(x))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lammatization after removing stopwords and punctuation","metadata":{}},{"cell_type":"code","source":"def stopword(txt):\n    l=[]\n    for tokens in txt:\n        if not tokens.is_stop and not tokens.is_punct:\n            l.append(tokens.lemma_.strip().lower())\n    return l        \ndf['Lmnt_text']=df.tokens.apply(stopword) \ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Corpus of Lammatized text","metadata":{}},{"cell_type":"code","source":"def final_corpus(lmt):\n    return (' '.join(lmt))\ndf['final_corpus']=df.Lmnt_text.apply(final_corpus)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Cloud","metadata":{}},{"cell_type":"code","source":"text = \" \".join(r for r,s in zip(df.final_corpus.astype(str),df.Class) if s == 'ham')\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 1200, height = 1200,\n                background_color ='white',contour_width=1, contour_color='green',\n                min_font_size = 20).generate(text)\nplt.figure(figsize=[15,20])\nplt.title(\"HAM WORD CLOUD\")\nplt.axis(\"off\")\nplt.imshow(wordcloud,interpolation='bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Frequently occuring words in Ham messages","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nf=Counter(text.split())\nprint(f.most_common(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(r for r,s in zip(df.final_corpus.astype(str),df.Class) if s == 'spam')\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 1200, height = 1200,\n                background_color ='black',contour_width=1, contour_color='green',\n                min_font_size = 20).generate(text)\nplt.figure(figsize=[15,20])\nplt.title(\"SPAM WORD CLOUD\")\nplt.axis(\"off\")\nplt.imshow(wordcloud,interpolation='bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Frequently occuring word in spam message","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nf=Counter(text.split())\nprint(f.most_common(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization","metadata":{}},{"cell_type":"markdown","source":"# TF-IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(use_idf=False)\nX= tfidf.fit_transform(df.final_corpus).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CountVectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nx = cv.fit_transform(df.final_corpus).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, df.Class, test_size=0.30, random_state=42)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(x, df.Class, test_size=0.30, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"Model=pd.DataFrame({\"Model\":[],\"Accuracy\":[],\"Vectorizer\":[]})\nModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression().fit(X_train1,y_train1)\nscore=lr.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Logistic Regression\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nlr=LogisticRegression().fit(X_train2,y_train2)\nscore=lr.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Logistic Regression\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\ndt= tree.DecisionTreeClassifier().fit(X_train1, y_train1)\nscore=dt.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Decision Tree\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\ndt=tree.DecisionTreeClassifier().fit(X_train2,y_train2)\nscore=dt.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Decision Tree\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf= RandomForestClassifier(random_state=42).fit(X_train1, y_train1)\nscore=rf.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Random Forest\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nrf=RandomForestClassifier(random_state=42).fit(X_train2,y_train2)\nscore=rf.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Random Forest\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-nearest Neighbours","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfor r in range(1,20,2):\n    knn = KNeighborsClassifier(n_neighbors=r).fit(X_train1, y_train1)\n    print(r,knn.score(X_test1,y_test1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3).fit(X_train1, y_train1)\nscore=knn.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"KNN\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nknn = KNeighborsClassifier(n_neighbors=3).fit(X_train2,y_train2)\nscore=knn.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"KNN\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"## For tfidf","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB,GaussianNB,ComplementNB,BernoulliNB\nmnb = MultinomialNB().fit(X_train1, y_train1).score(X_test1,y_test1)\ngnb = GaussianNB().fit(X_train1,y_train1).score(X_test1,y_test1)\ncnb = ComplementNB().fit(X_train1,y_train1).score(X_test1,y_test1)\nbnb = BernoulliNB().fit(X_train1,y_train1).score(X_test1,y_test1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_bayes=pd.DataFrame({\"Classifier\":[\"GaussianNB\",\"MultinomialNB\",\"ComplementNB\",\"BernoulliNB\"],\n                          \"Score\":[gnb,mnb,cnb,bnb]})\nModel=Model.append({\"Model\":\"Naive Bayes\",\"Accuracy\":mnb,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nnaive_bayes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For CountVector","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB,GaussianNB,ComplementNB,BernoulliNB\nmnb = MultinomialNB().fit(X_train2, y_train2).score(X_test2,y_test2)\ngnb = GaussianNB().fit(X_train2,y_train2).score(X_test2,y_test2)\ncnb = ComplementNB().fit(X_train2,y_train2).score(X_test2,y_test2)\nbnb = BernoulliNB().fit(X_train2,y_train2).score(X_test2,y_test2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naive_bayes1=pd.DataFrame({\"Classifier\":[\"GaussianNB\",\"MultinomialNB\",\"ComplementNB\",\"BernoulliNB\"],\n                          \"Score\":[gnb,mnb,cnb,bnb]})\nnaive_bayes1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multinomial Naive Bayes yields better result","metadata":{}},{"cell_type":"code","source":"Model=Model.append({\"Model\":\"Naive Bayes\",\"Accuracy\":mnb,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\nfor r in ['linear' , 'poly', 'rbf', 'sigmoid']:\n    sv=svm.SVC(kernel=r).fit(X_train1,y_train1).score(X_test1,y_test1)\n    print(r,sv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nfor r in ['linear' , 'poly', 'rbf', 'sigmoid']:\n    sv=svm.SVC(kernel=r).fit(X_train2,y_train2).score(X_test2,y_test2)\n    print(r,sv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nsv=svm.SVC(kernel='linear').fit(X_train1,y_train1)\nscore=sv.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"SVM\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nsv = svm.SVC(kernel=\"linear\").fit(X_train2,y_train2)\nscore=sv.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"SVM\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Gradient Descent","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfor r in [ 'hinge', 'log', 'modified_huber']:\n    sgd=SGDClassifier(loss=r).fit(X_train1,y_train1).score(X_test1,y_test1)\n    print(r,sgd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfor r in [ 'hinge', 'log', 'modified_huber']:\n    sgd=SGDClassifier(loss=r).fit(X_train2,y_train2).score(X_test2,y_test2)\n    print(r,sgd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd=SGDClassifier(loss='hinge').fit(X_train1,y_train1)\nscore=sgd.score(X_test1,y_test1)\nModel=Model.append({\"Model\":\"Stochastic Gradient Descent\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nsgd = SGDClassifier(loss=\"hinge\").fit(X_train2,y_train2)\nscore=sgd.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"Stochastic Gradient Descent\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost ","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nxg=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,use_label_encoder=True).fit(X_train1, y_train1)\nscore=xg.score(X_test1,y_test1) \nModel=Model.append({\"Model\":\"XGBoost\",\"Accuracy\":score,\"Vectorizer\":\"tfidf\"},ignore_index=True)\nxg=xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42,use_label_encoder=True).fit(X_train2, y_train2)\nscore=xg.score(X_test2,y_test2)\nModel=Model.append({\"Model\":\"XGBoost\",\"Accuracy\":score,\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Artificial neural network ","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel=keras.Sequential([\n            layers.Dense(units=6, activation='relu'), \n            layers.Dense(units=6, activation='relu'),  \n            layers.Dense(units=1, activation='sigmoid')])  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\noptimizer='adamax',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop=keras.callbacks.EarlyStopping(\npatience=10,\nmin_delta=0.01,\nrestore_best_weights=True)\n\nhistory=model.fit(\n    X_train2,y_train2.replace({\"ham\":0,\"spam\":1}),\n    validation_data=(X_test2,y_test2.replace({\"ham\":0,\"spam\":1})),\n    batch_size=32,\n    epochs=100,\n    callbacks=[stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model=Model.append({\"Model\":\"Artificial Neural Network\",\"Accuracy\":history_df['val_binary_accuracy'].max(),\"Vectorizer\":\"CountVector\"},ignore_index=True)\nModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel=keras.Sequential([\n            layers.Dense(units=6, activation='relu'), \n            layers.Dense(units=6, activation='relu'),  \n            layers.Dense(units=1, activation='sigmoid')])\nmodel.compile(\noptimizer='adamax',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'])\nstop=keras.callbacks.EarlyStopping(\npatience=10,\nmin_delta=0.01,\nrestore_best_weights=True)\n\nhistory=model.fit(\n    X_train1,y_train1.replace({\"ham\":0,\"spam\":1}),\n    validation_data=(X_test1,y_test1.replace({\"ham\":0,\"spam\":1})),\n    batch_size=32,\n    epochs=100,\n    callbacks=[stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model=Model.append({\"Model\":\"Artificial Neural Network\",\"Accuracy\":history_df['val_binary_accuracy'].max(),\"Vectorizer\":\"tfidf\"},ignore_index=True)\nModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Selection of State of The Art Model","metadata":{}},{"cell_type":"code","source":"Model.sort_values(by=['Accuracy'],ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Model.groupby([\"Vectorizer\",'Model']).min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Gradient Descent algorithm with count vectorization of corpus yields maximum of accuracy","metadata":{}},{"cell_type":"markdown","source":"# Building State Of The Art Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfinal_model=SGDClassifier(loss='hinge')\nfinal_model.fit(X_train2,y_train2)\nfinal_model.score(X_test2,y_test2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test2, final_model.predict(X_test2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\ndisp = plot_confusion_matrix(final_model, X_test2, y_test2,\n                                 display_labels=['ham','spam'],\n                                 cmap=plt.cm.Blues)\ndisp.ax_.set_title('SGDClassifier')\nprint(disp.confusion_matrix)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the trained model for future use","metadata":{}},{"cell_type":"code","source":"import pickle\nfilename = 'finalized_model.sav'\npickle.dump(final_model, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the pipeline for the prediction","metadata":{}},{"cell_type":"markdown","source":"# Load the Model","metadata":{}},{"cell_type":"code","source":"load_model = pickle.load(open(filename, 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp=spacy.load('en')\ndef prediction(text):\n    doc=nlp(text)\n    lt=[]\n    for tokens in doc:\n        if not tokens.is_stop and not tokens.is_punct:\n            lt.append(tokens.lemma_.strip().lower())\n    corpus= ' '.join(lt) \n    f_vct = cv.transform([corpus]).toarray()\n    pred=final_model.predict(f_vct)[0]\n    return pred\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"text=\"Free tones Hope you enjoyed your new content\"\noutput=prediction(text)\nprint(f\"The sms is {output}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}