{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Steps for Regression Modelling:</b>\n\n1. <b>Business Problem Definition</b> - How to predict Red Wine Quality based on Attributes with defined factors\n2. <b>Convert business problem</b> Into statistical problem  sales = F( attributes, product features, marketing info etc.)\n3. <b>Finding the right technique</b> - Since it is predicting value (Regression Problem) problem so we can use OLS as one of the technique. We can also use other Machine Learning techniques like Decision Trees, Ensemble learning, KNN, SVM, ANN etc.\n4. <b>Data colletion(Y, X)</b> - Identify the sources of information and collect the data\n5. <b>Consolidate the data</b> - aggregate and consolidate the data at Model level/customer level/store level depends on business problem\n6. <b>Data preparation for modeling</b> (create data audit report to identify the steps to perform as part of data preparation)\n    a. missinmg value treatment\n    b. outlier treatment\n    c. dummy variable creation\n7. Variable creation by using transformation and derived variable creation.\n8. <b>Basic assumptions</b> (Normality, linearity, no outliers, homoscadasticity, no pattern in residuals, no auto correlation etc)\n9. Variable reduction techniques (removing multicollinerity with the help of FA/PCA, correlation matrics, VIF)\n10. Create dev and validation data sets (50:50 if you have more data else 70:30 or 80:20)\n11. Modeling on dev data set (identify significant variables, model interpretation, check the signs and coefficients, multi-collinierity check, measures of good neess fit, final mathematical equation etc)\n12. validating on validation data set (check the stability of model, scoring, decile analysis, cross validation etc.)\n13. Output interpretation and derive insights (understand the limitations of the model and define strategy to implementation)\n14. convert statistical solution into business solutions (implementation, model monitoring etc)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Packages related to general operating system & warnings\nimport os \nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Packages related to data importing, manipulation, exploratory data analysis, data understanding\nimport numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport pandas_profiling\nimport scipy.stats as stats\n\n#Packages related to data visualizaiton\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Setting plot sizes and type of plot\nplt.rc(\"font\", size=14)\nplt.rcParams['axes.grid'] = True\nplt.figure(figsize=(6,3))\nplt.gray()\n\nfrom matplotlib.backends.backend_pdf import PdfPages\n\n#Modules related to split the data & gridsearch\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#Module related to calculation of metrics\nfrom sklearn import metrics\n\n#Module related to VIF \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#Modules related to preprocessing (Imputation of missings, standardiszation, new features creation, converting categorical to numerical)\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n\n#Moudles related to feature selection\nfrom sklearn.feature_selection import RFE, RFECV, SelectKBest, chi2, SelectPercentile, f_classif, mutual_info_classif, f_regression, VarianceThreshold, SelectFromModel, mutual_info_classif, mutual_info_regression, SelectFpr, SelectFdr, SelectFwe\n\n\n#Modules related to pipe line creation for faster processing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n#from sklearn.features.transformers import DataFrameSelector\n\n#Dumping model into current directory: joblib.dump(model_xg,\"my_model.pkl\") \n#Loading model: my_model_loaded=joblib.load(\"my_model.pkl\")\n\n#Modules related key techniques of supervised learning \nimport statsmodels.formula.api as smf\nimport statsmodels.tsa as tsa\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz, export\nfrom sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor \n#from xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import Data\n\nWine_Data = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Wine_Data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identifying Null Values\n\nprint(Wine_Data.isnull().sum())\nprint('Sum of Total Null Values is  {}'.format(sum(Wine_Data.isnull().sum())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Identifying Shape\n\nWine_Data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Wine_Data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Data Analysis using Bar Plot - Each Column in Bar Graph\n\nfor x in Wine_Data.columns:\n    Wine_Data[x].hist()\n    plt.xlabel(str(x))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation - Using Bar Plot\n\nAs per Target Variable - we have more Quality ranges between 5 & 6 (Average Quality) and above 7 Next range (Good Quality)"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Pandas Profiling\n\npandas_profiling.ProfileReport(Wine_Data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Target Variable\n\nWine_Data['quality'] = Wine_Data['quality'].apply(lambda x: 0 if x<7 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Wine_Data.quality.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Wine_Data.quality.value_counts()/Wine_Data.quality.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Target Variable - Value Counts\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\ny_axis = [x for x in Wine_Data.quality.value_counts()]\nax.pie(y_axis,labels=['Bad Quality','Good Quality'],autopct='%1.2f%%')\n\nax.set_title(\"Distribution of Wine Quality\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"quality\", data=Wine_Data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Wine_Data.groupby(by=['quality']).mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Data audit Report\n\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_data =Wine_Data.apply(lambda x: var_summary(x)).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### visualize correlation matrix in Seaborn using a heatmap\n\nsns.heatmap(Wine_Data.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Corr Relationship p>0.05\n\nCorr_Data = Wine_Data.corr()\nCorr_Data.to_csv('Corr_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Changing Column Name \n\nWine_Data.columns = [x.replace(' ','_') for x in Wine_Data.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### variable reduction (feature selection/reduction)\n\n- Univariate Regression\n- WOE - Binomial classification\n- RFE\n- SelectKBest\n- VIF\n- PCA"},{"metadata":{},"cell_type":"markdown","source":"#### Univariate Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"somersd_df = pd.DataFrame()\nfor num_varaible in Wine_Data.columns.difference(['quality']):\n    result = smf.logit(formula= str('quality~')+str(num_varaible),data=Wine_Data).fit()\n    somers_d = 2 * metrics.roc_auc_score(Wine_Data.quality,result.predict())-1\n    temp = pd.DataFrame([num_varaible,somers_d]).T\n    temp.columns = ['VariableName','SomersD']\n    somersd_df = pd.concat([somersd_df, temp], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_SD = somersd_df.sort_values('SomersD', ascending=False).head(11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_SD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_somerceD = imp_vars_SD.VariableName","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_somerceD = list(imp_vars_somerceD)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Wine_Data[Wine_Data.columns.difference(['quality'])]\n\nclassifier = RandomForestClassifier()\nrfe = RFE(classifier, 11)\nrfe = rfe.fit(X, Wine_Data[['quality']] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_RFE = list(X.columns[rfe.support_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_RFE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Select K-Best"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Wine_Data[Wine_Data.columns.difference(['quality'])]\nSKB = SelectKBest(f_classif, k=11).fit(X, Wine_Data[['quality']] )  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SKB.get_support()\nimp_vars_SKB = list(X.columns[SKB.get_support()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_vars_SKB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Final_list = list(set(imp_vars_SKB + imp_vars_somerceD + imp_vars_RFE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Wine_Data[Final_list]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using WOE (Weight of Evidence)\n\n- Identify important variables using WOE or log(odds) comparing with Y\n- Variable Transformation: (i) Bucketing if the variables are not having linear relationship with log(odds)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bp = PdfPages('WOE Plots.pdf')\n\nfor num_variable in Wine_Data.columns.difference(['quality']):\n    binned = pd.cut(Wine_Data[num_variable], bins=10, labels=list(range(1,11)))\n    #binned = binned.dropna()\n    odds = Wine_Data.groupby(binned)['quality'].sum() / (Wine_Data.groupby(binned)['quality'].count()-Wine_Data.groupby(binned)['quality'].sum())\n    log_odds = np.log(odds)\n    fig,axes = plt.subplots(figsize=(10,4))\n    sns.barplot(x=log_odds.index,y=log_odds)\n    plt.ylabel('Log Odds Ratio')\n    plt.title(str('Logit Plot for identifying if the bucketing is required or not for variable ') + str(num_variable))\n    bp.savefig(fig)\n\nbp.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_woe_iv(dataset, feature, target):\n    lst = []\n    for i in range(dataset[feature].nunique()):\n        val = list(dataset[feature].unique())[i]\n        lst.append({\n            'Value': val,\n            'All': dataset[dataset[feature] == val].count()[feature],\n            'Good': dataset[(dataset[feature] == val) & (dataset[target] == 0)].count()[feature],\n            'Bad': dataset[(dataset[feature] == val) & (dataset[target] == 1)].count()[feature]\n        })\n        \n    dset = pd.DataFrame(lst)\n    dset['Distr_Good'] = dset['Good'] / dset['Good'].sum()\n    dset['Distr_Bad'] = dset['Bad'] / dset['Bad'].sum()\n    dset['WoE'] = np.log(dset['Distr_Good'] / dset['Distr_Bad'])\n    dset = dset.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n    dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n    iv = dset['IV'].sum()\n    \n    dset = dset.sort_values(by='WoE')\n    \n    return dset, iv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in Wine_Data.columns:\n    if col == 'quality': continue\n    else:\n        print('WoE and IV for column: {}'.format(col))\n        df, iv = calculate_woe_iv(Wine_Data, col, 'quality')\n       # print(df)\n        print('IV score: {:.2f}'.format(iv))\n        print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### VIF - Varience Inflation Factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif['VIF Factor'] = [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['features'] = X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the Data into Train & Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = Wine_Data[['quality']]\nfeatures = X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data for sklearn methods\n\ntrain_y, test_y, train_X, test_X = train_test_split(target,features, test_size=0.3, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for logistic regression using statsmodels\n\ntrain, test = train_test_split(Wine_Data, test_size=0.5, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Building\n\n#### Logistic Regression Using Stats Models (Traditional Approach)"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = smf.logit(formula='quality ~ chlorides + free_sulfur_dioxide + fixed_acidity + total_sulfur_dioxide + pH + residual_sugar + citric_acid + volatile_acidity + sulphates + density + alcohol', data=train)\nresult = logreg.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy Deatils"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gini = 2 * metrics.roc_auc_score(train['quality'], result.predict(train)) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2 * metrics.roc_auc_score(test['quality'], result.predict(test)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)\n\ntrain_auc = metrics.roc_auc_score(train['quality'], result.predict(train))\ntest_auc = metrics.roc_auc_score(test['quality'], result.predict(test))\n\nprint(\"The AUC for the model built on the Train Data is : \", train_auc)\nprint(\"The AUC for the model built on the Test Data is : \", test_auc)                                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's - Train\n\ntrain_predicted_prob = pd.DataFrame(result.predict(train))\ntrain_predicted_prob.columns = ['prob']\ntrain_actual = train['quality']\n\n# making a DataFrame with actual and prob columns\ntrain_predict = pd.concat([train_actual, train_predicted_prob], axis=1)\ntrain_predict.columns = ['actual','prob']\ntrain_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's - Test\n\ntest_predicted_prob = pd.DataFrame(result.predict(test))\ntest_predicted_prob.columns = ['prob']\ntest_actual = test['quality']\n\n# making a DataFrame with actual and prob columns\ntest_predict = pd.concat([test_actual, test_predicted_prob], axis=1)\ntest_predict.columns = ['actual','prob']\ntest_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Intuition behind ROC curve - confusion matrix for each different cut-off shows trade off in sensitivity and specificity\n\nroc_like_df = pd.DataFrame()\ntrain_temp = train_predict.copy()\n\nfor cut_off in np.linspace(0,1,50):\n    train_temp['cut_off'] = cut_off\n    train_temp['predicted'] = train_temp['prob'].apply(lambda x: 0.0 if x < cut_off else 1.0)\n    train_temp['tp'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['fp'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['tn'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\n    train_temp['fn'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)\n    sensitivity = train_temp['tp'].sum() / (train_temp['tp'].sum() + train_temp['fn'].sum())\n    specificity = train_temp['tn'].sum() / (train_temp['tn'].sum() + train_temp['fp'].sum())\n    accuracy = (train_temp['tp'].sum()  + train_temp['tn'].sum() ) / (train_temp['tp'].sum() + train_temp['fn'].sum() + train_temp['tn'].sum() + train_temp['fp'].sum())\n    roc_like_table = pd.DataFrame([cut_off, sensitivity, specificity, accuracy]).T\n    roc_like_table.columns = ['cutoff', 'sensitivity', 'specificity', 'accuracy']\n    roc_like_df = pd.concat([roc_like_df, roc_like_table], axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_like_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding ideal cut-off for checking if this remains same in OOS validation\nroc_like_df['total'] = roc_like_df['sensitivity'] + roc_like_df['specificity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_like_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cut-off based on highest sum(sensitivity+specicity)   - common way of identifying cut-off\n\nroc_like_df[roc_like_df['total']==roc_like_df['total'].max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cut-off based on highest accuracy   - some teams use this as methodology to decide the cut-off\n\nroc_like_df[roc_like_df['accuracy']==roc_like_df['accuracy'].max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cut-off based on highest sensitivity\n\nroc_like_df[roc_like_df['sensitivity']==roc_like_df['sensitivity'].max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosen Best Cut-off is 0.53 based on highest (sensitivity+specicity)\n\ntest_predict['predicted'] = test_predict['prob'].apply(lambda x: 1 if x > 0.183673 else 0)\ntrain_predict['predicted'] = train_predict['prob'].apply(lambda x: 1 if x > 0.183673 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The overall accuracy score for the Train Data is : \", metrics.accuracy_score(train_predict.actual, train_predict.predicted))\nprint(\"The overall accuracy score for the Test Data  is : \", metrics.accuracy_score(test_predict.actual, test_predict.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(train_predict.actual, train_predict.predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(test_predict.actual, test_predict.predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decile Analysis\n\n#Decile analysis\n#Top-two deciles - High risk (Low Quality Wine) - will reject \n#3rd,4th, 5th deciles - medium risk (Medium Quality Wine) - will accept wine with proper quality\n#6th decile onwards - low risk Wine - accept the Wine\n\n\n#Decile analysis for validation of models - Business validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predict['Deciles'] = pd.qcut(train_predict['prob'],10,labels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict['Deciles'] = pd.qcut(test_predict['prob'],10,labels=False)\ntest_predict.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decile Analysis for train data\n\nno_1s = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).min().sort_index(ascending=False)['prob']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Decile_analysis_train = pd.concat([max_prob, min_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_train.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decile Analysis for test data\n\nno_1s = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).min().sort_index(ascending=False)['prob']\n\nDecile_analysis_test = pd.concat([max_prob, min_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_test.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Logistic Regression using SkLearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel_Reg = model.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X['pred_prob'] = pd.DataFrame(model_Reg.predict_proba(train_X), index=train_X.index)[1]\ntest_X['pred_prob'] = pd.DataFrame(model_Reg.predict_proba(test_X),index=test_X.index)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train_X, train_y], axis=1)\ntest  = pd.concat([test_X,test_y],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pred'] = np.where(train.pred_prob>0.183673, 1,0)\ntest['pred'] = np.where(test.pred_prob>0.183673, 1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(train.quality, train.pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(train.quality, train.pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(test.quality, test.pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### K-nearest Neighbours","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_data_train = pd.DataFrame(sc.fit_transform(train_X), columns=train_X.columns, index = train_X.index )\nstd_data_test = pd.DataFrame(sc.transform(test_X), columns=test_X.columns, index = test_X.index )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using GridsearchCV with standrardized data\nparam_grid = {'n_neighbors':[3,4,5,6,7],\n              'weights': ['uniform', 'distance']}\n\nmodel = GridSearchCV(KNeighborsClassifier(), param_grid = param_grid, cv=5, scoring = 'f1_weighted')\nmodel_KNN = model.fit(std_data_train, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_KNN.best_score_\nmodel_KNN.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model_KNN.predict(std_data_train)\ntest_pred  = model_KNN.predict(std_data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(train_y,train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(test_y,test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X[train_X.columns.difference(['pred_prob'])]\ntest_X = test_X[test_X.columns.difference(['pred_prob'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'max_depth':np.arange(2,5),\n              'max_features':np.arange(2,5)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = GridSearchCV(DecisionTreeClassifier(),param_grid,cv=5,n_jobs=-1)\ntree.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree- Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = tree.predict(train_X)\ntest_pred  = tree.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(train_y,train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(test_y,test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Final Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X[train_X.columns.difference(['pred_prob'])]\nclf_tree = DecisionTreeClassifier( max_depth = 3, max_features=3, max_leaf_nodes=5 )\nclf_tree.fit( train_X, train_y )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##ROC Curve - Using Decision Tree\n\nprint(metrics.classification_report(train_y, clf_tree.predict(train_X)))\nprint(metrics.classification_report(test_y, clf_tree.predict(test_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_tree.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the selection of the attributes\nimport itertools\nfeature_map = [(i, v) for i, v in itertools.zip_longest(train_X.columns, clf_tree.feature_importances_)]\n\nfeature_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_importance = pd.DataFrame(feature_map, columns=['Feature', 'importance'])\nFeature_importance.sort_values('importance', inplace=True, ascending=False)\nFeature_importance.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XG Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_estimator = XGBClassifier( learning_rate=0.01,\n                               n_estimators=1000,\n                               max_depth=5,\n                               min_child_weight=1,\n                               gamma=1,\n                               subsample=0.8,\n                               colsample_bytree=0.8,\n                               n_jobs=-1,\n                               reg_alpa=1,\n                               scale_pos_weight=1,\n                               random_state=42,\n                               verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_estimator.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(train_y,xgb_estimator.predict(train_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(test_y,xgb_estimator.predict(test_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.roc_auc_score(train_y,pd.DataFrame(xgb_estimator.predict_proba(train_X))[1]))\n\nprint(metrics.roc_auc_score(test_y,pd.DataFrame(xgb_estimator.predict_proba(test_X))[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Naive Bayes Classifier`"},{"metadata":{"trusted":true},"cell_type":"code","source":"Nb_Clf = GaussianNB()\nNb_Clf.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(train_y,Nb_Clf.predict(train_X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(train_y,Nb_Clf.predict(train_X)))\nprint(metrics.classification_report(test_y,Nb_Clf.predict(test_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}