{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T22:14:35.444709Z","iopub.execute_input":"2021-08-20T22:14:35.445069Z","iopub.status.idle":"2021-08-20T22:14:35.451702Z","shell.execute_reply.started":"2021-08-20T22:14:35.445036Z","shell.execute_reply":"2021-08-20T22:14:35.451011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/predict-test-scores-of-students/test_scores.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:14:35.452825Z","iopub.execute_input":"2021-08-20T22:14:35.453275Z","iopub.status.idle":"2021-08-20T22:14:35.500582Z","shell.execute_reply.started":"2021-08-20T22:14:35.453225Z","shell.execute_reply":"2021-08-20T22:14:35.499572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df.isnull())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:14:38.941815Z","iopub.execute_input":"2021-08-20T22:14:38.942301Z","iopub.status.idle":"2021-08-20T22:14:39.480099Z","shell.execute_reply.started":"2021-08-20T22:14:38.942262Z","shell.execute_reply":"2021-08-20T22:14:39.479049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:14:42.031917Z","iopub.execute_input":"2021-08-20T22:14:42.032394Z","iopub.status.idle":"2021-08-20T22:14:42.059752Z","shell.execute_reply.started":"2021-08-20T22:14:42.032331Z","shell.execute_reply":"2021-08-20T22:14:42.058527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.school.nunique(), df.classroom.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:14:44.577038Z","iopub.execute_input":"2021-08-20T22:14:44.577523Z","iopub.status.idle":"2021-08-20T22:14:44.586218Z","shell.execute_reply.started":"2021-08-20T22:14:44.577485Z","shell.execute_reply":"2021-08-20T22:14:44.585479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = df.select_dtypes('O')\ncategorical = categorical.drop(['classroom', 'student_id'], axis = 1)\nfig, ax = plt.subplots(3, 2, figsize = (15, 15))\nax = ax.flatten()\nfor idx, column in enumerate(categorical):\n    sns.boxplot(x = column, y = 'posttest', data = df, ax = ax[idx])","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:14:46.160533Z","iopub.execute_input":"2021-08-20T22:14:46.161422Z","iopub.status.idle":"2021-08-20T22:14:47.619484Z","shell.execute_reply.started":"2021-08-20T22:14:46.161372Z","shell.execute_reply":"2021-08-20T22:14:47.618491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Boxplots of the categorical variables vs the target variable (posttest), in order to see what categorical values actually matter.\nThere is a great variation in posttest scores with respect to:\n    school\n    school_type\n    lunch,\nAnd a slight difference with respect to:\n    chool_setting\n    teaching method","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5, 5, figsize = (15, 15))\nax = ax.flatten()\nschools = df['school'].unique()\nfor idx, school in enumerate(schools):\n    temp = df.loc[df['school'] == school]\n    sns.boxplot(x = 'classroom', y = 'posttest', data = temp, ax = ax[idx])\n    ax[idx].set_title(school)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:14:51.329004Z","iopub.execute_input":"2021-08-20T22:14:51.329469Z","iopub.status.idle":"2021-08-20T22:14:55.994044Z","shell.execute_reply.started":"2021-08-20T22:14:51.329428Z","shell.execute_reply":"2021-08-20T22:14:55.993315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the classroom actually has an effect in certain schools. For example, for VKWQH (4, 5) the classroom can change the posttest score by 20 pts (roughly 50% higher than the lower test scores). How does this actually effect the fitting though?","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\ndf['grade'] = df.loc[:, 'posttest'].map(lambda x: x // 5)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:15:38.57975Z","iopub.execute_input":"2021-08-20T22:15:38.580201Z","iopub.status.idle":"2021-08-20T22:15:38.59092Z","shell.execute_reply.started":"2021-08-20T22:15:38.580164Z","shell.execute_reply":"2021-08-20T22:15:38.589428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df['grade'], discrete = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:15:04.830996Z","iopub.execute_input":"2021-08-20T22:15:04.831745Z","iopub.status.idle":"2021-08-20T22:15:05.283563Z","shell.execute_reply.started":"2021-08-20T22:15:04.831676Z","shell.execute_reply":"2021-08-20T22:15:05.282723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = ['school', 'school_setting', 'lunch', 'school_type', 'teaching_method']\nnum = ['n_student', 'pretest']\nct = ColumnTransformer([('Scaling', MinMaxScaler(), num), ('Onehot Encoding', OneHotEncoder(), cat)])\nscores = []\nX = df[['school', 'school_setting', 'school_type', 'teaching_method', 'n_student', 'lunch', 'pretest']]\ny = df['posttest']\nz = df['grade']","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:15:09.062696Z","iopub.execute_input":"2021-08-20T22:15:09.063591Z","iopub.status.idle":"2021-08-20T22:15:09.073865Z","shell.execute_reply.started":"2021-08-20T22:15:09.06354Z","shell.execute_reply":"2021-08-20T22:15:09.072718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In another trial, where I excluded classrooms, I used label encoding instead of onehot, and the results are pretty much the same. I changed to onehot, and am using column transformers for practice.","metadata":{}},{"cell_type":"code","source":"rsf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 2)\nmodels = [Ridge, RandomForestRegressor, KNeighborsRegressor, DecisionTreeRegressor]\nmodel_names = ['Ridge', 'Forest', 'KNN', 'Tree']\ntrain_scores = defaultdict(list)\nval_scores = defaultdict(list)\nridge_score = []\n\nfor name, model in zip(model_names, models):\n    for train_idx, val_idx in rsf.split(X, z):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        pipe = Pipeline([('Column Transformer', ct), ('model', model())])\n        pipe.fit(X_train, y_train)\n        train_pred = pipe.predict(X_train)\n        train_scores[name].append(mean_squared_error(y_train, train_pred, squared = False))\n        val_pred = pipe.predict(X_val)\n        val_scores[name].append(mean_squared_error(y_val, val_pred, squared = False))\n\nfor name in model_names:\n    ts = train_scores[name]\n    vs = val_scores[name]\n    print(f'Model: {name} \\n avg train:{sum(ts)/len(ts)}, avg val: {sum(vs)/len(vs)} \\n min ts: {min(ts)}, min vs: {min(vs)} \\n max ts: {max(ts)}, max vs: {max(vs)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:15:43.519366Z","iopub.execute_input":"2021-08-20T22:15:43.519856Z","iopub.status.idle":"2021-08-20T22:16:08.564926Z","shell.execute_reply.started":"2021-08-20T22:15:43.519815Z","shell.execute_reply":"2021-08-20T22:16:08.563935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_score = []\nridge_score = []\nfor train_idx, val_idx in rsf.split(X, z):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        mod = Pipeline([('transformer', ct), ('model', Ridge())])\n        mod.fit(X_train, y_train)\n        val_pred = mod.predict(X_val)\n        val_score.append(mean_squared_error(y_val, val_pred, squared = False))\n        ridge_score.append(r2_score(y_val, val_pred))\n        \nnp.average(np.array(val_score)), np.average(np.array(ridge_score))","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:16:15.361295Z","iopub.execute_input":"2021-08-20T22:16:15.361906Z","iopub.status.idle":"2021-08-20T22:16:15.786839Z","shell.execute_reply.started":"2021-08-20T22:16:15.361866Z","shell.execute_reply":"2021-08-20T22:16:15.785273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = ['school', 'school_setting', 'lunch', 'school_type', 'teaching_method', 'classroom']\nnum = ['n_student', 'pretest']\nct = ColumnTransformer([('Scaling', MinMaxScaler(), num), ('Onehot Encoding', OneHotEncoder(), cat)])\nscores = []\nX = df[['school','classroom', 'school_setting', 'school_type', 'teaching_method', 'n_student', 'lunch', 'pretest']]\ny = df['posttest']\nz = df['grade']","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:16:19.185402Z","iopub.execute_input":"2021-08-20T22:16:19.18585Z","iopub.status.idle":"2021-08-20T22:16:19.195064Z","shell.execute_reply.started":"2021-08-20T22:16:19.185809Z","shell.execute_reply":"2021-08-20T22:16:19.193749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsf = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 2)\nmodels = [Ridge, RandomForestRegressor, KNeighborsRegressor, DecisionTreeRegressor]\nmodel_names = ['Ridge', 'Forest', 'KNN', 'Tree']\ntrain_scores = defaultdict(list)\nval_scores = defaultdict(list)\nridge_score = []\n\nfor name, model in zip(model_names, models):\n    for train_idx, val_idx in rsf.split(X, z):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        pipe = Pipeline([('Column Transformer', ct), ('model', model())])\n        pipe.fit(X_train, y_train)\n        train_pred = pipe.predict(X_train)\n        train_scores[name].append(mean_squared_error(y_train, train_pred, squared = False))\n        val_pred = pipe.predict(X_val)\n        val_scores[name].append(mean_squared_error(y_val, val_pred, squared = False))\n\nfor name in model_names:\n    ts = train_scores[name]\n    vs = val_scores[name]\n    print(f'Model: {name} \\n avg train:{sum(ts)/len(ts)}, avg val: {sum(vs)/len(vs)} \\n min ts: {min(ts)}, min vs: {min(vs)} \\n max ts: {max(ts)}, max vs: {max(vs)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:16:21.136006Z","iopub.execute_input":"2021-08-20T22:16:21.136386Z","iopub.status.idle":"2021-08-20T22:16:51.278293Z","shell.execute_reply.started":"2021-08-20T22:16:21.136329Z","shell.execute_reply":"2021-08-20T22:16:51.277576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_score = []\nridge_score = []\nfor train_idx, val_idx in rsf.split(X, z):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        mod = Pipeline([('transformer', ct), ('model', Ridge())])\n        mod.fit(X_train, y_train)\n        val_pred = mod.predict(X_val)\n        val_score.append(mean_squared_error(y_val, val_pred, squared = False))\n        ridge_score.append(r2_score(y_val, val_pred))\n        \nnp.average(np.array(val_score)), np.average(np.array(ridge_score))","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:17:05.505501Z","iopub.execute_input":"2021-08-20T22:17:05.506151Z","iopub.status.idle":"2021-08-20T22:17:05.91801Z","shell.execute_reply.started":"2021-08-20T22:17:05.506111Z","shell.execute_reply":"2021-08-20T22:17:05.916926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see that adding the classroom category decreases our RMSE by about 10% (3.18 -> 2.87), and increases our R2 acurracy by about 1%. When I did GridSearchCV, nothing really changed when I varied alpha.\n\nFor the other models, the training error vs validation error shows that these models are clearly overfitting, hence the lack of reduction in the RMSE error compared to Ridge. I tried Gridsearch on XGB or Forest previously, and none of the parameters seemed to effect the overfitting levels by much (including a max depth of 1).","metadata":{}},{"cell_type":"code","source":"X.loc[:,'student_id'] = df.loc[:, 'student_id']\nsns.scatterplot(x = df['student_id'].loc[val_idx], y = df['posttest'], color = 'red', label = 'Actual')\nsns.scatterplot(x = df['student_id'].loc[val_idx], y = val_pred, color = 'blue', label = 'Predicted')\nplt.title('Actual Scores (red) vs Predicted Scores (blue)')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T22:17:08.57011Z","iopub.execute_input":"2021-08-20T22:17:08.570597Z","iopub.status.idle":"2021-08-20T22:17:13.908195Z","shell.execute_reply.started":"2021-08-20T22:17:08.57056Z","shell.execute_reply":"2021-08-20T22:17:13.907422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to make sure nothing seems strange. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}