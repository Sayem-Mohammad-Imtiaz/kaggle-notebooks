{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe dataset is about a bank's marketing campaign that offers clients a term deposit in the bank. The company approched it's clients mostly by telephon and a target variable was assigned according to the client's answer: 'yes' if they want to make the investment and 'no' otherwise.\n\nA term deposit is a fixed-term investment that includes the deposit of money into an account at a financial institution. Term deposit investments usually carry short-term maturities ranging from one month to a few years and will have varying levels of required minimum deposits [https://www.investopedia.com/terms/t/termdeposit.asp](http://).\n\n## Table of Contents\n\n1. [Data Loading & Data Cleaning](#1.-Data-Loading-&-Data-Cleaning)\n2. [Descriptive Analysis and EDA](#2.-Descriptive-Analysis-and-EDA)\n3. [Feature Engineering](#3.-Feature-Engineering)\n4. [Classification model](#4.-Classification-model)\n6. [Part 2 next week](#Part-2-next-week)\n\n\n## Input variables:\n### bank client data:\n1. 1 - age (numeric)\n1. 2 - job : type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')\n1. 3 - marital : marital status (categorical: 'divorced', 'married', 'single', 'unknown'; note: 'divorced' means divorced or widowed)\n1. 4 - education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n1. 5 - default: has credit in default? (categorical: 'no', 'yes', 'unknown')\n1. 6 - housing: has housing loan? (categorical: 'no', 'yes', 'unknown')\n1. 7 - loan: has personal loan? (categorical: 'no', 'yes', 'unknown')\n\n### related with the last contact of the current campaign:\n1. 8 - contact: contact communication type (categorical: 'cellular', 'telephone')\n1. 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n1. 10 - day_of_week: last contact day of the week (categorical: 'mon', 'tue', 'wed', 'thu', 'fri')\n1. 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### other attributes:\n1. 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n1. 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n1. 14 - previous: number of contacts performed before this campaign and for this client (numeric)\n1. 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure', 'nonexistent', 'success')\n\n### social and economic context attributes\n1. 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n1. 17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n1. 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n1. 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n1. 20 - nr.employed: number of employees - quarterly indicator (numeric)\n\n### Output variable (desired target):\n1. 21 - y - has the client subscribed a term deposit? (binary: 'yes', 'no')"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from IPython.display import Image\nfrom datetime import datetime\n\n# runtime\nimport timeit\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n%matplotlib inline\n\nnp.warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Loading & Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank = pd.read_csv('../input/bank-marketing-campaigns-dataset/bank-additional-full.csv', sep=\";\")\ndisplay(bank.head(3))\nprint(bank.info())\nprint(bank.describe())\nprint(bank.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the datatypes are correct. We can convert the dates columns (\"month\", \"day_of_week\") for further analysis\n\nLet's begin by taking a look at the nulls. Then, we are going to change the data type of the month and day_of_week columns.\n\nlet's see where the nulls are located."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(bank.isnull())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfully, there aren't any null values in the dataset.\n\nLet's change the datatypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank['month'] = pd.to_datetime(bank['month'], format='%b').dt.month\nbank['month'].value_counts().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bank['day_of_week'].value_counts().index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'thu':4, 'mon':1, 'wed':3, 'tue':2, 'fri':5}\nbank['day_of_week'] = bank['day_of_week'].map(d)\nbank['day_of_week'].value_counts().index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the names of the month and day of the week will alow us to include this data in our machine learning model"},{"metadata":{},"cell_type":"markdown","source":"'pdays' has value 999 if the client was not previously contacted, this variable will make our analysis harder to interpret. For this reason, we are going to change this feature to a categorical one, if the value is 999, then it will be replaced with a 'no' which means that the client was not previously contacted, else, it will have a 'yes'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pcontacted(x):\n    if x == 999:\n        return 'no'\n    else:\n        return 'yes'\n\nbank['pdays'] = bank['pdays'].apply(pcontacted)\nbank.rename(columns={\"pdays\": \"bcontacted\"}, inplace=True)\nbank['bcontacted'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Descriptive Analysis and EDA\n"},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Categories\nLet's take a look at the categories and how the 'y' is distributed among them.\n\nFrom the first round, i notice that some features instead of having \"null\" values, they have \"unknow\" values, so i'm going to drop them before visualizing the findings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drops nulls\ndef drop(column):\n\n    bank[column].replace('unknown', np.nan, inplace=True)\n    bank.dropna(inplace=True)\n\ndrop('job')\ndrop('marital')\ndrop('education')\ndrop('housing')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=bank, x='y',palette='GnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = bank.select_dtypes('object').columns.to_list()\ncat = cat[:-1]\ncat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,4, figsize=(20,17))\n\ncat = bank.select_dtypes('object').columns.to_list()\ncat = cat[:-1]\n\nax = ax.ravel()\nposition = 0\n\nfor i in cat:\n    \n    order = bank[i].value_counts().index\n    sns.countplot(data=bank, x=i, ax=ax[position], hue='y', palette='GnBu', order=order)\n    ax[position].tick_params(labelrotation=90)\n    ax[position].set_title(i, fontdict={'fontsize':17})\n    \n    position += 1\n\nplt.subplots_adjust(hspace=0.7)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have a clear imbalanced data problem: the target feature 'y' is dominated by 'no's. When building the classification algorithm we will have to apply an oversampling method to avoid the model predicting based on size.\n- All of the features follow the same trend, there is a group that is mostly targeted and then it descends. The proportion of 'yes' and 'no', follow the same pattern as it descends, (around 10% of the sample)\n- The value, where 'yes' overcome 'no' is located in the feature  'poutcoume', where if there is a success in previous outcomes, the client will say 'yes' to the marketing campaign. The sample, though is extremely low.\n- 'yes' in 'bcontacted' is also bigger in this feature, but the sample is also quite small"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank['month'] = bank['month'].astype('str')\nbank['day_of_week'] = bank['day_of_week'].astype('str')\n\nnumbers = bank.select_dtypes(['int64', 'float64']).columns.to_list()\nlen(numbers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1 Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank.hist(figsize=(20,10), edgecolor='white', color='#00afb9')\nplt.show()\n\ndisplay(bank[numbers].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Couple of things could be said about these distributions.\n\n* **age.** The targetted age tend to be between 32 and 47 (interquantile range) which is the most productive age. It also has the most 'normal' distribution, this will be key for our machine learning algorithm\n* **duration.** Duration's mean (258.20) drastically differs from the median (180). This is because some high duration calls (outliers) that push the distribution to the right\n* **campaign.** clients tend to be contacted two times (median = 2) during the campaign\n* **previous.** most of the values are 0. Similarly to the 'pdays' attribute, this means that most of the clients haven't been contated before\n* **emp.var.rate** values tend to be between -1.8 and 1.4\n* **cons.price.idx.** and **cons.conf.idx** don't vary too much\n* **euribor3m.** The clients were contacted usually when the Euribor3 rate was between 1.3 and 4.96 \n* social and economic context attributes: 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m' and 'nr.employed' don't show any apparent distribution, my intuition tells me that they won't be that useful for the model, taking also into account that they arenÄt under the control of the bank, but lets leave them there until we finish exploring.\n\nMoreover, we can see that 'duration' and 'campaign' distributions have most the values located in the first bin and then it descends. To make this distributions a bit more useful for our machine learning model, we could transform them with the a log formula to make them more 'normal'. An example below"},{"metadata":{"trusted":true},"cell_type":"code","source":"TotalLog = np.log(bank['campaign'] + 1)\nTotalLog.hist(edgecolor='white', color='#00afb9')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2 correlations"},{"metadata":{},"cell_type":"markdown","source":"As mentioned in the introduction, if we want realistic results, we will have to drop 'duration'. Lets do that now."},{"metadata":{"trusted":true},"cell_type":"code","source":"bank.drop(['duration'], axis=1, inplace=True)\nnumbers = bank.select_dtypes(['int64', 'float64']).columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,9))\nsns.heatmap(bank[numbers].corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True, linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Except from 'previous', the variables that are under the control of the bank ('age' and 'campaign') don't show any high correlation. Thankfully, we don't have any continous target variable that we can apply a regression model to.\n* high correlation pairs are not relevant since they are not target variables.\n\nlet's take a look at the combinations of the variables with scatter plots and color them with the 'y' column to see if we can find any pattern.\n\n'duration' column is going to be discarted in this part since all '0' values are a 'no' and that include unanswered calls. "},{"metadata":{"trusted":true},"cell_type":"code","source":"numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(7,4, figsize=(20,27))\nax = ax.ravel()\nposition = 0\n\na = 0\nb = 1\n\n\nwhile a < len(numbers):\n    try:\n        sns.scatterplot(x=numbers[a], y=numbers[b], data=bank, hue='y', ax=ax[position])\n        position += 1\n        if b < len(numbers) - 1:\n            b += 1\n        else:\n            a += 1\n            b = a + 1\n    except:\n        break\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- I was hoping to find a trend in the age group since that would have helped us create a customer profile but that won't be the case since the graphs don't show any evident trend\n- 'campaign's 'yes' are located in  lower values, that means that calling the customer more won't lead to a yes (during the same campaign). But, according to feature 'previous', the customer tend to say 'yes' if he was more than 4 time previously contacted (during different campaigns). This puts in evidence the importance of customer relationship building\n- when 'emp.var.rate' is negative, customers tend to say yes. This is a variable outside the control of the bank, but it could be telling us something since it reveals a trend. It should be included in the model\n- When 'nr.employed' is less tha 5100, customers tend to say 'yes'\n- The remaining relationships don't show any evident trend"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank['month'] = bank['month'].astype('int')\nbank['day_of_week'] = bank['day_of_week'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bank['month']\nfig, ax = plt.subplots(1,2, figsize=(15,5))\n\norder = bank['month'].value_counts().sort_index().index\nsns.countplot(bank['month'], palette='Set3', ax=ax[0])\nsns.countplot(bank['month'], data=bank, palette='Set3', ax=ax[1], hue='y')\nplt.show()\n\ndf = pd.DataFrame(bank['month'].value_counts().sort_index())\ndf['%'] = np.around(df['month']/df['month'].sum() * 100, 2)\ndf['Yes'] = bank.groupby(['y','month']).size()['yes'].values\ndf['%Yes'] = np.around(df['Yes']/df['month']*100,2)\ndisplay(df.T)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the clients were contacted during summer (May-Jul)\n* November and October was pretty still until november when it peaked again before Chirstmas and January-February, which don't have any observations. "},{"metadata":{},"cell_type":"markdown","source":"## Descriptive Analysis and EDA Conclusions\n- Overall, most of the features don't show an evident trend towards 'yes'. The found trends were based mainly on absolute values and not on proportion. The fact that we have imbalanced data doesn't help either.\n- Most of the categorical features are distributed around 90% 'no' and 10% 'yes'. \n- **'poutcome'** and **'bcontacted'** are the only features where 'yes' is bigger than 'no' in one of their values. This tells us something important as both are based on previous results and previous contacts. Seems like customer relation plays an important role\n- Speaking about numbers, **'campaign'** shows the most evident trend, where 'yes' tends to be located in smaller values.\n- Surprisingly, one of the external variables, **'emp.var.rate'** shows a trend where when it's negative, customers tend to say 'yes'\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering\nThis section will focu on converting the categorical variables itno dummy variables to fit it into the model.\n\nWe are also going to be ending with lot's of features, so we are going to apply a reature reduction technique to properly analyze the model\n\nWe are also goinf to apply a log formula to 'campaign' to see if we can squeez some additional score values."},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"bank_features = bank.iloc[:,:-1]\nbank_features = pd.get_dummies(bank_features)\ndisplay(bank_features.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Model Based Feature Selection\nModel based feature selection uses a supervised machine learning model to judge the importance of each feature, and keeps only the most important ones. For this case, we are going to use a random forest classifier, since it usually yields good results and because this is a classification task"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bank_features.values\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determining optimal number of features\nn_features = [5, 10, 15, 20, 25, 30, 35, 40, 45]\n\nstart = timeit.default_timer()\n\nfor i in n_features:\n    # Building the model based feature selection\n    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=i)\n\n    select.fit(X_train, y_train)\n\n    mask = select.get_support()\n\n    X_train_rfe = select.transform(X_train)\n    X_test_rfe = select.transform(X_test)\n\n    train = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n    score = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_train_rfe, y_train)\n    \n    print(\"Train score: {:.3f}\".format(train), \"Test score: {:.3f}\".format(score), \" number of features: {}\".format(i))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a training score of 0.886 and a test score of 0.991, 20 features is a good enough choiche for me.\nLet's check what features the model selects"},{"metadata":{"trusted":true},"cell_type":"code","source":"select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=20)\n\nselect.fit(X_train, y_train)\n\nmask = select.get_support()\n\nX_train_rfe = select.transform(X_train)\nX_test_rfe = select.transform(X_test)\n\nscore = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\nprint(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(20))\n\nfeatures = pd.DataFrame({'features':list(bank_features.keys()), 'select':list(mask)})\nfeatures = list(features[features['select']==True]['features'])\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering Conclusions\n\n- As found in the EDA section, the model selected features based on majority. Features like:  'job_admin.', 'job_technician', 'marital_married', 'marital_single', 'education_high.school', 'education_university.degree'. This is why an oversampling technique is needed\n- We also were right about 'bcontacted', 'campaign', and 'emp.var.rate'. These features, mentioned on the EDA section ended to be important in the feature selection\n- 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed' were also selected. Seems like the economic situation also plays a role in the model"},{"metadata":{},"cell_type":"markdown","source":"# 4. Classification model\n* in this section we are gong to build a simple model (knn) with the reduced dataset, then a more complex one with the reduced data (Gradient Boosting classifier), and finally another one with the complete dataset and a Gradient Boosting classifier"},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Knn"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bank_features[features].values\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nbest_n = 0\nbest_training = 0\nbest_test = 0\n\nfor i in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    \n    training = knn.score(X_train, y_train)\n    test = knn.score(X_test, y_test)\n    \n    if test > best_test:\n        best_n = i\n        best_training = training\n        best_test = test\n\nprint(\"best number of neighbors: {}\".format(best_n))\nprint(\"best training set score : {:.3f}\".format(best_training))\nprint(\"best test set score: {:.3f}\".format(best_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = timeit.default_timer()\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(knn.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Gradient Boosting (Reduced Dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bank_features[features].values\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nstart = timeit.default_timer()\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Gradient Boosting (Complete Dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bank_features\ny = bank['y'].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nstart = timeit.default_timer()\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\n\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nstop = timeit.default_timer()\nprint('Time: ', stop - start) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification model Conclusions\n- We were able to build a simple Knn modelwith a set score of 0.891. Fairly good but the gradient models were more effective\n- We used the reduced dataset and the complete dataset to build the model, obtaining the same results (test score of 0.90). the reduced dataset, thought, run a bit faster (1.72 vs 2.07)"},{"metadata":{},"cell_type":"markdown","source":"# Part 2 next week\nWhat is missing:\n- feature engineering: we found that some values could be improved with log formulas\n- imbalanced data: we should apply an oversampling method to get a more realistic result\n- Visualize results\n- building a ML application. So... will your customer accept your offer?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}