{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Content-based Filtering\nThis filtration strategy is based on the data provided about the items. The algorithm recommends products that are similar to the ones that a user has liked in the past. This similarity (generally cosine similarity) is computed from the data we have about the items as well as the user’s past preferences.\nFor example, if a user likes movies such as ‘The Prestige’ then we can recommend him the movies of ‘Christian Bale’ or movies with the genre ‘Thriller’ or maybe even movies directed by ‘Christopher Nolan’.So what happens here the recommendation system checks the past preferences of the user and find the film “The Prestige”, then tries to find similar movies to that using the information available in the database such as the lead actors, the director, genre of the film, production house, etc and based on this information find movies similar to “The Prestige”.\n\n- Disadvantages\n\nDifferent products do not get much exposure to the user.\nBusinesses cannot be expanded as the user does not try different types of products."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Collaborative Filtering\nThis filtration strategy is based on the combination of the user’s behavior and comparing and contrasting that with other users’ behavior in the database. The history of all users plays an important role in this algorithm. The main difference between content-based filtering and collaborative filtering that in the latter, the interaction of all users with the items influences the recommendation algorithm while for content-based filtering only the concerned user’s data is taken into account.\nThere are multiple ways to implement collaborative filtering but the main concept to be grasped is that in collaborative filtering multiple user’s data influences the outcome of the recommendation. and doesn’t depend on only one user’s data for modeling.\n\nThere are 2 types of collaborative filtering algorithms:\n\n### User-based Collaborative filtering\nThe basic idea here is to find users that have similar past preference patterns as the user ‘A’ has had and then recommending him or her items liked by those similar users which ‘A’ has not encountered yet. This is achieved by making a matrix of items each user has rated/viewed/liked/clicked depending upon the task at hand, and then computing the similarity score between the users and finally recommending items that the concerned user isn’t aware of but users similar to him/her are and liked it.\n\nFor example, if the user ‘A’ likes ‘Batman Begins’, ‘Justice League’ and ‘The Avengers’ while the user ‘B’ likes ‘Batman Begins’, ‘Justice League’ and ‘Thor’ then they have similar interests because we know that these movies belong to the super-hero genre. So, there is a high probability that the user ‘A’ would like ‘Thor’ and the user ‘B’ would like The Avengers’.\n\n- Disadvantages\n\nPeople are fickle-minded i.e their taste change from time to time and as this algorithm is based on user similarity it may pick up initial similarity patterns between 2 users who after a while may have completely different preferences.\nThere are many more users than items therefore it becomes very difficult to maintain such large matrices and therefore needs to be recomputed very regularly.\nThis algorithm is very susceptible to shilling attacks where fake users profiles consisting of biased preference patterns are used to manipulate key decisions.\n\n\n\n### Item-based Collaborative Filtering\nThe concept in this case is to find similar movies instead of similar users and then recommending similar movies to that ‘A’ has had in his/her past preferences. This is executed by finding every pair of items that were rated/viewed/liked/clicked by the same user, then measuring the similarity of those rated/viewed/liked/clicked across all user who rated/viewed/liked/clicked both, and finally recommending them based on similarity scores.\n\nHere, for example, we take 2 movies ‘A’ and ‘B’ and check their ratings by all users who have rated both the movies and based on the similarity of these ratings, and based on this rating similarity by users who have rated both we find similar movies. So if most common users have rated ‘A’ and ‘B’ both similarly and it is highly probable that ‘A’ and ‘B’ are similar, therefore if someone has watched and liked ‘A’ they should be recommended ‘B’ and vice versa.\n\n- Advantages over User-based Collaborative Filtering\n\n\nUnlike people’s taste, movies don’t change.\nThere are usually a lot fewer items than people, therefore easier to maintain and compute the matrices.\nShilling attacks are much harder because items cannot be faked."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrating = pd.read_csv('/kaggle/input/netflix-prize-data/combined_data_4.txt',header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\n# Convert Ratings column to a float datatype\nrating['Rating'] = rating['Rating'].astype(float)\nrating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating = rating.iloc[:2000000,]\nrating.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# (24058263, 2)\nmovie = pd.read_csv('/kaggle/input/netflix-prize-data/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\n\n\nprint(movie.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To count all the 'nan' values in the Ratings column in the 'ratings' dataset\ndf_nan = pd.DataFrame(pd.isnull(rating.Rating),)\n\ndf_nan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.isnull(rating['Rating'])\ndf2 = pd.DataFrame(df1)\ndf3 = df2[df2['Rating']==True]\ndf3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df3.reset_index()\ndf_nan = df3.copy()\ndf_nan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To create a numpy array containing movie ids according the 'ratings' dataset\n\nmovie_np = []\nmovie_id = 1\n\nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n    # numpy approach\n    temp = np.full((1,i-j-1), movie_id)\n    movie_np = np.append(movie_np, temp)\n    movie_id += 1\n\n# Account for last record and corresponding length\n# numpy approach\nlast_record = np.full((1,len(rating) - df_nan.iloc[-1, 0] - 1),movie_id)\nmovie_np = np.append(movie_np, last_record)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To append the above created array to the datset after removing the 'nan' rows\nrating = rating[pd.notnull(rating['Rating'])]\n\nrating['Movie_Id'] = movie_np.astype(int)\nrating['Cust_Id'] =rating['Cust_Id'].astype(int)\nprint('-Dataset examples-')\nrating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 rating is very less let's check the percentage of distribution\nsns.countplot(rating['Rating']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage distribution of rating here we can either ignore 1 rating data because it's very less in percantage \n(rating['Rating'].value_counts()/len(rating))*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating= rating[rating['Rating']!=1]\nrating['Rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_dataset = rating.pivot(index='Movie_Id',columns='Cust_Id',values='Rating')\nfinal_dataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that userId 2649426 has watched movieId 17  and rated  4.0 but has not rated movieId other. This interpretation is harder to extract from this dataframe. Therefore, to make things easier to understand and work with, we are going to make a new dataframe where each column would represent each unique userId and each row represents each unique movieId."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will take only top 30% of cust_id \nthresh = len(final_dataset)*0.3\nfinal_dataset = final_dataset.dropna(thresh=thresh,axis=1)\nfinal_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s fix this and impute NaN with 0 to make things understandable for the algorithm and also making the data more eye-soothing."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_dataset.fillna(0,inplace=True)\nfinal_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing Noise from the data\nIn the real-world, ratings are very sparse and data points are mostly collected from very popular movies and highly engaged users. \nWe wouldn’t want movies that were rated by a small number of users because it’s not credible enough. Similarly, \nusers who have rated only a handful of movies should also not be taken into account.\n\nSo with all that taken into account and some trial and error experimentations,  we will reduce the noise by adding some filters for the final dataset.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Let’s visualize how these filters look like\nAggregating the number of users who voted and the number of movies that were voted."},{"metadata":{"trusted":true},"cell_type":"code","source":"no_user_voted = rating.groupby('Movie_Id')['Rating'].agg('count')\nno_movies_voted = rating.groupby('Cust_Id')['Rating'].agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are huge differnec in mean and median of no_of_voted users so we can remove outliers\nno_user_voted.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(no_user_voted);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_movies_voted.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# same way let's visulaize no_of_movies voted\nsns.distplot(no_movies_voted);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let’s visualize the number of users who voted with our threshold of 800.\n\nf,ax = plt.subplots(1,1,figsize=(16,4))\n# ratings['rating'].plot(kind='hist')\nplt.scatter(no_user_voted.index,no_user_voted,color='mediumseagreen')\nplt.axhline(y=800,color='r')\nplt.xlabel('MovieId')\nplt.ylabel('No. of users voted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_dataset = final_dataset.loc[no_user_voted[no_user_voted >200].index,:]\n# final_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,1,figsize=(16,4))\n#rating['Rating'].plot(kind='hist')\nplt.scatter(no_movies_voted.index,no_movies_voted,color='mediumseagreen')\nplt.axhline(y=10,color='r')\nplt.xlabel('CustId')\nplt.ylabel('No. of votes by user')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_dataset=final_dataset.loc[:,no_movies_voted[no_movies_voted > 10].index]\n# final_dataset\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing sparsity\n- Feature engineering tech\n\nOur final_dataset has dimensions of 75 * 5903 where most of the values are sparse. We are using only a small dataset but for the original large dataset of movie  which has more than 100000 features, our system may run out of computational resources when that is feed to the model. To reduce the sparsity we use the csr_matrix function from the scipy library."},{"metadata":{"trusted":true},"cell_type":"code","source":"# An example of how it works :\nsample = np.array([[0,0,3,0,0],[4,0,0,0,2],[0,0,0,0,1]])\nsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it will give count of non zero value\nnp.count_nonzero(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total number of values\nsample.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparsity = 1.0 - ( np.count_nonzero(sample) / float(sample.size) )\nprint(sparsity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csr_sample = csr_matrix(sample)\nprint(csr_sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there is no sparse value in the csr_sample and values are assigned as rows and column index. for the 0th row and 2nd column, the value is 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the csr_matrix method to the dataset :\n\ncsr_data = csr_matrix(final_dataset.values)\nfinal_dataset.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making the movie recommendation system model\nWe will be using the KNN algorithm to compute similarity with cosine distance metric which is very fast and more preferable than pearson coefficient."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=15, n_jobs=-1)\nknn.fit(csr_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making the recommendation function\nThe working principle is very simple. We first check if the movie name input is in the database and if it is we use our recommendation system to find similar movies and sort them based on their similarity distance and output only the top 10 movies with their distances from the input movie."},{"metadata":{},"cell_type":"markdown","source":"## Recomendation by movie name"},{"metadata":{"trusted":true},"cell_type":"code","source":"def movie_recommendation(movie_name):\n    n_movies_to_reccomend = 6\n    movie_list = movie[movie['Name']==movie_name]\n    print(movie_list)\n    if len(movie_list):        \n        movie_idx= movie_list.iloc[0]['Movie_Id']\n        movie_idx = final_dataset[final_dataset['Movie_Id'] == movie_idx].index\n        distances , indices = knn.kneighbors(csr_data[movie_idx],n_neighbors=n_movies_to_reccomend+1) \n        rec_movie_indices = sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]\n        recommend_frame = []\n        for val in rec_movie_indices:\n            movie_idx = final_dataset.iloc[val[0]]['Movie_Id']\n            idx = movie[movie['Movie_Id'] == movie_idx].index\n            recommend_frame.append({'Name':movie.iloc[idx]['Name'].values[0],'Distance':val[1]})\n        \n        df = pd.DataFrame(recommend_frame,index=range(1,n_movies_to_reccomend+1))\n        return df\n    else:\n        return \"No movies found try another movie\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_recommendation('Dinosaur Planet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_recommendation('DDLJ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_recommendation('Sick')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating['Rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recomendation by Cust_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"def movie_recommendation(Cust_Id):\n    n_movies_to_reccomend = 6\n    movie_idx = rating[rating.loc[:,'Cust_Id']==2385003]['Movie_Id'].iloc[0]\n    movie_list = movie[movie['Movie_Id']==movie_idx].loc[:,'Name']\n    print(movie_list)\n    if len(movie_list):        \n        movie_idx = final_dataset[final_dataset['Movie_Id'] == movie_idx].index\n        distances , indices = knn.kneighbors(csr_data[movie_idx],n_neighbors=n_movies_to_reccomend+1) \n        rec_movie_indices = sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]\n        recommend_frame = []\n        for val in rec_movie_indices:\n            movie_idx = final_dataset.iloc[val[0]]['Movie_Id']\n            idx = movie[movie['Movie_Id'] == movie_idx].index\n            recommend_frame.append({'Name':movie.iloc[idx]['Name'].values[0],'Distance':val[1]})\n        \n        df = pd.DataFrame(recommend_frame,index=range(1,n_movies_to_reccomend+1))\n        return df\n    else:\n        return \"No movies found try another movie\"\n    \nmovie_recommendation(2385003)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.merge(movie,rating,on='Movie_Id',how='inner')\nfinal_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_df.dropna()\nfinal_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.groupby('Name')['Rating'].mean().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.groupby('Name')['Rating'].count().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df = pd.DataFrame(final_df.groupby('Name')['Rating'].mean())\nrating_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df['no_of_rating'] = final_df.groupby('Name')['Rating'].count()\nrating_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\nplt.figure(figsize=(10,4))\nrating_df['no_of_rating'].hist(bins=70);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# by below plot we can understand most of the people given rating 3 to 4 but this follow normal distribution\nplt.figure(figsize=(10,4))\nrating_df['Rating'].hist(bins=70);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we can see number of ratings has huge outliers so we will filter data no of ratings near 25000\nsns.jointplot(x='Rating',y='no_of_rating',data=rating_df,alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_df = rating_df[rating_df['no_of_rating']<1000]\nsns.jointplot(x='Rating',y='no_of_rating',data=filter_df,alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.groupby('Cust_Id')['Rating'].mean().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.groupby('Cust_Id')['Rating'].count().sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_df = pd.DataFrame(final_df.groupby('Cust_Id')['Movie_Id'].mean().sort_values())\nlast_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_df['No_of_rating'] = final_df.groupby('Cust_Id')['Movie_Id'].count().sort_values()\nlast_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nlast_df['No_of_rating'].hist(bins=70);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# by below graph we can see that most number of rating given by user is 4 then 5 then 3 and we can say 2 rating as outliers\nplt.figure(figsize=(10,4))\nlast_df['Movie_Id'].hist(bins=70);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='Movie_Id',y='No_of_rating',data=last_df,alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will filter the data of having no of rating less then 100\nlast_filter_df = last_df[last_df['No_of_rating']<150]\nlast_filter_df.reset_index(inplace=True)\nlast_filter_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_filter_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivot = last_filter_df.groupby(['Cust_Id','Movie_Id'])['No_of_rating'].mean().to_frame()\npivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_df = last_filter_df.sort_values('No_of_rating',ascending=False).head(10)\nsorted_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's take two movie id from top 10 highest rated movie like 305344 and 303948\nno_of_rating_305344 = pivot.loc[305344]\nno_of_rating_303948 = pivot.loc[303948]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}