{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 다양한 파라미터의 모델 실험해보기\n\n오버피팅(과적합)\n- 학습용 데이터를 너무나도 완벽하게 학습한 경우. 다른 데이터 따위는 나몰라\n- => 학습용 데이터셋에서는 지나치게 좋은 성능 & 검증용 데이터셋에서는 지나치게 허접한 성능\n\n언더피팅(과소적합)\n- 학습 자체가 거의 안됨. 더 학습되어야 함\n- => 학습용 & 검증용 데이터셋 모두에서 지나치게 허접한 성능\n\n![underfitting_overfitting](http://i.imgur.com/2q85n9s.png)\n\n# 예제\n디시전트리를 만들 되, *max_leaf_nodes* 파라미터를 다양하게 구성해 비교한다:\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n# 모델을 만들 때마다 매번 아래 네 줄의 코드를 적기 귀찮으니\n# 함수로 묶는다\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is loaded into **train_X**, **val_X**, **train_y** and **val_y** using the code you've already seen (and which you've already written).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n    \n# 데이터 로드\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n\n# 누락된 값의 행 제거\nfiltered_melbourne_data = melbourne_data.dropna(axis=0)\n\n# 타깃 및 특징 선택\ny = filtered_melbourne_data.Price\nmelbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = filtered_melbourne_data[melbourne_features]\n\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 분할\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use a for-loop to compare the accuracy of models built with different values for *max_leaf_nodes.*","metadata":{}},{"cell_type":"code","source":"# 네 종류의 max_leaf_nodes 값으로 실험\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes 개수: %d    \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"네 개의 모델 중 max_leaf_nodex가 500인 경우의 성능이 그나마 가장 좋음\n\n---\n\n\n# Your Turn\n\nTry **[optimizing the model you've previously built](https://www.kaggle.com/kernels/fork/1259126)**.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161285) to chat with other Learners.*","metadata":{}}]}