{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Case-Problem\" data-toc-modified-id=\"Case-Problem-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Case Problem</a></span></li><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Import Packages</a></span></li><li><span><a href=\"#Set-Notebook-Options\" data-toc-modified-id=\"Set-Notebook-Options-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Set Notebook Options</a></span></li></ul></li><li><span><a href=\"#Data-Wrangling\" data-toc-modified-id=\"Data-Wrangling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Wrangling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Data\" data-toc-modified-id=\"Import-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import Data</a></span></li><li><span><a href=\"#Rename-Columns\" data-toc-modified-id=\"Rename-Columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Rename Columns</a></span></li><li><span><a href=\"#Rename-Target-Variable\" data-toc-modified-id=\"Rename-Target-Variable-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Rename Target Variable</a></span></li><li><span><a href=\"#Data-Type-Conversion\" data-toc-modified-id=\"Data-Type-Conversion-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Data Type Conversion</a></span></li><li><span><a href=\"#Identify-Missing-Values\" data-toc-modified-id=\"Identify-Missing-Values-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Identify Missing Values</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Handle-Missing-Value\" data-toc-modified-id=\"Handle-Missing-Value-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Handle Missing Value</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pair-Plot\" data-toc-modified-id=\"Pair-Plot-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pair Plot</a></span></li><li><span><a href=\"#Box-Plot\" data-toc-modified-id=\"Box-Plot-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Box Plot</a></span></li><li><span><a href=\"#Imbalance-Data\" data-toc-modified-id=\"Imbalance-Data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Imbalance Data</a></span></li></ul></li><li><span><a href=\"#Modeling:-Logistic-Regression\" data-toc-modified-id=\"Modeling:-Logistic-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling: Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Feature Selection</a></span></li><li><span><a href=\"#Handle-Outliers\" data-toc-modified-id=\"Handle-Outliers-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Handle Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Log-Transformation\" data-toc-modified-id=\"Log-Transformation-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Log Transformation</a></span></li><li><span><a href=\"#Square-Root-Transformation\" data-toc-modified-id=\"Square-Root-Transformation-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Square-Root Transformation</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Model-Interpretation\" data-toc-modified-id=\"Model-Interpretation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Model Interpretation</a></span></li><li><span><a href=\"#Model-Assumptions\" data-toc-modified-id=\"Model-Assumptions-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Model Assumptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linearity\" data-toc-modified-id=\"Linearity-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>Linearity</a></span></li><li><span><a href=\"#No-Multicollinearity\" data-toc-modified-id=\"No-Multicollinearity-4.6.2\"><span class=\"toc-item-num\">4.6.2&nbsp;&nbsp;</span>No Multicollinearity</a></span></li><li><span><a href=\"#Independence-of-Observation\" data-toc-modified-id=\"Independence-of-Observation-4.6.3\"><span class=\"toc-item-num\">4.6.3&nbsp;&nbsp;</span>Independence of Observation</a></span></li></ul></li></ul></li><li><span><a href=\"#Modeling:-K-Nearest-Neighbour\" data-toc-modified-id=\"Modeling:-K-Nearest-Neighbour-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling: K-Nearest Neighbour</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Feature Scaling</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nHello everyone! In this notebook, we'll be solving a binary classification problem on Indian liver patient records using Logistic Regression and K-Nearest Neighbour Algorithm. This use case is provided on [Kaggle](https://www.kaggle.com/uciml/indian-liver-patient-records) using the dataset from [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)).\n\n## Case Problem\nPatients with liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles, and drugs. The dataset contains 416 liver patient records and 167 non-liver patient records collected from North East of Andhra Pradesh, India. The collected data is used for evaluating prediction algorithms to reduce the burden on doctors."},{"metadata":{},"cell_type":"markdown","source":"## Import Packages\nFirst of all, let us import all necessary packages, mainly for data analysis, visualization, and modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# data analysis\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import boxplot_stats\nfrom IPython.display import Image\n\n# preprocessing (pre-modeling)\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# modeling\nimport statsmodels.api as sm\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# evaluation (post-modeling)\nfrom sklearn.metrics import *\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy import stats\nfrom scipy.special import logit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Notebook Options\n- Suppress package warning\n- Set color of the plot to be more contrast\n- Change float format to five decimal places\n- Display all content in a `pandas` column"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"ticks\", color_codes=True)\npd.options.display.float_format = '{:.5f}'.format\npd.options.display.max_colwidth = -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling\nBefore we jump into any visualization or modeling step, we have to make sure our data is ready. \n\n## Import Data\nLet's import `indian_liver_patient.csv` and analyze the data structure."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver = pd.read_csv(\"../input/indian-liver-patient-records/indian_liver_patient.csv\")\nliver.head()\nliver.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liver.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains 583 observations of patient records with 11 columns as follows ([Reference](https://labtestsonline.org/tests-index)):\n- `Age`: Age of the patient. Any patient whose age exceeded 89 is listed as being age 90.\n- `Gender`: Gender of the patient, either Male or Female.\n- `Total_Bilirubin`: Bilirubin is an orange-yellow pigment, a waste product primarily produced by the normal breakdown of heme. This test measures the amount of bilirubin in the blood to evaluate a person's liver function or to help diagnose anemias caused by red blood cell destruction (hemolytic anemia). Measured in mg/dL.\n- `Direct_Bilirubin`: Water-soluble forms of bilirubin. Measured in mg/dL.\n- `Alkaline_Phosphotase` (ALP): Enzyme found in several tissues throughout the body. The highest concentrations of ALP are present in the cells that comprise bone and the liver. Elevated levels of ALP in the blood are most commonly caused by liver disease or bone disorders. Measured in U/L.\n- `Alamine_Aminotransferase` (ALT): Enzyme found mostly in the cells of the liver and kidney. Normally, ALT levels in the blood are low, but when the liver is damaged, ALT is released into the blood and the level increases. This test measures the level of ALT in the blood and is useful for early detection of liver disease. Measured in U/L.\n- `Aspartate_Aminotransferase`: Enzyme found in cells throughout the body but mostly in the heart and liver and, to a lesser extent, in the kidneys and muscles. In healthy individuals, levels of AST in the blood are low. When liver or muscle cells are injured, they release AST into the blood. This makes AST a useful test for detecting or monitoring liver damage. Measured in U/L.\n- `Total_Protiens`: Measures the amount of protein in g/dL.\n- `Albumin`: Made by the liver and makes up about 60% of the total protein. Albumin keeps fluid from leaking out of blood vessels, nourishes tissues, and transports hormones, vitamins, drugs, and substances like calcium throughout the body. Measured in g/dL.\n- `Albumin_and_Globulin_Ratio`: Compares the amount of `Albumin` with `Globulin`. `Globulin` made up the remaining 40% of proteins in the blood which is a varied group of proteins, some produced by the liver and some by the immune system. They help fight infection and transport nutrients. Measured in g/dL.\n- `Dataset`: A class label used to divide the patient into two groups. Value 1 indicates patient with liver disease and 2 otherwise."},{"metadata":{},"cell_type":"markdown","source":"## Rename Columns\nRename the column to its abbreviation in order to make it shorter."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"column_names = {\n    \"Age\": \"Age\",\n    \"Gender\": \"Gender\",\n    \"TB\": \"Total Bilirubin\",\n    \"DB\": \"Direct Bilirubin\",\n    \"ALP\": \"Alkaline Phosphotase\",\n    \"ALT\": \"Alamine Aminotransferase\",\n    \"AST\": \"Aspartate Aminotransferase\",\n    \"TP\": \"Total Proteins\",\n    \"Albumin\": \"Albumin\",\n    \"A/G Ratio\": \"Albumin and Globulin Ratio\",\n    \"Disease\": \"Dataset\"\n}\nliver.columns = column_names.keys()\nliver.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rename Target Variable\nRename the values in the target variable `Disease`:\n- 1 indicates patient with liver disease, we subtitute this with \"Yes\"\n- 2 indicates a non-liver disease patient, we subtitute this with \"No\""},{"metadata":{"trusted":true},"cell_type":"code","source":"liver.loc[liver[\"Disease\"] == 1, \"Disease\"] = \"Yes\"\nliver.loc[liver[\"Disease\"] == 2, \"Disease\"] = \"No\"\nliver.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Type Conversion\nConvert these two categorical columns, from object to category type:\n- `Gender` with two levels: \"Female\" and \"Male\"\n- `Disease` with two levels: \"Yes\" and \"No\""},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"liver[[\"Gender\", \"Disease\"]] = liver[[\"Gender\", \"Disease\"]].astype(\"category\")\nliver.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in liver.select_dtypes('category').columns:\n    print(col, \":\", liver[col].cat.categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify Missing Values\nWe have to make sure our data is complete, means that there are no missing or `NA` values."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Four observations of `A/G Ratio` are missing, this will be handled later on after the feature engineering step."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nFeature engineering is the process of using domain knowledge to extract features from provided raw data. These features can be used to improve the performance of machine learning models. In this section, we'll be extracting features to reduce the correlation between the predictors and reduce the risk of multicollinearity on the model. So let's plot the Pearson correlation heatmap to find out which predictors have a strong correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotCorrelationHeatmap(data, figsize = (12,6)):\n    plt.figure(figsize=figsize)\n    corr_val = data.corr(method = \"pearson\")\n    mask = np.zeros_like(corr_val, dtype = np.bool)\n    mask[np.triu_indices_from(mask, k = 1)] = True\n    corr_heatmap = sns.heatmap(corr_val, mask = mask,\n                               annot = True, fmt='.3f', linewidths = 3, cmap = \"Reds\")\n    corr_heatmap.set_title(\"PEARSON CORRELATION HEATMAP\", fontsize = 15, fontweight = \"bold\")\n    corr_heatmap\n\nplotCorrelationHeatmap(liver)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Consider $|$correlation$| > 0.6$ as strong, we want to minimize the risk of multicollinearity on our model by considering the correlation of these features:\n- `TB` and `DB` (0.875): Merge into `DB/TB Percentage` ([Reference](https://synapse.koreamed.org/DOIx.php?id=10.3343/lmo.2018.8.4.127))\n- `AST` and `ALT` (0.792): We can merge them into `AST/ALT Ratio` ([Reference](https://en.wikipedia.org/wiki/AST/ALT_ratio)), but then we ended up with an insignificant predictor. Thus, we'll keep them as separate predictors.\n- `Albumin` and `A/G Ratio` (0.69): Calculate `Globulin` level, given the `A/G ratio`\n- `Albumin` and `TP` (0.784): Albumin is a group of protein, it certainly have a strong correlation with `TP` (Total Protein). These two variables will be discussed later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"liver[\"DB/TB Percentage\"] = liver[\"DB\"]/liver[\"TB\"]*100\n#liver[\"AST/ALT Ratio\"] = liver[\"AST\"]/liver[\"ALT\"]\nliver[\"Globulin\"] = liver[\"Albumin\"]/liver[\"A/G Ratio\"]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver = liver.drop([\"DB\", \"TB\", \"A/G Ratio\"], axis = 1)\nliver.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plotCorrelationHeatmap(liver, (14, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, `TP` is now strongly correlated with both `Albumin` and `Globulin`. Let's manually add the value of `Albumin` and `Globulin` as `Albumin+Globulin` then further investigate the correlation with `TP`:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"AG_df = liver[[\"Albumin\", \"Globulin\", \"TP\"]]\nAG_df[\"Albumin+Globulin\"] = AG_df[\"Albumin\"] + AG_df[\"Globulin\"]\nplotCorrelationHeatmap(AG_df, (6,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Such a strong (almost perfect) correlation between `TP` and `Albumin+Globulin`, indicating that `TP` can be explained by both `Albumin` and `Globulin`. We will use this information to handle missing value."},{"metadata":{},"cell_type":"markdown","source":"## Handle Missing Value"},{"metadata":{},"cell_type":"markdown","source":"Remember about the missing value in `A/G Ratio`? Now they are moved to `Globulin` since we calculate it using `A/G Ratio`."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"missing = liver[\"Globulin\"].isna()\nliver[missing]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's impute missing `Globulin` value by fitting a linear regression line $TP = Albumin + Globulin + constant$ using the non-missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = liver[-missing][[\"Albumin\", \"Globulin\"]]\ny = liver[-missing][\"TP\"].values\nlin_reg = sm.OLS(y, sm.add_constant(X)).fit()\nprint(\"Adjusted R-squared: {:.3f}%\".format(100*lin_reg.rsquared_adj))\nbeta = lin_reg.params.values \nprint(\"Estimate:\", beta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Adjusted R-squared is 91.812% indicating that the model is a good fit. Now, we have to change the subject of the formula from `TP` to `Globulin` as follows:\n\n$TP = \\beta_0 + \\beta_1*Albumin + \\beta_2*Globulin$ <br>\n\n$Globulin = \\dfrac{TP - \\beta_0 - \\beta_1*Albumin}{\\beta_2}$\n\nwhere:\n- $\\beta_0$ is the intercept of regression line. $TP$ is equal to 0.39838 when $Albumin = 0$ and $Globulin = 0$\n- $\\beta_1$ is the coefficient of $Albumin$. One unit increase in $Albumin$ will increase $TP$ as much as 1.05546\n- $\\beta_2$ is the coefficient of $Globulin$. One unit increase in $Globulin$ will increase $TP$ as much as 0.79825\n\nWe will use this formula to impute the missing value of `Globulin` based on `TP` and `Albumin`."},{"metadata":{"trusted":true},"cell_type":"code","source":"liver[\"Globulin\"] = liver.apply(\n    lambda row: (row.TP - beta[0] - beta[1]*row.Albumin)/beta[2] if np.isnan(row.Globulin) else row.Globulin, axis=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liver.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver[missing]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We drop `TP` as it strongly correlated to `Albumin` and `Globulin`."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver = liver.drop([\"TP\"], axis = 1)\nliver.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\nOur data is ready, now we visualize the distribution and proportion of the variables.\n## Pair Plot"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"pair_plot = sns.pairplot(liver, hue = \"Disease\", diag_kind = \"kde\", corner = True, markers = '+',)\npair_plot.fig.suptitle(\"PAIR PLOT OF NUMERICAL VARIABLES\", size = 25, fontweight = \"bold\")\npair_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the pair plot of numerical variables:\n- We expect little to no correlation between the independent variables, since we have done the feature engineering step except for `ALT` and `AST`.\n- `ALP`, `ALT`, `AST`, and `DB/TB Percentage` have a very positive skewed distribution. We'll analyze them further using box plot."},{"metadata":{},"cell_type":"markdown","source":"## Box Plot"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 4, figsize=(15,8))\n\nfor ax, col in zip(axes.flat, liver.select_dtypes('number').columns):\n    sns.boxplot(x = \"Disease\", y = col, data = liver, ax = ax)\n    # Outlier Count\n    outlier_count = 0\n    for disease in liver[\"Disease\"].cat.categories:\n        liver_disease = liver.loc[liver[\"Disease\"] == disease, col]\n        outlier_list = boxplot_stats(liver_disease).pop(0)['fliers']\n        outlier_count += len(outlier_list)\n    ax.set_title(\"Outlier Count: {} ({:.2f}%)\".format(outlier_count, 100*outlier_count/liver.shape[0]))\n\naxes[-1, -1].axis(\"off\")    \nplt.tight_layout()\nfig.suptitle(\"BOX PLOT OF NUMERICAL VARIABLES\", size = 28, y = 1.05, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the box plot:\n- Patient who has liver disease, in median, have higher `Age`, lower `Albumin`, and slightly higher `Globulin` compared to those who doesn't. This will be further explained in the modeling step.\n- More than 10% of the observations in `ALP`, `ALT`, and `AST` are considered as outliers. We'll apply some transformation during the modeling to handle the outlier."},{"metadata":{},"cell_type":"markdown","source":"## Imbalance Data\nNext we check the frequency of each levels in the target variable `Disease` by `Gender`"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index = liver.Gender,\n            columns = liver.Disease,\n            margins = True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def plotDiseaseCount(data):\n    ax = data.groupby(['Disease', 'Gender']).size().unstack().plot(kind='bar', stacked=True)\n    for rect in ax.patches:\n        height = rect.get_height()\n        width = rect.get_width()\n        padding = 0.25\n\n        ax.text(rect.get_x() + width - padding, \n                rect.get_y() + height / 2, \n                int(height), \n                ha = 'center', va = 'center',\n                color = \"white\")\n\nplotDiseaseCount(liver)\nplt.title(\"PROPORTION OF TARGET VARIABLE (IMBALANCE)\", fontsize = 14, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index = liver.Gender,\n            columns = liver.Disease,\n            margins = True,\n            normalize = \"index\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We consider our data as imbalanced with 71:29 ratio. Therefore, we can do either upsampling or downsampling to balance the positive and negative class of `Disease`. \n\n- Upsampling is a method to randomly subsample the observation from minority class to make the dataset balanced.\n- Downsampling is a method to randomly sample (with replacement) the observation from majority class to make the dataset balanced.\n\nIf we choose to downsample the data, we only end up with a small number of observations (2*167 = 334) and will lose some information from the data. Therefore, in this case we prefer to do upsampling as follows:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def upsampleData(data):\n    data_majority = data[data[\"Disease\"] == \"Yes\"]\n    data_minority = data[data[\"Disease\"] == \"No\"]\n    data_minority_upsampled = resample(data_minority,\n                                       n_samples = data_majority.shape[0],\n                                       replace = True,\n                                       random_state = 888)\n    data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n    return data_upsampled\n\nliver_upsampled = upsampleData(liver)\nliver_upsampled.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plotDiseaseCount(liver_upsampled)\nplt.title(\"PROPORTION OF TARGET VARIABLE (UPSAMPLED)\", fontsize = 14, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index = liver_upsampled.Gender,\n            columns = liver_upsampled.Disease,\n            margins = True,\n            normalize = \"index\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is now balanced with a total of 832 observations. Let's fit the upsampled data into the model."},{"metadata":{},"cell_type":"markdown","source":"# Modeling: Logistic Regression\nLogistic regression is a classification algorithm used to fit a regression curve.\n$\\log{\\frac{p}{1-p}} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$ where:\n- $p$ is the probability of an observation to be in the positive class\n- $\\frac{p}{1-p}$ is called as odds\n- $\\log{\\frac{p}{1-p}}$ is called as log-odds or logit\n- $X_1, X_2, ..., X_n$ are the predictors\n- $\\beta_0$ is a constant value\n- $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficient of $X_1, X_2, ..., X_n$ respectively\n- $n$ is the number of predictors\n\nThe formula above can be re-written as a sigmoid curve:\n\n$p = \\dfrac{1}{1 + e^{-z}}$\nwhere $z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$"},{"metadata":{"trusted":true},"cell_type":"code","source":"z_plot = np.linspace(-10, 10)\nplt.plot(z_plot, 1/(1 + np.exp(-z_plot)))\nplt.axvline(0, color = \"k\", ls = \"--\", alpha = 0.25)\nplt.axhline(0.5, color = \"k\", ls = \"--\", alpha = 0.25) \nplt.xlabel(\"z\")\nplt.ylabel(\"p\")\nplt.title(\"ILLUSTRATION: SIGMOID CURVE\", fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\nWe have to do some data preparation specifically for modeling:\n- Create dummy variables for the categorical variables `Gender` and `Disease`\n- Separate the target variable from the predictors\n- Train test split in order to evaluate our model, with 75% train and 25% test"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver_dummy = pd.get_dummies(liver_upsampled, columns = liver_upsampled.select_dtypes('category').columns, drop_first = True)\nX = liver_dummy.drop([\"Disease_Yes\"], axis = 1)\ny = liver_dummy.Disease_Yes.values\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 888)\nprint(\"X Train:\", X_train.shape)\nprint(\"X Test:\", X_test.shape)\nprint(\"y Train:\", y_train.shape)\nprint(\"y Test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection\nAs a starting point, let's us fit a logistic regression model with all existing predictors and see how it goes."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def fitLogisticRegression(X, y):\n    model = sm.Logit(y, sm.add_constant(X))\n    result = model.fit()\n    return result\n\nmodel_all = fitLogisticRegression(X_train, y_train)\nmodel_all.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We remove `Gender_Male` since the p-value is larger than significant level (5%), meaning that it doesn't have significant impact to `Disease`."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"remove_col_list = [\"Gender_Male\"]\nX_train_removed = X_train.drop(remove_col_list, axis = 1)\nX_test_removed = X_test.drop(remove_col_list, axis = 1)\nmodel_removed = fitLogisticRegression(X_train_removed, y_train)\nmodel_removed.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, all the predictors are significant (p-value < 5%), but can we further improve our model? One way is to handle the outlier present in our predictors."},{"metadata":{},"cell_type":"markdown","source":"## Handle Outliers\nIn this section, we'll be transforming the outliers present in these predictors: `ALP`, `ALT`, and `AST`. We will compare the following:\n1. Log transformation\n2. Square-root transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_col = [\"ALP\", \"ALT\", \"AST\"]\nX_train_untransformed = X_train.drop(transform_col + remove_col_list, axis = 1)\nX_test_untransformed = X_test.drop(transform_col + remove_col_list, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,5))\n\nx_axis = np.linspace(1, 5000, 1000)\ny_axis = [np.log(x_axis), np.sqrt(x_axis)]\ntitle_list = [\"LOG: $y = \\log(x)$\", \"SQUARE-ROOT: $y = \\sqrt{x}$\"]\n\nfor y, title, ax in zip(y_axis, title_list, axes):\n    ax.plot(x_axis, y)\n    ax.set_xlabel(\"Original Value\")\n    ax.set_ylabel(\"Transformed Value\")\n    ax.set_title(title)\n\nplt.tight_layout()\nfig.suptitle(\"TRANSFORMATION FUNCTION\", size = 28, y = 1.05, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Log Transformation\nTransform the predictors `ALP`, `ALT`, and `AST` using the natural logarithm function: $y = \\log(x)$"},{"metadata":{"trusted":true},"cell_type":"code","source":"liver_log_transform = liver[transform_col].transform(np.log)\nliver_log_transform.columns = [\"log_\" + col for col in transform_col]\nliver_log_transform.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def boxPlotTransformedData(data, figsize = (10,4)):\n    fig, axes = plt.subplots(1, data.shape[1]-1, figsize = figsize)\n    for ax, col in zip(axes, data.columns[:-1]):\n        sns.boxplot(x = \"Disease\", y = col, data = data, ax = ax)\n        # Outlier Count\n        outlier_count = 0\n        for flag in data[\"Disease\"].cat.categories:\n            flag_disease = data.loc[data[\"Disease\"] == flag, col]\n            outlier_list = boxplot_stats(flag_disease).pop(0)['fliers']\n            outlier_count += len(outlier_list)\n        ax.set_title(\"Outlier Count: {} ({:.2f}%)\".format(outlier_count, 100*outlier_count/liver.shape[0]))\n    plt.tight_layout()\n\nboxPlotTransformedData(pd.concat([liver_log_transform, liver[\"Disease\"]], axis = 1))\nplt.suptitle(\"BOX PLOT OF LOG-TRANSFORMED VARIABLES\", size = 25, y = 1.05, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The outlier count has been reduced, now let's fit the log-transformed data into the logistic regression model, without `Gender_Male` predictor."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_log = pd.concat([X_train_untransformed, liver_log_transform.iloc[X_train.index]], axis = 1)\nX_test_log = pd.concat([X_test_untransformed, liver_log_transform.iloc[X_test.index]], axis = 1)\n\nmodel_log = fitLogisticRegression(X_train_log, y_train)\nmodel_log.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Square-Root Transformation\nTransform the predictors `ALP`, `ALT`, and `AST` using the square-root function: $y = \\sqrt{x}$"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"liver_sqrt_transform = liver[transform_col].transform(np.sqrt)\nliver_sqrt_transform.columns = [\"sqrt_\" + col for col in transform_col]\nliver_sqrt_transform.head()\n\nboxPlotTransformedData(pd.concat([liver_sqrt_transform, liver[\"Disease\"]], axis = 1))\nplt.suptitle(\"BOX PLOT OF SQRT-TRANSFORMED VARIABLES\", size = 25, y = 1.05, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The outlier count has been reduced, but not much compared to the log-transformed one. Now, let's fit the square-root-transformed data into the logistic regression model, without `Gender_Male` predictor."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sqrt = pd.concat([X_train_untransformed, liver_sqrt_transform.iloc[X_train.index]], axis = 1)\nX_test_sqrt = pd.concat([X_test_untransformed, liver_sqrt_transform.iloc[X_test.index]], axis = 1)\n\nmodel_sqrt = fitLogisticRegression(X_train_sqrt, y_train)\nmodel_sqrt.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\nNext, we evaluate our binary logistic regression classifier by using a confusion matrix as follows:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(\n    [[\"True Positive (TP)\", \"False Negative (FN)\"], [\"False Positive (FP)\", \"True Negative (TN)\"]],\n    index = [[\"Actual\", \"Actual\"], [\"Positive\", \"Negative\"]],\n    columns = [[\"Predicted\", \"Predicted\"], [\"Positive\", \"Negative\"]],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are several metrics considered in this case:\n- **Recall** is the proportion of actual positives that are classified correctly.\n\n<center>$Recall = \\dfrac{TP}{TP+FN}$</center>\n\n- **Precision** is the proportion of TP out of all observations predicted as positive.\n\n<center>$Precision = \\dfrac{TP}{TP+FP}$</center>\n\n- **F1-score** is the harmonic mean of precision and recall.\n\n<center>$F1 = \\dfrac{2 \\times Precision \\times Recall}{Precision+Recall}$</center>\n\nBut Precision, Recall, F1-score do not account for the TN. We'll introduce another metrics that measure the overall performance of the model which includes all values in the confusion matrix:\n\n- **Accuracy** is the proportion of observations that are classified correctly. This metric is not robust when the class is imbalance.\n\n<center>$Accuracy = \\dfrac{TP+TN}{TP+TN+FP+FN}$</center>\n\n- **MCC**, stands for Matthew's Correlation Coefficient, ranges between -1 and 1.\n    - MCC = 1 indicates a perfect positive correlation, the classifier is perfect $(FP = FN = 0)$\n    - MCC = -1 indicates a perfect negative correlation, the classifier always misclassifies $(TP = TN = 0)$\n    \n    - MCC = 0 indicates no correlation, the classifier randomly classify observations\n\n<center>$MCC = \\dfrac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluateLogReg(result, X_true, y_true):\n    eval_list = []\n    y_pred_prob = result.predict(sm.add_constant(X_true))\n    for threshold in np.linspace(0, 0.99, 50):\n        y_pred_cl = (y_pred_prob > threshold).astype(int)\n        eval_res = {\n            \"Threshold\": threshold,\n            \"Recall\": recall_score(y_true, y_pred_cl),\n            \"Precision\": precision_score(y_true, y_pred_cl),\n            \"F1\": f1_score(y_true, y_pred_cl),\n            \"Accuracy\": accuracy_score(y_true, y_pred_cl),\n            \"MCC\": matthews_corrcoef(y_true, y_pred_cl)\n        }\n        eval_list.append(eval_res)\n    eval_df = pd.DataFrame(eval_list)\n    return eval_df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"eval_logreg_all = evaluateLogReg(model_all, X_test, y_test)\neval_logreg_removed = evaluateLogReg(model_removed, X_test_removed, y_test)\neval_logreg_log = evaluateLogReg(model_log, X_test_log, y_test)\neval_logreg_sqrt = evaluateLogReg(model_sqrt, X_test_sqrt, y_test)\n\neval_logreg_list = [eval_logreg_all, eval_logreg_removed, eval_logreg_log, eval_logreg_sqrt]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"title_list = [\"ALL PREDICTORS\", \"WITHOUT GENDER_MALE\", \"LOG-TRANSFORMED\", \"SQRT-TRANSFORMED\"]\nfig, axes = plt.subplots(2, 2, figsize=(10,10))\n\nthresh_list = []\nfor ax, eval_df, title in zip(axes.flat, eval_logreg_list, title_list):\n    # LINE PLOT\n    eval_df = eval_df.drop([\"Accuracy\"], axis = 1)\n    lineplot = eval_df.plot(x = \"Threshold\", color = \"rgbk\", legend = False, ax = ax)\n    \n    # IDENTIFY CENTER\n    diff = abs(eval_df[\"Recall\"] - eval_df[\"Precision\"])\n    thresh_eq = eval_df[diff == min(diff)][\"Threshold\"].values[0]\n    ax.axvline(x = thresh_eq, ls = '--', color = \"y\")\n    ax.text(x = thresh_eq + 0.01, y = 0.05,\n            s = \"CENTER\", \n            fontsize = 12, color = \"y\")\n    \n    # F1 MEASURE\n    row_max_F1 = eval_df[eval_df[\"F1\"] == max(eval_df[\"F1\"])]\n    thresh_max_F1 = row_max_F1[\"Threshold\"].values[0]\n    ax.axvline(x = thresh_max_F1, ls = '--', color = \"b\")\n    ax.text(x = thresh_max_F1 - 0.01, y = 0.7, \n            s = \"MAX F1\", \n            horizontalalignment = 'right',\n            fontsize = 12, color = \"b\")\n    \n    # LOCATE MCC\n    mcc = row_max_F1[\"MCC\"].values[0]\n    ax.plot(thresh_max_F1, mcc, marker = 'x', markersize = 10, color = \"k\")\n    ax.text(x = thresh_max_F1 - 0.025, y = mcc, \n            s = \"MCC = {:.3f}\".format(mcc), \n            horizontalalignment = 'right',\n            fontsize = 12, fontweight = \"bold\", color = \"k\")\n\n    ax.set_xticks([0, 1] + [thresh_eq, thresh_max_F1])\n    ax.set_title(title, fontweight = \"bold\")\n    handles, labels = ax.get_legend_handles_labels()\n    thresh_list.append(thresh_max_F1)\n    \nplt.tight_layout()\nplt.legend(handles = handles, loc = \"center\",\n           bbox_to_anchor = (-0.1, -0.2),\n           shadow = True, ncol = 4)\nfig.suptitle(\"LOGISTIC REGRESSION MODEL EVALUATION\", size = 28, y = 1.05, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output of our logistic regression is $p$, the probability of a particular patient to be classified as having a liver disease. For each of the model, we iterate the threshold from 0 to 1. If $p > threshold$, then the patient is classified as having a liver disease. Our goal is to find the **optimum threshold** by comparing metrics. Here's the thought process:\n- First, identify the **center** of threshold, where Recall is equal to Precision.\n- In this case, we want to minimize the case of False Negative, where patient with liver disease is predicted to be healthy. This can be a threat to one's life because they will not get the treatment. Therefore, we prioritize **Recall over Precision**, where the threshold is smaller than the **center** threshold.\n- In reality, we also do care about minimizing the case of False Positive too, where healthy patient is predicted to have liver disease. This can triggers panic, but not as severe as False Negative case. Therefore we have to choose the threshold with maximum value of **F1 score** which take account the Precision value.\n- Lastly, out of the four models, we pick the best overall model by the highest **MCC** value."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"eval_logreg_df = pd.concat([eval_df[eval_df[\"Threshold\"] == thresh_list[idx]] for idx, eval_df in enumerate(eval_logreg_list)])\neval_logreg_df.index = title_list\neval_logreg_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In conclusion, we choose logistic regression model with log-transformed data at $threshold = 0.36367$. Next, we interpret and check the assumptions of this model."},{"metadata":{},"cell_type":"markdown","source":"## Model Interpretation\nOne of the advantages of logistic regression model is its interpretability. We can interpret the coefficient and its significancy to the target variable."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"final_model = model_log\nfinal_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall that the formula of logistic regression is $\\log{\\frac{p}{1-p}} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$ where the coefficients are exactly the $\\beta$s. It means that coefficients represent the change in logit value for one unit increase in the predictor. For example, one unit increase of `Age` will increase the logit value by 0.0196 whereas one unit increase of `Albumin` will decrease the logit value by 0.2576.\n\nWe can apply exponent to the coefficient to get the **ratio of odds**. This represent how many percent increase in the odds for one unit increase in the predictor. Let's take a look on the table below."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_interpret = pd.DataFrame(final_model.params, columns = [\"Logit Difference\"])\nmodel_interpret[\"Ratio of Odds\"] = model_interpret[\"Logit Difference\"].transform(np.exp)\nmodel_interpret","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **ratio of odds** of `Age` is 1.01983 means that for one unit increase in `Age`, we expect to see 1.983% increase in the odds of a patient having liver disease. On the other hand, the **ratio of odds** of `Albumin` is 0.77290 means that for one unit increase in `Albumin`, we expect to see 22.71% decrease in the odds of a patient having liver disease.\n\nTake note that this is the increase/decrease in the **odds** not **probability**. The ratio of probability changes depending on the predictor value as follows:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for age in range(20, 23):\n    print(f\"\\nComparison of Age {age} with Age {age+1}\")\n    print(\"=================================\")\n    interpret_df = pd.DataFrame([[1, age, 5, 50, 5, 5, 5, 5],\n                                 [1, age+1, 5, 50, 5, 5, 5, 5]], columns=[\"const\"] + list(X_train_log.columns))\n    prob_interpret = final_model.predict(interpret_df)\n    logit_interpret = prob_interpret.transform(logit)\n    odds_20 = np.exp(logit_interpret[0])\n    odds_21 = np.exp(logit_interpret[1])\n\n    print(\"Logit Difference: {:.5f}\".format(logit_interpret[1] - logit_interpret[0]))\n    print(\"Ratio of Odds: {:.5f}\".format(odds_21/odds_20))\n    print(\"Ratio of Probability: {:.5f}\".format(prob_interpret[1]/prob_interpret[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to interpret the coefficient of `log_ALP`, `log_ALT`, and `log_AST`? The original value must be log-transformed first, then interpreted just like above. Here's the illustration of how one unit increase of log-transformed value looks like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_range = (1, 4.25)\nx_axis = np.linspace(*x_range, 100)\nx_ticks = np.arange(*x_range)\ny_ticks = np.exp(x_ticks)\n\nplt.plot(x_axis, np.exp(x_axis))\nplt.scatter(x_ticks, y_ticks, marker = \"x\")\nfor x, y in zip(x_ticks, y_ticks):\n    plt.axvline(x, color = \"k\", ls = \"--\", alpha = 0.25)\n    plt.axhline(y, color = \"k\", ls = \"--\", alpha = 0.25) \n\nplt.xticks(x_ticks)\nplt.yticks(y_ticks)\nplt.xlabel(\"Log-Transformed Value\")\nplt.ylabel(\"Original Value\")\nplt.title(\"ILLUSTRATION: ONE UNIT INCREASE OF LOG\", fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Assumptions\nThere are three assumptions of logistic regression model:\n\n### Linearity\nWe assume that the independent variables (predictors) are linearly related to the log odds of the target variable `Disease`, meaning that the data is assumed to be linearly separable. "},{"metadata":{},"cell_type":"markdown","source":"### No Multicollinearity\nWe expect the model to have little to no multicollinearity. It is a condition where at least two predictors have a strong linear relationship. Multicollinearity exists if the Variance Inflation Factor (VIF) value is greater than 10."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"vif_list = []\nfor idx, col in enumerate(final_model.model.exog_names[1:]):\n    vif_dict = {\"Variable\": col,\n                \"VIF\":  variance_inflation_factor(final_model.model.exog, idx+1)}\n    vif_list.append(vif_dict)\n    \npd.DataFrame(vif_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can conclude that there is only little multicollinearity exist in our model."},{"metadata":{},"cell_type":"markdown","source":"### Independence of Observation\nWe assume that the observations are independent and are not a repeated measurement of the same patient."},{"metadata":{},"cell_type":"markdown","source":"# Modeling: K-Nearest Neighbour\nK-Nearest Neighbour (KNN) is a non-parametric, lazy learning classification algorithm. No model are being learned, it uses the training examples to classify new data points. Here's the algorithm:\n1. Specify a positive integer $k$\n2. Compute the similarity/distance from the unlabeled data point to every points on the training examples. The most commonly used is Euclidean distance.\n3. Assign the class by majority voting based on $k$-nearest training examples. If tie, then assign randomly. To avoid this, choose $k$ to be an odd positive number.\n4. Repeat step 2-3 for each unlabeled data points in the test set."},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\nThis is a necessary step before fitting the data into KNN. The range of each predictor is not the same, they should be treated equally so that each feature’s contribution to the distance formula is equally weighted. We will perform two types of scaling to shrink/expand the range:\n1. Min-Max Normalization, scale a variable to have a range between 0 and 1\n2. Standardization, transforms data to have a mean of 0 and a standard deviation of 1"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_removed.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def scaleData(data, typ):\n    if typ == \"MinMax\":\n        scaler = MinMaxScaler()\n    elif typ == \"Standard\":\n        scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data)\n    data_scaled_df = pd.DataFrame(data_scaled, columns = data.columns)\n    return data_scaled_df\n\n# Min-Max Normalization\nX_train_minmax = scaleData(X_train_removed, \"MinMax\")\nX_test_minmax = scaleData(X_test_removed, \"MinMax\")\nX_train_minmax.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the min and max of all variables are 0 and 1 respectively."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Standardization\nX_train_standard = scaleData(X_train_removed, \"Standard\")\nX_test_standard = scaleData(X_test_removed, \"Standard\")\nX_train_standard.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the mean and std of all variables are 0 and 1 respectively."},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\nSince there are no model to be learned, we directly evaluate our binary KNN classifier by using a confusion matrix as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluateKNN(X_train, y_train, X_test, y_test, range_k):\n    eval_list = []\n    for k in range(*range_k, 2):\n        knn = KNeighborsClassifier(n_neighbors = k)\n        knn.fit(X_train, y_train)\n        y_pred_cl = knn.predict(X_test)\n        eval_res = {\n            \"k\": k,\n            \"Recall\": recall_score(y_test, y_pred_cl),\n            \"Precision\": precision_score(y_test, y_pred_cl),\n            \"F1\": f1_score(y_test, y_pred_cl),\n            \"Accuracy\": accuracy_score(y_test, y_pred_cl),\n            \"MCC\": matthews_corrcoef(y_test, y_pred_cl)\n        }\n        eval_list.append(eval_res)\n    eval_df = pd.DataFrame(eval_list)\n    return eval_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Normalized Data (Min-Max)\nrange_k = (1, X_train_removed.shape[0])\neval_knn_minmax = evaluateKNN(X_train_minmax, y_train, \n                              X_test_minmax, y_test, \n                              range_k)\neval_knn_minmax.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Use Standardized Data\neval_knn_standard = evaluateKNN(X_train_standard, y_train, \n                                X_test_standard, y_test, \n                                range_k)\neval_knn_standard.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we compare the performance of the two scaling types based on Recall, F1-score, and MCC. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"metric_list = [\"Recall\", \"F1\", \"MCC\"]\nfig, axes = plt.subplots(1, 3, figsize=(15,5))\n\neval_minmax = pd.melt(eval_knn_minmax, id_vars = \"k\",\n                      var_name = \"Metric\", value_name = \"Value\")\neval_minmax[\"Scaling Type\"] = \"Min-Max\"\neval_standard = pd.melt(eval_knn_standard, id_vars = \"k\",\n                        var_name = \"Metric\", value_name = \"Value\")\neval_standard[\"Scaling Type\"] = \"Standard\"\neval_df = pd.concat([eval_minmax, eval_standard])\n\nfor ax, metric in zip(axes.flat, metric_list):\n    df = eval_df[eval_df[\"Metric\"] == metric]\n    line = sns.lineplot(data = df, x = \"k\", y = \"Value\",\n                        hue = \"Scaling Type\", palette = \"gray\", ax = ax)\n    line.legend_.remove()\n    ax.set_title(metric.upper(), size = 15, fontweight = \"bold\")\n    \nplt.tight_layout()\nplt.legend(handles = axes[0].get_legend_handles_labels()[0], loc = \"center\",\n           bbox_to_anchor = (-0.7, -0.2),\n           shadow = True, ncol = 3)\nfig.suptitle(\"K-NEAREST NEIGHBOUR MODEL EVALUATION\", size = 28, y = 1.05, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As explained on the model evaluation of logistic regression, we prioritize **Recall over Precision** and taking maximum value of **F1 score**. So, we choose the model with **Min-Max normalization** data because the Recall and F1 is relatively higher than standardized one. Next, we choose the optimal number of neighbours $k$."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,5))\neval_df = eval_knn_minmax.drop([\"Accuracy\"], axis = 1)\n\n# LINE PLOT\neval_df.plot(x = \"k\", color = \"rgbk\", ax = ax)\n\n# F1 SCORE\nrow_max_F1 = eval_df[eval_df[\"F1\"] == max(eval_df[\"F1\"])]\nk_max_F1 = row_max_F1[\"k\"].values[0]\nax.axvline(x = k_max_F1, ls = '--', color = \"b\")\nax.text(x = k_max_F1 + 5, y = 0.2, \n        s = \"MAX F1\",\n        fontsize = 12, color = \"b\")\nax.set_xticks(list(ax.get_xticks())[1:-1] + [k_max_F1])\n\nplt.legend(loc = \"center\", bbox_to_anchor = (0.5, -0.2),\n           shadow = True, ncol = 4)\nfig.suptitle(\"K-NEAREST NEIGHBOUR METRICS (MIN-MAX NORMALIZATION)\", size = 16, fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot, we can see the Recall is relatively higher than Precision, that's what we want. Then we choose the optimal $k=137$ by considering the maximum F1 score because we do care both Recall and Precision."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"eval_knn_list = [eval_knn_minmax, eval_knn_standard]\neval_knn_df = pd.concat([eval_df[eval_df[\"k\"] == 137] for eval_df in eval_knn_list])\neval_knn_df.index = [\"MIN-MAX NORMALIZATION\", \"STANDARDIZATION\"]\neval_knn_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the same number of neighbours $k$, min-max normalization is overall outperforming the standarization one."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nFinally, let's compare the perfomance of logistic regression model and k-nearest neighbour side by side."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(\n    [eval_logreg_df.loc[\"LOG-TRANSFORMED\"][1:],\n     eval_knn_df.loc[\"MIN-MAX NORMALIZATION\"][1:]]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression with log-transformed data is better in recall but worse in other metrics. To overcome this dilemma, let's introduce another performance measurement which is the **Receiver Operating Characteristic (ROC) curve**. It is a probability curve which plots True Positive Rate (Recall) against False Positive Rate. Then the area under the ROC curve, called as **Area Under Curve (AUC)**, measures the degree of classification separability. Higher the AUC, better the model is at distinguishing between patients with liver disease and no disease.\n- AUC near to 1 indicates the model has good measure of separability.\n- AUC near to 0 means it has worst measure of separability.\n- When AUC is 0.5, it means model has no class separation capacity whatsoever.\n\n<center><a href=\"https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\">Illustration Reference</a></center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/roc-curvepng/ROC curve.png\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# BASELINE\nbase_probs = np.zeros(len(y_test))\nbase_fpr, base_tpr, _ = roc_curve(y_test, base_probs)\nbase_auc = roc_auc_score(y_test, base_probs)\n\n# LOGISTIC REGRESSION\nlogreg_probs = model_log.predict(sm.add_constant(X_test_log))\nlogreg_fpr, logreg_tpr, _ = roc_curve(y_test, logreg_probs)\nlogreg_auc = roc_auc_score(y_test, logreg_probs)\n\n# KNN\nknn_opt_model = KNeighborsClassifier(n_neighbors = 137).fit(X_train_minmax, y_train)\nknn_probs = knn_opt_model.predict_proba(X_test_minmax)[:,1]\nknn_fpr, knn_tpr, _ = roc_curve(y_test, knn_probs)\nknn_auc = roc_auc_score(y_test, knn_probs)\n\n# PLOT ROC\nplt.plot(base_fpr, base_tpr, linestyle = '--', label = \"Baseline (AUC = {:.3f})\".format(base_auc))\nplt.plot(logreg_fpr, logreg_tpr, linestyle = '-', label = \"Logistic Regression (AUC = {:.3f})\".format(logreg_auc), color = \"r\")\nplt.plot(knn_fpr, knn_tpr, linestyle = '-', label = \"KNN (AUC = {:.3f})\".format(knn_auc), color = \"g\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate (Recall)\")\nplt.title(\"RECEIVER OPERATING CHARACTERISTIC (ROC) CURVE\", fontweight = \"bold\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In conclusion, we choose the logistic regression model because the AUC score is better than KNN, indicating that the model have a good capability in separating patients with liver disease and no disease. Secondly, the model's interpretability gives us insight on which predictors to be used in classifying whether patient have liver disease or not, whereas we couldn't interpret KNN. From the model summary, we can conclude:\n- `Gender` doesn't significantly affect the probability of patient having a liver disease.\n- Older patient have significantly higher probability of having a liver disease.\n- Higher `Albumin` present in the blood significantly decrease the probability of patient having a liver disease.\n- Other test results such as higher `DB/TB Percentage`, `Globulin`, `ALP`, `ALT`, `AST` will significantly increase the probability of patient having a liver disease."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"final_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using log-transformed data and $threshold = 0.36367$, here's our final logistic regression model performance: "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(eval_logreg_df.loc[\"LOG-TRANSFORMED\"][1:])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":4}