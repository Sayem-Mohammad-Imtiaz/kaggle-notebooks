{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<b>Importing necessary libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd     # Data Wrangling & Preprocessing\nimport numpy as np      # Data Wrangling & Preprocessing\nimport seaborn as sns   # Plotting charts\nimport matplotlib.pyplot as plt    # Plotting charts\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split    #Splitting the data into training & testing set \nfrom sklearn.preprocessing import OneHotEncoder    #Encoding categorical variables\nfrom sklearn.pipeline import Pipeline    # To create pipelines for preprocessing steps\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression    # Linear Regression Model\nfrom sklearn.ensemble import RandomForestRegressor   # RandomForest Regressor Model\nfrom sklearn.metrics import mean_squared_error    # RMSE Evaluation Metric for Regression \nfrom sklearn.model_selection import cross_val_score    # To Compute validation score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nimport pickle    # To export the trained model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(r'../input/california-housing-prices/housing.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each row represents a district and there are 10 attributes in the dataset. Now let’s use the info() method which is useful for getting a quick description of the data, especially the total number of rows, the type of each attribute, and the number of non-zero values:","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 20,640 instances in the dataset. Note that the total_bedrooms attribute has only 20,433 non-zero values, which means 207 districts do not contain values. We will have to deal with that later.\n\nAll attributes are numeric except for the ocean_proximity field. Its type is an object, so it can contain any type of Python object. You can find out which categories exist in that column and how many districts belong to each category by using the value_counts() method:","metadata":{}},{"cell_type":"code","source":"data['ocean_proximity'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another quick way to get a feel for what kind of data you’re dealing with is to plot a histogram for each numerical attribute:","metadata":{}},{"cell_type":"code","source":"data.hist(bins=50, figsize=(16,12))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Split the data into Training and Testing set</b><br>\nCreating a test set is theoretically straightforward: select some instances at random, typically 20% of the dataset (or less if your dataset is very large), and set them aside:","metadata":{}},{"cell_type":"code","source":"train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s take a closer look at the histogram of median income, as most median income values cluster around 1.5 to 6, but some median income goes well beyond 6.\n\nIt is important to have a sufficient number of instances in your dataset for each stratum, otherwise, the estimate of the importance of a stratum may be biased. This means that you should not have too many strata and that each stratum should be large enough:","metadata":{}},{"cell_type":"code","source":"data['income_cat'] = pd.cut(data['median_income'], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\ndata['income_cat'].hist()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Stratified Sampling on Dataset</b><br>\nStratified Sampling is a method of sampling from a population that can be divided into a subset of the population.","metadata":{}},{"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"income_cat\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]\nprint(strat_test_set['income_cat'].value_counts() / len(strat_test_set))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now you need to remove the Income_cat attribute added by us to get the data back to its form:\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop('income_cat', axis=1, inplace=True)\ndata = strat_train_set.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now before creating a machine learning model for house price prediction with Python let’s visualize the data in terms of longitude and latitude:\ndata.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n                s=data['population']/100, label='population', figsize=(10,7),\n                cmap=plt.get_cmap('jet'), colorbar=True)\n\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.legend() \nplt.show()\n\n#Note: Add a paramters 'c = median_house_value' is you're working in jupyter notebook. Not working in kaggle.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph shows house prices in California where red is expensive, blue is cheap, larger circles indicate areas with a larger population.","metadata":{}},{"cell_type":"markdown","source":"<b> Finding Correlations</b></br>\n\nSince the dataset is not too large, you can easily calculate the standard correlation coefficient between each pair of attributes using the corr() method:","metadata":{}},{"cell_type":"code","source":"corr_matrix = data.corr()\nprint(corr_matrix.median_house_value.sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation ranges are between -1 and 1. When it is close to 1 it means that there is a positive correlation and when it is close to -1 it means that there is a negative correlation. When it is close to 0, it means that there is no linear correlation.","metadata":{}},{"cell_type":"markdown","source":"And now let’s look at the correlation matrix again by adding three new columns to the dataset; rooms per household, bedrooms per room and population per household:","metadata":{}},{"cell_type":"code","source":"data[\"rooms_per_household\"] = data[\"total_rooms\"]/data[\"households\"]\ndata[\"bedrooms_per_room\"] = data[\"total_bedrooms\"]/data[\"total_rooms\"]\ndata[\"population_per_household\"] = data[\"population\"]/data[\"households\"]\n\ncorr_matrix = data.corr()\nprint(corr_matrix[\"median_house_value\"].sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Data Preparation </b><br>\n\nNow, this is the most important step before a train a machine learning model for the task of house price prediction. Now let’s perform all the necessary data transformations:\n    ","metadata":{}},{"cell_type":"code","source":"# Data Preparation\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n\nmedian = housing[\"total_bedrooms\"].median()\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, there are many data transformation steps that need to be performed in the correct order. Fortunately, Scikit-Learn provides the Pipeline class to help you with such sequences of transformations. Here is a small pipeline for numeric attributes:","metadata":{}},{"cell_type":"code","source":"num_pipeline = Pipeline([\n    ('imputer',SimpleImputer(strategy=\"median\")),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing_num_tr = num_pipeline.fit_transform(housing_num)\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\nfull_pipeline = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs),\n])\nhousing_prepared = full_pipeline.fit_transform(housing)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to display scores\ndef display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: \", scores.mean())\n    print(\"Standard Deviation: \", scores.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Linear Regression for House Price Prediction with Python</b><br>\n\nNow I will use the linear regression algorithm for the task of house price prediction with Python:","metadata":{}},{"cell_type":"code","source":"# Model Training - LR\nlin_reg_model = LinearRegression()\nlin_reg_model.fit(housing_prepared, housing_labels)\n\ndata = housing.iloc[:5]\nlabels = housing_labels.iloc[:5]\ndata_preparation = full_pipeline.transform(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions and RMSE\nhousing_predictions = lin_reg_model.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint('RMSE value for Linear Regression: ', lin_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross Validation\nscores = cross_val_score(lin_reg_model, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()\ndisplay_scores(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Random Forest Regressor<b>","metadata":{}},{"cell_type":"code","source":"# Model Training - RFR\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions and RMSE\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nprint('RMSE value for Random Forest Regressor: ', forest_rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross Validation\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}