{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)\n%matplotlib inline\npd.options.display.float_format = '{:,}'.format\n\n#reading raw data\ndata_raw = pd.read_csv('/kaggle/input/student-performance-data-set/student-por.csv')\ndisplay(data_raw.head())\n\n#creating catagorical columns list and numeric columns list\ncat_columns = ['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian',\n               'schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']\nnum_columns = ['Medu','Fedu','traveltime','studytime','famrel','freetime','goout','Dalc','Walc','health']\ncont_columns = ['age','failures','absences','G1','G2','G3']","metadata":{"_uuid":"00602cb3-10f4-4de9-b44d-3c5f9a78005b","_cell_guid":"b649b87a-8902-4110-acc3-8be660412dab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In this kernel I'll fit a regression model on the student-performance-data-set only, to understand the data and its columns refer to [Student Performance Data Visualization](https://www.kaggle.com/mostafafathy4869/student-performance-data-visualization)","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"_uuid":"af59ea4a-cdfe-4ef2-ad49-ddcbaf1a67eb","_cell_guid":"e03918de-c5ef-4fbc-94da-6ee7730775b1","trusted":true}},{"cell_type":"markdown","source":"##### Most of the Machine learning algorithms can not handle categorical variables unless we convert them to numerical values. Many algorithmâ€™s performances vary based on how Categorical variables are encoded. [All about Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)\n\n##### For this reason I'll be using 3 method for encoding catagorical variables\n* **One Hot Encoding**\n* **Label Encoder**\n* **Leave One Out Encoding**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom category_encoders import LeaveOneOutEncoder\n\n#Creating different dataset copies for different label encoder\n#OneHotEncoding\ndummy_df = data_raw.copy()\n#target column\ntarget = dummy_df.pop('G3')\n#LeaveOneOutEncoding\nloo_df = dummy_df.copy()\n#LabelEncoder\nle_df = dummy_df.copy()\n\n#Creating Encoder\nloo = LeaveOneOutEncoder()\nle = LabelEncoder()\n#Encoding Catagorical Variables\nfor col in cat_columns:\n    loo_df[col] = loo.fit_transform(loo_df[col],target)\n    le_df[col] = le.fit_transform(le_df[col])\n    dummy_df = pd.get_dummies(dummy_df,columns=[col],drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection","metadata":{"_uuid":"1c7ef32d-3b0f-4b41-9b92-c1d95aac8eb8","_cell_guid":"c0968881-e3f1-40a0-9be9-efa2b372213b","trusted":true}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom lightgbm import LGBMRegressor\n\nfrom catboost import CatBoostRegressor\n\nfrom xgboost import XGBRegressor","metadata":{"_uuid":"bbfd2132-8576-490d-8f2d-5a2892b224c0","_cell_guid":"a7624b42-572c-4c25-8a13-36cb38ed1f8b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ml(data,target,model,pr=False):\n    X_train,X_test,y_train,y_test = train_test_split(data, target, random_state=0)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    r2_square = metrics.r2_score(y_test, y_pred)\n    if pr==True:\n        print('MAE:', mae)\n        print('MSE:', mse)\n        print('RMSE:', rmse)\n        print('R2 Square', r2_square)\n        print('__________________________________')\n    return model,mae,mse,rmse,r2_square\n\nMLA = [\n    LinearRegression(),\n    SVR(kernel='rbf'),\n    SVR(kernel='poly'),\n    LGBMRegressor(),\n    CatBoostRegressor(verbose=False),\n    XGBRegressor(),\n    GradientBoostingRegressor(),\n    RandomForestRegressor()\n]\n\nloo_models = {}\nle_models = {}\ndummy_models = {}","metadata":{"_uuid":"43429e8d-58c0-4498-bc18-c40cdee301d4","_cell_guid":"7f92c76f-af42-4c9b-baf8-88c49147673f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#Creating DataFrame to save model performance\nevaluation_metrics = ['MAE','MSE','RMSE','R2_Square']\nloo_df_performance = pd.DataFrame(columns=evaluation_metrics,)\nle_df_performance = pd.DataFrame(columns=evaluation_metrics)\ndummy_df_performance = pd.DataFrame(columns=evaluation_metrics)\n\nfor alg in MLA:\n    _name = alg.__class__.__name__\n    if _name == 'SVR':\n        _name = f'{_name}_{alg.kernel}'\n    \n    loo_model, loo_mae, loo_mse, loo_rmse, loo_r2_square = ml(loo_df,target,alg)\n    le_model, le_mae, le_mse, le_rmse, le_r2_square = ml(le_df,target,alg)\n    dummy_model, dummy_mae, dummy_mse, dummy_rmse, dummy_r2_square = ml(dummy_df,target,alg)\n    \n    loo_models[_name] = loo_model\n    le_models[_name] = le_model\n    dummy_models[_name] = dummy_model\n    \n    loo_df_performance = loo_df_performance.append(pd.Series({'MAE':loo_mae ,'MSE':loo_mse ,'RMSE':loo_r2_square ,'R2_Square':loo_r2_square},name=_name))\n    le_df_performance = le_df_performance.append(pd.Series({'MAE':le_mae ,'MSE':le_mse ,'RMSE':le_rmse ,'R2_Square':le_r2_square},name=_name))\n    dummy_df_performance = dummy_df_performance.append(pd.Series({'MAE':dummy_mae ,'MSE':dummy_mse ,'RMSE':dummy_rmse ,'R2_Square':dummy_r2_square},name=_name))","metadata":{"_uuid":"498dcb3a-06a8-43e2-adc3-6eb4f018d174","_cell_guid":"df13f60f-d305-4e12-9b14-ff24e2a80e9f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sortting = 'MAE'\nprint('Dummies Dataset')\ndisplay(dummy_df_performance.sort_values(sortting))\nprint('LabelEncoder Dataset')\ndisplay(le_df_performance.sort_values(sortting))\nprint('LeaveOneOut Encoder Dataset')\ndisplay(loo_df_performance.sort_values(sortting))","metadata":{"_uuid":"4ee0fcea-13bb-4dab-9af7-c3bd22f17e77","_cell_guid":"a6d14ffc-7d70-4c04-92e7-0ec89732aabd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rfr,_,_,_,_ = ml(loo_df,target,RandomForestRegressor())\n#plot_data = zip(loo_df.columns,rfr.feature_importances_)\n#plot_data = sorted(plot_data, key=lambda x:x[1], reverse=True)\n#x = [col for col,val in plot_data[:5]]\n#y = [val for col,val in plot_data[:5]]\n#plt.figure(figsize=(6,4))\n#plt.bar(x,y)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### RandomForestRegressor comes at the top for each dataset with XGBRregressor perform the best for LeaveOneOut Encoder Dataset, I will choose both algorithems to try hyperparameter tuning with optuna","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tuning","metadata":{"_uuid":"794d3108-506d-41a7-9675-34525b548705","_cell_guid":"bae45b73-cce7-4ea2-a4e2-4ecf538b00d3","trusted":true}},{"cell_type":"code","source":"import optuna as opt","metadata":{"_uuid":"8f0d6a7e-bcb2-4c6c-af85-8739e1748d64","_cell_guid":"7cb6cacf-6eec-4cbc-99fe-adcc6fa6a055","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBRegressor","metadata":{}},{"cell_type":"code","source":"def objective_xgbr(trial):\n    \n    xgb_params = {}\n    xgb_params['eval_metric'] = 'rmse'\n    xgb_params['eta'] = trial.suggest_uniform('eta', 0.05, 0.6)\n    xgb_params['max_depth'] = trial.suggest_int('max_depth', 2,15)\n    xgb_params['subsample'] = trial.suggest_uniform('subsample', 0.2,1)\n    \n    \n    \n    \n    X_train,X_test,y_train,y_test = train_test_split(loo_df, target, random_state=0)\n    model = XGBRegressor(**xgb_params)\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n    return rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy = opt.create_study( direction='minimize')\nstudy.optimize(objective_xgbr, n_trials=1000)","metadata":{"_uuid":"52e1c2b5-249a-4a16-9fbe-dbef01aa5eae","_cell_guid":"b6e6efb9-feec-4b4c-a760-a2fb18506423","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt.visualization.plot_optimization_history(study)","metadata":{"_uuid":"3ab364a2-c6cd-43ad-b0d1-873483a30673","_cell_guid":"f5778ce0-43fb-480f-9afc-ad6412dd8dfa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Tuned Model')\n_=ml(loo_df,target,XGBRegressor(eta=0.27565560438172737,max_depth=4,subsample=0.2631374852738718),pr=True)\nprint('Defaul Model')\n_=ml(loo_df,target,XGBRegressor(),pr=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We can see huge improving about 0.043 for tuning model which gives 0.995 R2 score","metadata":{}},{"cell_type":"markdown","source":"# RandomForestRegressor","metadata":{"_uuid":"fcbd1219-021b-42e5-970b-d40cc0dddb99","_cell_guid":"8676cb79-fa38-40e7-9fdf-e6db9dbb75f2","trusted":true}},{"cell_type":"code","source":"def objective_rfr(trial):\n    \n    rfr_params = {}\n    rfr_params['n_estimators'] = trial.suggest_int('n_estimators',50,800)\n    rfr_params['max_depth'] = trial.suggest_int('max_depth', 1, 100)\n    rfr_params['max_features'] = trial.suggest_int('max_features', 1, 20)\n    rfr_params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 2, 50)\n    rfr_params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 50)\n    \n    \n    \n    X_train,X_test,y_train,y_test = train_test_split(loo_df, target, random_state=0)\n    model = RandomForestRegressor(**rfr_params)\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n    return rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nstudy = opt.create_study( direction='minimize')\nstudy.optimize(objective_rfr, n_trials=1000)","metadata":{"_uuid":"83b3750f-5641-4c85-9711-9bde7603e456","_cell_guid":"6efbea66-6692-4313-81f0-4ebfcdcf8273","_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt.visualization.plot_optimization_history(study)","metadata":{"_uuid":"59978099-0c4c-483d-b8cf-22dbe78453b8","_cell_guid":"bc033add-8f8c-44a0-9697-63bd1a3d6710","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {'n_estimators': 68,\n                 'max_depth': 55,\n                 'max_features': 8,\n                 'min_samples_leaf': 3,\n                 'min_samples_split': 5}","metadata":{"_uuid":"79f022e6-85b2-4913-8e21-d46b6ef75484","_cell_guid":"b1eda8b7-093c-47a0-9b85-73b89563f03a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Tuned Model')\n_=ml(loo_df,target,RandomForestRegressor(**best_params),pr=True)\nprint('Defaul Model')\n_=ml(loo_df,target,RandomForestRegressor(),pr=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### we can see a 0.01 increase after tuning random forest regressor","metadata":{}},{"cell_type":"markdown","source":"# Final performance test","metadata":{"_uuid":"fb20edaa-fc12-4264-9e88-5035d41551a9","_cell_guid":"165415ca-6880-490b-8850-f4921ef51639","trusted":true}},{"cell_type":"markdown","source":"##### I'll take 200 entries from the data set to act as test dataset for our model and see how the model will perform with it","metadata":{}},{"cell_type":"code","source":"train,test,train_y,test_y = train_test_split(loo_df,target,random_state=0,test_size=200)\n\nX_train,X_test,y_train,y_test = train_test_split(train,train_y,random_state=0)\n\nxgbr = XGBRegressor(eta=0.27565560438172737,\n                     max_depth=4,\n                     subsample=0.2631374852738718)\n\n\nxgbr.fit(X_train,y_train)\ny_pred = xgbr.predict(test)\nmae = metrics.mean_absolute_error(test_y, y_pred)\nmse = metrics.mean_squared_error(test_y, y_pred)\nrmse = np.sqrt(metrics.mean_squared_error(test_y, y_pred))\nr2_square = metrics.r2_score(test_y, y_pred)\nprint('MAE:', mae)\nprint('MSE:', mse)\nprint('RMSE:', rmse)\nprint('R2 Square', r2_square)\nprint('__________________________________')","metadata":{"_uuid":"2640a2cd-8104-4511-8f32-e121a82d37a0","_cell_guid":"010de8aa-0eb5-4fda-ad97-96d7ef927e92","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### The r2 score drops to 0.97 but it is not that bad, and the model is pretty good for this dataset.\n\n##### Feel free to play around with the dataset and telling me what you think","metadata":{}}]}