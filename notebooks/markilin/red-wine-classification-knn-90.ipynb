{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Task #1 Red Wine Quality Prediction Using KNN","metadata":{}},{"cell_type":"markdown","source":"Task Details\n\nThis task is suitable for beginners who have just stepped into the world of data science. It requires you to use the K Nearest Neighbours algorithm to make a prediction on the 'quality' column of the dataset. You are encouraged to explore the different parameters you can work with in your model and understand the importance of data understanding and feature selection.\n(Note: Beginners can use the \"KNeighborsClassifier\" class available under \"sklearn.neighbors\" )\n\nExpected Submission\n\nThe submission must be a Notebook containing the process of Exploratory Data Analysis and making of the model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'accuracy' metric.\n\nEvaluation\n\nThe aim is to understand the KNN algorithm and its parameters, and evaluation would be based on the accuracy of the model.","metadata":{}},{"cell_type":"code","source":"#main libraries and graphics\nimport os\nimport numpy as np\nimport pandas as pd\n\n#ML libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#metrics\nfrom sklearn.metrics import roc_auc_score\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------  Reading data  ----------------------------------------------------","metadata":{}},{"cell_type":"code","source":"#reading data from file\n#creating DataFrame included red wine data\n\nwith open(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\") as red_wine_file:\n    red_wine_data = pd.read_csv(red_wine_file, delimiter=',')\n\n#reading data structure information\n\nred_wine_data.info(verbose = True, show_counts = True)\n\n#data example\n\nred_wine_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------- Statistical characteristics of variables  -----------------------------------------","metadata":{}},{"cell_type":"code","source":"#mean, median, min, max, std. error\n\nagg_func_list =  ['mean', 'median', 'min', 'max', 'std']\ncolumns_agg_func_list = {}\nfor column in red_wine_data.columns[:11]: columns_agg_func_list[column] = agg_func_list\nred_wine_data.agg(columns_agg_func_list).round(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"red_wine_data.hist('quality')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this example, we are faced with the problem of unbalanced classification, so a simple parameter of the model's\n accuracy will not reflect its true performance. Instead, we use the area under the receiver operating \ncharacteristics curve (ROC AUC).","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------- Correlation of variables  ----------------------------------------------","metadata":{}},{"cell_type":"code","source":"#creating the correlation matrix\n#'pearson' - standard correlation coefficient metod\n\ncorr_matrix = red_wine_data.corr(method = 'pearson')\ncorr_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the correlation coefficient |r| is greater than 0.95, then it is assumed that there is an almost linear relationship between the parameters. If the correlation coefficient |r| is in the range from 0.8 to 0.95, it indicates a strong degree of linear relationship between the parameters. If 0,6 < |r| < 0,8, they say that there is a linear relationship between the parameters. At |r| < 0.4, it is usually assumed that the linear relationship between the parameters could not be detected.","metadata":{}},{"cell_type":"markdown","source":"There is a weak correlation between the two input variables (at the very boundary of the metric) and the target \nvariable. In this regard, I assume that to build the algorithm, you will need to use more input variables.","metadata":{}},{"cell_type":"markdown","source":"----------------------------------------------- Data preparation ---------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"Let's illustrate the input variables that have a correlation between them. To do this, we will filter the values of the correlation matrix.","metadata":{}},{"cell_type":"code","source":"corr_matrix_copy = corr_matrix.copy()\nfor row in corr_matrix_copy.index:\n    for column in corr_matrix_copy[row].index: \n        if abs(corr_matrix_copy[row][column]) < 0.6 or abs(corr_matrix_copy[row][column]) == 1.0: \n            corr_matrix_copy[row][column] = '---'\n        else:\n            corr_matrix_copy[row][column] = round(corr_matrix_copy[row][column], 2)\n\ncorr_matrix_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is obvious that there is a significant correlation between some input variables, and therefore, when building a model, it makes sense to reduce the number of input variables. I will remove the following variables: \"citric acid\", \"density\", \"pH\", \"total sulfur dioxide\". ","metadata":{}},{"cell_type":"code","source":"#extracting target variable, cleaning input data\ntarget = np.array(red_wine_data.pop('quality'))\nred_wine_data_cleaned = red_wine_data.drop(['citric acid', 'density', 'pH', 'total sulfur dioxide'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting cleaned data (train/test = 80/20)\ntrain_X, test_X, train_y, test_y = train_test_split(red_wine_data_cleaned, target, stratify = target,\n                                                    test_size=0.2, shuffle = True, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scalling features\nscaler = StandardScaler()\nscaler.fit(train_X)\ntrain_X_scaled = scaler.transform(train_X)\ntest_X_scaled = scaler.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using PCA (Principal component analysis), we can study the cumulative sample variance of these features in order to understand which features explain most of the variance in the data.","metadata":{}},{"cell_type":"code","source":"#PCA test (n_components=7, to see the explained variance of all generated components)\npca_test = PCA(n_components = 7)\npca_test.fit(train_X_scaled)\nevr = pca_test.explained_variance_ratio_\ncvr = np.cumsum(pca_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr\ndisplay(pca_df.head(7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probably, with a small number of input variables (we have 7 of them), optimization by the principal component method does not make sense. The table shows that the level of more than 90% of the explained variance is achieved when using 6 main components.","metadata":{}},{"cell_type":"code","source":"#i select the number of main components to achieve maximum 'roc_auc_score' metric, it turned out \"5\"\npca = PCA(n_components = 5)\npca.fit(train_X_scaled)\ntrain_X_scaled_pca = pca.transform(train_X_scaled)\ntest_X_scaled_pca = pca.transform(test_X_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------ Creating the model (KNN classification) -----------------------------------------","metadata":{}},{"cell_type":"code","source":"#creating the model and the selection of a parameter 'n_neighbors' to maximize the metric 'roc_auc_score'\nmetric_values = []\nfor n_neighbors in range(1, 200):\n    knc = KNeighborsClassifier(n_neighbors = n_neighbors, weights = 'distance', p = 1)\n    knc.fit(train_X_scaled_pca, train_y)\n    #'roc_auc_score' metric\n    rf_predictions = knc.predict(test_X_scaled_pca)\n    rf_probs = knc.predict_proba(test_X_scaled_pca)\n    metric_values.append(roc_auc_score(test_y, rf_probs, multi_class = 'ovr'))\n\n#printing the max value of 'roc_auc_score' and appropriate value of the parameter 'n_neighbors'\nroc_auc_score_max = max(list(enumerate(metric_values, 1)), key=lambda i : i[1])\nprint(f'The value of the parameter \"n_neighbors\" in the KNN Classifier: {roc_auc_score_max[0]}')\nprint(f'The max value of the metric \"roc_auc_score\" in the test sample: {round(roc_auc_score_max[1], 4)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}