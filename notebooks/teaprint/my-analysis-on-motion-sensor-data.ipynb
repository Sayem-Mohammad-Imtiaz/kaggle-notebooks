{"cells":[{"metadata":{"_uuid":"66d6d2d87c2569d851eeea881b96226e55d2aef6"},"cell_type":"markdown","source":"Table of Content:\n* Data characterization\n    * shape of dataset\n    * Missing values\n    * class balance \n    * show the first few records of each motion type\n* Feature construction\n* Check similiarity between time series of each motion type\n    * 2D visualization with the help dimension reduction: pca, tsne\n    \n    * hierachical clustering with dynamic time warpping\n* Predictve model\n    * on motion type\n        * on overall dataset\n        <br> model performance\n        <br> feature importance\n        <br> check correctly-predicted samples\n        <br> check incorrectly-predicted samples\n        * on dataset of individual motion\n    * on gender\n    * on weight\n\n    reference:\n    * https://www.kaggle.com/morrisb/what-does-your-smartphone-know-about-you"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/A_DeviceMotion_data/A_DeviceMotion_data\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e8c8e34b630caef05cfe379ac38d7f7ec5e989f"},"cell_type":"markdown","source":"**Load the dataset**\n<br> I follow the [RoyT's kernel](https://www.kaggle.com/talmanr/a-simple-features-dnn-using-tensorflow) to segment 400 datapoints into one experiment, and construct features for each time series. "},{"metadata":{"trusted":true,"_uuid":"ef612a6819f633f516f986646ec80eef21ab16da"},"cell_type":"code","source":"# Data Folders:\nFolders = glob('../input/A_DeviceMotion_data/A_DeviceMotion_data/*_*')\nFolders = [s for s in Folders if \"csv\" not in s]\n\nDf_all_list = []\nExp = 0\n# Segment the data to 400 sampels frames , each one will be a different Expirament\nSegment_Size = 400\n\n# Activety types dict:\nactivity_codes = {'dws':1,'jog':2,'sit':3,'std':4,'ups':5,'wlk':6}        \nactivity_types = list(activity_codes.keys())\n\n# Load All data:\nfor j  in Folders:\n    Csv = glob(j + '/*' )\n\n\n    for i in Csv:\n        df = pd.read_csv(i)\n        # Add Activety label, Subject name and Experiment number\n        df['Activity'] = activity_codes[j[49:52]]\n        df['Sub_Num'] = i[len(j)+5:-4]\n        df['Exp_num'] = 1\n        ExpNum = np.zeros((df.shape[0])) \n        for i in range(0,df.shape[0]-Segment_Size,Segment_Size):\n            ExpNum[range(i,i+Segment_Size)] = i/Segment_Size +Exp*100 \n        df['Exp_num'] = ExpNum\n        #Df_all = pd.concat([Df_all,df])\n        Df_all_list.append(df)\n        Exp += 1        \n\nDf_all = pd.concat(Df_all_list,axis=0)  \nprint(Df_all.shape)\nprint(Df_all.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb1ccea48f39834b7879fc0d2192ed001509c165"},"cell_type":"code","source":"np.unique(Df_all['Sub_Num'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2ab182eb8a7dd1d19117b9ad27837ea6be02f6c"},"cell_type":"markdown","source":"**Data Characterization**\n    * Missing values\n    * class balance \n    * show the first few records of each motion type"},{"metadata":{"trusted":true,"_uuid":"3167b149a43a2d2df9e08d602450cbaf24f4397a"},"cell_type":"code","source":"### Missing values\nchecks = pd.isna(Df_all).sum()\nprint(checks)\n### class balance\nclass_counts = list()\n\nfor act in activity_types[:1]:\n    class_counts.append(Df_all[Df_all['Activity']==activity_codes[act]].count())\nplt.figure(1)\nplt.title('Size of each class')\nplt.xlabel('activity type')\nplt.hist(Df_all['Activity'],bins=range(1,8),rwidth=0.5,align='left')\n\n### Length of time series\nseries_length = list()\nfor act in activity_types:\n    for sub in range(1,25):\n        sub = str(sub)\n        series_length.append(Df_all[(Df_all['Sub_Num']==sub) & (Df_all['Activity']==activity_codes[act])].shape[0])\nplt.figure(2)\nplt.title('Histogram of length of raw time series')\nplt.hist(series_length,rwidth=0.5,align='left')\n\n\n### show the first few records of motion type\nplt.figure(3)\ncolors = ['r','g','b','c','m','y','k']\nfor act in activity_types:\n    plt.subplot('61'+str(activity_codes[act]))\n    plt.subplots_adjust(hspace=1.0)\n    df = Df_all[(Df_all['Sub_Num']=='1') & (Df_all['Activity']==activity_codes[act])]\n    plt.title(act)\n    plt.plot(df['userAcceleration.z'][:400])\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec72f9089619967d8510b1972b7cede81d7e03af"},"cell_type":"markdown","source":"From above analysis,\n* This dataset raised concern on class imbalance.\n* The distribution of time-series length is broad."},{"metadata":{"_uuid":"a9d4ab46ecdfffe1ff65c36a500524a0ff930f18"},"cell_type":"markdown","source":"**Feature Construction**:\n<Br> I follow [RoyT's work](https://www.kaggle.com/talmanr/a-simple-features-dnn-using-tensorflow) to calculate mean, squared_median, max, min, skewness and std for each segment"},{"metadata":{"trusted":true,"_uuid":"249b8b147f08b86fa2a37116801df5a21a0ac1c8"},"cell_type":"code","source":"#  Calculate features\ndf_sum = Df_all.groupby('Exp_num', axis=0).mean().reset_index()\ndf_sum.columns = df_sum.columns.str.replace('.','_sum_')\n\ndf_sum_SS = np.power(Df_all.astype(float),2).groupby('Exp_num', axis=0).median().reset_index() \ndf_sum_SS.columns = df_sum_SS.columns.str.replace('.','_sumSS_')\n\ndf_max = Df_all.groupby('Exp_num', axis=0).max().reset_index()\ndf_max.columns = df_max.columns.str.replace('.','_max_')\n\ndf_min = Df_all.groupby('Exp_num', axis=0).min().reset_index()\ndf_min.columns = df_min.columns.str.replace('.','_min_')\n\ndf_skew = Df_all.groupby('Exp_num', axis=0).skew().reset_index()\ndf_skew.columns = df_skew.columns.str.replace('.','_skew_')\n\ndf_std = Df_all.groupby('Exp_num', axis=0).std().reset_index()\ndf_std.columns = df_std.columns.str.replace('.','_std_')\n\nDf_Features = pd.concat([ df_max , df_sum[df_sum.columns[2:-2]], \n                         df_min[df_min.columns[2:-2]], df_sum_SS[df_sum_SS.columns[2:-2]], \n                         df_std[df_std.columns[2:-2]], df_skew[df_skew.columns[2:-2]]], axis=1)\n\nX = Df_Features.drop(['Exp_num','Unnamed: 0','Activity','Sub_Num'],axis=1)\nY = Df_Features['Activity']\n\nprint('Shape of X:', X.shape)\nprint('Shape of Y:', Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbcf6d1eea0bf0cbca03e0987d2f32a776f596af"},"cell_type":"markdown","source":"**Check the similarity between motion types**"},{"metadata":{"trusted":true,"_uuid":"e7a5fca1cbbcfcdfeeba0feca2a8d5e32875e956"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\n#### dimension reduction\n### use pca to reduce the dimension to 2D directly.\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\nplt.figure(1)\ncolors = ['r','g','b','c','m','y','k']\nlw = 2\n\nfor color, i, target_name in zip(colors, range(6), activity_types):\n    plt.scatter(X_r[Y == i, 0], X_r[Y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('Use PCA directly')\n\n### sklearn tsne\n# sites.google.com/s/1HAV-HEiBhPHLgdh5Ejmu31TrVIQqw9HU/p/1bPpOCDlxW7i5nOpy3bvpnmkqa8Y-SDVa/edit\n# Scale data\nscl = StandardScaler()\nscaled_X = scl.fit_transform(X)\n\n# Reduce dimensions before feeding into tsne\npca = PCA(n_components=0.9, random_state=3)\npca_transformed = pca.fit_transform(scaled_X)\n\n# Transform data\ntsne = TSNE(random_state=3)\ntsne_transformed = tsne.fit_transform(pca_transformed)\n\nplt.figure(2)\nfor color, i, target_name in zip(colors, range(6), activity_types):\n    plt.scatter(X_r[Y == i, 0], X_r[Y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('TSNE')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2960e934fee84061f38368511efca7d0c178eb6c"},"cell_type":"markdown","source":"**Build hierachical clustering w/ dynamic time warpping**\n<br> I tried to adopted the methods provided by library \"dtaidistance.\"  400 datapoints of each motion type are segmented and similiarity between each segment is calculated by dynamic time warpping."},{"metadata":{"trusted":true,"_uuid":"abed504a25cbf101af8d2a611bb49b51f47a8738"},"cell_type":"code","source":"series_list = list()\nlabels_list = list()\nfor act in activity_types:\n\n    df = Df_all[(Df_all['Sub_Num']=='1') & (Df_all['Activity']==activity_codes[act])]\n    series_list.append(df['userAcceleration.z'][:400])\n    labels_list.extend([act])\n\n#print(labels_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce391bb0bddd5cc020a3e1050174767bc1074465"},"cell_type":"code","source":"from dtaidistance import dtw\nimport numpy as np\nfrom dtaidistance import clustering\n\nseries = np.array(series_list)\nds = dtw.distance_matrix_fast(series)\n\nmodel = clustering.LinkageTree(dtw.distance_matrix_fast, {})\nmodel.fit(series)\n\nmodel.plot(show_ts_label=labels_list,\n           show_tr_label=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15802ddc32f0e4fdf279fea12d8feea07f8b481b"},"cell_type":"markdown","source":"Based on hierachical clustering, 'dws', 'ups', 'wlk' are similar to each other, 'sit' and 'std' are similair, and 'jog' is distinctive from the other five."},{"metadata":{"_uuid":"ee7b89086a5992ebe877f5e7d1b8dc36056bbcac"},"cell_type":"markdown","source":"* Predictve model\n    * on motion type\n        * train neural network\n            *  check model performance\n            * check correctly-predicted samples\n            * check incorrectly-predicted samples\n        * train tree-based classifier\n            * feature importance\n\n   * on subject\n       * On overall dataset\n       * On dataset of individual dataset\n   * on gender\n   * on weight"},{"metadata":{"_uuid":"1bc63e161a4b0f9b8ccf626ff2ac7b68e18f6411"},"cell_type":"markdown","source":"Train multi-layer perceptron classifier"},{"metadata":{"trusted":true,"_uuid":"51a54e8d8eb63dd68bbd982d1449b3e4e605eb60"},"cell_type":"code","source":"import itertools\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.datasets import mnist\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.regularizers import l2\nfrom keras.utils import to_categorical\nimport keras\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4f7ceafa5e099a8de7604c23fa5d6d172d9f3e3"},"cell_type":"code","source":"#### Construct neural Architeture for baseline model\ninput_dim = X.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(7, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0c997cbc0e00f7770c9029970dee9272140787b"},"cell_type":"code","source":"# One-hot encoding\nY = to_categorical(Y)\nprint(Y.shape)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55fdb5ecfbe909f46f123ad63b48cd6121dc4190"},"cell_type":"code","source":"#### Check model performance\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"832849c9a31955120052ce9bee30809b7406794c"},"cell_type":"code","source":"# One-hot decoding\ny_pred = np.argmax(model.predict(X_test),axis=1)\ny_test = np.argmax(Y_test,axis=1)\n#print(y_pred,y_test)\n\ncorrect = np.nonzero(y_pred==y_test)[0]\nincorrect = np.nonzero(y_pred!=y_test)[0]\n#print(correct)\n\n### Check the correctly-predicted samples\nplt.figure(1)\nfor i, cor in enumerate(correct[:9]):\n    plt.subplot(3,3,i+1)\n    plt.subplots_adjust(hspace=1.0,wspace=1.0)\n    plt.plot(X_test.iloc[cor,:])\n    plt.title(\"Predicted:{}\\nTrue:{}\".format(activity_types[y_pred[cor]-1], \n                                              activity_types[y_test[cor]-1]))\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels\n    #plt.tight_layout()\nplt.show()\n### Check the incorrectly-predicted samples\nplt.figure(2)\nfor i, cor in enumerate(incorrect[:9]):\n    plt.subplot(3,3,i+1)\n    plt.subplots_adjust(hspace=1.0,wspace=1.0)\n    plt.plot(X_test.iloc[cor,:])\n    plt.title(\"Predicted:{}\\nTrue:{}\".format(activity_types[y_pred[cor]-1], \n                                              activity_types[y_test[cor]-1]))\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels\n    #plt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33a4729563478ad9927726709aec5a07ee68c7ea"},"cell_type":"code","source":"### Confusion matrix (predictive performance on different classes)\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    function provided by sklearn example\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cnf_matrix, classes=activity_types,\n                      title='Confusion matrix, without normalization')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99ee444320d67c3370c1dc99c180d1c27f8398f7"},"cell_type":"markdown","source":"'ups' has the highest misclassification rate, and is clasified as 'dws' and 'wlk'"},{"metadata":{"trusted":true,"_uuid":"d728bb3b0b2d5b55a30fd2fe5e416d7c9a0a0e85"},"cell_type":"markdown","source":"Check the feature importance"},{"metadata":{"trusted":true,"_uuid":"e7eb7c0eed0c1fce0383a71a8f0ba5e2f3505806"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf1 = RandomForestClassifier(n_estimators=100, max_depth=None,\n     min_samples_split=2, random_state=0)\nclf1.fit(X_train, Y_train)\nfeatureImportance = clf1.feature_importances_\n\n# normalize by max importance\nfeatureImportance = featureImportance / featureImportance.max()\nfeature_names = X.columns\nidxSorted = np.argsort(featureImportance)[-10:]\nbarPos = np.arange(idxSorted.shape[0]) + .5\nplt.barh(barPos, featureImportance[idxSorted], align='center')\nplt.yticks(barPos, feature_names[idxSorted])\nplt.xlabel('Variable Importance')\nplt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbe70fc24d31fd2bae63093e71ec6055027de10f"},"cell_type":"code","source":"# Need to one-hot decode before feding into GBM\ny_train = np.argmax(Y_train,axis=1)\nclf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=1, random_state=0)\nclf2.fit(X_train, y_train)\nfeatureImportance = clf2.feature_importances_\n\n\n# normalize by max importance\nfeatureImportance = featureImportance / featureImportance.max()\nfeature_names = X.columns\nidxSorted = np.argsort(featureImportance)[-10:]\nbarPos = np.arange(idxSorted.shape[0]) + .5\nplt.barh(barPos, featureImportance[idxSorted], align='center')\nplt.yticks(barPos, feature_names[idxSorted])\nplt.xlabel('Variable Importance')\nplt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc0ed0d24851b6ecf155875258b21509b591b168"},"cell_type":"code","source":"### One-hot encoding for tree classifier\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1534e3e541634f5503ccec6e14c3a4073d89ff25"},"cell_type":"markdown","source":"Prediction on subjects"},{"metadata":{"trusted":true,"_uuid":"e0457135aa3c0777abab09987c182a080e17943f"},"cell_type":"code","source":"# Let target label become subjects\nY1 = Df_Features['Sub_Num'].iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2a8d64a3b638788c945063cba293b5a9a6a93fb"},"cell_type":"code","source":"#### Construct neural Architeture for baseline model\ninput_dim = X.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(25, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0139e207aa30aa32528a599a718d00d9e7a40fb5"},"cell_type":"code","source":"# One-hot encoding\n\ny1 = to_categorical(Y1)\nprint(Y1.shape)\nX_train, X_test, Y_train, Y_test = train_test_split(X, y1, test_size=0.3, random_state=0)\n\nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])\n\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25de4f4ea3ed5b7bb9b8165e927cba01d62ef047"},"cell_type":"markdown","source":"on individual dataset"},{"metadata":{"trusted":true,"_uuid":"605b3d316a8cf367eb73c5aca55b2957726f7e6e"},"cell_type":"code","source":"for i in range(1,7):\n    df = Df_Features[Df_Features['Activity']==i]\n    x = df.drop(['Exp_num','Unnamed: 0','Activity','Sub_Num'],axis=1)\n    y = df['Sub_Num'].iloc[:,0]\n    y = to_categorical(y)\n    \n    #### Construct neural Architeture\n    input_dim = x.shape[1]\n    input_img = Input(shape=(input_dim,))\n    d = Dense(50, activation='relu')(input_img)\n    d = Dense(20, activation='relu')(d)\n    output = Dense(25, activation='softmax', kernel_regularizer=l2(0.01))(d)\n    model = Model(input_img,output)\n    model.compile(optimizer='adam', loss='binary_crossentropy',\n                     metrics=['categorical_accuracy'])\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n\n    history = model.fit(X_train, Y_train,\n                    epochs=1000,\n                    batch_size=100,\n                    shuffle=True,\n                    verbose=0,\n                    validation_data=[X_test,Y_test])\n\n    ### Overall test accuracy\n    score = model.evaluate(X_test, Y_test)\n    print('Activity type:', activity_types[i-1])\n    print('keras test accuracy score:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bddbaa83a9dfd0fadac2f20126703ad7863e9988"},"cell_type":"markdown","source":"Overfitting??"},{"metadata":{"_uuid":"3962539f791f7d87809ec71e93fde1ba1673f806"},"cell_type":"markdown","source":"on gender:\n<br> 1 for male or female??"},{"metadata":{"trusted":true,"_uuid":"b623082d0db14ac78e9555628086a020015138b0"},"cell_type":"code","source":"mapping = {\n    '1': 1,\n    '2': 1,\n    '3': 0,\n    '4': 1,\n    '5': 0,\n    '6':1,\n    '7':0,\n    '8':0,\n    '9':1,\n    '10':0,\n    '11':1,\n    '12':1,\n    '13':1,\n    '14':1,\n    '15':1,\n    '16':0,\n    '17':1,\n    '18':0,\n    '19':0,\n    '20':1,\n    '21':1,\n    '22':1,\n    '23':0,\n    '24':0\n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df3bdcff5e2c0275e32ccb45d0d1f994f5e40d67"},"cell_type":"code","source":"# Create gender labels\nY2 = [mapping[i] for i in Df_Features['Sub_Num'].iloc[:,0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43a9de01b4c33f2682ee9cc5f2c681f934e9b471"},"cell_type":"code","source":"y2 = to_categorical(Y2)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y2, test_size=0.3, random_state=0)\nprint('The number of training samples:',X_train.shape[0])\nprint('The number of test samples:',X_test.shape[0])\n\n\n#### Construct neural Architeture for baseline model\ninput_dim = x.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])\n\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3375efc7e77f906069e6166d9490ac9af5ecb9d9"},"cell_type":"markdown","source":"on weight:\n<br> The subjects above the average weight 72.125 are labeled as 1; 0 for below average."},{"metadata":{"trusted":true,"_uuid":"da332a6d2db05d0043302980cc0a6fd20a0ec253"},"cell_type":"code","source":"mapping = {\n    '1': 1,\n    '2': 0,\n    '3': 0,\n    '4': 1,\n    '5': 0,\n    '6':1,\n    '7':0,\n    '8':0,\n    '9':1,\n    '10':0,\n    '11':0,\n    '12':0,\n    '13':0,\n    '14':0,\n    '15':0,\n    '16':1,\n    '17':1,\n    '18':0,\n    '19':1,\n    '20':1,\n    '21':0,\n    '22':1,\n    '23':0,\n    '24':1\n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4f541b924d1ea486d4be8b6567be4ba6615117e"},"cell_type":"code","source":"# Create gender labels\nY3 = [mapping[i] for i in Df_Features['Sub_Num'].iloc[:,0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8ffe6dec2dd1e2f09c5ee7513035396d755bf86"},"cell_type":"code","source":"y3 = to_categorical(Y3)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y3, test_size=0.3, random_state=0)\nprint('The number of training samples:',X_train.shape[0])\nprint('The number of test samples:',X_test.shape[0])\n\n\n#### Construct neural Architeture for baseline model\ninput_dim = x.shape[1]\ninput_img = Input(shape=(input_dim,))\nd = Dense(50, activation='relu')(input_img)\nd = Dense(20, activation='relu')(d)\noutput = Dense(2, activation='softmax', kernel_regularizer=l2(0.01))(d)\nmodel = Model(input_img,output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \nhistory = model.fit(X_train, Y_train,\n                epochs=1000,\n                batch_size=100,\n                shuffle=True,\n                verbose=0,\n                validation_data=[X_test,Y_test])\n\n### Overall test accuracy\nscore = model.evaluate(X_test, Y_test)\nprint ('keras test accuracy score:', score[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}