{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Finance related operations\nfrom pandas_datareader import data\n\n# Import this to silence a warning when converting data column of a dataframe on the fly\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load data\ndf = pd.read_csv('../input/df-out1/df_out.csv', index_col=0)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2018 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2018_Financial_Data.csv', index_col=0)\ndf_2018.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2018 = df_2018.rename(columns={'Enterprise Value': 'Enterprise Value 2018'})\ndf_2018x = df_2018[df_2018['Enterprise Value 2018'].notna()]\ndf_2018x = df_2018x['Enterprise Value 2018']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1418 = pd.merge(df, df_2018x, right_index =True, left_index = True, how='inner')\ndf_1418.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2014 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2014_Financial_Data.csv', index_col=0)\nEV_2014 = df_2014['Enterprise Value']\ndf_1418x = pd.merge(df_1418, EV_2014, right_index =True, left_index = True, how='inner')\ndf_1418x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1418x['Growth Rate'] = df_1418x['Enterprise Value 2018'] / df_1418x['Enterprise Value']\ndf_1418x.head()\ndf_1418x = df_1418x.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##category = pd.cut(df_1418x['Growth Rate'],bins=[-np.inf,0,1.461,np.inf],labels=[-1,1,10])\n##category = pd.cut(df_1418x['Growth Rate'],bins=[-np.inf,0,np.inf],labels=[-1,1])\ncategory = pd.cut(df_1418x['Growth Rate'],bins=[-np.inf,2,np.inf],labels=['NG','G'])\ndf_1418x.insert(67,'Stock Performance',category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1418x['Stock Performance'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features2 = ['Revenue', 'EPS', 'EBITDA Margin', 'returnOnEquity', 'EBIT Growth', 'Operating Income Growth', 'Sector']\ndf_1418x[(df_1418x['Stock Performance'] == 'G') & (df_1418x['Sector'] == 'Technology')][features2].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1418x[df_1418x['Stock Performance'] == 'G']['Sector'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1418x[(df_1418x['Stock Performance'] == 'NG') & (df_1418x['Sector'] == 'Technology')][features2].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1418x[df_1418x['Stock Performance'] == 'NG']['Sector'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Revenue', 'EPS', 'EBITDA Margin', 'returnOnEquity', 'Operating Income Growth', 'Sector']\nX = df_1418x[features]\ny = df_1418x.loc[:, df_1418x.columns.intersection(['Stock Performance'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_X = pd.get_dummies(X)\none_hot_X.replace([np.inf, -np.inf], np.nan)\none_hot_X.fillna(0)\none_hot_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.values.ravel()\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(one_hot_X, y, test_size=0.25, random_state=52)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n\n# Define the model. Set random_state to 1\nrf_model = forest_model = RandomForestRegressor(random_state=1)\n\n# fit your model\nrf_model.fit(X_train, y_train)\n\n# Calculate the mean absolute error of your Random Forest model on the validation data\nrf_val_mae = mean_absolute_error(y_test, rf_model.predict(X_test))\n\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(rf_model.predict(X_test))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"results = rf_model.predict(X_test)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"print(X_test)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"X_test.insert(1,'Actual', y_test)\nprint(X_test)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"results1 = pd.DataFrame({'Results': results}, index = X_test.index)\nprint(results1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"X_test.insert(2, 'Prediction', results1['Results'])\nprint(X_test)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"from sklearn.tree import export_graphviz\n\nexport_graphviz(rf_model.estimators_[0], out_file='tree.dot', feature_names=X.columns, filled=True, rounded=True)\n\n!dot -Tpng tree.dot -o tree.png\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\n# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n\nplt.xlabel('Log Loss')\nplt.title('Classifier Log Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestClassifier()\nforest.fit(X_train, y_train)\n\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}