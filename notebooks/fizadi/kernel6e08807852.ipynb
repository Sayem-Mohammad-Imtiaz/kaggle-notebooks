{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 99% of scores of any metric is 100% perfect. The remainimg 1% is 0.999... or 0.95....\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nimport xgboost\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mushrooms.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the data for missing values\ns =set(df.apply(lambda x: sum(x.isnull())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calling categorical columns\ncat_cols = [x for x in df.dtypes.index if df.dtypes[x]=='object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# TO get the feture_importance by XGBClassifier()\n#name = 'xgboost'\n\nlabelEncoder = preprocessing.LabelEncoder()\ndf.dtypes\n\nfor col in df.columns:\n    df[col] = labelEncoder.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.26) \ny_train = train['class']\nX_train = train[[x for x in train.columns if 'class' not in x]]\ny_test = test['class']\nX_test= test[[x for x in test.columns if 'class' not in x]]\nxgb =  XGBClassifier()\nxgb.fit(X_train, y_train)\nax = xgboost.plot_importance(xgb, color='magenta') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.apply(lambda x : len(x.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us do some frequecy plots\n\ndef cplt(col_name):\n    sns.countplot(x = df[col_name])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" cplt('habitat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cplt('gill-color') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cplt('class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the class column from the dat and call the rest X.\nX=df.drop(['class'],axis=1) \ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ploting the class again with bar chart.\nz.plot(kind='bar', colors=['red', 'blue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # To see the correlation among the columns\nX.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = X.corr()\nplt.figure(figsize=(16,8))\nsns.heatmap(c, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change all the categorical columns to numerical columns by Label Encoder and get dummies\nfrom sklearn.preprocessing import LabelEncoder\nEncoder_X = LabelEncoder()\nfor col in X.columns:\n    X[col] = Encoder_X.fit_transform(X[col])\n    Encoder_y=LabelEncoder()\ny = Encoder_y.fit_transform(y)\nX=pd.get_dummies(X, columns=X.columns, drop_first=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = X.corr()\nplt.figure(figsize=(16,8))\nsns.heatmap(c, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Get the feature_importance for dummy features.\n#from numpy import loadtxt\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_tree\nfrom xgboost import XGBClassifier\nimport xgboost\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\nax = xgboost.plot_importance(model, color='green')    \nX.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot single tree\nplot_tree(model)\nplt.show()\n#To get outliers in two different ways\nX.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_outliers_tukey(x):\n    q1 = np.percentile(x, 25)\n    q3 = np.percentile(x, 75)\n    iqr = q3-q1 \n    floor = q1 - 1.5*iqr\n    ceiling = q3 + 1.5*iqr\n    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n    outlier_values = list(x[outlier_indices])\n    return outlier_indices, outlier_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tukey_indices, tukey_values = find_outliers_tukey(X['cap-shape_1'])\nprint(np.sort(tukey_values))\n\n# No outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test Split\nnp.random.seed(1234)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled =  sc.fit_transform(X)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.26,random_state=0)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,roc_curve,confusion_matrix,roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)\ny_pred = model1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_pred)\n# The model is 100% perfect. Let us see the ROC curve.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import roc_curve \nfpr, tpr, threshold = roc_curve(y_test, y_pred)\nauc_roc = roc_auc_score(y_test,y_pred)\nauc_roc\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, confusion_matrix,roc_auc_score\nfpt, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpt, tpr)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title('ROC Curve')\nplt.plot(fpr,tpr, color ='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The ROC curve is also 100% perfect.\n# In the next part I am going to do 8 different classifiers model together.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1234)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled =  sc.fit_transform(X)\n#split train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.26,random_state=0)\nfrom sklearn.metrics import roc_curve, auc,confusion_matrix,roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nfrom sklearn.ensemble import ExtraTreesClassifier\nexc = ExtraTreesClassifier()\nimport xgboost as xgb\nboost= xgb.XGBClassifier(n_estimators=200, learning_rate=0.01)\nfrom sklearn.naive_bayes import GaussianNB\nmodel_naive = GaussianNB()\nfrom sklearn.svm import SVC\nsvm_model= SVC(gamma='scale')\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nkn = KNN()\nmodels = [lg, dt, rfc, exc, boost, model_naive, svm_model, kn]\nmodnames = ['LogisticRegression', 'DecisionTreeClassifier','RandomForestClassifier',\n            'ExtraTreesClassifier', 'XGBClassifier', 'GaussianNB', 'SVC', 'KNeighborsClassifier']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    confusion_matrix(y_test,y_pred)\n    print('The accuracy of ' + modnames[i] + ' is ' + str(accuracy_score(y_test,y_pred)))\n    print('The confution_matrix ' + modnames[i] + ' is ' + str(confusion_matrix(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since X_test is a numpy array, we need to convert it to a DataFrame to define the index.\ntest_X = pd.DataFrame(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output1 = pd.DataFrame({'index':test_X.index,'actual':y_test, 'pred_lg':lg.predict(X_test),'pred_dt':dt.predict(X_test), 'pred_rfc':\n                       rfc.predict(X_test), 'pred_exc':exc.predict(X_test), 'pred_boost':boost.predict(X_test), 'pred_model_naive':\n                       model_naive.predict(X_test),'pred_svm_model':svm_model.predict(X_test),'pred_kn':kn.predict(X_test),\n                      })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output1.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('submission1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that from 8 classifiers 6 of them scored 1 and one close to 1 and the other almost 95%. The confution_matrix\n#for all got the same as before.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    auc_roc = classification_report(y_test,y_pred)\n    print('The auc_roc report ' + modnames[i] + ' is ' + str(auc_roc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also these are all perfect for all models.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Two other classifiers\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n        max_depth=1, random_state=0)\n\nmodels = [gbc, gbc2]\nmodnames = ['GradientBoostingClassifier', 'GradientBoostingClassifier']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    confusion_matrix(y_test,y_pred)\n    print('The accuracy of ' + modnames[i] + ' is ' + str(accuracy_score(y_test,y_pred)))\n    print('The confution_matrix ' + modnames[i] + ' is ' + str(confusion_matrix(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, model in enumerate(models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    auc_roc = classification_report(y_test,y_pred)\n    print('The auc_roc report ' + modnames[i] + ' is ' + str(auc_roc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#**********************************************\n# 10 cross validation\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nknn = KNN()\nfrom sklearn.model_selection import cross_val_score\ncvs_knn = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\ncvs_knn.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\nfrom sklearn.neighbors import KNeighborsClassifier\nknn2 = KNeighborsClassifier(n_neighbors=5)\ncvs_knn2 = cross_val_score(knn2, X_train, y_train, cv=10, scoring='accuracy')\ncvs_knn2.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use average accuracy as an estimate of out-of-sample accuracy\nk_range = list(range(1, 10))\nk_scores = []\nfrom sklearn.neighbors import KNeighborsClassifier \nfor k in k_range:\n    knn3 = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn3, X_train, y_train, cv=10, scoring='accuracy')\n    k_scores.append(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_scores     \ncvs_knn3 = pd.Series(k_scores)\ncvs_knn3.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nmodel_naive = GaussianNB()\ncvs_GNB = cross_val_score(model_naive, X, y, cv=10, scoring='accuracy')\ncvs_GNB.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output3 = pd.DataFrame({'index':np.arange(1), 'cvs_knn':cvs_knn.mean(), 'cvs_knn2':cvs_knn2.mean(),\n                       'cvs_knn3':cvs_knn3.mean(), 'cvs_GNB':cvs_GNB.mean()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output3.to_csv('submission3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = pd.read_csv('submission3.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exhaustive search over specified parameter values for an estimator.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nclf = SVC(gamma='auto')\nparam_grid = {\n 'C': [1, 10, 100], 'kernel': ['linear','rbf'],\n 'C': [1, 20], 'gamma': [1,0.1,0.01], 'kernel': ['rbf']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_svm_GRID = GridSearchCV(clf, param_grid, scoring='accuracy',cv=5)\nmodel_svm_GRID.fit(X_train, y_train)\ny_pred_GRID= model_svm_GRID.predict(X_test)\nfrom sklearn.metrics import roc_curve, auc,confusion_matrix,roc_auc_score\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(y_pred,y_test))\ncm = confusion_matrix(y_test,y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr,tpr, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exhaustive search over specified parameter values for an estimator.\n#GridSearcgh using AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nparam_grid = {'n_estimators': [10,25,40 ]}\nfrom sklearn.model_selection import GridSearchCV\nmodel_ada_GRID =  GridSearchCV(ada, param_grid)                              \nmodel_ada_GRID.fit(X_train, y_train)\ny_pred_ada_GRID = model_ada_GRID.predict(X_test)\nmodel_ada_GRID.score(X_test, y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GridSearch using DecisionTreeClassifier and AdaBoostClassifier tp prevent overfitting of dt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2] }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndt2 = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\nada = AdaBoostClassifier(base_estimator = dt2)\n# run grid search\nGRID_ABC = GridSearchCV(ada, param_grid=param_grid, scoring = 'roc_auc',cv=5)\nGRID_ABC.fit(X_train, y_train)\ny_pred_GRID_ABC = GRID_ABC.predict(X_test)\nGRID_ABC.score(X_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.ensemble import AdaBoostClassifier\nada3 = AdaBoostClassifier(algorithm='SAMME.R', random_state=0)\nparams = {'n_estimators':[50,100,200],'learning_rate':[1.0, 3.0, 5.0]}\nscorer = make_scorer(accuracy_score)\ngrid_ada3 = GridSearchCV(ada3, params, scoring=scorer)\ngrid_fit = grid_ada3.fit(X_train,y_train)\nbest_ada3 = grid_fit.best_estimator_\nada3.fit(X_train, y_train)\ny_pred_ada3 = ada3.predict(X_test)\nada3.score(X_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output2 = pd.DataFrame({'index':test_X.index , 'actual':y_test , 'pred_kn':kn.predict(X_test) ,\n                        'pred_gbc':gbc.predict(X_test) , 'pred_gbc2':gbc2.predict(X_test), 'y_pred_GRID':\n                        model_svm_GRID.predict(X_test),\n                       'y_pred_ada_GRID':model_ada_GRID.predict(X_test),'y_pred_GRID_ABC':GRID_ABC.predict(X_test),\n                        'y_pred_ada3':ada3.predict(X_test)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output2.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('submission2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using PCA let us see how much data are in 5 first components\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5,svd_solver='full' )\npca.fit_transform(X)\nN = X.values\nx = pca.fit_transform(N)\nprint(pca.explained_variance_ratio_)\npca.explained_variance_ratio_.sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we see that allmost the first components have 48% of whole data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us put this data in 10 clusters\nplt.figure(figsize = (5,5))\nplt.scatter(x[:,0],x[:,1],c='red')\nplt.scatter(x[:,0],x[:,2],c='blue')\nplt.scatter(x[:,0],x[:,3],c='purple')\nplt.scatter(x[:,0],x[:,4],c='magenta')\nplt.scatter(x[:,1],x[:,2],c='black')\nplt.scatter(x[:,1],x[:,3],c='brown')\nplt.scatter(x[:,1],x[:,4],c='yellow')\nplt.scatter(x[:,2],x[:,3],c='green')\nplt.scatter(x[:,2],x[:,4],c='cyan')\nplt.scatter(x[:,3],x[:,4],c='olive')\nplt.xlabel(\"n-components is 5\", fontsize=15)\nplt.title(\"0.47658221797685124 PORTION OF DATA\", fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How about 10 components\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=10, svd_solver='arpack')\npca.fit_transform(X)\ncovariance = pca.get_covariance()\ncovariance\nexplained_variance = pca.explained_variance_\nlen(explained_variance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us use Label encoder to get exact data and exact number of components.\nfrom sklearn.preprocessing import LabelEncoder\nEncoder_X = LabelEncoder()\nfor col in X.columns:\n    X[col] = Encoder_X.fit_transform(X[col])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.get_dummies(X, columns=X.columns, drop_first=True)\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()\npca.fit_transform(X)\ncovariance = pca.get_covariance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covariance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance = pca.explained_variance_\nexplained_variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('bmh'):\n    plt.figure(figsize=(6, 4))\n    plt.bar(range(95), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance', color='blue')\n    plt.ylabel('Explained variance ratio',fontsize=10)\n    plt.xlabel('Principal components',fontsize=10 )\n    plt.legend(loc='best', fontsize=10)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It seems that with 60 compents we can recover the almost all the data. Let us try it in action.\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=60, svd_solver='arpack')\npca.fit_transform(X)\ncovariance = pca.get_covariance()\ncovariance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance = pca.explained_variance_\nlen(explained_variance)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)\npca.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is allmost 100%. However we can figure out the exact number of components by other means.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=95, svd_solver='full')\npca.fit_transform(X)\nexplained_variance\npca.explained_variance_ratio_\npca.explained_variance_ratio_.cumsum()\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95)+1\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that 38 components is covering the 95% od the data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us take only the first 4 principal components and visualise it using K-means clustering with k=4.\nN = X.values\npca = PCA(n_components=4)\nx = pca.fit_transform(N)\nprint(pca.explained_variance_ratio_)\npca.explained_variance_ratio_.sum()\nplt.figure(figsize = (5,5))\nplt.scatter(x[:,0],x[:,1], x[:,2],x[:,3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We may see the relation of the dimension of data with its percentage values in a plot.To do that, we need\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=895)\npca = PCA()\npca.fit(X_train)\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = np.argmax(cumsum >= 0.95)+1\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('DIMENSIONS', fontsize=15)\nplt.ylabel('Explained Variance',fontsize=15)\nplt.title('Elbow Curve for 95% of Data',fontsize=17)\nplt.plot(cumsum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Elbow plot shows that the 95% of the data are recovered by the first 38 principal components.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final step to get the submission files.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}