{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Gender Prediction with Logistic Regression and ANN","execution_count":null},{"metadata":{"_uuid":"de70986c27cbb89a45ba8a3810014d67dfd36da2"},"cell_type":"markdown","source":"> * Data Preparation\n> * Logistic Regression\n> * Logistic Regression with Scikit Learn\n> * 2 Layer ANN\n> * 3 Layer ANN with Keras","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration and Preparation","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # visualization\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0340feca6202fc9872b0ccff18a8da2f2f5b4cd9"},"cell_type":"code","source":"data.label = [1 if each == \"male\" else 0 for each in data.label]\ndata.label.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9977ba83590556fc63b905113bc595079cce9854"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df72c16c16f911ba82a68f828330f29d8569b69"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1a06916dc7ed762ef0cb60f06e0168b74f519d1"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"083e50f079aa0ab7f5dadb43b0b69b0f894889a1"},"cell_type":"code","source":"y = data.label.values\nx = data.drop(['label'],axis=1)\nx = (x-np.min(x))/(np.max(x)-np.min(x)).values  # Normalize\n\nprint(\"y: \", y.shape)\nprint(\"x: \", x.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27350f10995e3060c56a4e41f7dfbaacbfdee451"},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true,"_uuid":"de5037dd78ba4276aa77e6875631540db9dccf75"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nfeatures = x_train.T\nlabels = y_train.T\ntest_features = x_test.T\ntest_labels = y_test.T\n\nprint(\"features: \", features.shape)\nprint(\"labels: \", labels.shape)\nprint(\"test_features: \", test_features.shape)\nprint(\"test_labels: \", test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea3342a212a55f4768d32f82061f5549a94a54c0"},"cell_type":"code","source":"def init_weights_and_bias(dim):\n    '''\n    Shape of weights:  (20, 1)\n    Shape of bias:  (20, 1)\n    '''\n    weights = np.full((dim, 1),0.01)\n    bias = np.zeros(dim).reshape(-1, 1)\n    return weights, bias\n\nweights, bias = init_weights_and_bias(20)\nprint(\"Shape of weights: \", weights.shape)\nprint(\"Shape of bias: \", bias.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a4d0a6b564bcded13774e05f17e568f7ac141b6"},"cell_type":"code","source":"def sigmoid(Z):\n    yHat = 1 / (1 + np.exp(-Z))\n    return yHat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4634a0b0633d5acb61bfd87a9fb0831229bda7f0"},"cell_type":"code","source":"def feedforward_back_prop(weights, bias, features, labels):\n    '''\n    features:  (20, 2534)\n    weights: (20, 1)\n    weights.T: (1, 20)\n    bias: (20, 1)\n    yHat: (1, 2534)\n    '''\n    # Feed Forward Propagation\n    Z = np.dot(weights.T, features ) + bias\n    yHat = sigmoid(Z)\n    # Cost Function\n    loss = -labels*np.log(yHat)-(1-labels)*np.log(1-yHat)\n    cost = (np.sum(loss))/features.shape[1]\n    # Backward Propagation\n    dW = (np.dot(features, ((yHat-labels).T)))/features.shape[1]\n    dB = np.sum(yHat-labels)/features.shape[1]\n    grads = {\"dW\": dW, \"dB\": dB}\n    return cost, grads\n\ncost, grads = feedforward_back_prop(weights, bias, features, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4507bfaa228fb80ffe898ff2d94f02dec609dd6"},"cell_type":"code","source":"def update(weights, bias, features, labels, lr, reiter):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Updating (learning) parameters is number_of_iterations times\n    for i in range(reiter):\n        \n        cost, grads = feedforward_back_prop(weights, bias, features, labels)\n        #cost = cost_function(features, labels, weights, bias)\n        cost_list.append(cost)\n        #Let's update\n        weights = weights - lr * grads[\"dW\"]\n        bias = bias - lr * grads[\"dB\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i: %f\" %(i, cost))\n            \n    # We update (learn) parameters weights and bias\n    parameters = {\"weights\": weights, \"bias\": bias}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation = \"vertical\")\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, grads, cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"116f8d294ec115fe33c091e94fd74726ad047a7f"},"cell_type":"code","source":"def predict(weights, bias, test_features):\n    # test_features are a input for feed forward propagation\n    Z = sigmoid(np.dot(weights.T, test_features) + bias)\n    prediction = np.zeros((1, test_features.shape[1]))\n    for i in range(Z.shape[1]):\n        if Z[0, i] <= 0.5:\n            prediction[0, i] = 0\n        else:\n            prediction[0, i] = 1  \n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87812cf885b3360285f1fa56a38c1b383e61850a"},"cell_type":"code","source":"def logistic_regression(features, labels, test_features, test_labels, lr ,  reiter):\n    # Initialize\n    # lr: learning rate\n    \n    dim =  features.shape[0]\n    # dim =  features.shape[0]: 20 for our case\n    weights, bias = init_weights_and_bias(dim)\n    # Shape of weights:  (20, 1)\n    # Shape of bias:  (20, 1)\n    parameters, grads, cost_list = update(weights, bias, features, labels, lr, reiter)\n    \n    prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"],test_features)\n\n    # Print test Errors\n    print(\"Model A test accuracy: {} %\".format(100 - np.mean(np.abs(prediction_test - test_labels)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"583e4840bcb43c1dd763c6b96be4f59b1101b0ca"},"cell_type":"code","source":"logistic_regression(features, labels, test_features, test_labels, lr = 0.1 ,  reiter= 1250) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c07f09bc2af0d2d0c2393dbbb6c2586bd19030f1"},"cell_type":"markdown","source":"## Logistic Regression with ScikitLearn","execution_count":null},{"metadata":{"trusted":true,"_uuid":"024d37c9f1da32bff9228ea826b32404ccdec952"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nModel_B = LogisticRegression()\nModel_B.fit(x_train,y_train)\nprint(\"Model B test accuracy: {}\".format(Model_B.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94359deed20ed8ce0dd28c884fe36b0d5c5b5c5d"},"cell_type":"markdown","source":"## 2 Layer ANN","execution_count":null},{"metadata":{"trusted":true,"_uuid":"c26f5a13c1a4801dd017ce5adc54a601a60c5052"},"cell_type":"code","source":"labels = y_train.reshape(y_train[0], -1)\ntest_labels = y_test.reshape(y_test[1], -1)\n\nprint(\"features: \", features.shape)\nprint(\"labels: \", labels.shape)\nprint(\"test_features: \", test_features.shape)\nprint(\"test_labels: \", test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e6e5060d1fcec4f034543e6d16f91e024909e90"},"cell_type":"code","source":"class ArtificialNeuralNetwork(object):\n    \n    def __init__(self, xTrain, xTest, yTrain, yTest):\n        # Define train and test data\n        self.xTrain = xTrain\n        self.xTest = xTest\n        self.yTrain = yTrain.reshape(yTrain.shape[0],-1)\n        self.yTest = yTest.reshape(yTest.shape[0],-1)\n\n        # Define hyperparameters\n        self.inputLayerSize = self.xTrain.shape[0] # nx <-> Number of features\n        self.hiddenLayerSize = 4\n        self.outputLayerSize = self.yTrain.shape[0]\n        \n    def initializeWeightsAndBias(self): #, inputLayerSize, hiddenLayerSize, outputLayerSize):\n        \"\"\"\n        This function creates a vector of zeros of shape (inputLayerSize, 1) for w and initializes b to 0.\n\n        Argument:\n        inputLayerSize -- size of the input layer\n        hiddenLayerSize -- size of the hidden layer\n        outputLayerSize -- size of the output layer\n\n        Returns:\n        params -- python dictionary containing your parameters:\n                        W1 -- weight matrix of shape (hiddenLayerSize, inputLayerSize)\n                        b1 -- bias vector of shape (hiddenLayerSize, 1)\n                        W2 -- weight matrix of shape (outputLayerSize, hiddenLayerSize)\n                        b2 -- bias vector of shape (outputLayerSize, 1)\n        \"\"\"\n        np.random.seed(23) # We set up a seed so that your output matches ours \n                           # although the initialization is random.\n        \n        W1 = np.random.randn(self.inputLayerSize, \n                             self.hiddenLayerSize) * 0.01\n        b1 = np.zeros(shape=(self.hiddenLayerSize, 1))\n        W2 = np.random.randn(self.hiddenLayerSize,\n                             self.outputLayerSize) * 0.01\n        b2 = np.zeros(shape=(self.outputLayerSize, 1))\n        \n        # assert(isinstance(B1, float) or isinstance(B1, int))\n        \n        assert (W1.shape == (self.inputLayerSize, self.hiddenLayerSize)), \"[W1] -> Unsuitable matrix size\"\n        assert (b1.shape == (self.hiddenLayerSize, 1))\n        assert (W2.shape == (self.hiddenLayerSize, self.outputLayerSize)), \"[W2] -> Unsuitable matrix size\"\n        assert (b2.shape == (self.outputLayerSize, 1))\n        \n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}   \n        \n        return parameters\n    \n    def sigmoid(self, Z):\n        \"\"\" Apply and compute sigmoid activation function to scalar, vector, or matrix (Z)\n\n        Arguments:\n        Z -- A scalar or numpy array of any size.\n\n        Return:\n        s -- sigmoid(z)\n        \"\"\"\n        return 1/(1+np.exp(-Z))\n    \n    def forwardPropagation(self, X, parameters):\n        \"\"\" Propogate inputs though network\n        \n        Argument:\n        X -- input data of size (inputLayerSize, m)\n        parameters -- python dictionary containing your parameters (output of initialization function)\n\n        Returns:\n        A2 -- The sigmoid output of the second activation\n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Implement Forward Propagation to calculate A2 (probabilities)\n        Z1 = np.dot(W1.T, X) + b1\n        A1 = sigmoid(Z1)\n        Z2 = np.dot(W2.T, A1) + b2\n        yHat = self.sigmoid(Z2) # A2\n\n        assert(yHat.shape == (1, X.shape[1]))\n    \n        cache = {\"Z1\": Z1,\n                 \"A1\": A1,\n                 \"Z2\": Z2,\n                 \"yHat\": yHat}    # A2\n    \n        return yHat, cache\n    \n    def computeCost(self, yHat, Y, parameters):\n        \"\"\" Compute cost for given X,Y, use weights already stored in class \n\n        Arguments:\n        yHat -- The sigmoid output of the second activation, of shape (1, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n        parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n\n        Returns:\n        cost -- cross-entropy cost given equation (13)\n        \"\"\"\n        m = Y.shape[1] # number of example\n                      \n        # Retrieve W1 and W2 from parameters\n        W1 = parameters['W1']\n        W2 = parameters['W2']   \n                    \n        # Loss\n        logprobs = np.multiply(np.log(yHat), Y) + np.multiply((1 - Y), np.log(1 - yHat))\n        # Cost\n        cost = - (np.sum(logprobs)) / m     # m =  yTrain.shape[1]  is for scaling\n        \n        cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n        assert(isinstance(cost, float))\n                      \n        return cost\n\n    def backwardPropagation(self,parameters, cache,  X, Y):\n        \"\"\" Compute the gradients of parameters by implementing the backward propagation\n\n        Arguments:\n        parameters -- python dictionary containing our parameters \n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n        X -- input data of shape (2, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n\n        Returns:\n        grads -- python dictionary containing your gradients with respect to different parameters\n        \"\"\"\n        m = X.shape[1]   \n                      \n        # First, retrieve W1 and W2 from the dictionary \"parameters\".       \n        W1 = parameters['W1']\n        W2 = parameters['W2']\n                      \n        # Retrieve also A1 and A2 from dictionary \"cache\".\n        A1 = cache['A1']\n        yHat = cache['yHat']                    \n                      \n        # Backward propagation: calculate dW1, db1, dW2, db2.                     \n        dZ2 = yHat - Y\n        dW2 = (1 / m) * np.dot(A1, dZ2.T)\n        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n        dZ1 = np.multiply(np.dot(W2, dZ2), 1 - np.power(A1, 2))\n        dW1 = (1 / m) * np.dot(X, dZ1.T)#(1 / m) * np.dot(dZ1, self.xTrain.T) # MATRIS BOYUTLARINA BAK dW1 ve dW2 ICIN\n        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)   # m is for scaling \n\n        gradients = {\"dW1\": dW1,\n                     \"db1\": db1,\n                     \"dW2\": dW2,\n                     \"db2\": db2}\n                      \n        return gradients\n    \n    def updateParameters(self, parameters, gradients, learning_rate = 0.15):\n        \"\"\"\n        Updates parameters using the gradient descent update rule given above\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        grads -- python dictionary containing your gradients \n\n        Returns:\n        parameters -- python dictionary containing your updated parameters \n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Retrieve each gradient from the dictionary \"grads\"\n        dW1 = gradients['dW1']\n        db1 = gradients['db1']\n        dW2 = gradients['dW2']\n        db2 = gradients['db2']\n        \n        # Update rule for each parameter\n        W1 = W1 - learning_rate * dW1\n        b1 = b1 - learning_rate * db1\n        W2 = W2 - learning_rate * dW2\n        b2 = b2 - learning_rate * db2\n\n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}\n\n        return parameters\n                      \n    def model(self, X, Y, num_iterations=10000, print_cost=False):\n        \"\"\"\n        Arguments:\n        X -- dataset of shape (2, number of examples)\n        Y -- labels of shape (1, number of examples)\n        n_h -- size of the hidden layer\n        num_iterations -- Number of iterations in gradient descent loop\n        print_cost -- if True, print the cost every 1000 iterations\n\n        Returns:\n        parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        np.random.seed(3)\n        \n        costStr = []\n        indexStr = []\n        \n        # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n        parameters = self.initializeWeightsAndBias()\n\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n \n        # Loop (gradient descent)\n        for i in range(0, num_iterations):\n                      \n            # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n            yHat, cache = self.forwardPropagation(X, parameters)\n\n            # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n            cost = self.computeCost(yHat, Y, parameters)\n            \n            # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n            gradients = self.backwardPropagation(parameters, cache, X, Y)\n\n            # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n            parameters = self.updateParameters(parameters, gradients)\n\n            # Print the cost every 1000 iterations\n            if print_cost and i % 1000 == 0:\n                costStr.append(cost)\n                indexStr.append(i)\n                print (\"Cost after iteration %i: %f\" % (i, cost))\n            \"\"\"\n            # Plot Cost Function\n            plt.plot(indexStr,costStr)\n            plt.xticks(indexStr,rotation='vertical')\n            plt.xlabel(\"Number of Iterarion\")\n            plt.ylabel(\"Cost\")\n            plt.show()\n            \"\"\"\n        return parameters\n\n    def predict(self, parameters, X):\n        \"\"\"\n        Using the learned parameters, predicts a class for each example in X\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        X -- input data of size (n_x, m)\n\n        Returns\n        predictions -- vector of predictions of our model (red: 0 / blue: 1)\n        \"\"\"\n        # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n        yHat, cache = self.forwardPropagation(X, parameters)\n        predictions = np.round(yHat)\n\n        \n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ANN = ArtificialNeuralNetwork(features, test_features, labels, test_labels)\nparameters = ANN.model(features, labels, num_iterations = 12000, print_cost=True)\npredictions = ANN.predict(parameters, features)\nprint('Train Accuracy: %d' % float((np.dot(labels, predictions.T) + np.dot(1 - labels, 1 - predictions.T)) / float(labels.size) * 100) + '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = ANN.model(test_features, test_labels, num_iterations = 12000, print_cost=True)\npredictions = ANN.predict(parameters, test_features)\nprint(\"Test Accuracy: %d\" % float((np.dot(test_labels, predictions.T) + np.dot(1 - test_labels, 1 - predictions.T)) / float(test_labels.size) * 100) + '%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}