{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"heart = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\nheart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram\n\nsns.distplot(heart['age'], bins = 15)\nplt.title('Distribution of Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count how many people have the disease and how many people don't.\n\nax = plt.subplot()\nsns.countplot(heart['target'])\nax.set_xticks([0,1])\nax.set_xticklabels(['No Disease', 'Disease'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many men and women have heart disease?\n\nax = plt.subplot()\nsns.countplot(x = heart['target'], hue = 'sex', data = heart)\nax.set_xticklabels(['No Disease', 'Disease'])\nplt.legend(['Female', 'Male'])\nplt.title('Heart Disease Based on Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix\n\ncorr_matrix = heart.corr()\nplt.figure(figsize = (16,8))\nsns.heatmap(corr_matrix, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's investigate the distibution of age based on heart disease.\n\nplt.figure(figsize = (12,8))\nsns.distplot(heart.age[heart['target'] == 0], label = 'No Disease', bins = 20)\nsns.distplot(heart.age[heart['target'] == 1], label = 'Disease', bins = 20)\nplt.legend()\nplt.title('Age Distribution Based on Disease')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nplt.scatter(x = heart.age[heart['target'] == 0], y = heart.thalach[heart['target'] == 0], c = 'red')\nplt.scatter(x = heart.age[heart['target'] == 1], y = heart.thalach[heart['target'] == 1], c = 'blue')\nplt.legend(['No Disease', 'Disease'])\nplt.xlabel('Age')\nplt.ylabel('Maximum Heart Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary libraries\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummy variables.\n\na = pd.get_dummies(heart['sex'], prefix = 'sex')\nb = pd.get_dummies(heart['cp'], prefix = 'cp')\nc = pd.get_dummies(heart['restecg'], prefix = 'restecg')\nd = pd.get_dummies(heart['fbs'], prefix = 'fbs')\ne = pd.get_dummies(heart['exang'], prefix = 'exang')\nf = pd.get_dummies(heart['slope'], prefix = 'slope')\ng = pd.get_dummies(heart['ca'], prefix = 'ca')\nh = pd.get_dummies(heart['thal'], prefix = 'thal')\n\ndummies = [heart, a, b, c, d, e, f, g, h]\nheart = pd.concat(dummies, axis = 1)\nheart = heart.drop(columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\nheart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split up data by features and target variables\n\nX = heart.drop(['target'], axis = 1).values\ny = heart['target']\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale feature columns\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into train and test sets.\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint(logreg.score(X_test, y_test))\nprint(logreg.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick hyperparameters that result in the best accuracy rate.\n\nfrom sklearn.svm import SVC\n\nlargest = {'value':0, 'gamma':1, 'C':1}\nfor gamma in range(1,7):\n    for C in range(1,7):\n        classifier = SVC(kernel = 'linear', C = C, gamma = gamma)\n        classifier.fit(X_train, y_train)\n        score = classifier.score(X_test, y_test)\n        if (score > largest['value']):\n            largest['value'] = score\n            largest['gamma'] = gamma\n            largest['C'] = C\n\nprint(largest)\nprint(classifier.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 25, bootstrap = True, max_features = 'sqrt')\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(rf.score(X_train, y_train))\nprint(rf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_accuracies = []\ntest_accuracies = []\nfor k in range(1,11):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    train_accuracies.append(knn.score(X_train, y_train))\n    test_accuracies.append(knn.score(X_test, y_test))\n    \n# Plotting the results.\n\nk_list = range(1,11)\nplt.plot(k_list, test_accuracies)\nplt.xlabel('k')\nplt.ylabel('Validation Accuracy')\nplt.title('Accuracy Scores')\nplt.show()\n\nprint('Accuracy score for KNN: ', round(max(train_accuracies) * 100), '%.')\nprint('Accuracy score for KNN: ', round(max(test_accuracies) * 100), '%.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\nprint(gnb.score(X_train, y_train))\nprint(gnb.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_scores = {'Model':['Logistic Regression', 'SVM', 'Random Forest', 'KNN', 'Naive Bayes'], \n                   'Training Score':[87.6,87.19,99.58,100,46.69],\n                   'Test Score':[86.88,83.60,83.6,87,47.54]}\naccuracy_scores_df = pd.DataFrame(accuracy_scores)\naccuracy_scores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's compare train set accuracy score.\n\nsns.barplot(x = 'Model', y = 'Training Score', data = accuracy_scores_df)\nplt.title('Training Accuracy Rate per Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's compare test set accuracy score.\n\nsns.barplot(x = 'Model', y = 'Test Score', data = accuracy_scores_df)\nplt.title('Test Accuracy Rate per Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN algorithm has 100% accuracy rate from its training set and an 87% accuracy rate from its test set, making it the best choice for classifying heart disease patients. Naive Bayes was not a good classifier, it got less than half of the target values correct.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}