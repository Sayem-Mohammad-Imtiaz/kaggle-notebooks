{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA California housing prices\n\nThis dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). [*Source*](https://github.com/ageron/handson-ml/tree/master/datasets/housing)\n\nCalifornia is one of the 50 state which conforms USA. It's placed on the west coast. Sacramento is the capital of this state but Los Angeles is the most populated city.\n\nOther (very) important citty situated in California, is San Franciso. If you're here probably you know taht this citty is known as be closer to the Silicon Valley, place where the most valuated startups and companies was born. This is a \"problem\", beacause the ones who lives in SF has incredible incomes (almost 110k USD /year).\n\nThe Objective of this notebook is to give an idea about basics stadistics and univariate and bivariate analysis about features provided. Also diving into data in oder to find some nulls, extreme and missing values.\n\n## **What you are going to fin in this notebook?**\n\n**Part 1: Data QA**\n* Generals about Data set. (shape, column names and info about data type)\n* Information about null values and missing data. \n* Information about outliers.\n* Inconsistences\n* Conclusions and strategies about Data QA.\n\n**Part 2: Reporting**\n* Finding any relations or trends considering multiple features.\n* Analize the most valuated house.\n* Plot an interactive map\n\nIf you like the notebook and think that it helped you, **PLEASE UPVOTE**. It will helps me to keep motivated :)","metadata":{}},{"cell_type":"markdown","source":"#### Load packages","metadata":{}},{"cell_type":"code","source":"import math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport folium\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-15T01:57:26.738724Z","iopub.execute_input":"2021-09-15T01:57:26.739421Z","iopub.status.idle":"2021-09-15T01:57:27.743038Z","shell.execute_reply.started":"2021-09-15T01:57:26.739343Z","shell.execute_reply":"2021-09-15T01:57:27.742227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/california-housing-prices/housing.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:57:27.743957Z","iopub.execute_input":"2021-09-15T01:57:27.745628Z","iopub.status.idle":"2021-09-15T01:57:27.802457Z","shell.execute_reply.started":"2021-09-15T01:57:27.745608Z","shell.execute_reply":"2021-09-15T01:57:27.80202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part 1: Data QA\nFirst 5 rows of the data set","metadata":{}},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:00.558124Z","iopub.execute_input":"2021-09-15T01:58:00.558724Z","iopub.status.idle":"2021-09-15T01:58:00.580989Z","shell.execute_reply.started":"2021-09-15T01:58:00.558694Z","shell.execute_reply":"2021-09-15T01:58:00.58021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shape of data set","metadata":{}},{"cell_type":"code","source":"print('This data set has {} tuples and {} columns'.format(df.shape[0],df.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:01.145695Z","iopub.execute_input":"2021-09-15T01:58:01.145933Z","iopub.status.idle":"2021-09-15T01:58:01.150467Z","shell.execute_reply.started":"2021-09-15T01:58:01.145911Z","shell.execute_reply":"2021-09-15T01:58:01.149714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Column names","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(df.columns, columns=['Columnn names'])","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:02.688615Z","iopub.execute_input":"2021-09-15T01:58:02.689499Z","iopub.status.idle":"2021-09-15T01:58:02.697919Z","shell.execute_reply.started":"2021-09-15T01:58:02.689458Z","shell.execute_reply":"2021-09-15T01:58:02.696689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Info about data type al non-null\nThe info below show how many null values has each attribute","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:03.596167Z","iopub.execute_input":"2021-09-15T01:58:03.597349Z","iopub.status.idle":"2021-09-15T01:58:03.617933Z","shell.execute_reply.started":"2021-09-15T01:58:03.597325Z","shell.execute_reply":"2021-09-15T01:58:03.61717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Null values\nThe next two tables shows how completed is the dataset.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:04.244769Z","iopub.execute_input":"2021-09-15T01:58:04.245067Z","iopub.status.idle":"2021-09-15T01:58:04.251936Z","shell.execute_reply.started":"2021-09-15T01:58:04.245045Z","shell.execute_reply":"2021-09-15T01:58:04.251039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(1-df.isnull().sum()/df.isnull().count())*100","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:04.528152Z","iopub.execute_input":"2021-09-15T01:58:04.529044Z","iopub.status.idle":"2021-09-15T01:58:04.540183Z","shell.execute_reply.started":"2021-09-15T01:58:04.52902Z","shell.execute_reply":"2021-09-15T01:58:04.539254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, only 1 feature has null values.\ntotal_bedrooms is 98.99% completed (it has 207 null values).","metadata":{}},{"cell_type":"markdown","source":"##### Plot outliers","metadata":{}},{"cell_type":"code","source":"def plot_outliers(df,col):\n    \"\"\"  \n    The goal of this function is create boxplot of the continuos variables of the dataset recived as a parameter\n    \n    Args:\n        - df: pd.DataFrame.\n        - col: column that need to be ploted.\n        \n    Return: A boxplot made from data pased as parameter.\n    \n    \"\"\"\n    plt.title(col)\n    ax = sns.boxplot(data=df, x=col)\n    ax.set(xlabel='')\n    plt.show()\n    \ndef plot_hist(df,col):\n    \"\"\"  \n    The goal of this function is create histogram of the continuos variables of the dataset recived as a parameter\n    \n    Args:\n        - df: pd.DataFrame.\n        - col: column that need to be ploted.\n        \n    Return: A histogram made from data pased as parameter.\n    \n    \"\"\"\n    plt.hist(x=df[col],bins=40,color='#D11239')\n    plt.show();\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:05.804108Z","iopub.execute_input":"2021-09-15T01:58:05.804351Z","iopub.status.idle":"2021-09-15T01:58:05.81035Z","shell.execute_reply.started":"2021-09-15T01:58:05.80431Z","shell.execute_reply":"2021-09-15T01:58:05.809338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df.columns:\n    if df[col].dtype == 'float64':\n        plot_outliers(df,col)\n        plot_hist(df,col)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:06.255882Z","iopub.execute_input":"2021-09-15T01:58:06.256318Z","iopub.status.idle":"2021-09-15T01:58:09.239335Z","shell.execute_reply.started":"2021-09-15T01:58:06.256292Z","shell.execute_reply":"2021-09-15T01:58:09.238847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dulpicated values","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:12.677477Z","iopub.execute_input":"2021-09-15T01:58:12.678508Z","iopub.status.idle":"2021-09-15T01:58:12.695191Z","shell.execute_reply.started":"2021-09-15T01:58:12.678475Z","shell.execute_reply":"2021-09-15T01:58:12.694237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inconsistences to be checked\n\n* housing_median_age >= 0\n* total_beedrooms >= 0\n* population >= 0\n* households >= 0\n* median_income >= 0\n* median_house_value >= 0","metadata":{}},{"cell_type":"code","source":"features_inconsistences = ['housing_median_age','total_bedrooms', 'population',\n                           'households','median_income','median_house_value']\nfor feature in features_inconsistences:\n    if df[feature].min() <=0:\n        print('{} has values below 0.\\n'.format(feature))\n    else:\n        print('{} has no values below 0.\\n'.format(feature))","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:13.499387Z","iopub.execute_input":"2021-09-15T01:58:13.499625Z","iopub.status.idle":"2021-09-15T01:58:13.507143Z","shell.execute_reply.started":"2021-09-15T01:58:13.499595Z","shell.execute_reply":"2021-09-15T01:58:13.506384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Obervations abaout Data QA\n\n* The data set is almost completed\n* There are a little outliers in severals features but the most extrange features is median_house_value with the most values near to 206.855, but there are lot of values close to 500k.\n* There are 0 tuples duplicates\n* There are 0 features with inconsistences\n* About the outliers, the aim at Data QA is to show values, if the model required, feature engineering will do in the columns with outliers","metadata":{}},{"cell_type":"markdown","source":"### Part 2: Reporting\n\nThe next block contains a loop which helps to find some stadistics about the continuos variables.","metadata":{}},{"cell_type":"code","source":"desv_std = []\nfor col in df.select_dtypes([\"float64\"]).columns:\n    desv_std.append(\n        {\n            \"Feature\": col,\n            \"DesvStd\": df[col].std(),\n            \"Mean\": df[col].mean(),\n            \"Max\": int(df[col].max()),\n            \"Min\": df[col].min(),\n            \"Q_1\": df[col].quantile(0.25),\n            \"Q_3\": df[col].quantile(0.75),\n            \"Dif Max-Q_3\": int(\n                df[col].max() - df[col].quantile(0.75)\n            ),\n        }\n    )\n\ndf_desv_std = (\n    pd.DataFrame(desv_std)\n    .sort_values(by=\"DesvStd\", ascending=False)\n    .reset_index(drop=True)\n)\ndf_desv_std","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:15.034396Z","iopub.execute_input":"2021-09-15T01:58:15.034634Z","iopub.status.idle":"2021-09-15T01:58:15.080464Z","shell.execute_reply.started":"2021-09-15T01:58:15.034609Z","shell.execute_reply":"2021-09-15T01:58:15.079669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **median_house_value:** Has the highest standard deviation.\n","metadata":{}},{"cell_type":"markdown","source":"#### Heatmap\n\nThe corralation is a metric about how much related are two features. Correlation is a value between -1 and 1. Closer to 1 means very strong and positive (direct) relation (example: Mayor power on cars means to much use of fuel). On the other hand, closer to -1 mean a very strong and negative (inversely)relaation (example: Spend more hours at work, mean lees hours for sleep). At the end, if value is closer to 0, there is no relation between features.\n\nBelow you can see a plot called heatmap, which dives into corelation about the loaded dataset","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = df.corr()\nsns.heatmap(corr,\n            mask=np.zeros_like(corr, dtype=np.bool),\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True,\n            ax=ax);","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:16.550207Z","iopub.execute_input":"2021-09-15T01:58:16.550452Z","iopub.status.idle":"2021-09-15T01:58:16.873917Z","shell.execute_reply.started":"2021-09-15T01:58:16.550426Z","shell.execute_reply":"2021-09-15T01:58:16.873299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpreting The Heatmap\n* total_rooms, total_debrooms has a strong related with population and households becaouse. It is expected, because more people, in general, means more places to live.\n\n* median_income is related with median_house_value. Also it is expected (the richer the population, the higher prices).","metadata":{}},{"cell_type":"markdown","source":"#### Interactive Map\n\nThe aim of the next plot is to show the longitude and latitude in a map.\n\n**NOTE:** the plot has only the first 1000 rows as a example.","metadata":{}},{"cell_type":"code","source":"m = folium.Map(location=[20,0], tiles=\"OpenStreetMap\", zoom_start=2)\nfor i in range(len(df.head(1000))):\n    folium.Marker(\n      location=[df.iloc[i]['latitude'], df.iloc[i]['longitude']],\n   ).add_to(m)\n\nsw = df[['latitude', 'longitude']].min().values.tolist()\nne = df[['latitude', 'longitude']].max().values.tolist()\n\nm.fit_bounds([sw, ne]) \nm","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:58:30.97523Z","iopub.execute_input":"2021-09-15T01:58:30.975872Z","iopub.status.idle":"2021-09-15T01:58:32.092258Z","shell.execute_reply.started":"2021-09-15T01:58:30.975835Z","shell.execute_reply":"2021-09-15T01:58:32.091818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next Steps\n\nThe next notebook will include some feature engineering and a ML model.","metadata":{}}]}