{"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"cells":[{"execution_count":null,"metadata":{"collapsed":true},"source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport numpy as np\n\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"\n# Reading the data. Using 'id' as the column index\nbreast_cancer_df = pd.read_csv('../input/data.csv', index_col='id', header=0)\n\n# Preliminary EDA \nprint(breast_cancer_df.info())\n\n# Dropping the last column because it is full of NaNs\nbreast_cancer_df.drop('Unnamed: 32', axis=1, inplace=True)\n\n# Replacing with numeric values in order to make plots\nbreast_cancer_df['diagnosis'] = breast_cancer_df['diagnosis'].replace({'M':1, 'B':0})\n\nbreast_cancer_df.info()\n\n# Converting the diagnosis column to categorical to save space\nbreast_cancer_df['diagnosis'] = breast_cancer_df['diagnosis'].astype('category')\nheatmap_data = breast_cancer_df.corr()\nfig, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(heatmap_data, cbar = True, annot=True, ax=ax)\nplt.show()\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nbreast_cancer_df_scaled = scale(np.array(breast_cancer_df))\nX_train, X_test, y_train, y_test = train_test_split(breast_cancer_df_scaled,\n                                                    breast_cancer_df.iloc[:, 0],\n                                                    test_size=0.3, stratify=breast_cancer_df.iloc[:, 0])\n\ntest_accuracy = np.empty(25)\ntrain_accuracy = np.empty(25)\nfor n in range(1,25):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    test_accuracy[n] = knn.score(X_test, y_test) \n    train_accuracy[n] = knn.score(X_train, y_train)\n\nplt.style.use('ggplot')\nplt.plot(np.linspace(1,25,25),test_accuracy, color ='red', label='test accuracy')\nplt.plot(np.linspace(1,25,25),train_accuracy, color = 'blue', label='train accuracy')\nplt.xlim((2, 25))\nplt.xlabel(\"Number of neighbours\")\nplt.ylabel(\"Prediction Accuracy\")\nplt.title(\"Prediction Accuracy Vs. Number of Neighbours\")\nplt.ylim((0.85, 1.05))\nplt.legend(loc='upper right') \nplt.show()\n","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"# By inspection, k =4 seems like a good choice\nprint (test_accuracy[4])","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"# Trying to see if it is possible to decompose into fewer components\nfrom sklearn import decomposition\npca = decomposition.PCA()\npca.fit(breast_cancer_df)\ndf_reduced = pca.fit_transform(breast_cancer_df)\ndf_reduced.shape","outputs":[],"cell_type":"code"},{"metadata":{},"source":"PCA doesn't lead to significantly fewer features\n","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"# Checking to see if our inspection for 4 neighbors is correct\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\n\nsteps = [('scaler', StandardScaler()),('knn', KNeighborsClassifier())]\npipeline = Pipeline(steps)\nparameters = {'knn__n_neighbors':np.arange(1,50)}\ncv = GridSearchCV(pipeline,parameters)\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\nprint(cv.best_params_)\nprint(cv.score(X_test, y_test))\nprint(classification_report(y_test, y_pred))","outputs":[],"cell_type":"code"},{"metadata":{},"source":"Clearly, 4 nearest neighbors score very high\n","cell_type":"markdown"}]}