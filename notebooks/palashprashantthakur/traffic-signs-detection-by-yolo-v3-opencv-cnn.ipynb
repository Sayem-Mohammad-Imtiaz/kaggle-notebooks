{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Traffic Signs Detection with YOLO v3, OpenCV and Keras","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Importing needed libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\nfrom keras.models import load_model\nimport os\nprint(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Loading *labels*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('../input/traffic-signs-preprocessed/label_names.csv')\n#labels = pd.read_csv('../input/label1/Book1.csv',encoding='latin1')\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\nprint(labels.iloc[0][1])  # Speed limit (20km/h)\nprint(labels['SignName'][1]) # Speed limit (30km/h)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Loading trained Keras CNN model for Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\nmodel = load_model('../input/inputmodel/model.h5')\n#model = load_model('../input/classifier/model80x80.h5')\n\nwith open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')\n    \n#print(mean['mean_image_rgb'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Loading YOLO v3 network by OpenCV dnn library","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Loading *trained weights* and *cfg file* into the Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_to_weights = '../input/trafficsign/yolov3_ts.weights'\npath_to_cfg = '../input/traffic-signs-dataset-in-yolo-format/yolov3_ts_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For other objects\nnet = cv2.dnn.readNet(\"../input/yolo-object/yolov3.weights\", \"../input/yolo-cfg/yolov3.cfg\")\nclasses = []\nwith open(\"../input/yolo-cfg/coco.names\", \"r\") as f:\n    classes = [line.strip() for line in f.readlines()]\nlayer_names = net.getLayerNames()\noutput_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\ncolors = np.random.uniform(0, 255, size=(len(classes), 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting *output layers* where detections are made","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"layers_all = network.getLayerNames()\n# Getting only detection YOLO v3 layers that are 82, 94 and 106\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n# Check point\nprint()\nprint(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting *probability*, *threshold* and *colour* for bounding boxes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Minimum probability to eliminate weak detections\nprobability_minimum = 0.2\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.2\n\n# Generating colours for bounding boxes\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n\nprint(type(colours))  \nprint(colours.shape)  \nprint(colours[0])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Reading input video","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading video from a file by VideoCapture object\nvideo = cv2.VideoCapture('../input/newdata/watch-dogs-2-20200727-13081103_iEkmXFsa.compressed_Trim.mp4')\n#video = cv2.VideoCapture('../input/trafficsigntest/Desktop 2020.06.29 - 18.51.55.01.mp4')\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing frames in the loop","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# %matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n\n    # If the frame was not retrieved\n    if not ret:\n        break\n       \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    confidences = []\n    class_numbers = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                # Scaling bounding box coordinates to the initial frame size\n                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                # Getting top left corner coordinates\n                x_center, y_center, box_width, box_height = box_current\n                x_min = int(x_center - (box_width / 2))\n                y_min = int(y_center - (box_height / 2))\n\n                # Adding results into prepared lists\n                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                confidences.append(float(confidence_current))\n                class_numbers.append(class_current)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) > 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            #print(c_ts.shape)\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n                #blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n                #plt.imshow(blob_ts[0, :, :, :])\n                #plt.show()\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n                # print(labels['SignName'][prediction])\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                imgray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n                roi= imgray[y_min:y_min+int(box_height), x_min:x_min+int(box_width)]\n                edged = cv2.Canny(roi, 30, 200)\n                contours, hierarchy = cv2.findContours(edged,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n                cv2.drawContours(frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width),:],contours,-1,(0,255,0),1,maxLevel=5)\n                \n                #cv2.rectangle(frame, (x_min, y_min),\n                #              (x_min + box_width, y_min + box_height),\n                #              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n                \n                \n    height=h\n    width=w\n    \n    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n    net.setInput(blob)\n    outs = net.forward(output_layers)\n    class_ids = []\n    confidences = []\n    boxes = []\n    for out in outs:\n        for detection in out:\n            scores = detection[5:]\n            class_id = np.argmax(scores)\n            confidence = scores[class_id]\n            if confidence > 0.5:\n            # Object detected\n                center_x = int(detection[0] * width)\n                center_y = int(detection[1] * height)\n                w1 = int(detection[2] * width)\n                h1 = int(detection[3] * height)\n\n                # Rectangle coordinates\n                x = int(center_x - w1 / 2)\n                y = int(center_y - h1 / 2)\n\n                boxes.append([x, y, w1, h1])\n                confidences.append(float(confidence))\n                class_ids.append(class_id)\n\n    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n    #print(indexes)\n    font = cv2.FONT_HERSHEY_PLAIN\n    for i in range(len(boxes)):\n        if i in indexes:\n            x, y, w1, h1 = boxes[i]\n            label = str(classes[class_ids[i]])\n            if label!='stop sign':\n                try:\n                    color = colors[class_ids[i]]\n                    cv2.rectangle(frame, (x, y), (x + w1, y + h1), color, 2)\n                    cv2.putText(frame, label, (x, y + 30), font, 3, color, 3)\n                except:\n                    continue\n    \n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('result.mp4', fourcc, 30,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  FPS results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f / t), 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving locally without committing\nfrom IPython.display import FileLink\n\nFileLink('result.mp4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}