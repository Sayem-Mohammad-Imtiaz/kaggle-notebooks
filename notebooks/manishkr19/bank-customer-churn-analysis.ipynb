{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bank Customer Churn Analysis \n\nIn this kernel I am going to make an **Exploratory Data Analysis** (EDA) on [this](https://www.kaggle.com/manishkr19/bank-customer-churn-analysis/#data) dataset. Also I am going to make different predictive models and find out the best one with highest prediction accuracy.\n\n**Kernel Outlines:**\n\n* Importing Necessary Packages\n \n* Statistical Summary of the Dataset\n\n* Dropping Irrelevant Features\n\n* Data Visualization\n\n* Checking Correlation with Heatmap\n \n* ML predictive models\n\n* Improve the Predictive Model\n\n    Feature Scaling\n    \n    Over Sampling\n    \n    Balancing the data usinf SMOTE\n    \n\n* Conclusion\n\n\n\n**Importing Necessary Packages**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport matplotlib.pyplot as plt\nimport os\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read the data frame\ndf = pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv', delimiter=',')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NAN value\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping inappropriate columns\n#data=data.drop('customerID',axis=1)\ndata.drop(columns=data.columns[:3], axis=1, inplace=True)\n\ndata_col=data.columns\nprint(data_col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#labelencoding\nle = preprocessing.LabelEncoder()\nfor i in data_col:\n    data[i]=le.fit_transform(data[i])\n    \n\n    \n#features\nX=data.iloc[:,0:-1]\n#labels    \ny=data.iloc[:,-1]\n\nprint(type(X))\nprint(X.head())\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  OUR DATA IS UNBALENCE HENCE BALENCING THE DATA USING SMOTE\n\na = np.array(data.loc[:, data.columns != 'Exited'])\nb = np.array(data.loc[:, data.columns == 'Exited'])\nprint('Shape of X: {}'.format(a.shape))\nprint('Shape of y: {}'.format(b.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying smote\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\n\n\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOW APPLYING MODEL TO BALENCED CLASS DATA.\nf1 = []\nacc = []\nn = 50\nprint(\"Random Forest:\")\nfor i in range(n):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    rf = RandomForestClassifier(n_estimators = 100, random_state = 0) \n    rf.fit(X_train_res, y_train_res)\n    ypred=rf.predict(X_test)\n    f1.append(f1_score(y_test, ypred, average='weighted'))\n    acc.append(accuracy_score(y_test, ypred))\nprint(\"Avg F1-Score\",np.mean(f1))\nprint(\"Max F1-Score\",np.max(f1))\nprint(\"Min F1-Score\",np.min(f1))\nprint(\"Avg Accuracy\",np.mean(acc))\nprint(\"Max Accuracy\",np.max(acc))\nprint(\"Min Accuracy\",np.min(acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RANDOM FOREST\nscore = metrics.accuracy_score(y_test, ypred)\nprint(\"accuracy:   %0.3f\" % (score*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, ypred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\np = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#roc score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_test,ypred)\nprint('AUC: %.3f' % auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#roc plotting\nfpr, tpr, thresholds = roc_curve(y_test, ypred)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nIn this project we build a model that predict how likely a customer is going to churn. During exploratory data analysis we found out that the female customer are the most likely to churn, customer that are located in Germany are the most churned, and also customer using only one product are the most churned. After building several model we ended up with Random Forest which performed better than others. Since the problem is about binary classification with a imbalance dataset, I have used the most efficient metric for model performance which is the ROC-AUC score and my model achieved about  94%  accuary. This score is by far the most efficient accuracy as per industry standards. The model can achieve better performance providing a lot of historical data for the training phase.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**That's it for this kernel.** If you like this kernel then give a upvote! ðŸ˜œ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}