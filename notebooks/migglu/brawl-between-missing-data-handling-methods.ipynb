{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nfrom IPython.display import display\nbaseCodeHide=\"\"\"\n<style>\n.button {\n    background-color: #008CBA;;\n    border: none;\n    color: white;\n    padding: 8px 22px;\n    text-align: center;\n    text-decoration: none;\n    display: inline-block;\n    font-size: 16px;\n    margin: 4px 2px;\n    cursor: pointer;\n}\n</style>\n <script>\n   // Assume 3 input cells. Manage from here.\n   var divTag0 = document.getElementsByClassName(\"input\")[0]\n   var displaySetting0 = divTag0.style.display;\n   // Default display - set to 'none'.  To hide, set to 'block'.\n   // divTag0.style.display = 'block';\n   divTag0.style.display = 'none';\n   \n   var divTag1 = document.getElementsByClassName(\"input\")[1]\n   var displaySetting1 = divTag1.style.display;\n   // Default display - set to 'none'.  To hide, set to 'block'.\n      divTag1.style.display = 'block';\n   //divTag1.style.display = 'none';\n   \n   var divTag2 = document.getElementsByClassName(\"input\")[2]\n   var displaySetting2 = divTag2.style.display;\n   // Default display - set to 'none'.  To hide, set to 'none'.\n   divTag2.style.display = 'block';\n   //divTag2.style.display = 'none';\n \n    function toggleInput(i) { \n      var divTag = document.getElementsByClassName(\"input\")[i]\n      var displaySetting = divTag.style.display;\n     \n      if (displaySetting == 'block') { \n         divTag.style.display = 'none';\n       }\n      else { \n         divTag.style.display = 'block';\n       } \n  }  \n  </script>\n  <!-- <button onclick=\"javascript:toggleInput(0)\" class=\"button\">Show Code</button> -->\n\"\"\"\nh=HTML(baseCodeHide)\n\n\ndisplay(h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![ll](https://images.unsplash.com/photo-1519662596600-f508233aaf3c?ixlib=rb-1.2.1)\n<center>By Raden Masputra, at Unsplash</center>"},{"metadata":{},"cell_type":"markdown","source":"# Mission\n\nCreating a model whereby dementia can be predicted for any person, enabling early intervention. The long-term goal is to be able to integrate this model into any digital healthcare platform in the world that deals with mental health, or health in a holistic sense."},{"metadata":{},"cell_type":"markdown","source":"# Dataset description\n\n\n> The [Open Access Series of Imaging Studies (OASIS)](http://) is a project aimed at making MRI data sets of the brain freely available to the scientific community. By compiling and freely distributing MRI data sets, we hope to facilitate future discoveries in basic and clinical neuroscience. OASIS is made available by the Washington University Alzheimer’s Disease Research Center, Dr. Randy Buckner at the Howard Hughes Medical Institute (HHMI)( at Harvard University, the Neuroinformatics Research Group (NRG) at Washington University School of Medicine, and the Biomedical Informatics Research Network (BIRN).\n> \n> \n> **Cross-sectional MRI Data in Young, Middle Aged, Nondemented and Demented Older Adults:** This set consists of a cross-sectional collection of 416 subjects aged 18 to 96. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 100 of the included subjects over the age of 60 have been clinically diagnosed with very mild to moderate Alzheimer’s disease (AD). Additionally, a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session.\n> \n> **Longitudinal MRI Data in Nondemented and Demented Older Adults:** This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer’s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.\n\n### Columns\n\nThe datasets consist of the following columns. Some are only in one of the two. \n\n**Indexing**\n\n* ID / Subject ID\n* MRI ID - An ID of the particular MRI scan. (Longitudinal only) \n* Visit - The ordinality of the hospital visit: 1st, 2nd or 3rd. (Longitudinal only) \n\n**Demographics**\n\n* M/F - Gender\n* Hand - Handedness \n* Age - Age in years\n* Educ - Years of education\n* SES - Socioeconomic status, as assessed by the Hollingshead Index of Social Position. Categories 1-5, with 1 being dirt poor and 5 someone with a couple Teslas.\n\n**Clinical ratings**\n\n* MMSE - Mini Mental State Examination score (from 0 to 30, i.e., worst to best).\n* CDR - Clinical Dementia Rating (0 = no dementia, 0.5 = very mild, 1 = mild AD, 2 = moderate) (Cross-sectional only)\n* Group - The category of the subject, considering his/her change during the longitudinal study: Demented, Non-demented, or Converted (i.e., gained dementia during the study). (Longitudinal only) \n\n**Physical measurements** \n\n* eTIV - Estimated total intracranial volume, mm3\n* nWBV - Normalized whole-brain volume. Expressed as a percent of all voxels in the atlas-masked image that are labeled as white or gray matter by the automated tissue segmentation process\n* ASF - Atlas scaling factor (unitless). Computed scaling factor that transforms native-space brain and skull to the atlas target (i.e., the determinant of the transform matrix)\n* MR Delay - The time that the MRI scan took.\n\nThe above descriptions of nWBV and ASF are copied from \n*Shukla, Anupam Prof, and Ritu Tiwari. Discrete Problems in Nature Inspired Algorithms. p. 227, CRC Press, 2017.*\n\n### Columns to be trivially dropped\n\nThe Subject ID will be needed to merge the datasets. But afterwards, besides the three indexing columns, the following two will be dropped from the dataset:\n\n* *Hand:* Yields no info, as everyone here is right-handed.\n\n* *MR Delay:* The time a scan takes has nothing to do with the person being scanned. \n"},{"metadata":{},"cell_type":"markdown","source":"# Setup "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\n\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\nfrom scipy import stats\nfrom scipy.stats import skew\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 100)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os,random, math, psutil, pickle\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error, accuracy_score, f1_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the data and splitting it into the training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"long_df = pd.read_csv(\"/kaggle/input/mri-and-alzheimers/oasis_longitudinal.csv\")\ncross_df = pd.read_csv(\"/kaggle/input/mri-and-alzheimers/oasis_cross-sectional.csv\")\n\ndef split(df, test_size=0.2):\n    #df = df.sample(frac=1, random_state=6) # Shuffle the df.\n    split_index = int(len(df)*test_size)\n    return df[:-split_index], df[-split_index:]\n\nlong_train, long_val = split(long_df)\ncross_train, cross_val = split(cross_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a binary feature called *demented*, showing whether the person has any form of dementia. Every person in the Cross-sectional who has a CDR above 0, and everyone in the Longitudinal who belongs to groups Demented and Converted, will get *demented = 1*."},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import isnan\n\n# Preserving the NaNs so they can be imputed later.\ndef binarize_cdr(x):\n    if isnan(x):\n        return x\n    else:\n        return 1 if x > 0 else 0\n\n# There are no NaNs in the Group column.\ndef binarize_group(x):\n    return 1 if x != 'Nondemented' else 0\n\nlong_train['demented'] = long_train['Group'].apply(binarize_group)\ncross_train['demented'] = cross_train['CDR'].apply(binarize_cdr)\nlong_val['demented'] = long_val['Group'].apply(binarize_group)\ncross_val['demented'] = cross_val['CDR'].apply(binarize_cdr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining the Cross-sectional and the Longitudinal. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_dfs(long, cross):\n    \n    # Renaming the education column to match between the two dataframes.\n    long = long.rename(columns={'EDUC': 'Educ'})\n\n    # Combining the rows of each person in the Longitudinal into a single one, taking the mean of each feature's values.\n    mean_long = long.groupby('Subject ID').mean()\n\n    # Adding the columns of strings back, as they were dropped by the mean() \n    mean_long = pd.merge(mean_long, long[['Subject ID', 'M/F', 'Group']].drop_duplicates(), on='Subject ID', how='right').reset_index(drop=True)\n\n    # Then concatenating the new condensed dataframe with the Cross-sectional.\n    total = pd.concat((cross, mean_long)).reset_index(drop=True)\n    \n    return total, mean_long\n\ntotal_train, long_train = combine_dfs(long_train, cross_train)\ntotal_val, _ = combine_dfs(long_val, cross_val)\n\n# Dropping from the validation set the rows where the demented feature is NaN, as they cannot be used for validation.\ntotal_val = total_val[~np.isnan(total_val['demented'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the redundant columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train = total_train.drop(['Visit', 'Hand', 'MR Delay', 'Delay', 'Subject ID', 'ID'], axis=1)\ntotal_val = total_val.drop(['Visit', 'Hand', 'MR Delay', 'Delay', 'Subject ID', 'ID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploration\n\nI will use only the training sets here in order to prevent leaking info from the testing sets into the model and thereby biasing it. Indeed, exploration, when conducted so as to unearth insights for modeling, is already part of the the model's 'training'."},{"metadata":{},"cell_type":"markdown","source":"### Age distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,7))\nsns.distplot(cross_train[['Age']], hist=False, label='Cross')\nsns.distplot(long_train['Age'], hist=False, label='Long', color='g')\ng = sns.distplot(total_train['Age'], hist=False, rug=True, kde_kws=dict(linewidth=3), label='Total')\n_ = g.set_title('Distribution of ages in the two datasets and their combination.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standard deviation of ages in the longitudinal is pretty small; an elderly population was intentionally chosen for that study. From experience of living in society we can say that the expected probability of having dementia is far greater for old people than for young and middle-aged ones, and hence the longitudinal study is very valuable for increasing the accuracy of the eventual model. However, it would advisable to have more data of the boundary region between youth and oldness (that is, the middle ages); there is an unseemly trough at the 30 - 50 range."},{"metadata":{},"cell_type":"markdown","source":"### CDR and dementia state against various features"},{"metadata":{},"cell_type":"markdown","source":"**Age**\n\nLets split the males and females in order to be able to compare them better."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,9))\nax = fig.add_subplot(1, 2, 1)\nax.set_title('Cross-sectional')\nsns.catplot(x='CDR', y='Age', data=cross_train, kind='swarm', split=True, hue='M/F', ax=ax)\nax = fig.add_subplot(1, 2, 2)\nax.set_title('Longitudinal')\nsns.catplot(x='Group', y='Age', data=long_train, kind='swarm', split=True, hue='M/F', ax=ax);\nplt.close(2)\nplt.close(3)\n\nfig = plt.figure(figsize=(8,6))\ng = sns.catplot(x='demented', y='Age', data=total_train, kind='violin', split=True, hue='M/F')\ng.ax.set_title('Total data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross-sectional**\n\n* By far the most of the test subjects had no dementia at all. \n\n* Everyone who had even the slightest indications of dementia was over 60 years old.\n\n* There does not seem to be any relationship between gender and CDR. \n\n* There are no cases of severe dementia, i.e., CDR = 3.\n\n\n**Longitudinal**\n\nNote that the Nondemented and Converted groups are dominated by females, while the Demented group is dominated by males. Could this be an indication of women's having more mentally stimulating lifestyles, or at least having had decades ago, when these old people were young? \n\nIn any case, the scarcity of males in the Converted group is probably caused by the tendency of the males' dementia to have already begun by the time the study was conducted.\n\n**Total**\n\nThere is no significant difference between the genders in the age of demented people: the condition tends to begin creeping into people's brains at the same age (about 55)."},{"metadata":{},"cell_type":"markdown","source":"**Mini mental state examination**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,9))\nax = fig.add_subplot(1, 2, 1)\nax.set_title('Cross-sectional')\nsns.catplot(x='CDR', y='MMSE', data=cross_train, kind='swarm', hue='M/F', split=True, ax=ax)\nax = fig.add_subplot(1, 2, 2)\nax.set_title('Longitudinal')\nsns.catplot(x='Group', y='MMSE', data=long_train, kind='swarm', hue='M/F', split=True, ax=ax);\nplt.close(2)\nplt.close(3)\n\nfig = plt.figure(figsize=(8,6))\ng = sns.catplot(x='demented', y='MMSE', data=total_train, kind='violin', split=True, hue='M/F')\ng.ax.set_title('Total data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A considerable amount of demented people got scores in the same range (26 - 30) as the non-demented ones in both the cross-sectional and longitudinal. As such, it is no wonder that the majority of the converted people too lie in this range. It seems that this test does not pick up dementia as well as the one used for the CDR.\n\nIn the cross-sectional, the majority of people with any dementia (ie. non-zero CDR) are males. On the other hand, in the longitudinal, there is no gender bias for existing dementia, though the majority of the people who were developed dementia after the first test were female."},{"metadata":{},"cell_type":"markdown","source":"**Socioeconomic status**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,9))\nax = fig.add_subplot(1, 2, 1)\nax.set_title('Cross-sectional')\nsns.catplot(x='CDR', y='SES', data=cross_train, kind='swarm', split=False, hue='M/F', ax=ax)\nax = fig.add_subplot(1, 2, 2)\nax.set_title('Longitudinal')\nsns.catplot(x='Group', y='SES', data=long_train, kind='swarm', split=False, hue='M/F', ax=ax);\nplt.close(2)\nplt.close(3)\n\nfig = plt.figure(figsize=(8,6))\ng = sns.catplot(x='demented', y='SES', data=total_train, kind='violin', split=True, hue='M/F')\ng.ax.set_title('Total data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most striking thing is that in the cross-sectional study, the people with the highest socioeconomic status are all males, while in the longitudinal they are all females. Otherwise the socioeconomic status does not seem to have a correlation with dementia.\n\nNote also the clear median of 2 for non-demented females, and the median of 3 for demented. It is surprising that people of higher socioeconomic status would have a higher probability of developing dementia, since they tend to have engage more in mentally stimulating activities (e.g., reading literature). "},{"metadata":{},"cell_type":"markdown","source":"**Years of education**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,9))\nax = fig.add_subplot(1, 2, 1)\nax.set_title('Cross-sectional')\nsns.catplot(x='CDR', y='Educ', data=cross_train, kind='violin', ax=ax)#, split=True, hue='M/F')\nax = fig.add_subplot(1, 2, 2)\nax.set_title('Longitudinal')\nsns.catplot(x='Group', y='Educ', data=long_train, kind='violin', ax=ax, split=True, hue='M/F')\n# These are needed to suppress the extra plots that seaborn tries to create.\nplt.close(2)\nplt.close(3)\n\nfig = plt.figure(figsize=(8,6))\ng = sns.catplot(x='demented', y='Educ', data=total_train, kind='violin', split=True, hue='M/F')\ng.ax.set_title('Total data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As is to be expected, there seems to be a slight negative relationship between dementia and years of education. In the Cross-sectional, this is visible as the shifting of the bulk of the distributions downwards from CDR = 0 to 0.5 and 1, and of course blatantly in the great swath of people near 0 years in the CDR = 2 group. However, looking at the Longitudinal and the Total, even though the demented distribution is slightly lower than the non-demented in its head, tail and median, the overlap between the two categories is nevertheless too significant to be able to assert that demented people tend to have shorter educations than non-demented.\n\nLooking at the total data, note the great median at around 12 years for demented females, which contrasts with the more even distribution of the demented males. (This is mostly from the equivalent peak in the Longitudinal's Demented group). It is difficult to conceive of a reason for this. One hypothesis is that women with barely any education tend to (or tended a few decades ago) to accept the jobs of housemaking and raising children more readily than more educated ones. However, it is not intuitively obvious that such activities would less mentally stimulating than doing an 'actual' job. In fact, the empathetic and linguistic capabilities trained during interactions with children seem to me more stimulating than many jobs."},{"metadata":{},"cell_type":"markdown","source":"### Volumes"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.2)\n#ax = fig.add_subplot(1, 2, 1)\n#_ = sns.distplot(total_train['eTIV']).set_title('eTIV distribution in the whole training data.')\n#ax = fig.add_subplot(1, 2, 2)\n\nfor i, x in enumerate(['eTIV', 'nWBV', 'ASF']):\n    ax = fig.add_subplot(2, 2, i+1)\n    sns.distplot(cross_train[x], hist=False, label='Cross', ax=ax)\n    sns.distplot(long_train[x], hist=False, label='Long', color='g', ax=ax)\n    g = sns.distplot(total_train[x], hist=False, rug=True, kde_kws=dict(linewidth=3), label='Total', ax=ax)\n    _ = g.set_title('Distribution of {} in the two datasets and their combination.'.format(x))\n\n# These are needed to suppress the extra plots that seaborn tries to create.\nplt.close(2)\nplt.close(3)\n\n#fig = plt.figure(figsize=(8,6))\n#_ = sns.distplot(total_train['ASF']).set_title('Atlas scaling factor in the whole training data.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dodgy distributions! (Except for the comely nWBV Longitudinal distribution). Natural scale attributes (that is, related to size) are usually normally distributed (or, more accurately, lognormally), but none of these except for the green nWBV curve seem to be so. Since the curves of the Cross-sectional and Longitudinal are almost equivalent for eTIV, their unruly shape must be caused by the measurement methodology. As for the ASF, it is no wonder that the distribution looks like that of eTIV, being derived from it."},{"metadata":{},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a correlation heatmap with all the dataset's features.\nplt.figure(figsize=(14, 8))\ncorr = total_train.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, annot=True, cmap='bwr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is time to choose the features to use in the model.\n\n* Looking at the *demented* row and column, it has a great or decent correlation with everything except ASF and eTIV, so they will be dropped. \n* Since CDR was used to engineer the demented feature, it will be dropped.\n* The correlation of nWBV with age is so significant that one of them should perhaps be dropped. Which one? Even though the magnitude of the nWBV correlation is 0.11 higher than the Age one, I will go for the latter in order to keep the model more practical. After all, it is far easier to find out a person's age than to conduct a brain scan. The model could therefore be used  digitally before any hospital visit to find out the need to for such a visit. However, lets see the effect on the model's accuracy of removing each one separately.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_drop = ['Group', 'CDR', 'ASF', 'eTIV', 'nWBV']\ntotal_train, total_val = total_train.drop(features_to_drop, axis=1), total_val.drop(features_to_drop, axis=1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding the M/F column. In other words, mapping the M and F strings into 0 and 1 respectively so that it could be used by  ML models."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train['M/F'], total_val['M/F'] = pd.get_dummies(total_train['M/F']), pd.get_dummies(total_val['M/F'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing data\n\nThere are various ways to handle missing data. I will try five methods, four of which are imputations (that is, assigning values by inference) and the fifth is simply dropping the NaN rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(total_train)\n\n###############################\n# Can also use describe(data)\n###############################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately a massive amount is missing."},{"metadata":{},"cell_type":"markdown","source":"### Method 1: Imputing the most frequent value"},{"metadata":{"trusted":true},"cell_type":"code","source":"h1_train, h1_val = total_train.copy(), total_val.copy()\n\n\nfrom sklearn_pandas import CategoricalImputer\nci = CategoricalImputer().fit(h1_train.to_numpy())\nh1_train.loc[:,:] = ci.transform(h1_train.to_numpy())\nh1_val.loc[:,:] = ci.transform(h1_val.to_numpy()) \n\n#i2_train.loc[:,:] = CategoricalImputer().fit(i2_train.to_numpy()).transform(i2_train.to_numpy())\n#i2_val.loc[:,:] = CategoricalImputer().fit(i2_val.to_numpy()).transform(i2_val.to_numpy()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method 2: Imputing by linear regression; ordering and splitting the target feature.\n\nWhile linear regression is good for the continuous features, it will not work well for the target feature (*demented*), as it is binary. It will have to be handled differently."},{"metadata":{"trusted":true},"cell_type":"code","source":"h2_train, h2_val = total_train.copy(), total_val.copy()\n\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(max_iter=10, random_state=0).fit(h2_train)\nh2_train.loc[:,:] = imp.transform(h2_train)\nh2_val.loc[:,:] = imp.transform(h2_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check out the values of the target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"h2_train['demented'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, none of of the imputed values are above 0.5.\n\nA different threshold is therefore needed to map the values into 0 and 1. I could of course simply use the point equidistant from the min and max of the values. However, there is no reason to assume that these values can be interpreted as probabilities. So I will instead not use a threshold at all, but rather assume that the expected value of the probability of dementia is the same in this NaN-subsample as in the non-NaN one. I will then find out the ratio *X* of demented people in the non-Nan subsample of the Cross-validated set (I am excluding the Longitudinal from this ratio because its *demented* column contains no missing data), order the imputed values ascending, and map the last *XN* values to 1, the rest to 0, where *N* is the amount of values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def binarize_imputed(total, cross):\n    # Find the ratio of people in the training set (excluding the ones with NaNs) that do not have dementia.\n    nonnan_demented = cross['demented'].dropna()\n    demented_ratio = nonnan_demented.value_counts()[1] / len(nonnan_demented)\n\n    # Getting the imputed values of the target column and ordering them ascending.\n    sorted_imputed = cross.loc[cross['demented'].isna(), 'demented'].sort_values()\n    to_zeros, to_ones = split(sorted_imputed, demented_ratio)\n\n    # Conducting the mapping.\n    total.loc[to_zeros.index, 'demented'] = 0\n    total.loc[to_ones.index, 'demented'] = 1\n    return total['demented']\n\nh2_train['demented'] = binarize_imputed(h2_train, cross_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method 3: Imputing by linear regression; using logistic regression for the target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copying the linear regression-imputed data from the previous method.\nh3_train, h3_val = h2_train.copy(), h2_val.copy()\n\n\nnan_rows = h3_train.loc[total_train['demented'].isna()]\nnon_nan_rows = h3_train.loc[~total_train['demented'].isna()]\n\ny, X = non_nan_rows['demented'], non_nan_rows.drop('demented', axis=1)\nX = StandardScaler(copy=True, with_mean=True, with_std=True).fit_transform(X)\n\nparam_grid = [ {   \n                'penalty': ['l1', 'l2'], \n                'solver': ['liblinear', 'saga'], \n                'fit_intercept': [True, False],\n                'C': np.logspace(0, 4, 20)\n             } ]\nbest_tree = GridSearchCV(LogisticRegression(), param_grid, cv=5)\nh3_train.loc[nan_rows.index, 'demented'] = best_tree.fit(X, y).predict(nan_rows.drop('demented', axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method 4: Imputing by decision tree\n\nDecision trees can handle both continuous and categorical data, so the target will not be a problem this time."},{"metadata":{"trusted":true},"cell_type":"code","source":"h4_train, h4_val = h2_train.copy(), h2_val.copy()\n\n\nparam_grid = [ {\n                'max_depth': [2,3,4,5,6,7,8,9,10,11,12], \n                'criterion': ['entropy', 'gini'],\n                'splitter': ['best', 'random'],\n             } ]\nbest_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\nh4_train.loc[nan_rows.index, 'demented'] = best_tree.fit(X, y).predict(nan_rows.drop('demented', axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method 5: Dropping the NaN rows \n\nNote that they are dropped from training set, but not from the validation set. Method 1 will instead be used for the latter. The reasons for this are the following:\n1. The validation set is already distressingly small; do not dare make the validation precision even poorer.\n2. There is no guarantee that there would be no missing data in real applications of the eventual model."},{"metadata":{"trusted":true},"cell_type":"code","source":"h5_train = total_train.dropna()\nh5_val = h1_val.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"### Logreg"},{"metadata":{},"cell_type":"markdown","source":"Separating the features from the targets, and scaling them. [Here](https://stats.stackexchange.com/a/112152/49044) you can view a great explanation for the need for scaling (and the vanity of scaling the target)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Xy(train, val):\n    y_train, y_val = train['demented'], val['demented']\n    X_train, X_val = train.drop('demented', axis=1), val.drop('demented', axis=1)\n    \n    scaler = StandardScaler(copy=True, with_mean=True, with_std=True).fit(train.drop('demented', axis=1))\n    return y_train, y_val, scaler.transform(X_train), scaler.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will use logistic regression, the most natural model for binary classification. And I will wrap it in a bag, i.e., use BaggingClassifier with an ensemble of LogisticRegressions. Bagging is the splitting of the training data into folds, and training of a separate model for each. A prediction is then made by predicting with each of these models, and averaging their results. This is a great way to raise regularization: the averaging lessens the effect of outliers, as only one of the folds will include each outlier (if the folds are sampled without replacement, as I do below).\n\nNow, which scoring function is best in this case? Certainly not accuracy, even though this dataset is not unbalanced (as the 0/1 ratio is about 40/60). Accuracy should never be used in data science, [as it is an improper scoring rule](https://stats.stackexchange.com/a/312787/49044) (in fact, it is not a scoring rule at all), discontinuous at the threshold. Of the X widely used evaluation metrics for binary classification ([here](https://neptune.ml/blog/evaluation-metrics-binary-classification) is a great summary) there are two candidates for this project:\n\n1. Logloss: the natural function for logistic regression.\n2. Recall (i.e., true positive rate): how many of the true positives are classified as positive. This is calculated by\n\n![image.png](attachment:image.png)\n\nwhere *tp* is the amount of true positives and *fn* is the amount of false negatives. In clinical contexts, the metrics of the confusion matrix, including recall, are of primary importance ([but have to be handled with care due to cognitive bias](https://www.fharrell.com/post/mlconfusion/)) because of the high risk associated with erroneous decisions. This project's aim is to create a dementia detection model, to be integrated into any digital healthcare platform. Although dementia (or more specifically, its most common cause, Alzheimer's) is near the low end of the scale of disease severity, it is nevertheless critical to get potentially affected people into treatment as soon as possible, given the great decline in quality of life (for the person him/herself as well as for family members). As such, it would be better to erroneously send someone on a clinical visit than to fail to notice a budding dementia. I therefore choose the Recall as the metric, rather than the elegant logloss.","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{"trusted":true},"cell_type":"code","source":"def logreg(X, y, bag=True):\n    \n    # Create a hyperparameter space. There are two dictionaries because \n    # newton-cg, lbfgs and sag cannot use l1.\n    param_grid = [\n                      {\n                           'penalty': ['l1', 'l2'], \n                           'solver': ['liblinear', 'saga'], \n                           'fit_intercept': [True, False],\n                           'C': np.logspace(0, 4, 20)\n                    },{\n                           \n                           'penalty': ['l2'], \n                           'solver': ['newton-cg', 'lbfgs', 'sag'], \n                           'fit_intercept': [True, False],\n                           'C': np.logspace(0, 4, 20)\n                      },\n                 ]\n    \n    # Adding a prefix to the dictionary keys if bagging is used,\n    # as BaggingClassifier needs it to distinguish its base estimator's\n    # parameters from its own.\n    if bag: param_grid = [\n                            {'base_estimator__' + k: v for k, v in param_grid[0].items()},\n                            {'base_estimator__' + k: v for k, v in param_grid[1].items()}\n                         ]\n    \n    # The two alternative models.\n    lr = LogisticRegression()\n    bc = BaggingClassifier(base_estimator=lr)\n\n    # Use stratified instead of shuffle split so that each split would contain a sufficient amount of each category.\n    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n    \n    # Conduct the search.\n    best_model = GridSearchCV(estimator = (bc if bag else lr), \n                              param_grid = param_grid, \n                              cv = cv, \n                              scoring = 'recall',#'neg_log_loss', \n                              n_jobs = -1\n                             ).fit(X,y)\n    \n    # Outputing the optimal point in the hyperparameter space.\n    keys = param_grid[0].keys()\n    grid_optimum = [best_model.best_estimator_.get_params()[p] for p in keys]\n    for p, x in zip(keys, grid_optimum):\n        print('Best {}: {}'.format(p,x))\n\n    return best_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validating the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputations = [(h1_train, h1_val), (h2_train, h2_val), (h3_train, h3_val), (h4_train, h4_val), (h5_train, h5_val)]\n\ndef validate_logreg(bag):\n    for i, (train, val) in enumerate(imputations):\n        print(\"Handling method {}:\".format(i+1))\n        y_train, y_val, X_train, X_val = Xy(train, val)\n        model = logreg(X_train, y_train, bag=bag)\n        pred = model.predict(X_val)\n        print(classification_report(y_val, pred))\n        \nvalidate_logreg(bag=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alas, looking at the Recall scores (the averages between the 0 and 1 values), none of the handling methods could beat the most popular one, imputation by most frequent value. Note also that for some reason the scores of the two worst ones, the logistic regression and decision tree (methods 3 and 4) are equivalent.\n\nThe Recall of 0.9 for negatuves (that is, not being demented) is decently high. However, our primary aim, the maximization of Recall for positives, has not succeeded, its being only 0.74. Lets see whether it can be raised by bagging."},{"metadata":{},"cell_type":"markdown","source":"### Bagged logreg"},{"metadata":{"trusted":true},"cell_type":"code","source":"validate_logreg(bag=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, the only thing that bagging achieved was the slight lowering of Recall for method 3. It may well be that this dataset is  too small for bagging to be beneficial."},{"metadata":{},"cell_type":"markdown","source":"### Boosted logreg\n\nDue to AdaBoost's excessive time, I use only two splits in the grid search, and restricted some other parameters too."},{"metadata":{"trusted":true},"cell_type":"code","source":"def boosted_logreg(X, y):\n    \n    # Create a hyperparameter space. There are two dictionaries because \n    # newton-cg, lbfgs and sag cannot use l1.\n    \n    # The base estimator's subspace of the parameter space.\n    logreg_param_grid = [\n                              {\n                                   'penalty': ['l1', 'l2'], \n                                   'solver': ['liblinear', 'saga'], \n                                   'fit_intercept': [True, False],\n                                   'C': np.logspace(0, 4, 10)\n                            },{\n\n                                   'penalty': ['l2'], \n                                   'solver': ['newton-cg', 'lbfgs', 'sag'], \n                                   'fit_intercept': [True, False],\n                                   'C': np.logspace(0, 4, 10)\n                              },\n                        ]\n    \n    # The booster's subspace of the parameter space.\n    adaboost_param_grid = { \n                              'learning_rate': [0.1, 0.5, 1],\n                              'n_estimators': np.arange(100, 200, 15)\n                          }\n    \n    \n    # Add a prefix to the logreg_param_grid keys,\n    # as AdaBoostClassifier needs it to distinguish its base estimator's\n    # parameters from its own.\n    param_grid = [\n                    {'base_estimator__' + k: v for k, v in logreg_param_grid[0].items()},#.update(adaboost_param_grid),\n                    {'base_estimator__' + k: v for k, v in logreg_param_grid[1].items()}#.update(adaboost_param_grid)\n                 ]\n    \n    # Combine the above two subspaces. Cannot use list comprehension with update() as it modifies in place.\n    param_grid[0].update(adaboost_param_grid), param_grid[1].update(adaboost_param_grid)\n    \n    # The model.\n    ab = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=LogisticRegression())\n                            #learning_rate=1.0, \n                            #n_estimators=50,)\n                            #random_state=None),\n\n    # Use stratified instead of shuffle split so that each split would contain a sufficient amount of each category.\n    cv = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n    \n    # Conduct the search.\n    best_model = GridSearchCV(estimator = ab, \n                              param_grid = param_grid, \n                              cv = cv, \n                              scoring = 'recall',#'neg_log_loss', \n                              n_jobs = -1,\n                              verbose = -1,\n                             ).fit(X,y)\n    \n    # Outputing the optimal point in the hyperparameter space.\n    keys = param_grid[0].keys()\n    grid_optimum = [best_model.best_estimator_.get_params()[p] for p in keys]\n    for p, x in zip(keys, grid_optimum):\n        print('Best {}: {}'.format(p,x))\n\n    return best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputations = [(h1_train, h1_val), (h2_train, h2_val), (h3_train, h3_val), (h4_train, h4_val), (h5_train, h5_val)]\n\ndef validate_boosted():\n    for i, (train, val) in enumerate(imputations):\n        print(\"Handling method {}:\".format(i+1))\n        y_train, y_val, X_train, X_val = Xy(train, val)\n        model = boosted_logreg(X_train, y_train)\n        pred = model.predict(X_val)\n        print(classification_report(y_val, pred))\n\nvalidate_boosted()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Alas, AdaBoostClassifier is no better than BaggingClassifier. I hypothesize the reason to be the dataset's smallness, so that no model can get a good grasp on the reality of the problem space in order to push down the bias to a decent level.\n\nIt is strange that for Method 1, the recall and precision of the negative results (i.e., 0) is 0. For some reason the model ascribes positive to all the people. It may well be that since Method 1 imputes only 0s (as the non-demented are more numerous in the data), the dataset becomes too unbalanced for AdaBoost to handle. This should be investigated."},{"metadata":{},"cell_type":"markdown","source":"### TODO: try Decision Tree instead"},{"metadata":{},"cell_type":"markdown","source":"# Process insights\n\n* Simply imputing the mean of most frequent value is superior to replacing categorical data with some fancy function.\n* Bagging and boosting offer no benefit with small datasets (a few hundred elements).\n* Beware: it seems that for unbalanced data, AdaBoost tends to maximize recall (and other metrics too?) in a [non-continuous way](https://stats.stackexchange.com/a/312787/49044)."},{"metadata":{},"cell_type":"markdown","source":"Note that as these are derived only from this project, they may be biased. I --- and you too if the above does not correlate with your experience, whether negatively or positively --- need to work on many more projects in order to regularize these mental models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nfrom IPython.display import display\n\ncellNum=2\ncellDisp='none'  # Other option is 'block'\ncell=\"\"\"\n<script>\n   var divTag = document.getElementsByClassName(\"input\")[%s]\n   var displaySetting = divTag.style.display;\n   // Default display - set to 'none'.  To hide, set to 'block'.\n   // divTag.style.display = 'block';\n   divTag.style.display = '%s';\n<script>\n<!-- <button onclick=\"javascript:toggleInput(%s)\" class=\"button\">Toggle Code</button> -->\n\"\"\" % (cellNum,'none',cellNum)\nh=HTML(cell)\ndisplay(h)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}