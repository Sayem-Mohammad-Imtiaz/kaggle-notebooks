{"cells":[{"metadata":{},"cell_type":"markdown","source":"*  **Goal:** Getting the diameter of an asteroid from the other data given about that asteroid, in other words supervised regression with the target being $\\log(diameter)$.   \nThe metric used during testing will be the $R^2$ score.\n* **How:**\n    * Step 1: Cleaning and preparing the data\n        * a. I cleared the samples with nan diameter.\n        * b. I dropped the features with more than half nan values.\n        * c. The dataframe had a lot of nan values, I chose to substitute them with the average value for the corresponding feature.\n    * Step 2: Train-test splitting of the data, and then normalizing (standard) using the test dataframe. \n    * Step 3: Trying different regression algorithms (Linear Regression, Elastic Net, Decision Tree, Random Forest, XGBoost, SVM, Neural Network) and picking the best one.\n* **Extra:**  Since NASA's own estimator uses $H$ and $albedo$ to calculate the diameter, I decided to drop those to add difficulty and have less linearity in problem to solve.\n\n\n**Dependencies: numpy, pandas, matplotlib, seaborn, scikit-learn, xgboost, keras.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom random import seed \nseed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part I: Importing, exploration & cleaning of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing only the first 30000 rows\ndf = pd.read_csv('/kaggle/input/prediction-of-asteroid-diameter/Asteroid.csv',nrows = 30000)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are lots of feature columns to check:**   \nFirst I wanted to know if there are NaN values (there are and will be dealt with later)"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Checking which columns(features) have nan values\nfor column in df.columns:\n    print(column, df[column].isna().sum()/df.shape[0]) #returns the fraction of NAN values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next thing is to understand what type of data we're dealing with"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Printing the first ten unique values of each feature\nfor column in df.columns:\n    print(column, df[column].unique()[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Cleaning and prepping the dataframe:**  \n**Steps:**\n    * 0/ 'diameter' is string type, I will convert to numeric. This gave errors for some diameters because they were corrupted, so I added the argument \"errors='coerce'\" to set corrupted diameters to nan, and later dropped those.\n    * 1/ Dropping irrelevent features and choosing my battles:\n        * 1a/ dropping names because I dont believe asteroids are named according to their diameter.\n        * 1b/ Dropping all features with more than half nan values\n        * 1c/ dropping condition_code and neo and pha because most seems to be 0 or nan.\n    * 2/ Replace nans entries with mean value of column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Steps 0\ndf['diameter']=pd.to_numeric(df['diameter'],errors='coerce') #transforming to numeric, setting errors to NaN\ndropindexes = df['diameter'][df['diameter'].isnull()].index #rows with nan diameters to drop\ndropped_df = df.loc[dropindexes] #saving dropped rows for the future\ndf = df.drop(dropindexes, axis=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Steps 1\ntooMuchNa = df.columns[df.isna().sum()/df.shape[0] > 0.5]\ndf = df.drop(tooMuchNa,axis=1)\ndf = df.drop(['condition_code','full_name'],axis=1)\ndf = df.drop(['neo','pha'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 2\ndf = df.fillna(df.mean())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Last sanity check for nan values\ndf.isna().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nasa's own Asteroid Size Estimator website uses the $H$ value and $\\log(albedo)$: https://cneos.jpl.nasa.gov/tools/ast_size_est.html so I decided to drop those features for curiosity and added difficulty (regression scores went down from 0.98 to 0.8)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['albedo','H'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since a lot of values in physics are more relevant when you consider their log, I'll add columns to the dataframe corresponding to the log of the original columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diameter']= df['diameter'].apply(np.log)\nfor column in df.columns.drop(['diameter']):\n    df['log('+column+')']=df[column].apply(np.log)\ndf = df.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['diameter'].abs().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part II: Splitting the dataframe into train and test dataframes and normalizing them for our regressions."},{"metadata":{},"cell_type":"markdown","source":"Splitting:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\npredictors = df.drop('diameter',axis=1) \ntarget = df['diameter']\nX_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\n#Input standard normalization:\nstd_scaler = preprocessing.StandardScaler().fit(X_train)\n\ndef scaler(X):\n    x_norm_arr= std_scaler.fit_transform(X)\n    return pd.DataFrame(x_norm_arr, columns=X.columns, index = X.index)\n\nX_train_norm = scaler(X_train)\nX_test_norm = scaler(X_test)\n\ndef inverse_scaler(X):\n    x_norm_arr= std_scaler.inverse_transform(X)\n    return pd.DataFrame(x_norm_arr, columns=X.columns, index = X.index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part III:  Trying different regressions and ranking them according to their $R^2$ score.\n**Algorithms used:** Linear Regression, Elastic Net, k-Nearest Neighbours, Decision Tree, Random Forest, SVM, Neural Network and XGBoost. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nimport seaborn as sns\n\ndef plot(prediction):\n    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,7)) \n    sns.distplot(Y_test.values,label='test values', ax=ax1)\n    sns.distplot(prediction ,label='prediction', ax=ax1)\n    ax1.set_xlabel('Distribution plot')\n    ax2.scatter(Y_test,prediction, c='orange',label='predictions')\n    ax2.plot(Y_test,Y_test,c='blue',label='y=x')\n    ax2.set_xlabel('test value')\n    ax2.set_ylabel('estimated $\\log(radius)$')\n    ax1.legend()\n    ax2.legend()\n    ax2.axis('scaled') #same x y scale\ndef score(prediction):\n    score = r2_score(prediction,Y_test)\n    return score\ndef announce(score):\n    print('The R^2 score achieved using this regression is:', round(score,3))\nalgorithms = []\nscores = []\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n###Training\nlr.fit(X_train,Y_train)\n\n###Predicting\nY_pred_lr = lr.predict(X_test)\n\n###Scoring\nscore_lr = score(Y_pred_lr)\nannounce(score_lr)\n\nalgorithms.append('LR')\nscores.append(score_lr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elastic Net regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Defining the Model\nfrom sklearn.linear_model import ElasticNetCV\nenet = ElasticNetCV(cv=9,max_iter=10000)\n\n### Training\nenet.fit(X_train_norm,np.ravel(Y_train))\n\n### Predicting\nY_pred_enet = enet.predict(X_test_norm)\n\n###Scoring\nscore_enet = score(Y_pred_enet)\nannounce(score_enet)\n\nalgorithms.append('eNet')\nscores.append(score_enet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_enet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k-Nearest Neighbours regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Defining the Model\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n##For weighted metric, more accurate but longer calculation\n#weights = X_train_norm.corrwith(Y_train).abs()\n#neigh = KNeighborsRegressor(n_neighbors=3, metric_params={'w' : weights.values}, metric='wminkowski')\n\nneigh = KNeighborsRegressor(n_neighbors=3)\n\n### Training\nneigh.fit(X_train_norm,Y_train)\n\n### Predicting \nY_pred_neigh = neigh.predict(X_test_norm)\n\n### Scoring\nscore_neigh=score(Y_pred_neigh)\nannounce(score_neigh)\n\nalgorithms.append('k-NN')\nscores.append(score_neigh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_neigh)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Defining the model\nfrom sklearn import tree\ndecTree = tree.DecisionTreeRegressor()\n\n### Training\ndecTree = decTree.fit(X_train_norm,Y_train)\n\n### Predicting\nY_pred_tree = decTree.predict(X_test_norm)\n\n### Scoring\nscore_tree = score(Y_pred_tree)\nannounce(score_tree)\n\nalgorithms.append('DTree')\nscores.append(score_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Defining the model\nfrom sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(max_depth=32, n_estimators=50)\n\n### Training \nforest.fit(X_train_norm,np.ravel(Y_train))\n\n###Predicting\nY_pred_forest = forest.predict(X_test_norm)\n\n### Scoring\nscore_forest = score(Y_pred_forest)\nannounce(score_forest)\n\nalgorithms.append('RForest')\nscores.append(score_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_forest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine regression:\n(too slow when n_samples > 30000)"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Defining the model\nfrom sklearn import svm\nsvmreg = svm.SVR()\n\n### Training\nsvmreg.fit(X_train_norm,np.ravel(Y_train))\n\n### Predicting\nY_pred_svm = svmreg.predict(X_test_norm)\n\n### Scoring\nscore_svm = score(Y_pred_svm)\nannounce(score_svm)\n\nalgorithms.append('SVM')\nscores.append(score_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_svm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural Network regression:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"### Defining the model\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\n\nAdam(learning_rate=0.005)\nmodel = Sequential()\nmodel.add(Dense(24,activation='tanh',input_dim=X_train_norm.shape[1]))\nmodel.add(Dense(12,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error',optimizer='adam')\n\n### Training\n\nmodel.fit(X_train_norm,Y_train,epochs=100,batch_size=256,verbose=False)\n\n### Predicting\n\nY_pred_nn = model.predict(X_test_norm)\n\n### Scoring\nscore_nn = score(Y_pred_nn)\nannounce(score_nn)\n\nalgorithms.append('NNet')\nscores.append(score_nn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(Y_pred_nn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost regression:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"### Defining the model\nimport xgboost as xgb \nxgReg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, \n                         learning_rate = 0.08 ,\n                max_depth = 4, n_estimators = 500)\n\n### Training\nxgReg.fit(X_train_norm,Y_train)\n\n### Predicting\nY_pred_xgb = xgReg.predict(X_test_norm)\n\n### Scoring\nscore_xgb = score(Y_pred_xgb)\nannounce(score_xgb)\n\nalgorithms.append('XGB')\nscores.append(score_xgb)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plot(Y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# One bonus of using xgboost is being able to \n# simply see how important the different features where when creating the learners.\n\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgReg, height=0.5, ax=ax, importance_type='weight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing all regression algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(algorithms,scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoosting wins today with an $R^2$ score of 0.845"},{"metadata":{},"cell_type":"markdown","source":"Thank you for reading !"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}