{"cells":[{"metadata":{"_cell_guid":"5e74e227-04c8-4995-8785-a132143f2e4a","_uuid":"16eb01d9110fc7e1cd65e5fbcc334feb8b884099"},"cell_type":"markdown","source":"**Predicting Breast Cancer From Nuclear Shape**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"370cf8b4-e66e-4b81-865c-71e861006684","_uuid":"788f07f2d13cfcdf85f65ff013719d7b8bf52735","collapsed":true},"cell_type":"markdown","source":"**The nucleus is an organelle present within all eukaryotic cells, including human cells.  Abberant nuclear shape can be used to identify cancer cells (e.g. pap smear tests and the diagnosis of cervical cancer). Likewise, a growing body of literature suggests that there is some connection between the shape of the nucleus and human disease states such as cancer and aging. As such, the quantitative analysis of nuclear size of shape has important biomedical applications.\n**\nFor more information, please refer to the following resources:\n* http://www.uwyo.edu/levy_lab/\n* Vukovic LD, Jevtic P, Edens LJ, Levy DL. (2016) New Insights into Mechanisms and Functions of Nuclear Size Regulation. Int Rev Cell Mol Biol. 322:1–59.\n* Webster, M., Witkin, K.L., and Cohen-Fix, O. (2009). Sizing up the nucleus: nuclear shape, size and nuclear-envelope assembly. J. Cell Sci. 122, 1477–1486.\n* Zink, D., Fischer, A.H., and Nickerson, J.A. (2004). Nuclear structure in cancer cells. Nat. Rev. Cancer 4, 677–687.\n\n**This script takes as an input the CSV file from the Kaggle Breast Cancer Wisconsin Dataset (https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) containing information on various features describing the size and shape of the nucleus.  The measurements were made from digital images of a fine needle aspirate of a breast tissue mass.  The output of this script is a prediction of whether or not a given sample is cancerous.**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"01f6fae0-4d15-41d4-86d1-907781bd435a","_uuid":"d31d049dcaf7b87894c55ab288744e208ffdf722"},"cell_type":"markdown","source":"*Step 1: Import Modules*","outputs":[],"execution_count":null},{"metadata":{"_kg_hide-input":true,"_cell_guid":"f0de23a8-00bf-4d28-a66b-a65ecf84dce8","_uuid":"80eee6a772fb0503efa1b9d9cfa8d40eaa22227a","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport itertools\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\n#from sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.metrics import confusion_matrix\n%matplotlib inline\n# os.chdir('/Users/ptm/desktop/Current_working_directory')\n# trainingData = pd.read_csv('train.csv')\ntrainingData = pd.read_csv('../input/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"090a7f29-c5c4-466c-b43a-6ac9e0529cea","_uuid":"462720090de8608733c73a11c7c4b4b68fe5fcab"},"cell_type":"markdown","source":"*Step 2: Describe Data*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b718e39c-2e9b-4303-bfaf-c801f85f07f5","_uuid":"f00cf2bea29e07f2adbed2a8532652af8ccf59fe","trusted":true},"cell_type":"code","source":"def printColumnTitles(input):\n    print('\\nColumn Values:\\n')\n    print(input.columns.values)\n    print('')\nprintColumnTitles(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"749186d3-6976-48e1-897a-5b111a9dbb9f","_uuid":"3bd578c1edc98937f100dfa67fa944aca9a9f595"},"cell_type":"markdown","source":"*Step 3: Minimze and Describe Dataset*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6d4b143f-987d-4ea9-8485-e53a54ba35ea","_uuid":"9cbfec4b9423a9f2f653e1cb277036383a6d4810","trusted":true},"cell_type":"code","source":"trainingData = trainingData.drop(['id', 'radius_mean', 'perimeter_mean',\n 'compactness_mean', 'fractal_dimension_mean', 'radius_se',\n 'texture_se', 'perimeter_se', 'smoothness_se', 'compactness_se',\n 'concavity_se', 'concave points_se', 'fractal_dimension_se',\n 'radius_worst', 'texture_worst', 'perimeter_worst',\n 'smoothness_worst', 'compactness_worst', 'concavity_worst',\n 'concave points_worst', 'fractal_dimension_worst', 'Unnamed: 32', 'area_se', 'symmetry_se',\n 'area_worst', 'symmetry_worst', 'concavity_mean'], axis=1)\n\ndef describeDataAgain(input):\n    print('\\nNew summary of data after making changes:\\n')\n    print('Column Values:\\n')\n    print(input.columns.values)\n    print('\\nFirst Few Values:]n')\n    print(input.head())\n    print('\\nNull Value Counts:\\n')\n    print(input.isnull().sum())\ndescribeDataAgain(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bb1d4ce-c5f0-4c17-af6c-8875352ffeb6","_uuid":"ae4f34d90b46574ef41b9947a305788b46d365c3"},"cell_type":"markdown","source":"*Step 4: Plot Data*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"211150b0-ddaa-4e89-a17c-5100ba112d5f","_uuid":"bd640714c395363b15ca1c47a9839872a83f9f08"},"cell_type":"markdown","source":"I predict that the nuclei from the malignant samples were larger than the nuclei from the benign samples.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"50cca690-5adf-42e5-82ef-b92f180930ba","_uuid":"813fc9fc54bed0490da7bf7e7ca7cf39c5f78fe2","trusted":true},"cell_type":"code","source":"def plotSizeDistribution(input):\n    \"\"\" \n    The output is three graphs that each illustrate the two distributions of nuclear sizes for samples\n    that are either malignant or benign.\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionOne.map(plt.hist, 'area_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('area_mean', 'Count')\n    distributionOne.fig.suptitle('Area vs Diagnosis ((Blue = Malignant; Green = Benign)')\n    distributionTwo = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionTwo.map(sns.kdeplot,'area_mean',shade= True)\n    distributionTwo.set(xlim=(0, input['area_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('area_mean', 'Proportion')\n    distributionTwo.fig.suptitle('Area vs Diagnosis (Blue = Malignant; Green = Benign)')\nplotSizeDistribution(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53ff0898-1489-4462-8653-f753dd312a1a","_uuid":"ad74b15b2d614e539d6aceaff7501d5a206aa33a"},"cell_type":"markdown","source":"This confirms my prediction that healthy nuclei have a default size and that cancer cells have a wide range of sizes, typically greater than the default size.  Let's look at all of the features now.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"302d2bca-ef7b-4336-ad3b-bb9df5c5eb3e","_uuid":"ea100df3aa2b9bb9be3b066915023a664db2e1d3","trusted":true},"cell_type":"code","source":"def plotConcaveDistribution(input):\n    \"\"\" \n    The output is three graphs that each illustrate the two distributions of nuclear shapes for samples\n    that are either malignant or benign.\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionOne.map(plt.hist, 'concave points_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('concave points_mean', 'Count')\n    distributionOne.fig.suptitle('# of Concave Points vs Diagnosis (Blue = Malignant; Green = Benign)')\n    distributionTwo = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionTwo.map(sns.kdeplot,'concave points_mean',shade= True)\n    distributionTwo.set(xlim=(0, input['concave points_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('concave points_mean', 'Proportion')\n    distributionTwo.fig.suptitle('# of Concave Points vs Diagnosis (Blue = Malignant; Green = Benign)')\nplotConcaveDistribution(trainingData)\n\ndef plotSymmetryDistribution(input):\n    \"\"\" \n    The output is three graphs that each illustrate the two distributions of nuclear shapes for samples\n    that are either malignant or benign.\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionOne.map(plt.hist, 'symmetry_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('symmetry_mean', 'Count')\n    distributionOne.fig.suptitle('Symmetry vs Diagnosis (Blue = Malignant; Green = Benign)')\n    distributionTwo = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionTwo.map(sns.kdeplot,'symmetry_mean',shade= True)\n    distributionTwo.set(xlim=(0, input['symmetry_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('symmetry_mean', 'Proportion')\n    distributionTwo.fig.suptitle('Symmetry vs Diagnosis (Blue = Malignant; Green = Benign)')\nplotSymmetryDistribution(trainingData)\n\ndef plotTextureDistribution(input):\n    \"\"\" \n    The output is three graphs that each illustrate the two distributions of nuclear shapes for samples\n    that are either malignant or benign.\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionOne.map(plt.hist, 'texture_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('texture_mean', 'Count')\n    distributionOne.fig.suptitle('Texture vs Diagnosis (Blue = Benign; Green = Malignant)')\n    distributionTwo = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionTwo.map(sns.kdeplot,'texture_mean',shade= True)\n    distributionTwo.set(xlim=(0, input['texture_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('texture_mean', 'Proportion')\n    distributionTwo.fig.suptitle('Texture vs Diagnosis (Blue = Benign; Green = Malignant)')\nplotTextureDistribution(trainingData)\n\ndef plotSmoothnessDistribution(input):\n    \"\"\" \n    The output is three graphs that each illustrate the two distributions of nuclear shapes for samples\n    that are either malignant or benign.\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionOne.map(plt.hist, 'smoothness_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('smoothness_mean', 'Count')\n    distributionOne.fig.suptitle('Smoothness vs Diagnosis (Blue = Benign; Green = Malignant)')\n    distributionTwo = sns.FacetGrid(input, hue=\"diagnosis\",aspect=2)\n    distributionTwo.map(sns.kdeplot,'smoothness_mean',shade= True)\n    distributionTwo.set(xlim=(0, input['smoothness_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('smoothness_mean', 'Proportion')\n    distributionTwo.fig.suptitle('Smoothness vs Diagnosis (Blue = Benign; Green = Malignant)')\nplotSmoothnessDistribution(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b779ffa-1c61-4c0e-aacf-ff8c41f301eb","_uuid":"f102a4d0c52b943668cc4a5770dae38b675762e3"},"cell_type":"markdown","source":"*Step 5: Preprocess Data*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"401b8f6f-eda3-4e4d-a600-1030dd22d53d","_uuid":"58fb755aa7f196935e55ef80af422f202383c0f9","collapsed":true,"trusted":true},"cell_type":"code","source":"def diagnosisToBinary(input):\n    \"\"\" \n    The output is a modified dataframe where 0 = \"malignant\" and 1 = \"benign\".\n    \"\"\" \n    input[\"diagnosis\"] = input[\"diagnosis\"].astype(\"category\")\n    input[\"diagnosis\"].cat.categories = [0,1]\n    input[\"diagnosis\"] = input[\"diagnosis\"].astype(\"int\")\ndiagnosisToBinary(trainingData)     \n\ndef areaToCategory(input):\n    \"\"\" \n    The output is a modified dataframe where the area measurements are replaced with numbers between \n    zero and five based on their position within predetermined bins (based on the previous distribution plots).\n    \"\"\" \n    input['area_mean'] = input.area_mean.fillna(-0.5)\n    bins = (-0.01, 100, 750, 1250, 2000, 10000)\n    categories = pd.cut(input.area_mean, bins, labels=False)\n    input.area_mean = categories\nareaToCategory(trainingData)\n\ndef concaveToCategory(input):\n    \"\"\" \n    The output is a modified dataframe where the shape measurements are replaced with numbers between \n    zero and five based on their position within predetermined bins.\n    \"\"\" \n    # Get rid of the space in the file name\n    cols = trainingData.columns\n    cols = cols.map(lambda x: x.replace(' ', '_') if isinstance(x, (str, bytes)) else x)\n    trainingData.columns = cols\n    # Run the function\n    input['concave_points_mean'] = input.concave_points_mean.fillna(-0.5)\n    bins = (-0.01, 0.03, 0.06, 0.1, 1.0)\n    categories = pd.cut(input.concave_points_mean, bins, labels=False)\n    input.concave_points_mean = categories\nconcaveToCategory(trainingData)\n\ndef symmetryToCategory(input):\n    \"\"\" \n    The output is a modified dataframe where the shape measurements are replaced with numbers between \n    zero and five based on their position within predetermined bins.\n    \"\"\" \n    input['symmetry_mean'] = input.symmetry_mean.fillna(-0.5)\n    bins = (-0.01, 0.15, 0.17, 0.2, 1.0)\n    categories = pd.cut(input.symmetry_mean, bins, labels=False)\n    input.symmetry_mean = categories\nsymmetryToCategory(trainingData)\n\ndef textureToCategory(input):\n    \"\"\" \n    The output is a modified dataframe where the shape measurements are replaced with numbers between \n    zero and five based on their position within predetermined bins.\n    \"\"\" \n    input['texture_mean'] = input.texture_mean.fillna(-0.5)\n    bins = (-0.01, 10, 15, 19, 25, 100)\n    categories = pd.cut(input.texture_mean, bins, labels=False)\n    input.texture_mean = categories\ntextureToCategory(trainingData)\n\ndef smoothnessToCategory(input):\n    \"\"\" \n    The output is a modified dataframe where the shape measurements are replaced with numbers between \n    zero and five based on their position within predetermined bins.\n    \"\"\" \n    input['smoothness_mean'] = input.smoothness_mean.fillna(-0.5)\n    bins = (-0.01, 0.07, 0.09, 0.11, .13, 1)\n    categories = pd.cut(input.smoothness_mean, bins, labels=False)\n    input.smoothness_mean = categories\nsmoothnessToCategory(trainingData)\n# Furthermore, we will need to split up our training data, setting aside 20%\n# of the training data for cross-validation testing, such that we can avoid\n# potentially overfitting the data.\nxValues = trainingData.drop(['diagnosis'], axis=1)\nyValues = trainingData['diagnosis']\nX_train, X_test, Y_train, Y_test = train_test_split(xValues, yValues, test_size=0.2, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cee17fd8-8d16-4741-a542-c29b9d3fd9fe","_uuid":"1202c08867fb9815e42df268c4b7d4d65ae9ccad"},"cell_type":"markdown","source":"*Step 6: Describe Processed Data*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"141bced9-8eb1-4530-8d96-9cf167a276d9","_uuid":"c19e2c3f006f1a7347fd9492ddcf035df76e9b8e","trusted":true},"cell_type":"code","source":"describeDataAgain(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a036e1b-6ed2-4c51-9bf1-ff12a90cc0c1","_uuid":"2f3507fa14629e41e0c27e2c30c68c52040b2a11","trusted":true},"cell_type":"code","source":"def makeAHeatMap(input):\n    \"\"\" The output is a heatmap showing the relationship between each numerical feature\"\"\"  \n    plt.figure(figsize=[8,6])\n    heatmap = sns.heatmap(input.corr(), vmax=1.0, square=True, annot=True)\n    heatmap.set_title('Pearson Correlation Coefficients')\nmakeAHeatMap(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8b0259b-6ebc-4c56-83c1-711ddece3845","_uuid":"c9ff5860d3ab790c99a3d5bfbe86caad549b3408"},"cell_type":"markdown","source":"Here with this heatmap we can see that big, mis-shapen nuclei are typicaly from cancerous samples. Let's explore that in more detail.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"63dd9ac1-b5ec-4ffd-a02d-b9279050f653","_uuid":"e088233643d5bced29e0d66c45e80c9e157af53e","trusted":true},"cell_type":"code","source":"def pivotTheData(input):\n    \"\"\" The output is a couple of pivot tables showing the relationship between each feature.\"\"\"    \n    print('\\nPivot Tables:\\n')\n    print(input[[\"area_mean\", \"diagnosis\"]].groupby(['area_mean'], as_index=False).mean().sort_values(by='diagnosis', ascending=False))\n    print('')\n    print(input[[\"concave_points_mean\", \"diagnosis\"]].groupby(['concave_points_mean'], as_index=False).mean().sort_values(by='diagnosis', ascending=False))\n    print('')\n    print(input[['symmetry_mean', 'diagnosis']].groupby(['symmetry_mean'], as_index=False).mean().sort_values(by='diagnosis', ascending=False))\n    print('')\n    print(input[['texture_mean', 'diagnosis']].groupby(['texture_mean'], as_index=False).mean().sort_values(by='diagnosis', ascending=False))\n    print('')\n    print(input[['smoothness_mean', 'diagnosis']].groupby(['smoothness_mean'], as_index=False).mean().sort_values(by='diagnosis', ascending=False))\n    print('')\npivotTheData(trainingData)\n\ndef plotTheData(input):\n    \"\"\" The output is a bunch of bar graphs illustrating the relationships between features.\"\"\"  \n    fig = plt.figure(figsize=[10,8])\n    fig.subplots_adjust(wspace=0.3,hspace=2.0)\n    plt.subplot(321)\n    plotOne = sns.barplot('area_mean', 'diagnosis', data=input, capsize=.1, linewidth=2.5, facecolor=(1, 1, 1, 0), errcolor=\".2\", edgecolor=\".2\")\n    plotOne.set_title('Diagnosis vs Area')\n    plotOne.set(xlabel='Surface Area \\n (0 = smallest nuclei, 4 = largest nuclei)', ylabel='Probability of \\n Malignant Diagnosis')\n    plt.subplot(322)\n    plotTwo = sns.barplot('concave_points_mean', 'diagnosis', data=input, capsize=.1, linewidth=2.5, facecolor=(1, 1, 1, 0), errcolor=\".2\", edgecolor=\".2\")\n    plotTwo.set_title('Diagnosis vs # Concave Points')\n    plotTwo.set(xlabel='Number of Concave Points \\n (0 = least points, 3 = most points)', ylabel='Probability of \\n Malignant Diagnosis')\n    plt.subplot(323)\n    plotThree = sns.barplot('texture_mean', 'diagnosis', data=input, capsize=.1, linewidth=2.5, facecolor=(1, 1, 1, 0), errcolor=\".2\", edgecolor=\".2\")\n    plotThree.set_title('Diagnosis vs Texture')\n    plotThree.set(xlabel='Texture \\n (0 = low gray value stdev, 4 = high gray value stdev)', ylabel='Probability of \\n Malignant Diagnosis')\n    plt.subplot(324)\n    plotFour = sns.barplot('symmetry_mean', 'diagnosis', data=input, capsize=.1, linewidth=2.5, facecolor=(1, 1, 1, 0), errcolor=\".2\", edgecolor=\".2\")\n    plotFour.set_title('Diagnosis vs Symmetry') \n    plotFour.set(xlabel='Symmetry \\n (0 = low symmetry score, 3 = high symmetry score)', ylabel='Probability of \\n Malignant Diagnosis')\n    plt.subplot(325)\n    plotFive = sns.barplot('smoothness_mean', 'diagnosis', data=input, capsize=.1, linewidth=2.5, facecolor=(1, 1, 1, 0), errcolor=\".2\", edgecolor=\".2\")\n    plotFive.set_title('Diagnosis vs Smoothness') \n    plotFive.set(xlabel='Smoothness \\n (0 = low variation in radius lengths, 0 = high variation in radius lengths)', ylabel='Probability of \\n Malignant Diagnosis')\nplotTheData(trainingData)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5242c72-e1ab-4085-8554-e316c7055830","_uuid":"ea7dae20bd98fda14420d0c5da3dbbadaa538b13"},"cell_type":"markdown","source":"Great!  This means that our classification algorithms should have somethinggood to work with.  Next we will identify a suitable classification algorithm that we can use to predict whether or not a given sample is malignant.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8898dc3d-e821-4a92-bb34-22531c89880b","_uuid":"e072fd758fc6646a3fce9e4b2074c4076a73b4d6"},"cell_type":"markdown","source":"*Step 7: Compare Classification Algorithms*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c05e3594-76cb-43c2-aca0-a2db28bb2939","_uuid":"52d484a218dce19b4a0c57f11b9ef734c57a995b","trusted":true},"cell_type":"code","source":"def compareABunchOfDifferentModelsAccuracy(a, b, c, d):\n    \"\"\"\n    The output is a table and boxplot illustrating the accuracy score for each of nine algorithms given this input.\n    \"\"\"    \n    print('\\nCompare Multiple Classifiers:\\n')\n    print('K-Fold Cross-Validation Accuracy:\\n')\n    models = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('RF', RandomForestClassifier()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('SVM', SVC()))\n    models.append(('LSVM', LinearSVC()))\n    models.append(('GNB', GaussianNB()))\n    models.append(('DTC', DecisionTreeClassifier()))\n    models.append(('GBC', GradientBoostingClassifier()))\n    models.append(('LDA', LinearDiscriminantAnalysis()))\n    resultsAccuracy = []\n    names = []\n    for name, model in models:\n        model.fit(a, b)\n        kfold = model_selection.KFold(n_splits=10, random_state=7)\n        accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n        resultsAccuracy.append(accuracy_results)\n        names.append(name)\n        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n        print(accuracyMessage)\n    # boxplot algorithm comparison\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparison: Accuracy')\n    ax = fig.add_subplot(111)\n    plt.boxplot(resultsAccuracy)\n    ax.set_xticklabels(names)\n    ax.set_ylabel('Cross-Validation: Accuracy Score')\n    plt.show()\n    return\ncompareABunchOfDifferentModelsAccuracy(X_train, Y_train, X_test, Y_test)\n\ndef defineModels():\n    print('\\nLR = LogisticRegression')\n    print('RF = RandomForestClassifier')\n    print('KNN = KNeighborsClassifier')\n    print('SVM = Support Vector Machine SVC')\n    print('LSVM = LinearSVC')\n    print('GNB = GaussianNB')\n    print('DTC = DecisionTreeClassifier')\n    print('GBC = GradientBoostingClassifier')\n    print('LDA = LinearDiscriminantAnalysis\\n')\ndefineModels()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36bd7090-1921-4756-846d-bd12bd7a0edb","_uuid":"d0614452108305269c8323c2408c799f00fb3321","trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nplot_learning_curve(LogisticRegression(), 'Learning Curve For Logistic Regression Classifier', X_train, Y_train, (0.85,1), 10)\nplot_learning_curve(SVC(), 'Learning Curve For SVM Classifier', X_train, Y_train, (0.85,1), 10)\nplot_learning_curve(LinearDiscriminantAnalysis(), 'Learning Curve For LDA Classifier', X_train, Y_train, (0.85,1), 10)\nplot_learning_curve(KNeighborsClassifier(), 'Learning Curve For K-Nearest Neighbors Classifier', X_train, Y_train, (0.85,1), 10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c14343d-3bda-4e82-b19f-46cf68e9d813","_uuid":"6e4b1d040432a5ab84aea40344730c021ca0f8db","trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndict_characters = {0: 'Malignant', 1: 'Benign'}\n\ndef selectParametersForSVM(a, b, c, d):\n    model = SVC()\n    parameters = {'C': [0.01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100],\n                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n    accuracy_scorer = make_scorer(accuracy_score)\n    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)\n    grid_obj = grid_obj.fit(a, b)\n    model = grid_obj.best_estimator_\n    model.fit(a, b)\n    print('Selected Parameters for SVM:\\n')\n    print(model,\"\\n\")\n#    predictions = model.predict(c)\n#    print(accuracy_score(d, predictions))\n#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))\n    kfold = model_selection.KFold(n_splits=10, random_state=7)\n    accuracy = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n    mean = accuracy.mean() \n    stdev = accuracy.std()\n    print('Support Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev))\n    print('')\n    prediction = model.predict(c)\n    cnf_matrix = confusion_matrix(d, prediction)\n    np.set_printoptions(precision=2)\n    class_names = dict_characters \n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix')\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n    plot_learning_curve(model, 'Learning Curve For SVM Classifier', X_train, Y_train, (0.85,1), 10)\nselectParametersForSVM(X_train, Y_train, X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2cb4a15a-fc8c-4388-9e19-a69dcdaf1ff7","_uuid":"c895b7cda17a4e2b09edac611207c4485922e8c8","collapsed":true},"cell_type":"markdown","source":"It looks like our model can predict with about 90% accuracty whether or not a given sample is malignant.  That is pretty good!\nIn order to improve the accuracty of our model, however, we will need to add back some of the features that we previously removed, and we will need to engineer some new features.  Furthermore, I will need to add in a feature selection\nstep, and I will also need to add in a parameter optimization step.  These additions can be found in the following kernel: https://www.kaggle.com/paultimothymooney/predicting-breast-cancer-from-nuclear-shape","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}