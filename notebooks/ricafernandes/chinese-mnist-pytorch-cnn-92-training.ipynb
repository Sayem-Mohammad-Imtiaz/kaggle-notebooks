{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-14T20:37:31.756041Z","iopub.execute_input":"2021-07-14T20:37:31.756378Z","iopub.status.idle":"2021-07-14T20:37:31.766087Z","shell.execute_reply.started":"2021-07-14T20:37:31.756346Z","shell.execute_reply":"2021-07-14T20:37:31.764933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42 # \"Answer to the Ultimate Question of Life, the Universe, and Everything\"\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nimg_size = 64\n\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:31.767811Z","iopub.execute_input":"2021-07-14T20:37:31.768356Z","iopub.status.idle":"2021-07-14T20:37:31.778718Z","shell.execute_reply.started":"2021-07-14T20:37:31.768319Z","shell.execute_reply":"2021-07-14T20:37:31.777618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chin_mnist_df = pd.read_csv('../input/chinese-mnist/chinese_mnist.csv')\nchin_mnist_df.head(5)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-14T20:37:31.780914Z","iopub.execute_input":"2021-07-14T20:37:31.781607Z","iopub.status.idle":"2021-07-14T20:37:31.810789Z","shell.execute_reply.started":"2021-07-14T20:37:31.781564Z","shell.execute_reply":"2021-07-14T20:37:31.809733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chin_mnist_df['value'].nunique())\nprint(chin_mnist_df['value'].unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:31.812613Z","iopub.execute_input":"2021-07-14T20:37:31.812966Z","iopub.status.idle":"2021-07-14T20:37:31.822497Z","shell.execute_reply.started":"2021-07-14T20:37:31.81293Z","shell.execute_reply":"2021-07-14T20:37:31.820477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# file from csv: f\"input_{suiteid}_{sampleid}_{code}.jpg\"\nitem = chin_mnist_df.iloc[0, :]\n\nimg = Image.open(f\"../input/chinese-mnist/data/data/input_{item['suite_id']}_{item['sample_id']}_{item['code']}.jpg\")\nprint(type(img))\nimg = np.array(img) # convert to np.array\nprint(type(img))\nplt.imshow(img, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:31.824109Z","iopub.execute_input":"2021-07-14T20:37:31.824731Z","iopub.status.idle":"2021-07-14T20:37:31.983517Z","shell.execute_reply.started":"2021-07-14T20:37:31.824691Z","shell.execute_reply":"2021-07-14T20:37:31.982608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chin_mnist_df['character'].apply(lambda char: char)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:31.98613Z","iopub.execute_input":"2021-07-14T20:37:31.986587Z","iopub.status.idle":"2021-07-14T20:37:31.997563Z","shell.execute_reply.started":"2021-07-14T20:37:31.986542Z","shell.execute_reply":"2021-07-14T20:37:31.996702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(np.sort(chin_mnist_df['value'].unique()).astype(np.str), chin_mnist_df['character'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.000739Z","iopub.execute_input":"2021-07-14T20:37:32.001182Z","iopub.status.idle":"2021-07-14T20:37:32.165365Z","shell.execute_reply.started":"2021-07-14T20:37:32.001138Z","shell.execute_reply":"2021-07-14T20:37:32.164479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1000 samples for each class, so we will have 700 train, 200 valid and 100 test samples from each class","metadata":{}},{"cell_type":"code","source":"# get sample for each class\n# sample is already without replacement\n#chin_mnist_df.groupby('value').apply(lambda x: x.sample(1))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.166951Z","iopub.execute_input":"2021-07-14T20:37:32.167321Z","iopub.status.idle":"2021-07-14T20:37:32.172088Z","shell.execute_reply.started":"2021-07-14T20:37:32.167281Z","shell.execute_reply":"2021-07-14T20:37:32.170645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sampling keeps track of sampled indices even when called in different scopes","metadata":{}},{"cell_type":"code","source":"#testing_df = chin_mnist_df.groupby('value').apply(lambda x: x.sample(3)).reset_index(drop=True)\n\n#testing_df_1 = testing_df.groupby('value').apply(lambda x: x.sample(1))\n#testing_df_2 = testing_df.groupby('value').apply(lambda x: x.sample(2))\n\n#testing_df","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.173709Z","iopub.execute_input":"2021-07-14T20:37:32.17408Z","iopub.status.idle":"2021-07-14T20:37:32.180999Z","shell.execute_reply.started":"2021-07-14T20:37:32.174035Z","shell.execute_reply":"2021-07-14T20:37:32.179999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing_df_1","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.182459Z","iopub.execute_input":"2021-07-14T20:37:32.182911Z","iopub.status.idle":"2021-07-14T20:37:32.190009Z","shell.execute_reply.started":"2021-07-14T20:37:32.182874Z","shell.execute_reply":"2021-07-14T20:37:32.18913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing_df_2","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.190962Z","iopub.execute_input":"2021-07-14T20:37:32.191243Z","iopub.status.idle":"2021-07-14T20:37:32.199212Z","shell.execute_reply.started":"2021-07-14T20:37:32.191219Z","shell.execute_reply":"2021-07-14T20:37:32.198462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, validation and testing sets and loaders","metadata":{}},{"cell_type":"code","source":"labels = np.sort(chin_mnist_df['value'].\\\nunique())\nlabels","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.200923Z","iopub.execute_input":"2021-07-14T20:37:32.201391Z","iopub.status.idle":"2021-07-14T20:37:32.210668Z","shell.execute_reply.started":"2021-07-14T20:37:32.201315Z","shell.execute_reply":"2021-07-14T20:37:32.209421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.n_classes = y.nunique()\n        self.labels    = np.sort(y.unique())\n                \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, index):\n        img_path_info = self.x.iloc[index, :]\n        img = Image.open(f\"../input/chinese-mnist/data/data/input_{self.x.iloc[index, 0]}_{self.x.iloc[index, 1]}_{self.x.iloc[index, 2]}.jpg\")\n        img = np.array(img) # convert to np.array\n        \n        label = self.y.iloc[index]\n        label_index   = np.where(self.labels == label)[0][0]\n        \n        return img, label_index # returns features (image) and target index in self.labels, which corresponds to the target softmax index in the model","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.212322Z","iopub.execute_input":"2021-07-14T20:37:32.212757Z","iopub.status.idle":"2021-07-14T20:37:32.223156Z","shell.execute_reply.started":"2021-07-14T20:37:32.212718Z","shell.execute_reply":"2021-07-14T20:37:32.222293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train (70%), valid (20%) and test (hold-out) (10%) splits\n\n# Not securing samples of each class\n#train_df = chin_mnist_df.sample(frac=0.7, random_state=SEED)\n#chin_mnist_df.drop(train_df.index)\n#valid_df = chin_mnist_df.sample(frac=0.7, random_state=SEED) # 66% (ceiled to 70%) of the remaining 30% from original\n#chin_mnist_df.drop(valid_df.index)\n#test_df = chin_mnist_df\n#chin_mnist_df.drop(test_df.index)\n\n# Securing equal number of samples from each class (sample bootstraps without replacement by default)\ntrain_df = chin_mnist_df.groupby('value').apply(lambda x: x.sample(700, random_state=SEED)).reset_index(drop=True)\nx_train, y_train  = train_df.iloc[:, :-2], train_df.iloc[:, -2]\n\nvalid_df = chin_mnist_df.groupby('value').apply(lambda x: x.sample(200, random_state=SEED)).reset_index(drop=True)\nx_valid, y_valid  = valid_df.iloc[:, :-2], valid_df.iloc[:, -2]\n\ntest_df  = chin_mnist_df.groupby('value').apply(lambda x: x.sample(100, random_state=SEED)).reset_index(drop=True)\nx_test, y_test    = test_df.iloc[:, :-2], test_df.iloc[:, -2]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.224648Z","iopub.execute_input":"2021-07-14T20:37:32.225205Z","iopub.status.idle":"2021-07-14T20:37:32.300003Z","shell.execute_reply.started":"2021-07-14T20:37:32.225166Z","shell.execute_reply":"2021-07-14T20:37:32.299128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset(x_train, y_train)\ntrain_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n\nvalid_ds = Dataset(x_valid, y_valid)\nvalid_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=16, shuffle=True)\n\ntest_ds = Dataset(x_test, y_test)\ntest_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.301639Z","iopub.execute_input":"2021-07-14T20:37:32.302036Z","iopub.status.idle":"2021-07-14T20:37:32.312209Z","shell.execute_reply.started":"2021-07-14T20:37:32.301997Z","shell.execute_reply":"2021-07-14T20:37:32.311551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, labels = next(iter(test_dataloader))\nplt.imshow(images[0], cmap='gray')\nprint(images.shape)\nprint(labels[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.314467Z","iopub.execute_input":"2021-07-14T20:37:32.315273Z","iopub.status.idle":"2021-07-14T20:37:32.47397Z","shell.execute_reply.started":"2021-07-14T20:37:32.315231Z","shell.execute_reply":"2021-07-14T20:37:32.473051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n                \n        convs = [\n            nn.Conv2d(1, 32, kernel_size=3, stride=1),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=2, stride=1, dilation=2),\n            nn.Conv2d(128, 256, kernel_size=2, stride=2, dilation=2),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.Conv2d(256, 512, kernel_size=2, stride=1, dilation=2),\n            #nn.Conv2d(512, 1024, kernel_size=2, stride=2, dilation=2),\n            nn.MaxPool2d(2),\n            nn.GELU()\n        ]\n        \n        self.conv = nn.Sequential(*convs)\n        \n        self.linear = nn.Linear(2048, 15)\n        \n        self.log_softmax = nn.LogSoftmax(dim=0) # avoid overflowing: large number -> exp() -> NaN -> log() -> NaN. I think I could also solve this through batch normalization.\n        \n    def forward(self, x):\n        x = x.unsqueeze(1) # single channel image\n        #print(x.size())\n        \n        hidden = self.conv(x)\n        #print(hidden.size())\n        \n        hidden = torch.flatten(hidden, start_dim=1)\n        #print(hidden.size())\n        \n        \n        output = self.log_softmax(self.linear(hidden))\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.475702Z","iopub.execute_input":"2021-07-14T20:37:32.476142Z","iopub.status.idle":"2021-07-14T20:37:32.49136Z","shell.execute_reply.started":"2021-07-14T20:37:32.476097Z","shell.execute_reply":"2021-07-14T20:37:32.490504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ConvNet()\nmodel = model.to(device)\nprint(model)\nprint(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=10e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.493189Z","iopub.execute_input":"2021-07-14T20:37:32.493726Z","iopub.status.idle":"2021-07-14T20:37:32.514148Z","shell.execute_reply.started":"2021-07-14T20:37:32.493654Z","shell.execute_reply":"2021-07-14T20:37:32.513269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\ntrain_size = len(train_ds)\nvalid_size = len(valid_ds)\n\nvalid_conf_matrixes = []\n\nfor epoch in range(epochs):\n    labels = torch.tensor([]).to(device).detach()\n    preds  = torch.tensor([]).to(device).detach()\n    \n    total_preds = 0\n    correct_preds = 0\n    \n    train_running_loss = 0.0\n    \n    for index, data in enumerate(train_dataloader):\n        model.train()\n        \n        batch_inputs, batch_labels = data[0][:].to(device).type(torch.float), data[1][:].to(device)\n        \n        outputs = model(batch_inputs)\n        \n        loss = criterion(outputs, batch_labels) # expects distribution from model softmax as pred and target_index as target\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_running_loss += loss.mean()\n        \n        labels = torch.cat((labels, batch_labels))\n        #total_preds += 1\n        \n        for idx, item in enumerate(outputs):                \n            preds  = torch.cat((preds, torch.argmax(item).unsqueeze(-1)))\n            \n        if index % 10 == 1 :\n            if index == 1:\n                print(f'Training Epoch: {epoch+1}, step: {index+1}, first step training loss: {train_running_loss/1}')\n            else:\n                print(f'Training Epoch: {epoch+1}, step: {index+1}, moving average of training loss: {train_running_loss/10}')\n            train_running_loss = 0.0\n    \n    print('Calculating conf_matrix')\n    conf_mat = confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy())\n    \n    total = np.sum(conf_mat)\n    \n    correct_count = 0\n    \n    for i, data in enumerate(conf_mat[0]):\n        correct_count += conf_mat[i][i]\n    \n    \n    print(f'Training Epoch {epoch+1}:\\n Accuracy: {correct_count/total}\\n{conf_mat}\\n')\n    print()\n    \n    labels = torch.tensor([]).to(device).detach()\n    preds  = torch.tensor([]).to(device).detach()\n    \n    total_preds = 0\n    correct_preds = 0\n    \n    valid_running_loss = 0.0\n    \n    for index, data in enumerate(valid_dataloader):\n        model.eval()\n        \n        batch_inputs, batch_labels = data[0][:].to(device).type(torch.float), data[1][:].to(device)\n        \n        outputs = model(batch_inputs)\n        \n        loss = criterion(outputs, batch_labels) # expects distribution from model softmax as pred and target_index as target\n        \n        valid_running_loss += loss.mean()\n        \n        labels = torch.cat((labels, batch_labels))\n        #total_preds += 1\n        \n        for i, item in enumerate(outputs):                \n            preds  = torch.cat((preds, torch.argmax(item).unsqueeze(-1)))\n        \n        if index % 10 == 1:\n            print(f'Validation Epoch: {epoch+1}, step: {index+1}, running average validation loss: {valid_running_loss/10}')\n            valid_running_loss = 0.0\n        \n    print('Calculating conf_matrix')\n    conf_mat = confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy())\n    \n    total = np.sum(conf_mat)\n    \n    correct_count = 0\n    \n    for i, data in enumerate(conf_mat[0]):\n        correct_count += conf_mat[i][i]\n    \n    valid_conf_matrixes.append(conf_mat)   \n    \n    print(f'Validation Epoch {epoch+1}:\\n Accuracy: {correct_count/total}\\n{conf_mat}')\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:37:32.515908Z","iopub.execute_input":"2021-07-14T20:37:32.516341Z","iopub.status.idle":"2021-07-14T20:37:33.4719Z","shell.execute_reply.started":"2021-07-14T20:37:32.516298Z","shell.execute_reply":"2021-07-14T20:37:33.469639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = len(test_ds)\n\nlabels = np.array([])\npreds  = np.array([])\n\ntest_running_loss = 0.0\n\nfor index, data in enumerate(test_dataloader):\n    model.eval()\n\n    batch_inputs, batch_labels = data[0][:].to(device).type(torch.float), data[1][:].to(device)\n\n    outputs = model(batch_inputs)\n\n    loss = criterion(outputs, batch_labels) # expects distribution from model softmax as pred and target_index as target\n\n    test_running_loss += loss.mean()\n\n    labels = np.concatenate((labels, batch_labels.cpu().numpy()))\n\n    for index, item in enumerate(outputs):\n        preds  = np.concatenate((preds, torch.argmax(item).unsqueeze(-1).detach().cpu().numpy()))\n\n    if index % 10 == 1:\n        print(f'Testing Step: {index+1}, mean testing loss: {test_running_loss / 10}')\n        test_running_loss = 0.0\n\nconf_mat = confusion_matrix(labels, preds)\n\ntotal_preds = np.sum(conf_mat)\n    \ncorrect_preds = 0\n\nfor i, data in enumerate(conf_mat[0]):\n    correct_preds += conf_mat[i][i]\n\nprint(f'Test Accuracy: {correct_preds/total_preds}\\n{conf_mat}')\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T20:41:44.629199Z","iopub.execute_input":"2021-07-14T20:41:44.629599Z","iopub.status.idle":"2021-07-14T20:42:07.997003Z","shell.execute_reply.started":"2021-07-14T20:41:44.62952Z","shell.execute_reply":"2021-07-14T20:42:07.995358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}