{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"#   Question or problem definition\n#  การกรองข้อมูลหนังเสนอคำแนะนำว่าความนิยมของภาพยนตร์ว่าเรื่องไหนที่ได้รับความนิยมมากและหาค่าว่าหนังถูกฉายใน100ปีมีกี่เปอร์เซน\n\n#  Acquire training and testing data.\n#  ชุดข้อมูลมีคุณสมบัติดังต่อไปนี้: movie_id - ตัวระบุที่ไม่ซ้ำกันสำหรับภาพยนตร์แต่ละเรื่องนักแสดง - ชื่อของนักแสดงนำและผู้สนับสนุน - ชื่อของผู้อำนวยการบรรณาธิการนักแต่งเพลงนักเขียน\n#  งบประมาณ - งบประมาณในการสร้างภาพยนตร์ประเภท - ประเภทของภาพยนตร์แอ็คชั่นตลกระทึกขวัญ ฯลฯหน้าแรก - ลิงก์ไปยังหน้าแรกของภาพยนตร์","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport pandas as pd\n#___________________________\ndef load_tmdb_movies(path):\n    df = pd.read_csv(path)\n    df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n    json_columns = ['genres', 'keywords', 'production_countries',\n                    'production_companies', 'spoken_languages']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n#___________________________\ndef load_tmdb_credits(path):\n    df = pd.read_csv(path)\n    json_columns = ['cast', 'crew']\n    for column in json_columns:\n        df[column] = df[column].apply(json.loads)\n    return df\n#___________________\nLOST_COLUMNS = [\n    'actor_1_facebook_likes',\n    'actor_2_facebook_likes',\n    'actor_3_facebook_likes',\n    'aspect_ratio',\n    'cast_total_facebook_likes',\n    'color',\n    'content_rating',\n    'director_facebook_likes',\n    'facenumber_in_poster',\n    'movie_facebook_likes',\n    'movie_imdb_link',\n    'num_critic_for_reviews',\n    'num_user_for_reviews']\n#____________________________________\nTMDB_TO_IMDB_SIMPLE_EQUIVALENCIES = {\n    'budget': 'budget',\n    'genres': 'genres',\n    'revenue': 'gross',\n    'title': 'movie_title',\n    'runtime': 'duration',\n    'original_language': 'language',\n    'keywords': 'plot_keywords',\n    'vote_count': 'num_voted_users'}\n#_____________________________________________________\nIMDB_COLUMNS_TO_REMAP = {'imdb_score': 'vote_average'}\n#_____________________________________________________\ndef safe_access(container, index_values):\n    # return missing value rather than an error upon indexing/key failure\n    result = container\n    try:\n        for idx in index_values:\n            result = result[idx]\n        return result\n    except IndexError or KeyError:\n        return pd.np.nan\n#_____________________________________________________\ndef get_director(crew_data):\n    directors = [x['name'] for x in crew_data if x['job'] == 'Director']\n    return safe_access(directors, [0])\n#_____________________________________________________\ndef pipe_flatten_names(keywords):\n    return '|'.join([x['name'] for x in keywords])\n#_____________________________________________________\ndef convert_to_original_format(movies, credits):\n    tmdb_movies = movies.copy()\n    tmdb_movies.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES, inplace=True)\n    tmdb_movies['title_year'] = pd.to_datetime(tmdb_movies['release_date']).apply(lambda x: x.year)\n    # I'm assuming that the first production country is equivalent, but have not been able to validate this\n    tmdb_movies['country'] = tmdb_movies['production_countries'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['language'] = tmdb_movies['spoken_languages'].apply(lambda x: safe_access(x, [0, 'name']))\n    tmdb_movies['director_name'] = credits['crew'].apply(get_director)\n    tmdb_movies['actor_1_name'] = credits['cast'].apply(lambda x: safe_access(x, [1, 'name']))\n    tmdb_movies['actor_2_name'] = credits['cast'].apply(lambda x: safe_access(x, [2, 'name']))\n    tmdb_movies['actor_3_name'] = credits['cast'].apply(lambda x: safe_access(x, [3, 'name']))\n    tmdb_movies['genres'] = tmdb_movies['genres'].apply(pipe_flatten_names)\n    tmdb_movies['plot_keywords'] = tmdb_movies['plot_keywords'].apply(pipe_flatten_names)\n    return tmdb_movies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # จากนั้นฉันก็โหลดแพ็กเกจทั้งหมดที่จะใช้ตลอดทั้งโน๊ตบุ๊คจากนั้นโหลดชุดข้อมูล จากนั้นฉันจะให้ข้อมูลเกี่ยวกับประเภทคอลัมน์และจำนวนค่าที่หายไป\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/tmdb-movie-metadata/\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math, nltk, warnings\nfrom nltk.corpus import wordnet\nfrom sklearn import linear_model\nfrom sklearn.neighbors import NearestNeighbors\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud, STOPWORDS\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"last_expr\"\npd.options.display.max_columns = 50\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nPS = nltk.stem.PorterStemmer()\n#__________________\n# load the dataset\ncredits = load_tmdb_credits(\"../input/tmdb-movie-metadata/tmdb_5000_credits.csv\")\nmovies = load_tmdb_movies(\"../input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\ndf_initial = convert_to_original_format(movies, credits)\nprint('Shape:',df_initial.shape)\n#__________________________________________\n# info on variable types and filling factor\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ntab_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ในการพัฒนาเครื่องมือแนะนำฉันวางแผนที่จะใช้ประโยชน์อย่างกว้างขวางของคำหลักที่อธิบายถึงภาพยนตร์ อันที่จริงสมมติฐานขั้นพื้นฐานคือภาพยนตร์ที่อธิบายโดยคำหลักที่คล้ายกันควรมีเนื้อหาที่คล้ายกัน\n# ดังนั้นฉันวางแผนที่จะมองอย่างใกล้ชิดถึงวิธีการกำหนดคำหลักและเป็นขั้นตอนแรกฉันจะอธิบายลักษณะของสิ่งที่มีอยู่ในนั้นอย่างรวดเร็ว โดยให้ฉันทำรายการคำหลักที่อยู่ในชุดข้อมูลก่อน\nset_keywords = set()\nfor liste_keywords in df_initial['plot_keywords'].str.split('|').values:\n    if isinstance(liste_keywords, float): continue  # only happen if liste_keywords = NaN\n    set_keywords = set_keywords.union(liste_keywords)\n#_________________________\n# remove null chain entry\nset_keywords.remove('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# จากนั้นกำหนดฟังก์ชั่นที่นับจำนวนครั้งที่แต่ละครั้งปรากฎ:\ndef count_word(df, ref_col, liste):\n    keyword_count = dict()\n    for s in liste: keyword_count[s] = 0\n    for liste_keywords in df[ref_col].str.split('|'):        \n        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue        \n        for s in [s for s in liste_keywords if s in liste]: \n            if pd.notnull(s): keyword_count[s] += 1\n    #______________________________________________________________________\n    # convert the dictionary in a list to sort the keywords by frequency\n    keyword_occurences = []\n    for k,v in keyword_count.items():\n        keyword_occurences.append([k,v])\n    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n    return keyword_occurences, keyword_count\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# โปรดทราบว่าฟังก์ชั่นนี้จะใช้อีกครั้งในส่วนอื่น ๆ ของสมุดบันทึกนี้เมื่อสำรวจเนื้อหาของตัวแปร 'ประเภท' และหลังจากนั้นเมื่อทำความสะอาดคำหลัก สุดท้ายการเรียกฟังก์ชั่นนี้จะช่วยให้เข้าถึงรายการคำหลักที่เรียงลำดับตามความถี่ลดลง\nkeyword_occurences, dum = count_word(df_initial, 'plot_keywords', set_keywords)\nkeyword_occurences[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ในขั้นตอนนี้มีการสร้างรายการคำหลักและเรารู้จำนวนครั้งที่คำหลักแต่ละคำปรากฏในชุดข้อมูล ในความเป็นจริงรายการนี้สามารถใช้เพื่อให้มีความรู้สึกของเนื้อหาของภาพยนตร์ยอดนิยม \n#วิธีแฟนซีในการให้ข้อมูลใช้ประโยชน์จากแพ็คเกจ wordcloud ในการเป็นตัวแทนประเภทนี้คำทั้งหมดจะถูกจัดเรียงในรูปที่มีขนาดที่ขึ้นอยู่กับความถี่ที่เกี่ยวข้อง แทนที่จะเป็น wordcloud\n#เราสามารถใช้ฮิสโตแกรมเพื่อให้ข้อมูลเดียวกัน สิ่งนี้ช่วยให้มีรูปที่คำหลักถูกจัดเรียงตามการเกิดขึ้นและที่สำคัญที่สุดสิ่งนี้ให้\n# จำนวนครั้งที่ปรากฏข้อมูลที่ไม่สามารถเรียกคืนได้จากการเป็นตัวแทนของ wordcloud ในรูปต่อไปนี้ฉันเปรียบเทียบการรับรองทั้งสองประเภท:\ndef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    h = int(360.0 * tone / 255.0)\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n#_____________________________________________\n# UPPER PANEL: WORDCLOUD\nfig = plt.figure(1, figsize=(18,13))\nax1 = fig.add_subplot(2,1,1)\n#_______________________________________________________\n# I define the dictionary used to produce the wordcloud\nwords = dict()\ntrunc_occurences = keyword_occurences[0:50]\nfor s in trunc_occurences:\n    words[s[0]] = s[1]\ntone = 55.0 # define the color of the words\n#________________________________________________________\nwordcloud = WordCloud(width=1000,height=300, background_color='black', \n                      max_words=1628,relative_scaling=1,\n                      color_func = random_color_func,\n                      normalize_plurals=False)\nwordcloud.generate_from_frequencies(words)\nax1.imshow(wordcloud, interpolation=\"bilinear\")\nax1.axis('off')\n#_____________________________________________\n# LOWER PANEL: HISTOGRAMS\nax2 = fig.add_subplot(2,1,2)\ny_axis = [i[1] for i in trunc_occurences]\nx_axis = [k for k,i in enumerate(trunc_occurences)]\nx_label = [i[0] for i in trunc_occurences]\nplt.xticks(rotation=85, fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xticks(x_axis, x_label)\nplt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax2.bar(x_axis, y_axis, align = 'center', color='g')\n#_______________________\nplt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ชุดข้อมูลประกอบด้วย 5043 ภาพยนตร์หรือละครโทรทัศน์ที่อธิบายโดยตัวแปร 28 ตัว ในบางครั้งการวิเคราะห์เราจะต้องจัดการกับค่าที่หายไปและในขั้นตอนแรกฉันจะกำหนดปริมาณของข้อมูลที่หายไปในตัวแปรทุกตัว\nmissing_df = df_initial.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['filling_factor'] = (df_initial.shape[0] \n                                - missing_df['missing_count']) / df_initial.shape[0] * 100\nmissing_df.sort_values('filling_factor').reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ตัวแปร title_year บ่งชี้ว่าเมื่อภาพยนตร์ได้รับการปล่อยตัว เพื่อให้ทั่วโลกมองถึงวิธีการกระจายภาพยนตร์ตามตัวแปรนี้ฉันจัดกลุ่มภาพยนตร์หลายทศวรรษ\ndf_initial['decade'] = df_initial['title_year'].apply(lambda x:((x-1900)//10)*10)\n#__________________________________________________________________\n# function that extract statistical parameters from a grouby objet:\ndef get_stats(gr):\n    return {'min':gr.min(),'max':gr.max(),'count': gr.count(),'mean':gr.mean()}\n#______________________________________________________________\n# Creation of a dataframe with statitical infos on each decade:\ntest = df_initial['title_year'].groupby(df_initial['decade']).apply(get_stats).unstack()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# และแสดงผลลัพธ์ในแผนภูมิวงกลม\nsns.set_context(\"poster\", font_scale=0.85)\n#_______________________________\n# funtion used to set the labels\ndef label(s):\n    val = (1900 + s, s)[s < 100]\n    chaine = '' if s < 50 else \"{}'s\".format(int(val))\n    return chaine\n#____________________________________\nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(11, 6))\nlabels = [label(s) for s in  test.index]\nsizes  = test['count'].values\nexplode = [0.2 if sizes[i] < 100 else 0.01 for i in range(11)]\nax.pie(sizes, explode = explode, labels=labels,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow=False, startangle=0)\nax.axis('equal')\nax.set_title('% of films per decade',\n             bbox={'facecolor':'k', 'pad':5},color='w', fontsize=16);\ndf_initial.drop('decade', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ตัวแปรประเภทจะมีความสำคัญอย่างแน่นอนในขณะที่สร้างเอนจิ้นการแนะนำเนื่องจากมันอธิบายเนื้อหาของภาพยนตร์ เช่นละคร, ตลก, แอ็คชั่น, ...  \n# หากต้องการดูว่าประเภทใดที่ได้รับความนิยมมากที่สุดฉันใช้วิธีเดียวกันมากกว่าคำหลัก (ด้วยการใช้บรรทัดรหัสที่คล้ายกัน) ก่อนอื่นให้ทำการสำรวจประชากร\ngenre_labels = set()\nfor s in df_initial['genres'].str.split('|').values:\n    genre_labels = genre_labels.union(set(s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# แล้วนับว่าแต่ละครั้งมีกี่ครั้ง\nkeyword_occurences, dum = count_word(df_initial, 'genres', genre_labels)\nkeyword_occurences[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ในที่สุดผลลัพธ์จะแสดงเป็น wordcloud\nwords = dict()\ntrunc_occurences = keyword_occurences[0:50]\nfor s in trunc_occurences:\n    words[s[0]] = s[1]\ntone = 100 # define the color of the words\nf, ax = plt.subplots(figsize=(14, 6))\nwordcloud = WordCloud(width=550,height=300, background_color='black', \n                      max_words=1628,relative_scaling=0.7,\n                      color_func = random_color_func,\n                      normalize_plurals=False)\nwordcloud.generate_from_frequencies(words)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wrangle, prepare, cleanse the data.\n# ตรวจสอบข้อมูลว่ามีตัวใดสูญหายบ้าง"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_duplicate_cleaned = df_initial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# รวบรวมคำหลักที่ปรากฏในตัวแปร plot_keywords รายการนี้จะถูกทำความสะอาดโดยใช้แพ็คเกจ NLTK ในที่สุดฉันก็มองหาจำนวนการปรากฏของคำหลักต่างๆ\n# Collect the keywords\n#----------------------\ndef keywords_inventory(dataframe, colonne = 'plot_keywords'):\n    PS = nltk.stem.PorterStemmer()\n    keywords_roots  = dict()  # collect the words / root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys = []\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        for t in s.split('|'):\n            t = t.lower() ; racine = PS.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n            else:\n                keywords_roots[racine] = {t}\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords, keywords_roots, keywords_select = keywords_inventory(df_duplicate_cleaned,\n                                                               colonne = 'plot_keywords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of a sample of keywords that appear in close varieties \n#------------------------------------------------------------\nicount = 0\nfor s in keywords_roots.keys():\n    if len(keywords_roots[s]) > 1: \n        icount += 1\n        if icount < 15: print(icount, keywords_roots[s], len(keywords_roots[s]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacement of the keywords by the main form\n#----------------------------------------------\ndef remplacement_df_keywords(df, dico_remplacement, roots = False):\n    df_new = df.copy(deep = True)\n    for index, row in df_new.iterrows():\n        chaine = row['plot_keywords']\n        if pd.isnull(chaine): continue\n        nouvelle_liste = []\n        for s in chaine.split('|'): \n            clef = PS.stem(s) if roots else s\n            if clef in dico_remplacement.keys():\n                nouvelle_liste.append(dico_remplacement[clef])\n            else:\n                nouvelle_liste.append(s)       \n        df_new.set_value(index, 'plot_keywords', '|'.join(nouvelle_liste)) \n    return df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_keywords_cleaned = remplacement_df_keywords(df_duplicate_cleaned, keywords_select,\n                                               roots = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords.remove('')\nkeyword_occurences, keywords_count = count_word(df_keywords_cleaned,'plot_keywords',keywords)\nkeyword_occurences[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analyze, identify patterns, and explore the data.\n> การวิเคราะ และ สำรวจข้อมูล**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ล้างรายการคำหลักในสองขั้นตอน ในขั้นตอนแรกฉันจะระงับคำหลักที่ปรากฏน้อยกว่านั้น 5 ครั้งและแทนที่ด้วยคำย่อที่มีความถี่สูงกว่า เป็นขั้นตอนที่สองฉันจะระงับคำหลักทั้งหมดที่ปรากฏในภาพยนตร์น้อยกว่า 3 เรื่อง\ndef get_synonymes(mot_cle):\n    lemma = set()\n    for ss in wordnet.synsets(mot_cle):\n        for w in ss.lemma_names():\n            #_______________________________\n            # We just get the 'nouns':\n            index = ss.name().find('.')+1\n            if ss.name()[index] == 'n': lemma.add(w.lower().replace('_',' '))\n    return lemma   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mot_cle = 'alien'\nlemma = get_synonymes(mot_cle)\nfor s in lemma:\n    print(' \"{:<30}\" in keywords list -> {} {}'.format(s, s in keywords,\n                                                keywords_count[s] if s in keywords else 0 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_keyword(mot, key_count, threshold):\n    return (False , True)[key_count.get(mot, 0) >= threshold]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword_occurences.sort(key = lambda x:x[1], reverse = False)\nkey_count = dict()\nfor s in keyword_occurences:\n    key_count[s[0]] = s[1]\n\nremplacement_mot = dict()\nicount = 0\nfor index, [mot, nb_apparitions] in enumerate(keyword_occurences):\n    if nb_apparitions > 5: continue  # only the keywords that appear less than 5 times\n    lemma = get_synonymes(mot)\n    if len(lemma) == 0: continue     # case of the plurals\n    \n    liste_mots = [(s, key_count[s]) for s in lemma \n                  if test_keyword(s, key_count, key_count[mot])]\n    liste_mots.sort(key = lambda x:(x[1],x[0]), reverse = True)    \n    if len(liste_mots) <= 1: continue       # no replacement\n    if mot == liste_mots[0][0]: continue    # replacement by himself\n    icount += 1\n    if  icount < 8:\n        print('{:<12} -> {:<12} (init: {})'.format(mot, liste_mots[0][0], liste_mots))    \n    remplacement_mot[mot] = liste_mots[0][0]\n\nprint(90*'_'+'\\n'+'The replacement concerns {}% of the keywords.'\n      .format(round(len(remplacement_mot)/len(keywords)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Keywords that appear both in keys and values:'.upper()+'\\n'+45*'-')\nicount = 0\nfor s in remplacement_mot.values():\n    if s in remplacement_mot.keys():\n        icount += 1\n        if icount < 10: print('{:<20} -> {:<20}'.format(s, remplacement_mot[s]))\n\nfor key, value in remplacement_mot.items():\n    if value in remplacement_mot.keys():\n        remplacement_mot[key] = remplacement_mot[value]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keywords_synonyms = \\\n            remplacement_df_keywords(df_keywords_cleaned, remplacement_mot, roots = False)   \nkeywords, keywords_roots, keywords_select = \\\n            keywords_inventory(df_keywords_synonyms, colonne = 'plot_keywords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords.remove('')\nnew_keyword_occurences, keywords_count = count_word(df_keywords_synonyms,\n                                                    'plot_keywords',keywords)\nnew_keyword_occurences[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remplacement_df_low_frequency_keywords(df, keyword_occurences):\n    df_new = df.copy(deep = True)\n    key_count = dict()\n    for s in keyword_occurences: \n        key_count[s[0]] = s[1]    \n    for index, row in df_new.iterrows():\n        chaine = row['plot_keywords']\n        if pd.isnull(chaine): continue\n        nouvelle_liste = []\n        for s in chaine.split('|'): \n            if key_count.get(s, 4) > 3: nouvelle_liste.append(s)\n        df_new.set_value(index, 'plot_keywords', '|'.join(nouvelle_liste))\n    return df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keywords_occurence = \\\n    remplacement_df_low_frequency_keywords(df_keywords_synonyms, new_keyword_occurences)\nkeywords, keywords_roots, keywords_select = \\\n    keywords_inventory(df_keywords_occurence, colonne = 'plot_keywords')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords.remove('')\nnew_keyword_occurences, keywords_count = count_word(df_keywords_occurence,\n                                                    'plot_keywords',keywords)\nnew_keyword_occurences[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5 Model, predict and solve the problem.**\n6 Visualize, report, and present the problem solving steps and final solution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# แสดงหราฟภาพก่อนและหลังทำ cleaning\nfont = {'family' : 'fantasy', 'weight' : 'normal', 'size'   : 15}\nmpl.rc('font', **font)\n\nkeyword_occurences.sort(key = lambda x:x[1], reverse = True)\n\ny_axis = [i[1] for i in keyword_occurences]\nx_axis = [k for k,i in enumerate(keyword_occurences)]\n\nnew_y_axis = [i[1] for i in new_keyword_occurences]\nnew_x_axis = [k for k,i in enumerate(new_keyword_occurences)]\n\nf, ax = plt.subplots(figsize=(9, 5))\nax.plot(x_axis, y_axis, 'r-', label='before cleaning')\nax.plot(new_x_axis, new_y_axis, 'b-', label='after cleaning')\n\n# Now add the legend with some customizations.\nlegend = ax.legend(loc='upper right', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nfor label in legend.get_texts():\n    label.set_fontsize('medium')\n            \nplt.ylim((0,25))\nplt.axhline(y=3.5, linewidth=2, color = 'k')\nplt.xlabel(\"keywords index\", family='fantasy', fontsize = 15)\nplt.ylabel(\"Nb. of occurences\", family='fantasy', fontsize = 15)\n#plt.suptitle(\"Nombre d'occurences des mots clés\", fontsize = 18, family='fantasy')\nplt.text(3500, 4.5, 'threshold for keyword delation', fontsize = 13)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}