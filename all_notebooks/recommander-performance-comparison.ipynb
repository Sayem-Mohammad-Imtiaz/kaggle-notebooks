{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Recommander systems performance comparision\n\nIn this notebook we compare many models performance on the [Restaurant Data with Consumer Ratings](https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings) dataset. For that we use [Surprise](http://surpriselib.com/) for the ease of use and good documentation. The dataset used here (rating_final) has 1161 lines, so we are not very worried about (fitting) performance."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Note that there are no NANs in these data; '?' is\n# used when there is missing information\nrating = pd.read_csv('../input/rating_final.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Surprise's Input is a df of this shape :\noverall_rating = rating[['userID','placeID','rating']]\noverall_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from surprise import SVD,SVDpp,KNNBasic,KNNWithZScore\nfrom surprise.dataset import Reader, Dataset\nfrom surprise.model_selection import LeaveOneOut\nfrom surprise import accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To import a df for use in Surprise, we need to load it into a `Surprise.Dataset`, for that we use `Dataset.load_from_df(dataframe,reader)` where `reader` has to specify the range of ratings in the dataset. \n\n⚠️ We use a different kind of validation, since we're trying to validate a recommender system we use the LeaveOneOut validation. From [Surprise Documentation on Cross Validators](https://surprise.readthedocs.io/en/stable/model_selection.html#surprise.model_selection.split.LeaveOneOut) :\n> Cross-validation iterator where each user has exactly one rating in the testset.\n> \n> Contrary to other cross-validation strategies, LeaveOneOut does not guarantee that all folds will be different, although this is still very likely for sizeable datasets.\n\nIndeed, we leave one vote out from each user as a test set (user with less than the `min_n_ratings` are eliminated), that way we test the performance of the recommender system in a realistic situation."},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = Reader(rating_scale=(0, 2))\nds = Dataset.load_from_df(overall_rating,reader)\nloo = LeaveOneOut(n_splits=1,min_n_ratings=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = [0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classical use of Suprise models\n\nLearning, calculating/plotting metrics and making predictions\n\n### Fitting model using different parameters\n\nSteps followed :\n\n1. Model initializing :\n    - Call the model's class with chosen parameters\n2. dataset splitting : \n    - Loop over cross validation splits extracting trainset and testset\n3. Fitting model :\n    - Fit model on the train set\n4. Testing :\n    - Test model on train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = []\ntest_loss = []\nmodels = []\nfor i in range(len(LR)) :\n    lr_all = LR[i]\n    algo = SVD(n_epochs=50,reg_all=0.01,lr_all=lr_all)\n    models.append(algo)\n    for trainset,testset in loo.split(ds) : #train - validation split with leave one out\n        # train and test algorithm.\n        algo.fit(trainset)\n        train_pred = algo.test(trainset.build_testset())\n        test_pred = algo.test(testset)\n\n        # Compute and print Root Mean Squared Error\n        train_rmse = accuracy.rmse(train_pred, verbose=False)\n        test_rmse = accuracy.rmse(test_pred, verbose=False)\n        train_loss.append(train_rmse)\n        test_loss.append(test_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting test and train rmse lines"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(LR,train_loss,label='train')\nplt.plot(LR,test_loss, label = 'test')\nplt.xlabel('learning_rate')\nplt.ylabel('rmse')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting a rating"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Index of minimum element\ni = test_loss.index(min(test_loss))\n# using the best model\nalgo = models[i]\n# predicting rating\nalgo.predict('U1077','132825')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search using GridSearchCV\n\nUsing Grid Search to try different parameters\n\n### Workflow :\n\n1. preparing parameter map :\n    - Create a dict of all the parameters you want to try (dict of arrays)\n2. dataset splitting : \n    - Prepare an instance of your preferred cross-validator\n3. Fitting model :\n    - Fit the whole dataset to the grid search instance (separation handled internally)\n4. Testing :\n    - Handled Internally\n5. Visualization :\n    - use the cv_results attribute of your gridsearch instance to visualize results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import GridSearchCV\nparam_grid = {\n    'n_factors' : [10, 20, 50, 100, 130, 150, 200],\n    'n_epochs': [10, 15, 30, 50, 100], \n    'lr_all': [0.001, 0.005, 0.007, 0.01, 0.05, 0.07, 0.1],\n    'reg_all': [0.01, 0.05, 0.07, 0.1, 0.2, 0.4, 0.6]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVD and SVD++ :"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_svd = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=loo,return_train_measures = True)\ngs_svdpp = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae'], cv=loo,return_train_measures = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_svd.fit(ds)\ngs_svdpp.fit(ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scores and best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# best RMSE score\nprint (\"Best RMSE Scores\")\nprint(f'SVD : {gs_svd.best_score[\"rmse\"]}')\nprint(f'SVDpp : {gs_svdpp.best_score[\"rmse\"]}')\n\n\n# combination of parameters that gave the best RMSE score\nprint(\"Parameters\")\nprint(f\"SVD : {gs_svd.best_params['rmse']}\")\nprint(f\"SVDpp : {gs_svdpp.best_params['rmse']}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Saving results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_frame_svd = pd.DataFrame.from_dict(gs_svd.cv_results)\nresults_frame_svd['model'] = 'SVD'\nresults_frame_svdpp = pd.DataFrame.from_dict(gs_svdpp.cv_results)\nresults_frame_svdpp['model'] = 'SVDpp'\nresults_frame = pd.concat([results_frame_svd, results_frame_svdpp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_frame.sort_values(by='mean_test_rmse').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN and KNN With Z Score :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sim_options = {'name': ['pearson', 'cosine', 'msd'],\n               'user_based':[ True, False] # compute  similarities between users\n               }\nparam_grid_knn = {\n    'sim_options' : sim_options,\n    'k' : [10, 20, 40, 100],\n    'min_k' : [1, 5 , 10],\n    'verbose' : [False]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_knn= GridSearchCV(KNNBasic, param_grid=param_grid_knn, measures=['rmse', 'mae'], cv=loo,return_train_measures = True)\ngs_knnZ= GridSearchCV(KNNWithZScore, param_grid=param_grid_knn, measures=['rmse', 'mae'], cv=loo,return_train_measures = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_knn.fit(ds)\ngs_knnZ.fit(ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scores and best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# best RMSE score\nprint (\"Best RMSE Scores\")\nprint(f'KNN : {gs_knn.best_score[\"rmse\"]}')\nprint(f'KNNWithZScore : {gs_knnZ.best_score[\"rmse\"]}')\n\n\n# combination of parameters that gave the best RMSE score\nprint(\"Parameters\")\nprint(f\"KNN : {gs_knn.best_params['rmse']}\")\nprint(f\"KNNWithZScore : {gs_knnZ.best_params['rmse']}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Saving results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_frame_knn = pd.DataFrame.from_dict(gs_knn.cv_results)\nresults_frame_knn['model'] = 'KNN'\nresults_frame_knnZ = pd.DataFrame.from_dict(gs_knnZ.cv_results)\nresults_frame_knnZ['model'] = 'KNNWithZScore'\nresults_frame = pd.concat([results_frame, results_frame_knn, results_frame_knnZ],sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finalize results set :\n\n#### Creating global ranks for metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_frame['rank_test_mae'] = results_frame['mean_test_mae'].rank()\nresults_frame['rank_test_rmse'] = results_frame['mean_test_rmse'].rank()\nresults_frame.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Export models performance for visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export for visualization\nresults_frame.to_csv('results.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}