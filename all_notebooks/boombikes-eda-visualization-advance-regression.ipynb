{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1>BoomBikes Demand Prediction</h1></center>"},{"metadata":{},"cell_type":"markdown","source":"<h2>Problem Statement</h2>\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider **BoomBikes** has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n<ul>\n    <li>Which variables are significant in predicting the demand for shared bikes.</li>\n    <li>How well those variables describe the bike demands</li>\n</ul>\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n\n<h2>Business Goal:</h2>\n\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. \n\n\n<h2>Dataset characteristics:</h2>\n<table>\n    <tr><th style=\"text-align:center\">instant</th><th style=\"text-align:center\">record index</th></tr>\n\t<tr><td style=\"text-align:center\">dteday</td><td style=\"text-align:left\">date</td></tr>\n\t<tr><td style=\"text-align:center\">season</td><td style=\"text-align:left\">season (1:spring, 2:summer, 3:fall, 4:winter)</td></tr>\n\t<tr><td style=\"text-align:center\">yr</td><td style=\"text-align:left\">year (0: 2018, 1:2019)</td></tr>\n\t<tr><td style=\"text-align:center\">mnth</td><td style=\"text-align:left\">month ( 1 to 12)</td></tr>\n\t<tr><td style=\"text-align:center\">holiday</td><td style=\"text-align:left\">weather day is a holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)</td></tr>\n\t<tr><td style=\"text-align:center\">weekday</td><td style=\"text-align:left\">day of the week</td></tr>\n\t<tr><td style=\"text-align:center\">workingday</td><td style=\"text-align:left\">if day is neither weekend nor holiday is 1, otherwise is 0.</td></tr>\n\t<tr><td style=\"text-align:center\">weathersit</td><td style=\"text-align:left\">\n        1: Clear, Few clouds, Partly cloudy, Partly cloudy<br>\n\t\t2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>\n\t\t3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n\t\t4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog</td></tr>\n\t<tr><td style=\"text-align:center\">temp</td><td style=\"text-align:left\">temperature in Celsius</td></tr>\n\t<tr><td style=\"text-align:center\">atemp</td><td style=\"text-align:left\">feeling temperature in Celsius</td></tr>\n\t<tr><td style=\"text-align:center\">hum</td><td style=\"text-align:left\">humidity</td></tr>\n\t<tr><td style=\"text-align:center\">windspeed</td><td style=\"text-align:left\">wind speed</td></tr>\n\t<tr><td style=\"text-align:center\">casual</td><td style=\"text-align:left\">count of casual users</td></tr>\n\t<tr><td style=\"text-align:center\">registered</td><td style=\"text-align:left\">count of registered users</td></tr>\n\t<tr><td style=\"text-align:center\">cnt</td><td style=\"text-align:left\">count of total rental bikes including both casual and registered</td></tr>\n    </table>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import required packages\nimport numpy as np\nimport pandas as pd\nimport random\nimport calendar\nimport warnings\nimport seaborn as sns\nimport os.path\nimport datetime\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib import colors\nfrom matplotlib import cm\nfrom matplotlib.font_manager import FontProperties\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n\nimport statsmodels.api as sm  \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# surpress warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import data from csv file\ndf = pd.read_csv(\"/kaggle/input/boombikes/day.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import font awesome file and check if file exist to prevent error on execution\nfont_exists = os.path.exists(\"/kaggle/input/boombikes-data/Font Awesome 5 Free-Solid-900.otf\")\nfp = FontProperties(fname=r\"/kaggle/input/boombikes-data/Font Awesome 5 Free-Solid-900.otf\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the shape of the dataframe\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview the head of the datframe\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the datatypes and null columns for the given dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have any null values in any of the features.\n\nLets validate and convert the datatypes of columns to appropriate format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting dteday to datetime object\ndf.dteday = pd.to_datetime(df.dteday,format=\"%d-%m-%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On Analysis the day of the weekday columns seems to be having Monday as the first day of the week. \n# In our case the weekday starts with Sunday = 0, Monday = 1 ... Saturday = 6. \n# So we have to map the number in ordered days from Sunday to Saturday.\n# making sunday as firstday of the week\n#\n# df.weekday = (df.dteday.dt.weekday + 1 ) % 7\n# df.weekday.head(7)\n#\n# commented and not performing the above operation as per analysis from the calendar for the years 2018 and 2019 \n# workingday, weekdays and holiday features are incorrectly mapped for the dteday so it might mislead\n# the data for all these columns, instead we will drop dteday.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping unncessary columns\ndf.drop(columns=['instant'],inplace=True) # instant is a record index variable which is not required for our analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let look at the overview of the table value description"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the data looks good and error free now we can proceed to data visualtisation step"},{"metadata":{},"cell_type":"markdown","source":"We can now visualize the dataset to understand the data better and observe patterns to get more insights and product a meaningful output"},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"Lets perform some EDA on our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let plot a pair plot to see how the relations are between feature values\nsns.pairplot(df,vars=['season','temp','windspeed','casual','atemp','workingday','registered','weekday','mnth','yr','holiday','cnt']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set figure size\nplt.figure(figsize=(10,10))\n\n# plot heatmap for the correlation\nsns.heatmap(round(df.corr(),2),annot=True,cmap=\"viridis\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Drop Temp feature**\n\nWe can see that there is a high correlation between temp and atemp so lets drop temp feature from the dataset to prevent multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = []\ncolumns_to_drop.append('temp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert to categorical columns"},{"metadata":{},"cell_type":"markdown","source":"We have few categorical columns with numerical values. It would be better to apply the conventional name for visualisation"},{"metadata":{},"cell_type":"markdown","source":"## Weekday"},{"metadata":{"trusted":true},"cell_type":"code","source":"# assigning day names to day number of the week\n# 0 sunday\n# 1 monday\n# ...\n# 6 saturday\n\ndf.weekday = df.weekday.apply(lambda x: calendar.day_abbr[x-1]).astype('category')\ndf.weekday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set month abbrivation for month number\nmonth_name = {i : calendar.month_abbr[i] for i in np.arange(1,13)}\n\n# map the month names in the data set mnth column and set to categorical datatype\ndf.mnth = pd.Categorical(df.mnth.map(month_name),categories=[calendar.month_abbr[i] for i in np.arange(1,13)],ordered=True)\n\n# deleting month_name variable as it is not required anymore\ndel month_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have four season numbers which can be changed to categorical column but before that lets name the feature values"},{"metadata":{},"cell_type":"markdown","source":"## Season"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create season mapping variable\nseason_name  = {1:'spring',2:'summer',3:'fall',4:'winter'}\n\n# Mapping Season names for the number\ndf.season = df.season.map(season_name).astype(\"category\")\n\n# deleting season_name variable as it is not required anymore\ndel season_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have four type of weather that are recorder in numbers which can be added as a categorical feature. Hence lets name the feature values"},{"metadata":{},"cell_type":"markdown","source":"## Weather"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create weather mapping variable\nweather_name = {1:'clear',2:'cloudy',3:'rain',4:'snow'}\n\n# Mapping Season names for the number\ndf.weathersit = df.weathersit.map(weather_name).astype(\"category\")\n\n# deleting weather_name variable as it is not required anymore\ndel weather_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating derived column week_month"},{"metadata":{"trusted":true},"cell_type":"code","source":"def week_in_month(dt):\n    weekday_one = datetime.date(dt.year, dt.month, 1).weekday()\n    weekday_day = datetime.date(dt.year, dt.month, dt.day).weekday()\n    x=(dt.day - 1)//7 + 1 + (weekday_day < weekday_one)\n    #print(x,' ',dt)\n    return 'week '+str(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['week_month']=df['dteday'].apply(week_in_month).astype('category')\ndf['week_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the columns names to pick the categorical columns\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday','weathersit','week_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualising categorical variables in our dataset with count\nplt.figure(figsize=(30, 20))\n\n# iterate over each column and plot a box plot\nfor i,v in enumerate(categorical_features):\n    plt.subplot(3,3,i+1)\n    sns.boxplot(x = v , y = 'cnt', data = df,palette='viridis')\nplt.suptitle(\"Box Plot for all categorical features\",size=25,y=0.95)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy original dataset to temp dataset for visualization purpose\ndf_temp = df.copy()\n# change date format\ndf_temp.dteday = pd.to_datetime(df_temp.dteday.dt.strftime('%d-%m'),format='%d-%m')\n# create month locatore object\nmonths_locator = mdates.MonthLocator()\n# seperate the dataset based on year to temporary dataset\ndf_2018 = df_temp[df_temp['yr']==0]\ndf_2019 = df_temp[df_temp['yr']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create figure size\nplt.figure(figsize=(15,5))\n\n# creata scatter plot for plotinh holidays in months\nplt.scatter('dteday','cnt',data=df_temp[df_temp['holiday']==1],c='r',label='holiday')\n\n# plot the graph for the yaer 2018 and 2019 based on cnt on each day\nplt.plot('dteday','cnt',data=df_2018,label='2018')\nplt.plot('dteday','cnt',data=df_2019,label='2019')\n\n# set the majorlocator to months instead of day\nplt.gca().xaxis.set_major_locator(months_locator)\n\n# set tick labels and length of ticks\nplt.gca().set_xticklabels([calendar.month_abbr[i] for i in np.arange(1,13)])\nplt.gca().tick_params(axis='x',length=0)\n\n# plot legend title and lebels\nplt.legend()\nplt.title(\"2018 vs 2019 bike sharing\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Days(Months)\")\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nWe can see that moslty when its a holiday the bike sharing is lower.\n\nThe bike sharing has increased in the year 2019 compared to its previous year\n\nThe trend looks alike on the increaseing and decreasing points over the period of time in a year"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a method to plot graph for differnet years in a seperate plot\ndef plot_temp(temp_df,year):\n    # create subplot with figuersize 15 width and 5 height\n    plt.subplots(figsize=(15,5))\n    \n    # convert date object to number via matplotlib dates library\n    x=mdates.date2num(temp_df.dteday)\n    y=temp_df.cnt.values\n    c=temp_df['atemp'].values\n    \n    # combine value of x and y to an array and Transform it to rows and reshape it to 3d array\n    points = np.array([x, y]).T.reshape(-1, 1, 2)\n    \n    # connect the point with next point in series to make a connect line between the scatter points.\n    # ie paring the points to make each point to plot a line\n    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n    \n    # plot the graph with color map and norm to set color scale for temperature\n    lc = LineCollection(segments,cmap=plt.get_cmap('coolwarm'), norm=plt.Normalize(3.5, 42.5))\n    \n    # set the array with temperature values\n    lc.set_array(c)\n    \n    # set the line width property\n    lc.set_linewidth(2)\n    ax=plt.gca()\n    fig=plt.gcf()\n    \n    # add collection event to axis object\n    ax.add_collection(lc)\n    \n    # create color bar for the line collection\n    axcb = fig.colorbar(lc)\n    \n    # set min and max limit for x and y axis\n    plt.xlim(min(x), max(x))\n    plt.ylim(min(y), max(y))\n    \n    # set major locator and its format\n    ax.xaxis.set_major_locator(months_locator)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m'))\n    \n    # set ticke labels for the major locators\n    ax.set_xticklabels([calendar.month_abbr[i] for i in np.arange(1,13)])\n    ax.tick_params(axis='x',length=0)\n\n    # set title and labes \n    plt.title(\"Bikes booked for the Year \"+year+\" (colored in Actual Temperature)\")\n    plt.xlabel(\"Months\")\n    plt.ylabel(\"Booked count\")\n    axcb.set_label('Actual Temperature')\n    \n    # set the layout to fit\n    plt.tight_layout()\n    \n    # show plot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting graphs\nplot_temp(df_2018,'2018')\nplot_temp(df_2019,'2019')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Observation**\n\nWe can see that when the bike sharing is comparitevly low  in lower temperature than in higher temperatures for the years 2018 and 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete dataframe that is not need to save memeory\ndel df_temp;\ndel df_2018;\ndel df_2019;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count plot based on weather condition\n_, ax = plt.subplots(figsize=(15,5))\n\n# plot graph for weather vs count\n(df.groupby('weathersit')[['casual','registered']].sum()/1000).plot(kind='bar',ax=ax)\n\n# plot icons if font awesome exists\nif font_exists:\n    ax.text(0, 1500, \"\\uf185\", fontproperties=fp, size=50, color=\"orange\", ha=\"center\", va=\"center\")\n    ax.text(1, 1250, \"\\uf6c4\", fontproperties=fp, size=50, color=\"lightslategrey\", ha=\"center\", va=\"center\")\n    ax.text(2, 1000, \"\\uf740\", fontproperties=fp, size=50, color=\"lightskyblue\", ha=\"center\", va=\"center\")\nelse:\n    print(\"please extract 'Font Awesome 5 Free-Solid-900.otf' from zip\")\n\n# remove borders    \nsns.despine(top=True,right=True)\n\n# set axis, labels and title\nplt.ylabel(\"Count in Thousands\")\nplt.xlabel(\"Weather\")\nplt.title(\"Number of bikes booked\\n in defferent weather\")\n\n# plot values for bars\nfor patch in ax.patches:\n    ax.text(patch.get_x()+0.125,patch.get_height()+10,int(patch.get_height()),ha='center')\n\n# show plot\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nThe graph depicts that the bike sharing is less on rainy days and more on days in clear sky"},{"metadata":{"trusted":true},"cell_type":"code","source":"# craete a temporary df for month data count\nmonth_data = (df.groupby('mnth')['cnt'].sum()/1000).to_frame().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create subplots\nfig , ax = plt.subplots(figsize=(12,6))\n\n# check if font awesome file exists\nif font_exists:\n    # plot the graph for total counts in month\n    plt.plot(month_data['cnt'],linewidth=5,c='saddlebrown',zorder=1);\n    \n    # plot bike direction to indicate increase or decrease in trend\n    ax.text(1,175,\"\\uf84a\",rotation=60,fontproperties=fp,size=20)\n    ax.text(4,338,\"\\uf84a\",rotation=18,fontproperties=fp,size=20)\n    ax.text(9.355,290,\"\\uf84a\",rotation=-50,fontproperties=fp,size=20)\n    \n    # fill color below and above the slope\n    ax.fill_between(month_data['mnth'],month_data['cnt'],color='forestgreen')\n    ax.fill_between(month_data['mnth'],np.ones(len(month_data['mnth']))*400,color='lightskyblue',alpha=0.5)\n    \n    # remove margin spacing\n    plt.margins(0,0)\nelse:\n    print(\"please extract 'Font Awesome 5 Free-Solid-900.otf' from zip\")\n    # plot a normal graph\n    plt.plot(month_data['cnt']);\n    \n# set liimit for y axis\nax.set_ylim(100,400)\n\n# remove axis border\nsns.despine(top=True,right=True)\n\n# set axis labels and title\nplt.ylabel(\"Total Count in Thousands\",size=15)\nplt.xlabel(\"Months\",size=15)\nplt.title(\"Bike shares trend in each month\",size=20);\n\n# set tick values\nax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11])\nax.set_xticklabels([calendar.month_abbr[i] for i in np.arange(1,13)]);\nax.set_yticks([100,150,200,250,300,350,400]);\nax.set_yticklabels([100,150,200,250,300,350,400]);\nplt.tight_layout()\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Observation</h3>\n\nThe bike shares increases from Jan to May and starts decreasing from the month of September.\n\nSo we could say that the first half of the year undergoes a increase in bookings and after september the bike booking startes depleating"},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete the temporary dataframe\ndel month_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.groupby('weekday')['cnt'].sum()/1000).plot(kind='bar', color=list(sns.color_palette('summer',7)));\nplt.title(\"Weeday vs Cnt\")\nplt.ylabel(\"Numer of boooking(Thousands)\")\nplt.xlabel(\"Weekday\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nWeekday Doesn't show much difference so it can not be used for our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.groupby('week_month')['cnt'].sum()/1000).plot(kind='bar',color=list(sns.color_palette('summer',6)));\nplt.title(\"Week of Month vs Cnt\")\nplt.ylabel(\"Number of bookings(Thousands)\")\nplt.xlabel(\"Week of Month\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.groupby('workingday')['cnt'].sum()/1000).plot(kind='bar',color=['yellowgreen','darkgreen']);\nplt.title(\"Workingday vs Cnt\")\nplt.ylabel(\"Number of bookings(Thousands)\")\nplt.xlabel(\"Workingday\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nBike booking is more on workingdays"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('default')\n\n# creata subplot with figure size 15,6\nfig , ax = plt.subplots(figsize=(15,6))\n\n# create a plot for total count in season\n(df.groupby('season')['cnt'].sum()/1000).plot(kind='barh',ax=ax,width=0.5,color=['burlywood','darkgreen','saddlebrown','lightsteelblue']);\n\n# set x-axis limit\nplt.xlim(0,1200)\n\n# plot bar values \nfor patch in ax.patches:\n    ax.text(patch.get_width()+30,patch.get_y()+0.2,\"{0:.0f} K\".format(int(patch.get_width())),ha='center',size=12)\n    \nax.tick_params(axis='both',labelsize=15,length=0)\nplt.ylabel('Season',size=17)\nplt.xlabel('Count',size=17)\nplt.title('Number of bookings in Season',size=20)    \n\nsns.despine(left=True,right=True,top=True,bottom=True)\n\nif font_exists:\n    ax.text(125,3.3,\"\\uf84a\",fontproperties=fp,size=20)\n    ax.text(200,2.29,\"\\uf84a\",fontproperties=fp,size=20)\n    ax.text(75,1.3,\"\\uf84a\",fontproperties=fp,size=20)\n    ax.text(300,0.3,\"\\uf84a\",fontproperties=fp,size=20)\n    \n    ax.text(random.randrange(800),2.5,\"\\uf185\",fontproperties=fp,size=20,color='orange')\n    \n    for i in np.arange(1,100):\n        ax.text(random.randrange(800),3.06,\"\\uf7ad\",fontproperties=fp,size=20,zorder=2,color='lightskyblue')\n        ax.text(random.randrange(830),random.uniform(3.1, 3.5),\"\\uf2dc\",fontproperties=fp,size=5,zorder=2,color='lightskyblue')\n    for i in np.arange(1,20):\n        ax.text(random.randrange(900),2.28,\"\\uf1bb\",fontproperties=fp,size=20,zorder=random.randrange(2),color='g')\n    for i in np.arange(1,10):\n        ax.text(random.randrange(450),1.25,\"\\uf5bb\",fontproperties=fp,size=13,zorder=random.randrange(2),color='tomato')\n    for i in np.arange(1,50):        \n        ax.text(random.randrange(50),1.25,\"\\uf773\",fontproperties=fp,size=5,zorder=random.randrange(2),color='cornflowerblue')\n        ax.text(random.randint(300, 350),1.235,\"\\uf773\",fontproperties=fp,size=5,zorder=random.randrange(2),color='cornflowerblue')\n    for i in np.arange(1,50):\n        ax.text(random.randrange(1030),0.15,\"\\uf06c\",rotation=random.randrange(360),fontproperties=fp,size=13,zorder=random.randrange(2),color='orange')\n        ax.text(random.randrange(1030),0.15,\"\\uf06c\",rotation=random.randrange(360),fontproperties=fp,size=13,zorder=random.randrange(2),color='darkorange')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Drop casual and registered feature**\n\nThere is also a high correlation between registered and cnt. Since cnt is the sum of registered and casual we can drop these two features as well."},{"metadata":{},"cell_type":"markdown","source":"## Dropping the columns to Final Dataframe"},{"metadata":{},"cell_type":"markdown","source":"We need to preserve the original dataset so we are dropping the columns and assigning it to a new dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop.extend(['dteday','casual','registered'])\ncolumns_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droppping the columns as it is not required for our Predicting model\ndf_final = df.drop(columns=columns_to_drop).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change the oridnal categorical variable back to default category\ndf_final.mnth = df_final.mnth.astype('object').astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Before we start with our Regression model we have to fix our data for our regression model to fit the line for the feature values.\n\nWe have most of our data as categorical features which needs to be converted to numberical values. Factorizing the features wouldn't be good idea reather we could create dummy variables for the categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the dataset we can see that season,mnth,weekday,weathersit has different levels that can be converted to dummy variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the list of dummy variables\ndummy_vars = ['season','mnth','weekday','weathersit','week_month']\n\n# creating dummy vairable for the selected categories\nfor feature in dummy_vars:\n    # dropping the first column from the dummy feature as it would give an redundant correlation\n    dummy = pd.get_dummies(df_final[feature],drop_first=True)\n    df_final = pd.concat([df_final,dummy],axis=1)\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now remove the original categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping categorical features\ndf_final.drop(dummy_vars,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WE have converted all our categorical data into numerical values with one-hot encoding. This will help in regression"},{"metadata":{},"cell_type":"markdown","source":"# Prepare Dataset for Model"},{"metadata":{},"cell_type":"markdown","source":"As the data is ready our first step is to perform test-train split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to keep our trianing set split on same data set we can specify rand seed sonstant\nnp.random.seed(0)\n\n# lets split our data set into 75% train and 25% test data\ndf_train, df_test = train_test_split(df_final,test_size=0.25,random_state=100) # specifying either on one size is enough","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview the train set\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview the test set\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape,df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling the features"},{"metadata":{},"cell_type":"markdown","source":"we need to scale few features as they are not in a comaprable range. If we miss to scale the coefficeints obtained will be very large/small to other feature coeffiecients."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create scaler object\nscaler = MinMaxScaler()\n\n# picking the features that has more varience in range\nnumerical_features = ['atemp','hum','windspeed','cnt']\n\n# fitting the dataset to get min-max\nscaler.fit(df_train[numerical_features])\n\n# performing scaler transform\ndf_train[numerical_features] = scaler.transform(df_train[numerical_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview the scaled data in train set \ndf_train[numerical_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statisticl view of the scaled data in train set \ndf_train[numerical_features].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating predictor X and Target variable y"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pop target varaible and set it to target training data set\ny_train = df_train.pop('cnt')\n# set the training data set for predictor variables\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the total number of features\nlen(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have 28 columns but we dont neeed all the columns to bild a good model it may lead to overfitting.\n\nFirst lets build the linear regressor to find the intercept and coefficient and fit it to RFE"},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create linear Regression Model\nlm = LinearRegression()\n\n# fit the data to our model\nlm.fit(X_train,y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the intercept\nlm.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the coefficients\nlm.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create an **RFE** model to automate the feature selection for our dataset that helps us identify the features that has higher impact on our target variable. Using RFE we can reduce the number of features to **15**"},{"metadata":{},"cell_type":"markdown","source":"# RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create RFE for our Linear Regressor Model\nrfe = RFE(lm,15)\n\n# fit the train data to RFE\nrfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets view the feature rank the rank with number greater than one can be dropped which is shown in the selected column .ie the column selected is provided with a flag True is selected and false is rejected"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# print the rank of each features\npd.DataFrame(zip(X_train.columns,rfe.support_,rfe.ranking_),columns=['Feature','Selected','Rank']).sort_values('Rank')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lest extract the selected columns by RFE\nselected_columns = X_train.columns[rfe.support_]\nselected_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rejected columns suggested by RFE in our training data set\nX_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model"},{"metadata":{},"cell_type":"markdown","source":"Lets sort the columns based on correlation value to add the best correlated column to the model one by one"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a correlation list for the selected features with cnt(target variable) \ncorr_cols = list(selected_columns)\ncorr_cols.append('cnt')\ncorr_cols = df_final[corr_cols].corr().iloc[:,-1:].sort_values('cnt',ascending=False).drop('cnt',axis=0)\ncorr_cols = corr_cols.reset_index()\ncorr_cols = corr_cols[corr_cols.cnt>0].append(corr_cols[corr_cols.cnt<0].sort_values('cnt'))\n\n# preview the result\ncorr_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build Model in Statsmodel API for detailed analysis"},{"metadata":{},"cell_type":"markdown","source":"Now lets build this model for the selected features using Stats package"},{"metadata":{},"cell_type":"markdown","source":"Creating predefind function to make it reusable code"},{"metadata":{},"cell_type":"markdown","source":"**OLS model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(X_train_rfe):\n    # adding a constant variable for intercept\n    X_train_rfe = sm.add_constant(X_train_rfe)\n\n    # Initialize an OLS model for our dataset and fit the data to model\n    lm = sm.OLS(y_train,X_train_rfe).fit()\n\n    # view the summary of the model for selected features\n    print(lm.summary())\n\n    return lm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VIF Analysis\nThe VIF score should be below 5 for a good model so lets remove the features one by one"},{"metadata":{"trusted":true},"cell_type":"code","source":"def VIF(X_train_rfe):\n    # create a dummy dataframe\n    vif = pd.DataFrame()\n    \n    # extract the column values to vif features column value\n    vif['Features'] = X_train_rfe.columns\n    \n    # calculate vif for the train data for the added features\n    vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\n    \n    # round the value to 2 decimals\n    vif['VIF'] = round(vif['VIF'], 2)\n    \n    # sort values by hightevif value first\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    \n    # print vif table\n    display(vif)\n    \n    # retrun vif object\n    return vif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting an Approach to build the model"},{"metadata":{},"cell_type":"markdown","source":"We have two way to build a model to find the best features selected by RFE that would fit.\n<ul><li>Dropping a feature one by one from the model built with 15 features until it shows good performane without overfitting</li><li>Adding a feature one by one to the model until it shows a good performance metrics</li></ul>\n\nLets add features one by one and build our model"},{"metadata":{},"cell_type":"markdown","source":"# Logic to build with best features"},{"metadata":{},"cell_type":"markdown","source":"Lets create a custom logic based on the below condition\n\n<table>\n<tr><th style=\"text-align:center\">Order</th><th style=\"text-align:center\">P-value</th><th style=\"text-align:center\">VIF</th><th style=\"text-align:center\">Action</th></tr>\n<tr><td>1</td><td>High</td><td>High</td><td>Drop these columns First</td></tr>\n<tr><td>2</td><td>High</td><td>Low</td><td>Drop these columns one by one, because this could lower the VIF values of other columns to prevent it from being dropped in next step </td></tr>\n<tr><td>3</td><td>Low</td><td>High</td><td>Drop the colums with VIF greater than 5</td></tr>\n<tr><td>4</td><td>Low</td><td>Low</td><td>Keep these features</td></tr>\n</table>\t"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a empty data frame for xtrain and vif\nX_train_rfe = pd.DataFrame()\nvif = pd.DataFrame()\n\n# creating this object to ignore vif for a single feature\ncount = 1\n\n# created this varible to stop the outer loop of adding futher features for model\nstop = False\n\n# iteratre over the columns in order\nfor i,v in corr_cols.iterrows():\n    \n    # extract the first column name to be added\n    col = v.values[0]\n    \n    # add the column to the traing data set\n    X_train_rfe[col] = X_train[col]\n    \n    # rebuild the model again to ckeck for high vifs and p-values \n    # once a feature is dropped on the above conditions after adding \n    # the new feature from the previous step to the model\n    while True:\n        # build the model\n        lm = build_model(X_train_rfe)\n        \n        # ignore vif and p-value check since there will\n        # be only 1 column on first iteration\n        if count != 1:\n            \n            # calculate VIF\n            vif = VIF(X_train_rfe)\n\n            # if the model reaches required r2 score stop the model from executing furher steps\n            if lm.rsquared >= 0.80:\n                stop = True\n                break\n            \n            # Check if the p-value if high\n            if (lm.pvalues > 0.05).sum() > 0:\n                \n                # extract feature fo high p-value\n                feature = lm.pvalues[lm.pvalues > 0.05].index\n                \n                # check if this feature is not const\n                if feature[0] != 'const':\n                    \n                    # if the VIF value is aslo high drop this columns first\n                    if feature[0] in vif.loc[vif.VIF > 5,'Features']:\n                        X_train_rfe.drop(feature[0],axis=1,inplace=True)                # order 1\n                    else:\n                        # if only the p-value is high drop it\n                        X_train_rfe.drop(feature[0],axis=1,inplace=True)                # order 2\n                \n                # if the p-value column is 2nd in the list extract \n                # that feature name to drop if from dataset if there is \n                # a third value with high p-value it will be\n                # validated in the next loop after rebuild on dropping the current feature\n                elif (feature[0] == 'const') & (len(feature) > 1):\n                    X_train_rfe.drop(feature[1],axis=1,inplace=True)                    # order 2\n            \n            # if VIF value is high drop it\n            if (vif.VIF > 5).sum() > 0:\n                X_train_rfe.drop(vif.loc[vif.VIF > 5,'Features'],axis=1,inplace=True)   # order 3\n            else:\n                break                                                                   # order 4\n        else:\n            break\n    # stop the process\n    if stop:\n        break\n    \n    # increment count on adding new feature\n    count = count + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The iteration stops when the model reaches above 0.8 but on the last loop of iteration we can see that atemp VIF increased above 5 once the feature windspeed was added, so instead of dropping atemp lets drop **windspeed** as atemp has higher impact/explaenation to the model.\nAnd Lets add the next feature from the corr_cols list"},{"metadata":{},"cell_type":"markdown","source":"## Manual Feature selection testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # dropping windspeed as it influenses other variable's VIF with less contribution on model output\n    X_train_vif = X_train_rfe.drop('windspeed',axis=1)\nexcept:\n    print(\"windspeed is not available in the train dataset\")    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets add the next feature after windspeed from the corr_cols which is **cloudy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding cloudy to the dataset for testing the model output\nX_train_vif['cloudy'] = X_train['cloudy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the model\nlm1 = build_model(X_train_vif);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the VIF for our new model\n_ = VIF(X_train_vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nThe model has a good r-square value but it feels like we have overfitted the model with too much columns lets try to remove two columns from the training dataset"},{"metadata":{},"cell_type":"markdown","source":"## Dropping few columns to prevent overfitting of the model"},{"metadata":{},"cell_type":"markdown","source":"lets try some manual dropping as **trial and error** method to see if dropping these columns does not affect the model and provided decent output.\n\n"},{"metadata":{},"cell_type":"markdown","source":"we have three freatures from same category in our final model **summer, winter and spring**. It would be fine to drop one of them"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # dropping summer to the dataset for testing the model output\n    X_train_vif1 = X_train_vif.drop('summer',axis=1)\n\n    #rebuild model\n    lm2 = build_model(X_train_vif1)\n\n    # check the VIF for our new model\n    _ = VIF(X_train_vif1)\nexcept:\n    print(\"summer is not available in the train dataset\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nThe model give a decent R-square even after dorpping so lets pick another feature to drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # dropping winter from the dataset for testing the model output\n    X_train_vif2 = X_train_vif1.drop('winter',axis=1)\n\n    # rebuild model\n    lm3 = build_model(X_train_vif2)\n\n    # check the VIF for our new model\n    _ = VIF(X_train_vif2)\nexcept:\n    print(\"winter is not available in the train dataset\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vif2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nThe model looks good with 8 features , the **r-Square** value is above 0.8 which i assume to be a good model and on comparing the value of **F-Staticstics** it has a good value compared to other models.\n\nWiith this final model lets proceed to Prediction and residual analysis."},{"metadata":{},"cell_type":"markdown","source":"# Final Model"},{"metadata":{},"cell_type":"markdown","source":"Now we need to know if the error terms are normally distributed lets plot a histogram and check"},{"metadata":{},"cell_type":"markdown","source":"## Using Sklearn library for prediction\n\nFor Prediction and error we can use **Sklearn** model as it has many validation terms.\n\nWe already know the best features for our model which was derieved from our best model **lm3** so we can take and use features  from that model with X_train_vif2 as final data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating linar regressor\nlm_final = LinearRegression()\n\n# Initializing our model with the best features for prediction\n# which was obtained from lm3 model\nlm_final.fit(X_train_vif2,y_train)\n\n# Predcting values in our train data set\ny_train_pred = lm_final.predict(X_train_vif2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction and Residual Analysis for the final model on Trainset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram for the error terms\nsns.distplot((y_train - y_train_pred), bins = 20)\n# set title for plot\nplt.title('Error Terms', fontsize = 20)                  \n# set x-axis label\nplt.xlabel('Errors', fontsize = 18)                     \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Intercept:', lm_final.intercept_)\nprint('Coefficients:', lm_final.coef_)\nprint('Mean squared error (MSE): {:.2f}'.format(mean_squared_error(y_train, y_train_pred)))\nprint('Coefficient of Determination (R2): {:.2f}'.format(r2_score(y_train, y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validating model on testset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling test datset\ndf_test[numerical_features] = scaler.transform(df_test[numerical_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting target column\ny_test = df_test.pop('cnt')\n\n# taking predictor feactures for test set\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filetiring our test data set with the best columns for prediction\nX_test_vif = X_test[X_train_vif2.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making prediction on the final model\ny_test_pred = lm_final.predict(X_test_vif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation on testset"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\n# Plot title \nplt.title('y_test vs y_test_pred', fontsize=20,pad=10)\n# plot X-label and y-label\nplt.xlabel('y_test', fontsize=18)                          \nplt.ylabel('y_test_pred', fontsize=16)    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Trainset Mean squared error (MSE): {:.2f}'.format(mean_squared_error(y_train, y_train_pred)))\nprint('Trainset Coefficient of Determination (R2): {:.2f}'.format(r2_score(y_train, y_train_pred)))\nprint('\\nTestset Mean squared error (MSE): {:.2f}'.format(mean_squared_error(y_test, y_test_pred)))\nprint('Testset Coefficient of Determination (R2): {:.2f}'.format(r2_score(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# lm3 is the best model\nprint(lm3.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Model Conslusion </h1><br>\n<h2>\nTrain r2 score : 0.82<br/>    \n    \nTest r2 score : 0.80\n</h2>\n<h3>\nWe can conclude with the best fit line as:\n</h3>\n<h4>    \ncnt= const * 0.218 + atemp * 0.3880 + yr * 0.2345 + Sep * 0.0695 + workingday * 0.0565 + Sat * 0.0628 + spring * -0.1624 + rain * -0.2943 + cloudy * -0.0780\n</h4>\n\nThe demand of bikes is mainly influenced by actual temperature,year, september, workingday, saturday.<br><br>\nThe demand of bikes mostly decreases with attributes spring, light rain/light snow and cloudy"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}