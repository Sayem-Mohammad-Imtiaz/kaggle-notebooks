{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hiii!! , this is my first kaggle notebook and i am quite excited to share my ideas with the community, being a newbie i might do some cranky stuff below, sorry for it :) , thanks for providing me with this oppurtunity."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# **IMPORTING OUR DATA:**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data = pd.read_csv('../input/public-tenders-romania-20072016/contracts.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FIRST IMPRESSIONS:"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A initial look of the data can provide us with some insights to proceed with.First lets try to get some quantitative and qualitative understanding about missing data points so that we can come up with ideas to clean it up."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"missing_data_percentage =  data.isnull().sum().sum()/np.product(data.shape)*100\nprint(\"Missing Data: \",missing_data_percentage,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OBSERVATION**: A whooping 25 % of our data cells are unfilled. \n\nWhile it may sound bad , this is where we must question the context of the missing data,we must answer a few questions like:\n \n 1. In the context of that particular column, does NaN mean something else?\n \n 2. is it just a missing valuable data point?\n \n 3. Why is that particular data point unfilled?\n \n \nThe answers to these questions will be the clue for us on what steps to follow when we actually start working with the data."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning up the Data:"},{"metadata":{},"cell_type":"markdown","source":"# In the context of that particular column, does NaN mean something else?"},{"metadata":{},"cell_type":"markdown","source":"**YES:**  In this Dataset the columns that contain information about Yes/No type questions refers a Yes as *DA* and a No as *NaN* ,so let us try to remove those null entries by replacing such columns with apt boolean data types.  \n\nLets start by converting the yes/no type series to a boolean series by replacing **DA/NAN  as  True/False**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\ndata['EU_Funds'].replace(np.NaN,False,inplace =True)\ndata['EU_Funds'].replace('DA',True,inplace =True)\n\ndata['Periodic_Contract'].replace(np.NaN,False,inplace =True)\ndata['Periodic_Contract'].replace('DA',True,inplace =True)\n\ndata['Subcontracted'].replace(np.NaN,False,inplace =True)\ndata['Subcontracted'].replace('DA',True,inplace =True)\n\ndata['With_Electronic_Auction'].replace(np.NaN,False,inplace =True)\ndata['With_Electronic_Auction'].replace('DA',True,inplace =True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"missing_data_percentage =  data.isnull().sum().sum()/np.product(data.shape)*100\nprint(\"Missing Data: \",missing_data_percentage,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That has reduced our null entry  by around **10%** , not only that but it also has made the columns boolean based which might turn out to be hugely helpful when it comes to data analysis"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data.isnull().sum().sort_values(ascending=False).head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# is it just a missing valuable data point?"},{"metadata":{},"cell_type":"markdown","source":"Some of our columns fall in this category too , they are just plain unfilled data point which might be extremly valuable during data analysis,\nto get the most out of this dataset it becomes essential for us to find ways to extract and fill in those missing data points. For instance the Winner Country tells us about the country of the company which won the tender offer , around 79733 datapoints about Winner country is missing ,it is so huge that it can make a considerable amount of difference when it comes to data analytics.\n\nBefore we start with this idea it is important for us to procure the dataframe and remove inconsistancies due to the way our data is structured\nfor ex a company named \"aaaaa s.n\" and \"aaaaa S.N\" are grouped as different entities , we will fist try to come up with a solution for this."},{"metadata":{},"cell_type":"markdown","source":"**Plan:** To try to extract raw data from columns so that Things like spaces,case,punctuations dont make a difference when we group data"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"missing_data_due_to_winner_country_percentage =  data.isnull().sum()[\"Winner_Country\"]/np.product(data.shape)*100\nprint(\"Missing Data: \",missing_data_due_to_winner_country_percentage,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data[\"Winner_Country\"].replace(np.NaN,\"__\",inplace =True)\ndata[\"Winner_Country\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have replaced all the NaN s in the Winner_Country column by a placeHolder **__** so that it is not neglected when called upon by some pandas fxn's  "},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data[\"Winner_City\"].replace(np.NaN,\"NO_CITY_STAT\",inplace =True)\ndata[\"Winner_City\"].replace(\"-\",\"NO_CITY_STAT\",inplace =True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have also replaced Nan entries in Winner_City column by **NO_CITY_STAT** for similar reasons"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"\ndisplay(data[\"Winner_City\"].value_counts().head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import string\ndata[\"Raw_City_Data\"] = data[\"Winner_City\"].str.strip().str.translate(str.maketrans('', '', string.punctuation)).str.replace(\" \",\"\").str.upper()\n\ndisplay(data[\"Raw_City_Data\"].value_counts().head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(\"Relative change in size of group BUCURESTI:\",(351435-333776)/333776*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By doing a little bit of string manipulation, we have removed all the spaces,punctuations and special characters from the Winner_City column \ndoing this we get what i would call as a **RAW DATA POINT** which conserves useful data but lacks the luxury of being a well punctuated and \nspaced string, this raw datapoint removes problems like case sensitivity,equality of city name being disrupted by things like spaces, commas ,periods.\n\nAnd as we see above , we notice how Bucuresti a city in Romania had **333776** entries in our first output , but when we group them in terms of RAWDATA we see that BUCURESTI has 351435 entries , around **20000** datapoints was being missed out in the first due to complications byspaces,case sensitivity and punctuation that accounts for a relative change of around **5%**"},{"metadata":{},"cell_type":"markdown","source":"We will do something similar to the columns winner and contract_type"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data[\"Raw_Winner_Data\"] = data[\"Winner\"].str.strip().str.translate(str.maketrans('', '', string.punctuation)).\\\nstr.replace(\" \",\"\").str.upper()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"display(data[\"Winner\"].value_counts()[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"display(data[\"Raw_Winner_Data\"].value_counts().head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data[\"Raw_Contract_Title\"] = data[\"Contract_Title\"].str.strip().str.translate(str.maketrans('', '', string.punctuation)).\\\nstr.replace(\" \",\"\").str.upper()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"display(data[\"Contract_Title\"].value_counts().head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"display(data[\"Raw_Contract_Title\"].value_counts().head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(\"Relative change for MEDICAMENTE:\",(14627-7188)/7128*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thats is a whooping **Relative change of 104%** , this is by far one of the best examples on why this approach helps\nthis approach can be extended to many such columns in our dataframe , so that a more cured version of the dataset can be extracted \n\nNow that we have made our database more grouping friendly we can start adressing the null problem . to start with lets take the Winners_Country column which has 79733 null points and let us try to come up with an idea to fill in these spaces."},{"metadata":{},"cell_type":"markdown","source":"**Plan:** we can collect  the city data of data points with missing country, and check if there exists a entry with country value which has the same city value so that we can fill in the country of the missing value."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data[\"Winner_Country\"].value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"list_of_countries = list(data[\"Winner_Country\"].unique())\nlist_of_countries.remove(\"__\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"for x in list_of_countries:\n    x_cities = list(data[data[\"Winner_Country\"] == x][\"Raw_City_Data\"].unique())\n    if \"NOCITYSTAT\" in x_cities:\n        x_cities.remove(\"NOCITYSTAT\")\n    data.loc[(data[\"Winner_Country\"]==\"__\") & (data[\"Raw_City_Data\"].isin(x_cities)),\"Winner_Country\"] = x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"display(data[\"Winner_Country\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(\"Relative change in Null values of Winner_Country\",(-1203+79733)/79733*100 , \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have reduced the no of null points by a whooooooping **98.5%** , from having around **79000 null points** , we now have only **1203 null points**,we have by comparing and extracting data from other columns **added another 78000 datapoints** to our data frame , this added datapoints are not random but appropriate datapoints which enriches our dataframe.\n\nlets check if we can do something about any other columns"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data.isnull().sum().sort_values(ascending=False).head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look up we can observe how the column **EU_Fund** is **number 1** in our list , to understand why it is so , it is important for us to look into another data column called as the **EU_Funds** which is a **boolean column** saying wFlag to indicate if the tender is about accessing EU Funds, it is a yes/no boolean column on the other hand  the column EU_Fund says about that specific fund name which was used."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"EU_Funds\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see how around 860000 entries of data points did not use a EU fund and hence those columns are left empty , so all we want to do is to check if the tender uses a EU fund and incase if it is False we can replace the null Value with a string like **\"NO EU Funds Used\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[~data[\"EU_Funds\"],\"EU_Fund\"] = \"NO EU FUNDS USED\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum().sort_values(ascending=False).head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Relative change in no of null points for EU funds: \",(875511-15991)/875511*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have reduced the no of null entries in EU_Funds col by 98.17% and filled it with good informative data.\n\nWhile this may sound fancy,what we have essentialy did is just replace NaN s with I Dont Know, while this might provide some extra layer of info, to do this or not will be completley an individuals choice. The idea of using other columns to extract missing values cannot be extended to fields like Financing_Type,Contracting_Authority_Type,Garantee_Deposits ... and so on , Filling them with I dont know's will not be a much of a improvement to the quality of our dataset.\n\nThe series of columns  Participation_Announcement_Number,Participation_Announcement_Date,Participation_Estimated_Value, Participation Estimated_Value_Currency are some of the least data rich field in the dataset, they are also the ones with consiting data which cannot be filled in by looking into other columns"},{"metadata":{},"cell_type":"markdown","source":"# CPV CODES:"},{"metadata":{},"cell_type":"markdown","source":"INFO FROM [Wikipedia](http://en.wikipedia.org/wiki/Common_Procurement_Vocabulary)"},{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION:\n\nThe Common Procurement Vocabulary (CPV) has been developed by the European Union to facilitate the processing of invitations to tender published in the Official Journal of the European Union (OJEU) by means of a single classification system to describe the subject matter of public contracts. It was established by Regulation (EC) No 2195/2002 of the European Parliament and of the Council on the Common Procurement Vocabulary (CPV) and amended by European Commission Regulation (EU) No. 213/2008  issued on 28 November 2007."},{"metadata":{},"cell_type":"markdown","source":"# STRUCTURE:\nCPV codification consists of a main vocabulary which defines the subject of the contract, and a supplementary vocabulary to add further qualitative information. The main vocabulary is based on a tree structure made up with codes of up to 9 digits (an 8 digit code plus a check digit). This combination of digits is associated with a wording that describes the type of supplies, works or services defining the subject of the contract. A Call for Tender is quite often described by more than one CPV Code, aiming to give a better and more detailed description of the object of the contract. Commercial organisations promoting public contracts to their members or readers generally use CPV codes to identify business sectors likely to be interested in specific tenders, along with NUTS Codes which indicate the country and region within which the contract is to be performed.\n\nMain Vocabulary classification structure\n\nThe numerical code consists of 8 digits, subdivided into:\n\nDivisions: first two digits of the code XX000000-Y.\n\nGroups: first three digits of the code XXX00000-Y.\n\nClasses: first four digits of the code XXXX0000-Y.\n\nCategories: first five digits of the code XXXXX000-Y."},{"metadata":{},"cell_type":"markdown","source":"# REMARKS:"},{"metadata":{},"cell_type":"markdown","source":"The CPV_Code column in our data is by far the most important part of the dataset doing a bit of research on how cpv codes work, we come to know how much data rich the codes are , they supply a plethora of information , sadly our dataset doesnot use the power of csv code to its fullest "},{"metadata":{},"cell_type":"markdown","source":"# Extending the Dataset:"},{"metadata":{},"cell_type":"markdown","source":"From [Here](http://simap.ted.europa.eu/cpv)\nwe can get a xls file containing a list of cpv codes and thier defenitions but when i did this first time i found out a few codes missing from the xls file which was found in a pdf file from the same site so i went on  to write a python script to extract data from that pdf file and wrote it back in a good csv form so that it can be used effectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"code = pd.read_excel('../input/cpv-codes/code2003.xls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"code.columns = [\"2003_CODE\",\"2003_DESC\",\"2007_CODE\",\"2007_DESC\",\"q\",\"w\",\"e\",\"r\",\"t\",\"y\",\"u\",\"i\",\"o\"]\ncode.drop(columns=[\"q\",\"w\",\"e\",\"r\",\"t\",\"y\",\"u\",\"i\",\"o\"],inplace=True)\ncode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like our excel data requires a bit of cleanup"},{"metadata":{"trusted":true},"cell_type":"code","source":"stat_2003 = code[[\"2003_CODE\",\"2003_DESC\"]].copy()\nstat_2003.dropna(inplace=True)\nstat_2007 = code[[\"2007_CODE\",\"2007_DESC\"]].copy()\nstat_2007.dropna(inplace=True)\nstat_2007.drop_duplicates(subset=['2007_CODE'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat_2003.columns = [\"CODE\",\"DESC\"]\nstat_2007.columns = [\"CODE\",\"DESC\"]\ncode_stats = pd.concat([stat_2003, stat_2007], ignore_index=False, sort=False)\ncode_stats.drop_duplicates(subset=['CODE'],inplace=True)\ninfo = code_stats[code_stats[\"CODE\"].str.contains(\"000000\")].copy()\ninfo[\"KEY\"] = \"\"\ninfo[\"KEY\"] = info[\"CODE\"].str[:2]\ninfo.drop_duplicates(subset=['KEY'],inplace=True)\ninfo.set_index(\"KEY\",inplace = True)\ndiv_dict = info.to_dict('index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have merged both **2003 and 2007** cpv codes into one single dataframe which has entries of the form **XX000000** , which describes the division of the tender we have also converted it to a dictionary which returns the division description when passed on the code."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(how='all',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_division(code):\n        if len(code)>2:\n            u = str(div_dict.get(code[:2])['DESC'])\n            return str(u)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have created a fxn that gets a key and returns the appropriate value from our dictionary,We will now create a new column called the **Division**  which will give us the division of that particular tender."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Division\"] = \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Division\"] =  data.apply(lambda x:find_division(x[\"CPV_Code\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Division\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have added the first layer of data to our data frame extracted from the cpv codes, a division gives us a broad idea about our tender. similarly we will create groups , who are more descriptive and specific than a division, and so on we will create a class and then a category column too"},{"metadata":{"trusted":true},"cell_type":"code","source":"info.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info = code_stats[code_stats[\"CODE\"].str.contains(\"00000\")].copy()\ninfo[\"KEY\"] = \"\"\ninfo[\"KEY\"] = info[\"CODE\"].str[:3]\ninfo.drop_duplicates(subset=['KEY'],inplace=True)\ninfo.set_index(\"KEY\",inplace = True)\ngroup_dict = info.to_dict('index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_grp(code):\n    if len(code)>3:\n        u = str(group_dict.get(code[:3])['DESC'])\n        if u is not None:\n            return u\n        else:\n            return \"None\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Group\"] = \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Group\"] =  data.apply(lambda x:find_grp(x[\"CPV_Code\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Group\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code for creating a class column\n\ninfo = code_stats[code_stats[\"CODE\"].str.contains(\"0000\")].copy()\n\ninfo[\"CODE\"] = info[\"CODE\"].str[:4]\ninfo.drop_duplicates(subset=['CODE'],inplace=True)\ninfo.set_index(\"CODE\",inplace = True)\nclass_dict = info.to_dict('index')\n\ndef find_class(code):\n    if len(code)>3:\n        u = class_dict.get(code[:4])\n        if u is not None:\n            return u[\"DESC\"]\n        else:\n            return \"None\"\n\ndata[\"Class\"] = \"_\"\ndata[\"Class\"] =  data.apply(lambda x:find_class(x[\"CPV_Code\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#code for creating a category column\n\ninfo = code_stats[code_stats[\"CODE\"].str.contains(\"000\")].copy()\n\ninfo[\"CODE\"] = info[\"CODE\"].str[:5]\ninfo.drop_duplicates(subset=['CODE'],inplace=True)\ninfo.set_index(\"CODE\",inplace = True)\ncateg_dict = info.to_dict('index')\n\ndef find_categ(code):\n    if len(code)>3:\n        u = categ_dict.get(code[:5])\n        if u is not None:\n            return u[\"DESC\"]\n        else:\n            return \"None\"\n\ndata[\"Category\"] = \"_\"\ndata[\"Category\"] =  data.apply(lambda x:find_class(x[\"CPV_Code\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And to end it all we will add a final level of data which is the most specific of all we will call it **CPV_Data**, this will contain the most specific descriptive details of the tender,when i did it for the first time i found out that the xls file which we used missed a few data points, so i scrapped of data from pdf file in the same site wrote a script to change it to csv and we will now use that here "},{"metadata":{"trusted":true},"cell_type":"code","source":"stat_2003 = code[[\"2003_CODE\",\"2003_DESC\"]].copy()\nstat_2003.dropna(inplace=True)\n\nstat_2007 = code[[\"2007_CODE\",\"2007_DESC\"]].copy()\nstat_2007.dropna(inplace=True)\n\nstat_2007.drop_duplicates(subset=['2007_CODE'],inplace=True)\nstat_2007.set_index(\"2007_CODE\",inplace=True)\n\nstat_2003.set_index(\"2003_CODE\",inplace=True)\n\n\ndict_2003 = stat_2003.to_dict('index')\ndict_2007 = stat_2007.to_dict('index')\n\ncode1 = set(data[\"CPV_Code\"].unique())\ncode2 = set(stat_2007.index)\nz = code1.intersection(code2)\n\ndef extract_stat(code):\n    if code in z:\n        u =  dict_2007.get(code)\n        return u['2007_DESC']\n    else:\n        return \"None\"\n    \n    \ndata[\"CPV_Data\"] = \"\"\ndata[\"CPV_Data\"] =  data.apply(lambda x:extract_stat(x[\"CPV_Code\"]),axis=1)\ndata[\"CPV_Data\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have extracted data of 2007 based cpv codes from the excel sheet in a very similar fashion , we can see how we still miss around 62000 datapoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"code1 = set(data[\"CPV_Code\"].unique())\ncode2 = set(stat_2003.index)\nz =code1.intersection(code2)\ndef second_extract_stat(code,data):\n    if str(data) == \"None\":\n        if code in z:\n            v= str(dict_2003.get(code)['2003_DESC'])\n            return v\n        else:\n            return str(data)\n    else:\n        return str(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"CPV_Data\"] =  data.apply(lambda x:second_extract_stat(x[\"CPV_Code\"],x[\"CPV_Data\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"CPV_Data\"]==\"None\"][\"CPV_Data\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"after extracting the 2003 based cpv codes from our .xls file we can still see that we are missing around 1300 datapoints , this is where our pdf based .csv file comes in."},{"metadata":{"trusted":true},"cell_type":"code","source":"cpv_code = pd.read_csv(\"../input/cpv-codes/code.csv\")\ncpv_code.set_index('CODE',inplace=True)\ncpv_data_dict = cpv_code.to_dict('index')\n\ncode1 = set(data[\"CPV_Code\"].unique())\ncode2 = set(cpv_code.index)\nz =code1.intersection(code2)\ndef thirdExtract(code,data):\n    if str(data) == \"None\":\n        if code in z:\n            v= str(cpv_data_dict.get(code)['DESC'])\n            return v\n        else:\n            return str(data)\n    else:\n        return str(data)\n\n    \ndata[\"CPV_Data\"] =  data.apply(lambda x:thirdExtract(x[\"CPV_Code\"],x[\"CPV_Data\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"CPV_Data\"]==\"None\"][\"CPV_Data\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**WOW that is a 0% null entry column**\n\nAfter reading in the csv file , we can see how no data point has None value to it , every single data point in our dataframe has a CPV Data to it, while fields like contract type are prepresent in the dataset there are around **18000 Null entries** to it, moreover the information in such field are incomparable to what is provided by the cpv codes\n\nWith columns like division,classes,categories we can not only analyse the dataset as a whole but also as seperate groups , for example we can now do a brief analysis on Tenders for the agriculture department,what % of the tenders were for rice,what is the avg value of tenders to **Uranium ores** and so on\n\nUsing this idea of cpv codes will enrich  our dataset with a plhetora of information and data"},{"metadata":{},"cell_type":"markdown","source":"# WHATS UP NEXT?"},{"metadata":{},"cell_type":"markdown","source":"there are lots of things that i have missed when it comes to cleaning the dataset , things like Winner_Adress has a lot of issues when it comes to data grouping, because of spaces,punctuation and case sensitivity, so maybe using the idea of RAWDATA will improve the data set,things like legislation ID are of little use as they are mostly Nulls,data scrapping a few UN/EU sites will enrich the dataset more than anything.\n\n\n        \"The greatest challenges humans face throw-out their lives are two\"\n\n                        1. the challenge of where to start\n\n                        2. the challenge of when to stop. \n"},{"metadata":{},"cell_type":"markdown","source":"THANK YOUUUUU!!!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}