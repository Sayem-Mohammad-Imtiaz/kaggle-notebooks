{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Categorical Encoding for Feature Eng. Course"},{"metadata":{},"cell_type":"markdown","source":"# Motivation and Strategy\n\nMachine learning for classification problem as presented in https://www.kaggle.com/matleonard/categorical-encodings\nThis kernel target is for timeseries dependent data (i.e. timeseries as index).\nCross-validation timeline should not be used carelessly, since we don't want data leakage.\nWe need to sort the data to separate training, validation and test set.\n\nGeneral workflow is:\n1. Preprocess data, sort by timeseries\n2. Do Train/Validation/Test split in chronologically consistent manner\n3. Try various encoder schemes (e.g. label, count, target, catboost)\n4. Apply feature enginering + feature selection (optional)\n5. Apply model (e.g. lgbm for the moment, will include random forest and xgboost)\n\n\n#### Update 2020-01-07 --> worked on encoding for categorical features.\n#### Update on 2020-01-09 --> organize the encoding functions to a single convenient function\n\n#### Credit: inspired by Kaggle's Course on Feature Engineering, part Categorical Encoding\nhttps://www.kaggle.com/matleonard/categorical-encodings"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing, metrics\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions to Test Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to Test Models\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    \"\"\" Splits a dataframe into train, validation, and test sets. First, orders by \n        the column 'click_time'. Set the size of the validation and test sets with\n        the valid_fraction keyword argument.\n    \"\"\"\n\n    dataframe = dataframe.sort_values('click_time')\n    valid_rows = int(len(dataframe) * valid_fraction)\n    train = dataframe[:-valid_rows * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_rows * 2:-valid_rows]\n    test = dataframe[-valid_rows:]\n    \n    return train, valid, test\n\ndef train_model(train, valid, test=None, feature_cols=None):\n    if feature_cols is None:\n        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n                                           'is_attributed'])\n    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n    \n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    num_round = 1000\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n                    early_stopping_rounds=20, verbose_eval=False)\n    \n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n    print(f\"Validation AUC score: {valid_score}\")\n    \n    if test is not None: \n        test_pred = bst.predict(test[feature_cols])\n        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n        return bst, valid_score, test_score\n    else:\n        return bst, valid_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model\n\nWe do simple process on timestamps in dataset0, then\nwe save it as dataset1. \n\ndataset1 is the baseline dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset0.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset0 = pd.read_csv('../input/feature-engineering-data/train_sample.csv',\n                         parse_dates=['click_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### 1) Features from timestamps\n\ndataset0_index = 'click_time'\n\n### dataset1 is a dataframe columns with additional timestamp features day, hour, minute, and second\n### we use to represent time series elaboration\n# Add new columns for timestamp features day, hour, minute, and second\ndataset1 = dataset0.copy()\ndataset1['day']    = dataset1[dataset0_index].dt.day.astype('uint8')\ndataset1['hour']   = dataset1[dataset0_index].dt.hour.astype('uint8')\ndataset1['minute'] = dataset1[dataset0_index].dt.minute.astype('uint8')\ndataset1['second'] = dataset1[dataset0_index].dt.second.astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multi Encoders Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport category_encoders as ce\n\n# Function: multi_encoders_function(train, valid, categorical_features, target_column, method print_transform_request = False)\ndef multi_encoders_function(train, valid, categorical_features, target_column, method = 'label', print_transform_request = False):\n    print(\"These are the categorical features used: \", categorical_features)\n    print_before_encoder(train, valid, print_transform_request)\n    if method == 'label':\n        train_enc, valid_enc = my_label_encoder(train, valid, categorical_features)\n    elif method == 'count':\n        train_enc, valid_enc = my_count_encoder(train, valid, categorical_features)\n    elif method == 'target':\n        train_enc, valid_enc = my_target_encoder(train, valid, categorical_features, target_column)\n    elif method == 'catboost':\n        train_enc, valid_enc =my_catboost_encoder(train, valid, categorical_features, target_column)\n    print_after_encoder(train_enc, valid_enc, print_transform_request)\n    return train_enc, valid_enc\n\n# Function to Print Original Dataframe\ndef print_before_encoder(train, valid, print_transform_request):\n    if print_transform_request == True:\n        print(\"\")\n        print(\"These are the columns from original training dataframe: \")\n        print(train.iloc[0:4])\n        print(\"\")\n\n# Function to Print Transformed Dataframe\ndef print_after_encoder(train_encoded, valid_encoded, print_transform_request):\n    if print_transform_request == True:\n        print(\"\")\n        print(\"These are the columns on the transformed training dataframe: \")\n        print(train_encoded.iloc[0:4])  \n        print(\"\")\n\n# Label Encoder Function\ndef my_label_encoder(train, valid, categorical_features):\n    print(\"my_label_encoder function is called.\")\n    #\n    # Create a LabelEncoder instance\n    label_encoder = preprocessing.LabelEncoder()\n    #\n    # Create new columns in dataframe using preprocessing.LabelEncoder()\n    for feature in categorical_features:\n        encoded_feature_train = label_encoder.fit_transform(train[feature])    # apply label_encoder\n        train[feature + '_labels'] = encoded_feature_train                     # save the result to a differently named column, with '_labels'\n        encoded_feature_valid = label_encoder.fit_transform(valid[feature])    # apply label_encoder\n        valid[feature + '_labels'] = encoded_feature_valid                     # save the result to a differently named column, with '_labels'\n    #    \n    return train, valid\n\n# Count Encoder Function\ndef my_count_encoder(train, valid, categorical_features):\n    ######################### Count Encoder###############################\n    # Count encoding replaces each categorical value with the number of times it appears \n    # in the original dataset. \n    # For example, if the value \"GB\" occured 10 times in the country feature, then each \"GB\" \n    # would be replaced with the number 10.\n    ######################################################################\n    print(\"my_count_encoder function is called.\")\n    # Create a Count Encoder instance\n    count_enc = ce.CountEncoder(cols=categorical_features)\n    #\n    # Learn encoding from the training set\n    count_enc.fit(train[cat_features])\n    #\n    # Apply encoding to the train and validation sets as new columns\n    # Make sure to add `_count` as a suffix to the new columns\n    train_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix('_count'))    \n    valid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix('_count'))\n    #      \n    return train_encoded, valid_encoded\n\ndef my_target_encoder(train, valid, categorical_features, target_column):\n    ############################ Target Encoding ##################################\n    # Target encoding replaces a categorical value with the average value of the target for that value of the feature. \n    # For example, given the country value \"CA\", you'd calculate the average outcome for all the rows with country == 'CA', \n    # around 0.28. This is often blended with the target probability over the entire dataset to reduce the \n    # variance of values with few occurences.\n    ################################################################################ \n    print(\"my_target_encoder function is called.\")\n    #\n    # Create the target encoder. \n    target_enc = ce.TargetEncoder(cols=cat_features)\n    #\n    # Learn encoding from the training set. Use the the target column as target_column.\n    target_enc.fit(train[cat_features], train[target_column])\n    #\n    # Apply encoding to the train and validation sets as new columns\n    # Make sure to add `_target` as a suffix to the new columns\n    train_encoded = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n    valid_encoded = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n    #\n    return train_encoded, valid_encoded\n\ndef my_catboost_encoder(train, valid, categorical_features, target_column):\n    ########################### CatBoost Encoding #####################################\n    # CatBoost encoding. This is similar to target encoding in that it's based on the target probablity \n    # for a given value. However with CatBoost, for each row, the target probability is calculated only from the \n    # rows before it.\n    ###################################################################################\n    print(\"my_catboost_encoder function is called.\")\n    #\n    # Have to tell it which features are categorical when they aren't strings\n    cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n    #\n    # Learn encoding from the training set\n    cb_enc.fit(train[cat_features], train[target_column])\n    #\n    train_encoded = train.join(cb_enc.transform(train[cat_features]).add_suffix('_cb'))\n    valid_encoded = valid.join(cb_enc.transform(valid[cat_features]).add_suffix('_cb'))\n    #\n    return train_encoded, valid_encoded\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test the Various Encoder Performance with Various Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test multi_encoders_function  label, count, target, catboost\ncat_features = ['ip', 'app', 'device', 'os', 'channel']\ntarget_column = 'is_attributed'\nmethod_list = ['label', 'count', 'target', 'catboost']\n\n# warning: train_model function return three variables if used with train, valid, test as argument\n# for the moment, only use train, valid for the argument\n# because we only want to check the encoding method\ndictionary_encoder_result = {}\nfor method_i in method_list:\n    train, valid, test = get_data_splits(dataset1)\n    train, valid = multi_encoders_function(train, valid, cat_features, target_column, method_i , print_transform_request = False)\n    bst, valid_score = train_model(train, valid)\n    dictionary_encoder_result[method_i] = valid_score\n    print(dictionary_encoder_result)\n    print(\"\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result and Discussion\n\n### If Categorical Feature used = ['ip', 'app', 'device', 'os', 'channel'], count encoder have maximum score\n* Label Encoder score: 0.96201\n* Count Encoder score: 0.96494\n* Target Encoder score: 0.95405\n* Catboost Encoder score: 0.95742\n\n\n### If Categorical Feature used = ['app', 'device', 'os', 'channel'], most encoder have slightly similar score within <0.001.\n* Label Encoder score: 0.96205\n* Count Encoder score: 0.96279\n* Target Encoder score: 0.96276\n* Catboost Encoder score: 0.96255\n\n### If Categorical Feature used = ['ip', 'device', 'os', 'channel'], count encoder have maximum score\n* Label Encoder score: 0.96201\n* Count Encoder score: 0.96480\n* Target Encoder score: 0.95143\n* Catboost Encoder score: 0.95643\n\n### If Categorical Feature used = ['ip', 'app', 'os', 'channel'], count encoder have maximum score\n* Label Encoder score: 0.96204\n* Count Encoder score: 0.96479\n* Target Encoder score: 0.95352\n* Catboost Encoder score: 0.95754\n\n### If Categorical Feature used = ['ip', 'app', 'device', 'channel'],count encoder have maximum score\n* Label Encoder score: 0.96201\n* Count Encoder score: 0.96495\n* Target Encoder score: 0.95352\n* Catboost Encoder score: 0.95719\n\n### If Categorical Feature used = ['ip', 'app', 'device', 'os'], count encoder have maximum score\n* Label Encoder score: 0.96233\n* Count Encoder score: 0.96488\n* Target Encoder score: 0.95293\n* Catboost Encoder score: 0.95688\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}