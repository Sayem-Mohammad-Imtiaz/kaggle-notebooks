{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split # train-test split\nfrom sklearn.metrics import confusion_matrix, classification_report # classification metrics\nfrom imblearn.over_sampling import SMOTE # SMOTE\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler # scaling methods\n\nfrom sklearn.model_selection import GridSearchCV # grid search cross validation\nfrom sklearn.model_selection import RandomizedSearchCV # randomized search cross validation\n\n# supervised learning algorithms\nfrom sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbbors\nfrom sklearn.naive_bayes import GaussianNB # Gaussain Naive Bayes\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.ensemble import AdaBoostClassifier # Adaptive Boosting Classifier\nfrom sklearn.ensemble import BaggingClassifier # Bootstrap Aggregating Classifier\n\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combining trian and test datasets\n\ndf = pd.concat([pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv'),pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')], ignore_index=True)\ndf.drop('Unnamed: 0',axis=1,inplace=True) # unnecessary column\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset descriptions","metadata":{}},{"cell_type":"code","source":"import pandas_profiling\n\ndf.profile_report()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Checking Null values\npd.DataFrame(df.isnull().value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Binarizing Gender column\ndef gender_binarizer(x):\n    if x=='F':\n        return 1\n    if x=='M':\n        return 0\n    \ndf['gender'] = df['gender'].transform(gender_binarizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperating nominal from numeric\n# Note:There are almost 2M records in dfz.In order to avoid the heavy calculation,only the first 100000 rows were selected.\ndf2 = df.loc[:99999,df.dtypes!=np.object]\ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"X = df2.drop(['cc_num','is_fraud'],axis=1)\ny = df2['is_fraud']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resampling via SMOTE","metadata":{}},{"cell_type":"code","source":"sm = SMOTE()\nX_train_new, y_train_new = sm.fit_resample(X_train, y_train.ravel())\n\n# to demonstrate the effect of SMOTE over imbalanced datasets\nfig, (ax1, ax2) = plt.subplots(ncols = 2, figsize =(15, 5))\nax1.set_title('Before SMOTE')\npd.Series(y_train).value_counts().plot.bar(ax=ax1)\n\nax2.set_title('After SMOTE')  \npd.Series(y_train_new).value_counts().plot.bar(ax=ax2)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = sm.fit_resample(X_train, y_train.ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is heavily imbalanced. Through resampling, fraud transactions (Class = 1) are randomly increased to the same amount as non-fraud transactions (Class = 0) in order to avoid the bias results toward the non-fraudulent class.","metadata":{}},{"cell_type":"markdown","source":"## Scaling","metadata":{}},{"cell_type":"markdown","source":"### Robust Scaler VS MinMaxScaler VS Standard Scaler","metadata":{}},{"cell_type":"code","source":"# to compare the effect of each scaler on our dataset\nscaler = RobustScaler()\nrobust_df = scaler.fit_transform(df2)\nrobust_df = pd.DataFrame(robust_df)\n  \nscaler = StandardScaler()\nstandard_df = scaler.fit_transform(df2)\nstandard_df = pd.DataFrame(standard_df)\n  \nscaler = MinMaxScaler()\nminmax_df = scaler.fit_transform(df2)\nminmax_df = pd.DataFrame(minmax_df)\n\n# using KDE plot\n#Note: some columns are opted out in order to speed up the process\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(df2['merch_long'], ax = ax1)\nsns.kdeplot(df2['merch_lat'], ax = ax1)\nsns.kdeplot(df2['city_pop'], ax = ax1)\nsns.kdeplot(df2['long'], ax = ax1)\nsns.kdeplot(df2['lat'], ax = ax1)\n\n\nax2.set_title('After Robust Scaling')  \nsns.kdeplot(robust_df[9], ax = ax2)\nsns.kdeplot(robust_df[8], ax = ax2)\nsns.kdeplot(robust_df[7], ax = ax2)\nsns.kdeplot(robust_df[5], ax = ax2)\nsns.kdeplot(robust_df[4], ax = ax2)\n\n\nax3.set_title('After Standard Scaling')  \nsns.kdeplot(standard_df[9], ax = ax3)\nsns.kdeplot(standard_df[8], ax = ax3)\nsns.kdeplot(standard_df[7], ax = ax3)\nsns.kdeplot(standard_df[5], ax = ax3)\nsns.kdeplot(standard_df[4], ax = ax3)\n\n\nax4.set_title('After Min-Max Scaling')  \nsns.kdeplot(minmax_df[9], ax = ax4)\nsns.kdeplot(minmax_df[8], ax = ax4)\nsns.kdeplot(minmax_df[7], ax = ax4)\nsns.kdeplot(minmax_df[5], ax = ax4)\nsns.kdeplot(minmax_df[4], ax = ax4)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have a huge amount of data, its better to normalize the dataset by using RobustScaler which scales the data according to the quantile range.\n","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"param_grid = {'n_neighbors': range(1,20)}\nclf = RandomizedSearchCV(KNeighborsClassifier(), param_grid)\nclf.fit(X_train,y_train)\nclf_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Determining the number of neighbors using RandomizedSearchCV\nparam_grid = {'n_neighbors': range(1,20)}\nknn = RandomizedSearchCV(KNeighborsClassifier(), param_grid, verbose=3)\nknn.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn.best_params_ # best parameter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_pred = knn.predict(X_test)\n\nprint(confusion_matrix(y_test,knn_pred))\nprint('\\n')\nprint(classification_report(y_test,knn_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train,y_train)\ngnb_pred = gnb.predict(X_test)\n\nprint(confusion_matrix(y_test,gnb_pred))\nprint('\\n')\nprint(classification_report(y_test,gnb_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\ndtree_pred = dtree.predict(X_test)\n\nprint(confusion_matrix(y_test,dtree_pred))\nprint('\\n')\nprint(classification_report(y_test,dtree_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\nrfc_pred = rfc.predict(X_test)\n\nprint(confusion_matrix(y_test,rfc_pred))\nprint('\\n')\nprint(classification_report(y_test,rfc_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost","metadata":{}},{"cell_type":"code","source":"adabc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200)\nadabc.fit(X_train,y_train)\nadabc_pred = adabc.predict(X_test)\n\nprint(confusion_matrix(y_test,adabc_pred))\nprint('\\n')\nprint(classification_report(y_test,adabc_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bagging","metadata":{}},{"cell_type":"code","source":"bgc = BaggingClassifier(DecisionTreeClassifier(),n_estimators=200)\nbgc.fit(X_train,y_train)\nbgc_pred = bgc.predict(X_test)\n\nprint(confusion_matrix(y_test,bgc_pred))\nprint('\\n')\nprint(classification_report(y_test,bgc_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Evaluation via AUROC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\n\n# Instantiate the classfiers and make a list\nclassifiers = [GaussianNB(), \n               KNeighborsClassifier(n_neighbors= knn.best_params_.get('n_neighbors')),\n               DecisionTreeClassifier(random_state=42),\n               RandomForestClassifier(random_state=42),\n               AdaBoostClassifier(random_state=42),\n               BaggingClassifier(random_state=42)\n              ]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(X_train, y_train)\n    yproba = model.predict_proba(X_test)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting ROC curve \n\nfig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}