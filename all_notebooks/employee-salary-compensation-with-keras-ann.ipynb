{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n     Welcome\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:Beige;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:black;\">\n    \nHello Kagglers, <br>\n\nIn this notebook, I am going to predict the compensation of employees working at SF controller's office. But, first I am going to do deal with missing values in the dataset. Then, I am going to use Keras ANN on our dataset with fine tuning our model and also visualize it. <br>\n    So, let's get started.\n</p>\n</div> ","metadata":{}},{"cell_type":"markdown","source":" <div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n     Table of Contents\n</div>","metadata":{}},{"cell_type":"markdown","source":"1. [Importing Libraries](#1)<a href='1' ></a> <br>\n2. [Importing Dataset](#2)<a href='2' ></a> <br>\n3. [Taking care of Missing Values](#3)<a href='3' ></a> <br>\n    3.1. [Visualizing Missing Values](#3.1)<a href='3.1' ></a> <br>\n    3.2. [Categorical Data](#3.2)<a href='3.2' ></a> <br>\n    3.3. [Numerical Data](#3.3)<a href='3.3' ></a> <br>\n4. [Keras ANN](#4)<a href='4' ></a> <br>\n    4.1. [Building ANN](#4.1)<a href='4.1' ></a> <br>\n    4.2. [Training ANN](#4.2)<a href='4.2' ></a> <br>\n    4.3. [Visualizing Training and Validation Loss](#4.3)<a href='4.3' ></a> <br>\n    4.4. [Visualizing Training and Validation MAE](#4.4)<a href='4.4' ></a> <br>\n    4.5. [Train and Test Scores](#4.5)<a href='4.5' ></a> <br>\n5. [Conclusion](#5)<a href='5' ></a> <br>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n            letter-spacing:0.5px\"> <a id='1'></a>\n    \n    Importing Libraries \n</div>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.regularizers import l2\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '2'></a>\n    \n    Importing Dataset \n</div>","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/san-francisco-employee-salary-compensation/Employee_Salary_Compensation.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ We have 678524 rows and 22 columns in our dataset. <br>\nðŸ“Œ We can see that the dataset contains mixture of *categorical* and *numerical* variables. <br>\nðŸ“Œ Also, there are some missing values in the dataset. Let's check it out.","metadata":{}},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ There are not many missing values in our dataset.","metadata":{}},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ The above command *df.describe()* helps us to view the statistical properties of numerical variables. It excludes character variables.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '3'></a>\n\n    Taking care of Missing Values \n</div>","metadata":{}},{"cell_type":"markdown","source":"## **Visualizing Missing Values** <a id='3.1' ></a>","metadata":{}},{"cell_type":"code","source":"import missingno as msno\nmsno.matrix(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(dataset, sort= 'descending')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping because they have similar features in column.\ndataset.drop(['Organization Group','Department','Union','Job Family','Job','Employee Identifier','Total Salary','Total Benefits'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ Splitting our dataset in Categorical and Numerical values.","metadata":{}},{"cell_type":"code","source":"categorical = [i for i in dataset.columns if dataset[i].dtype=='object']\ncategorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical = [i for i in dataset.columns if dataset[i].dtype!='object']\nnumerical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Categorical Data** <a id='3.2' ></a>","metadata":{}},{"cell_type":"markdown","source":"ðŸ“Œ Missing values in categorical data.","metadata":{}},{"cell_type":"code","source":"dataset[categorical].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ Replacing the missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"dataset['Department Code'].fillna(dataset['Department Code'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[categorical].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ As you can see there are no missing values in our categorical columns.","metadata":{}},{"cell_type":"markdown","source":"## **Numerical Data** <a id='3.2' ></a>","metadata":{}},{"cell_type":"markdown","source":"ðŸ“Œ Missing values in numerical data.","metadata":{}},{"cell_type":"code","source":"dataset[numerical].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ Imputing missing values in numerical data with the median.","metadata":{}},{"cell_type":"code","source":"dataset['Union Code'].replace(to_replace=np.nan, value=dataset['Union Code'].median(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ We can see that there are no missing values present now in our dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le =  LabelEncoder()\nfor i in dataset:\n    if dataset[i].dtype=='object':\n        dataset[i] = le.fit_transform(dataset[i])\n    else:\n        continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ As you can see, object data type is converted to *int64* data type.","metadata":{}},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ Union Code is also int64 type. Let's convert it into int64 type.","metadata":{}},{"cell_type":"code","source":"dataset['Union Code'] = dataset['Union Code'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '4'></a>\n\n    Keras ANN \n</div>","metadata":{}},{"cell_type":"code","source":"x = dataset.drop('Total Compensation', axis=1).values\ny = dataset['Total Compensation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ Splitting Dataset into Train and Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number transactions x_train dataset: \", x_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions x_test dataset: \", x_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ Feature Scaling with Standard Scaler","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Building the ANN** <a id='4.1' ></a>","metadata":{}},{"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.optimizers import SGD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units= 32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 1,activation='linear'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training the ANN** <a id='4.2' ></a>","metadata":{}},{"cell_type":"code","source":"opt = tf.keras.optimizers.RMSprop(0.001)\nann.compile(optimizer= opt, loss= 'mean_squared_error', metrics= ['mae'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\nann_history = ann.fit(x_train, y_train, batch_size= 32, epochs= 50, validation_split= 0.2,callbacks=[early_stop])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualizing Training and Validation Loss** <a id='4.3' ></a>","metadata":{}},{"cell_type":"code","source":"loss_train = ann_history.history['loss']\nloss_val = ann_history.history['val_loss']\nepochs = range(1,51)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualizing Training and Validation MAE** <a id='4.4' ></a>","metadata":{}},{"cell_type":"code","source":"loss_train = ann_history.history['mae']\nloss_val = ann_history.history['val_mae']\nepochs = range(1,51)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Train and Test Scores** <a id='4.4' ></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score,explained_variance_score\ny_pred = ann.predict(x_test)\ntrain_prediction = ann.predict(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#R2 Score\nprint(\"r_square score: \", r2_score(y_test,y_pred))\nprint(\"r_square score (train dataset): \", r2_score(y_train,train_prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Variance Score\nprint(\"explained_variance_score: \", explained_variance_score(y_test,y_pred))\nprint(\"explained_variance_score (train dataset): \", explained_variance_score(y_train,train_prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '5'></a>\n\n    Conclusion \n</div>","metadata":{}},{"cell_type":"markdown","source":"ðŸ“Œ After dealing with missing values in our dataset, I took care of the categorical values with the help of Label Encoding. <br>\nðŸ“Œ Then, I used Keras ANN to evaluate the performance of our model by tuning the parameters of our model. <br>\nðŸ“Œ I have also visualized the model's performance with also displaying the r-square and variance score of our model, which you can see is good.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           border:2px solid DodgerBlue;\n           background-color:white;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n    Thank You!\n</div>","metadata":{}}]}