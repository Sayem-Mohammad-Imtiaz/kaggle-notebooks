{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/banking-dataset-classification/new_train.csv')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf.drop_duplicates(inplace=True)\ndf\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['pdays', 'previous'], 1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_obj = df.select_dtypes(include='object').copy()\ndf_obj.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_obj.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting bar chart for each categorical variable\nplt.style.use(\"ggplot\")\n\nfor column in df_obj:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    df[column].value_counts().plot(kind=\"bar\")\n    plt.xlabel(column)\n    plt.ylabel(\"number of customers\")\n    plt.title(column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in df_obj:\n  mode = df[column].mode()[0]\n  df[column]=df[column].replace('unknown', mode)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in df_obj:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    df[column].value_counts().plot(kind=\"bar\")\n    plt.xlabel(column)\n    plt.ylabel(\"number of customers\")\n    plt.title(column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"ggplot\")\nfor column in df_obj:\n    plt.figure(figsize=(20,4))\n    plt.subplot(121)\n    sns.countplot(df[column], hue=df[\"y\"])\n    plt.title(column)    \n    plt.xticks(rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# time for encoding categorical data.¶\n","metadata":{}},{"cell_type":"code","source":"df_encoded = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n       'month', 'day_of_week', 'poutcome'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_encoded.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nX = df_encoded.drop('y', 1).copy()\ny  = df_encoded['y'].copy()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing machine learning models and all other requirements¶\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import  SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#KNN Algorthm\n\nclf = KNeighborsClassifier()\nclf.fit(X_train, y_train)\nAccuracy = clf.score(X_test,y_test)\nAccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# The normal way of using KNN gives mere 88.359% which is pretty low for a machine learning model, so we would use RandomSearchCV for Hyper-parameter tuning and try to find the best parameters to get the maximun accuracy.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HyperParameters Tuning\nn_neighbors = [int(x) for x in np.linspace(start = 1, stop = 100, num = 50)]   \nweights = ['uniform','distance']\nmetric = ['euclidean','manhattan','chebyshev','seuclidean','minkowski'] \nrandom_grid = {\n    'n_neighbors': n_neighbors,\n    'weights': weights,\n    'metric': metric,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = RandomizedSearchCV(knn, param_distributions=random_grid, verbose=2, cv=3, random_state=42, n_iter=10, scoring='accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN.fit(X_train, y_train)\nKNN.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_knn = KNeighborsClassifier(metric= 'seuclidean', n_neighbors= 61, weights= 'distance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_knn.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_knn.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LOgistic Regression\n\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\nAccuracy = clf.score(X_test,y_test)\nAccuracy\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dist = {\n    'penalty' : ['l1', 'l2'],\n    'C' : [0, 1, 2, 3, 4]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegression()\nLG = RandomizedSearchCV(logreg, param_distributions=param_dist, verbose=2, cv=3, random_state=42, n_iter=10, scoring='accuracy')\nLG.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LG.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LG.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_best = LogisticRegression(C = 2, penalty='l2')\nlg_best.fit(X_train, y_train)\nlg_best.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GaussianNB (Naive Bayes Classifier)¶\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = GaussianNB()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\nNB = GaussianNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_ran = RandomizedSearchCV(NB, param_distributions=params_NB, verbose=2, cv=3, random_state=42, n_iter=10, scoring='accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_ran.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_ran.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_ran.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_best = GaussianNB(var_smoothing=1.51991108295293320)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_best.fit(X_train, y_train)\nNB_best.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nn_estimator = [int(x) for (x) in np.linspace(100, 1200, num=12)]\nmax_depth = [int(x) for x in np.linspace(5, 30, num=6)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]\ncriterion = ['gini', 'entropy']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dist = {\n    \"n_estimators\" : n_estimator,\n    \"max_depth\" : max_depth,\n    \"min_samples_leaf\":min_samples_leaf,\n    \"criterion\":criterion,\n    \"min_samples_split\":min_samples_split \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rb_hyper = RandomizedSearchCV(rf, param_distributions=param_dist, verbose=2, cv=3, random_state=42, n_iter=10, scoring='accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rb_hyper.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rb_hyper.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rb_hyper.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_rf = RandomForestClassifier(criterion= 'gini',\n max_depth=30,\n min_samples_leaf= 1,\n min_samples_split= 15,\nn_estimators= 300)\nbest_rf.fit(X_train, y_train)\nbest_rf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine","metadata":{}},{"cell_type":"code","source":"clf = SVC()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nparam_dist = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']}\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_hyper = RandomizedSearchCV(SVC(), param_distributions=param_dist, verbose=2, cv=3, random_state=42, n_iter=10, scoring='accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_hyper.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_hyper.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_hyper.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nbest_svc = SVC(C=10, gamma=0.001, kernel='rbf')\nbest_svc.fit(X_train, y_train)\nbest_svc.score(X_test, y_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# that's it, noting more to do. I dont think, the outliers would give, it to raise the accuracy beyong 90%.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}