{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since feature `gameId` does not have any predictive power in our case, we remove it.\ndf = df.drop(['gameId'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the features that are the most correlated with the label `blueWins`\n#By \"most correlated\", we mean that the absolute value of the correlation coefficient exceeds 0.3\n\ncorrs = df.apply(lambda x: x.corr(df['blueWins']))\ncorr_feat = df[corrs[abs(corrs) > 0.3].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now check correlation between features.\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr_feat.corr(),\n            cmap='plasma',\n            annot=True,\n            fmt='0.2f',\n            vmin=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can see that there are numerous pairs of distinct features that are perfectly correlated with each other.\n#e.g (blueKills, redDeaths), (redGoldPerMin,redTotalGold) etc.\n#Since having a pair of perfectly correlated features does not add a predictive power, for each pair, remove 1 feature.\n\ncols_to_remove = ['redKills','redDeaths','blueGoldPerMin','redGoldPerMin','redGoldDiff','redExperienceDiff']\ndf1 = corr_feat.drop(cols_to_remove,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having dataset tidied up a little bit, we can try to classify.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df1[df1.columns[1:]].values\ny = df1['blueWins']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gaussian NB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n\n\nX = df1[df1.columns[1:]].values\ny = df1['blueWins']\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state=199)\n\n\nclf = GaussianNB()\nclf.fit(X_train,y_train)\n\n\nprint('---------- PERFORMANCE ON THE TEST DATA---------- \\n')\nprint(classification_report(y_test,clf.predict(X_test)))\n\nprint('---------- PERFORMANCE ON THE TRAIN DATA---------- \\n')\nprint(classification_report(y_train,clf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n\n\nX = df1[df1.columns[1:]].values\ny = df1['blueWins']\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state=199)\n\n\nclf = LogisticRegression()\nclf.fit(X_train,y_train)\n\n\nprint('---------- PERFORMANCE ON THE TEST DATA---------- \\n')\nprint(classification_report(y_test,clf.predict(X_test)))\n\nprint('---------- PERFORMANCE ON THE TRAIN DATA---------- \\n')\nprint(classification_report(y_train,clf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report,f1_score\nfrom sklearn.metrics import make_scorer\n\nrec = make_scorer(f1_score,average='weighted')\n\n\nX = df1[df1.columns[1:]].values\ny = df1['blueWins']\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state=199)\n\n\nclf = KNeighborsClassifier()\ngrid_param = dict(n_neighbors=np.arange(1,11))\ngrid_s = GridSearchCV(clf,param_grid=grid_param, cv=5,refit=True,scoring=rec)\nclf = grid_s.fit(X_train,y_train).best_estimator_\n\n\nprint('---------- PERFORMANCE ON THE TEST DATA---------- \\n')\nprint(classification_report(y_test,clf.predict(X_test)))\n\nprint('---------- PERFORMANCE ON THE TRAIN DATA---------- \\n')\nprint(classification_report(y_train,clf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n\nX = df1[df1.columns[1:]].values\ny = df1['blueWins']\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state=199)\n\n\nclf = SVC(C=10e+5) #I've also tried 10e+2,10e+3,10e+4,10e+5: they seem to be yielding same results.\nclf.fit(X_train,y_train)\n\n\nprint('---------- PERFORMANCE ON THE TEST DATA---------- \\n')\nprint(classification_report(y_test,clf.predict(X_test)))\n\nprint('---------- PERFORMANCE ON THE TRAIN DATA---------- \\n')\nprint(classification_report(y_train,clf.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So far we see that KNN performs the worst (every metric on the test set), while all other algorithms (NB, Logistics, SVM) are doing roughly the same job.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Now let's try MLP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\n\n\nX = df1[df1.columns[1:]].values\ny = df1['blueWins']\n#Note that we are using random seed, so the split of the data is identical to the previous splits.\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state=199)\n\n#Scale the data beforehand\nscaler = MinMaxScaler()\nX_train_transormed = scaler.fit_transform(X_train)\nX_test_transformed = scaler.transform(X_test)\n\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(3,input_shape=(10,),activation='relu'))\nmodel.add(keras.layers.Dense(1,activation='sigmoid'))\n\n\nadam = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='binary_crossentropy')\n\n\nhistory = model.fit(X_train_transormed,y_train,epochs=5,verbose=0,batch_size=20)\n\n\nprint(classification_report(y_test, model.predict_classes(X_test_transformed)))\nprint(classification_report(y_train, model.predict_classes(X_train_transormed)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"I have tried different number of neurons in the hidden layer (1,2,3,100,1000,2000,5000), different number of layers (1,2), optimizers, different learning rates and different number of epochs. In the end, NN doesn't seem to be doing a better job than the algorithms we've tried so far. And on the top of that, it is fairly unstable: recall for both classes vary widly after each run. (In one extreme case, I got .9 recall on class 1 and 0.4 on class 0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}