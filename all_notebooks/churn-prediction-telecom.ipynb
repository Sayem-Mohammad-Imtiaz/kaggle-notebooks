{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import GradientBoostingClassifier \n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import metrics\nimport seaborn as sns\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c2fb17d52ee0dca55952b7830904e11d7a745d0","collapsed":true},"cell_type":"code","source":"df = pd.read_csv('../input/bigml_59c28831336c6604c800002a.csv')\n#print first 10 rows\ndf.head()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"ff0b847b906410a71d3eeb39b860c2706c071c65"},"cell_type":"markdown","source":"we want to check how many nulls, int, objects we have in the data to do a better preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df.info()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"a336852165a0d66ce6462cc3142b8019f1f7e4eb"},"cell_type":"markdown","source":"we can see that there are no null columns. <br>\nwhy null is so important? most of the algorithms don’t know how to deal with nulls so they have to be replaced. <br>\nin addition, we saw that there is no null substitutes like 'na' or '-1' therefore we can assume we have a full data set.<br>\nhowever, we can see than not all the columns are numerical, we will need to dig dipper for the object type."},{"metadata":{"trusted":true,"_uuid":"86af6c744324a4fcafe857af1ca34841bbba6a17","collapsed":true},"cell_type":"code","source":"df.describe(include=['O'])","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"7c39490a40f82b4b87633b719931a9fdc80752f7"},"cell_type":"markdown","source":"from the data description, we can see that phone number is unique - therefor it not provides us information we can learn. \nwe will drop phone number column and enumerate all the categorial objects columns. \nenumeration advantage is for easier use of the algorithms witch often accept only numbers. \n\nwe enumerate with encoder-decoder to have a fast way to switch between the two if needed"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7281242299b165946b30aa6da78c8e82bb0a6617"},"cell_type":"code","source":"label_encoder = preprocessing.LabelEncoder()\n\ndf['state'] = label_encoder.fit_transform(df['state'])\ndf['international plan'] = label_encoder.fit_transform(df['international plan'])\ndf['voice mail plan'] = label_encoder.fit_transform(df['voice mail plan'])\ndf['churn'] = label_encoder.fit_transform(df['churn'])\n\n# too specific\ndf.drop([\"phone number\"], axis = 1, inplace=True)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03b679020345d0fa05e2ccfd4eddfff3b8e53fbd","collapsed":true},"cell_type":"code","source":"stay = df[(df['churn'] ==0) ].count()[1]\nchurn = df[(df['churn'] ==1) ].count()[1]\nprint (\"num of pepole who stay: \"+ str(stay))\nprint (\"num of pepole who churn: \"+ str(churn))","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"53141edd9a3f1a516f06ce4f85d0e52b04b818fe"},"cell_type":"markdown","source":"Unfortunately, there are more churn = False then True. \nwe will need to balance the data before making predictions. \nfor balancing we will use synthetic data from SMOTE: Synthetic Minority Over-sampling Technique"},{"metadata":{"trusted":true,"_uuid":"8e60281532eb7450d5a9d4a36daa2510a2f3e12e","collapsed":true},"cell_type":"code","source":"# calculate the correlation matrix\ncorr = df.corr()\n\n# plot the heatmap\nfig = plt.figure(figsize=(5,4))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            linewidths=.75)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"13fa10451e35b928417a7d1b7407987ca34bb22b"},"cell_type":"markdown","source":"we can see strong correlation between the features: \n\ntotal day/eve/night/intl charge - total day/eve/night/intl minutes \nwe can assume they charge per call time.\n\nanother correlation is between voice mail plan and number vmail mail massages. \n\ncorrelation with churn:\n\ninternational plan\ntotal day minutes\ntotal day charge\ncustomers service call"},{"metadata":{"_uuid":"1b9add8ea5d7ae700a37f77bfffe0f4ad1a088d8"},"cell_type":"markdown","source":"we can try not to use all the \"duplicate\" columns and seek for stronger prediction.\n- we tried to do so however the result didn’t got batter, they stayed the same or +- 0.5%"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dd812bb7f721d3507a1c946da132483f04fd1b64"},"cell_type":"code","source":"#we will normalize our data so the prediction on all features will be at the same scale\nX = df.iloc[:,0:19].values\ny = df.iloc[:,19].values\n#nurmalize the data\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\ndfNorm = pd.DataFrame(X_std, index=df.index, columns=df.columns[0:19])\n# # add non-feature target column to dataframe\ndfNorm['churn'] = df['churn']\ndfNorm.head(10)\n\nX = dfNorm.iloc[:,0:19].values\ny = dfNorm.iloc[:,19].values","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34d584e62866482b224fb7c1736eca1b32ff5ca8","collapsed":true},"cell_type":"code","source":"# after we learn and preprocessed our data, we need to split it n to train and test.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape , y_test.shape","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"07e639f75f8f5a29cc399ffd1aa4b3ba49545ea1"},"cell_type":"markdown","source":"**### helping functions**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"03f1eb2e6ee199d897c1a9a839ec9511689bb2e0"},"cell_type":"code","source":"results_test = {}\nresults_train = {}\ndef prdict_date(algo_name,X_train,y_train,X_test,y_test,verbose=0):\n    algo_name.fit(X_train, y_train)\n    Y_pred = algo_name.predict(X_test)\n    acc_train = round(algo_name.score(X_train, y_train) * 100, 2)\n    acc_val = round(algo_name.score(X_test, y_test) * 100, 2)\n    results_test[str(algo_name)[0:str(algo_name).find('(')]] = acc_val\n    results_train[str(algo_name)[0:str(algo_name).find('(')]] = acc_train\n    if verbose ==0:\n        print(\"acc train: \" + str(acc_train))\n        print(\"acc test: \"+ str(acc_val))\n    else:\n        return Y_pred","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ee08ca3e3916d1c9a59b9fcdbf05a8737c142205"},"cell_type":"code","source":"### helping function\n\ndef conf(algo_name,X_test, y_test):\n    y_pred = algo_name.predict(X_test)\n    forest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\n    sns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"1\", \"0\"] , yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.title(str(algo_name)[0:str(algo_name).find('(')])","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"dc9578ab764cefa5ebf6cab12d5fdab444684e99"},"cell_type":"markdown","source":"# smote \nwe do sentetic data only on the train: SMOTE creates synthetic observations of the minority class (churn) by:\n\nFinding the k-nearest-neighbors for minority class observations (finding similar observations)\nRandomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.\n\nwe use smote only on the training data set"},{"metadata":{"trusted":true,"_uuid":"ea8d071453633723f4cbb016b5da2dd9a9d3ab71","collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0, ratio = 1.0)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a45c040269ae1a298fcf40d0a8ffd566f9fa83","collapsed":true},"cell_type":"code","source":"#data labels before SMOTE:\nimport collections\ncollections.Counter(y_train)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad1c1c6ffa7c8c1f0d6c4cb0787f3a931b344fb0","collapsed":true},"cell_type":"code","source":"#after SMOTE:\nimport collections\ncollections.Counter(y_train_res)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"e5e04723230cddda6888954c94ce46dae5c18056"},"cell_type":"markdown","source":"### predictions \nwe will try the  relevant sklearn function and compere there prediction with confusion matrix"},{"metadata":{"_uuid":"718dfe95d5ffe5a8b8a9e3c9a0e99ee3ccfad629"},"cell_type":"markdown","source":"### RandomForestClassifier"},{"metadata":{"trusted":true,"_uuid":"6df0ff60060a825dd9ffd9dd09314b6c36eb3a58","collapsed":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=75 , random_state=0  )\nprdict_date(random_forest,X_train_res,y_train_res,X_test,y_test)\nprint(classification_report(y_test, random_forest.predict(X_test)))\nconf(random_forest,X_test, y_test)\n","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"1fd97b6a0b2caaba6319e44a299676f68a62f0a3"},"cell_type":"markdown","source":"### Gradient Boosting"},{"metadata":{"trusted":true,"_uuid":"4aa9434dcb5d574e5cecbed7dd3e3548bdc4246a","collapsed":true},"cell_type":"code","source":"# Train: Gradient Boosting\ngbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.2, n_estimators=200 , max_depth=6)\nprdict_date(gbc,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, gbc.predict(X_test)))\nconf(gbc,X_test, y_test)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"17f9de82c5fd2b48d5f56924e7f014cfd490d9c4"},"cell_type":"markdown","source":"### SVM\nwe can’t know ahead witch type of kernel will predict the best results- therefor we tried multiple different kernels types."},{"metadata":{"trusted":true,"_uuid":"91fa60622f3a76a5097f99d3fc18f783426bb7be","collapsed":true},"cell_type":"code","source":"###### linear svm:\n#  SVM\nsvm = SVC(kernel='linear', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6a38f0284f7a594dfb4c6a8ac067b1df21f50c5","collapsed":true},"cell_type":"code","source":"###### linear rbf:\nsvm = SVC(kernel='rbf', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f42e69291641b3696199323553a10ab04d57edfb","collapsed":true},"cell_type":"code","source":"######  poly svm :\n#  SVM\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad79967b52acd4fc954cfcde026e69345217e036","collapsed":true},"cell_type":"code","source":"# Train: SVM\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, svm.predict(X_test)))\nconf(svm,X_test, y_test)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"ca83417f75acc0128411e32cb56bc1e270939521"},"cell_type":"markdown","source":"### K Neighbors Classifier\n"},{"metadata":{"trusted":true,"_uuid":"c2432dc4b4d63b217fb37cf3547140e5f98c4ae0","collapsed":true},"cell_type":"code","source":"#we will try to find witch K is the best on our data\n# first, we will look which give us the best predictions on the train:\nfrom sklearn import model_selection\n\n#Neighbors\nneighbors = [x for x in list(range(1,50)) if x % 2 == 0]\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nseed=0\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    scores = model_selection.cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    #print(\"k=%d %0.2f (+/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint(( \"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k])))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75647fae901c9e0c723c70fb6d7041a864cf1437"},"cell_type":"code","source":"# then on the test:\ncv_preds = []\n\n#Perform 10-fold cross validation on testing set for odd values of k\nseed=0\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    preds = model_selection.cross_val_predict(knn, X_test, y_test, cv=kfold)\n    cv_preds.append(metrics.accuracy_score(y_test, preds)*100)\n    #print(\"k=%d %0.2f\" % (k_value, 100*metrics.accuracy_score(test_y, preds)))\n\noptimal_k = neighbors[cv_preds.index(max(cv_preds))]\nprint(\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_preds[optimal_k]))\n\nplt.plot(neighbors, cv_preds)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Test Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22ff7b2f41530828eb034c43836fb3b55bf4aaf6","collapsed":true},"cell_type":"code","source":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 4)\nprdict_date(knn,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, knn.predict(X_test)))\nconf(knn,X_test, y_test)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"b10f73f173e16b37da26f2781544bc83f0d070b1"},"cell_type":"markdown","source":"### Logistic Regression\n"},{"metadata":{"trusted":true,"_uuid":"669728d5fc75bcb17375c3089a819abb9c9bad7e","collapsed":true},"cell_type":"code","source":"# Train: Logistic Regression\nlogr = LogisticRegression()\nprdict_date(logr,X_train,y_train,X_test,y_test)\n\nprint(classification_report(y_test, logr.predict(X_test)))\nconf(logr,X_test, y_test)","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"b0977c0378165e913dc6e21cb6e632016c3c9b7a"},"cell_type":"markdown","source":"### compere results"},{"metadata":{"trusted":true,"_uuid":"0a6c89a424fde09c26e9b7c51a314282428ccebc","collapsed":true},"cell_type":"code","source":"df_test =pd.DataFrame(list(results_test.items()),\n                      columns=['algo_name','acc_test'])\ndf_train =pd.DataFrame(list(results_train.items()),\n                      columns=['algo_name','acc_train'])\ndf_results = df_test.join(df_train.set_index('algo_name'), on='algo_name')\ndf_results.sort_values('acc_test',ascending=False)\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262ae59f7e15bfdec21615f86e27954902eed149","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n# set jupyter's max row display\npd.set_option('display.max_row', 100)\n\n# set jupyter's max column width to 50\npd.set_option('display.max_columns', 50)\n\n# Load the dataset\nax = df_results[['acc_test', 'acc_train']].plot(kind='barh',\n              figsize=(10,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"The Best ALGO is?\",\nfontsize=18)\nax.set_xlabel(\"ACC\", fontsize=18)\nax.set_ylabel(\"Algo Names\", fontsize=18)\nax.set_xticks([0,10,20,30,40,50,60,70,80,90,100,110])\nax.set_yticklabels(df_results.iloc[:,0].values.tolist())\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+4, i.get_y()+.1, \\\n            str(round((i.get_width()), 2)), fontsize=11, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"cac0391f64e90c5af8befd1d71d1102887c3c323"},"cell_type":"markdown","source":"our two best models were random forest and gradient boosting which are both sorts of decision trees ensembles.\nwe can see the influences of features as the gbc classified them\n"},{"metadata":{"trusted":true,"_uuid":"77dc1afcc8e24ddf29493db966761ba64db99237","collapsed":true},"cell_type":"code","source":"feature_importance = gbc.feature_importances_\nfeat_importances = pd.Series(gbc.feature_importances_, index=df.columns[:-1])\nfeat_importances = feat_importances.nlargest(19)\n\nfeature = df.columns.values.tolist()[0:-1]\nimportance = sorted(gbc.feature_importances_.tolist())\n\n\nx_pos = [i for i, _ in enumerate(feature)]\n# \nplt.barh(x_pos, importance , color='dodgerblue')\nplt.ylabel(\"feature\")\nplt.xlabel(\"importance\")\nplt.title(\"feature_importances\")\n\nplt.yticks(x_pos, feature)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e2227dc22f5044fc6686627a09baaccb683f139","collapsed":true},"cell_type":"code","source":"#next, we can try and take only the best parameters and see how they do\ngbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.2, n_estimators=200 , max_depth=6)\nprdict_date(gbc,X_train[:,3:],y_train,X_test[:,3:],y_test)\nprint(classification_report(y_test, gbc.predict(X_test[:,3:])))\nconf(gbc,X_test[:,3:], y_test)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"64d5e8e7c03746552b7237375a6f72b5db8c18c3"},"cell_type":"markdown","source":"*** we can see that it is  batter! ***"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}