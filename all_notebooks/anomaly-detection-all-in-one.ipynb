{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data Preparation","metadata":{"id":"XO6TJi-Wz4c4"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.covariance import EllipticEnvelope\nimport seaborn as sns","metadata":{"id":"ng7hUMxtX8Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('/content/drive/MyDrive/part-088.csv')","metadata":{"id":"0WEY1Avo7zeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"Gnkmabtv-ZZw","outputId":"6d7f5140-e3b5-4983-cb64-7be0ea89bc8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dimensions\ndata.shape","metadata":{"id":"fptssYKyDwaA","outputId":"a60f24cb-76f2-4be6-849e-4e84ceaca73e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing rows with constant values\n\ndef remove_constant_value_features(df):\n    return [e for e in df.columns if df[e].nunique() == 1]\ndata=data.drop(columns=remove_constant_value_features(data))","metadata":{"id":"0naruw1EcKq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#null values\nnp.sum(np.sum(data.isna()))","metadata":{"id":"SyMOppASDnS9","outputId":"d4ed0e82-897e-43b8-e0bb-f131a3565c93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#anomalies by hour\ntimedelta = pd.to_datetime(data['timestamp'])\ndata['Time_hour'] = (timedelta.dt.hour).astype(int)\n\nplt.figure(figsize=(12,5))\nsns.distplot(data[data['isAnomaly'] == 0][\"Time_hour\"], color='g')\nsns.distplot(data[data['isAnomaly'] == 1][\"Time_hour\"], color='r')\nplt.title('Fraud and Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","metadata":{"id":"2ifK0J9DYA8p","outputId":"dac20511-3045-401f-adcc-80536e323575"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Peak in fraud transcations at 3 pm","metadata":{"id":"DMVH7fdVZugA"}},{"cell_type":"code","source":"target=data['isAnomaly']\ndata=data.drop(columns=['timestamp']) #dropping right now but will require in a later section","metadata":{"id":"vNlwH0EdYtkW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['isAnomaly'].value_counts() #number of anomalies","metadata":{"id":"NYnY1pq3GaDS","outputId":"4065216d-028b-4c35-ae59-f03376f6e24d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Selection\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 0)\nrnd_clf.fit(data,target);\n\nnot_imp=[]\nfor name, importance in zip(data.columns, rnd_clf.feature_importances_):\n  if importance > 0.020 :\n    not_imp.append(name)\n\ndata=data.drop(columns=not_imp)","metadata":{"id":"zYVXQpTD20qt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_tuples = list(zip(data.columns, rnd_clf.feature_importances_))\npd.DataFrame(list_of_tuples, columns = ['Columns', 'Importance']).sort_values(by='Importance', ascending=False)","metadata":{"id":"wsOtqxSScWyt","outputId":"78ec51c5-99d0-40ce-e44f-fd17b0cf4064"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping high correlated columns\ncor_matrix=data.corr()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\ndata=data.drop(columns=to_drop)","metadata":{"id":"ETPHZxUsmL45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Auto Outlier Detection Algorithms","metadata":{"id":"oDa1sOUmPuGy"}},{"cell_type":"code","source":"factor=1723/38797 #number of fraud cases","metadata":{"id":"IpAaGaarw0UU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the outlier detection methods\n\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(data), \n                                       contamination=factor,random_state=0, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=factor),\n    \"Support Vector Machine\":OneClassSVM(kernel='linear', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1),\n    \"Elliptic Envelope\":EllipticEnvelope(contamination=factor)\n   \n}","metadata":{"id":"rRWniBRfxr-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\nn_outliers = 1723\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(data)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(data)\n        y_pred = clf.predict(data)\n    else:    \n        clf.fit(data)\n        scores_prediction = clf.decision_function(data)\n        y_pred = clf.predict(data)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != target).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(target,y_pred))\n    print(confusion_matrix(target, y_pred))","metadata":{"id":"uw7Uv7jRxwu_","outputId":"bbe0c81c-f96e-423d-f542-a35db27ef34c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Auto-Encoders","metadata":{"id":"GWCO0DJwkFSo"}},{"cell_type":"code","source":"#Looking for clusters\nfrom sklearn.decomposition import PCA\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(data)\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], alpha=.1, color='black')","metadata":{"id":"wRxjlziOuX4X","outputId":"ad4b45d8-9d62-4eb0-c1c0-aed6ee0bf132"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport random as rn\n# manual parameters\nRANDOM_SEED = 42\nVALIDATE_SIZE = 0.2\n\n# setting random seeds for libraries to ensure reproducibility\nnp.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","metadata":{"id":"A_9tQ6TeybpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef tsne_scatter(features, labels, dimensions=2, save_as='graph.png'):\n    if dimensions not in (2, 3):\n        raise ValueError('tsne_scatter can only plot in 2d or 3d (What are you? An alien that can visualise >3d?). Make sure the \"dimensions\" argument is in (2, 3)')\n\n    # t-SNE dimensionality reduction\n    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n    \n    # initialising the plot\n    fig, ax = plt.subplots(figsize=(8,8))\n    \n    # counting dimensions\n    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n\n    # plotting data\n    ax.scatter(\n        *zip(*features_embedded[np.where(labels==1)]),\n        marker='o',\n        color='r',\n        s=2,\n        alpha=0.7,\n        label='Fraud'\n    )\n    ax.scatter(\n        *zip(*features_embedded[np.where(labels==0)]),\n        marker='o',\n        color='g',\n        s=2,\n        alpha=0.3,\n        label='Clean'\n    )\n\n    # storing it to be displayed later\n    plt.legend(loc='best')\n    plt.savefig(save_as);\n    plt.show;","metadata":{"id":"zmCJpy1E_oya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne_scatter(data, target, dimensions=2, save_as='tsne_initial_2d.png')","metadata":{"id":"DoZLMu7tA-YX","outputId":"a3643828-fb99-418f-bb5d-363f0aaaad7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Anomalies aren't apparent","metadata":{"id":"eguhyxU_DG4i"}},{"cell_type":"code","source":"data=pd.concat([data,target],axis=1)","metadata":{"id":"Ki_cr_FIDdln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud = data[data.isAnomaly == 1]\nclean = data[data.isAnomaly == 0]","metadata":{"id":"VSRSsHBeBMMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"Shape of the datasets:\n    clean (rows, cols) = {clean.shape}\n    fraud (rows, cols) = {fraud.shape}\"\"\")","metadata":{"id":"wUTGAR6rDsen","outputId":"b71ed102-1eb9-4855-f615-f5bef50d4325"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_SAMPLE = 25952\n# shuffle our training set\nclean = clean.sample(frac=1).reset_index(drop=True)\n\n# training set: exlusively non-fraud transactions\nX_train = clean.iloc[:TRAINING_SAMPLE].drop('isAnomaly', axis=1)\n\n# testing  set: the remaining non-fraud + all the fraud \nX_test = clean.iloc[TRAINING_SAMPLE:].append(fraud).sample(frac=1)","metadata":{"id":"2RWMuRHaDxKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"Our testing set is composed as follows:\n\n{X_test.isAnomaly.value_counts()}\"\"\")","metadata":{"id":"WM-PDaZED_1c","outputId":"3b98bc92-8ab2-431b-b6f7-d4a18027fedd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# train // validate - no labels since they're all clean anyway\nX_train, X_validate,y_train, y_validate = train_test_split(X_train, \n                                       test_size=VALIDATE_SIZE, \n                                       random_state=RANDOM_SEED)\n\n# manually splitting the labels from the test df\nX_test, y_test = X_test.drop('isAnomaly', axis=1).values, X_test.isAnomaly.values","metadata":{"id":"c7UkjCDVEnV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"Shape of the datasets:\n    training (rows, cols) = {X_train.shape}\n    validate (rows, cols) = {X_validate.shape}\n    holdout  (rows, cols) = {X_test.shape}\"\"\")","metadata":{"id":"EvT1maKsE3tV","outputId":"a47561cc-3b6c-4627-8da7-9fc90464e6f5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import Normalizer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# configure our pipeline\npipeline = Pipeline([('normalizer', Normalizer()),\n                     ('scaler', MinMaxScaler())])","metadata":{"id":"w0S9rSVfFDk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline.fit(X_train);","metadata":{"id":"aPwzrnXOF31e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_transformed = pipeline.transform(X_train)\nX_validate_transformed = pipeline.transform(X_validate)","metadata":{"id":"nqKwJVkWF5xP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.PairGrid(X_train.iloc[:,:3].sample(600, random_state=RANDOM_SEED))\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Before:')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot);","metadata":{"id":"3akG8AvQF859","outputId":"dfc71533-f63b-489c-a5a0-7bc6d9a72fa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.PairGrid(pd.DataFrame(X_train_transformed).iloc[:,:3].sample(600, random_state=RANDOM_SEED))\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('After:')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot);","metadata":{"id":"KwWkx6KFGC1J","outputId":"fc80fca1-0168-40df-bb67-8181eea63425"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell the data is slightly more uniform and proportionally distributed. \nThe ranges were also shrunk to fit between 0 and 1.","metadata":{"id":"2EpqdTOaGkLi"}},{"cell_type":"code","source":"input_dim = X_train_transformed.shape[1]\nBATCH_SIZE = 256\nEPOCHS = 100\n\nautoencoder = tf.keras.models.Sequential([\n    \n    # deconstruct / encode\n    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )), \n    tf.keras.layers.Dense(16, activation='elu'),\n    tf.keras.layers.Dense(8, activation='elu'),\n    tf.keras.layers.Dense(4, activation='elu'),\n    tf.keras.layers.Dense(2, activation='elu'),\n    \n    # reconstruction / decode\n    tf.keras.layers.Dense(4, activation='elu'),\n    tf.keras.layers.Dense(8, activation='elu'),\n    tf.keras.layers.Dense(16, activation='elu'),\n    tf.keras.layers.Dense(input_dim, activation='elu')\n    \n])\n\nautoencoder.compile(optimizer=\"adam\", \n                    loss=\"mse\",\n                    metrics=[\"acc\"])\n\n# print an overview of our model\nautoencoder.summary();","metadata":{"id":"dcYUYmbjGS1l","outputId":"402ef697-f355-4a42-bfc3-a855b5234ac5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\n# current date and time\nyyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n\n# new folder for a new run\nlog_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(autoencoder.layers)}'\n\n# define our early stopping\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0001,\n    patience=10,\n    verbose=1, \n    mode='min',\n    restore_best_weights=True\n)\n\nsave_model = tf.keras.callbacks.ModelCheckpoint(\n    filepath='autoencoder_best_weights.hdf5',\n    save_best_only=True,\n    monitor='val_loss',\n    verbose=0,\n    mode='min'\n)\n\ntensorboard = tf.keras.callbacks.TensorBoard(\n    f'logs/{log_subdir}',\n    batch_size=BATCH_SIZE,\n    update_freq='batch'\n)\n\n# callbacks argument only takes a list\ncb = [early_stop, save_model, tensorboard]","metadata":{"id":"wOx2wLZGHzZ3","outputId":"7a3b336e-20a9-439f-89c3-ba06ddc686b5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = autoencoder.fit(\n    X_train_transformed, X_train_transformed,\n    shuffle=True,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=cb,\n    validation_data=(X_validate_transformed, X_validate_transformed)\n);","metadata":{"id":"1CHIl2UCIXVs","outputId":"cb740cd1-96a2-4840-fc0d-e6aefeb97d34"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform the test set with the pipeline fitted to the training set\nX_test_transformed = pipeline.transform(X_test)\n\n# pass the transformed test set through the autoencoder to get the reconstructed result\nreconstructions = autoencoder.predict(X_test_transformed)","metadata":{"id":"7ai7QVd2Ilku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating the mean squared error reconstruction loss per row in the numpy array\nmse = np.mean(np.power(X_test_transformed- reconstructions, 2), axis=1)","metadata":{"id":"NnM5mb8hKLSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESHOLD = 3\n\ndef mad_score(points):\n    \n    m = np.median(points)\n    ad = np.abs(points - m)\n    mad = np.median(ad)\n    \n    return 0.6745 * ad / mad\n\nz_scores = mad_score(mse)\noutliers = z_scores > THRESHOLD","metadata":{"id":"pi4IVw25KTqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Detected {np.sum(outliers):,} outliers in a total of {np.size(z_scores):,} operations [{np.sum(outliers)/np.size(z_scores):.2%}].\")","metadata":{"id":"j0IaFGx9Lvra","outputId":"54bb4e0e-75ae-4979-878b-76372e359263"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import (confusion_matrix, \n                             precision_recall_curve)\n\n# get (mis)classification\ncm = confusion_matrix(y_test, outliers)\n\n# true/false positives/negatives\n(tn, fp, \n fn, tp) = cm.flatten()","metadata":{"id":"tzEYGRHGLywU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n{cm}\n\n% of transactions labeled as fraud that were correct (precision): {tp}/({fp}+{tp}) = {tp/(fp+tp):.2%}\n% of fraudulent transactions were caught succesfully (recall):    {tp}/({fn}+{tp}) = {tp/(fn+tp):.2%}\"\"\")","metadata":{"id":"exuupW-iMDf0","outputId":"787300a5-94d8-416b-f5df-0f8553385da9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy\n(9701/12845)*100","metadata":{"id":"eEB8NQFfMnu7","outputId":"c878fe0f-cff7-44c6-a0bc-b7f8c3ddd00b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FBProphet","metadata":{"id":"bJKwf82wfZ0t"}},{"cell_type":"code","source":"!pip install fbprophet\nfrom fbprophet import Prophet\nimport os","metadata":{"id":"lKW0A71afdzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the data as a table\ndf_ = pd.DataFrame(data, columns=['timestamp', r'Available db connection activity : (d/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumAvailable))'])\ndf_['ds']=df_['timestamp']\ndf_['y']=df_[r'Available db connection activity : (d/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumAvailable))'].astype(float)\ndf_=df_.drop(['timestamp',r'Available db connection activity : (d/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumAvailable))'],axis=1)\ndf_.head()","metadata":{"id":"1zse54zvfkJ8","outputId":"8b746879-1c45-40b2-bd85-bca26c96a684"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_predict_model(dataframe, interval_width = 0.99, changepoint_range = 0.8):\n    m = Prophet(daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = False,\n#                 seasonality_mode = 'multiplicative', \n                interval_width = interval_width,\n                changepoint_range = changepoint_range)\n    m = m.fit(dataframe)\n    \n    forecast = m.predict(dataframe)\n    forecast['fact'] = dataframe['y'].reset_index(drop = True)\n    print('Displaying Prophet plot')\n    fig1 = m.plot(forecast)\n    return forecast\n    \npred = fit_predict_model(df_)","metadata":{"id":"q0TAwDSmgxWQ","outputId":"4142b475-3006-4ff8-ab39-b2284201393e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_anomalies(forecast):\n    forecasted = forecast[['ds','trend', 'yhat', 'yhat_lower', 'yhat_upper', 'fact']].copy()\n    #forecast['fact'] = df['y']\n\n    forecasted['anomaly'] = 0\n    forecasted.loc[forecasted['fact'] > forecasted['yhat_upper'], 'anomaly'] = 1\n    forecasted.loc[forecasted['fact'] < forecasted['yhat_lower'], 'anomaly'] = 1 #-1\n\n    #anomaly importances\n    forecasted['importance'] = 0\n    forecasted.loc[forecasted['anomaly'] ==1, 'importance'] = \\\n        (forecasted['fact'] - forecasted['yhat_upper'])/forecast['fact']\n    forecasted.loc[forecasted['anomaly'] ==-1, 'importance'] = \\\n        (forecasted['yhat_lower'] - forecasted['fact'])/forecast['fact']\n    \n    return forecasted\n\npred = detect_anomalies(pred)","metadata":{"id":"rhJcY3Tdg6A-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"id":"TmdzJHGNhKl4","outputId":"8b72b61c-3fbf-413a-93cd-14d64e8146d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred[ r'anomaly'].value_counts()","metadata":{"id":"RLYJ76-KhMCc","outputId":"d9313d9c-fe4d-4082-8019-04129c74e9be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy\n1361/1723","metadata":{"id":"zKPQ_f5whVTU","outputId":"5bb72f14-452c-4a32-b005-8de0a2a120ac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Supervised","metadata":{"id":"pobuxcU1zjp2"}},{"cell_type":"code","source":"#Handling imbalance\nfrom imblearn.under_sampling import NearMiss\n\nnm = NearMiss()\n\nx_nm, y_nm = nm.fit_resample(data, target)","metadata":{"id":"vg_r02oe4OTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_nm.shape,y_nm.shape)","metadata":{"id":"icZIRK7c7Lg1","outputId":"9ac6c2d1-eb49-44b4-b25f-63201f8317aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\nx_scaled = scalar.fit_transform(x_nm)","metadata":{"id":"tCBb0x3e-t3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nx_train,x_test,y_train,y_test = train_test_split(x_scaled,y_nm, test_size = 0.25)","metadata":{"id":"tDpO_XV7-zmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = {}\nacc = []\ncv_scores = []\ndef model(model):\n    model.fit(x_train,y_train)\n    score = model.score(x_test,y_test)\n    print(\"Accuracy: {}\".format(score))\n    cv_score = cross_val_score(model,x_train,y_train,cv=5)\n    print(\"Cross Val Score: {}\".format(np.mean(cv_score)))\n    acc.append(score)\n    cv_scores.append(np.mean(cv_score))","metadata":{"id":"kOULw2yp-2So"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nclf = XGBClassifier()\nmodel(clf)\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nmodel(clf)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nmodel(clf)\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nmodel(clf)\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier()\nmodel(clf)\nfrom sklearn.svm import SVC\nclf = SVC()\nmodel(clf)\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nmodel(clf)\nfrom sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier()\nmodel(clf)\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier()\nmodel(clf)","metadata":{"id":"iMwIvfiG-46g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [\"XGBClassifier\",\"LogisticRegression\",\"RandomForestClassifier\",\"DecisionTreeClassifier\",\"KNeighborsClassifier\",\"SVC\",\"GaussianNB\",\"AdaBoostClassifier\",\"GradientBoostingClassifier\"]\nscores = { \"Model Name\" : models , \"Accuracy Score\" : acc, \"Cross val Score\": cv_scores}\ndf1 = pd.DataFrame(scores)","metadata":{"id":"qOCNpZSZ_RGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"id":"YElV6kfK_5a8","outputId":"2ececa71-2988-4307-9f0e-076ff0da7485"},"execution_count":null,"outputs":[]}]}