{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport argparse\n# from trainer import Trainer\nimport torch as t\n\nimport pandas as pd\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n# from model import SkipGramEmbeddings\n# from sgns_loss import SGNSLoss\nfrom tqdm import tqdm\n# from datasets.pypi_lang import PyPILangDataset\n# from datasets.world_order import WorldOrderDataset\nfrom torch.utils.tensorboard import SummaryWriter\n# from datasets.fine_food import FineFoodDataset\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n# from utils import AliasMultinomial\n\n# from .preprocess import Tokenizer\n\nfrom gensim.corpora import Dictionary\nfrom torch.utils.data.dataset import Dataset\n\nimport spacy\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Tokenizer:\n\n    def __init__(self, args, custom_stop=set()):\n        self.args = args\n        self.custom_stop = custom_stop\n        # Define pipeline - use different nlp is using pretrained\n        import en_core_web_sm\n        self.nlp = en_core_web_sm.load()\n        # self.nlp = spacy.load(\"en_core_web_sm\", disable=[])\n        # Merge named entities\n        merge_ents = self.nlp.create_pipe(\"merge_entities\")\n        self.nlp.add_pipe(merge_ents)\n\n    def tokenize_doc(self, doc_str):\n        \"\"\"\n        Tokenize a document string\n        Modified version of Moody's Tokenization in:\n        https://github.com/cemoody/lda2vec/blob/master/lda2vec/preprocess.py\n\n        :params doc_str: String\n        :returns: list of Strings, i.e. tokens\n        \"\"\"\n\n        # Send doc_str through pipeline\n        spacy_doc = self.nlp(doc_str)\n        # Filter\n        filtered_doc = filter(self.is_valid_token, spacy_doc)\n        # Convert to text make lowercase\n        clean_doc = [t.text.lower().strip() for t in filtered_doc]\n        # Only allow characters in the alphabet, '_', and digits\n        clean_doc = [re.sub('[^a-zA-Z0-9]', '', t) for t in clean_doc]\n        # Remove any resulting empty indices\n        clean_doc = [t for t in clean_doc if len(t) > 0]\n        # Filter out any custom stop\n        clean_doc = [t for t in clean_doc if t not in self.custom_stop]\n\n        return clean_doc\n\n    def is_valid_token(self, token):\n        \"\"\"\n        Determines if a token is valid or not\n\n        :params token: String\n        :returns: Boolean\n        \"\"\"\n        if token.like_url:\n            return False\n        if token.like_email:\n            return False\n        if token.is_stop or token.text in self.custom_stop:\n            return False\n\n        return True\n\n    def moodys_merge_noun_chunks(self, doc):\n        \"\"\"\n        Merge noun chunks into a single token.\n\n        Modified from sources of:\n        - https://github.com/cemoody/lda2vec/blob/master/lda2vec/preprocess.py\n        - https://spacy.io/api/pipeline-functions#merge_noun_chunks\n\n        :params doc: Doc object.\n        :returns: Doc object with merged noun chunks.\n        \"\"\"\n        bad_deps = ('amod', 'compound')\n\n        if not doc.is_parsed:\n            return doc\n        with doc.retokenize() as retokenizer:\n            for np in doc.noun_chunks:\n\n                # Only keep adjectives and nouns, e.g. \"good ideas\"\n                while len(np) > 1 and np[0].dep_ not in bad_deps:\n                    np = np[1:]\n\n                if len(np) > 1:\n                    # Merge NPs\n                    attrs = {\"tag\": np.root.tag, \"dep\": np.root.dep}\n                    retokenizer.merge(np, attrs=attrs)\n        return doc\n\n\nclass SimpleTokenizer:\n\n    def __init__(self, args, custom_stop=set()):\n        self.args = args\n        self.custom_stop = custom_stop\n\n    def tokenize_doc(self, doc_str):\n        # Filter out any custom stop\n        clean_doc = [t for t in doc_str.split() if t not in self.custom_stop]\n        return clean_doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SkipGramDataset(Dataset):\n\n    def __init__(self, args):\n        self.args = args\n        self.dictionary = None\n        self.examples = []\n        self.name = ''\n\n    def __getitem__(self, index):\n        return self._example_to_tensor(*self.examples[index])\n\n    def __len__(self):\n        return len(self.examples)\n\n    # 修改這裡的檔案名字\n    def save(self, examples_path, dict_path):\n        print('Saving Dataset Examples...')\n        torch.save({\n             'examples': self.examples,\n        }, './test_examples.pth')\n        print('Saving Dataset Dictionary...')\n        self.dictionary.save('./test_dict.pth')\n        print('Saved Dataset!')\n\n    def load(self, examples_path, dict_path):\n        print('Loading Dataset Examples...')\n        self.examples = torch.load(examples_path)['examples']\n        print('Loading Dataset Dictionary...')\n        self.dictionary = Dictionary().load(dict_path)\n        print('Loaded Saved Dataset!')\n\n    def generate_examples_serial(self):\n        \"\"\"\n        Generates examples with no multiprocessing - straight through!\n        :return: None - updates class properties\n        \"\"\"\n        # Now we have a Gensim Dictionary to work with\n        self._build_dictionary()\n        # Remove any tokens with a frequency less than 10\n        self.dictionary.filter_extremes(no_below=10, no_above=0.75)\n\n        self.examples = []\n        for file in tqdm(self.load_files(), desc=\"Generating Examples (serial)\"):\n            file = self.dictionary.doc2idx(file)\n            self.examples.extend(self._generate_examples_from_file(file))\n\n    def load_files(self):\n        \"\"\"\n        Sets self.files as a list of tokenized documents!\n        :returns: List of files\n        \"\"\"\n        # Needs to be implemented by child class\n        raise NotImplementedError\n\n    def _build_dictionary(self):\n        \"\"\"\n        Creates a Gensim Dictionary\n        :return: None - modifies self.dictionary\n        \"\"\"\n        print(\"Building Dictionary...\")\n        self.dictionary = Dictionary(self.load_files())\n        self.word_freq = self.dictionary.cfs\n    def _generate_examples_from_file(self, file):\n        \"\"\"\n        Generate all examples from a file within window size\n        :param file: File from self.files\n        :returns: List of examples\n        \"\"\"\n\n        examples = []\n        for i, token in enumerate(file):\n            if token == -1:\n                # Out of dictionary token\n                continue\n\n            # Generate context tokens for the current token\n            context_words = self._generate_contexts(i, file)\n\n            # Form Examples:\n            # center, context - follows form: (input, target)\n            new_examples = [(token, ctxt) for ctxt in context_words if ctxt != -1]\n\n            # Add to class\n            examples.extend(new_examples)\n        return examples\n\n    def _generate_contexts(self, token_idx, tokenized_doc):\n        \"\"\"\n        Generate Token's Context Words\n        Generates all the context words within the window size defined\n        during initialization around token.\n\n        :param token_idx: Index at which center token is found in tokenized_doc\n        :param tokenized_doc: List - Document broken into tokens\n        :returns: List of context words\n        \"\"\"\n        contexts = []\n        # Iterate over each position in window\n        for w in range(-self.args.window_size, self.args.window_size + 1):\n            context_pos = token_idx + w\n\n            # Make sure current center and context are valid\n            is_outside_doc = context_pos < 0 or context_pos >= len(tokenized_doc)\n            center_is_context = token_idx == context_pos\n\n            if is_outside_doc or center_is_context:\n                # Not valid - skip to next window position\n                continue\n\n            contexts.append(tokenized_doc[context_pos])\n        return contexts\n\n    def _example_to_tensor(self, center, target):\n        # print(center, target)\n        \"\"\"\n        Takes raw example and turns it into tensor values\n\n        :params example: Tuple of form: (center word, document id)\n        :params target: String of the target word\n        :returns: A tuple of tensors\n        \"\"\"\n        center, target = torch.tensor([int(center)]), torch.tensor([int(target)])\n        return center, target, self.word_freq[int(center)], self.word_freq[int(target)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WorldOrderDataset(SkipGramDataset):\n\n    def __init__(self, args, examples_path=None, dict_path=None):\n        SkipGramDataset.__init__(self, args)\n        self.name = 'World Order Book Dataset'\n        self.queries = ['nuclear', 'mankind', 'khomeini', 'ronald']\n\n        if examples_path is not None and dict_path is not None:\n            self.load(examples_path, dict_path)\n        else:\n            self.files = self.tokenize_files()\n            self.generate_examples_serial()\n\n        print(f'There are {len(self.dictionary)} tokens and {len(self.examples)} examples.')\n\n    def load_files(self):\n        return self.files\n\n    def tokenize_files(self):\n        files = []\n        with open('data/world_order_kissinger.txt') as f:\n            for line in f:\n                words_no_dig_punc = (re.sub(r'[^\\w]', ' ', line.lower())).split()\n                words_no_dig_punc = [x for x in words_no_dig_punc if not any(c.isdigit() for c in x)]\n                files.append(words_no_dig_punc)\n\n        return files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PyPILangDataset(SkipGramDataset):\n\n    def __init__(self, args, examples_path=None, dict_path=None):\n        SkipGramDataset.__init__(self, args)\n        self.name = 'PyPI Language Dataset'\n        self.queries = ['tensorflow', 'pytorch', 'nlp', 'performance', 'encryption']\n\n        if examples_path is not None and dict_path is not None:\n            self.load(examples_path, dict_path)\n        else:\n            self.tokenizer = Tokenizer(args)\n            self.files = self.tokenize_files()\n            self.generate_examples_serial()\n\n            self.save('pypi_examples.pth', 'pypi_dict.pth')\n\n        print(f'There are {len(self.dictionary)} tokens and {len(self.examples)} examples.')\n\n    def load_files(self):\n        return self.files\n\n    def tokenize_files(self):\n        node_lang_df = pd.read_csv(self.args.dataset_dir, na_filter=False)\n        lang_data = node_lang_df['language'].values\n        return [self.tokenizer.tokenize_doc(f) for f in tqdm(lang_data, desc='Tokenizing Docs')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AliasMultinomial(object):\n    \"\"\"\n    Fast sampling from a multinomial distribution.\n    https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n\n    Code taken from: https://github.com/TropComplique/lda2vec-pytorch/blob/master/utils/alias_multinomial.py\n    \"\"\"\n\n    def __init__(self, probs, device):\n        \"\"\"\n        probs: a float tensor with shape [K].\n            It represents probabilities of different outcomes.\n            There are K outcomes. Probabilities sum to one.\n        \"\"\"\n        self.device = device\n\n        K = len(probs)\n        self.q = t.zeros(K).to(device)\n        self.J = t.LongTensor([0] * K).to(device)\n\n        # sort the data into the outcomes with probabilities\n        # that are larger and smaller than 1/K\n        smaller = []\n        larger = []\n        for kk, prob in enumerate(probs):\n            self.q[kk] = K * prob\n            if self.q[kk] < 1.0:\n                smaller.append(kk)\n            else:\n                larger.append(kk)\n\n        # loop though and create little binary mixtures that\n        # appropriately allocate the larger outcomes over the\n        # overall uniform mixture\n        while len(smaller) > 0 and len(larger) > 0:\n            small = smaller.pop()\n            large = larger.pop()\n\n            self.J[small] = large\n            self.q[large] = (self.q[large] - 1.0) + self.q[small]\n\n            if self.q[large] < 1.0:\n                smaller.append(large)\n            else:\n                larger.append(large)\n\n        self.q.clamp(0.0, 1.0)\n        self.J.clamp(0, K - 1)\n\n    def draw(self, N):\n        \"\"\"Draw N samples from the distribution.\"\"\"\n\n        K = self.J.size(0)\n        r = t.LongTensor(np.random.randint(0, K, size=N)).to(self.device)\n        q = self.q.index_select(0, r).clamp(0.0, 1.0)\n        j = self.J.index_select(0, r)\n        b = t.bernoulli(q)\n        oq = r.mul(b.long())\n        oj = j.mul((1 - b).long())\n\n        return oq + oj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SGNSLoss(nn.Module):\n    BETA = 0.75  # exponent to adjust sampling frequency\n    NUM_SAMPLES = 2\n\n    def __init__(self, dataset, word_embeddings, device):\n        super(SGNSLoss, self).__init__()\n        self.dataset = dataset\n        self.criterion = nn.BCEWithLogitsLoss(reduction = 'none')\n        self.vocab_len = len(dataset.dictionary)\n        self.word_embeddings = word_embeddings\n        self.device = device\n\n        # Helpful values for unigram distribution generation\n        # Should use cfs instead but: https://github.com/RaRe-Technologies/gensim/issues/2574\n        self.transformed_freq_vec = t.tensor(\n            np.array([dataset.dictionary.dfs[i] for i in range(self.vocab_len)]) ** self.BETA\n        )\n        self.freq_sum = t.sum(self.transformed_freq_vec)\n        # Generate table\n        self.unigram_table = self.generate_unigram_table()\n        # print(self.freq_sum)\n        # print(type(self.freq_sum))\n    def forward(self, center, context, center_id, context_id, word_freq):\n        center, context = center.squeeze(), context.squeeze()  # batch_size x embed_size\n        # Compute true portion\n        freq_reg = self.get_reg_param(word_freq)\n        freq_reg = torch.tensor(freq_reg)\n        # try:\n        reg = freq_reg[context_id]\n        # except:\n        #     print(context_id.shape)\n        #     raise NotImplementedError\n        reg = reg.to(context.device)\n        reg = reg * ((context**2).sum())\n        # print('reg', reg.shape)\n        true_scores = (center * context).sum(-1)  # batch_size\n        loss = self.criterion(true_scores, t.ones_like(true_scores))\n        # print('loss',loss.shape)\n        # **need** loss = loss + reg **need**\n        # loss = loss + reg\n        loss = loss.mean()\n        #test_loss = loss.detach().item()\n        # Compute negatively sampled portion -\n        for i in range(self.NUM_SAMPLES):\n            samples, sample_reg = self.get_unigram_samples(n=center.shape[0], freq_reg= freq_reg)\n            # print(samples.shape)\n            # reg = (  0.01 / (( F.tanh(context_freq-10/2)+2.2))) * (context**2.sum())\n            neg_sample_scores = (center * samples).sum(-1)\n            # Update loss\n\n            new = self.criterion(neg_sample_scores, t.zeros_like(neg_sample_scores))\n            new = new.to(loss.device)\n            sample_reg = sample_reg.to(loss.device)\n            samples = samples.to(loss.device)\n            # **need** new += (sample_reg)*((samples**2).sum())\n            new = new.mean()\n            new = new.to(loss.device)\n            loss = loss + new\n            #x3 = neg_sample_scores.clone().detach().numpy()\n            #test_loss += self.bce_loss_w_logits(x3, t.zeros_like(neg_sample_scores).numpy())\n\n        return loss#, test_loss\n\n    def get_reg_param(self, freq ):\n        # reg = [ 1/(np.tanh((xi-20)/2)+2.2) for xi in freq ]\n        reg = [ 0.001/ np.log(xi+2) for xi in freq ]\n        return reg\n\n    @staticmethod\n    def bce_loss_w_logits(x, y):\n        max_val = np.clip(x, 0, None)\n        loss = x - x * y + max_val + np.log(np.exp(-max_val) + np.exp((-x - max_val)))\n        # print('===loss===',loss.shape)\n        return loss.mean()\n\n    def get_unigram_samples(self, n ,freq_reg):\n        \"\"\"\n        Returns a sample according to a unigram distribution\n        Randomly choose a value from self.unigram_table\n        \"\"\"\n        rand_idxs = self.unigram_table.draw(n).to(self.device)\n        sample_reg = freq_reg[rand_idxs]\n        return self.word_embeddings(rand_idxs).squeeze(), sample_reg\n\n    def get_unigram_prob(self, token_idx):\n        return (self.transformed_freq_vec[token_idx].item()) / self.freq_sum.item()\n\n    def generate_unigram_table(self):\n        # Probability at each index corresponds to probability of selecting that token\n        pdf = [self.get_unigram_prob(t_idx) for t_idx in range(0, self.vocab_len)]\n        # Generate the table from PDF\n        return AliasMultinomial(pdf, self.device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SkipGramEmbeddings(nn.Module):\n\n    def __init__(self, vocab_size, embed_len):\n        super(SkipGramEmbeddings, self).__init__()\n        self.word_embeds = nn.Embedding(vocab_size, embed_len)#, sparse=True)\n        #self.context_embeds = nn.Embedding(vocab_size, embed_len)# sparse=True)\n\n    def forward(self, center, context):\n        \"\"\"\n        Acts as a lookup for the center and context words' embeddings\n\n        :param center: The center word index\n        :param context: The context word index\n        :return: The embedding of the target word\n        \"\"\"\n        return self.word_embeds(center), self.word_embeds(context)\n\n    def nearest_neighbors(self, word, dictionary):\n        \"\"\"\n        Finds vector closest to word_idx vector\n        :param word_idx: Integer\n        :return: Integer corresponding to word vector in self.word_embeds\n        \"\"\"\n        vectors = self.word_embeds.weight.data.cpu().numpy()\n        index = dictionary.token2id[word]\n        query = vectors[index]\n\n        ranks = vectors.dot(query).squeeze()\n        denom = query.T.dot(query).squeeze()\n        denom = denom * np.sum(vectors ** 2, 1)\n        denom = np.sqrt(denom)\n        ranks = ranks / denom\n        mostSimilar = []\n        [mostSimilar.append(idx) for idx in ranks.argsort()[::-1]]\n        nearest_neighbors = mostSimilar[:10]\n        nearest_neighbors = [dictionary[comp] for comp in nearest_neighbors]\n\n        return nearest_neighbors\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer:\n\n    def __init__(self, args):\n        # Load data\n        self.args = args\n        self.writer = SummaryWriter(log_dir='./experiments/', flush_secs=3)\n        #self.dataset = PyPILangDataset(args, examples_path='../input/ffdata/pypi_examples.pth', dict_path='../input/ffdata/pypi_dict.pth')\n        self.dataset = FineFoodDataset(args,dict_path = '../input/ffdata/dict.pth')\n        self.vocab_size = len(self.dataset.dictionary)\n        # print(self.vocab_size)\n        # print(self.dataset.dictionary['amazon'])\n        # return\n        print(\"Finished loading dataset\")\n        self.word_freq = self.dataset.word_freq\n        self.dataloader = DataLoader(self.dataset, batch_size=args.batch_size,\n                                     shuffle=True, num_workers=args.workers)\n\n        self.model = SkipGramEmbeddings(self.vocab_size, args.embedding_len).to(args.device)\n        # self.dictionary\n        self.optim = optim.Adam(self.model.parameters(), lr=args.lr)\n        self.sgns = SGNSLoss(self.dataset, self.model.word_embeds, self.args.device)\n        print(self.word_freq[ self.dataset.dictionary.token2id['word'] ])\n        # Add graph to tensorboard\n        #self.writer.add_graph(self.model, iter(self.dataloader).next()[0])\n    \n    def vis_freq(self):\n        x = set(self.word_freq.values())\n        freq_values = list(self.word_freq.values())\n        y = [ freq_values.count(xi) for xi in x ]\n        # print(x)\n        # print(y)\n        x = list(x)\n        plt.plot(x,y)\n        plt.show()\n        self.vis_tanh(0.01,20, x,y)\n       \n    def reg_param(self, beta, v, freq_param):\n        v = torch.tensor(v, dtype = torch.float)\n        freq_param = torch.tensor(freq_param, dtype = torch.float)\n        return 1 / (  beta / (( torch.tanh(v-freq_param/2)+2.2)))\n\n    def vis_tanh(self, beta, freq_param,x,y):\n        # y = torch.tensor(y,dtype = torch.float)\n        # y = F.softmax(y)\n        y_h = [self.reg_param(beta, xi, freq_param) for xi in y ]\n        plt.plot(x, y_h)\n        plt.show()\n    \n\n    def train(self):\n        print('Training on device: {}'.format(self.args.device))\n\n        # Log embeddings!\n        print('\\nRandom embeddings:')\n        # for word in self.dataset.queries:\n        #     print(f'word: {word} neighbors: {self.model.nearest_neighbors(word, self.dataset.dictionary)}')\n\n        # for epoch in range(self.args.epochs):\n        for epoch in range(5):\n\n            print(f'Beginning epoch: {epoch + 1}/{self.args.epochs}')\n            running_loss = 0.0 #testing_loss = 0.0, 0.0\n            global_step = epoch * len(self.dataloader)\n            num_examples = 0\n\n            for i, data in enumerate(tqdm(self.dataloader, total = len(self.dataloader), desc = 'epoch : '+str(epoch))):\n                # Unpack data\n                center, context, center_freq, context_freq  = data\n                center, context = center.to(self.args.device), context.to(self.args.device)\n                # print(context.shape) # 942, 1\n                # Remove accumulated gradients\n                self.optim.zero_grad()\n                # Get context vectors\n                center_embed, context_embed = self.model(center, context)\n                # Calc loss: SGNS\n                loss = self.sgns(center_embed, context_embed, center, context, self.word_freq)\n                # Backprop and update\n                loss.backward()\n                self.optim.step()\n\n                # Keep track of loss\n                running_loss += loss.item()\n                global_step += 1\n                num_examples += len(data)  # Last batch's size may not equal args.batch_size\n\n                # TESTING LOSS\n                #testing_loss += test_loss\n\n                # Log at step\n                #if global_step % self.args.log_step == 0:\n                #    norm = (i + 1) * num_examples\n                #    self.log_step(epoch, global_step, running_loss/norm, center, context)\n\n            norm = (i + 1) * num_examples\n            # self.log_and_save_epoch(epoch, running_loss / norm)\n            torch.save(self.model.state_dict(), './model_result/normal/emb_model' + str(epoch)+'.pth')\n            self.log_step(epoch, global_step, running_loss / norm)#, testing_loss / norm)\n            print('\\nGRAD:', np.sum(self.model.word_embeds.weight.grad.clone().detach().cpu().numpy()))\n\n        self.writer.close()\n\n    def log_and_save_epoch(self, epoch, loss):\n        # Visualize document embeddings\n        self.writer.add_embedding(\n            self.model.word_embeds.weight,\n            global_step=epoch,\n            tag=f'we_epoch_{epoch}',\n        )\n\n        # Save checkpoint\n        print(f'Beginning to save checkpoint')\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optim.state_dict(),\n            'loss': loss,\n        }, f'epoch_{epoch}_ckpt.pth')\n        print(f'Finished saving checkpoint')\n\n    def log_step(self, epoch, global_step, loss):\n        print(f'#############################################')\n        print(f'EPOCH: {epoch} | STEP: {global_step} | LOSS {loss}')# | TEST LOSS {test_loss}')\n        print(f'#############################################')\n\n        #self.writer.add_scalar('train_loss', loss, global_step)\n\n        # Log embeddings!\n        print('\\nLearned embeddings:')\n        for word in self.dataset.queries:\n            print(f'word: {word} neighbors: {self.model.nearest_neighbors(word, self.dataset.dictionary)}')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def get_args():\n    parser = argparse.ArgumentParser(description=\"PyTorch LDA2Vec Training\")\n\n    \"\"\"\n    Data handling\n    \"\"\"\n    parser.add_argument('--dataset-dir', type=str, default='../input/ffdata/',\n                        help='dataset directory (default: ../input/ffdata/)')\n    parser.add_argument('--workers', type=int, default=4, metavar='N',\n                       help='dataloader threads (default: 4)')\n    parser.add_argument('--window-size', type=int, default=5, help='Window size\\\n                        used when generating training examples (default: 5)')\n    parser.add_argument('--file-batch-size', type=int, default=250, help='Batch size\\\n                        used when multi-threading the generation of training examples\\\n                        (default: 250)')\n\n    \"\"\"\n    Model Parameters\n    \"\"\"\n    parser.add_argument('--embedding-len', type=int, default=256, help='Length of\\\n                        embeddings in model (default: 256)')\n\n    \"\"\"\n    Training Hyperparameters\n    \"\"\"\n    parser.add_argument('--epochs', type=int, default=15, metavar='N',\n                        help='number of epochs to train for - iterations over the dataset (default: 15)')\n    parser.add_argument('--batch-size', type=int, default=1024,\n                        metavar='N', help='number of examples in a training batch (default: 1024)')\n    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n                        help='learning rate (default: 1e-3)')\n    parser.add_argument('--seed', type=int, default=42, metavar='S',\n                        help='random seed (default: 42)')\n\n    \"\"\"\n    Checkpoint options\n    \"\"\"\n    parser.add_argument('--log-step', type=int, default=250, help='Step at which for every step training info\\\n                        is logged. (default: 250)')\n\n    \"\"\"\n    Training Settings\n    \"\"\"\n    parser.add_argument('--device', type=str, default=t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\"),\n                        help='device to train on (default: cuda:0 if cuda is available otherwise cpu)')\n\n    return parser.parse_args(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FineFoodDataset(SkipGramDataset):\n\n    def __init__(self, args, examples_path='./examples.pth', dict_path='./dict.pth'):\n        SkipGramDataset.__init__(self, args)\n        self.name = 'Fine Food Dataset'\n        self.queries = ['good', 'tasty','bad', 'negtively', 'charming']\n        # 如果要直接讀取的話，要把 self.raw_doc = None, examples_path = None 都註解掉, 改成已經存下來的\n        self.raw_doc = None\n        examples_path = None\n        dict_path = None\n        if examples_path is not None and dict_path is not None:\n            self.load(examples_path, dict_path)\n        else:\n            # nlp = English()\n            # Tokenizer(nlp.vocab)\n            self.tokenizer = Tokenizer(args)\n            # self.tokenizer = Tokenizer(nlp.vocab)\n            self.files = self.tokenize_files()\n            self.generate_examples_serial()\n\n            #def save(self, examples_path, dict_path):\n            self.save('Reviews.pth', 'Reviews.pth')\n\n        print(f'There are {len(self.dictionary)} tokens and {len(self.examples)} examples.')\n        self._build_dictionary()\n        self.queries = self.get_unusual_words()\n        # self.word_freq[id] = new_freq\n        \n    def get_unusual_words(self):\n    #     # self.dictionary : dict \n    #         # key: word id \n    #         # value: word freq\n    #     # 取出來要轉成字（gensim dictionary class)\n        total = 0\n        dict2 = {}\n        list2 = []\n        for k in self.word_freq.keys():\n            if self.word_freq[k] <= 10: ## self.word_freq[k] <= 9 | 輸出 0 個\n                dict2.setdefault(k,self.word_freq[k])\n                total += 1 \n        for i in dict2.keys():\n            list2.append(self.dictionary[i])\n        return list2\n        \n        \n        \n    def load_files(self):\n        return self.files\n    \n    \n    def remove_token(self,Target = 'good'):\n        remove_word_count , after_word_count = 0 , 0\n        target_word = Target\n        for sentence_token in self.raw_doc:\n            for word in sentence_token: \n                if word == target_word:\n                    prob = np.random.randint(10000)\n                    if prob >= 100:  #rate\n                        sentence_token.remove(target_word)\n                        remove_word_count += 1\n                    else:\n                        after_word_count += 1 \n            # print(sentence_token)\n        print(\"'\",target_word, \"' is the target word!\")  \n        print(\"Original: \",remove_word_count + after_word_count,\"words, we have removed: \",remove_word_count, \", currently remain: \", after_word_count )\n        return self.raw_doc\n    \n\n    def tokenize_files(self):\n        ff_df = pd.read_csv('../input/ffdata/Reviews.csv', na_filter=False)\n        review_data = ff_df['Text'].values\n        review_data = review_data[:100000]\n        # f = self.preprocess(review_data)\n        self.raw_doc = [self.tokenizer.tokenize_doc(f) for f in tqdm(review_data, desc='Tokenizing Docs')]\n        # self.raw_doc = self.remove_token('good')\n        self.raw_doc = self.remove_token('tasty')\n        # self.raw_doc = self.remove_token('bad')\n        return self.raw_doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"args = get_args()\ntrainer = Trainer(args)\nw = trainer.dataset.word_freq","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}