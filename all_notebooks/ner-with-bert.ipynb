{"cells":[{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert==0.4.0\n!pip install transformers\n!pip install seqeval","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nmale_name=pd.read_csv(\"../input/indian-names/Indian-Male-Names.csv\")\nfemale_name=pd.read_csv(\"../input/indian-names/Indian-Female-Names.csv\")\ndata = pd.read_csv(\"../input/entity-annotated-corpus/ner_dataset.csv\", encoding=\"latin1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"male_name.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"male_list=[y for x in np.array(male_name.name) for y in str(x).split()]\nfemale_list=[y for x in np.array(female_name.name) for y in str(x).split()]\n(len(male_list),len(female_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing dublicate names\nname_list= list(set((male_list+female_list)))\n# removing invailid names\nname_list= [re.sub('[^a-zA-Z]+', '', x) for x in name_list if len(x) > 1]\nname_list= list(set([x for x in name_list if len(x) > 4]))\n# name_list=[ for x in name_list]\nlen(name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if name is present in the list\nname_list.index(\"kumar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\nprint(len(tokenizer))\ntokenizer.tokenize(\"my name is abhishek kumar.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how to increase the vocabulary of Bert model and tokenizer\nnum_added_toks = tokenizer.add_tokens(name_list)\n# some names are already present in the dictionary\nprint('We have added', num_added_toks, 'tokens')\nprint(len(tokenizer))\ntokenizer.tokenize(\"my name is abhishek kumar.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndf1 = pd.DataFrame({ \"Sentence #\":['Sentence: 47960']*6, \n                    \"Word\":['my', 'name', 'is', 'abhishek', 'kumar','.'],  \n                    \"POS\":[None]*6,\n                    \"Tag\":['O','O','O','B-per','I-per','O']})\ndf2 = pd.DataFrame({ \"Sentence #\":['Sentence: 47961']*7, \n                    \"Word\":['my', 'name', 'is', 'ritik', 'kumar','gupta','.'],  \n                    \"POS\":[None]*7,\n                    \"Tag\":['O','O','O','B-per','I-per','I-per','O']})\ndf3 = pd.DataFrame({ \"Sentence #\":['Sentence: 47962']*6, \n                    \"Word\":['I', 'am', 'pranav', 'singh', 'murari','.'],  \n                    \"POS\":[None]*6,\n                    \"Tag\":['O','O','B-per','I-per','I-per','O']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding custom data in ner_dataset \ndata=data.append([df1,df2,df3]).reset_index(drop=True)\ndata = data.fillna(method=\"ffill\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.fillna(method=\"ffill\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentenceGetter(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getter = SentenceGetter(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\nsentences[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [[s[2] for s in sent] for sent in getter.sentences]\nprint(labels[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_vals = list(set(data[\"Tag\"].values))\ntag2idx = {t: i for i, t in enumerate(tags_vals)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertForTokenClassification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 75\nbs = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.get_device_name(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint(tokenized_texts[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n                     dtype=\"long\", truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_masks = [[float(i>0) for i in ii] for ii in input_ids]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n                                                            random_state=2018, test_size=0.1)\ntr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_inputs = torch.tensor(tr_inputs)\nval_inputs = torch.tensor(val_inputs)\ntr_tags = torch.tensor(tr_tags)\nval_tags = torch.tensor(val_tags)\ntr_masks = torch.tensor(tr_masks)\nval_masks = torch.tensor(val_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\nmodel.cuda()\n\nFULL_FINETUNING = True\nif FULL_FINETUNING:\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0}\n    ]\nelse:\n    param_optimizer = list(model.classifier.named_parameters()) \n    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\noptimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n# model.resize_token_embeddings(len(tokenizer))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from seqeval.metrics import f1_score\nfrom tqdm import trange\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=2).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nmax_grad_norm = 1.0\n\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # TRAIN loop\n    model.train()\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(train_dataloader):\n        # add batch to gpu\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        # forward pass\n        loss = model(b_input_ids, token_type_ids=None,\n                     attention_mask=b_input_mask, labels=b_labels)\n        # backward pass\n        loss.backward()\n        # track train loss\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        # update parameters\n        optimizer.step()\n        model.zero_grad()\n    # print train loss per epoch\n    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n    # VALIDATION on validation set\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    predictions , true_labels = [], []\n    for batch in valid_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():\n            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n                                  attention_mask=b_input_mask, labels=b_labels)\n            logits = model(b_input_ids, token_type_ids=None,\n                           attention_mask=b_input_mask)\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n        true_labels.append(label_ids)\n        \n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        eval_loss += tmp_eval_loss.mean().item()\n        eval_accuracy += tmp_eval_accuracy\n        \n        nb_eval_examples += b_input_ids.size(0)\n        nb_eval_steps += 1\n    eval_loss = eval_loss/nb_eval_steps\n    print(\"Validation loss: {}\".format(eval_loss))\n    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\npredictions = []\ntrue_labels = []\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\nfor batch in valid_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n                              attention_mask=b_input_mask, labels=b_labels)\n        logits = model(b_input_ids, token_type_ids=None,\n                       attention_mask=b_input_mask)\n        \n    logits = logits.detach().cpu().numpy()\n    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n    label_ids = b_labels.to('cpu').numpy()\n    true_labels.append(label_ids)\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n    eval_loss += tmp_eval_loss.mean().item()\n    eval_accuracy += tmp_eval_accuracy\n\n    nb_eval_examples += b_input_ids.size(0)\n    nb_eval_steps += 1\n\npred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\nvalid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\nprint(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\nprint(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\nprint(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\nsent=\"my name is ajay parker singh.\"\na=[tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))]\ninput_ids=pad_sequences(a,maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\nattention_masks=[[float(i>0) for i in ii] for ii in input_ids]\ntr_inputs = torch.tensor(input_ids)\ntr_masks = torch.tensor(attention_masks)\nvalid_data = TensorDataset(tr_inputs, tr_masks)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor batch in valid_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask = batch\n    with torch.no_grad():\n#         tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n#                               attention_mask=b_input_mask, labels=b_labels)\n        logits = model(b_input_ids, token_type_ids=None,\n                       attention_mask=b_input_mask)\n        \n    logits = logits.detach().cpu().numpy()\n    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\ntags=[[p_i for p_i in p] for p in predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom Tokenizer\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{:15}||{}\".format(\"Word\", \"Prediction\"))\nprint(30 * \"=\")\nfor w, pred in zip(tokenize(sent), tags[0]):\n    print(\"{:15}: {:5}\".format(w, tags_vals[pred]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}