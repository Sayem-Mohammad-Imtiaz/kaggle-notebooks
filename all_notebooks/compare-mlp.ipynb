{"cells":[{"metadata":{"_uuid":"8308250b-060b-477f-b778-2b448c2dfa0a","_cell_guid":"fcabfef3-32e5-4620-92c5-46974dce4f9a","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read word development indicator data\nclimate = pd.read_csv('../input/world-development-indicators-by-countries/climate.csv', delimiter=',')\ngg_emissions =pd.read_csv('../input/world-development-indicators-by-countries/greenhouse_gas_emissions.csv', delimiter=',')\nsustainability = pd.read_csv('../input/world-development-indicators-by-countries/sustainability.csv', delimiter=',')\nagriculture=pd.read_csv('../input/world-development-indicators-by-countries/argicultural_inputs.csv', delimiter=',')\nemissions =pd.read_csv('../input/world-development-indicators-by-countries/emissions.csv', delimiter=',')\nrural =pd.read_csv('../input/world-development-indicators-by-countries/rural.csv', delimiter=',')\nenergy =pd.read_csv('../input/world-development-indicators-by-countries/energy.csv', delimiter=',')\nwater =pd.read_csv('../input/world-development-indicators-by-countries/freshwater.csv', delimiter=',')\n\n# read footprint data\nfootprint = pd.read_csv('../input/national-footprint-accounts-2018/NFA 2018.csv', delimiter=',')\n\n# create a tuple for world development indicator data\nindicators = (climate, gg_emissions, sustainability, agriculture, emissions, rural, energy, water)\n\n# import functions to build data set\nfrom dataprocess import *\n\n# initialize footprint data\nfp=initialize_footprint(footprint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set 2 features\n#follow the order climate, gg_emissions, sustainability, agriculture, emissions, rural, energy, water\nkeyList = [\n    [],\n    [],\n    [3, 4, 9],\n    [2,10],\n    [10,12],\n    [2, 6,12],\n    [10,3],\n    []\n]  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build wdi data frame\n#Set 1 features\nkeyList = [\n    [],\n    [2],\n    [3,9,4],\n    [8],\n    [],\n    [],\n    [2],\n    [3]\n] \n\nwdi_df = build_wdi_df(indicators, keyList, 0)\n\n# use the following metrics to build dataset\nmetric1 = 'BiocapTotGHA'\nmetric2 = 'EFConsTotGHA'\ndataName = 'total'\nyear = 2014\ncountries, data, labels = build_dataset(wdi_df, fp, metric1, metric2, dataName, year)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Experiment with different ANN\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\n\nclf = MLPClassifier(hidden_layer_sizes=(16,5),\n                    max_iter=5000,\n                    activation = 'relu',\n                    solver='adam',\n                    random_state=1)\nscores=[]\nestimators = []\ntrain_data = []\ntrain_labels = []\ntest_data = []\ntest_labels = []\n\nfor k in range(20):\n    X_train, X_test, y_train, y_test = train_test_split(\n    data, labels, test_size=0.33)\n    clf.fit(X_train, y_train) \n    scores.append(clf.score(X_test, y_test))\n    estimators.append(clf)\n    train_data.append(X_train)\n    train_labels.append(y_train)\n    test_data.append(X_test)\n    test_labels.append(y_test)\nprint(scores)\n\nprint('Average score:', sum(scores)/len(scores))\nprint('Standard deviation:', np.std(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nclf = MLPClassifier(hidden_layer_sizes=(16, 5), \n                    max_iter=10,\n                    activation = 'relu',\n                    solver='adam',\n                    random_state=1,\n                    warm_start=True)\nj = 19\ntrain_scores = []\ntest_scores =[]\n\nfor k in range(1000):\n    clf.fit(train_data[j], train_labels[j])\n    train_scores.append(clf.score(train_data[j], train_labels[j]))\n    test_scores.append(clf.score(test_data[j], test_labels[j]))\n\nylim=()\ntrain_iter = [k for k in range(10, 10010, 10)]\n\nplt.figure()\nplt.title('Learning Curve')\nif ylim is not None:\n    plt.ylim(*ylim)\n\nplt.plot(train_iter, train_scores, color='r', label='Training')\nplt.plot(train_iter, test_scores, color='b', label='Validation')\nplt.legend(loc=\"best\")\nplt.show()   ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}