{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Recurrent Neural Network in PyTorch</center></h1>","metadata":{}},{"cell_type":"markdown","source":"Table of Contents: <a id=100></a>\n\n1. [Packages](#1)\n2. [Data definition](#2)\n    - 2.1 [Declaring a tensor `x`](#3)\n    - 2.2 [Creating a tensor `y` as a sin function of `x`](#4)\n    - 2.3 [Plotting `y`](#5)\n3. [Batching the data](#6)\n    - 3.1 [Splitting the data in train/test set](#7)\n    - 3.2 [Creating the batches of data](#8)\n4. [Defining the model](#9)\n    - 4.1 [Model class](#10)\n    - 4.2 [Model instantiation](#11)\n    - 4.3 [Training](#12)\n5. [Alcohol Sales dataset](#13)\n    - 5.1 [Loading and plotting](#14)\n    - 5.2 [Prepare and normalize](#15)\n    - 5.3 [Modelling](#16)\n    - 5.4 [Predictions](#17)","metadata":{}},{"cell_type":"markdown","source":"Recurrent Neural Networks are a type of neural networks that are designed to work on sequence prediction models. RNNs can be used for text data, speech data, classification problems and generative models. Unlike ANNs, RNNs' prediction are based on the past prediction as well as the current input. RNNs are networks with loops in them allowing information to persist.\n\nEach node of an **RNN** consists of 2 inputs:\n1. Memory unit\n2. Event unit\n\n`M(t-1)` is the memory unit or the output of the previous prediction. `E(t)` is the current event or the information being provided at the present time. `M(t)` is the output of the current node or the output at the present time in the sequence.","metadata":{}},{"cell_type":"markdown","source":"### 1. Packages <a id=1></a>\n[back to top](#100)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Data definition <a id=2></a>\n[back to top](#100)","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I'm going to train a very simple LSTM model, which is a type of RNN architecture to do time series prediction. Given some input data, it should be able to generate a prediction for the next step. I'll be using a **Sin** wave as an example as it's very easy to visualiase the behaviour of a sin wave.\n","metadata":{}},{"cell_type":"markdown","source":"#### 2.1 Declaring a tensor `x` <a id=3></a>","metadata":{}},{"cell_type":"code","source":"x = torch.linspace(0,799,800)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Creating a tensor `y` as a sin function of `x` <a id=4></a>","metadata":{}},{"cell_type":"code","source":"y = torch.sin(x*2*3.1416/40)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3 Plotting `y` <a id=5></a>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.xlim(-10,801)\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"sin\")\nplt.title(\"Sin plot\")\nplt.plot(y.numpy(),color='#8000ff')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Batching the data <a id=6></a>\n[back to top](#100)","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Splitting the data in train/test set <a id=7></a>","metadata":{}},{"cell_type":"code","source":"test_size = 40\ntrain_set = y[:-test_size]\ntest_set = y[-test_size:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1.1 Plotting the training/testing set","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.xlim(-10,801)\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"sin\")\nplt.title(\"Sin plot\")\nplt.plot(train_set.numpy(),color='#8000ff')\nplt.plot(range(760,800),test_set.numpy(),color=\"#ff8000\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2 Creating the batches of data <a id=8></a>","metadata":{}},{"cell_type":"markdown","source":"While working with LSTM models, we divide the training sequence into series of overlapping windows. The label used for comparison is the next value in the sequence.\n\nFor example if we have series of of 12 records and a window size of 3, we feed [x1, x2, x3] into the model, and compare the prediction to `x4`. Then we backdrop, update parameters, and feed [x2, x3, x4] into the model and compare the prediction to `x5`. To ease this process, I'm defining a function `input_data(seq,ws)` that created a list of (seq,labels) tuples. If `ws` is the window size, then the total number of (seq,labels) tuples will be `len(series)-ws`.","metadata":{}},{"cell_type":"code","source":"def input_data(seq,ws):\n    out = []\n    L = len(seq)\n    \n    for i in range(L-ws):\n        window = seq[i:i+ws]\n        label = seq[i+ws:i+ws+1]\n        out.append((window,label))\n    \n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2.1 Calling the `input_data` function\nThe length of `x` = 800\n\nThe length of `train_set` = 800 - 40 = 760\n\nThe length of `train_data` = 760 - 40 - 720","metadata":{}},{"cell_type":"code","source":"window_size = 40\ntrain_data = input_data(train_set, window_size)\nlen(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2.2 Checking the 1st value from train_data","metadata":{}},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Defining the model <a id=9></a>\n[back to top](#100)","metadata":{}},{"cell_type":"markdown","source":"#### 4.1 Model Class <a id=10></a>","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    \n    def __init__(self,input_size = 1, hidden_size = 50, out_size = 1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(input_size, hidden_size)\n        self.linear = nn.Linear(hidden_size,out_size)\n        self.hidden = (torch.zeros(1,1,hidden_size),torch.zeros(1,1,hidden_size))\n    \n    def forward(self,seq):\n        lstm_out, self.hidden = self.lstm(seq.view(len(seq),1,-1), self.hidden)\n        pred = self.linear(lstm_out.view(len(seq),-1))\n        return pred[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2 Model Instantiation <a id = 11></a>","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\nmodel = LSTM()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 4.2.1 Printing the model","metadata":{}},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3 Training <a id = 12></a>","metadata":{}},{"cell_type":"markdown","source":"During training, I'm visualising the prediction process for the test data on the go. It will give a better understanding of how the training is being carried out in each epoch. The training sequence is represented in <span style=\"color:#8000ff\">purple</span> while the predicted sequence in represented in <span style=\"color:#ff8000\">orange</span>.","metadata":{}},{"cell_type":"code","source":"epochs = 10\nfuture = 40\n\nfor i in range(epochs):\n    \n    for seq, y_train in train_data:\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size),\n                       torch.zeros(1,1,model.hidden_size))\n        \n        y_pred = model(seq)\n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n    print(f\"Epoch {i} Loss: {loss.item()}\")\n    \n    preds = train_set[-window_size:].tolist()\n    for f in range(future):\n        seq = torch.FloatTensor(preds[-window_size:])\n        with torch.no_grad():\n            model.hidden = (torch.zeros(1,1,model.hidden_size),\n                           torch.zeros(1,1,model.hidden_size))\n            preds.append(model(seq).item())\n        \n    loss = criterion(torch.tensor(preds[-window_size:]), y[760:])\n    print(f\"Performance on test range: {loss}\")\n    \n    plt.figure(figsize=(12,4))\n    plt.xlim(700,801)\n    plt.grid(True)\n    plt.plot(y.numpy(),color='#8000ff')\n    plt.plot(range(760,800),preds[window_size:],color='#ff8000')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Alcohol Sales dataset <a id=13></a>\n[back to top](#100)","metadata":{}},{"cell_type":"markdown","source":"#### 5.1 Loading and plotting <a id=14></a>","metadata":{}},{"cell_type":"markdown","source":"##### 5.1.1 Importing the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/for-simple-exercises-time-series-forecasting/Alcohol_Sales.csv\", index_col = 0, parse_dates = True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.1.2 Dropping the empty rows","metadata":{}},{"cell_type":"code","source":"df.dropna(inplace=True)\nlen(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.1.3 Plotting the Time Series Data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12,4))\nplt.title('Alcohol Sales')\nplt.ylabel('Sales in million dollars')\nplt.grid(True)\nplt.autoscale(axis='x',tight=True)\nplt.plot(df['S4248SM144NCEN'],color='#8000ff')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.2 Prepare and normalize <a id=15></a>","metadata":{}},{"cell_type":"markdown","source":"##### 5.2.1 Preparing the data","metadata":{}},{"cell_type":"code","source":"#extracting the time series values\ny = df['S4248SM144NCEN'].values.astype(float) \n\n#defining a test size\ntest_size = 12\n\n#create train and test splits\ntrain_set = y[:-test_size]\ntest_set = y[-test_size:]\ntest_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.2.2 Normalize the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# instantiate a scaler\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\n# normalize the training set\ntrain_norm = scaler.fit_transform(train_set.reshape(-1, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.2.3 Prepare data for LSTM model","metadata":{}},{"cell_type":"code","source":"# convert train_norm to a tensor\ntrain_norm = torch.FloatTensor(train_norm).view(-1)\n\n# define a window size\nwindow_size = 12\n# define a function to create sequence/label tuples\ndef input_data(seq,ws):\n    out = []\n    L = len(seq)\n    for i in range(L-ws):\n        window = seq[i:i+ws]\n        label = seq[i+ws:i+ws+1]\n        out.append((window,label))\n    return out\n\n# apply input_data to train_norm\ntrain_data = input_data(train_norm, window_size)\nlen(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.2.4 Printing the first tuple","metadata":{}},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.3 Modelling <a id=16></a>","metadata":{}},{"cell_type":"markdown","source":"##### 5.3.1 Model definition","metadata":{}},{"cell_type":"code","source":"class LSTMnetwork(nn.Module):\n    def __init__(self,input_size=1,hidden_size=100,output_size=1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # add an LSTM layer:\n        self.lstm = nn.LSTM(input_size,hidden_size)\n        \n        # add a fully-connected layer:\n        self.linear = nn.Linear(hidden_size,output_size)\n        \n        # initializing h0 and c0:\n        self.hidden = (torch.zeros(1,1,self.hidden_size),\n                       torch.zeros(1,1,self.hidden_size))\n\n    def forward(self,seq):\n        lstm_out, self.hidden = self.lstm(\n            seq.view(len(seq),1,-1), self.hidden)\n        pred = self.linear(lstm_out.view(len(seq),-1))\n        return pred[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.3.3 Instantiation, loss and optimizer","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\n\n# instantiate\nmodel = LSTMnetwork()\n\n# loss\ncriterion = nn.MSELoss()\n\n#optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.3.4 Training","metadata":{}},{"cell_type":"code","source":"epochs = 100\n\nimport time\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    for seq, y_train in train_data:\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size),\n                        torch.zeros(1,1,model.hidden_size))\n        \n        y_pred = model(seq)\n        \n        loss = criterion(y_pred, y_train)\n        loss.backward()\n        optimizer.step()\n        \n    print(f'Epoch: {epoch+1:2} Loss: {loss.item():10.8f}')\n    \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.4 Predictions <a id=17></a>","metadata":{}},{"cell_type":"markdown","source":"##### 5.4.1 Test set predictions","metadata":{}},{"cell_type":"code","source":"future = 12\n\npreds = train_norm[-window_size:].tolist()\n\nmodel.eval()\n\nfor i in range(future):\n    seq = torch.FloatTensor(preds[-window_size:])\n    with torch.no_grad():\n        model.hidden = (torch.zeros(1,1,model.hidden_size),\n                        torch.zeros(1,1,model.hidden_size))\n        preds.append(model(seq).item())\npreds[window_size:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.4.2 Original test set","metadata":{}},{"cell_type":"code","source":"df['S4248SM144NCEN'][-12:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.4.3 Inverting the normalised values","metadata":{}},{"cell_type":"code","source":"true_predictions = scaler.inverse_transform(np.array(preds[window_size:]).reshape(-1, 1))\ntrue_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.4.4 Plotting","metadata":{}},{"cell_type":"code","source":"x = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\nplt.figure(figsize=(12,4))\nplt.title('Alcohol Sales')\nplt.ylabel('Sales in million dollars')\nplt.grid(True)\nplt.autoscale(axis='x',tight=True)\nplt.plot(df['S4248SM144NCEN'], color='#8000ff')\nplt.plot(x,true_predictions, color='#ff8000')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 5.5.5 Zooming the test predictions","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,4))\nplt.title('Alcohol Sales')\nplt.ylabel('Sales in million dollars')\nplt.grid(True)\nplt.autoscale(axis='x',tight=True)\nfig.autofmt_xdate()\n\nplt.plot(df['S4248SM144NCEN']['2017-01-01':], color='#8000ff')\nplt.plot(x,true_predictions, color='#ff8000')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you liked the notebook, consider giving an upvote.\n[back to top](#100)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}