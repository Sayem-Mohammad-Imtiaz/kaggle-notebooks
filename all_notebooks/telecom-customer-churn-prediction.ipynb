{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telecom Customer Churn Prediction\n\n<b><br>Problem statement: Based on all information from this data set, a model was made to predict whether a particular customer will churn or not.\n<br><br>During the model development, data set was separated in train (70% of data) and test(30% of data) data. \n<br>On the train data, a model was build that calculates which attributes are significantly related to churn (eg. 'tenure - Contract Duration', 'PhoneService', 'PaperlessBilling', 'TotalCharges', OnlineBackup', 'TechSupport'..). \n<br> When model was applied on the test data, Churn was predicted with an accuracy of 78%</b>\n<br><br> Source: Kaggle\n<br>This data set contains the following data with following features:\n1. churn_data.csv\n    * 'customerID'\n    * 'tenure'\n    * 'PhoneService'\n    * 'PaperlessBilling'\n    * 'PaymentMethod'\n    * 'MonthlyCharges'\n    * 'TotalCharges'\n    * 'Churn' \n2. customer_data.csv\n    * 'customerID'\n    * 'gender'\n    * 'SeniorCitizen'\n    * 'Partner'\n    * 'Dependents'    \n3. internet_data.csv\n    * 'customerID'\n    * 'MultipleLines'\n    * 'InternetService'\n    * 'OnlineSecurity'\n    * 'OnlineBackup'\n    * 'DeviceProtection'\n    * 'TechSupport'\n    * 'StreamingTV'\n    * 'StreamingMovies'\n\n"},{"metadata":{},"cell_type":"markdown","source":"### IMPORTING NECESSARY LIBRARIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### IMPORTING AND MERGING DATASETS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing datasets\nchurn = pd.read_csv('../input/logisticregression-telecomcustomer-churmprediction/churn_data.csv')\ncustomer = pd.read_csv('../input/logisticregression-telecomcustomer-churmprediction/customer_data.csv')\ninternet = pd.read_csv('../input/logisticregression-telecomcustomer-churmprediction/internet_data.csv')\n\n# merging churn and customer dataframe on customerID\ndf_1 = pd.merge(churn, customer, how='inner', on='customerID')\n\n# merging df_1 and internet dataframe on customerID\ndata = pd.merge(df_1,internet, how='inner', on = 'customerID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. EXPLORATORY DATA ANALYSIS AND DATA CLEANING\n### Checking merged dataframe and data statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With data.info() can be seen that the data set has 7042 entries with textual (object) and numerical (int64 & float64) data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Data set does not contain any null values"},{"metadata":{},"cell_type":"markdown","source":"With data analysis, it is discovered that some of the values are in the wrong format and that some of the data contain whitespaces.\n<br> In order for a model to work properly, it is necessary to make 'data wrangling', eg. remove whitespace and convert data to right format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TotalCharges is an object and not float!!!\n# We donÂ´t have null values but from error we can see that column 'TotalCharges' contains whitespace = ' '\n\n# data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many whitespace = ' ' we have in column 'TotalCharges'\ndata['TotalCharges'].str.isspace().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Data set contains 11 data that contain whitespaces"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['TotalCharges'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing whitespace to NAN values and converting to numeric data (float)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing whitespace to NAN values and converting to numeric data (float)\ndata['TotalCharges'] = data['TotalCharges'].replace(' ', np.nan)\ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many NAN values is in column\ndata['TotalCharges'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing NAN values with mean value from all data in column 'TotalCharges'\n\n#new_value = data['TotalCharges'].astype('float').mean(axis=0)\n\nnew_value = (data['TotalCharges']/data['MonthlyCharges']).mean()*data['MonthlyCharges']\ndata['TotalCharges'].replace(np.nan, new_value, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many NAN values is in column 'TotalCharges' after replacing NAN with mean \ndata['TotalCharges'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'Contract', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ration of contracts: Month-to-month vs. One year Contract vs. Two year"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'PaymentMethod', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'Churn', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From ~7000 customers approximately ~2000 has churned"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'gender', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. DATA PRETPROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Making list for columns for One Hot Encoding\nlista = ['PhoneService','PaperlessBilling', 'Churn', 'Partner', 'Dependents']\n\n# With .map method and lambda function turning Yes/No into 1/0\ndata[lista] = data[lista].apply(lambda x:x.map({'Yes': 1, \"No\": 0}))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking other data npr. 'StreamingMovies'\ndata['StreamingMovies'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making dummy variables for categorical data with more inputs\n\ndata_dummy = pd.get_dummies(data[['Contract', 'PaymentMethod', 'gender', 'MultipleLines', 'InternetService', \n                                     'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n                                    'TechSupport', 'StreamingTV', 'StreamingMovies']], drop_first=True)\ndata_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging original data frame with 'dummy' dataframe\ndata = pd.concat([data,data_dummy], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping attributes for which we made dummy variables\n\ndata = data.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', \n                        'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n                        'TechSupport', 'StreamingTV', 'StreamingMovies'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. MODEL DEVELOPMENT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting Independent variable (X) and Dependent variable (y)\nX = data.drop(['Churn','customerID'], axis=1)\ny = data['Churn']\n\n# spliting data into train and test samples\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since data are within long-range (0 - 8684) it is necessary to perform data standardization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# just checking X_train before standardization\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardization on X_train\nX_train[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ordinary Least Squares: sm.OLS(y, X)\nmod1 = sm.OLS(y_train,X_train,data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results1 = mod1.fit()\nprint(results1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LogisticRegression object as lr\nlr = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#  RFE - Feature ranking with recursive feature elimination.\nrfe = RFE(estimator=lr, n_features_to_select=20, step=1)    \nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print summaries for the selection of attributes\nprint(rfe.support_)\nprint(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making list and dataframe to see what attributes was selected\nlist_for_df = list(zip(X_train.columns, rfe.support_, rfe.ranking_))\ndf = pd.DataFrame(list_for_df, columns = ['X_train.columns', 'rfe.support_', 'rfe.ranking_'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the list of attributes that are selected\nsel_att = X_train.columns[rfe.support_]\nsel_att","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding a constant\nX_train_const = sm.add_constant(X_train[sel_att])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ordinary Least Squares: sm.OLS(y, X)\nmod2 = sm.OLS(y_train,X_train_const,data=data)\nresults2 = mod2.fit()\nprint(results2.summary())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_predicted_train = results2.predict(X_train_const)\ny_predicted_train.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# making dataframe for train values and predicted values with 'customerID'as index\nfinal_y_predicted_df = pd.DataFrame(index= y_train.index, columns=('Churn','Churn_Predicted_Initial'))\nfinal_y_predicted_df = pd.DataFrame({'Churn':y_train.values, 'Churn_Predicted_Initial':y_predicted_train})\nfinal_y_predicted_df.index.name = 'customerID'\nfinal_y_predicted_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# TRAIN DATA & PREDICTED ON TRAIN DATA\n#Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\nfinal_y_predicted_df['Churn_Predicted_Final'] = final_y_predicted_df.Churn_Predicted_Initial.map(lambda x: 1 if x > 0.5 else 0)\nfinal_y_predicted_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for train data\nfrom sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(final_y_predicted_df['Churn'], final_y_predicted_df['Churn_Predicted_Final'])\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy_score for train data\nfrom sklearn.metrics import classification_report\nprint(classification_report(final_y_predicted_df['Churn'], final_y_predicted_df['Churn_Predicted_Final']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall accuracy.\nmetrics.accuracy_score(final_y_predicted_df['Churn'], final_y_predicted_df['Churn_Predicted_Final'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Testing"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# standardization on X_test\nX_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_test[['tenure','MonthlyCharges','TotalCharges']])\n\n#Adding a constant\nX_test_const = sm.add_constant(X_test[sel_att])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the test set\ny_predicted_test = results2.predict(X_test_const)\ny_predicted_test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# making dataframe for test values and predicted values with 'customerID'as index\nfinal_y_predicted_train = pd.DataFrame(index= y_test.index, columns=('Churn','Churn_Predicted_Initial'))\nfinal_y_predicted_train = pd.DataFrame({'Churn':y_test.values, 'Churn_Predicted_Initial':y_predicted_test})\nfinal_y_predicted_train.index.name = 'customerID'\nfinal_y_predicted_train.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# TEST DATA & PREDICTED ON TEST DATA\n#Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\nfinal_y_predicted_train['Churn_Predicted_Final'] = final_y_predicted_train.Churn_Predicted_Initial.map(lambda x: 1 if x > 0.5 else 0)\nfinal_y_predicted_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for test data\nfrom sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(final_y_predicted_train['Churn'], final_y_predicted_train['Churn_Predicted_Final'])\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy_score for train data\nfrom sklearn.metrics import classification_report\nprint(classification_report(final_y_predicted_train['Churn'], final_y_predicted_train['Churn_Predicted_Final']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall accuracy.\nmetrics.accuracy_score(final_y_predicted_train['Churn'], final_y_predicted_train['Churn_Predicted_Final'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Comparison: Logistic Regression without RFE - Recursive Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}