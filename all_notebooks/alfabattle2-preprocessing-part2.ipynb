{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Дипломная работа Александра Соколова\n\n#### Предобработка данных для градиентного бустинга\nКернел 2 из 5 в разделе ML (отредактирован 21.04.2021)\n---\n\n# 1. Импорт библиотек, инициализация глобальных констант\n## 1.1. Импорт библиотек","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport gc\nimport os\nimport tqdm\nimport numpy as np\n\nnp.warnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Глобальные константы","metadata":{}},{"cell_type":"code","source":"# CURRENT_DIR = './'  # имя текущей директории для локальной машины \nCURRENT_DIR = '../'  # имя текущей директории для каггл\n\nPATH_TO_TRAIN = CURRENT_DIR + 'input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_train_transactions_contest/train_transactions_contest'\nPATH_TO_TEST = CURRENT_DIR + 'input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_test_transactions_contest/test_transactions_contest'\n\nPATH_TO_TRAIN_TARGET = CURRENT_DIR + 'input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_train_target.csv'\nPATH_TO_TEST_TARGET = CURRENT_DIR + 'input/alfabattle2-sandbox/alfabattle2_sand_alfabattle2_test_target_contest.csv'\n\nPATH_TO_WORKDIR = CURRENT_DIR + 'working/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip freeze > requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Вспомогательные функции","metadata":{}},{"cell_type":"code","source":"def read_parquet_dataset_from_local(path_to_dataset: str, \n                                    start_from: int = 0,\n                                    num_parts_to_read: int = 2, \n                                    columns=None, \n                                    verbose=False,\n                                    info_num_parts=False) -> pd.DataFrame:\n    \"\"\"\n    читает num_parts_to_read партиций, преобразует их к pd.DataFrame и возвращает\n    :param path_to_dataset: путь до директории с партициями\n    :param start_from: номер партиции, с которой начать чтение\n    :param num_parts_to_read: количество партиций, которые требуется прочитать\n    :param columns: список колонок, которые нужно прочитать из партиции\n    :return: pd.DataFrame\n    \"\"\"\n\n    res = []\n    list_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset) \n                              if filename.startswith('part')])\n    if info_num_parts:\n        print(f'Кол-во партиций в папке: {len(list_paths)}')\n    start_from = max(0, start_from)\n    list_path_to_partitions = list_paths[start_from: start_from + num_parts_to_read]\n    if verbose:\n        print('Reading chunks:\\n')\n        for path_to_partition in list_path_to_partitions:\n            print(path_to_pirtition)\n    for path_to_parquet in tqdm.tqdm_notebook(list_path_to_partitions, \n                                              desc=\"Читаем файлы:\"):\n        temp_parquet = pd.read_parquet(path_to_parquet,columns=columns)\n        res.append(temp_parquet)\n        del temp_parquet\n        gc.collect()\n    return pd.concat(res).reset_index(drop=True)\n\n\ndef __amnt_pivot_table_by_column_as_frame(frame, column, agg_funcs=None) -> pd.DataFrame:\n    \"\"\"\n    Строит pivot table для между колонкой `amnt`  и column на основе переданных aggregations_on\n    :param frame: pd.DataFrame транзакций\n    :param column: название колонки, на основе `amnt`  и column будет построен pivot_table\n    :param agg_funcs: список из функций, которые нужно применить, по умолчанию ['mean', 'count']\n    :return: pd.DataFrame\n    \"\"\"\n    if agg_funcs is None:\n        agg_funcs = ['mean', 'count']\n    aggs = pd.pivot_table(frame, values='amnt',\n                          index=['app_id'], columns=[column],\n                          aggfunc={'amnt': agg_funcs},\n                          fill_value=0.0)\n    aggs.columns = [f'{col[0]}_{column}_{col[1]}' for col in aggs.columns.values]\n    return aggs\n\ndef extract_basic_aggregations(transactions_frame: pd.DataFrame, \n                               cat_columns=None, \n                               agg_funcs=None) -> pd.DataFrame:\n    \"\"\"\n    :param transactions_frame: pd.DataFrame с транзакциями\n    :param cat_columns: список категориальных переменных, для которых будут построены агрегаты по `amnt`\n    :param agg_funcs: список функций, который нужно применить для подсчета агрегатов, \n    :по умолчанию ['sum', 'mean', 'count']\n    :return: pd.DataFrame с извлеченными признаками\n    \"\"\"\n    if not cat_columns:\n        cat_columns = CAT_COLUMNS\n\n    pivot_tables = []\n    for col in cat_columns:\n        pivot_tables.append(__amnt_pivot_table_by_column_as_frame(transactions_frame, column=col,\n                                                                  agg_funcs=agg_funcs))\n    pivot_tables = pd.concat(pivot_tables, axis=1)\n\n    aggs = {\n        # посчитаем статистики для транзакций\n        'amnt': ['mean', 'median', 'sum', 'std'],\n        # посчитаем разумные агрегаты для разницы в часах между транзакциями\n        'hour_diff': ['max', 'mean', 'median', 'var', 'std'],\n        # добавим самую раннюю/позднюю и среднюю дату транзакции до подачи заявки на кредит\n        'days_before': ['min', 'max', 'median']}\n\n    numeric_stats = transactions_frame.groupby(['app_id']).agg(aggs)\n\n    # дадим разумные имена новым колонкам; может не работать в python 3.5, так как порядок ключей в словаре не\n    # гарантирован\n    numeric_stats.columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n\n    return pd.concat([pivot_tables, numeric_stats], axis=1).reset_index()\n\ndef prepare_transactions_dataset(path_to_dataset: str, \n                                 num_parts_to_preprocess_at_once: int = 1, \n                                 num_parts_total: int=50, \n                                 save_to_path=None, \n                                 verbose: bool=False):\n    \"\"\"\n    :возвращает готовый pd.DataFrame с признаками, на которых можно учить модель для целевой задачи.\n    :path_to_dataset: str - путь до датасета с партициями\n    :num_parts_to_preprocess_at_once: int - количество партиций, которые будут одновременно держаться в памяти и обрабатываться\n    :num_parts_total: int - общее количество партиций, которые нужно обработать\n    :save_to_path: str - путь до папки, в которой будет сохранен каждый обработанный блок в .parquet формате. Если None, то не будет сохранен \n    :verbose: bool - логирует каждый обрабатываемый кусок данных\n    \"\"\"\n    preprocessed_frames = []\n    block = 0\n    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once), \n                                   desc=\"Общий прогресс препроцессинга:\"):\n        transactions_frame = read_parquet_dataset_from_local(path_to_dataset, \n                                                             step, \n                                                             num_parts_to_preprocess_at_once, \n                                                             verbose=verbose)\n        features = extract_basic_aggregations(transactions_frame, \n                                              cat_columns=['mcc_category', \n                                                           'day_of_week', \n                                                           'operation_type'])\n        if save_to_path:\n            block_as_str = str(block)\n            if len(block_as_str) == 1:\n                block_as_str = '00' + block_as_str\n            else:\n                block_as_str = '0' + block_as_str\n            features.to_parquet(os.path.join(save_to_path, f'processed_chunk_{block_as_str}.parquet'))\n            \n        preprocessed_frames.append(features)\n    return pd.concat(preprocessed_frames)\n\ndef reduce_mem_usage_df(d_df: pd.DataFrame)-> [pd.DataFrame, list]:\n    \"\"\"\n    :перебирает все столбцы датафрейма и изменяет тип данных для уменьшения использования памяти.\n    \"\"\"\n    start_mem = d_df.memory_usage().sum() / 1024**2\n    print('Размер памяти исходного датафрейма {:.2f} MB'.format(start_mem))\n    \n    d_log = []\n    for col in d_df.columns:\n        col_type = d_df[col].dtype\n\n        if col_type != object:\n            c_min = d_df[col].min()\n            c_max = d_df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    d_df[col] = d_df[col].astype(np.int8)\n                    d_log.append(f'{col} :from int64 to int8')\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    d_df[col] = d_df[col].astype(np.int16)\n                    d_log.append(f'{col} :from int64 to int16')\n\n    end_mem = d_df.memory_usage().sum() / 1024**2\n    print('Размер памяти после оптимизации: {:.2f} MB'.format(end_mem))\n    print('Уменьшено на {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    print('Список внесенных изменений вы можете посмотреть в логе')\n    \n    return d_df, d_log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Импорт и препроцессинг данных\n---\n## 3.1 Оценка размеров данных и необходимой памяти\nВ данном соревновании сырые данные представлены в формате Parquet. Это крайне эффективный бинарный формат сжатия данных по колонкам. Однако для непосредственной работы с данными и построением моделей, нам нужно их прочитать и трансформировать в pandas.DataFrame. При этом сделать это эффективно по памяти. Для примера прочитаем одну партицию в память и оценим, сколько RAM она занимает.","metadata":{}},{"cell_type":"code","source":"%%time\ntransactions_frame = read_parquet_dataset_from_local(PATH_TO_TRAIN, \n                                                     start_from=0, \n                                                     num_parts_to_read=1,\n                                                     info_num_parts=True)\n\nmemory_usage_of_frame = transactions_frame.memory_usage(index=True).sum() / 10**9\nexpected_memory_usage = memory_usage_of_frame * 50\nprint(f'Объем памяти в  RAM одной партиции данных с транзакциями: {round(memory_usage_of_frame, 3)} Gb')\nprint(f'Ожидаемый размер в RAM всего датасета: {round(expected_memory_usage, 3)} Gb')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"На платформе у нас выделено 16Гб (но по факту с учетом реализации python через docker менее 12ГБ). Датасет в памяти не уместится - нам потребуется значительный объем RAM или дополнительные ресурсы. Решение - читать данные итеративно. Партиции организованы таким образом, что  для конкретного клиента вся информация о его транзакциях до момента подачи заявки на кредит расположена внутри одной партиции (транзакции сгруппированы по полю `app_id`). Это позволяет загружать данные в память кусками, выделять все необходимые признаки и получать результирующий фрейм для моделирования. Для этих целей мы будем использовать функцию `utils.read_parquet_dataset_from_local`.  \nПо времени обработка одной партиции занимает 4,5 секунды. С учетом 50 партиций, общее время обработки должно быть в пределах 5 минут.","metadata":{}},{"cell_type":"code","source":"del transactions_frame\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Импорт и препроцессинг данных трейна","metadata":{}},{"cell_type":"markdown","source":"Опытным путем было установлено, что максимальное кол-во файлов, которое способна переварить оперативка за одну итерацию = 5 ","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_data = prepare_transactions_dataset(PATH_TO_TRAIN, \n                                    num_parts_to_preprocess_at_once=5, \n                                    num_parts_total=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Объем в RAM всего train датасета с агрегатами: {round(train_data.memory_usage(index=True).sum() / 10**9, 3)} Gb')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Импорт и препроцессинг данных теста","metadata":{}},{"cell_type":"code","source":"%%time\ntest_data = prepare_transactions_dataset(PATH_TO_TEST, \n                                         num_parts_to_preprocess_at_once=5, \n                                         num_parts_total=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Объем в RAM всего test датасета с агрегатами: {round(test_data.memory_usage(index=True).sum() / 10**9, 3)} Gb')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Сокращение размеров датасетов\n---\nПеред сохранением датасетов для дальнейшего их использования в кернелах при построении моделей и их анализа желательно сократить их размер, для этого мы используем функцию 'reduce_mem_usage_df'","metadata":{}},{"cell_type":"code","source":"train_data, log_reduce_mem_train = reduce_mem_usage_df(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# log_reduce_mem_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data, log_reduce_mem_test = reduce_mem_usage_df(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# log_reduce_mem_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Мерджинг датасетов с признаком product","metadata":{}},{"cell_type":"code","source":"train_targets = pd.read_csv(PATH_TO_TRAIN_TARGET)\ntrain_targets.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train_data = train_data.merge(train_targets[['app_id', 'product', 'flag']], on=['app_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_target =  pd.read_csv(PATH_TO_TEST_TARGET)\ntest_target.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test_data = test_data.merge(test_target[['app_id', 'product']], on='app_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Сохранение датасетов","metadata":{}},{"cell_type":"code","source":"merged_train_data.to_csv('./merged_train_data.csv', index=None)\nmerged_test_data.to_csv('./merged_test_data.csv', index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}