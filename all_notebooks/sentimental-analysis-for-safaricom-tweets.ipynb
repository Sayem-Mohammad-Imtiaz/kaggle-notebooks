{"cells":[{"metadata":{"id":"dmAThhkb6NG4"},"cell_type":"markdown","source":"<h1> Importing pandas and other basic libraries</h1>"},{"metadata":{"id":"ixx1pTYazRLU","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"id":"C0Y1_zYV6LNf"},"cell_type":"markdown","source":"<h1>Get the dataset (Safaricom hashtag tweets upto 31st March)</h1>"},{"metadata":{"id":"tGyVIqrBDf_c","outputId":"6f3f912c-d858-490d-e161-fcca0a04feb5","trusted":true},"cell_type":"code","source":"file = pd.read_csv(\"../input/hashtag-safaricom-tweets/safaricom_tweets.csv\")\nfile.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"_HRvP7ft6sb0"},"cell_type":"markdown","source":"<h1>Data overview</h1>"},{"metadata":{"id":"e6kezdLdJsUI","outputId":"659031e7-ce9f-4580-cbab-2c10c9870c77","trusted":true},"cell_type":"code","source":"file.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"xKz7lV-J63tk"},"cell_type":"markdown","source":"<h1>Rows and Columns count</h1>"},{"metadata":{"id":"B32RFSOpJ8vK","outputId":"162a377f-41d5-43dc-f4e4-3a2b0c23cd14","trusted":true},"cell_type":"code","source":"file.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"vH1FWkOoKRz5","outputId":"71551fa5-8b94-4edf-cf4f-75d60b4e9ea0","trusted":true},"cell_type":"code","source":"tweets_df = file[[\"Tweet Id\", \"Screen Name\", \"Text\"]]\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"8IoxkfMiLZrC","outputId":"b9422675-4a39-47d1-a723-65b88ea5499b","trusted":true},"cell_type":"code","source":"tweets_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"2zywaJDD7DUy"},"cell_type":"markdown","source":"<h1>Data cleaning and preprocessing</h1>\n<ul>\n<li>Tokenization</li>\n<li>Lemmertization</li>\n<li>Remove punctuation tags</li>\n<li>Remove emojis</li>\n<li>Strip numerical values</li>\n<li>Remove stop-words</li>\n</ul>"},{"metadata":{"id":"Y1-P-bL-XfG4","trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"id":"ysGlj9k6aAhV","outputId":"7cef98ee-01e8-455c-d181-549cbc81730f","trusted":true},"cell_type":"code","source":"nltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"id":"JaVy4NuDwe_W","outputId":"feb9b3ab-2163-4a7f-b231-af3ff5c82c2d","trusted":true},"cell_type":"code","source":"!pip install emoji","execution_count":null,"outputs":[]},{"metadata":{"id":"Gfz_X0TfY9qV","outputId":"6f362f7f-0623-4c94-aeb8-53b41759fe38","trusted":true},"cell_type":"code","source":"import emoji\ndef tokenize_tweets(text):\n  #remove emojis\n  text = emoji.demojize(text)\n  #remove urls\n  text = re.sub('http[s]?://\\S+', '', text)\n  #remove punctuations\n  text = re.sub(r'[^\\w\\s]','',text)\n  #strip numbers\n  text = re.sub('[0-9]+', '', text)\n  text = word_tokenize(text)\n  \n  return text\ntweets_df[\"Tweets\"] = tweets_df[\"Text\"].apply(lambda x: tokenize_tweets(x))\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop = stopwords.words(\"english\")\ntweets_df[\"stop_words\"] = tweets_df[\"Tweets\"].apply(lambda x: [w for w in x if w in stop])\ntweets_df[\"Tweets\"] = tweets_df[\"Tweets\"].apply(lambda x: [w.lower() for w in x if w not in stop])\n\ntweets_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"lYskUr6qaHJK","outputId":"fde8b1a2-ee88-4ee6-f77e-e82299fa6793","trusted":true},"cell_type":"code","source":"tweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"qEXGBkOeMux4","outputId":"38a801ee-002f-42f7-cff7-32f2881c0416","trusted":true},"cell_type":"code","source":"string.punctuation","execution_count":null,"outputs":[]},{"metadata":{"id":"R7BkOR_dux6H","outputId":"f7d700c3-0ee1-4350-f663-7b8f116f3da9","trusted":true},"cell_type":"code","source":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\ntweets_df[\"Tweets\"] = tweets_df[\"Tweets\"].apply(lambda x: [stemmer.stem(w) for w in x])\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Fb9a5HIGMyRo","outputId":"cb05b8b8-043b-47af-a080-45353d165cd2","trusted":true},"cell_type":"code","source":"\ndef remove_punct(text):\n  text = \" \".join([char for char in text if char not in string.punctuation])\n  text = re.sub('[0-9]+', '', text)\n  \n  \n  return text\ntweets_df['tweet_punct'] = tweets_df['Tweets'].apply(lambda x: remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1oS3wdfJOMUx","outputId":"0dc162ef-1923-422f-c15f-0f92770e4731","trusted":true},"cell_type":"code","source":"tweets_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"rptWAf1DoO0Y","outputId":"0e8806e9-e85f-4e12-cdbe-89470ba2acc9","trusted":true},"cell_type":"code","source":"\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"FOo_ry3473fr"},"cell_type":"markdown","source":"<h1>Data visualization (word cloud)</h1>"},{"metadata":{"id":"6ALNlxZMy-kL","outputId":"a7fc138e-d36f-49e9-e80a-c5de954682b8","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nall_words = ' '.join([text for text in tweets_df['tweet_punct']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fn6Gj8Wi8GJT"},"cell_type":"markdown","source":"<h1>Get most frequent words</h1>"},{"metadata":{"id":"54uutCTnxxbn","outputId":"f5f46e95-c45f-4dbe-983d-d261b6df6361","trusted":true},"cell_type":"code","source":"from collections import Counter\n\ncnt = Counter()\nfor text in tweets_df[\"tweet_punct\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"kMZThxyBE2eI","outputId":"82f77910-9386-4ccb-edc1-84a432622534","trusted":true},"cell_type":"code","source":"!pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"id":"GQgMyYsk8R8L"},"cell_type":"markdown","source":"<h1>Import sklearn</h1>"},{"metadata":{"id":"_4iNifAlEVFs","trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"id":"p0vojsuY_6Ts"},"cell_type":"markdown","source":"<h1>Getting sentiments label</h1>"},{"metadata":{"id":"m-WHjtSJFGku","outputId":"63df1c24-d601-43ed-ece4-1a80ccf1bba7","trusted":true},"cell_type":"code","source":"def sentiment_score_compound(sentence):\n    score = analyzer.polarity_scores(sentence)\n    return score['compound']\n\ndef sentiment_score_pos(sentence):\n    score = analyzer.polarity_scores(sentence)\n    return score['pos']\n\ndef sentiment_score_neg(sentence):\n    score = analyzer.polarity_scores(sentence)\n    return score['neg']\n\ndef sentiment_score_neu(sentence):\n    score = analyzer.polarity_scores(sentence)\n    return score['neu']\ntweets_df[\"tweets_sent_compound\"] = tweets_df[\"tweet_punct\"].apply(lambda x: sentiment_score_compound(x))\ntweets_df[\"tweets_sent_pos\"] = tweets_df[\"tweet_punct\"].apply(lambda x: sentiment_score_pos(x))\ntweets_df[\"tweets_sent_neg\"] = tweets_df[\"tweet_punct\"].apply(lambda x: sentiment_score_neg(x))\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"o2Jbe3-LI2w8","outputId":"5df49369-0f65-4f9f-9877-eda83ef5f372","trusted":true},"cell_type":"code","source":"tweets_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"ITvMhED5hfOX","trusted":true},"cell_type":"code","source":"wordlist = nltk.FreqDist(all_words)\nword_features = wordlist.keys()","execution_count":null,"outputs":[]},{"metadata":{"id":"v5I-3blcAS4a"},"cell_type":"markdown","source":"<h1>Vectorization</h1>"},{"metadata":{"id":"ARAuwQqXiFQG","outputId":"586c2e9e-f159-40de-f5a4-6d1001f98d5f","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\ntweets_list = []\nfor tweet in tweets_df[\"tweet_punct\"]:\n  \n  tweets_list.append(tweet)\nlen(tweets_list)\n#tweets_df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dJLFW2SJ7GAY","trusted":true},"cell_type":"code","source":"X = tweets_df[\"tweet_punct\"]\n\nvec = TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True,use_idf = True,ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{"id":"4P8dn-kN2KhU","outputId":"8b97ee9b-b539-4f2f-fcbe-6e16f67e4eac","trusted":true},"cell_type":"code","source":"len(all_words)","execution_count":null,"outputs":[]},{"metadata":{"id":"LCMpTn-FC5QK"},"cell_type":"markdown","source":"<p> If tweet is negative lable is 0 neutral 1  positive 2 </p>\n<p> This is because classifiers only take integers </p> "},{"metadata":{"id":"ud4R22rRcOvU","outputId":"09ac0fde-3c24-48c0-9658-4af026e87c83","trusted":true},"cell_type":"code","source":"def label_value(val):\n  if val < 0:\n    return 0\n  elif val == 0:\n    return 1\n  else:\n    return 2\ntweets_df[\"label\"] = tweets_df[\"tweets_sent_compound\"].apply(lambda x: label_value(x))\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"swhwHlv2rBRb","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(binary=True)\ncv.fit(tweets_list)\nX = cv.transform(tweets_list)\ny = tweets_df[\"label\"].values\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wYsSzYxQAmMp"},"cell_type":"markdown","source":"<h1>Classification model</h1>"},{"metadata":{"id":"wJ60PDO7AQPC","trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, train_size = 0.2, random_state = 0\n)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kpRYAkZxAwtD"},"cell_type":"markdown","source":"<h1>Logistic Regression</h1>"},{"metadata":{"id":"kh2b1t1KA2tc","outputId":"e78b0020-6051-4793-fc38-7aa0597060bc","trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"0l1jMOEtDFV0","outputId":"ab05cf9d-cfcb-439d-f678-e35da4bd0aec","trusted":true},"cell_type":"code","source":"pred = lr.predict(X_val)\nprint(accuracy_score(y_val, pred))\nprint(classification_report(y_val, pred))\nprint(confusion_matrix(y_val, pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"BPROrINeBFHj"},"cell_type":"markdown","source":"<h1>tf-idf vectorization</h1>"},{"metadata":{"id":"R9YWCVLKiKW3","trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(tweets_list)\nX = tfidf_vectorizer.transform(tweets_list)\ny = tweets_df[\"label\"].values\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, train_size = 0.2, random_state = 0\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"P5qEx4kIxH5f","outputId":"a92a028c-8485-4945-9b16-6abddc8094a9","trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"RtQgrlLxxi9K","outputId":"5242cb8b-4e21-487a-da93-1fa15239f7fc","trusted":true},"cell_type":"code","source":"pred = lr.predict(X_val)\nprint(accuracy_score(y_val, pred))\nprint(classification_report(y_val, pred))\nprint(confusion_matrix(y_val, pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"phPk1NdJBPIe"},"cell_type":"markdown","source":"<h1>Support Vector Machine</h1>"},{"metadata":{"id":"a0ZEh7m_xvh5","outputId":"15b83cb3-d6f8-4245-9f7b-7812eea8f89e","trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3))\nngram_vectorizer.fit(tweets_list)\nX = ngram_vectorizer.transform(tweets_list)\ny = tweets_df[\"label\"].values\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, train_size = 0.2, random_state = 0\n)\nsvm = LinearSVC()\nsvm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"9J63tDTC8CQt","outputId":"60ce93aa-b210-41dc-d4ea-6cf86fb7c5a6","trusted":true},"cell_type":"code","source":"pred = svm.predict(X_val)\nprint(accuracy_score(y_val, pred))\nprint(classification_report(y_val, pred))\nprint(confusion_matrix(y_val, pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"qqeBQGjwBvqL"},"cell_type":"markdown","source":"<h1>Naïve Bayes classifier</h1>"},{"metadata":{"id":"nhe0haIV8bVw","outputId":"64695f0a-c180-4abc-ef15-cc62819d77c6","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nMNB = MultinomialNB()\nMNB.fit(X_train, y_train)\npred = MNB.predict(X_val)\nprint(accuracy_score(y_val, pred))\nprint(classification_report(y_val, pred))\nprint(confusion_matrix(y_val, pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}