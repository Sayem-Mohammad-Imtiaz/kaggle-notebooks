{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tensorflow Version"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(\"Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/sentiment140/training.1600000.processed.noemoticon.csv\"\ntotal_words = 8000\nmax_seq_len = 240\nembedding_dim_size = 200\nTRAIN_PROP=0.9","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading and visualizing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(data_dir, encoding =\"ISO-8859-1\" , names= [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting text and targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taken from another notebook\ndf=df.sample(frac=0.002)\ntext_target = df[['text', 'target']]\ntargets, texts = df.iloc[:,0], df.iloc[:,5]\ntargets = targets/4 # 0 and 4 --> 0 and 1 \ntargets.astype(int)\nprint(len(texts))\nprint(len(targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split into train and test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_SIZE=int(len(texts)*TRAIN_PROP)\n\ntrain_texts = texts[0:TRAIN_SIZE]\ntrain_targets = targets[0:TRAIN_SIZE]\n\nvalidation_texts = texts[TRAIN_SIZE:]\nvalidation_targets = targets[TRAIN_SIZE:]\nprint(\"len(train_texts)\",len(train_texts))\nprint(\"len(train_targets)\",len(train_targets))\n\nprint(\"len(validation_texts)\",len(validation_texts))\nprint(\"len(validation_targets)\",len(validation_targets))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=total_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(train_texts)\n\ntrain_sequences = tokenizer.texts_to_sequences(train_texts)\nvalidation_sequences = tokenizer.texts_to_sequences(validation_texts)\n\ntrain_sequences_padded = pad_sequences(train_sequences,maxlen=max_seq_len)\nvalidation_sequences_padded = pad_sequences(validation_sequences,maxlen=max_seq_len)\n\nword_index = tokenizer.word_index\nprint(\"total words in model\", total_words)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading glove embeddings for selected words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taken from:  https://www.kaggle.com/imvkhandelwal/tensorflow-2-0-rnn-with-glove-vectors\nembeddings_index = {}\nwith open('../input/glove6b/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembeddings_matrix = np.zeros((len(word_index),100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Renaming input datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_sequences_padded\ntrain_target = train_targets\nvalidation = validation_sequences_padded\nvalidation_target = validation_targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index), 100, input_length=max_seq_len, weights=[embeddings_matrix], trainable=False))\nmodel.add(Bidirectional(LSTM(64, return_sequences = True)))\nmodel.add(Dropout(0.05))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(Dropout(0.05))\nmodel.add(Bidirectional(LSTM(8, return_sequences = False)))\nmodel.add(Dropout(0.05))\n\nmodel.add(keras.layers.Dense(100))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\n\nmodel.add(keras.layers.Dense(50))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\n\nmodel.add(Dense(1, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train, train_target, epochs=1, validation_data=(validation, validation_target))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_num =range(0,epochs_num)\n\nplt.plot(1, acc, 'b', label='Training acc')\nplt.plot(1, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(1, loss, 'b', label='Training loss')\nplt.plot(1, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Writing model to disk"},{"metadata":{"trusted":true},"cell_type":"code","source":"# serialize model to JSON\nimport os\nmodel_json = model.to_json()\nos.mkdir(\"..output\")\nwith open(\"..output/model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}