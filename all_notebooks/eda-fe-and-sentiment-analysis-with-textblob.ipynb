{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n%matplotlib inline\n\n# We want to see whole content (non-truncated)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First look","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Load the tweets\ntweets_raw = pd.read_csv(\"/kaggle/input/tweets-about-distance-learning/tweets_raw.csv\")\n\n# Print the first five rows\ndisplay(tweets_raw.head())\n\n# Print the summary statistics\nprint(tweets_raw.describe())\n\n# Print the info\nprint(tweets_raw.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At a first glance, we can see that there are 202.645 tweets including the content, location, username, Retweet count, Favorites count, and the creation time features in the DataFrame. There are also some missing values in the Location column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nUnnamed: 0 and Unnamed: 0.1 columns are not informative to us so we'll drop them. The data type of Created at column should be also datetime. As well as we need to get rid of duplicated tweets if there are.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# We do not need first two columns. Let's drop them out.\ntweets_raw.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1, inplace=True)\n\n# Drop duplicated rows\ntweets_raw.drop_duplicates(inplace=True)\n\n# Created at column's type should be datatime\ntweets_raw[\"Created at\"] = pd.to_datetime(tweets_raw[\"Created at\"])\n\n# Print the info again\nprint(tweets_raw.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tweets count has been reduced to 187.052 (There were 15.593 duplicated rows). Created at column's data type also changed to datatime64[ns]\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the minimum datetime\nprint(\"Since:\",tweets_raw[\"Created at\"].min())\n\n# Print the maximum datetime\nprint(\"Until\",tweets_raw[\"Created at\"].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about locations ?\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill the missing values with unknown tag\ntweets_raw[\"Location\"].fillna(\"unknown\", inplace=True)\n\n# Print the unique locations and number of unique locations\nprint(\"Unique Values:\",tweets_raw[\"Location\"].unique())\nprint(\"Unique Value count:\",len(tweets_raw[\"Location\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the creation hours?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the seaborn style\nsns.set()\n# Plot the histogram of hours\nsns.distplot(tweets_raw[\"Created at\"].dt.hour, bins=24)\nplt.title(\"Hourly Distribution of Tweets\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the most popular tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the most popular tweets\ndisplay(tweets_raw.sort_values(by=[\"Favorites\",\"Retweet-Count\", ], axis=0, ascending=False)[[\"Content\",\"Retweet-Count\",\"Favorites\"]].head(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's remove the stopwords, hashtags and punctuation on tweets and tokenize them. To do this we'll define a function. \n> Since it can take a lot (for about 1 hr). I will load the processed csv from my github repository.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\"\"\"\nnltk.download('punkt')\nnltk.download('wordnet')\n\"\"\"\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \n\"\"\"\ndef process_tweets(tweet):\n    \n    # Remove links\n    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n    \n    # Remove mentions and hashtag\n    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n    \n    # Tokenize the words\n    tokenized = word_tokenize(tweet)\n\n    # Remove the stop words\n    tokenized = [token for token in tokenized if token not in stopwords.words(\"english\")] \n\n    # Lemmatize the words\n    lemmatizer = WordNetLemmatizer()\n    tokenized = [lemmatizer.lemmatize(token, pos='a') for token in tokenized]\n\n    # Remove non-alphabetic characters and keep the words contains three or more letters\n    tokenized = [token for token in tokenized if token.isalpha() and len(token)>2]\n    \n    return tokenized\n    \n# Call the function and store the result into a new column\ntweets_raw[\"Processed\"] = tweets_raw[\"Content\"].str.lower().apply(process_tweets)\n\n\"\"\"\n# After function call I have saved the file as tweets_processed.csv.\ntweets_raw = pd.read_csv(\"/kaggle/input/tweets-processed/tweets_processed.csv\", parse_dates=[\"Created at\"])\n\n# Print the first fifteen rows of Processed\ndisplay(tweets_raw[[\"Processed\"]].head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vectorize the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import TfidfVectorizer from sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create our contextual stop words\ntfidf_stops = [\"online\", \"class\", \"course\", \"learning\", \"learn\",\"teach\", \"teaching\", \"distance\", \\\n               \"distancelearning\", \"education\", \"teacher\", \"student\", \"grade\", \"classes\", \"computer\", \"resource\", \\\n               \"onlineeducation\", \"onlinelearning\", \"school\", \"students\", \"class\", \"virtual\", \"eschool\", \"thing\", \\\n               \"virtuallearning\", \"educated\", \"educates\", \"teaches\", \"studies\", \"study\", \"semester\", \"elearning\", \\\n               \"teachers\", \"lecturer\", \"lecture\", \"amp\", \"academic\", \"admission\", \"academician\", \"account\", \"action\",\\\n               \"add\", \"app\", \"announcement\", \"application\", \"adult\", \"classroom\", \"system\", \"video\", \"essay\", \"training\", \\\n               \"homework\",\"work\",\"assignment\", \"paper\", \"get\", \"math\", \"project\", \"science\", \"physics\", \"lesson\", \"schools\", \\\n               \"courses\", \"assignments\", \"know\", \"instruction\",\"email\", \"discussion\",\"home\", \"college\", \"exam\", \"university\", \\\n               \"use\", \"fall\", \"term\", \"proposal\", \"one\", \"review\", \"proposal\", \"calculus\", \"search\", \"research\", \"algebra\", \\\n               \"internet\", \"remote\", \"remotelearning\"]\n\n# Initialize a Tf-idf Vectorizer\nvectorizer = TfidfVectorizer(max_features=5000, stop_words= tfidf_stops)\n\n# Fit and transform the vectorizer\ntfidf_matrix = vectorizer.fit_transform(tweets_raw[\"Processed\"])\n\n# Let's see what we have\ndisplay(tfidf_matrix)\n\n# Create a DataFrame for tf-idf vectors and display the first five rows\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns= vectorizer.get_feature_names())\ndisplay(tfidf_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a word cloud.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import wordcloud\nfrom wordcloud import WordCloud\n\n# Create a new DataFrame called frequencies\nfrequencies = pd.DataFrame(tfidf_matrix.sum(axis=0).T,index=vectorizer.get_feature_names(),columns=['total frequency'])\n\n# Sort the words by frequency\nfrequencies.sort_values(by='total frequency',ascending=False, inplace=True)\n# Display the most 20 frequent words\ndisplay(frequencies.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join the indexes\nfrequent_words = \" \".join(frequencies.index)+\" \"\n\n# Initialize the word cloud\nwc = WordCloud(width = 500, height = 500, min_font_size = 10, max_words=2000, background_color ='white', stopwords= tfidf_stops)\n\n# Generate the world clouds for each type of label\ntweets_wc = wc.generate(frequent_words)\n\n# Plot the world cloud                     \nplt.figure(figsize = (10, 10), facecolor = None) \nplt.imshow(tweets_wc, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.title(\"Common words in the tweets\")\nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nStore the tweets length and number of words in new columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the tweet lengths\ntweets_raw[\"Length\"] = tweets_raw[\"Content\"].str.len()\n\n# Get the number of words in tweets\ntweets_raw[\"Words\"] = tweets_raw[\"Content\"].str.split().str.len()\n\n# Display the new columns\ndisplay(tweets_raw[[\"Length\", \"Words\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s define a function called get_countries which returns the country codes of the given locations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since the processes takes a lot, I have used preprocessed column on Kaggle\n\n# Import pycountry\nimport pycountry\n\"\"\"\ndef get_countries(location):\n    \n    # If location is a country name return its alpha2 code\n    if pycountry.countries.get(name= location):\n        return pycountry.countries.get(name = location).alpha_2\n    \n    # If location is a subdivisions name return the countries alpha2 code\n    try:\n        pycountry.subdivisions.lookup(location)\n        return pycountry.subdivisions.lookup(location).country_code\n    except:\n        # If the location is neither country nor subdivision return the \"unknown\" tag\n        return \"unknown\"\n\n# Call the function and store the country codes in the Country column\ntweets_raw[\"Country\"] = tweets_raw[\"Location\"].apply(get_countries)\n\n# Print the unique values\nprint(tweets_raw[\"Country\"].unique())\n\"\"\"\n\n# Print the number of unique values\nprint(\"Number of unique values:\",len(tweets_raw[\"Country\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the most tweeted countries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to exclude unknowns\ncountries = tweets_raw[tweets_raw.Country!='unknown']\n\n# Select the top 20 countries\ntop_countries = countries[\"Country\"].value_counts(sort=True).head(20)\n\n# Convert alpha2 country codes to country names and store in a list\ncountry_fullnames = []\nfor alpha2 in top_countries.index:\n    country_fullnames.append(pycountry.countries.get(alpha_2=alpha2).name)\n\n# Visualize the top 20 countries\nplt.figure(figsize=(12,10))\nsns.barplot(y=country_fullnames,x=top_countries, orient=\"h\", palette=\"RdYlGn\")\nplt.xlabel(\"Tweet count\")\nplt.ylabel(\"Countries\")\nplt.title(\"Top 20 Countries\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Sentiment Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After preprocessing and EDA, we can finally focus on our main aim in this project. We are going to calculate the tweets’ sentimental features such as polarity and subjectivity by using **TextBlob**. <br><br> It gives us these values by using the predefined word scores. You can check the documentation for more information.<br><br>\n***polarity*** is a value changes between -1 to 1. It shows us how positive or negative the sentence given is.<br><br>\n***subjectivity*** is another value changes between 0 to 1 which shows us whether the sentence is about a fact or opinion (objective or subjective).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the TextBlob\nfrom textblob import TextBlob\n\n\"\"\"\n# Add polarities and subkectivities into the DataFrame by using TextBlob\ntweets_raw[\"Polarity\"] = tweets_raw[\"Processed\"].apply(lambda word: TextBlob(word).sentiment.polarity)\ntweets_raw[\"Subjectivity\"] = tweets_raw[\"Processed\"].apply(lambda word: TextBlob(word).sentiment.subjectivity)\n\"\"\"\n\n# Since the processes takes a lot, I have used preprocessed column on Kaggle\ntweets_raw = pd.read_csv(\"/kaggle/input/tweets-sentiments/tweets_sentiments.csv\", parse_dates=[\"Created at\"])\n\n# Display the Polarity and Subjectivity columns\ndisplay(tweets_raw[[\"Polarity\",\"Subjectivity\"]].head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classify the polarities. <br>\nIf, <br>\n* polarity > 0 --> **positive**\n* polarity = 0 --> **neutral**\n* polarity < 0 --> **negative**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to classify polarities\ndef analyse_polarity(polarity):\n    if polarity > 0:\n        return \"Positive\"\n    if polarity == 0:\n        return \"Neutral\"\n    if polarity < 0:\n        return \"Negative\"\n\n# Apply the funtion on Polarity column and add the results into a new column\ntweets_raw[\"Label\"] = tweets_raw[\"Polarity\"].apply(analyse_polarity)\n\n# Display the Polarity and Subjectivity Analysis\ndisplay(tweets_raw[[\"Label\"]].head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also count them up like the following.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the value counts of the Label column\nprint(tweets_raw[\"Label\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We tagged the tweets as positive, neutral, and negative so far. Let's go over our findings deeply. Let's visualize the counts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change the datatype as \"category\"\ntweets_raw[\"Label\"] = tweets_raw[\"Label\"].astype(\"category\")\n\n# Visualize the Label counts\nsns.countplot(tweets_raw[\"Label\"])\nplt.title(\"Label Counts\")\nplt.show()\n\n# Visualize the Polarity scores\nplt.figure(figsize = (10, 10)) \nsns.scatterplot(x=\"Polarity\", y=\"Subjectivity\", hue=\"Label\", data=tweets_raw)\nplt.title(\"Subjectivity vs Polarity\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the lexicon-based analysis is not always reliable, we have to check the results manually. Let's see the popular (in terms of retweets and favorites) tweets that have the highest/lowest polarity scores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the positive tweets\ndisplay(tweets_raw.sort_values(by=[\"Polarity\",\"Favorites\",\"Retweet-Count\", ], axis=0, ascending=[False, False, False])[[\"Content\",\"Retweet-Count\",\"Favorites\",\"Polarity\"]].head(20))\n\n# Display the negative tweets\ndisplay(tweets_raw.sort_values(by=[\"Polarity\", \"Favorites\", \"Retweet-Count\"], axis=0, ascending=[True, False, False])[[\"Content\",\"Retweet-Count\",\"Favorites\",\"Polarity\"]].head(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, we have been successful in labeling the polarities. Let's check the positive/negative tweets by country. Now we can make word clouds for positive and negative tweets to understand what people like and dislike.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_wordcloud(data, label):\n\n    # Initialize a Tf-idf Vectorizer\n    polarity_vectorizer = TfidfVectorizer(max_features=5000, stop_words= tfidf_stops)\n\n    # Fit and transform the vectorizer\n    tfidf_matrix_polarity = polarity_vectorizer.fit_transform(tweets_raw[\"Processed\"])\n\n    # Create a new DataFrame called frequencies\n    frequencies_polarity = pd.DataFrame(tfidf_matrix_polarity.sum(axis=0).T,index=polarity_vectorizer.get_feature_names(),columns=['total frequency'])\n\n    # Sort the words by frequency\n    frequencies_polarity.sort_values(by='total frequency',ascending=False, inplace=True)\n\n    # Join the indexes\n    frequent_words_polarity = \" \".join(frequencies_polarity.index)+\" \"\n\n    # Initialize the word cloud\n    wc = WordCloud(width = 500, height = 500, min_font_size = 10, max_words=2000, background_color ='white', stopwords= tfidf_stops)\n\n    # Generate the world clouds for each type of label\n    tweets_polarity = wc.generate(frequent_words_polarity)\n\n    # Plot the world cloud                     \n    plt.figure(figsize = (10, 10), facecolor = None) \n    plt.imshow(tweets_polarity, interpolation=\"bilinear\") \n    plt.axis(\"off\") \n    plt.title(\"Common words in the \" + label +\" tweets\")\n    plt.tight_layout(pad = 0) \n    plt.show() \n\n# Create DataFrames for each label\npositive_popular_df = tweets_raw.sort_values(by=[\"Polarity\",\"Favorites\",\"Retweet-Count\", ], axis=0, ascending=[False, False, False])[[\"Content\",\"Retweet-Count\",\"Favorites\",\"Polarity\",\"Processed\"]].head(50)\nnegative_popular_df = tweets_raw.sort_values(by=[\"Polarity\", \"Favorites\", \"Retweet-Count\"], axis=0, ascending=[True, False, False])[[\"Content\",\"Retweet-Count\",\"Favorites\",\"Polarity\",\"Processed\"]].head(50)\n\n# Call the function\nmake_wordcloud(positive_popular_df, \"positive\")\nmake_wordcloud(negative_popular_df, \"negative\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the positive and negative tweet counts by country.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the positive/negative counts by country\npositives_by_country = tweets_raw[tweets_raw.Country!='unknown'].groupby(\"Label\")[\"Country\"].value_counts().Negative.sort_values(ascending=False)\nnegatives_by_country =tweets_raw[tweets_raw.Country!='unknown'].groupby(\"Label\")[\"Country\"].value_counts().Positive.sort_values(ascending=False)\n\n# Print them out\nprint(\"Positive \\n\")\nprint(positives_by_country)\nprint(\"\\nNegative\\n\")\nprint(negatives_by_country)\n\n# Create a mask for top 1 countries (by tweets count)\nmask = tweets_raw[\"Country\"].isin(top_countries.index[:10]).values\n\n# Create a new DataFrame only includes top10 country\ntop_20df = tweets_raw.iloc[mask,:]\n\n# Visualize the top 20 countries\nplt.figure(figsize=(12,10))\nsns.countplot(x=\"Country\", hue=\"Label\", data=top_20df, order=top_20df[\"Country\"].value_counts().index)\nplt.xlabel(\"Countries\")\nlocs, labels = plt.xticks()\nplt.xticks(locs, country_fullnames[:10])\nplt.xticks(rotation=45)\nplt.ylabel(\"Tweet count\")\nplt.title(\"Top 10 Countries\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can the time tell us something about polarity ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive = tweets_raw.loc[tweets_raw.Label==\"Positive\"][\"Created at\"].dt.hour\nnegative = tweets_raw.loc[tweets_raw.Label==\"Negative\"][\"Created at\"].dt.hour\n\nplt.hist(positive, alpha=0.5, bins=24, label=\"Positive\", density=True)\nplt.hist(negative, alpha=0.5, bins=24, label=\"Negative\", density=True)\nplt.xlabel(\"Hour\")\nplt.ylabel(\"PDF\")\nplt.title(\"Hourly Distribution of Tweets\")\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram above demonstrates that there is no relationship between the time and tweets' polarities.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I would like to finish my work here. You can look at my complete notebook [here](https://github.com/Bhasfe/distance_learning) . I am looking forward to your work on this data. I wish you goodluck!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}