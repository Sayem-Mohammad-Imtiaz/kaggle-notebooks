{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_decision_boundaries(X, y, model_class, **model_params):\n    try:\n        X = np.array(X)\n        y = np.array(y).flatten()\n    except:\n        print(\"Coercing input data to NumPy arrays failed\")\n    # Reduces to the first two columns of data\n    reduced_data = np.hstack([X[:, 0:1],X[:, 1:2]])\n    # Instantiate the model object\n    model = model_class(**model_params)\n    # Fits the model with the reduced data\n    model.fit(reduced_data, y)\n    y[y=='Not Placed']=0\n    y[y=='Placed']=1\n    \n    # Step size of the mesh. Decrease to increase the quality of the VQ.\n    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    \n\n    # Plot the decision boundary. For that, we will assign a color to each\n    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n    # Meshgrid creation\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Obtain labels for each point in mesh using the model.\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    \n    \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n\n    # Predictions to obtain the classification results\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    Z[Z=='Not Placed']=0\n    Z[Z=='Placed']=1\n    # Plotting\n    plt.contourf(xx, yy, Z, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1],c=y, alpha=0.8)\n    plt.xlabel(\"ssc_p\",fontsize=15)\n    plt.ylabel(\"hsc_p\",fontsize=15)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.legend(['Not Placed','Placed'])\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\nThe college placement cell has decided to organize special training programming tied up with a placement training school. However due to funds limitation the college has decided to make it compulsory only for those who are likely to not get placed.\nOBJECTIVES: \nDetect a list of people who will not get placed.\nWhich factor influenced a candidate in getting placed?\nDoes percentage matters for one to get placed? "},{"metadata":{},"cell_type":"markdown","source":"# Data Overview"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\").drop('sl_no',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above information we know salary has null values and we know salary depends on whether the person is placed and hence isnt needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"c1=data['status'].values.copy()\nc1[c1==\"Not Placed\"]=0\nc1[c1==\"Placed\"]=1\ndata.plot(kind='scatter',y='ssc_p',x='hsc_p',s='degree_p',c=c1,cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend([\"Not Placed\",\"Placed\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Imbalance \nThe ouptut class has imbalnce since there are only 31% Negative class and 69% positve. Since we negative class is important to us are metric should be chosen keeping \"Not Placed\" class in mind."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"status\"].value_counts()/len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stakeholders(highlight for which you will be working)\n1. Students\n2. Placement Cell\n3. Training School\n4. College \n5. Recruiters"},{"metadata":{},"cell_type":"markdown","source":"# Business metric\nOur aim is to segment the student list into \"Placed\" or \"Not Placed\" such that the \"Not Placed\" student do not get false categorised. "},{"metadata":{},"cell_type":"markdown","source":"# Data science metric \nTrue Negative Rate is the metric used to evaluate model performance.\nIt is i given by:\nTNR=True Negative/(True Negative+False Positive)\nWe choose TNR because we need to find all those student who will not get placed. For this problem statement it is okay to get a few False Negatives i.e. a \"Placed\" student can be allowed classified into \"Not Placed\" but otherwise is dangerous. Also since Negative rate is less in number our model performance should prefer negative class performance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(plt.imread(\"../input/confusion/Confusion.png\"))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom(x,y,beta=2):\n    r=confusion_matrix(x,y)[0,0]/np.sum(confusion_matrix(x,y)[0,:])\n    p=confusion_matrix(x,y)[0,0]/np.sum(confusion_matrix(x,y)[:,0])\n    fbeta =((1+beta**2)*(r*p))/((beta**2)*p+r)\n    return r\n    \nTNR = make_scorer(custom,greater_is_better=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Stratified Train-Test Split\nStratified Train Test Split keeps the ratio of output classes same in train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat=StratifiedShuffleSplit(n_splits=1,test_size=0.15,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in strat.split(data,data['status']):\n    strat_train=data.loc[train_index]\n    strat_test=data.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are the ratios of \"Placed\" category in train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Tain_Placed \",strat_train[\"status\"].value_counts()[\"Placed\"]/len(strat_train[\"status\"]))\nprint(\"Test_Placed \",strat_test[\"status\"].value_counts()[\"Placed\"]/len(strat_test[\"status\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spliting status attribute from attribute data. Removing Salary attribute from the attribute data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X=strat_train.drop(['status','salary'],axis=1)\ntrain_Y=strat_train[\"status\"].values\ntest_X=strat_test.drop(['status','salary'],axis=1)\ntest_Y=strat_test[\"status\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning And Preparing for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.preprocessing import LabelBinarizer,StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline,FeatureUnion","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ATTRIBUTE LIST: ['sl_no', 'gender', 'ssc_p', 'ssc_b', 'hsc_p', 'hsc_b', 'hsc_s',\n       'degree_p', 'degree_t', 'workex', 'etest_p', 'specialisation', 'mba_p']"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data=['ssc_p','hsc_p','degree_p','etest_p','mba_p']\ncategroical_data=['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex','specialisation']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Pipeline\n### 1. Numerical Data Pipeline\n    1.1. Numerical data is extracted out of the data\n    1.2. Data is standard Scaled ((x-mean)/std)\n### 2. Categorical Data Pipeline\n    2.1. Cateegorical data is extracted out of the data\n    2.2. Data is transformed into one hot encoding(Binarizing) \n### 3. Feature Union\n    Both the pipelines are merged such that numerical and categorical transformed data are horizontaly stacked."},{"metadata":{},"cell_type":"markdown","source":"* **DataFrame_selector**: this class splits numerical and categorical attributes\n* **CustomLabelBinarizer**: Transforms data into one hot vectors\n* **StandardScaler**: Standardises the numerical data ((x-mean)/std)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrame_selector(BaseEstimator,TransformerMixin):\n    def __init__(self,column_list):\n        self.column_list=column_list\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        return X[self.column_list].values\nclass CustomLabelBinarizer(BaseEstimator, TransformerMixin):\n    def __init__(self, sparse_output=False):\n        self.sparse_output = sparse_output\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        enc = LabelBinarizer(sparse_output=self.sparse_output)\n        for i in range(len(X[0,:])):\n            if i ==0:\n                out=enc.fit_transform(X[:,i])\n            else:\n                out=np.hstack((out,enc.fit_transform(X[:,i])))\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"Numerical_pipeline=Pipeline([\n    ('df_selector',DataFrame_selector(numerical_data)),\n    ('StandardScaler',StandardScaler())\n])\nNumerical_pipeline.fit_transform(train_X).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorical_pipeline=Pipeline([\n    ('df_selector',DataFrame_selector(categroical_data)),\n    ('binary',CustomLabelBinarizer(sparse_output=False))\n])\nCategorical_pipeline.fit_transform(train_X).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Union"},{"metadata":{"trusted":true},"cell_type":"code","source":"final=FeatureUnion(transformer_list=[\n    ('numerical',Numerical_pipeline),\n    ('categorical',Categorical_pipeline)\n])\nfinal.fit_transform(train_X).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing train and test data"},{"metadata":{},"cell_type":"markdown","source":"### Train data transformed by our Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"proccessed_train=final.fit_transform(train_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test data transformed by our Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"proccessed_test=final.fit_transform(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FINAL_X=final.fit_transform(data.drop(['status','salary'],axis=1))\nFINAL_Y=data[\"status\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling and Model Selection\n"},{"metadata":{},"cell_type":"markdown","source":"## We will train \n1. Random Forest Classifier\n2. Decision Tree Classifier\n3. Logistic Regression\n4. K Nearest Neighbors Classifier\n5. Gaussian Naive Bayes\n6. Support Vector Machine Classifier\n#### We will tune the Hyperparameters against 10 fold crossvalidation and choose the Hyperparameters that yeild the best validation score for each model. After getting the best models for each classifier we predict against the test set to get the final performance of the model. The best performing model on TNR metric will be choosen. We use gridsearch for hyperparameter tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split=StratifiedKFold(n_splits=10,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgr=LogisticRegression()\n\nparam_grid = [{}]\ngrid_search = GridSearchCV(lgr, param_grid, cv=split,scoring=TNR)\nlgr_model=grid_search.fit(X=proccessed_train,y=train_Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"DTC=DecisionTreeClassifier(random_state=1,class_weight=\"balanced\",splitter='random')\nparam_grid = [\n    {'max_depth':[1,2,3,4],'max_features':[\"auto\", \"sqrt\", \"log2\"],'criterion':[\"gini\", \"entropy\"]}\n  ]\ngrid = GridSearchCV(DTC, param_grid, cv=split,scoring=TNR)\nDTC_model=grid.fit(X=proccessed_train,y=train_Y)\nDTC_model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import tree \ncl=['ssc_p','hsc_p','degree_p','etest_p','mba_p']+['gender', 'ssc_b', 'hsc_b', 'Commerce','Science','Arts', 'Sci&Tech','Comm&Mgmt','Others','workex','specialisation']\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=100)\ntree.plot_tree(DTC_model.best_estimator_,class_names=['Placed','Not Placed'],feature_names=cl)\nfig.savefig('imagename.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"GNB=GaussianNB()\nparam_grid = [\n    {'var_smoothing':[5,4,3,2.15,2.1,2,1,1e-2,1e-6,1e-7,1e-8,1e-9,1e-10]}]\ngrid = GridSearchCV(GNB, param_grid, cv=10,scoring=TNR)\nGNB_model=grid.fit(X=proccessed_train,y=train_Y)\nGNB_model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundaries(FINAL_X, FINAL_Y, GaussianNB,**GNB_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC=RandomForestClassifier()\nparam_grid = [\n    {'max_depth':[1,2,3],'n_estimators':[6,7,8],'criterion' : [\"gini\", \"entropy\"],'max_features':[\"auto\", \"sqrt\", \"log2\"],'random_state':[1],'class_weight' : [\"balanced\", \"balanced_subsample\",None]}]\ngrid = GridSearchCV(RFC, param_grid, cv=split,scoring=TNR)\nRFC_model=grid.fit(X=proccessed_train,y=train_Y)\nRFC_model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundaries(FINAL_X, FINAL_Y, RandomForestClassifier,**RFC_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K Nearest Neighbors Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNC=KNeighborsClassifier()\nparam_grid = [{'n_neighbors':[2,3,4],'leaf_size':[1,10],'p':[1,2,3,4],'weights':['uniform', 'distance'],'algorithm':['auto', 'ball_tree', 'kd_tree']}]\ngrid = GridSearchCV(KNC, param_grid, cv=split,scoring=TNR)\nKNC_model=grid.fit(X=proccessed_train,y=train_Y)\nKNC_model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nresults = permutation_importance(KNC_model, FINAL_X, FINAL_Y, scoring=TNR)\n# get importance\nimportance = results.importances_mean\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundaries(FINAL_X, FINAL_Y, KNeighborsClassifier,**KNC_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc=SVC(probability=True)\nparam_grid = [{'C':[0.1,0.5,1,2,3,3.5,4],'kernel':['linear', 'rbf'],'degree':[1,2,3,4,5],'gamma' : ['scale', 'auto'],'class_weight' : [None,'balanced']}]\ngrid = GridSearchCV(svc, param_grid, cv=split,scoring=TNR)\nSVC_model=grid.fit(X=proccessed_train,y=train_Y)\nSVC_model.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_decision_boundaries(FINAL_X, FINAL_Y, SVC,**SVC_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf=VotingClassifier(\n    estimators=[('lr',lgr_model.best_estimator_),\n            ('GNB',GNB_model.best_estimator_),\n            ('DTC',DTC_model.best_estimator_),\n            ('RFC',RFC_model.best_estimator_),\n            ('KNN',KNC_model.best_estimator_),\n            ('SVC',SVC_model.best_estimator_)\n           ],\nvoting='soft'\n)\n\nvoting_model=voting_clf.fit(proccessed_train,train_Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy of Model "},{"metadata":{},"cell_type":"markdown","source":"## Scoring on the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LOGISTIC REGRESSION CLASSIFIER    \",lgr_model.score(proccessed_test,test_Y))\nprint(\"GAUSSIAN NAIVE BAYES CLASSIFIER   \",GNB_model.score(proccessed_test,test_Y))\nprint(\"DECISION TREE CLASSIFIER          \",DTC_model.score(proccessed_test,test_Y))\nprint(\"RANDOM FOREST CLASSIFIER          \",RFC_model.score(proccessed_test,test_Y))\nprint(\"K NEAREST NEIGHBORS CLASSIFIER    \",KNC_model.score(proccessed_test,test_Y))\nprint(\"SUPPORT VECTOR MACHINE CLASSIFIER \",SVC_model.score(proccessed_test,test_Y))\nprint(\"VOTING                            \",voting_model.score(proccessed_test,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=[\n    lgr_model.score(proccessed_test,test_Y),\n    GNB_model.score(proccessed_test,test_Y),\n    DTC_model.score(proccessed_test,test_Y),\n    RFC_model.score(proccessed_test,test_Y),\n    KNC_model.score(proccessed_test,test_Y),\n    SVC_model.score(proccessed_test,test_Y),\n    voting_model.score(proccessed_test,test_Y)\n]\n\nx=[\n    \"LOGISTIC REGRESSION CLASSIFIER\",\"GAUSSIAN NAIVE BAYES CLASSIFIER\",\"DECISION TREE CLASSIFIER\",\n    \"RANDOM FOREST CLASSIFIER\",\n    \"K NEAREST NEIGHBORS CLASSIFIER\",\n    \"SUPPORT VECTOR MACHINE CLASSIFIER\",\n    \"VOTING CLASSIFIER (SOFT VOTING)\"\n]\n\nfig = plt.figure(figsize=[18,10])\nax = fig.add_axes([0,0,1,1])\nax.bar(x,y,width=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Global score"},{"metadata":{},"cell_type":"markdown","source":"## Let's Check how well have the models genralised"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LOR \",custom(FINAL_Y,lgr_model.predict(FINAL_X)))\nprint(\"GNB \",custom(FINAL_Y,GNB_model.predict(FINAL_X)))\nprint(\"DTC \",custom(FINAL_Y,DTC_model.predict(FINAL_X)))\nprint(\"RFC \",custom(FINAL_Y,RFC_model.predict(FINAL_X)))\nprint(\"KNC \",custom(FINAL_Y,KNC_model.predict(FINAL_X)))\nprint(\"SVC \",custom(FINAL_Y,SVC_model.predict(FINAL_X)))\nprint(\"VOTING \",custom(FINAL_Y,voting_model.predict(FINAL_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=[ \n    custom(FINAL_Y,lgr_model.predict(FINAL_X)),\n    custom(FINAL_Y,GNB_model.predict(FINAL_X)),\n    custom(FINAL_Y,DTC_model.predict(FINAL_X)),\n    custom(FINAL_Y,RFC_model.predict(FINAL_X)),\n    custom(FINAL_Y,KNC_model.predict(FINAL_X)),\n    custom(FINAL_Y,SVC_model.predict(FINAL_X)),\n    custom(FINAL_Y,voting_model.predict(FINAL_X))\n]\nx=[\"LOGISTIC REGRESSION CLASSIFIER\",\n\"GAUSSIAN NAIVE BAYES CLASSIFIER\",\n\"DECISION TREE CLASSIFIER\",\n\"RANDOM FOREST CLASSIFIER\",\n\"K NEAREST NEIGHBORS CLASSIFIER\",\n\"SUPPORT VECTOR MACHINE CLASSIFIER\",\n  \"VOTING CLASSIFIER (SOFT VOTING)\"]\nfig = plt.figure(figsize=[18,10])\n\nax = fig.add_axes([0,0,1,1])\nax.bar(x,y,width=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K Nearest Neighbors Classifier is best performing model and we select it with hyper parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNC_model.best_params_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}