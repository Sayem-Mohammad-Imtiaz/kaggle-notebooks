{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ec0c5193-86f0-bdb7-5f6d-df898a620c70"},"source":"A first attempt towards modeling the data supplied by Lumin. It's been a while since I played Settlers of Catan, but I find this data set very enjoyable :) Also, this is my first kernel upload here!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ce057dc-3ff6-5847-0e41-4a3a772e994c"},"outputs":[],"source":"%matplotlib inline\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')\nimport numpy as np"},{"cell_type":"markdown","metadata":{"_cell_guid":"7c74f45d-eb32-ba96-0b70-b3b15128a9e8"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34aa0320-872f-4e36-e0fe-2ebd3bf76051"},"outputs":[],"source":"dfCatan = pd.read_csv(\"../input/catanstats.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"6356ee7c-8c14-dde6-edf3-b9d7a33d1807"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"071eb60b-fbba-46c5-0df3-d3ac5c28e5b0"},"outputs":[],"source":"dfCatan['win'] = (dfCatan['points'] >= 10).astype(int)\ndfCatan['me'].fillna(0, inplace = True)\ndfCatan.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c3c3cdb8-9f49-2d50-bdfa-4170a69b0e56"},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"3cd62497-8b94-6ba4-bf09-24750aa49add"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"825b8ddc-9c3f-c8f4-e343-d7f4284a1138"},"outputs":[],"source":"#Is the position of the player important for winning?\nfig = plt.figure(figsize=(13,4))\n\nax = fig.add_subplot(1,2,1)\nplayer_win = dfCatan[dfCatan['win'] == 1]['player'].value_counts()\nplayer_loss = dfCatan[dfCatan['win'] == 0]['player'].value_counts()/3\n\ndfTempPlot = pd.DataFrame([player_win,player_loss])\ndfTempPlot.index = ['Win','Loss']\ndfTempPlot.plot(kind = 'bar',stacked = True, title = 'Winning depends on player position...', ax=ax)\nax.set_ylabel('Games (scaled \"loss\" bar for comparison)')\n\n#Does this also hold for \"me\"?\nax2 = fig.add_subplot(1,2,2)\nme_win = dfCatan[(dfCatan['win'] == 1) & (dfCatan['me'] == 1.0)]['player'].value_counts()\nme_loss = dfCatan[(dfCatan['win'] == 0) & (dfCatan['me'] == 1.0)]['player'].value_counts()\n\ndfTempPlot = pd.DataFrame([me_win,me_loss])\ndfTempPlot.index = ['Win','Loss']\ndfTempPlot.plot(kind = 'bar',stacked = True, title = '... also for \"me\" - being player 2 is a good thing', ax=ax2)\nax2.set_ylabel('Games')\n\nprint('\"me\" wins ' + str(100*sum(dfCatan[dfCatan['me'] == 1.0]['win'])/float(max(dfCatan['gameNum']))) + '% of all games!')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc892dc9-f6a4-1642-2615-3ac0e57f90c7"},"source":"I will take a look at the effects of the dice rolls and settlement properties later on (not in the current version of this document). "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9b4abae-cde3-fc82-2f6e-fd43a8f6f2f7"},"outputs":[],"source":"#Let's look at some correlations - small data set, so take p-value with grain of salt\nfrom scipy.stats.stats import pearsonr\n\nfig = plt.figure(figsize=(15,6))\nax = fig.add_subplot(2,3,1)\nax.scatter(dfCatan['production'], dfCatan['points'], c='black')\nax.set_title('Points vs Production')\nax.text(20, 12, 'r = '+ str(round(pearsonr(dfCatan['production'], dfCatan['points'])[0],2)))\n\nax2 = fig.add_subplot(2,3,2)\nax2.scatter(dfCatan['tradeGain'], dfCatan['points'], c='black')\nax2.set_title('Points vs Trade gain')\nax2.text(0, 12, 'r = '+ str(round(pearsonr(dfCatan['tradeGain'], dfCatan['points'])[0],2)))\n\nax3 = fig.add_subplot(2,3,3)\nax3.scatter(dfCatan['robberCardsGain'], dfCatan['points'], c='black')\nax3.set_title('Points vs Robber cards gain')\nax3.text(0, 12, 'r = '+ str(round(pearsonr(dfCatan['robberCardsGain'], dfCatan['points'])[0],2)))\n\nax4 = fig.add_subplot(2,3,4)\nax4.scatter(dfCatan['tribute'], dfCatan['points'], c='red')\nax4.set_title('Points vs Tribute')\nax4.text(0, 1, 'r = '+ str(round(pearsonr(dfCatan['tribute'], dfCatan['points'])[0],2)))\n\nax5 = fig.add_subplot(2,3,5)\nax5.scatter(dfCatan['tradeLoss'], dfCatan['points'], c='red')\nax5.set_title('Points vs Trade loss')\nax5.text(0, 1, 'r = '+ str(round(pearsonr(dfCatan['tradeLoss'], dfCatan['points'])[0],2)))\n\nax6 = fig.add_subplot(2,3,6)\nax6.scatter(dfCatan['robberCardsLoss'], dfCatan['points'], c='red')\nax6.set_title('Points vs Robber cards loss')\nax6.text(0, 1, 'r = '+ str(round(pearsonr(dfCatan['robberCardsLoss'], dfCatan['points'])[0],2)))\n\nfig.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6b11eb7e-366d-abb1-d978-907e1d6c5b34"},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"4d4b804b-ba18-443a-4b7e-15f818733fc4"},"source":"Based on our brief exploration above, we might expect the variables 'production', 'tradeGain', 'robberCardsGain' and 'tradeLoss' to be helpful in predicting the outcome of the game. Let's start with this (note that I'm not using the 'player' variable we considered above). Eventually, though, one would probably also want to include settlement locations and probabilities of obtaining certain resources into the model (since these underpin the values of e.g. production).\n\nFirst, we split the data into train (80%) and test (20%) sets - and then we create a simple random forest model, evaluate the accuracy and create a confusion matrix."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bba1bf9e-71b9-36ff-c409-8cd997f91c81"},"outputs":[],"source":"#Create train and test data sets\nfrom sklearn.cross_validation import train_test_split\n\n#Let's use production, tradeGain, robberCardsGain and tradeLoss only\ninputData = dfCatan[['production','tradeGain','robberCardsGain','tradeLoss']]\ntargetVal = dfCatan['win']\n\n#And scale the data to have mean=0 and std=1\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(inputData)\ninputData_scaled = scaler.transform(inputData)\n\nX_train, X_test, y_train, y_test = train_test_split(inputData_scaled, targetVal, test_size=0.20)#, random_state=42)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6611801c-3b6f-b709-bb74-26746648ed27"},"outputs":[],"source":"#Let's make an RF model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.grid_search import GridSearchCV\n\nforest = RandomForestClassifier()\n\nparameter_grid = {'max_depth': [4,5,6,7,8], \n                  'n_estimators': [120,200,250,500]\n                   }\ncross_validation = StratifiedKFold(y_train, n_folds=5)\ngrid_search = GridSearchCV(estimator=forest, param_grid=parameter_grid, cv=cross_validation)\n\n#Fit the model\ngrid_search.fit(X_train, y_train)\n\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"650c9448-238d-fb18-385a-33235a92b25d"},"outputs":[],"source":"#Piece of code for plotting a neat confusion matrix\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(2)\n    plt.xticks(tick_marks, ['Loss', 'Win'], rotation=45)\n    plt.yticks(tick_marks, ['Loss', 'Win'])\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a80afa2e-70b7-09c9-6963-24ff71ad50a9"},"outputs":[],"source":"from sklearn.metrics import confusion_matrix\n\n#Now it's time for predictions...\noutput = grid_search.predict(X_test)\nprint('Accuracy: ' + str(sum(output == y_test)/float(len(y_test))))\n\ncm = confusion_matrix(y_test, output)\n\nprint('Confusion matrix')\nprint(cm)\nfig = plt.figure(figsize=(10,4))\nplot_confusion_matrix(cm)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ecab2510-5241-6673-54ef-93f330a9561d"},"source":"The accuracy generally hovers around 0.75-0.80, using just random forest and 4 variables. Logistic regression and SVC perform roughly equally well. (To be continued :) )"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}