{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Since I've already done the data exploration, cleaning and visualization in a previous notebook, www.kaggle.com/rogges23/ap-liver-data-analysis\n\nI'll move straight into using my cleaned dataset and evaluating models.\n\nThe general plan is to:\n1. Load up the data, get rid of any artifacts, and a stratified train-test-split.\n\n2. Scaling numerical data; the protein/enzyme data especially are extremely variable and could probably benefit from standardization.\n\n3. Using 10 fold cross validation to compare a LinearSVC, SVC, Logistic Regression, K Neighbors Classifier, and Random Forest Classifier.\n\n4. After comparing mean accuracy and the standard deviation of accuracy of the models on 10 fold CV, I'll pick out the best one for hyperparameter tuning and final training. Ideally, I'd compare the models with a classification report (precision/recall/F1 score) but I'll save that for the final model.\n\n5. Final model tuning, training, testing."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import (\n    train_test_split, RandomizedSearchCV, GridSearchCV, KFold, cross_val_score\n)\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/liverpatientclean/liver-patient-clean.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After I cleaned my data and saved it as a csv, Unnamed: 0 appeared as a column for whatever reason. Both the Liver Patient and Gender categorical variables are severely imbalanced (more males and those with liver disease) so it's important to stratify the train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"Unnamed: 0\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_strat = data[[\"Liver Patient\", \"Gender_Female\"]]\n\nx_train, x_test, y_train, y_test = train_test_split(data.drop([\"Liver Patient\"], axis = 1),\n                                                    data[\"Liver Patient\"],\n                                                    test_size = 0.2,\n                                                    stratify = to_strat,\n                                                    random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the Standard Scaler to separately scale the training data and then using that to transform the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_scale = [\"Age\", \"Total_Bilirubin\", \"Direct_Bilirubin\", \"ALP\", \"ALT\", \"AST\", \"Protein\", \"Albumin\", \"AGR\"]\n\nscaler = StandardScaler()\nscaler.fit(x_train[to_scale])\nx_train[to_scale] = scaler.transform(x_train[to_scale])\nx_test[to_scale] = scaler.transform(x_test[to_scale])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm evaluating the models by compiling a dictionary of instances first. From there, I just made a general function where I can pass in the data and dictionary of models with the desired metric and just compare them all with print out statements."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\"LSVC\": LinearSVC(), \"SVC\": SVC(), \"K Neighbors\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier(),\n          \"Logistic Regression\": LogisticRegression()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_models(models, X_data, y_data, n_fold, score):\n    for name, model in models.items():\n        kfold = KFold(n_splits = n_fold)\n        cv_results = cross_val_score(model, X_data, y_data, cv = kfold, scoring = score)\n        eval_msg = f\"{name}: {cv_results.mean()} ({cv_results.std()})\"\n        print(eval_msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_models(models, x_train, y_train, 10, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Model - Hyperparameter Tuning and Training"},{"metadata":{},"cell_type":"markdown","source":"The Random Forest Classifier ended up consistently better on my system on multiple runs since I ran through this process multiple times when attempting to come up with an acceptable workflow, so I'm choosing RFC. Based on the multiple runs, or even just this iteration, it shouldn't matter much as long as you don't choose K Neighbors. Choosing RFC also makes it that the standardized scaling was ultimately unnecessary but it definitely made the comparison phase more competitive."},{"metadata":{},"cell_type":"markdown","source":"The following function is just an automated way of finding a good set of hyperparameters for RFC with RandomizedSearchCV. It'll be easy to copy and paste into future projects. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_hyper_RFC(X_Train, Y_Train, cross_val = 5):\n    hyperparameters = {'n_estimators': [100, 300, 500, 800, 1200], \n                       'max_depth': [5, 10, 15, 25, 30],\n                       'min_samples_split': [2, 5, 10, 25, 100],\n                       'min_samples_leaf': [1, 2, 5, 10]}\n    random_search = RandomizedSearchCV(RandomForestClassifier(), hyperparameters, cv = cross_val)\n    best_fit = random_search.fit(X_Train, Y_Train)\n    \n    return best_fit.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_RFC_params = random_hyper_RFC(x_train, y_train)\ntuned_RFC = RandomForestClassifier(**new_RFC_params)\ntuned_RFC.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report = classification_report(y_test, tuned_RFC.predict(x_test), output_dict = True)\nreport_simple = {\"No Liver Disease\": report[\"0\"], \"Liver Disease\": report[\"1\"]}\npd.DataFrame(report_simple)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True, cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Some Redundant Features"},{"metadata":{},"cell_type":"markdown","source":"I'm going to remove some features based on clinical knowledge and then repeat the general flow for the RFC."},{"metadata":{},"cell_type":"markdown","source":"Total_Bilirubin and Direct_Bilirubin are heavily correlated as in they're a part of a set of 3 values (Indirect Bilirubin being the third), where if you have any 2 values, you can calculate the third. Total Bilirubin is sum of Direct Bilirubin and Indirect Bilirubin. Of the three, I'd have Direct Bilirubin since that's the amount of Bilirubin that's been conjugated by the liver. \n\nI also don't need Protein or AGR because Albumin is likely the biggest contributor to both which is something I have as well. Albumin is the most common plasma protein so if you have decreased plasma protein, you most likely have decreased albumin as well and vice versa. AGR is the Albumin to Globulin ratio and is most often decreased clinically speaking. Decreased AGR can occur due to a decrease in Albumin (as in liver disease) or an increase in Globulins (e.g. infection/neoplasia). In this setting, AGR is probably even more tied to Albumin than normal. \n\nI could have done this with either a feature selection method or a correlation matrix but I thought it might be interesting to just try using clinical knowledge to make the decision. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train2 = x_train.drop([\"Total_Bilirubin\", \"Protein\", \"AGR\"], axis = 1).copy()\nx_test2 = x_test.drop([\"Total_Bilirubin\", \"Protein\", \"AGR\"], axis = 1).copy()\n\nnew_RFC_params2 = random_hyper_RFC(x_train2, y_train)\ntuned_RFC2 = RandomForestClassifier(**new_RFC_params2)\ntuned_RFC2.fit(x_train2, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report2 = classification_report(y_test, tuned_RFC2.predict(x_test2), output_dict = True)\nreport_simple2 = {\"No Liver Disease\": report2[\"0\"], \"Liver Disease\": report2[\"1\"]}\npd.DataFrame(report_simple2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(pd.DataFrame(report2).iloc[:-1, :].T, annot=True, cmap = \"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impressions"},{"metadata":{},"cell_type":"markdown","source":"Despite the poorer scores on predicting people without liver disease, I'm actually more impressed by that. The class imbalance in this dataset is considerable (approx. 400 liver patients, and 150 non-liver patients). I could've just made something that predicts everyone has liver disease, that would've given me a liver patient recall of 1, and precision of approximately 0.72. The non-liver patient recall and precision scores would have been 0 however. \n\nBoth models with the scaled features performed better on precision and recall for non-liver patients than when I was just messing around without processing the data (0.1-0.3 precision and 0.01-0.15 recall). The better performance of the data without the redundant features removed could be because my clinical knowledge helped improve the model or it could just be a fluke based on the random_state in the train_test_split. I'd like to believe it was the clinical knowledge though.\n\nGoing forward, I'd probably look into undersampling or oversampling to help address the class imbalance and use hypothesis tests to determine if removing the features I thought extraneous actually were hindering the model. There may also be some value in engineering a new feature (ratio of Direct Bilirubin to Indirect Bilirubin). "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}