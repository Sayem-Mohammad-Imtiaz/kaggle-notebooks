{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# News Category Dataset-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Converting JSON to CSV-->","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_json ('/kaggle/input/news-category-dataset/News_Category_Dataset_v2.json',lines=True)\ndf.to_csv (r'output.csv', index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['headline'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As it is a large dataset so dropping the duplicates..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates('headline',keep = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\nstop_words = text.ENGLISH_STOP_WORDS.union(punc)\ndesc = df['headline'].values\nvectorizer = TfidfVectorizer(stop_words = stop_words)\nX = vectorizer.fit_transform(desc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_features = vectorizer.get_feature_names()\nprint(len(word_features))\nprint(word_features[7000:7300])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mini Batch K Means-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_no_of_clusters(data, max_k):\n    \n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        value=MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data)\n        sse.append(value.inertia_)           \n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Number of Clusters')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('Inertia')\n    ax.set_title('Inertia vs Number of Clusters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_no_of_clusters(X, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So we will go with number of clusters as 14..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = MiniBatchKMeans(n_clusters=14, init_size=1024, batch_size=2048, random_state=20).fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating top keywords of each cluster..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_top_keywords(X, clusters, vectorizer.get_feature_names(), 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Here we are seeing that the clusters are quite relatable..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Department of Justice Press Releases Dataset-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Converting JSON to CSV-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data= pd.read_json ('/kaggle/input/department-of-justice-20092018-press-releases/combined.json',lines=True)\ndata.to_csv (r'output.csv', index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\nstop_words = text.ENGLISH_STOP_WORDS.union(punc)\ndesc = data['contents'].values\nvectorizer = TfidfVectorizer(stop_words = stop_words)\nnew_x = vectorizer.fit_transform(desc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mini Batch K Means-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_no_of_clusters(data, max_k):\n    \n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        value=MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data)\n        sse.append(value.inertia_)           \n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Number of Clusters')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('Inertia')\n    ax.set_title('Inertia vs Number of Clusters')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_no_of_clusters(new_x, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So we will go with number of clusters as 12","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = MiniBatchKMeans(n_clusters=12, init_size=1024, batch_size=2048, random_state=20).fit_predict(new_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top keywords of each Cluster..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_top_keywords(new_x, clusters, vectorizer.get_feature_names(), 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In another dataset also our Mini Batch K Means algorithm is working nicely..","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Trying with K Means -->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n    kmeans.fit(new_x)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.savefig('elbow.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I was trying for k means algorithm in this dataset but it was taking too much time to process so i aborted.\n# The Mini Batch K Means algorithm performed well..","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}