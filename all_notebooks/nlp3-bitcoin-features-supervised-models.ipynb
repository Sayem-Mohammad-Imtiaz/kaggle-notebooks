{"cells":[{"metadata":{},"cell_type":"markdown","source":"ATTENTION : this workbook needs to run and import the dataset created at the end of this step : [step2](https://www.kaggle.com/leticehs/nlp-bitcoin-analysis-2-preprocessing)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# \napplying deep learning on twitterâ€™s sentiment analysis\n\nTrain Model - use keras to build and train a deep neural network model\n\nEvaluate Model - measure the accuracy of the predictive model, and suggest further improvements"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#1 IMPORTING LIBRARIES AND DATASET","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nimport pandas as pd\nimport numpy as np\nimport re\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport datetime\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn import decomposition, ensemble\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#being able to read csv stored in google drive \nfrom google.colab import drive\ndrive.mount(\"/content/drive\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the dataset with no columns titles and with latin encoding \ndf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP/tweetsClean.csv')\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there is any missing value and datatype \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# checking for null values, if any\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ditching all row when text is null, as need text for analysis\ndf.dropna(how='any', inplace=True)\ndf.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2- EXTRACTING FEATURES FROM CLEANED TWEETS 10 min"},{"metadata":{},"cell_type":"markdown","source":".1 Count Vectors as features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#bag of words = OPTION A - need to limit to 1000, else too long and gets 951k results\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=500, stop_words='english',analyzer='word', token_pattern=r'\\w{1,}')\nbow = bow_vectorizer.fit_transform(df['clean'])\nbow.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ditc= bow_vectorizer.vocabulary_\nprint(ditc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bow_vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".2 TF-IDF Vectors as features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TfIdf = OPTION B - also had to downsize features as too long for not much value added words\n\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=500, stop_words='english',analyzer='word', token_pattern=r'\\w{1,}')\ntfidf = tfidf_vectorizer.fit_transform(df['clean'])\ntfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ditc2 = tfidf_vectorizer.vocabulary_\n print(ditc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf_vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ngram level tf-idf TAKES 12 minutes to run , after the above that takes 6 min\nvect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=500)\nngram = vect_ngram.fit_transform(df['clean'])\nngram.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".3 Vectors - cannot be used, as words are too small and do not rhyme at all (BTC, ETH, USD...) no dict for that\nWord 2 Vec : KeyError: \"word 'eth vs btc relative vol spread interesting junction esp given btc dominance v alt season sentiment participants cryptooptions releativevalue' not in vocabulary\" or \"word 'bizpaye trading platform system unique never done history modern day trade exchanges bizpaye marketplace hodl bartercredit crypto cryptotrading btc onlineshopping merchants ecommerce bb bc retail' not in vocabulary\""},{"metadata":{},"cell_type":"markdown","source":"3- LAUNCHING MODEL BASES - all above takes 20 min to load"},{"metadata":{},"cell_type":"markdown","source":".1 DEFINING X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"#2- CREATING a FAKE Y\n#ate 11 dec 2017\n#ate 10 dec 2018\n#ate end\n\ndef senti(x):\n  if x < 2018:\n    return 'BULL'\n  elif x > 2018:\n    return 'BULL'\n  else:\n    return 'BEAR'\n\ndf['sent'] = df['year'].apply(lambda x: senti(x) )\ndf.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sent'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y = np.array(df['sent']).ravel()\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(df['sent'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":".2 LAUNCHING MODEL BASES"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, y_test), metrics.precision_score(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ALL CLASSIFIER MODELS - can start after 13 min of running above"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes on Count Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), X_count_train, y_train, X_count_test)\nprint(\"NB, on Bag of Words : Accuracy, Precision \", accuracy)\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), X_tfidf_train, y_train, X_tfidf_test)\nprint(\"NB, on TF-IDF  : Accuracy, Precision \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log Reg Classifier on Count Vectors - takes 4 min\naccuracy = train_model(linear_model.LogisticRegression(), X_count_train, y_train, X_count_test)\nprint(\"LR, Bag of Words: : Accuracy, Precision \", accuracy)\n\n# Log Reg Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=100), X_tfidf_train, y_train, X_tfidf_test)\nprint(\"LR, on TF-IDF  : Accuracy, Precision \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM on Ngram Level TF IDF Vectors - takes forever to calculate MORE THAN 10 MN\naccuracy = train_model(svm.SVC(kernel='linear'), X_ngram_train, y_train, X_ngram_test)\nprint(\"SVM,  on NGRAM  : Accuracy, Precision, Recall \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RF on Count Vectors - takes more than 20 min, never finished\naccuracy = train_model(ensemble.RandomForestClassifier(n_estimators = 100),X_count_train, y_train, X_count_test)\nprint(\"RF, on Bag of Words : Accuracy, Precision, Recall \", accuracy)\n\n# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), X_tfidf_train, y_train, X_tfidf_test)\nprint(\"RF, on TF-IDF  : Accuracy, Precision, Recall \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extereme Gradient Boosting on Count Vectors - 31 mn for bag of words results and counting\naccuracy = train_model(xgboost.XGBClassifier(), X_count_train.tocsc(), y_train, X_count_test.tocsc())\nprint(\"Xgb, on Bag of Words : Accuracy, Precision, Recall \", accuracy)\n\n# Extereme Gradient Boosting on Word Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), X_tfidf_train.tocsc(), y_train, X_tfidf_test.tocsc())\nprint(\"Xgb, on TF-IDF  : Accuracy, Precision, Recall \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ON THE MODEL THAT SHOWS MOST ACCURACY, DIG ON ITS SPECS\n# Model Accuracy: how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict_svm))\n# Model Precision: what percentage of positive tuples are labeled as such?\nprint(\"Precision:\",metrics.precision_score(y_test, y_predict_svm))\n# Model Recall: what percentage of positive tuples are labelled as such?\nprint(\"Recall:\",metrics.recall_score(y_test, y_predict_svm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"THE END ---- TRYING TO CLASSIFY OUR TWEETS INTO A BULL/BEAR SENTIMENT LEADS NO WHERE, I WILL NOW RUN THE CLUSTER WORD ANALYSIS IN THE NEXT NLP4 notebook "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}