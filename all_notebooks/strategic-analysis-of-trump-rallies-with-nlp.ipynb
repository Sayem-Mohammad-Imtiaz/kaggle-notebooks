{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Strategic Analysis of Trump Rallies with NLP\n\n- Initially obtained dataset of 35 rallies from Kaggle but they were found to be too few for time series.\n- Web scrapped about 100 Trump rallies from https://factba.se/ from first rally in 2017 to the first rally in 2020 when Trump recovered from Covid-19.\n- Planned to do topic modelling on all Trump rallies, and develop a time series analysis of the rallies. Where possible, I plan to make time series predictions of his trending topics.\n- Also possible to do clustering of Trump rallies based on topics.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import NMF  \nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_absolute_error,silhouette_score\n\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom wordcloud import WordCloud\nfrom nltk.sentiment import vader\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import ne_chunk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\n#nltk.download('punkt')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('maxent_ne_chunker')\n#nltk.download('words')\n#nltk.download('stopwords')\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nfrom plotly.offline import iplot\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 2000)\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/donald-trump-political-rallies-transcripts/Trump_Rallies_Dataset.csv',parse_dates=['Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop('Unnamed: 0',axis=1)\ndf = df.drop(97, axis=0)\ndf = df.reset_index().drop('index',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Pre-processing\n- Regular Expression\n- Tokenization using NLTK Regex Tokenizer\n- Stopwords removal using Gensim","metadata":{}},{"cell_type":"code","source":"df['Place'] = df['Place'].apply(lambda x: re.sub(r'^.*(?=Rally)','',x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transcript'] = df['Transcript'].apply(lambda x: re.sub(r'\\[.*?\\]','',x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transcript'] = df['Transcript'].apply(lambda x: x.lower())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = RegexpTokenizer('[a-z][a-z]+[a-z]')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transcript'] = df['Transcript'].apply(lambda x: tokenizer.tokenize(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transcript'] = df['Transcript'].apply(lambda x: ' '.join(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transcript'] = df['Transcript'].apply(lambda x: x.replace('\\\\',''))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def nltk_tag_to_wordnet_tag(nltk_tag):\n#     if nltk_tag.startswith('J'):\n#         return wordnet.ADJ\n#     elif nltk_tag.startswith('V'):\n#         return wordnet.VERB\n#     elif nltk_tag.startswith('N'):\n#         return wordnet.NOUN\n#     elif nltk_tag.startswith('R'):\n#         return wordnet.ADV\n#     else:          \n#         return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def lemmatize_sentence(sentence):\n#     #tokenize the sentence and find the POS tag for each token\n#     tokenizer = RegexpTokenizer('[a-z][a-z]+[a-z]')\n#     nltk_tagged = nltk.pos_tag(tokenizer.tokenize(sentence))  \n#     #tuple of (token, wordnet_tag)\n#     wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n#     lemmatized_sentence = []\n#     for word, tag in wordnet_tagged:\n#         if tag is None:\n#             #if there is no available tag, append the token as is\n#             lemmatized_sentence.append(word)\n#         else:        \n#             #else use the tag to lemmatize the token\n#             lemmatized_sentence.append(WordNetLemmatizer().lemmatize(word, tag))\n#     return \" \".join(lemmatized_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['Transcript'] = df['Transcript'].apply(lambda x: lemmatize_sentence(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transcript'] = df['Transcript'].apply(lambda x: remove_stopwords(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[0,'Transcript']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n- Using WordCloud on first 2 and last 2 rallies","metadata":{}},{"cell_type":"code","source":"wordcloud = WordCloud(max_words=30, background_color='white',colormap='magma',width=800, height=400, random_state=48).generate(df.loc[100,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud1.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(max_words=30, background_color='white',colormap='magma',width=800, height=400, random_state=71).generate(df.loc[101,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud2.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(max_words=30,background_color='white',colormap='seismic',width=800, height=400,random_state=28).generate(df.loc[0,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud3.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(max_words=30, background_color='white',colormap='seismic',width=800, height=400,random_state=48).generate(df.loc[1,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud4.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Topic Modeling and Exploratory Data Analysis with Heatmap\n- Scikit-learn Count Vectorizer\n- Scikit-learn Non-negative Matrix Factorization (NMF)\n- Scikit-learn Cosine Similarity","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words = 'english')\ndoc_term = count_vectorizer.fit_transform(list(df['Transcript']))\ncountvec = count_vectorizer.fit(list(df['Transcript']))\ndt_matrix = pd.DataFrame(doc_term.toarray().round(3), index=[i for i in df['Place']], columns=count_vectorizer.get_feature_names()).head(10)\ndt_matrix ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Try NMF with Countvectorizer (Random = 42)","metadata":{}},{"cell_type":"code","source":"nmf_model = NMF(n_components = 3,random_state=42)\nmodel = nmf_model.fit(doc_term)\ndoc_topic = model.transform(doc_term)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_word = pd.DataFrame(nmf_model.components_.round(3),\n             index = [\"component_1\",\"component_2\",\"component_3\"],\n             columns = count_vectorizer.get_feature_names())\ntopic_word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_topics(model, feature_names, no_top_words, topic_names=None):\n    for ix, topic in enumerate(model.components_):\n        if not topic_names or not topic_names[ix]:\n            print(\"\\nTopic \", ix)\n        else:\n            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n        print(\", \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_topics(nmf_model, count_vectorizer.get_feature_names(), 30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_index = []\nfeature_names = count_vectorizer.get_feature_names()\nfor ix, topic in enumerate(nmf_model.components_):\n    topic_index.append(\" \".join([feature_names[i] for i in topic.argsort()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H = pd.DataFrame(doc_topic.round(3),\n             index = [i for i in df['Place']],\n             columns = [\"component_1\",\"component_2\",\"component_3\"])\nH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H = H.reset_index()\nH = df.join(H).drop(['Transcript','index'],axis=1)\nH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H = H.rename(columns={'component_1':'Achievements','component_2':'Plans and Appealing Support','component_3':'Political Adversaries'})\nH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DF = H.copy()\nDF = DF.drop(['Place','Date'],axis=1)\nDF = DF.iloc[:21]\ncos_similar_matrix = pd.DataFrame(cosine_similarity(DF.values),columns=H['Date'].iloc[:21].astype(str),index=H['Date'].iloc[:21].astype(str))\ncos_similar_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=1.4)\nfig = plt.figure(figsize=[20,18])\nmask = np.triu(np.ones_like(cos_similar_matrix, dtype=bool))\nsns.heatmap(cos_similar_matrix,cmap='Blues',linewidth=3,linecolor='white',vmax = 1, vmin=0.1,mask=mask, annot=True,fmt='0.2f')\nplt.title('Cosine Similarity Heatmap - Last 20 Rallies', weight='bold',fontsize=25)\nplt.xlabel('')\nplt.ylabel('')\nplt.savefig('heatmap.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time Series with SARIMA\n- Create Simple Validation manually\n- Observe seasonality in topics in Trump rallies\n- Time Series is observed to be non-stationary with trend and seasonality => SARIMA\n- Manually optimize the period and trend parameter in SARIMA\n- MAE is used as a metric for prediction","metadata":{}},{"cell_type":"code","source":"data = H.drop(['Place','Date'],axis=1)\ndata.index = H['Date']\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.iloc[::-1]\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = data[:int(0.90*(len(data)))]\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid = data[int(0.90*(len(data))):int(0.95*(len(data)))]\nvalid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = data[int(0.95*(len(data))):]\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_Achievements = train['Achievements']\ntrain_Support = train['Plans and Appealing Support']\ntrain_Adversaries = train['Political Adversaries']\nvalid_Achievements = valid['Achievements']\nvalid_Support = valid['Plans and Appealing Support']\nvalid_Adversaries = valid['Political Adversaries']\ntest_Achievements = test['Achievements']\ntest_Support = test['Plans and Appealing Support']\ntest_Adversaries = test['Political Adversaries']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mae_vector= []\n\n# for period in np.arange(8,13,1):    \n#     for trend in ['n','c','t','ct']:\n#         for p in [1]:\n#             for d in [0]:\n#                 for q in [0]:\n#                     for P in [1]:\n#                         for D in [1]:\n#                             for Q in [1]:\n#                                 forecast_Adversaries = SARIMAX(train_Adversaries,order=(p,d,q),seasonal_order=(P,D,Q,period),trend=trend).fit().forecast(steps=len(valid_Adversaries))\n#                                 MAE_Adversaries = mean_absolute_error(valid['Political Adversaries'],forecast_Adversaries) \n#                                 mae_vector.append((MAE_Adversaries,[(p,d,q),(P,D,Q,period),trend]))\n#     print(period)\n\n# mae, para = zip(*mae_vector)        \n\n# print(f'Best Parameters is {para[np.argmin(mae)]}')    \n# print(f'Lowest MAE is {min(mae)}') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mae_vector= []\n\n# for period in np.arange(8,13,1):    \n#     for trend in ['n','c','t','ct']:\n#         for p in [1]:\n#             for d in [0]:\n#                 for q in [0]:\n#                     for P in [1]:\n#                         for D in [1]:\n#                             for Q in [1]:\n#                                 forecast_Support = SARIMAX(train_Support,order=(p,d,q),seasonal_order=(P,D,Q,period),trend=trend).fit().forecast(steps=len(valid_Support))\n#                                 MAE_Support = mean_absolute_error(valid['Plans and Appealing Support'],forecast_Support) \n#                                 mae_vector.append((MAE_Support,[(p,d,q),(P,D,Q,period),trend]))\n#     print(period)\n\n# mae, para = zip(*mae_vector)        \n\n# print(f'Best Parameters is {para[np.argmin(mae)]}')    \n# print(f'Lowest MAE is {min(mae)}') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mae_vector= []\n\n# for period in np.arange(8,13,1):    \n#     for trend in ['n','c','t','ct']:\n#         for p in [1]:\n#             for d in [0]:\n#                 for q in [0]:\n#                     for P in [1]:\n#                         for D in [1]:\n#                             for Q in [1]:\n#                                 forecast_Achievements = SARIMAX(train_Achievements,order=(p,d,q),seasonal_order=(P,D,Q,period),trend=trend).fit().forecast(steps=len(valid_Achievements))\n#                                 MAE_Achievements = mean_absolute_error(valid['Achievements'],forecast_Achievements) \n#                                 mae_vector.append((MAE_Achievements,[(p,d,q),(P,D,Q,period),trend]))\n#     print(period)\n\n# mae, para = zip(*mae_vector)        \n\n# print(f'Best Parameters is {para[np.argmin(mae)]}')    \n# print(f'Lowest MAE is {min(mae)}') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainvalid = data[:int(0.95*(len(data)))]\ntrainvalid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainvalid_Achievements = trainvalid['Achievements']\ntrainvalid_Support = trainvalid['Plans and Appealing Support']\ntrainvalid_Adversaries = trainvalid['Political Adversaries']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast_Adversaries = SARIMAX(trainvalid_Adversaries,order=(1,0,0),seasonal_order=(1,1,1,10),trend='ct').fit().forecast(steps=len(test_Adversaries))\nMAE_Adversaries = mean_absolute_error(test['Political Adversaries'],forecast_Adversaries)\nprint(f'MAE_Adversaries: {MAE_Adversaries}')\n    \nforecast_Support = SARIMAX(trainvalid_Support,order=(1,0,0),seasonal_order=(1,1,1,8),trend='ct').fit().forecast(steps=len(test_Support))\nMAE_Support = mean_absolute_error(test['Plans and Appealing Support'],forecast_Support) \nprint(f'MAE_Support: {MAE_Support}')\n    \nforecast_Achievements = SARIMAX(trainvalid_Achievements,order=(1,0,0),seasonal_order=(1,1,1,9),trend='n').fit().forecast(steps=len(test_Achievements))\nMAE_Achievements = mean_absolute_error(test['Achievements'],forecast_Achievements)  \nprint(f'MAE_Achievements: {MAE_Achievements}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = test.copy()\npredicted['Achievements'] = list(forecast_Achievements)\npredicted['Plans and Appealing Support'] = list(forecast_Support)\npredicted['Political Adversaries'] = list(forecast_Adversaries)\npredicted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted2 = predicted.copy()\npredicted2 = predicted2.reset_index()\npredicted2['Date'] = predicted2['Date'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H2 = H.copy()\nH2['Date'] = H2['Date'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H3 = H2.copy()\nH3 = H3.iloc[::-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.xticks(list(H2['Date'])[0::20])\n#ymin, ymax = plt.ylim()\n#plt.vlines(list(H2['Date'])[3::10], ymin, ymax, linestyle='dashed')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Attacking Political Adversaries',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot1_0.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x=H2.index[::-1],y=H2['Political Adversaries'],style=True,markers=True,ci=None,color='r')\nlst = ['']*102\nlst[-4::-10] = list(H2['Date'][3::10])\nplt.xticks(ticks=range(1,103),labels=lst)\nymin, ymax = plt.ylim()\nplt.vlines(np.arange(8,103,10), ymin, ymax, linestyle='dashed')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Attacking Political Adversaries',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot1.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Range = np.arange(8,103,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=1.2,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[9,6])\n#sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.plot(H3['Date'],H3['Political Adversaries'],marker='.',markersize=10,linewidth=1.5,linestyle='-',color='r',label='Actual')\nplt.plot(predicted2['Date'],predicted2['Political Adversaries'],marker='.',markersize=10,linewidth=2,linestyle='--',color='k',label='Predicted')\nplt.xticks(list(H2['Date'])[3::10])\nymin, ymax = plt.ylim()\nplt.vlines(list(H2['Date'])[3::10], ymin, ymax, linestyle=':')\nplt.xlim(['2019-11-06','2020-10-12'])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=15)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=15)\nplt.title('Trump Attacks Political Adversaries',weight='bold',fontsize=15)\nplt.legend()\nsns.despine()\nplt.savefig('PLTlineplot1.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x='Date',y='Plans and Appealing Support',data=H2,style=True,markers=True,ci=None,color='g')\nplt.xticks(list(H2['Date'])[0::20])\nplt.ylim([0,10])\n#ymin, ymax = plt.ylim()\n#plt.vlines(list(H2['Date'])[5::8], ymin, ymax, linestyle='dashed')\nplt.yticks([0,2,4,6,8,10])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Talks About Plans and Appeals for Support',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot2_0.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x=H2.index[::-1],y=H2['Plans and Appealing Support'],style=True,markers=True,ci=None,color='g')\nlst = ['']*102\nlst[-6::-8] = list(H2['Date'][5::8])\nplt.xticks(ticks=range(1,103),labels=lst)\nplt.ylim([0,10])\nymin, ymax = plt.ylim()\nplt.vlines(np.arange(0,103,8), ymin, ymax, linestyle='dashed')\nplt.yticks([0,2,4,6,8,10])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Talks About Plans and Appeals for Support',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot2.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=1.2,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[9,6])\n#sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.plot(H3['Date'],H3['Plans and Appealing Support'],marker='.',markersize=10,linewidth=1.5,linestyle='-',color='g',label='Actual')\nplt.plot(predicted2['Date'],predicted2['Plans and Appealing Support'],marker='.',markersize=10,linewidth=2,linestyle='--',color='k',label='Predicted')\nplt.xticks(list(H2['Date'])[5::8])\nplt.ylim([0,10])\nymin, ymax = plt.ylim()\nplt.vlines(list(H2['Date'])[5::8], ymin, ymax, linestyle=':')\nplt.xlim(['2019-11-06','2020-10-12'])\nplt.yticks([0,2,4,6,8,10])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=15)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=15)\nplt.title('Trump Talks About Plans, Appeals for Support',weight='bold',fontsize=15)\nplt.legend()\nsns.despine()\nplt.savefig('PLTlineplot2.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x='Date',y='Achievements',data=H2,style=True,markers=True,ci=None)\nplt.xticks(list(H2['Date'])[0::20])\n#ymin, ymax = plt.ylim()\n#plt.vlines(list(H2['Date'])[8::9], ymin, ymax, linestyle='dashed')\nplt.ylabel('Achievements and Bragging')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Brags about Achievement and Progress',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot3_0.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x=H2.index[::-1],y=H2['Achievements'],style=True,markers=True,ci=None)\nlst = ['']*102\nlst[-9::-9] = list(H2['Date'][8::9])\nplt.xticks(ticks=range(1,103),labels=lst)\nymin, ymax = plt.ylim()\nplt.vlines(np.arange(3,102,9), ymin, ymax, linestyle='dashed')\nplt.ylabel('Achievements and Bragging')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Brags about Achievement and Progress',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot3.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=1.2,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[9,6])\n#sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.plot(H3['Date'],H3['Achievements'],marker='.',markersize=10,linewidth=1.5,linestyle='-',color='b',label='Actual')\nplt.plot(predicted2['Date'],predicted2['Achievements'],marker='.',markersize=10,linewidth=2,linestyle='--',color='k',label='Predicted')\nplt.xticks(list(H2['Date'])[8::9])\nymin, ymax = plt.ylim()\nplt.vlines(list(H2['Date'])[8::9], ymin, ymax, linestyle=':',)\nplt.xlim(['2019-11-06','2020-10-12'])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=15)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=15)\nplt.title('Trump Brags about Achievement and Progress',weight='bold',fontsize=15)\nplt.legend()\nsns.despine()\nplt.savefig('PLTlineplot3.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clustering of Trump Rallies\n- K-means for Clustering\n- Used Elbow method and Silhouette coefficients to determine optimum clusters\n- Used 3D plot in Plotly to visualize clusters","metadata":{}},{"cell_type":"code","source":"sns.set(style='white',font_scale=1)\ninertia = []\nfor num_clusters in range(1,11):\n    km = KMeans(n_clusters=num_clusters,random_state=71)\n    km.fit(H[['Achievements','Plans and Appealing Support','Political Adversaries']])\n    inertia.append(km.inertia_)\n    \nplt.plot(range(1,11),inertia,marker='x')\nplt.ylabel('Sum Inertia',fontsize=15,weight='bold')\nplt.xlabel('No. of Clusters',fontsize=15,weight='bold')\nsns.despine()\nplt.savefig('Elbow.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='white',font_scale=1)\nsilhouette = []\nfor num_clusters in range(2,11):\n    km = KMeans(n_clusters=num_clusters,random_state=71)\n    km.fit(H[['Achievements','Plans and Appealing Support','Political Adversaries']])\n    labels = km.labels_\n    silhouette.append(silhouette_score(H[['Achievements','Plans and Appealing Support','Political Adversaries']],labels=labels))\n    \nplt.plot(range(2,11),silhouette,marker='x',color='g')\nplt.ylabel('Silhouette Coefficient',fontsize=15,weight='bold')\nplt.xlabel('No. of Clusters',fontsize=15,weight='bold')\nsns.despine()\nplt.savefig('Silhouette.png',transparent=True, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km = KMeans(n_clusters=2,random_state=71)\nkm.fit(H[['Achievements','Plans and Appealing Support','Political Adversaries']])\nlabels = km.labels_\nlabels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.where(labels==0,'Trump Mainly Attacking','Trump Mainly Bragging')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = []\nclusters = []\ncolors = ['rgb(228,26,28)','rgb(55,126,184)'] # set our dot colors\n\nfor i in range(len(np.unique(labels))): # allows us to split our data into three distinct groups\n    name = np.unique(labels)[i]\n    color = colors[i]\n    x = H[ labels == name ]['Achievements']\n    y = H[ labels == name  ]['Plans and Appealing Support']\n    z = H[ labels == name  ]['Political Adversaries']\n    \n    trace = dict(  # trace is how we \"trace\" or draw our data on the canvas\n        name = name,\n        x = x, y = y, z = z,\n        type = \"scatter3d\",    \n        mode = 'markers',\n        marker = dict( size=2, color=color, line=dict(width=0) ) )\n    data1.append( trace )\n\nlayout = dict( # we modify our canvas here, including initial layout and styles\n    width=800,\n    height=550,\n    autosize=True,\n    title='Trump Rally Topic Clusters',\n    scene=dict(\n        xaxis=dict(\n            gridcolor='rgb(255, 255, 255)',\n            zerolinecolor='rgb(255, 255, 255)',\n            showbackground=True,\n            backgroundcolor='rgb(230, 230,230)',\n            title='Achievements and Bragging',  # set titles, very important\n            titlefont=dict(\n            family='Courier New',\n            size=9,\n            color='#2f2f2f'),  # we can use hex, rgba, or other color variants\n        ),\n        yaxis=dict(\n            gridcolor='rgb(255, 255, 255)',\n            zerolinecolor='rgb(255, 255, 255)',\n            showbackground=True,\n            backgroundcolor='rgb(230, 230,230)',\n            title='Plans and Appealing Support',  # set titles, very important\n            titlefont=dict(\n            family='Courier New',\n            size=9,\n            color='#4f4f4f'),\n        ),\n        zaxis=dict(\n            gridcolor='rgb(255, 255, 255)',\n            zerolinecolor='rgb(255, 255, 255)',\n            showbackground=True,\n            backgroundcolor='rgb(230, 230,230)',\n            title='Political Adversaries',  # set titles, very important\n            titlefont=dict(\n            family='Courier New',\n            size=9,\n            color='#7f7f7f'),\n        ),\n        aspectratio = dict( x=1, y=1, z=1 ), # we can compress large dimensions this way\n        aspectmode = 'manual'        \n    ),\n)\n\nfig = dict(data=data1, layout=layout) # this finally compiles our figure\n\n# run locally in notebook\niplot(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}