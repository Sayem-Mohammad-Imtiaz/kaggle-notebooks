{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-19T07:44:45.477532Z","iopub.execute_input":"2021-07-19T07:44:45.478375Z","iopub.status.idle":"2021-07-19T07:44:45.511311Z","shell.execute_reply.started":"2021-07-19T07:44:45.478202Z","shell.execute_reply":"2021-07-19T07:44:45.510018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:45.513041Z","iopub.execute_input":"2021-07-19T07:44:45.51343Z","iopub.status.idle":"2021-07-19T07:44:47.142109Z","shell.execute_reply.started":"2021-07-19T07:44:45.513395Z","shell.execute_reply":"2021-07-19T07:44:47.141252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.143678Z","iopub.execute_input":"2021-07-19T07:44:47.144096Z","iopub.status.idle":"2021-07-19T07:44:47.16502Z","shell.execute_reply.started":"2021-07-19T07:44:47.144065Z","shell.execute_reply":"2021-07-19T07:44:47.16423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.166553Z","iopub.execute_input":"2021-07-19T07:44:47.167033Z","iopub.status.idle":"2021-07-19T07:44:47.226226Z","shell.execute_reply.started":"2021-07-19T07:44:47.16699Z","shell.execute_reply":"2021-07-19T07:44:47.225102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.fillna(method='ffill', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.227618Z","iopub.execute_input":"2021-07-19T07:44:47.228273Z","iopub.status.idle":"2021-07-19T07:44:47.237633Z","shell.execute_reply.started":"2021-07-19T07:44:47.228027Z","shell.execute_reply":"2021-07-19T07:44:47.236645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.2389Z","iopub.execute_input":"2021-07-19T07:44:47.239218Z","iopub.status.idle":"2021-07-19T07:44:47.271951Z","shell.execute_reply.started":"2021-07-19T07:44:47.239186Z","shell.execute_reply":"2021-07-19T07:44:47.270839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.273341Z","iopub.execute_input":"2021-07-19T07:44:47.273654Z","iopub.status.idle":"2021-07-19T07:44:47.315259Z","shell.execute_reply.started":"2021-07-19T07:44:47.273618Z","shell.execute_reply":"2021-07-19T07:44:47.314174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####Droping serial number as it is all unique value and wont contribute to the model.\ndf1.drop(['sl_no'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.318448Z","iopub.execute_input":"2021-07-19T07:44:47.318847Z","iopub.status.idle":"2021-07-19T07:44:47.325376Z","shell.execute_reply.started":"2021-07-19T07:44:47.318814Z","shell.execute_reply":"2021-07-19T07:44:47.324159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Ploting the distribution of continious values\n###As we can see some the features are not normally distributed, we need to treat the skew.Power Transform in sklearn are created for this purpose.\n\nfor i in df1.columns:\n    if df1[i].dtype !='object':\n        sns.histplot(df1[i], kde=True)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:47.329393Z","iopub.execute_input":"2021-07-19T07:44:47.330478Z","iopub.status.idle":"2021-07-19T07:44:48.834411Z","shell.execute_reply.started":"2021-07-19T07:44:47.330402Z","shell.execute_reply":"2021-07-19T07:44:48.833133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\nfor i in df1.columns:\n    if df1[i].dtype !='object' and i != 'salary':\n        pw=PowerTransformer()\n        df1[i]=pw.fit_transform(df1[i].values.reshape(-1,1))\n        sns.histplot(df1[i], kde=True)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:48.83553Z","iopub.execute_input":"2021-07-19T07:44:48.835816Z","iopub.status.idle":"2021-07-19T07:44:49.809156Z","shell.execute_reply.started":"2021-07-19T07:44:48.835788Z","shell.execute_reply":"2021-07-19T07:44:49.807786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###As the salary column is highly skewed hence going for discretization to contain outliers and remove noise\nfrom sklearn.preprocessing import KBinsDiscretizer\nkb=KBinsDiscretizer(n_bins=10, encode='ordinal',strategy=\"quantile\")\ndf1['salary']=kb.fit_transform(df1.salary.values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.810926Z","iopub.execute_input":"2021-07-19T07:44:49.811423Z","iopub.status.idle":"2021-07-19T07:44:49.822333Z","shell.execute_reply.started":"2021-07-19T07:44:49.811377Z","shell.execute_reply":"2021-07-19T07:44:49.820992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####Categorical values encoding####\n###Checking the frequency distribution of the object dtype features\n\nfor i in df1.columns:\n    if df1[i].dtype =='object':\n        print(i)\n        print(df1[i].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.824032Z","iopub.execute_input":"2021-07-19T07:44:49.824567Z","iopub.status.idle":"2021-07-19T07:44:49.848235Z","shell.execute_reply.started":"2021-07-19T07:44:49.824478Z","shell.execute_reply":"2021-07-19T07:44:49.847183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trying different categorical encoding techniques","metadata":{}},{"cell_type":"code","source":"####1)trying out frequency encoding for categorical feature transformation\n\n#for i in df1.columns:\n    #if df1[i].dtype =='object' and i !='status':\n    #    vl_1=df1.groupby(i).size()/len(df1)\n    #    df1[i]=df1[i].map(vl_1)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.84978Z","iopub.execute_input":"2021-07-19T07:44:49.850194Z","iopub.status.idle":"2021-07-19T07:44:49.85465Z","shell.execute_reply.started":"2021-07-19T07:44:49.850147Z","shell.execute_reply":"2021-07-19T07:44:49.853691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#####2)trying out one hot encoding for categorical feature transformation\n\n#df1=pd.get_dummies(df1, columns=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'], drop_first=True, prefix=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'])","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.85568Z","iopub.execute_input":"2021-07-19T07:44:49.855936Z","iopub.status.idle":"2021-07-19T07:44:49.8701Z","shell.execute_reply.started":"2021-07-19T07:44:49.855911Z","shell.execute_reply":"2021-07-19T07:44:49.868834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######3)Trying mean encoding for categorical feature transformation\n#for i in df1.columns:\n#    if df1[i].dtype =='object' and i !='status':\n#        vl_1=df1.groupby(i).status.count()/len(df1)\n#        df1[i]=df1[i].map(vl_1)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.87204Z","iopub.execute_input":"2021-07-19T07:44:49.87252Z","iopub.status.idle":"2021-07-19T07:44:49.882278Z","shell.execute_reply.started":"2021-07-19T07:44:49.872475Z","shell.execute_reply":"2021-07-19T07:44:49.881474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.883616Z","iopub.execute_input":"2021-07-19T07:44:49.884022Z","iopub.status.idle":"2021-07-19T07:44:49.895541Z","shell.execute_reply.started":"2021-07-19T07:44:49.883992Z","shell.execute_reply":"2021-07-19T07:44:49.894336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_fit(df1, str1, enc_type):\n    df2=df1.copy(deep=True)\n    if enc_type =='frequency_encoding':\n        for i in df2.columns:\n            if df2[i].dtype =='object' and i !='status':\n                vl_1=df2.groupby(i).size()/len(df2)\n                df2[i]=df2[i].map(vl_1)\n    elif enc_type =='mean_encoding':\n        for i in df2.columns:\n            if df2[i].dtype =='object' and i !='status':\n                vl_1=df2.groupby(i).status.count()/len(df2)\n                df2[i]=df2[i].map(vl_1)\n    else:\n        df2=pd.get_dummies(df2, columns=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'], drop_first=True, prefix=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'])\n        \n    X=df2.drop('status', axis=1)\n    y=df2.status\n    ####Removing Multicolinearity \n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    vif=pd.DataFrame()\n    vif['columum_values']=X.columns.tolist()\n    lst_1=[]\n    for i in range(0,len(X.columns)):\n        vl_1=variance_inflation_factor(X.values, i)\n        lst_1.append(vl_1)\n    vif['VIF_value']=lst_1\n    lst_2=vif[vif.VIF_value >5].columum_values.tolist()\n    df2=df2.drop(lst_2, axis=1)\n    lg=LogisticRegression()\n    rf=RandomForestClassifier()\n    xg=XGBClassifier()\n    nb=GaussianNB()\n    lb=LabelEncoder()\n    target_names=df2.status.unique().tolist()\n    target=lb.fit_transform(df2.status)\n    df3=df2.drop('status', axis=1)\n    X_train,X_test,y_train,y_test=train_test_split(df3,target, test_size=0.2,stratify=target)\n    print(str1)\n    for i in [lg,rf,xg,nb]:\n        i.fit(X_train,y_train)\n        y_pred=i.predict(X_test)\n        print(i)\n        print(classification_report(y_test, y_pred, target_names=target_names))\n        print('=================================')\n        pred_prob1 = i.predict_proba(X_test)\n        fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\n        df3=pd.DataFrame({'Fale_Postive_Rate': fpr1, 'True_Positive_Rate': tpr1, 'Threshold': thresh1})\n        print(df3)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.896918Z","iopub.execute_input":"2021-07-19T07:44:49.897367Z","iopub.status.idle":"2021-07-19T07:44:49.915656Z","shell.execute_reply.started":"2021-07-19T07:44:49.897335Z","shell.execute_reply":"2021-07-19T07:44:49.914598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str1='Creating models with frequency encoding'\nmodel_fit(df1, str1,enc_type='frequency_encoding')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:49.916905Z","iopub.execute_input":"2021-07-19T07:44:49.917223Z","iopub.status.idle":"2021-07-19T07:44:50.472872Z","shell.execute_reply.started":"2021-07-19T07:44:49.917192Z","shell.execute_reply":"2021-07-19T07:44:50.472045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str1='Creating models with mean encoding'\nmodel_fit(df1, str1,enc_type='mean_encoding')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:50.474219Z","iopub.execute_input":"2021-07-19T07:44:50.474752Z","iopub.status.idle":"2021-07-19T07:44:50.862653Z","shell.execute_reply.started":"2021-07-19T07:44:50.474704Z","shell.execute_reply":"2021-07-19T07:44:50.861517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str1='Creating models with onehot encoding'\nmodel_fit(df1, str1,enc_type='onehot_encoding')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:44:50.863853Z","iopub.execute_input":"2021-07-19T07:44:50.864227Z","iopub.status.idle":"2021-07-19T07:44:51.253195Z","shell.execute_reply.started":"2021-07-19T07:44:50.864191Z","shell.execute_reply":"2021-07-19T07:44:51.25201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}