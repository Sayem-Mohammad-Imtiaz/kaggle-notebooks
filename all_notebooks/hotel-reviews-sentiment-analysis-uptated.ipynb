{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Importing Important Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk import ngrams\n# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Modeling packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making two copies of Reviews to edit"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Edits After Removing Stopwords\nEdited_Review = data['Review'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Having a look at 1st ten reviews in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to preprocess the hotel data\ndef preprocess_hotel_data(data,name):\n    # Proprocessing the data\n    data[name]=data[name].str.lower()\n    # Code to remove the Hashtags from the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    # Code to remove the links from the text\n    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Code to remove the Special characters from the text \n    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n    # Code to substitute the multiple spaces with single spaces\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n    # Code to remove all the single characters in the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n    # Remove the twitter handlers\n    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Function to tokenize and remove the stopwords    \ndef rem_stopwords_tokenize(data,name):\n      \n    def getting(sen):\n        example_sent = sen\n        \n        filtered_sentence = [] \n\n        stop_words = set(stopwords.words('english')) \n\n        word_tokens = word_tokenize(example_sent) \n        \n        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n        \n        return filtered_sentence\n    # Using \"getting(sen)\" function to append edited sentence to data\n    x=[]\n    for i in data[name].values:\n        x.append(getting(i))\n    data[name]=x\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatization Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef Lemmatization(data,name):\n    def getting2(sen):\n        \n        example = sen\n        output_sentence =[]\n        word_tokens2 = word_tokenize(example)\n        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n        \n        # Remove characters which have length less than 2  \n        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n        # Remove numbers\n        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n        \n        return cleaned_data_title\n    # Using \"getting2(sen)\" function to append edited sentence to data\n    x=[]\n    for i in data[name].values:\n        x.append(getting2(i))\n    data[name]=x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Converting all the texts back to sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_sentences(data,name):\n    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n    # Removing double spaces if created\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the Lemmatization function to lemmatize the hotel data\ndata['Review_without_stopwords'] = Edited_Review\n\n# Using the preprocessing function to preprocess the hotel data\npreprocess_hotel_data(data,'Review_without_stopwords')\n# Using tokenizer and removing the stopwords\nrem_stopwords_tokenize(data,'Review_without_stopwords')\n# Converting all the texts back to sentences\nmake_sentences(data,'Review_without_stopwords')\n\n\n#Edits After Lemmatization\nfinal_Edit = data['Review_without_stopwords'].copy()\ndata[\"After_lemmatization\"] = final_Edit\n\nLemmatization(data,'After_lemmatization')\n# Converting all the texts back to sentences\nmake_sentences(data,'After_lemmatization')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results of Preprocessing data (Removing stopwords & Lemmatization)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"- Old Review -\")\nprint(data['Review'][3])\nprint(\"\\n- New Review -\")\nprint(data['Review_without_stopwords'][3])\nprint(\"\\n- Last Edit Review -\")\nprint(data['After_lemmatization'][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['reviews_text_new'] = data['After_lemmatization'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapping the ratings\ndata['Sentiment_rating'] = np.where(data.Rating > 3,1,0)\n\n## Removing neutral reviews \ndata = data[data.Rating != 3]\n\n# Printing the counts of each class\ndata['Sentiment_rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['reviews_text_new','Rating','Sentiment_rating']].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a machine learning model"},{"metadata":{},"cell_type":"markdown","source":"# Bag-of-words"},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = CountVectorizer()\nX = vec.fit_transform(data['reviews_text_new'])\ndf = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer()\nvect.fit(data['reviews_text_new'])\nvect.get_feature_names()\n# transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(data['reviews_text_new'])\nprint(simple_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating a python object of the class CountVectorizer\nnoise_words = []\nbow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n                             stop_words=noise_words, # List of stopwords\n                             ngram_range=(1,1)) # number of n-grams\n\nbow_data = bow_counts.fit_transform(data['reviews_text_new'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Divide into training and test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n                                                                    data['Sentiment_rating'], # Target variable\n                                                                    test_size = 0.2, # 20% test size\n                                                                    random_state = 0) # random state for replication purposes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training the model \nlr_model_all = LogisticRegression() # Logistic regression\nlr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model\n\n## Predicting the output\ntest_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction\n\n## Calculate key performance metrics\n\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Changes with respect to the previous code\n### 1. Increasing the n-grams from just having 1-gram to (1-gram, 2-gram, 3-gram, and 4-gram)\n### 2. Including the stopwords in the bag of words features\n\nbow_counts = CountVectorizer(tokenizer= word_tokenize,\n                             ngram_range=(1,4))\n\nbow_data = bow_counts.fit_transform(data.reviews_text_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notice the increase in features with inclusion of n-grams\nbow_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data,\n                                                                    data['Sentiment_rating'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining and training the model\nlr_model_all_new = LogisticRegression(max_iter = 200)\nlr_model_all_new.fit(X_train_bow, y_train_bow)\n\n# Predicting the results\ntest_pred_lr_all = lr_model_all_new.predict(X_test_bow)\n\n\n## Calculate key performance metrics\n\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n### Creating a python object of the class CountVectorizer\ntfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n                               stop_words=noise_words, # List of stopwords\n                               ngram_range=(1,1)) # number of n-grams\n\ntfidf_data = tfidf_counts.fit_transform(data['reviews_text_new'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,\n                                                                            data['Sentiment_rating'],\n                                                                            test_size = 0.2,\n                                                                            random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Setting up the model class\nlr_model_tf_idf = LogisticRegression()\n\n## Training the model \nlr_model_tf_idf.fit(X_train_tfidf,y_train_tfidf)\n\n## Prediciting the results\ntest_pred_lr_all = lr_model_tf_idf.predict(X_test_tfidf)\n\n## Calculate key performance metrics\n\n\n# Print a classification report\nprint(classification_report(y_test_tfidf,test_pred_lr_all))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}