{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install lazypredict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport inspect # Debugging","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data \ndata    = pd.read_csv(\"../input/company-bankruptcy-prediction/data.csv\")\ncolumns = data.columns.values\n\n# No missing data\n# data.isna().any()\n\n# Strongly unbalanced dataset (w/ respect to target)\ndata[\"Bankrupt?\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation\n\nm, n = data.shape\nmean = data.mean().values             # Mean value for each column\nstd  = data.std().values * np.sqrt(m) # Std values for each column (* num samples)\ncorr = ((data.values - mean) * (data.values[:,0] - mean[0]).reshape(m, 1)).sum(axis=0) / (std * std[0])\n\nfig, corr_ax = plt.subplots(figsize=(24, 8))\ncorr_ax.bar(x=np.arange(len(columns)), height=corr)\ncorr_ax.set_title(\"Correlation with target ('Bankrupt')\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing   import StandardScaler\n\ntrain  = data.drop(columns=[\"Bankrupt?\"]).values\ntarget = data[\"Bankrupt?\"].values\n\nscaler = StandardScaler()\ntrain  = scaler.fit_transform(train)\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\nfor train_index, test_index in sss.split(train, target):\n    x_train, y_train = train[train_index, :], target[train_index]\n    x_test, y_test = train[test_index, :], target[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick models testing\n\nfrom lazypredict.Supervised import LazyClassifier\n\nfrom sklearn.metrics import recall_score, accuracy_score\n\nlazy_clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=recall_score)\nmodels, predictions = lazy_clf.fit(x_train, x_test, y_train, y_test)\n\nprint(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring deeper the models suggested above\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics         import make_scorer, recall_score, accuracy_score\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors   import NearestCentroid\n\nestimators = {\"BernoulliNB\"   : {\"func\"  : BernoulliNB(),\n                                 \"params\": {\"alpha\"    : [0.1, 0.5, 1, 1.5, 2],\n                                            \"fit_prior\": [True, False]}},\n             \"NearestCentroid\": {\"func\"  : NearestCentroid(),\n                                 \"params\": {\"metric\"          : [\"euclidean\", \"manhattan\"],\n                                            \"shrink_threshold\": [None, 0.1, 0.5, 1, 2]}}                   \n             }\n\nmodels_to_test = estimators.keys()\nfor estimator_name in models_to_test:\n    model = GridSearchCV(estimator=estimators[estimator_name][\"func\"],\n                         param_grid=estimators[estimator_name][\"params\"],\n                         scoring=make_scorer(recall_score))\n    model.fit(x_train, y_train)\n    preds    = model.predict(x_test)\n    recall   = recall_score(y_test, preds)\n    accuracy = accuracy_score(y_test, preds)\n    print(\"{}: \\n ACCURACY: {:.2f}, \\n RECALL: {:.2f}, \\n BEST PARAMS: {} \\n\".format(estimator_name, accuracy, recall, model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix\n\nfrom sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(model, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nnormalized_data = MinMaxScaler().fit_transform(data.values)\nnormalized_data = pd.DataFrame(normalized_data, columns=columns)\n\n'''\nComparison of each feature mean values for bankrupted (and non) companies. \nThere is no significative difference between the two cases, both in\nmean and in dispersion.\n\n'''\npos_mean = normalized_data.loc[data[\"Bankrupt?\"] == 1].mean()\npos_std  = normalized_data.loc[data[\"Bankrupt?\"] == 1].std()\nneg_mean = normalized_data.loc[data[\"Bankrupt?\"] == 0].mean()\nneg_std  = normalized_data.loc[data[\"Bankrupt?\"] == 0].std()\n\navg_diff = np.sqrt(np.square(pos_mean - neg_mean).mean())\n\nfig, mean_ax = plt.subplots(figsize=(24, 8))\nmean_ax.bar(x=np.arange(len(columns)), height=pos_mean, yerr=pos_std)\nmean_ax.bar(x=np.arange(len(columns)), height=-neg_mean, yerr=neg_std)\nmean_ax.set_xticks([])\nmean_ax.set_title(\"MinMax scaled features, mean & std comparison (Avg. mean difference {:.3f})\".format(avg_diff))\n\nplt.plot()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}