{"cells":[{"metadata":{"_cell_guid":"659fc7c9-7801-4836-b176-c9aca1fb5b1e","_uuid":"640a111a44342ee63da8211b1d54a29f9a659406","collapsed":true},"cell_type":"markdown","source":"## Summary\n\n* Extract features for answers from both questions and answers\n* Both shallow (statistical or numerical) features, e.g., content length, and semantic features, e.g., question-answer similarity are considered\n* Question-based accuracy (i.e., the percentage that whether the question is correctly answered) is used as evaluation, instead of answer-based accuracy (i.e., if the answer is correctly classified as the best or not)\n* The best answer for a question is defined as the answer with the highest score (most upvotes), becuase we don't have information about user-selected answers in the dataset\n* First, last, and the longest answers serve as baselines\n* Use simple Random Forest to achieve 0.4722 question-based accuracy, improving baselines by 0.075"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport pickle\nfrom bs4 import BeautifulSoup\n\n# for training and prediction\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestClassifier\n\n# for calculating similarities among text\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom gensim import corpora, models, similarities","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"22ee268b-3c6f-43ac-9a21-fd109d5f009f","_uuid":"66f808c651589855a5225c3f63baa25e52041268","trusted":true},"cell_type":"code","source":"# dataset and pre-stored object files\nprint(os.listdir(\"../input/pythonquestions\"))\nprint(os.listdir(\"../input/object-files-for-computed-matrices\"))","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"1b571abc-5543-4c6e-ba45-ded75b4539af","_uuid":"00c1c5415b84cc7b92edb10f1b747f2fa55d6bb8","collapsed":true,"trusted":true},"cell_type":"code","source":"# Functions for save and load derived data objects\ndef save_obj(obj, fpath):\n    with open(fpath, 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\ndef load_obj(fpath):\n    if os.path.exists(fpath):\n        with open(fpath, 'rb') as f:\n            return pickle.load(f)\n    else:\n        return None","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"9d73021ea7f95794aabf254905dd72e50fa01139"},"cell_type":"markdown","source":"## Data preprocessing"},{"metadata":{"_cell_guid":"b7adb9af-8b2c-4fb0-9798-ea4a09ea6556","_uuid":"4e3ee8a96ece5e232e63360e0476268ae7a77b84","collapsed":true,"trusted":true},"cell_type":"code","source":"# load the data\ncol_types = {'Id': 'int', \n             'OwnerUserId': 'float', \n             'CreationDate': 'str', \n             'ParentId': 'int', \n             'Score': 'int',\n             'Title': 'str',\n             'Body':'str'}\n\nquestions = pd.read_csv('../input/pythonquestions/Questions.csv', encoding = \"ISO-8859-1\", dtype=col_types)\nanswers = pd.read_csv('../input/pythonquestions/Answers.csv', encoding = \"ISO-8859-1\", dtype=col_types)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"e002a23d-0985-4d87-800f-ff6356dd813a","_uuid":"54130111670643d4f807f97fb7d466d6893f9dd9","collapsed":true,"trusted":true},"cell_type":"code","source":"# question to answer: (ans_id, score, owner_id)\nq_to_a = load_obj('../input/object-files-for-computed-matrices/q_to_a.pkl')\nif not q_to_a:\n    q_to_a = dict()\n    for _, row in answers[['Id', 'ParentId', 'Score', 'OwnerUserId']].iterrows():\n        q_id = row['ParentId']\n        a_id = row['Id']\n        a_score = row['Score']\n        a_owner_id = row['OwnerUserId'] if not np.isnan(row['OwnerUserId']) else None\n        if q_id not in q_to_a:\n            q_to_a[q_id] = [(a_id, a_score, a_owner_id)]\n        else:\n            q_to_a[q_id].append((a_id, a_score, a_owner_id))\n            \n    save_obj(q_to_a, 'obj/q_to_a.pkl')\n\n# Keep only the questions with 4-10 answers and a distinguishable answer\nq_to_a = {k:v for k, v in q_to_a.items() if len(v)>3 and len(v)<11 and max(v, key=lambda x: x[1])[1]>0}\n    \n# keep only qualified questions\nquestions = questions[questions['Id'].isin(q_to_a)]","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"c4dbfbd0-756d-4299-a047-450c64cded5c","_uuid":"f42f958e0c71fa518fbe0cd729f7c73d5488199e","collapsed":true,"trusted":true},"cell_type":"code","source":"# Keep only answers related to a qualified question\ndef answer_in_use(answer):\n    if answer['ParentId'] in q_to_a:\n        for item in q_to_a[answer['ParentId']]:\n            if answer['Id'] == item[0]:\n                return True\n    return False\n\nanswers = answers[answers.apply(lambda x: answer_in_use(x), axis=1)]","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"4f13db21d44e90f91949a238b4349451c292104a"},"cell_type":"markdown","source":"## We retrieved 44184 questions with at least four and at most ten answers from the dataset, and made sure that the best answer can be determined from the candidates"},{"metadata":{"_cell_guid":"3986364d-8acd-49f9-8ae0-83e76f71f4bd","_uuid":"76d8ba2538f32643c8dd494bec602b73fed85695","trusted":true},"cell_type":"code","source":"questions.info()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"d7767d88-b5b4-49d0-9fc9-a2f8baef9c22","_uuid":"41243cb9b7d6cfeb33e820c105a258076160b272","trusted":true},"cell_type":"code","source":"answers.info()","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"b947b7f0-43aa-44c4-b637-9b2245c8b284","_uuid":"8f3063b1aaa54554833059b5775cd1592fc3c752"},"cell_type":"markdown","source":"## Feature engieering. Refer to [this paper](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/view/6096) for extracted features"},{"metadata":{"_cell_guid":"e4135548-f13f-4a03-9748-76da4dc87906","_uuid":"3992a4b3bbb1ed23677b19a9200b2a3da52e3a4b"},"cell_type":"markdown","source":"## Shallow features: numerical or statistical values derived from the content\n* TotalLength: The length (word counting) of the content, including the text and code snippets\n* LinkCount: The number of hyperlinks in the answer\n* CodeLength: Word counting of the code snippets\n* PostOrder: The chronological order of the answer. First-posted answer is set as 1, second answer is 2, and so on\n* Reputation: The reputation score of the person writing the answer. The score of a user is computed by aggregating all upvotes she got from all the answers (in the training dataset)"},{"metadata":{"_cell_guid":"3399961b-8ce1-470f-843d-b7e891b99c3f","_uuid":"d176c7e0968cb859cd8a1ec5353a7845b7c5bd30","collapsed":true,"trusted":true},"cell_type":"code","source":"def strip_code(html):\n    bs = BeautifulSoup(html, 'html.parser')\n    [s.extract() for s in bs('code')]\n    return bs.get_text()\n\nanswers = answers.assign(BodyEnglishText = answers['Body'].apply(strip_code))\nanswers = answers.assign(EnglishCount = answers['BodyEnglishText'].apply(lambda x: len(x.split())))","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"297e6966-f95f-4b40-a811-3469100cd557","_uuid":"f6a89710e32303317e4cc2ef9981393887e4656e","collapsed":true,"trusted":true},"cell_type":"code","source":"def count_links(html):\n    bs = BeautifulSoup(html, 'html.parser')\n    return len(bs.find_all('a'))\n\nanswers = answers.assign(LinkCount = answers['Body'].apply(count_links))","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"c7fd55e0-2aff-46b1-b348-e98863f07bc0","_uuid":"c1c72b10c4f13099984660715a1200101bf66d24","collapsed":true,"trusted":true},"cell_type":"code","source":"def code_length(html):\n    bs = BeautifulSoup(html, 'html.parser')\n    contents = [tag.text.split() for tag in bs.find_all('code')]\n    return sum(len(item) for item in contents)\n\nanswers = answers.assign(CodeLength = answers['Body'].apply(code_length))\nanswers = answers.assign(TotalLength = answers.apply(lambda row: row['EnglishCount'] + row['CodeLength'], axis=1))","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"f1c2a094-9121-4f86-a3e9-90e14b95ebe7","_uuid":"6b244a5f943ced5d8253407ef935b52142edcb43","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_order(qid, aid):\n    ids = [i[0] for i in q_to_a[qid]]\n    return ids.index(aid)+1\n\nanswers = answers.assign(PostOrder = answers.apply(lambda row: get_order(row['ParentId'], row['Id']), axis=1))","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"223537fc-ea37-4382-8999-b0c8b3a66984","_uuid":"a268be52510f22910888a89849759978fcfc7036","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculate reputation scores for users\nuser_rep = dict()\nfor ans_list in q_to_a.values():\n    for _, score, owner_id in ans_list:\n        if owner_id:\n            if owner_id in user_rep:\n                user_rep[owner_id] += score\n            else:\n                user_rep[owner_id] = score","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"47dc213d-9e7f-442f-a984-45ae5216a5bd","_uuid":"dc7b1f593d44e60549983957ee5968841537972a","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_reputation(userId):\n    if not np.isnan(userId) and userId in user_rep:\n        return user_rep[userId]\n    else:\n        return 0\n\nanswers = answers.assign(Reputation = answers['OwnerUserId'].apply(get_reputation))","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"883bd977-8742-4815-8708-e27d18604a2d","_uuid":"e2bdec020446eb6b4a1109e19aa70c9ce30171c0","trusted":true},"cell_type":"code","source":"answers.head(3)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"6b7cab76-67bc-4619-acf1-07d55a73f806","_uuid":"23a2c63614bc4c350c39ce7b0a115e94115c4c00"},"cell_type":"markdown","source":"## Semantic features: Question-Ans similarity and Ans-Ans similarity\nCosine similarity between two documents is computed by a series of transformations: \n* bag-of-words\n* tf-idf (term frequency-inverse document frequency)\n* LSA (Latent Semantic Analysis, or Latent Semantic Indexing)\n\n### Similarity scores computed\n* SimToQ: The similarity between an answer and the question\n* MaxSimToA: The maximal similarity between an answer and other answers of the original question\n* MinSimToA: The minimal similarity between an answer and other answers of the original question"},{"metadata":{"_cell_guid":"c5c6be09-9533-4cab-bb4e-d2e95d6b4157","_uuid":"e15743d9ec2412a83fb94f4dd2956c87e7c11c19"},"cell_type":"markdown","source":"### (Note: the following `compute_sim()` function took 1.5 hr to run on my Core-i7-7700 / 32GB RAM laptop)"},{"metadata":{"_cell_guid":"13b53742-08e8-459a-9a07-4589f6d4af45","_uuid":"26194a0edf9e95d229d519ca2c16f05e3a6959a0","collapsed":true,"trusted":true},"cell_type":"code","source":"def compute_sim(q_to_a, df_questions, df_answers):\n    a_sim = dict()\n    tokenizer = RegexpTokenizer(r'\\w+')\n    print(len(q_to_a))\n    c = 0\n    for q_id, a_list in q_to_a.items():\n        # show progress\n        c+=1\n        print(str(c) + ' ' + str(len(a_list)), end='\\r')\n        \n        # get split body text for a question and the answers\n        q_body = df_questions[df_questions['Id']==q_id].iloc[0]['Body']\n        q_body = BeautifulSoup(q_body, 'html.parser').get_text()#.split()\n        q_body = tokenizer.tokenize(q_body.lower())\n        q_body = [w for w in q_body if w not in stopwords.words('english')]\n        \n        a_bodies = list()\n        for a_id, _, _ in a_list:\n            a_body = df_answers[df_answers['Id']==a_id].iloc[0]['Body']\n            a_body = BeautifulSoup(a_body, 'html.parser').get_text()#.split()\n            a_body = tokenizer.tokenize(a_body.lower())\n            a_body = [w for w in a_body if w not in stopwords.words('english')]\n            a_bodies.append(a_body)\n        \n        # apply a series of transformations to the answers: bag-of-word, tf-idf, and lsi\n        dictionary = corpora.Dictionary(a_bodies)\n        corpus = [dictionary.doc2bow(a) for a in a_bodies]\n        tfidf = models.TfidfModel(corpus)\n        corpus_tfidf = tfidf[corpus]\n        lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=4)\n        corpus_lsi = lsi[corpus_tfidf]\n        \n        index = similarities.MatrixSimilarity(corpus_lsi)\n        \n        # question-to-answer similarity\n        q_bow = dictionary.doc2bow(q_body)\n        q_lsi = lsi[q_bow]\n        q_to_a_sim = index[q_lsi]\n        \n        # ans-to-ans similarity, excluding the answer itself\n        for idx, a_lsi in enumerate(corpus_lsi):\n            a_to_a_sim = index[a_lsi]\n            a_to_a_sim = [a_to_a_sim[i] for i in range(len(a_to_a_sim)) if i != idx]  # exclude itself\n            \n            # construct the dictionary a_sim\n            a_id = a_list[idx][0]\n            sim_to_q = q_to_a_sim[idx]\n            max_sim_to_a = max(a_to_a_sim)\n            min_sim_to_a = min(a_to_a_sim)\n            a_sim[a_id] = (sim_to_q, max_sim_to_a, min_sim_to_a)\n    \n    return a_sim\n\n# a_sim: a_id -> (sim_to_question, max_sim_to_other_ans, min_sim_to_other_ans)\na_sim = load_obj('../input/object-files-for-computed-matrices/a_sim.pkl')\nif not a_sim:\n    a_sim = compute_sim(q_to_a, questions, answers)\n    save_obj(a_sim, '../input/object-files-for-computed-matrices/a_sim.pkl')","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"2a55d3ba-097f-40b3-ae5f-9e29898d23df","_uuid":"f47ec99bda1eeb7f46f6864c6d8ff5384ed6faf7","trusted":true},"cell_type":"code","source":"answers = answers.assign(SimToQ = answers['Id'].apply(lambda a_id: a_sim[a_id][0]))\nanswers = answers.assign(MaxSimToA = answers['Id'].apply(lambda a_id: a_sim[a_id][1]))\nanswers = answers.assign(MinSimToA = answers['Id'].apply(lambda a_id: a_sim[a_id][2]))\nanswers.head(3)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"7eebe485-e46a-4614-927a-300fe1c4da3c","_uuid":"284b6ea8048c7fd0f8af99cda7e2cc8e85a3b6a1"},"cell_type":"markdown","source":"## Column for the best answer"},{"metadata":{"_cell_guid":"57627942-742b-498e-99bc-2e09172b4853","_uuid":"c1e3a5b67f72390a8991675092d1f32b305b5d1a","trusted":true},"cell_type":"code","source":"def is_answer(qid, aid):\n    if aid == max(q_to_a[qid], key=lambda item: item[1])[0]:\n        return 1\n    else:\n        return 0\n    \nanswers = answers.assign(IsAnswer = answers.apply(lambda row: is_answer(row['ParentId'], row['Id']), axis=1))\nanswers.head(3)","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"6add09ff-011c-4397-82b6-cb1bcbc243e0","_uuid":"25dd3bcd6095394633a599ac73f56c369931da2e"},"cell_type":"markdown","source":"## Split the data and train with RandomForest\n(Should apply cross validation but skip for simplicity)"},{"metadata":{"_cell_guid":"d375bbe7-6dfa-4712-ac44-dd61734ac120","_uuid":"e595e2d5e52315d6e9751121b1c7a98e7ac61de9","collapsed":true,"trusted":true},"cell_type":"code","source":"q_train, q_test = model_selection.train_test_split(questions, test_size=0.2, random_state=42)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"a97f9315-d6d0-45a7-983c-7724f23197cd","_uuid":"8f1c0ef1e9a867c95ca5153184cbdeef4c306a14","collapsed":true,"trusted":true},"cell_type":"code","source":"a_train = answers[answers['ParentId'].isin(q_train['Id'])]\na_test = answers[answers['ParentId'].isin(q_test['Id'])]","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"e846c7d7-f47d-41c3-943e-e63e55e2fb1e","_uuid":"c2e116a880ab935277428b4b739d9bdeeeaeec53","collapsed":true,"trusted":true},"cell_type":"code","source":"x_train = a_train[['TotalLength','LinkCount', 'CodeLength', 'PostOrder', 'Reputation', 'SimToQ', 'MaxSimToA', 'MinSimToA']]\ny_train = a_train[['IsAnswer']]\nx_test = a_test[['Id', 'TotalLength','LinkCount', 'CodeLength', 'PostOrder', 'Reputation', 'SimToQ', 'MaxSimToA', 'MinSimToA']]\ny_test = a_test[['IsAnswer']]","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"cb639fe9-fd8b-4f72-a26c-7290451c33b8","_uuid":"7cc3a7510bef48c709a45969310e215fb3912f8b","trusted":true},"cell_type":"code","source":"rf_classifier = RandomForestClassifier(\n    n_estimators=1000, min_samples_leaf=4, n_jobs=-1, oob_score=True, random_state=42)\n\nrf_classifier.fit(x_train, y_train.values.ravel())","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"080a860a-5c00-4b68-a72c-4fddf68fb2a8","_uuid":"7ea82d1a24acec5453560bc354793a12e8b4ac54","collapsed":true,"trusted":true},"cell_type":"code","source":"y_pred = rf_classifier.predict_proba(x_test.iloc[:, 1:])","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"295bb088-29d5-4a27-830d-805226ae524e","_uuid":"98477c2153efae8dfcfeb69ad413eec6ccebdbb9","trusted":true},"cell_type":"code","source":"# question-based accuracy, i.e., the percentage that whether the question is correctly answered\ndef get_accuracy(q_ids, a_ids, prob_pred):\n    a_to_prob = dict()\n    for idx, a_id in enumerate(a_ids):\n        prob = prob_pred[:, 1][idx]\n        a_to_prob[a_id] = prob\n        \n    count = 0\n    for q_id in q_ids:\n        right_answer = max(q_to_a[q_id], key=lambda item: item[1])[0]\n        predict_answer = 0\n        highest_score = 0\n        for a_id, score, _ in q_to_a[q_id]:\n            pred_score = a_to_prob[a_id]\n            if pred_score > highest_score:\n                predict_answer = a_id\n                highest_score = pred_score\n        if right_answer==predict_answer:\n            count += 1\n    return count/len(q_ids)\n\nprint('Random Forest accuracy:', get_accuracy(q_test['Id'].tolist(), a_test['Id'].tolist(), y_pred))","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"0213f06d-d6a7-4cda-ad30-58789cdbc8e0","_uuid":"f20ff8b9ffd57ef68086084ae39905683e99c674"},"cell_type":"markdown","source":"## Baselines"},{"metadata":{"_cell_guid":"690948f7-94ea-4b0c-9dff-3cd8248b027e","_uuid":"1fb8886007300f174271a42834e5a7ed8f585021","trusted":true},"cell_type":"code","source":"def baseline(q_ids, a_val):\n    count_first = 0\n    count_last = 0\n    count_long = 0\n    for q_id in q_ids:\n        right_answer = max(q_to_a[q_id], key=lambda item: item[1])[0]\n        \n        first_answer = q_to_a[q_id][0][0]\n        if right_answer==first_answer:\n            count_first += 1\n        \n        last_answer = q_to_a[q_id][-1][0]\n        if right_answer==last_answer:\n            count_last += 1\n            \n        longest_answer = -1\n        max_length = 0\n        for a_id, score, _ in q_to_a[q_id]:\n            leng = a_val[a_val.Id==a_id].iloc[0]['TotalLength']\n            if leng > max_length:\n                longest_answer = a_id\n                max_length = leng\n        if right_answer==longest_answer:\n            count_long += 1\n    print('Baseline for the first answer:', count_first/len(q_ids))\n    print('Baseline for the last answer:', count_last/len(q_ids))\n    print('Baseline for the longest answer:', count_long/len(q_ids))\n\nbaseline(q_test['Id'].tolist(), a_test)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"62b7180d-8958-4186-b9e4-cb23cd0fb0dc","_uuid":"7b35e9e9b90326d97b1231f609f9c72c52cf241a"},"cell_type":"markdown","source":"## Explaintion of important features"},{"metadata":{"_cell_guid":"17addfe1-383b-4be7-9c33-6a56d04624bc","_uuid":"64295aa3f6995b5c3c08b20a07291a1869563110","trusted":true},"cell_type":"code","source":"importances = rf_classifier.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf_classifier.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\nprint(\"Feature ranking:\")\nfor i in range(x_test.shape[1]-1):\n    print(\"%d. feature %d (%f) (%s)\" % (i + 1, indices[i], importances[indices[i]], list(x_test.columns.values)[indices[i]+1]))","execution_count":26,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}