{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Share Raw Data"},{"metadata":{},"cell_type":"markdown","source":"In London, UK and Taipei, Taiwan, there are bike-share systems using docks. A rental event is recorded at least as a start station, an end station, and the two corresponding times. This defines a spatiotemporal data set, or in other words, a multi-dimensional time-series on a graph.\n\nThe raw data files have been retrieved from the respective bike-sharing system operators, parsed and moderately reformatted (e.g. making column names uniform over all data), then saved into one CSV file per city, rows sorted on start date. Only raw data from 2017 and up until mid-2020 are included.\n\nThe raw data sources are:\n* https://cycling.data.tfl.gov.uk\n* https://data.taipei/#/dataset?topic=topic-transportation\n\nFor examples of analyses of this data as blog posts, follow these links:\n\n* Build usage baseline and contrast how COVID-19 impact usage when and where: https://towardsdatascience.com/responses-to-covid-19-in-taipei-and-london-as-revealed-by-bike-sharing-5fcb215f2341\n* Simple graph convolutional neural networks for forecasting rental events: https://towardsdatascience.com/london-bike-ride-forecasting-with-graph-convolutional-networks-aee044e48131\n* Descriptive analysis and visualization for London data: https://medium.com/@AJOhrn/data-footprint-of-bike-sharing-in-london-be9e11425248\n\nAcademic article using the same type of data:\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093851\n\nNext I provide a handful of helpful commands to enable analysis."},{"metadata":{},"cell_type":"markdown","source":"## Load Data and Basic Cleaning and Extension"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start with London, and load all bike-share data for the Santander London system, January 2017 up until mid-2020. Parse the two time columns as datetime types."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_london = pd.read_csv('/kaggle/input/bikeshare-usage-in-london-and-taipei-network/london.csv', parse_dates=[3,6], infer_datetime_format=True)\ndf_london.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data includes rare rental events that are started but not ended. For some analysis these are best removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_london = df_london.loc[~df_london.isnull().any(axis=1)]\ndf_london = df_london.astype({'bike_id' : 'int32', 'end_station_id' : 'int32', 'start_station_id' : 'int32'})\ndf_london.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Augment datetime data to make it easier for binning and applying conditions with respect to time."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_london.loc[:, 'year'] = df_london['start_rental_date_time'].dt.year\ndf_london.loc[:, 'month'] = df_london['start_rental_date_time'].dt.month\ndf_london.loc[:, 'week'] = df_london['start_rental_date_time'].dt.isocalendar().week\ndf_london.loc[:, 'day'] = df_london['start_rental_date_time'].dt.day\ndf_london.loc[:, 'hour'] = df_london['start_rental_date_time'].dt.hour\ndf_london.loc[:, 'dayofweek'] = df_london['start_rental_date_time'].dt.dayofweek\ndf_london.loc[:, 'satsun'] = df_london['dayofweek'].map({0: False, 1: False, 2: False, 3: False, 4: False, 5: True, 6:True})\ndf_london.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A handful of stations are very rarely part of any rental events. In some cases these are temporary stations or stations that become decommissioned. In some analysis these low count stations are best removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"below_or_above_stations = df_london['start_station_name'].value_counts() < 500\nlow_stations = below_or_above_stations.loc[below_or_above_stations].index\ndf_london = df_london.loc[~df_london['start_station_name'].isin(low_stations)]\ndf_london = df_london.loc[~df_london['end_station_name'].isin(low_stations)]\ndf_london.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis Can Begin (London)"},{"metadata":{},"cell_type":"markdown","source":"On this dataset descriptive analysis can be made. Examples of that include seasonal effects, weekday verus weekend, particular stations that change the most between years etc. A simple example is the number of rental events in the months of a particular year, say 2018."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nper_month_2018_group = df_london.loc[df_london['year'] == 2018].groupby('month')\nmonth_counts = per_month_2018_group.count()\n\nsns.barplot(x=month_counts.index, y=month_counts.iloc[:,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another example is to analyze time-series for specific stations within a day, like station 14 (a major commuter station in London) during weekdays."},{"metadata":{"trusted":true},"cell_type":"code","source":"weekday_14_start = df_london.loc[(df_london['start_station_id'] == 14) & (df_london['satsun'] == False)]\nweekday_14_end = df_london.loc[(df_london['end_station_id'] == 14) & (df_london['satsun'] == False)]\nweekday_14_start = weekday_14_start.groupby('hour').count()\nweekday_14_end = weekday_14_end.groupby('hour').count()\nweekday_14_data = pd.DataFrame({'start_counts' : weekday_14_start.iloc[:,0],\n                                'end_counts' : weekday_14_end.iloc[:,0]})\n\nsns.lineplot(data=weekday_14_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Longitude and latitude data is also available for each station."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_london_geo = pd.read_csv('/kaggle/input/bikeshare-usage-in-london-and-taipei-network/london_stations.csv')\ndf_london_geo.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additional analysis and data mining are easy to image, some of which can be found in this blog post:  https://medium.com/@AJOhrn/data-footprint-of-bike-sharing-in-london-be9e11425248\n\nMore advanced analysis can use the data to model the distributions of expected rental events at different stations and/or time of day. For example, it has been found that weather matters a great deal to rental events, at least extreme weather. A Bayesian analysis that models what weather conditions implies for the distribution of rental events, can be attempted by cross-referencing current data with weather data for London.\n\n**Note that the data contains a major concept shift in March 2020** when the COVID-19 pandemic impacts London. That itself can be subject of analysis, as shown in this blog post:  https://towardsdatascience.com/responses-to-covid-19-in-taipei-and-london-as-revealed-by-bike-sharing-5fcb215f2341\n\nHave fun!"},{"metadata":{},"cell_type":"markdown","source":"## Graph Analysis"},{"metadata":{},"cell_type":"markdown","source":"The data can be subject to graph analysis, with each station a vertex in the graph and the edge weight between two vertices related to the number of rental events involves the corresponding pair of stations. The directed weighted adjacency matrix is the key quantity."},{"metadata":{"trusted":true},"cell_type":"code","source":"station_pair_group = df_london.groupby(['start_station_name', 'end_station_name'])\nstation_pair_count = station_pair_group.count().iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are multiple valid relations between edge weight and number of rental events involving the corresponding pair of stations. One is to relate the weight to the percentage of all rental events from the given start station that flows through the given edge."},{"metadata":{"trusted":true},"cell_type":"code","source":"total_by_start_station = station_pair_count.groupby('start_station_name').sum()\nrel_weight = 100.0 * station_pair_count.div(total_by_start_station, level=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rare connections can sometimes be removed in order to make the graph representation sparser."},{"metadata":{"trusted":true},"cell_type":"code","source":"rel_weight = rel_weight.loc[rel_weight > 1.0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In some analysis (and especially visualizations) the adjacency matrix needs to account for no adjacency. The `rel_weight` data frame accounts for non-zero adjacencies only. The full matrix is thus constructed by adding zeros."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nstations = np.union1d(df_london['start_station_name'].unique(), df_london['end_station_name'].unique())\nmindex_square = pd.MultiIndex.from_product([stations, stations])\nrel_weight_square = rel_weight.reindex(index=mindex_square, fill_value=0.0)\nrel_weight_square = rel_weight_square.unstack()\nsns.heatmap(rel_weight_square.values, vmin=0.0, vmax=5.0, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some network analysis can be done by representing the directed weighted graph as a `DiGraph` instance from the `networkx` library."},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\ndg = nx.DiGraph()\ndg.add_nodes_from(stations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The edge definitions in `networkx` require edges to be defined as a three-membered tuple."},{"metadata":{"trusted":true},"cell_type":"code","source":"edge_weights_dict = rel_weight.to_dict()\nedge_weights_data = [(key1, key2, val) for (key1, key2), val in edge_weights_dict.items()]\ndg.add_weighted_edges_from(edge_weights_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A range of analysis can be run using methods in `networkx.algorithms`. One example is the node clusterings."},{"metadata":{"trusted":true},"cell_type":"code","source":"node_clusterings = nx.algorithms.cluster.clustering(dg)\nsorted(node_clusterings.items(), key=lambda kv: kv[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Graph Convolution Networks"},{"metadata":{},"cell_type":"markdown","source":"A continuation of the graph analysis is to employ Graph Convolution Networks (GCNs) to model the rental time-series. One possible application is in forecasting, taking into account the known spatial relations between the stations, as illustrated in this blog post: https://towardsdatascience.com/london-bike-ride-forecasting-with-graph-convolutional-networks-aee044e48131 Other applications can be classification of stations on basis of its \"when-and-where\" place in the bike-sharing system, or maybe system-wide properties, like weather, special events, highly resolved consumer spending in SoHo... Possible options are in no shortage, useful options is another question.\n\nOne possibility is to use the [PyTorch Geometric library](https://pytorch-geometric.readthedocs.io/en/latest/index.html). The graph of the bike system has then to be represented as required by PyTorch Geometric. \n\nUnless installed already, run the `pip install` and if GPU is used, replace `cpu` with the CUDA version, like `cu101`. See the PyTorch Geometric installation help page for details."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n!pip install torch-geometric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch_geometric.data import Data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As illustration I create dummy node input features, one per node, which should be replaced with *input features* to the neural network. These could, for example, represent the number of rental events at said station in the previous hour. \n\nI also create dummy one-feature-per-node output. The dimension of the output should reflect the prediction task, so a single system-wide scalar feature would instead correspond to a tensor containing just one value."},{"metadata":{"trusted":true},"cell_type":"code","source":"stations_dummy_inp_features = torch.tensor([np.random.ranf() for s in stations])\nstations_dummy_out_features = torch.tensor([np.random.ranf() for s in stations])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The adjacency matrix is represented in the COO format, and the weights are made into edge attributes. For those GCN methods that use weights, that is what PyTorch Geometric expects."},{"metadata":{"trusted":true},"cell_type":"code","source":"edge_s = []\nedge_t = []\nedge_attr = []\nfor row, value in rel_weight.items():\n    edge_s.append(list(stations).index(row[0]))\n    edge_t.append(list(stations).index(row[1]))\n    edge_attr.append(value)\n    \nedge_index = torch.tensor([edge_s, edge_t])\nedge_attr = torch.tensor(edge_attr).unsqueeze(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Put it all together into a single instance of input features on a graph and corresponding output features."},{"metadata":{"trusted":true},"cell_type":"code","source":"pyg_graphtensor = Data(x=stations_dummy_inp_features, y=stations_dummy_out_features,\n                       edge_index=edge_index, edge_attr=edge_attr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The neural network model has to be built after this, along with a dataset comprised of several instances of `Data`. These are details for another notebook.\n\nBecause the bike-share data imply a natural graph structure, the inductive bias of GCN should be able to represent some spatial features of the bike-sharing data, in case these are relevant to the prediction objective that is. At least that is a hypothesis worth exploring."},{"metadata":{},"cell_type":"markdown","source":"## Analysis Continues (Taipei)"},{"metadata":{},"cell_type":"markdown","source":"The data for Taipei bike-sharing system can undergo the same analysis as outlined above for London. The data for Taipei is slightly different, however. I will highlight these differences only.\n\nNote that this is rather large CSV file, so loading in chunks or rows subsets may be needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_taipei = pd.read_csv('/kaggle/input/bikeshare-usage-in-london-and-taipei-network/taipei.csv', parse_dates=[0,2,5], infer_datetime_format=False, \n                        nrows=10000)\ndf_taipei.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_taipei.loc[:, 'year'] = df_taipei['start_rental_date_hour'].dt.year\ndf_taipei.loc[:, 'month'] = df_taipei['start_rental_date_hour'].dt.month\ndf_taipei.loc[:, 'week'] = df_taipei['start_rental_date_hour'].dt.isocalendar().week\ndf_taipei.loc[:, 'day'] = df_taipei['start_rental_date_hour'].dt.day\ndf_taipei.loc[:, 'hour'] = df_taipei['start_rental_date_hour'].dt.hour\ndf_taipei.loc[:, 'dayofweek'] = df_taipei['start_rental_date_hour'].dt.dayofweek\ndf_taipei.loc[:, 'satsun'] = df_taipei['dayofweek'].map({0: False, 1: False, 2: False, 3: False, 4: False, 5: True, 6:True})\ndf_taipei.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_taipei.head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Contrasts with London dataset:\n* Taipei bike-sharing is larger than London (a bit over double the number of rental events).  \n* The stations are not assigned a numeric ID by the operator. As part of the analysis that can be added.\n* The station names are in traditional chinese. In order to enable easy filtering for users who cannot easily read or write such characters, the `pinyin` library has been used to create basic Pinyin Romanizations of the station names. This is *not* the same as a translation, and the strings are rather clunky.\n* The times for start and end of rental is only resolved up to the hour (hence the suffix `_hour` rather than `time`). Note that duration of the bike rental is in units of seconds, however.\n\nOtherwise the data tracks the same transport activity as the data set for London. Comparison between the two cities can therefore be done along across many different features. For example, in a previous comparison it has been found that the rental events across stations in the two systems distribute quite distinctly over time of day, see the discussion about MDS and Jensen-Shannon in this blog post:  https://towardsdatascience.com/responses-to-covid-19-in-taipei-and-london-as-revealed-by-bike-sharing-5fcb215f2341\n\nGood luck with the exploration."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}