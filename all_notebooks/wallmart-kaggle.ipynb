{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Importing packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport datetime as dt\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Reading data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the dataset\nos.chdir('/kaggle/input/wallmart/')\ncal = pd.read_csv('calendar.csv')\nsales = pd.read_csv('sales_train_validation.csv')\nsell_prices = pd.read_csv('sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Preprocessing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Calculating quantities","execution_count":null},{"metadata":{"id":"omQMH3FiOqDz","trusted":true},"cell_type":"code","source":"cal['date']=pd.to_datetime(cal['date'])\ncal = cal.rename(columns = {'d':'id'})\ncal_1913 = cal[:1913]\ncal_1913.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"oS6ZAzwsKtZS","outputId":"cc234c69-7e75-4691-c220-e0a0db85a50f","trusted":true},"cell_type":"code","source":"sales_new = sales.drop(columns = ['item_id','dept_id','cat_id','store_id','state_id'])\nsales_new = sales_new.sort_values(['id'])\nsales_new = sales_new.set_index(['id'])\nsales_new = sales_new.transpose()\nsales_new = sales_new.reset_index()\nsales_new.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"5lAObZxwWuBn","trusted":true},"cell_type":"code","source":"cal_sales = pd.concat([cal_1913,sales_new],axis=1)\ncal_sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_sales.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Calculating prices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices['state_id'] = sell_prices.item_id.map(str) \\\n                          + '_'  + sell_prices.store_id.map(str) + '_validation' \\\n\nsell_prices.head()            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices[(sell_prices['state_id'] == 'HOBBIES_1_001_CA_1_validation') & (sell_prices['wm_yr_wk']>11613)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_set = set(cal_sales.columns[15:])         # Set of all items\nprices_arr = np.zeros((1,30490))               # Initializing prices with zeros as the first observation\nfor week_no in cal_sales['wm_yr_wk'].unique() :      \n    single_row_sell = sell_prices[sell_prices['wm_yr_wk'] == week_no][['sell_price','state_id']]  # Retrieving prices of all items in the week\n    differ_set = main_set.difference(set(single_row_sell['state_id'])) # Finding missing items with no price tag in the week \n    data = {'sell_price':[np.nan]*len(differ_set),'state_id':list(differ_set)} # Assigning NaN values to those missing items\n    dfl = pd.DataFrame.from_dict(data) # Converting missing items (with NaN tags) to a dataframe\n    new_df = pd.concat([single_row_sell,dfl],axis=0).sort_values('state_id').reset_index().iloc[:,1:] # Concatinating items with and without prices in the week\n    prices_arr = np.vstack((prices_arr,np.array(list(new_df.set_index('state_id').T.values)*7))) # Stacking prices of each week\n\nprices_arr = prices_arr[1:-5] # Removing the intial zeros we initialized with and the extra prices in the last week\nprices_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Multiplying quantities and prices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_value = cal_sales.iloc[:,15:] * prices_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = total_value\ndf2['date'] = cal['date'].iloc[:1913]\ndf2 = df2.fillna(0)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Adding new features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### a. Total sales of 10 stores per day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Total'] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    df2['Total'] += df2[i]\ndf2['Total'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df2, x='date', y='Total', title='Wallmart Sales 2011-2016/10 stores',width=1200)\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The downward lines in the above chart corresponds to no sales on the Eve of Christmas holiday","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### b. Total sales per day per state","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30490):\n    i = df2.columns[i]\n    state = i.split('_')[3]\n    if state not in df2.columns:\n        df2[state] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    state = i.split('_')[3]\n    df2[state] += df2[i]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['CA'].values,\n                    mode='lines',\n                    name='CA'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['TX'].values,\n                    mode='lines',\n                    name='TX'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['WI'].values,\n                    mode='lines',\n                    name='WI'))\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart statewise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\nfig.update_xaxes(rangeslider_visible=True,)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are 4 stores in California compared to 3 stores each in Texas and Wisconsin, California is on top the chart","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### c. Total sales per day per category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30490):\n    i = df2.columns[i]\n    category = i.split('_')[0]\n    if category not in df2.columns:\n        df2[category] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    category = i.split('_')[0]\n    df2[category] += df2[i]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['FOODS'].values,\n                    mode='lines',\n                    name='FOODS'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['HOBBIES'].values,\n                    mode='lines',\n                    name='HOBBIES'))\nfig.add_trace(go.Scatter(x=df2['date'], y=df2['HOUSEHOLD'].values,\n                    mode='lines',\n                    name='HOUSEHOLD'))\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart category wise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### d. Total sales per day per state per store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30490):\n    i = df2.columns[i]\n    store = i.split('_')[3] + '_' + i.split('_')[4]\n    if store not in df2.columns:\n        df2[store] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    store = i.split('_')[3] + '_' + i.split('_')[4]\n    df2[store] += df2[i]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30490):\n    i = df2.columns[i]\n    item = i.split('_')[3] + '_' + i.split('_')[4] + '_' + i.split('_')[0]\n    if item not in df2.columns:\n        df2[item] = 0\nfor i in range(30490):\n    i = df2.columns[i]\n    item = i.split('_')[3] + '_' + i.split('_')[4] + '_' + i.split('_')[0]\n    df2[item] += df2[i]\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Analysis of California stores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor i in range(30498,30502): \n    i = df2.columns[i]\n    fig.add_trace(go.Scatter(x=df2['date'], y=df2[i].values,\n                        mode='lines',\n                        name=i.split('_')[0] + ' store ' +i.split('_')[1]))\n    \nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart California store wise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.1 Analysis of California Store 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor i in range(30510,30531,10): \n    i = df2.columns[i]\n    fig.add_trace(go.Scatter(x=df2['date'], y=df2[i].values,\n                        mode='lines',\n                        name=i.split('_')[2]))\n    \nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n    title=\"Walmart California store 3 category wise sales\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Sales\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#042a30\"\n    )\n)\n\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Events analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal = pd.read_csv('calendar.csv')\nevents = cal[['date','event_name_1','event_type_1','event_name_2','event_type_2']]\nevents = events.fillna(0)\nevents = events[(events['event_name_1'] != 0) | (events['event_name_2'] != 0)]\nevents.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = []\nc = 0\nfor x in events['date'].values:\n    c +=1\n    l.append(\n    dict(\n        type=\"line\",\n        yref='paper',\n        y0=0,\n        y1=1,\n        xref='x1',\n        x0=x,\n        x1=x,\n        line=dict(\n            color=\"Red\",\n            width=2,\n            dash=\"dashdot\",\n    )))\nprint(c)\nfig = px.line(df2, x='date', y='CA_3')\nfig.update_layout(shapes=l)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nevents[events['date'] == dt.datetime(2015, 4 , 12)]\nevents[(events['date'].apply(lambda a : dt.datetime.strptime(a, \"%Y-%m-%d\").month) == 5) | (events['date'].apply(lambda a : dt.datetime.strptime(a, \"%Y-%m-%d\").month) == 6)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2[['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']] = cal[['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']]\ndf2[['date','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Disaster analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis = pd.read_csv('us_disasters_m5.csv')\ndis.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca = dis[dis['state'] == 'CA']\nprint(dis_ca.shape)\ndis_ca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca['declaration_date'] = pd.to_datetime(dis_ca['declaration_date'].apply(lambda x : x[:10]))\ndis_ca['declaration_date'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline = dis_ca[['incident_type','declaration_date']]\ndis_ca_timeline['declaration_date'] = pd.to_datetime(dis_ca_timeline['declaration_date'])\ndis_ca_timeline['declaration_date'] = dis_ca_timeline['declaration_date'].apply(lambda x : x.strftime(\"%Y-%m-%d\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndis_ca_timeline = dis_ca_timeline.reset_index()\ndis_ca_timeline = dis_ca_timeline.drop(columns='index') \ndis_ca_timeline.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2[['date','Total']].set_index('date').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = []\nfor i in range(54):\n    x = dis_ca_timeline['declaration_date'].iloc[i]\n    l.append(\n    dict(\n        type=\"line\",\n        yref='paper',\n        y0=0,\n        y1=1,\n        xref='x1',\n        x0=x,\n        x1=x,\n        line=dict(\n            color=\"Red\",\n            width=2,\n            dash=\"dashdot\",\n    )))\nfig = px.line(df2, x='date', y='Total')\n\ni=40\nx = dis_ca_timeline['declaration_date'].iloc[i]\nfig.update_layout(shapes=l)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline['declaration_date'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline['Threat level'] = 'Minor'\n\nd = dis_ca_timeline['incident_type'] == 'Tsunami'\n\ndis_ca_timeline.loc[d.values,'Threat level'] = 'Major'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline[dis_ca_timeline['incident_type'] != 'Fire']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis[dis['state'] == 'CA']\ndis.iloc[327:329,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline1 = dis_ca[['incident_type','incident_begin_date']]\ndis_ca_timeline1['incident_begin_date'] = pd.to_datetime(dis_ca_timeline1['incident_begin_date'])\ndis_ca_timeline1['incident_begin_date'] = dis_ca_timeline1['incident_begin_date'].apply(lambda x : x.strftime(\"%Y-%m-%d\"))\ndis_ca_timeline1 = dis_ca_timeline1.reset_index()\ndis_ca_timeline1 = dis_ca_timeline1.drop(columns='index') \ndis_ca_timeline1.set_index('incident_begin_date')\n#dis_ca_timeline1 = dis_ca_timeline1[dis_ca_timeline1['incident_type'] != 'Fire']\ndis_ca_timeline1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_ca_timeline2 = pd.read_excel('dis_ca_timeline1_modified.xlsx')\ndis_ca_timeline2[dis_ca_timeline2['Threat level'] == 'Medium']['incident_begin_date'].values\ndis_ca_timeline2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.4.1 Disaster is Medium","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"l = []\nfor x in dis_ca_timeline2[dis_ca_timeline2['Threat level'] == 'Medium']['incident_begin_date'].values:\n    l.append(\n    dict(\n        type=\"line\",\n        yref='paper',\n        y0=0,\n        y1=1,\n        xref='x1',\n        x0=x,\n        x1=x,\n        line=dict(\n            color=\"Red\",\n            width=2,\n            dash=\"dashdot\",\n    )))\n    \nfig = px.line(df2, x='date', y='CA_3')\nfig.update_layout(shapes=l)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Extracting single item ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat([cal,total_value['HOBBIES_1_001_CA_1_validation']],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Head of the dataset\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the dataset\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Info of the dataset\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary statistics\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we could see the max and min are almost same for all the columns so no need of scaling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null values check\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Data preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(['wm_yr_wk', 'weekday','d', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_TX', 'snap_WI'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['event_name_1'] = dataset['event_name_1'].fillna(0)\ndataset['event_name_1'] = np.where(dataset['event_name_1'] != 0,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['date'] = pd.to_datetime(dataset['date'])\ndataset['dayofmonth'] = dataset['date'].dt.day\ndom = pd.get_dummies(dataset['dayofmonth'],prefix='dayofmonth_',drop_first=True)\nmonth = pd.get_dummies(dataset['month'],prefix='month_',drop_first=True)\nyear = pd.get_dummies(dataset['year'],prefix='year_',drop_first=True)\nwday = pd.get_dummies(dataset['wday'],prefix='wday_',drop_first=True)\ndataset.drop(['month','year','dayofmonth','wday'],1,inplace=True)\ndataset = pd.concat([dataset,month,year,dom,wday],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.iloc[896:-28,:]\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_date = '2016-04-24'\nTrain = dataset.loc[dataset['date'] <= split_date].copy()\nTest = dataset.loc[dataset['date'] > split_date].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.drop(['date'],1,inplace=True)\nTest.drop(['date'],1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = Train.drop(['HOBBIES_1_001_CA_1_validation'],1)\ny_train = Train['HOBBIES_1_001_CA_1_validation']\nx_test = Test.drop(['HOBBIES_1_001_CA_1_validation'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/wallmart-sales/')\nevaluation_df = pd.read_csv('sales_train_evaluation.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = pd.Series(evaluation_df.iloc[0,-28:].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = pd.concat([x_train,x_test],0)\ny = pd.concat([y_train,y_test],0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=54)\npca.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative variance explained')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=20)\npca.fit(x)\nx = pca.transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = pd.DataFrame(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x.iloc[:-28,:]\nx_test = x.iloc[-28:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test *= 8.38","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Building models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.2.1 Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Linear Regression and fitting the model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train,y_train)\nlr_pred = lr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing metrics and evaluating the model\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score \nlr_rmse = np.sqrt(metrics.mean_squared_error(lr_pred,y_test))\nlr_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score\nlr_r2score = metrics.r2_score(lr_pred,y_test)\nlr_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score\nlr_train = lr.score(x_train,y_train)\nlr_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\nlr_test = lr.score(x_test,y_test)\nlr_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.2 Decision Tree  <a id='dt'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Decision Tree and performing decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(x_train,y_train)\ndt_pred = dt.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Decision Tree\ndt_rmse = np.sqrt(metrics.mean_squared_error(dt_pred,y_test))\ndt_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Decision Tree\ndt_r2score = metrics.r2_score(dt_pred,y_test)\ndt_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Decision Tree\ndt_train = dt.score(x_train,y_train)\ndt_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Decision Tree\ndt_test = dt.score(x_test,y_test)\ndt_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Randomizedsearchcv and finding out optimal parameters for Decision Tree\nfrom sklearn.model_selection import RandomizedSearchCV\nparams = {'max_depth': np.arange(1,20),'criterion':['mse','mae']}\ndt = DecisionTreeRegressor()\ntree = RandomizedSearchCV(dt, params, cv=3 , return_train_score = True) # RandomizedSearchCV\ntree.fit(x,y)# Fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimal parameters\ntree.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model and training and testing after parameter tuning\ndtr = DecisionTreeRegressor(criterion='mse',max_depth=1)\ndtr.fit(x_train,y_train)\ndtr_pred = dtr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for DT after parameter tuning\ndt_tune_rmse = np.sqrt(mean_squared_error(dtr_pred,y_test))\ndt_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for DT after parameter tuning\ndt_tune_r2score = r2_score(dtr_pred,y_test)\ndt_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for DT after parameter tuning\ndt_tune_train = dtr.score(x_train,y_train)\ndt_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for DT after parameter tuning\ndt_tune_test = dtr.score(x_test,y_test)\ndt_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.3 Random Forest  <a id='rf'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Random Forest Regressor and fitting the model\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train,y_train)\nrf_pred = rf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Random Forest\nrf_rmse = np.sqrt(mean_squared_error(rf_pred,y_test))\nrf_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 Score for Random Forest\nrf_r2score = r2_score(rf_pred,y_test)\nrf_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Random Forest\nrf_train = rf.score(x_train,y_train)\nrf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Random Forest\nrf_test = rf.score(x_test,y_test)\nrf_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Randomized SearchCV and finding optimal parameters\nrf = RandomForestRegressor()\nparams1 = {'n_estimators': np.arange(1,20),'criterion':['mse','mae']}\nforest = RandomizedSearchCV(rf, params, cv=3 , return_train_score = True) # GridSearchCV\nforest.fit(x,y)# Fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimal parameters\nforest.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest after parameter tuning\nrfr = RandomForestRegressor(criterion='mse',max_depth=2)\nrfr.fit(x_train,y_train)\nrfr_pred = rfr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score  for Random Forest after parameter tuning\nrf_tune_rmse = np.sqrt(metrics.mean_squared_error(rfr_pred,y_test))\nrf_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Random Forest after parameter tuning\nrf_tune_r2score = metrics.r2_score(rfr_pred,y_test)\nrf_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Random Forest after parameter tuning\nrf_tune_train = rfr.score(x_train,y_train)\nrf_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Random Forest after parameter tuning\nrf_tune_test = rfr.score(x_test,y_test)\nrf_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  5.2.4 Support Vector Machine <a id = 'svm'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Support Vector Regressor and fitting the model\nfrom sklearn.svm import SVR\nsvm = SVR()\nsvm.fit(x_train,y_train)\nsvm_pred = svm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for SVM\nsvm_rmse = np.sqrt(metrics.mean_squared_error(svm_pred,y_test))\nsvm_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for SVM\nsvm_r2score = metrics.r2_score(svm_pred,y_test)\nsvm_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for SVM\nsvm_train = svm.score(x_train,y_train)\nsvm_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for SVM\nsvm_test = svm.score(x_test,y_test)\nsvm_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Randomized Search cv to find the optimal parameters\nparams2 = {'kernel':['linear','rbf'],'C': [0.01, 0.1, 1, 10],'gamma': [0.01,0.1,1,10]}\nsvr = SVR()\nsupport = RandomizedSearchCV(svr, params2, cv=3 , return_train_score = True) # RandomizedSearchCV\nsupport.fit(x,y)# Fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimal parameters\nsupport.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model and training and testing\nsvrr = SVR(C = 10,gamma = 10,kernel = 'rbf')\nsvrr.fit(x_train,y_train)\nsvrr_pred = svrr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for SVM after parameter tuning\nsvm_tune_rmse = np.sqrt(metrics.mean_squared_error(svrr_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for SVM after parameter tuning\nsvm_tune_r2score = metrics.r2_score(svrr_pred,y_test)\nsvm_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for SVM after parameter tuning\nsvm_tune_train = svrr.score(x_train,y_train)\nsvm_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for SVM after parameter tuning\nsvm_tune_test = svrr.score(x_test,y_test)\nsvm_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.5 KNearest Neighbors <a id = 'knn'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing KNearest Neighbors and fitting the model\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor()\nknn.fit(x_train,y_train)\nknn_pred = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for KNN\nknn_rmse = np.sqrt(metrics.mean_squared_error(knn_pred,y_test))\nknn_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for KNN\nknn_r2score = metrics.r2_score(knn_pred,y_test)\nknn_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for KNN\nknn_train = knn.score(x_train,y_train)\nknn_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for KNN\nknn_test = knn.score(x_test,y_test)\nknn_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding optimal parameters using Randomized Search CV\nparams4 = {'leaf_size':np.arange(1,50),'n_neighbors':np.arange(1,30),'p':[1,2]}\nknn = KNeighborsRegressor()\nneighbor = RandomizedSearchCV(knn, params4, cv=3 , return_train_score = True) # RandomizedSearchCV\nneighbor.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimal parameters\nneighbor.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model and training and testing\nknn = KNeighborsRegressor(n_neighbors=21,p=1,leaf_size=38)\nknn.fit(x_train,y_train)\nknnr_pred = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for KNN after parameter tuning\nknn_tune_rmse = np.sqrt(metrics.mean_squared_error(knnr_pred,y_test))\nknn_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for KNN after parameter tuning\nknn_tune_r2score = metrics.r2_score(knnr_pred,y_test)\nknn_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for KNN after parameter tuning\nknn_tune_train = knn.score(x_train,y_train)\nknn_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for KNN after parameter tuning\nknn_tune_test = knn.score(x_test,y_test)\nknn_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2.6 Ada Boost Classifier <a id = 'ada'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the Ada Boost model\nfrom sklearn.ensemble import AdaBoostRegressor\nab = AdaBoostRegressor()\nab.fit(x_train,y_train)\nab_pred = ab.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Ada Boost\nab_rmse = np.sqrt(metrics.mean_squared_error(ab_pred,y_test))\nab_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Ada Boost\nab_r2score = metrics.r2_score(ab_pred,y_test)\nab_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Ada Boost\nab_train = ab.score(x_train,y_train)\nab_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Ada Boost\nab_test = ab.score(x_test,y_test)\nab_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the optimal parameters for Ada Boost Regressor using Randomized Search CV\n\nparam_grid1 = {\"n_estimators\": range(5,20,2) ,  \n              \"learning_rate\": [0.01,0.05,0.1,0.5,1],'loss':['linear','square','exponential']}\n \n\nAB = RandomizedSearchCV(ab,param_distributions=param_grid1,\n                           cv = 5,\n                           n_jobs=-1,\n                           verbose=2)\nAB.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimal parameters\nAB.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model after parameter tuning\nabr = AdaBoostRegressor(n_estimators=9,learning_rate=0.05,loss='linear')\nabr.fit(x_train,y_train)\nabr_pred = abr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Ada Boost after parameter tuning\nab_tune_rmse = np.sqrt(metrics.mean_squared_error(abr_pred,y_test))\nab_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Ada Boost after parameter tuning\nab_tune_r2score = metrics.r2_score(abr_pred,y_test)\nab_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Ada Boost after parameter tuning\nab_tune_train = abr.score(x_train,y_train)\nab_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Ada Boost after parameter tuning\nab_tune_test = abr.score(x_test,y_test)\nab_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###5.2.7 Gradient Boosting <a id = 'grad'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the Gradient Boost model\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor()\ngb.fit(x_train,y_train)\ngb_pred = gb.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Gradient Boosting\ngb_rmse = np.sqrt(metrics.mean_squared_error(gb_pred,y_test))\ngb_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Gradient Boosting\ngb_r2score = metrics.r2_score(gb_pred,y_test)\ngb_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Gradient Boosting\ngb_train = gb.score(x_train,y_train)\ngb_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Gradient Boosting\ngb_test = gb.score(x_test,y_test)\ngb_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the optimal parameters For Gradient Boosting Regressor using Randomized Search CV\n\nparam_grid1 = {\"n_estimators\": range(5,20,2) ,  \n              \"learning_rate\": [0.01,0.05,0.1,0.5,1]}\n \n\nGB = RandomizedSearchCV(gb,param_distributions=param_grid1,\n                           cv = 5,\n                           n_jobs=-1,\n                           verbose=2)\nGB.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimal parameters\nGB.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the Gradient model after parameter tuning\ngbr = GradientBoostingRegressor(n_estimators=9,learning_rate=0.01)\ngbr.fit(x_train,y_train)\ngbr_pred = gbr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Gradient Boosting after parameter tuning\ngb_tune_rmse = np.sqrt(metrics.mean_squared_error(gbr_pred,y_test))\ngb_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Gradient Boosting after parameter tuning\ngb_tune_r2score = metrics.r2_score(gbr_pred,y_test)\ngb_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Gradient Boosting after parameter tuning\ngb_tune_train = gbr.score(x_train,y_train)\ngb_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for parameter tuning\ngb_tune_test = gbr.score(x_test,y_test)\ngb_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.8 XG Boost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\nxgb = XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\nxgb.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the model\nxgb_pred=xgb.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for XG Boost\nxgb_rmse = np.sqrt(metrics.mean_squared_error(xgb_pred,y_test))\nxgb_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for XG Boost\nxgb_r2score = metrics.r2_score(xgb_pred,y_test)\nxgb_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for XG Boost\nxgb_train = xgb.score(x_train,y_train)\nxgb_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for XG Boost\nxgb_test = xgb.score(x_test,y_test)\nxgb_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding optimal parameters for XG Boost Regressor using Randomized Search CV\nparam_grid1 = {\"max_depth\": [10,15,20,30],\n              \"n_estimators\": range(5,20,2) , \n              \"gamma\": [0.03,0.05], \n              \"learning_rate\": [0.01,0.05]}\n \n\nXGB = RandomizedSearchCV(xgb,param_distributions=param_grid1,\n                           cv = 5)\nXGB.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimal parameters\nXGB.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model after parameter tuning\nxgbr = XGBRegressor(n_estimators=9,max_depth=10,learning_rate=0.05,gamma=0.03)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model after parameter tuning\nxgbr.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the model after parameter tuning\nxgbr_pred = xgbr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for XG Boost after parameter tuning\nxgb_tune_rmse = np.sqrt(metrics.mean_squared_error(xgbr_pred,y_test))\nxgb_tune_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for XG Boost after parameter tuning\nxgb_tune_r2score = metrics.r2_score(xgbr_pred,y_test)\nxgb_tune_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for XG Boost after parameter tuning\nxgb_tune_train = xgbr.score(x_train,y_train)\nxgb_tune_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for XG Boost after parameter tuning\nxgb_tune_test = xgbr.score(x_test,y_test)\nxgb_tune_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.9 Arima models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_arima_train = Train['HOBBIES_1_001_CA_1_validation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python3.7 -m pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pmdarima.arima import auto_arima\nstepwise_model = auto_arima(df_arima_train,start_p=1,start_q=1,max_p=3,max_q=3,m=7,start_P=0,seasonal=True,d=1,D=1,trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ar_day_pred = stepwise_model.predict(n_periods=28)\nar_day_rmse = np.sqrt(metrics.mean_squared_error(ar_day_pred, y_test))\nar_day_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stepwise_model1 = auto_arima(df_arima_train,start_p=1,start_q=1,max_p=3,max_q=3,m=12,start_P=0,seasonal=True,d=1,D=1,trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)\nar_month_pred = stepwise_model1.predict(n_periods=28) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ar_month_rmse = np.sqrt(metrics.mean_squared_error(ar_month_pred,y_test))\nar_month_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2.10 Comparison Table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dictionary for all the metrics and models\nmetrics_dict = {'Metrics': ['Before Parameter Tune Train Score','Before Parameter Tune Test Score','After Parameter Tune Train Score','After Parameter Tune Test Score','Before Parameter Tune RMSE Score','After Parameter Tune RMSE Score','Before Parameter Tune R2 Score','After Parameter Tune R2 Score'],'Linear Regression':[lr_train,lr_test,'NA','NA',lr_rmse,'NA',lr_r2score,'NA'],\n          'Decision Tree Regressor':[dt_train,dt_test,dt_tune_train,dt_tune_train,dt_rmse,dt_tune_rmse,dt_r2score,dt_tune_r2score],'Ramdom Forest Regressor':[rf_train,rf_test,rf_tune_train,rf_tune_test,rf_rmse,rf_tune_rmse,rf_r2score,rf_tune_r2score],'Support Vector Regressor':[svm_train,svm_test,'NA','NA',svm_rmse,'NA',svm_r2score,'NA'],\n          'KNearestNeighbor Regressor':[knn_train,knn_test,knn_tune_train,knn_tune_test,knn_rmse,knn_tune_rmse,knn_r2score,knn_tune_r2score],\n          'XG Boost Regressor':[xgb_train,xgb_test,xgb_tune_train,xgb_tune_test,xgb_rmse,xgb_tune_rmse,xgb_r2score,xgb_tune_r2score],\n          'Ada Boost Regressor':[ab_train,ab_test,ab_tune_train,ab_tune_test,ab_rmse,ab_tune_rmse,ab_r2score,ab_tune_r2score],\n          'Gradient Boosting Regressor':[gb_train,gb_test,gb_tune_train,gb_tune_test,gb_rmse,gb_tune_rmse,gb_r2score,gb_tune_r2score]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting dictionary to dataframe\nmetrics_df = pd.DataFrame(metrics_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe of metrics\nmetrics_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Perform the Stacking models Voting and Mlxtend and analyze the metrics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.1 Voting Regressor ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning estimator models for voting classifier\nvote_est = [('lr',lr),('ab',ab),('dt',dt)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Voting Regressor\nfrom sklearn.ensemble import VotingRegressor\nvote = VotingRegressor(estimators=vote_est)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\nvote.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the model\nvote_pred = vote.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing metrics\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Voting Regressor\nvote_rmse = np.sqrt(metrics.mean_squared_error(vote_pred,y_test))\nvote_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Voting Regressor\nvote_r2score = metrics.r2_score(vote_pred,y_test)\nvote_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Voting Regressor\nvote_train = vote.score(x_train,y_train)\nvote_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Voting Regressor\nvote_test = vote.score(x_test,y_test)\nvote_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.2 Mlxtend Stacking Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# mlxtend regressor\nfrom mlxtend.regressor import StackingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning individual models to variables\nxgb = XGBRegressor()\nada = AdaBoostRegressor()\ngrad = GradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\nst = StackingRegressor(regressors=[dt,ab,rf,xgb,ada,grad],meta_regressor=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\nst.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the model\nst_pred = st.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE score for Stacking Regressor\nst_rmse = np.sqrt(metrics.mean_squared_error(st_pred,y_test))\nst_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score for Stacking Regressor\nst_r2score = metrics.r2_score(st_pred,y_test)\nst_r2score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train score for Stacking Regressor\nst_train = st.score(x_train,y_train)\nst_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score for Stacking Regressor\nst_test = st.score(x_test,y_test)\nst_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Create a dataframe with model Stacking model names and metric scores and compare along with the first dataframe and give inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dictionary for all the metrics and converting it to dataframe\nmetrics_stack = {'Models': ['Voting Regressor','Stacking Regressor'],'RMSE score':[vote_rmse,st_rmse],'R2 Score':[vote_rmse,st_rmse],'Train score':[vote_train,st_train],'Test score':[vote_test,st_test]}\n\nmetrics_stack = pd.DataFrame(metrics_stack)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe\nmetrics_stack","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model being Voting Regressor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.5 Performing Vecstack","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Vecstack\nfrom vecstack import stacking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1st level model\nmodels = [lr,ab,dt,svm]\nS_train, S_test = stacking(models, x_train, y_train, x_test, \n    regression = True, metric = metrics.r2_score, n_folds = 4 , \n    shuffle = True, random_state = 0, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2nd level model\nmodels = [knn,xgb,grad,ada]\nS_train, S_test = stacking(models, x_train, y_train, x_test, \n    regression = True, metric = metrics.r2_score, n_folds = 4 , \n    shuffle = True, random_state = 0, verbose = 2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}