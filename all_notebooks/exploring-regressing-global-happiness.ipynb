{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"273c96cc-fff5-b165-a0ee-9c26d8a4825f"},"source":"This kernel is just a fun explorative project to get a feel for this very interesting dataset. I don't have any intention to accomplish anything besides enjoy doing some visualizations. At the end I'll run a linear regression and see how accurate I can get."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ccc7058-a11b-f1fb-7fdd-54bd599a313d"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12df8217-77cc-b56c-303a-43e64e1cdff1"},"outputs":[],"source":"data2015 = pd.read_csv('../input/2015.csv')\ndata2016 = pd.read_csv('../input/2016.csv')\n\ndata2015['Year'] = 2015\ndata2016['Year'] = 2016\ndata = pd.concat([data2015, data2016])\ndata = data.reset_index()\ndata.head()\ndata.tail()\ndata.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0586fb7c-8058-7b11-35ad-36e5e55df795"},"source":"Looks like we don't have upper and lower confidence intervals for all of our countries. Might we assume that this data was collected in 2016 but not in 2015? Let's look and see."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1aa36a2a-30b9-d60d-19fb-be673c402b7d"},"outputs":[],"source":"i = data['Lower Confidence Interval'][data['Year'] == 2015].isnull()\nassert i.any() == True\nprint(len(i))\nl = data['Lower Confidence Interval'][data['Year'] == 2016].isnull()\nassert l.any() == False\nprint(len(l))\nb = data['Standard Error'][data['Year'] == 2015].isnull()\nassert b.any() == False\nprint(len(b))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e80dae7-2c85-281c-aa41-298004e936be"},"source":"OK, well fortunately I have enough data that I can figure out the standard error for my 2016 data. I don't have enough information to figure out the upper/lower confidence intervals for 2015 so I'll simple drop both those columns and drive on with just my standard error for both years."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b9f66a9-53a7-bf46-bb7f-fe9fb194319a"},"outputs":[],"source":"midpoint = (data2016['Upper Confidence Interval'] - data2016['Lower Confidence Interval']) / 2\ndata2016['Sample Mean'] = data2016['Lower Confidence Interval'] + midpoint\ndata2016['Standard Error'] = midpoint / 1.96\n\ndel data2016['Sample Mean']\ndel data2016['Upper Confidence Interval']\ndel data2016['Lower Confidence Interval']\n\ndata = pd.concat([data2015, data2016])\ndata = data.reset_index()\ndata.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8b40b780-75ad-ffe3-ed55-113c006138ff"},"source":"OK, we have no null values and we're ready to begin doing some plotting. I'll start by throwing up some kernel density estimate plots with my various continuous variables arrayed against each countries happiness score."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1de20c79-9667-5023-a6c3-cf30140f9092"},"outputs":[],"source":"print(data.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"063aaeeb-29b8-2180-ca43-d373be07a43c"},"outputs":[],"source":"_ = sns.kdeplot(data=data['Happiness Score'], data2=data['Economy (GDP per Capita)'], shade=True)\n_ = plt.scatter(x=data['Happiness Score'], y=data['Economy (GDP per Capita)'], alpha=0.2, color='green')\n_ = plt.xlabel('Happiness Score')\n_ = plt.ylabel('Economy (GDP per Capita)')\n_ = plt.title('Happiness vs. GDP')\nplt.show()\n\n_ = sns.kdeplot(data=data['Happiness Score'], data2=data['Family'], shade=True)\n_ = plt.scatter(x=data['Happiness Score'], y=data['Family'], alpha=0.2, color='green')\n_ = plt.xlabel('Happiness Score')\n_ = plt.ylabel('Family')\n_ = plt.title('Happiness vs. Family')\nplt.show()\n\n_ = sns.kdeplot(data=data['Happiness Score'], data2=data['Freedom'], shade=True)\n_ = plt.scatter(x=data['Happiness Score'], y=data['Freedom'], alpha=0.2, color='green')\n_ = plt.xlabel('Happiness Score')\n_ = plt.ylabel('Freedom')\n_ = plt.title('Happiness vs. Freedom')\nplt.show()\n\n_ = sns.kdeplot(data=data['Happiness Score'], data2=data['Generosity'], shade=True)\n_ = plt.scatter(x=data['Happiness Score'], y=data['Generosity'], alpha=0.2, color='green')\n_ = plt.xlabel('Happiness Score')\n_ = plt.ylabel('Generosity')\n_ = plt.title('Happiness vs. Generosity')\nplt.show()\n\n_ = sns.kdeplot(data=data['Happiness Score'], data2=data['Health (Life Expectancy)'], shade=True)\n_ = plt.scatter(x=data['Happiness Score'], y=data['Health (Life Expectancy)'], alpha=0.2, color='green')\n_ = plt.xlabel('Happiness Score')\n_ = plt.ylabel('Health (Life Expectancy)')\n_ = plt.title('Happiness vs. Health (Life Expectancy)')\nplt.show()\n\n_ = sns.kdeplot(data=data['Happiness Score'], data2=data['Trust (Government Corruption)'], shade=True)\n_ = plt.scatter(x=data['Happiness Score'], y=data['Trust (Government Corruption)'], alpha=0.2, color='green')\n_ = plt.xlabel('Happiness Score')\n_ = plt.ylabel('Trust (Government Corruption)')\n_ = plt.title('Happiness vs. Trust (Government Corruption)')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"de69262e-bdc8-3df2-5029-d3be633444d0"},"source":"There are some interesting things to note here, chiefly that health, family, freedom, and wealth seem to correlate well with overall happiness while trust in government and generosity are less important. We can quantify this with a seaborn heatmap."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6608f405-f969-586a-ce67-211ca0249847"},"outputs":[],"source":"cols = ['Dystopia Residual', 'Economy (GDP per Capita)', 'Family', 'Freedom', 'Generosity', 'Happiness Score', 'Health (Life Expectancy)', 'Trust (Government Corruption)']\n\nheatmap = data[cols]\ncorr = heatmap.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, cmap=\"YlGnBu\", annot=True)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0c7fff7-b5dc-6cd8-aa1a-132c06a0aa3e"},"source":"Again, we see that there is a high degree of correlation between happiness and the economy, family, freedom, and health. Less important are trust in government (although at 40% certainly not something to ignore) and generosity.\n\nNow let's use some lag plots to see if our data is random. Lag plots are a neat tool in the pandas library \"used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random.\" You can check out the documentation [here][1].\n\n\n  [1]: http://pandas.pydata.org/pandas-docs/stable/visualization.html"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a158ed-350d-d14d-3d5d-f168b0b34abf"},"outputs":[],"source":"from pandas.tools.plotting import lag_plot\n\nplt.figure(1)\nlag_plot(data['Freedom'])\nplt.title('Freedom')\n\nplt.figure(2)\nlag_plot(data['Family'])\n_ = plt.title('Family')\n\nplt.figure(3)\nlag_plot(data['Dystopia Residual'])\n_ = plt.title('Dystopia Residual')\n\nplt.figure(4)\nlag_plot(data['Economy (GDP per Capita)'])\n_ = plt.title('Economy (GDP per Capita)')\n\nplt.figure(5)\nlag_plot(data['Generosity'])\n_ = plt.title('Generosity')\n\nplt.figure(6)\nlag_plot(data['Happiness Score'])\n_ = plt.title('Happiness Score')\n\nplt.figure(7)\nlag_plot(data['Health (Life Expectancy)'])\n_ = plt.title('Health (Life Expectancy)')\n\nplt.figure(8)\nlag_plot(data['Trust (Government Corruption)'])\n_ = plt.title('Trust (Government Corruption)')\n\nplt.tight_layout()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e16aa091-1f96-f6e4-4f30-12290b6f4303"},"source":"Well, the happiness score itself is decidedly non-random. The rest of these variables don't exhibit any particularly marked pattern to me.\n\nNow we'll use the radviz plots native to the pandas library. Per the documentation:\n\n\"RadViz is a way of visualizing multi-variate data. It is based on a simple spring tension minimization algorithm. Basically you set up a bunch of points in a plane. In our case they are equally spaced on a unit circle. Each point represents a single attribute. You then pretend that each sample in the data set is attached to each of these points by a spring, the stiffness of which is proportional to the numerical value of that attribute (they are normalized to unit interval). The point in the plane, where our sample settles to (where the forces acting on our sample are at an equilibrium) is where a dot representing our sample will be drawn. Depending on which class that sample belongs it will be colored differently.\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"243b6e91-9fb3-a781-eb63-4e03b9afa221"},"outputs":[],"source":"from pandas.tools.plotting import radviz\npd.options.mode.chained_assignment = None\n\ndel heatmap['Happiness Score']\nheatmap['Region'] = data['Region']\n\nplt.figure()\nradviz(heatmap, 'Region')\nplt.legend(bbox_to_anchor=(1,1))\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f9989301-0181-51fa-fbed-a0d3fe8db8fb"},"source":"We can see that Sub-Saharan Africa places more importance on family and economic success relative to the other variables. However, it is a bit hard to tell from this chart. I can bring out these tendencies in sharp relief by raising each value to an exponent. This dramatically accentuates the differences between the various values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b4b44e6-8b91-8a83-513b-57fceb33f05f"},"outputs":[],"source":"del heatmap['Region']\nheatmap = heatmap ** 6\nheatmap['Region'] = data['Region']\n\nplt.figure()\nradviz(heatmap, 'Region')\nplt.legend(bbox_to_anchor=(1,1))\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a21b152f-1833-eca9-74a8-bf71fcfce025"},"source":"It's a little clearer now that freedom, family, economy, and dystopia residual are more important to Sub-Saharan Africans than they are to the rest of the world's population. We have a few countries with decidedly unusual preferences for generosity, a good number from around the world that are more heavily influenced by health considerations, and a strong overall tendency towards the importance of freedom and family.\n\nIt's also interesting to note that trust in government is not decisive for anybody, apparently. Or, more accurately, that nobody has a high enough trust in government for that factor to be one of the most important ones.\n\nNext let's look at trends over time by region."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc78837e-287e-c3c1-0853-813f0f5f8227"},"outputs":[],"source":"order =['Sub-Saharan Africa', 'Southern Asia', 'Southeastern Asia', 'Eastern Asia', 'Australia and New Zealand', 'Central and Eastern Europe', 'Western Europe', 'Latin America and Caribbean', 'North America']\n\n\n_ = sns.barplot(x=data['Region'], y=data['Happiness Score'], order=order, hue=data['Year'], hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Average Happiness Score 2015-2016')\n_ = plt.title('Happiness by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Economy (GDP per Capita)', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Average GDP per Capita 2015-2016')\n_ = plt.title('GDP per Capita by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Freedom', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Freedom 2015-2016')\n_ = plt.title('Freedom by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Family', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Family 2015-2016')\n_ = plt.title('Family by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Health (Life Expectancy)', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Health (Life Expectancy) 2015-2016')\n_ = plt.title('Health (Life Expectancy) by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Trust (Government Corruption)', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Trust (Government Corruption) 2015-2016')\n_ = plt.title('Trust (Government Corruption) by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Generosity', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Generosity 2015-2016')\n_ = plt.title('Generosity by Region 2015-2016')\nplt.show()\n\n_ = sns.barplot(x='Region', y='Dystopia Residual', data=data, hue='Year', order=order, hue_order=[2015, 2016])\n_ = plt.xticks(rotation=75)\n_ = plt.xlabel('Regions')\n_ = plt.ylabel('Dystopia Residual 2015-2016')\n_ = plt.title('Dystopia Residual by Region 2015-2016')\nplt.show()\n\n\n\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"a444bef5-8d06-ab84-9246-380979519764"},"source":"So in general we can say that in 2015-2016:\n\n1. Generosity and Happiness were effectively unchanged.\n\n2. Dystopia and GDP rose.\n\n3. Life Expectancy, Family, and Freedom declined pretty much across the board.\n\n4. Trust in government decreased slightly, but was already so low it hardly seems to matter.\n\nAnd now, just because this is Kaggle and I can, I'll use a Random Forest to predict happiness scores using some of our measured variables. Let's see how accurately these numbers predict the happiness score. I would assume its pretty close to 100% since the happiness score is actually derived from these metrics, but there's no need to take that assumption for granted when we can test it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42fc19bd-2da5-2d23-36f4-9067b3159870"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\n\ndata.Region = LabelEncoder().fit_transform(data.Region)\ndata.Country = LabelEncoder().fit_transform(data.Country)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cca1618c-e65e-381c-1ba4-554253a01ef7"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as m\n\ntarget = data['Happiness Score']\nfeatures = data[['Country', 'Dystopia Residual', 'Economy (GDP per Capita)', 'Family', 'Freedom', 'Generosity', 'Health (Life Expectancy)', 'Region', 'Trust (Government Corruption)']]\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=42)\n\nmodel = LinearRegression().fit(X_train, y_train)\npredictions = model.predict(X_test)\nmae = m.mean_absolute_error(y_test, predictions)\nmse = m.mean_squared_error(y_test, predictions)\nprint(mae)\nprint(mse)\n\n_ = plt.hist(predictions, alpha=0.5, color='red', cumulative=True, normed=True, bins=len(predictions), histtype='stepfilled', stacked=True)\n_ = plt.hist(y_test, alpha=0.5, color='blue', cumulative=True, normed=True, bins=len(predictions), histtype='stepfilled', stacked=True)\nplt.show()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"c9f3c9dd-d1d6-4247-b372-cdfb8e7c5378"},"source":"Well, I have to say that's pretty accurate. My mean absolute and mean squared errors are tiny. \n\nTo illustrate the results I have plotted my results as cumulative, normed histograms. My predictions are red and the true values blue, so where the two overlap you see purple. Only where a little blue or a little red pops out was there some inaccuracy in the prediction.\n\nI think that's all for now. Please upvote if you like what you see, and leave a comment if you have any suggestions for improvement."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}