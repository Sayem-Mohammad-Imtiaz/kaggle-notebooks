{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Automating NPI labelling with tweet augmentations\n\n## Introduction\n\n**Question:** Can we automate the collection of NPIs?\n\n**Problem:** Performing an environmental scan to collect non-pharmaceutical interventions (NPIs) is laborious, and currently requires manual labelling from multiple reviewers. This manual system is time-consuming and prone to error as labelling is subjective. For example, in the Northwest Territories “cancellation of Arctic Winter Games” can be classified as a “Recreational / Entertainment Facility Closure\" but could fall under the “Public event/ meeting cancellation or postponement” category.\n\n**Sub-Problem**: While the CAN-NPI dataset is fairly comprehensive with about 2,546 rows, to train a robust model, we would need more data.\n\n**Long-term Goal:** Robustly predict the `intervention_category` for tweets and automate the collection of NPIs from twitter. \n\n**Short-term Goal:** Robustly predict a *single* `intervention_category` for tweets.\n\n**Solution:** A binary classification model where we augment the dataset with relevant tweets.\n\n**Take-aways:**\n* Using a tf-idf representation achieved generally higher F1-scores\n* A linear SVC model achieved an F1-score of ____\n* A method for augmenting more NPI data that increased our dataset from 2,546 rows to 42,367 rows!\n\nAs always, would love to get some feedback and any expertise :)\n"},{"metadata":{},"cell_type":"markdown","source":"\n## Prediction task\n\nPredict \"General case announcements\" from the free-text fields in CAN-NPI.\n\n## Data\n\nI use the Canadian NPI dataset (`covid19-challenges/npi_canada.csv`), and pull all tweets that contain media URLs of NPIs in Canadian NPI dataset with tweepy (`npi-twitterverse-april30/tweets_to_intervention_category.source_urls.tsv`).\n\n## Workflow\n\n1. **Data preprocessing:** extract `intervention_category`, `intervention_summary`, `source_title`, `source_full_text` fields in CAN-NPI only\n2. **Data augmentation:** merge tweets and CAN-NPI dataset\n3. **Data representation:** tf-idf and doc2vec\n4. **Model selection:** I evaluate 6 models (Logistic regression (LR), a random forest classifier (RF), a linear support vector classfier (LinearSVC), an Stochastic Gradient Descent (SGD) classifier, K-nearest neighbors (KNN) classifier, and a simple neural network (NN))."},{"metadata":{},"cell_type":"markdown","source":"## Method\n\n### Set up\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nfrom datetime import datetime, date, timedelta\nimport numpy as np\nimport re\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk \nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit,train_test_split, GroupShuffleSplit\nfrom langdetect import detect\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> "},{"metadata":{},"cell_type":"markdown","source":"### Load CAN-NPI dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get NPI data\nnpis_csv = \"/kaggle/input/covid19-challenges/npi_canada.csv\"\nraw_data = pd.read_csv(npis_csv,encoding = \"ISO-8859-1\")\n# remove any rows that don't have a start_date, region, or intervention_category\ndf = raw_data.dropna(how='any', subset=['start_date', 'region', 'intervention_category'])\ndf['region'] = df['region'].replace('Newfoundland', 'Newfoundland and Labrador')\nnum_rows_removed = len(raw_data)-len(df)\nprint(\"Number of rows removed: {}\".format(num_rows_removed))\n\n# get all regions\nregions = list(set(df.region.values))\nprint(\"Number of unique regions: {}\".format(len(regions)))\n\n# get all intervention categories\nnum_cats = list(set(df.intervention_category.values))\nnum_interventions = len(num_cats)\nprint(\"Number of unique intervention categories: {}\".format(len(num_cats)))\n\n# get earliest start date and latest start date\ndf['start_date'] = pd.to_datetime(df['start_date'], format='%Y-%m-%d')\nearliest_start_date = df['start_date'].min()\nlatest_start_date = df['start_date'].max()\nnum_days = latest_start_date - earliest_start_date\nprint(\"Analyzing from {} to {} ({} days)\".format(earliest_start_date.date(), latest_start_date.date(), num_days))\nprint(\"Total number of record from CAN-NPI = {}\".format(len(df)))\nprint(\"DONE READING DATA\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load relevant tweets\n\nTo get tweets only related to the NPIs in the CAN-NPI dataset, I pull any tweets that contains any of the `source_urls`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load tweets\nmerged_tweets_csv = '/kaggle/input/npi-twitterverse-april-30/tweets_to_intervention_category.source_urls.tsv'\ncolnames = [\"npi_record_id\", \"intervention_category\", \"oxford_government_response_category\", \"source_url\", \"id\", \"conversation_id\", \"created_at\", \"date\", \"time\", \"timezone\", \"user_id\", \"username\", \"name\", \"place\", \"tweet\", \"mentions\", \"urls\", \"photos\", \"replies_count\", \"retweets_count\", \"likes_count\", \"hashtags\", \"cashtags\", \"link\", \"retweet\", \"quote_url\", \"video\", \"near\", \"geo\", \"source\", \"user_rt_id\", \"user_rt\", \"retweet_id\", \"reply_to\", \"retweet_date\", \"translate\", \"trans_src\", \"trans_dest\"]\ntweets_df = pd.read_csv(merged_tweets_csv, encoding = \"utf-8\", error_bad_lines=False, engine='python', names=colnames)\n# drop any rows without tweets - aka any interventions supported by non-tweeted media urls\ntweets_df = tweets_df.dropna(how='any', subset=['npi_record_id', 'intervention_category', 'tweet'])\nprint(len(tweets_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge tweets and CAN-NPI dataset\n\nFor all the NPIs in CAN-NPI, I merge the `intervention_category`, `intervention_summary`, `source_title`, and `source_full_text` into a single `text` column.\n\nFor all the tweets, I pull all the tweets that were english only.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge twitter dataset and the npi dataset\n# npi dataset\ndata = []\nfor index, row in df.iterrows():\n    data.append([row['intervention_category'], str(row['intervention_summary']) + ' ' + str(row['source_title']) + ' ' + str(row['source_full_text'])])\n# tweet dataset\nfor index, row in tweets_df.iterrows():\n    # detect only english tweets\n    tweet = row['tweet'].strip()\n    if tweet != \"\":\n      language =\"\"\n      try:\n          language = detect(tweet)\n      except:\n          language = \"error\"\n      if language == \"en\":\n        data.append([row['intervention_category'], tweet])\nprint(len(data))\n# make it into a pandas dataframe\nfull_df = pd.DataFrame(data, columns=[\"intervention_category\", \"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-hot encodings - binary labels\n\nFor simplicity, I am going to predict a binary label where 1 indicates a \"General case announcement\" and 0 means every other intervention category. I chose \"General case announcement\" because it is one of the more abundant classes asides from \"Public case announcement\" which can greatly vary."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_binary_labels(df, target_category):\n  '''Return binary labels in a list\n  where 1 = category, 0 = everything else\n  '''\n  labels = pd.DataFrame(df['intervention_category'])\n  labels.loc[labels.intervention_category == target_category, 'intervention_category'] = 1\n  labels.loc[labels.intervention_category != 1, 'intervention_category'] = 0\n  labels = labels.intervention_category.values.tolist()\n  print(\"Number of {} = {}, Total = {} \".format(target_category, sum(labels), len(labels)))\n  return labels\n\n# binary labels for just one intervention category\ny = get_binary_labels(full_df, \"General case announcement\")\ny = np.asarray(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n  '''Return tokenized text with \n  removed URLs, usernames, hashtags, weird characters, repeated\n  characters, stop words, and numbers\n  '''\n  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text) # remove URLs\n  text = re.sub(\"[^\\w]\", \" \",  text).lower()\n  text = re.sub('@[^\\s]+', 'USER', text) # removes any usernames in tweets\n  text = re.sub(r'#([^\\s]+)', r'\\1', text) # remove the # in #hashtag\n  text = re.sub('[^a-zA-Z0-9-*. ]', ' ', text) # remove any remaining weird characters\n  words = word_tokenize(text)  # remove repeated characters (helloooooooo into hello)\n  ignore = set(stopwords.words('english'))\n  more_ignore = {'at', 'and', 'also', 'or', \"http\", \"ca\", \"www\", \"https\", \"com\", \"twitter\", \"html\", \"news\", \"link\"}\n  ignore.update(more_ignore)\n  #porter = PorterStemmer()\n  #cleaned_words_tokens = [porter.stem(w) for w in words if w not in ignore]\n  cleaned_words_tokens = [w for w in words if w not in ignore]\n  cleaned_words_tokens = [w for w in cleaned_words_tokens if w.isalpha()]\n\n  return cleaned_words_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data representations\n\nI tried two different data representations: tf-idf and doc2vec."},{"metadata":{},"cell_type":"markdown","source":"#### tf-idf representation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tfidf_rep(df):\n  '''Return array where each entry is a \n  tfidf represention of the NPI\n  '''\n  corpus = []\n  for index, row in df.iterrows():\n    # merge the texts and remove any stopwords, store as one string\n    clean_tokens = preprocess(row['text'])\n    clean_tokens_str = \" \".join(clean_tokens)\n    corpus.append(clean_tokens_str)\n\n  # tf-idf representation\n  vectorizer = TfidfVectorizer(max_features=5000)\n  d = vectorizer.fit_transform(corpus)\n  X = d.toarray().tolist()\n    \n  print(d.shape)\n\n  data = []\n  for l in X:\n    data.append(np.asarray(l))\n\n  return np.asarray(data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### doc2vec representation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tagged_doc_corpus(X, labels, tokens_only=False):\n  '''Return list of documents in gensim\n  document object format'''\n  corpus = []\n  i = 0\n  for index, row in X.iterrows():\n    # merge the texts and remove any stopwords, store as one string\n    clean_summary = preprocess(row[\"text\"])\n    # make binary label\n    if tokens_only:\n      corpus.append(clean_summary)\n    else:\n      corpus.append(gensim.models.doc2vec.TaggedDocument(clean_summary, [labels[i]]))\n    i+=1\n  return corpus\n\n# CODE FROM: https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-p\ndef train_doc2vec(df, y, category):\n  '''Return a trained doc2vec model\n  '''\n  # split into train and test\n  X_train, X_test, y_train, y_test = train_test_split(df, y, train_size=0.70,test_size=0.30, random_state=101)\n  train_corpus = get_tagged_doc_corpus(X_train, y_train)\n  test_corpus = get_tagged_doc_corpus(X_test, y_test, True)\n\n  model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=40)\n  model.build_vocab(train_corpus)\n  model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n\n  return model\n\ndef get_doc2vec_rep(model, y, df):\n  '''Return entire dataframe in a list\n  where each entry is the doc2vec vector\n  for each NPI record'''\n  X = []\n  # gets tokenized versions of each text\n  corpus = get_tagged_doc_corpus(df, y, True)\n  for doc_id in range(len(corpus)):\n    inferred_vector = model.infer_vector(corpus[doc_id])\n    #sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n    X.append(inferred_vector)\n  return np.asarray(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Store all representations"},{"metadata":{"trusted":true},"cell_type":"code","source":"reps = {}\n\n# tfidf representation\nreps[\"tfidf\"] = get_tfidf_rep(full_df)\n\n# doc2vec representation\n#model = train_doc2vec(full_df, y, \"General case announcement\")\n#reps[\"doc2vec\"] = get_doc2vec_rep(model, y, full_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualize representations\n\nWe can visualize the representations with principal component analysis (PCA)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef run_pca(X, labels, rep_name):\n    pca = PCA(n_components=2)\n    principalComponents = pca.fit_transform(X)\n    principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2'])\n    finalDf = pd.concat([principalDf, labels], axis = 1)\n\n    # choose a color palette with seaborn.\n    num_classes = len(np.unique(labels))\n    print(num_classes)\n    palette = np.array(sns.color_palette(\"husl\", num_classes))\n    print(palette)\n\n    plt.figure(figsize=(16,10))\n    sns.scatterplot(\n        x=\"pc1\", y=\"pc2\",\n        hue=\"labels\",\n        palette=palette,\n        s=60,\n        data=finalDf,\n        legend=\"full\",\n        alpha=0.5\n    )\n\n    margin = 0.05\n    plt.xlim(min(finalDf['pc1'])-margin, max(finalDf['pc1'])+margin)\n    plt.ylim(min(finalDf['pc2'])-margin, max(finalDf['pc2'])+margin)\n    plt.title('PCA analysis ' + rep_name, fontsize = 15)\n    plt.xlabel('Principal Component 1', fontsize=12)\n    plt.ylabel('Principal Component 2', fontsize=12)\n\n    plt.savefig('pca_' + rep_name + '.png', dpi=1000)\n    plt.show()\n\n\n# visualize representations\nfor r in reps:\n    run_pca(reps[r], pd.DataFrame(y, columns = ['labels']), r)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model selection\n\nI evaluate 6 models for this binary classification task: logistic regression, random forest classifier, linear support vector classifier, SGD classifier, a K-nearest neighbour "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom sklearn.metrics import f1_score\n \n\ndef create_nn(X_train):\n    input_dim = X_train.shape[1]  # Number of features\n\n\n    model = Sequential()\n    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy'])\n    return model\n\ndef custom_cross_val_score(model, X, labels, scoring, cv): \n    scores = []\n\n    for train_index,test_index in cv.split(X,labels):\n        x_train,x_test=X[train_index],X[test_index]\n        y_train,y_test=labels[train_index],labels[test_index]\n\n        model=create_nn(x_train)\n        model.fit(x_train, y_train,epochs=20)\n\n        y_pred = model.predict_classes(x_test)\n        score = f1_score(y_pred, y_test, average='macro')\n        scores.append(score)\n\n    return scores\n\ndef find_best_models(X, labels, rep_name):\n    neural_network = KerasClassifier(build_fn=create_nn, \n                                 epochs=10, \n                                 batch_size=100, \n                                 verbose=0)\n    models = [\n      RandomForestClassifier(n_estimators=20, max_depth=3, random_state=0),\n      LinearSVC(),\n      LogisticRegression(random_state=0),\n      SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None),\n      KNeighborsClassifier(n_neighbors=3),\n      neural_network\n    ]\n    CV = 5\n    cv_df = pd.DataFrame(index=range(CV * len(models)))\n    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n    entries = []\n    max_acc = -9\n    best_model = \"\"\n    for model in models:\n        model_name = model.__class__.__name__\n        accuracies = []\n        if model_name == \"KerasClassifier\":\n            accuracies = custom_cross_val_score(model, X, labels, scoring='f1_macro', cv=sss)\n        else:\n            accuracies = cross_val_score(model, X, labels, scoring='f1_macro', cv=sss)\n        if max(accuracies) > max_acc:\n            max_acc = max(accuracies)\n            best_model = model\n        for fold_idx, accuracy in enumerate(accuracies):\n            entries.append((model_name, fold_idx, accuracy))\n        print(\"{} with {}\".format(model_name, str(accuracies)))\n\n    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'F1-score macro'])\n    \n    return cv_df, best_model\n\ndef plot_model_perf(cv_df, rep_name):\n    # plotting\n    plt.figure( figsize=(18, 10), dpi=200, facecolor='w', edgecolor='k')\n\n    # configure plot\n    SMALL_SIZE = 6\n    MEDIUM_SIZE = 10\n    BIGGER_SIZE = 20\n\n    plt.rc('font', size=MEDIUM_SIZE)         # controls default text sizes\n    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\n    sns.boxplot(x='model_name', y='F1-score macro', data=cv_df)\n    sns.stripplot(x='model_name', y='F1-score macro', data=cv_df, \n                size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n    plt.xlabel(\"Model\", fontsize=15)\n    plt.xticks(fontsize=11)\n    plt.yticks(fontsize=11)\n    plt.ylim([0.0, 1.0])\n    plt.ylabel(\"F1-score macro\", fontsize=11)\n    plt.title(\"Model selection - binary classification\\nPredict General Case Announcement\\n\"+rep_name, fontsize = 15)\n\n    #plt.figure( figsize=(8, 11), dpi=200, facecolor='w', edgecolor='k')\n    plt.savefig(\"model_selection.\" + rep_name + \".f1_score.png\")\n\n# =============================\n# TRAIN MODELS \n# =============================\ncv_scores = {}\nbest_models = {}\nfor r in reps:\n    cv_scores[r], best_model = find_best_models(reps[r], y, r)\n    plot_model_perf(cv_scores[r], r)\n    best_models[r] = best_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nThe plots below indicate that using the tf-idf representation yields overall better f1-scores.\nThe best f1-scores were achieved using a LinearSVC with the tf-idf representation.\n\n![title](model_selection.tfidf.f1_score.png)\n![title](model_selection.doc2vec.f1_score.png)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}