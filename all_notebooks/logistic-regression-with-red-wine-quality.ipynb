{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n\nIn this kernel, I will explain how we can implement the logistic regression algorithm from scratch and using the sklearn library.\n    \n* [Logistic Regression From Scratch](#1)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.quality.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is more than two quality measures, but logistic regression gives us only two classes. We need to reduce quality into two classes which are: 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.quality = [1 if each > 7 else 0 for each in data.quality]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To classify quality 0 and 1, I used 'greater than 7' condition because I tried two different values 6 and 7. 7 gives better accuracy to both training and testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can start implementing the logistic regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.quality = data.quality.astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '1'></a>\n## Logistic Regression From Scratch\n\nIn this part, I will implement logistic regression method from scratch. You can see the steps below.\n\n* Train - Test Splitting\n* Initialize w and b and Sigmoid Function"},{"metadata":{},"cell_type":"markdown","source":"### Train - Test Splitting\n\nTo split train and test sets, I will use train_test_split method from sklearn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.iloc[:,0:11].values\ny = data.iloc[:,[11]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initialize w and b and Sigmoid Function"},{"metadata":{},"cell_type":"markdown","source":"To initialize weights and bias, I implemented a function named 'initialize_w_b' and I take one parameter: dimension. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_w_b(dimension):\n    \n    w = np.full((dimension,1), 0.1)\n    b = 0.0\n    return w, b\n\n#initializes a weight with dimensions (dimension,1) and return it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    sig = 1 / (1 + np.exp(-x))\n    return sig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forward and Backward Propagation\nIn this step, I will implement both forward and backward propagations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w, b, x_train, y_train):\n    \n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    \n    loss = - y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) / x_train.shape[1]\n    \n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) / x_train.shape[1]\n    derivative_bias = (np.sum(y_head - y_train)) / x_train.shape[1]\n    \n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    return cost, gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"We need to do the propagation as number of iterations in our algorithm and we must update the weight and bias using the learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate, epochs):\n    \n    costs = []\n    costs2 = []\n    index = []\n    \n    for iteration in range(epochs):\n        \n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        costs.append(cost)\n        \n        w = w - (learning_rate * gradients[\"derivative_weight\"])\n        b = b - (learning_rate * gradients[\"derivative_bias\"])        \n        \n        if iteration % 10 == 0:\n            costs2.append(cost)\n            index.append(iteration)\n            print (\"Cost after iteration %i: %f\" %(iteration, cost))\n            \n    \n    param = {\"w\" : w, \"b\" : b}\n    plt.plot(index, costs2)\n    plt.xticks(index, rotation = 90)\n    plt.xlabel(\"Num of Iterations\")\n    plt.ylabel(\"Costs\")\n    plt.show()\n    \n    return param, gradients, costs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w, b, x_test):\n    \n    z = sigmoid(np.dot(w.T, x_test) + b)\n    y_pred = np.zeros((1, x_test.shape[1]))\n    \n    for iteration in range(z.shape[1]):\n        \n        if z[0, iteration] <= 0.5:\n            z[0, iteration] = 0\n        else:\n            z[0, iteration] = 0\n            \n    return y_pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, epochs):\n    \n    dimension = x_train.shape[0]\n    w, b = initialize_w_b(dimension)\n    \n    param, gradients, costs = update(w, b, x_train, y_train, learning_rate, epochs)\n    \n    y_pred_test = predict(param[\"w\"], param[\"b\"], x_test)\n    y_pred = predict(param[\"w\"], param[\"b\"], x_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_pred - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, epochs = 300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression with sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = linear_model.LogisticRegression(random_state = 42,max_iter= 150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(x_train.T, y_train.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test accuracy: \".format(lr.score(x_train.T, y_train.T)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}