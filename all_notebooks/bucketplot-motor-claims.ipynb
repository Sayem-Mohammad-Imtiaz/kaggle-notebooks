{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bucket plot visualisations for motor insurance claim frequency models\nUsing an open source data set of French Motor Claims as an example to demonstrate how `bucketplot` visualisations can be used in the modelling process.\n\nThis notebook is available in the following locations. These versions are kept in sync *manually* - there should not be discrepancies, but it is possible.\n- On Kaggle: <https://www.kaggle.com/btw78jt/bucketplot-motor-claims>\n- In the GitHub project repo: <https://github.com/A-Breeze/bucketplot>. See the project `README.md` for important information and further instructions.\n\n<!-- This table of contents is updated *manually* -->\n# Contents\n1. [Setup](#Setup)\n1. [Modelling data](#Modelling-data): Load data, Subset, Pre-processing\n1. [Build models](#Build-models): Split for modelling, Mean model, Vehicle-only model, Simple features model, Score\n1. [Bucket plot visualisation](#Bucket-plot-visualisation): Motivation, Steps, Data types, Examples to use\n1. [Assigning buckets](#Assigning-buckets): [divide_n](#divide_n), [custom_width](#custom_width), [weighted_quantiles](#weighted_quantiles), [all_levels](#all_levels)\n1. [Group and aggregate](#Group-and-aggregate): NOT COMPLETE\n1. [Plot](#Plot): NOT COMPLETE\n1. [Worked examples](#Worked-example): NOT COMPLETE\n1. [Rough work](#Rough-work)"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set warning messages\nimport warnings\n# Show all warnings in IPython\nwarnings.filterwarnings('always')\n# Ignore specific numpy warnings (as per <https://github.com/numpy/numpy/issues/11788#issuecomment-422846396>)\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n# Other warnings that sometimes come up\nwarnings.filterwarnings(\"ignore\", message=\"unclosed file <_io.TextIOWrapper\")\nwarnings.filterwarnings(\"ignore\", message=\"Anscombe residuals currently unscaled\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine whether this notebook is running on Kaggle\nfrom pathlib import Path\n\non_kaggle = False\nprint(\"Current working directory: \" + str(Path('.').absolute()))\nif str(Path('.').absolute()) == '/kaggle/working':\n    on_kaggle = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import built-in modules\nimport sys\nimport platform\nimport os\nfrom pathlib import Path\nimport functools\nimport inspect\n\n# Import external modules\nfrom IPython import __version__ as IPy_version\nimport numpy as np\nimport pandas as pd\nimport bokeh\nimport bokeh.palettes\nimport bokeh.io\nimport bokeh.plotting\nfrom sklearn import __version__ as skl_version\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom statsmodels import __version__ as sm_version\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Import project modules\nif not on_kaggle:\n    # Allow modules to be imported relative to the project root directory\n    from pyprojroot import here\n    root_dir_path = here()\n    if not sys.path[0] == root_dir_path:\n        sys.path.insert(0, str(root_dir_path))\nimport bucketplot as bplt\n\n# For development, allow the project modules to be reloaded every time they are used\n%load_ext autoreload\n%aimport bucketplot\n%autoreload 1\n\n# Check they have loaded and the versions are as expected\nassert platform.python_version_tuple() == ('3', '6', '6')\nprint(f\"Python version:\\t\\t{sys.version}\")\nassert IPy_version == '7.13.0'\nprint(f'IPython version:\\t{IPy_version}')\nassert np.__version__ == '1.18.2'\nprint(f'numpy version:\\t\\t{np.__version__}')\nassert pd.__version__ == '0.25.3'\nprint(f'pandas version:\\t\\t{pd.__version__}')\nassert bokeh.__version__ == '2.0.1'\nprint(f'bokeh version:\\t\\t{bokeh.__version__}')\nassert skl_version == '0.22.2.post1'\nprint(f'sklearn version:\\t{skl_version}')\nassert mpl.__version__ == '3.2.1'\nprint(f'matplotlib version:\\t{mpl.__version__}')\nassert sm_version == '0.11.0'\nprint(f'statsmodels version:\\t{sm_version}')\nprint(f'bucketplot version:\\t{bplt.__version__}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the matplotlib defaults\nplt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = 8, 6\n\n# Load Bokeh for use in a notebook\nbokeh.io.output_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output exact environment specification, in case it is needed later\nprint(\"Capturing full package environment spec\")\nprint(\"(But note that not all these packages are required)\")\n!pip freeze > requirements_snapshot.txt\n!jupyter --version > jupyter_versions_snapshot.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configuration variables\nif on_kaggle:\n    claims_data_filepath = Path('/kaggle/input/french-motor-claims-datasets-fremtpl2freq/freMTPL2freq.csv')\nelse:\n    claims_data_filepath = Path('freMTPL2freq.csv')\nif claims_data_filepath.is_file():\n    print(\"Correct: CSV file is available for loading\")\nelse:\n    print(\"Warning: CSV file not yet available in that location\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Modelling data"},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"expected_dtypes = {\n    **{col: np.dtype('int64') for col in [\n        'IDpol', 'ClaimNb', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density']},\n    **{col: np.dtype('float64') for col in ['Exposure']},\n    **{col: np.dtype('O') for col in ['Area', 'VehBrand', 'VehGas', 'Region']},\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# The first download can take approx 1 min on Binder\nif not claims_data_filepath.is_file():\n    from sklearn.datasets import fetch_openml\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", category=UserWarning,\n            message='Version 1 of dataset freMTPL2freq is inactive'\n        )\n        print(\"Fetching data...\")\n        df_raw = fetch_openml(data_id=41214, as_frame=True, cache=False).frame.apply(\n            lambda col_sers: col_sers.astype(expected_dtypes[col_sers.name])\n        ).sort_values('IDpol').reset_index(drop=True)\n    # Cache data within the repo, so we don't have to download it many times\n    print(\"Saving data...\")\n    df_raw.to_csv(claims_data_filepath, index=False)\n    print(\"Save complete\")\nelse:\n    print(\"Loading data from CSV...\")\n    df_raw = pd.read_csv(\n        claims_data_filepath, delimiter=',', dtype=expected_dtypes,\n        # Get index sorted with ascending IDpol, just in case it is out or order\n    ).sort_values('IDpol').reset_index(drop=True)\n    print(\"Load complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reasonableness checks that it has loaded as expected\nnRows, nCols = (678013, 12)\nassert df_raw.shape == (nRows, nCols)\nprint(f\"Correct: Shape of DataFrame is as expected: {nRows} rows, {nCols} cols\")\nassert df_raw.dtypes.equals(pd.Series(expected_dtypes)[df_raw.columns])\nprint(\"Correct: Data types are as expected\")\nassert df_raw.isna().sum().sum() == 0\nprint(\"Correct: There are no missing values in the dataset\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Subset\nThis notebook is about visualisation, rather than model building, so we can choose a smaller sample of the data to allow the code to run quickly. If you are working on a larger system, consider increasing the sample size to the whole data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hard-coded stats for reasonableness checks\nmean_approx = pd.Series({\n    'ClaimNb': 0.0532,\n    'Exposure': 0.5288,\n    'VehPower': 6.45,\n    'VehAge': 7.04,\n    'DrivAge': 45.50,\n    'BonusMalus': 59.8,\n    'Density': 1792.,\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows_sample = int(1e4)\nif not on_kaggle or on_kaggle:  # <<<<<<<<<<<<<<<<<<< TODO: redo this\n    df_raw, df_unused = train_test_split(\n        df_raw, train_size=nrows_sample, random_state=35, shuffle=True\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check it is as expected, within reason\ntol_pc = 0.05\ndf_sample_means = df_raw[mean_approx.index].mean()\nassert df_sample_means.between(\n    mean_approx * (1 - tol_pc),\n    mean_approx * (1 + tol_pc)\n).all()\nprint(\"Correct: Reasonableness checks have passed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_extra(df_raw):\n    \"\"\"\n    Given a DataFrame of that contains the raw data columns (and possibly additional columns), \n    return the DataFrame with additional pre-processed columns\n    \"\"\"\n    df_extra = df_raw.copy()\n    \n    # Calculate frequency per year on each row\n    df_extra['Frequency'] = df_extra['ClaimNb'] / df_extra['Exposure']\n    \n    return(df_extra)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run pre-processing to get a new DataFrame\ndf = get_df_extra(df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"expl_var_names = [\n    col_name for col_name in df.columns.to_list() \n     if col_name not in ['IDpol', 'ClaimNb', 'Exposure', 'Frequency']\n]\nprint(\"Explanatory variables\\n\" + '\\t'.join(expl_var_names))\nsimple_features = expl_var_names[:9]\nprint(\"\\nOf which the following are simple features\\n\" + '\\t'.join(simple_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Build models\n## Split modelling data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split training into train and test\ndf_train, df_test = train_test_split(\n    df, test_size=0.3, random_state=34, shuffle=True\n)\nprint(\"Train sample size: \" + str(df_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add indicator column\ndf = df.assign(\n    split=lambda df: np.select(\n        [df.index.isin(df_train.index)],\n        ['Train'],\n        default='Test'\n    )\n)\ndf['split'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nGLMres_mean = smf.glm(\n    \"ClaimNb ~ 1\",\n    data=df_train, exposure=np.asarray(df_train['Exposure']),\n    family=sm.families.Poisson(sm.genmod.families.links.log()),\n).fit()\nprint(GLMres_mean.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check that this is the mean model\nmean_mod_pred = np.exp(GLMres_mean.params[0])\nassert np.abs(\n    GLMres_mean.family.link.inverse(GLMres_mean.params[0]) - \n    GLMres_mean.predict(pd.DataFrame([1]))[0]\n) < 1e-10\nassert np.abs(\n    df_train.ClaimNb.sum() / df_train.Exposure.sum() - \n    mean_mod_pred\n) < 1e-10\nprint(\"Correct: Reasonableness tests have passed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vehicle-only model"},{"metadata":{"trusted":true},"cell_type":"code","source":"veh_features =['VehPower', 'VehAge', 'VehBrand', 'VehGas']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Takes a few secs\nGLMres_veh = smf.glm(\n    \"ClaimNb ~ \" +  ' + '.join(veh_features),\n    data=df_train, exposure=np.asarray(df_train['Exposure']),\n    family=sm.families.Poisson(sm.genmod.families.links.log()),\n).fit()\nprint(GLMres_veh.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple factors model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Takes approx 10 secs\nGLMres_simple = smf.glm(\n    \"ClaimNb ~ \" +  ' + '.join(simple_features),\n    data=df_train, exposure=np.asarray(df_train['Exposure']),\n    family=sm.families.Poisson(sm.genmod.families.links.log()),\n).fit()\nprint(GLMres_simple.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score data\nAll data (training and test rows) on both models"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Score all data (training and test)\ndf = df.assign(\n    Freq_pred_mean=lambda df: GLMres_mean.predict(df),\n    Freq_pred_veh=lambda df: GLMres_veh.predict(df),\n    Freq_pred_simple=lambda df: GLMres_simple.predict(df),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check reasonableness\n# The actual sum of ClaimNB should exactly match each model's predicted sum on the training data\npred_claims_df = df.assign(\n    ClaimNb_pred_mean=lambda df: df['Freq_pred_mean'] * df['Exposure'],\n    ClaimNb_pred_veh=lambda df: df['Freq_pred_veh'] * df['Exposure'],\n    ClaimNb_pred_simple=lambda df: df['Freq_pred_simple'] * df['Exposure'],\n).groupby('split').agg(\n    n_obs=('split', 'size'),\n    Exposure=('Exposure', 'sum'),\n    ClaimNb=('ClaimNb', 'sum'),\n    ClaimNb_pred_mean=('ClaimNb_pred_mean', 'sum'),\n    ClaimNb_pred_veh=('ClaimNb_pred_veh', 'sum'),\n    ClaimNb_pred_simple=('ClaimNb_pred_simple', 'sum'),\n)\nassert np.max(np.abs(\n    pred_claims_df.loc['Train'].iloc[-3:] - pred_claims_df.loc['Train', 'ClaimNb']\n)) < 1e-8\npred_claims_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Bucket plot visualisation\n## Motivation\nSuppose we want to view the one-way of how the response `Frequency` varies according to some other variable. Let's try a scatter plot for `DrivAge`."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_var, stat_var = 'DrivAge', 'Frequency'\ndf.plot.scatter(x_var, stat_var)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This isn't very helpful because:\n- With a large amount of data, a scatter plot doesn't give a good indication of where it is most concentrated. Points overlap.\n- For imbalanced count (or frequency) data, a large proportion of the data rows have a response of zero.\n- The plot must cover the whole range, including any outliers on either axis.\n\nWhat we want to do is partition the x-axis variable into *buckets*, and plot the *average* respose in each bucket. At the same time, we also want to plot along with the *distribution* of the x-axis variable, to give a sense of the relative credbility of the estimate from each bucket. As follows, this is a task for `pd.cut()` + `groupby` + `agg`. "},{"metadata":{},"cell_type":"markdown","source":"## Steps\n1. Assign rows to buckets. \n    - *Every* row is assigned to one bucket. There may be buckets that contain no rows (use `n_obs` aggregate variable to determine this).\n    - The buckets are `Categorical` with each bucket being an `Interval`. There are no gaps between the intervals. The bucket can be missing (i.e. `NaN`), in which case the row will be excluded on grouping the data.\n1. Group the data by bucket. Within each bucket consider the distribution of columns values *weighted* by `stat_wgt`. Calculate aggregate figures for each bucket:\n    - *x* coordinate edges: (`x_left`, `x_right`). Must not overlap (but there can be gaps). Must be `x_left` < `x_right` *or* both `NaN`.\n    - Sum of `stat_wgt` to be plotted as a histogram\n    - One point coordinate per statistic to be plotted at (`x_point`, `stat_val`)\n1. Plot "},{"metadata":{},"cell_type":"markdown","source":"## Data types\nData types of individual columns for bucket plots:\n- Numeric (`int` or `float`)\n    - Continuous = a high number (or high proportion) of unique values\n        - Pure\n        - With concentration at one value\n    - Discrete / ordinal = a low number of unique values. Ordered. May have categories with no occurrences in the data.\n- Non-numeric (`str`)\n    - Ordinal = as above.\n    - Nominal = finite number of categories, not ordered.\n- Others: not appropriate to plot\n\nOther specifications:\n- Weights:\n    - All non-negative with at least one positive.\n    - Might contain repeated values."},{"metadata":{},"cell_type":"markdown","source":"## Examples to use"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at first few rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weights\n# df['Exposure'] - none are zero\n# df['ClaimNb'] - many are zero\n\n# Numeric\n# df['Density'] - close to continuous with a large skew\n# df['DrivAge'] - between discrete (ordinal) and continuous\n# df['Exposure'] - close to continuous, odd distribution\n# df['Frequency'] - continuous with a concentration at 0\n# df['Freq_pred_veh'] and df['Freq_pred_simple'] - continuous and positive\n\n# Non-numeric\n# df['Area'] - nominal with a low number of levels\n# df['Region'] and df['VehBrand'] - nominal with a higher number of levels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Assigning buckets\nEvery method:\n- Takes a DataFrame `df` and the name `bucket_var` of a column.\n- Uses other arguments to append a column `bucket` of which bucket each row is assigned to. The column data type will be `Categorical` where the categories are:\n    - `Interval`s for numeric bucket methods.\n    - Single values for non-numeric bucket methods.\n- Returns the enlarged `df`.\n\nPossible methods:\n- Numeric:\n    - `divide_n`: Split range into `n_bins` equal width buckets. Any numeric data. For discrete data, want `n_bins` to be much smaller than the number of unique values.\n    - `custom_width`: Specify the `width` and `boundary` point. Options for `first_break` and `last_break` for larger width buckets at either end. Any numeric data.\n    - `weighted_quantile`: Quantiles of `bucket_var` weighted by `bucket_wgt` (which can be special value `const`). Aim for `n_bins` but note that there are only a finite number of cut points (especially when `bucket_var` is discrete or is concentrated at only a few points).\n- Non-numeric:\n    - `all_levels`: One bucket for each level.\n\nOther methods would also be possible."},{"metadata":{},"cell_type":"markdown","source":"### Technicalities\n- An interval index can only contain half-open intervals that are all closed on the same side, i.e. all $(a,b]$ or $[a,b)$. We will *always* stick to the default of closed on the *right*. To ensure all rows fall within a bucket, `pd.cut()` extends the bottom bucket by 0.1% of the entire range (when passing an `int` to `bins`). In any custom implementations, we replicate this convention."},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n## divide_n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def divide_n(df, bucket_var, n_bins=10, bucket_col='bucket'):\n    \"\"\"\n    Assign each row of `df` to a bucket by dividing the range of the \n    `bucket_var` column into `n_bins` number of equal width intervals.\n    \n    df: DataFrame\n    bucket_var: Name of the column of df to use for dividing\n    n_bins: positive integer number of buckets\n    bucket_col: Name of the resulting `bucket` column\n    \n    Returns: df with the additional `bucket` column \n        The `bucket` column is Categorical data type consisting of Intervals\n        that partition the interval from just below min(bucket_var) to \n        max(bucket_var).\n    \"\"\"\n    df_w_buckets = df.assign(**{bucket_col: (\n        lambda df: pd.cut(df[bucket_var], bins=n_bins)\n    )})\n    return(df_w_buckets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bucket_var = 'Density'\n# bucket_var = 'Exposure'\n# bucket_var = 'DrivAge'\ntmp1 = df.pipe(divide_n, bucket_var, 10)\ntmp1.groupby('bucket').agg(\n    n_obs=('bucket', 'size'),\n    stat_wgt_sum=('Exposure', 'sum'),\n    stat_sum=('ClaimNb', 'sum'),\n    x_min=(bucket_var, 'min'),\n    x_max=(bucket_var, 'max'),\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Edge cases\n# Resulting bucket with no obs\npd.Series([0, 1]).to_frame('val').pipe(\n    divide_n, 'val', 3\n).groupby('bucket').agg(n_rows=('bucket', 'size'))\n\n# Constant bucket_var\npd.Series([0, 0]).to_frame('val').pipe(\n    divide_n, 'val', 2\n).groupby('bucket').agg(n_rows=('bucket', 'size'))\n\n# n_bins = 1\npd.Series([0, 1]).to_frame('val').pipe(\n    divide_n, 'val', 1\n).groupby('bucket').agg(n_rows=('bucket', 'size'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing vals\nunit_w_miss = pd.Series([0, 1, np.nan]).to_frame('val').pipe(\n    divide_n, 'val', 3\n)\ndisplay(unit_w_miss)  # Given a bucket 'NaN'...\ndisplay(  # ...which is not included after grouping\n    unit_w_miss.groupby('bucket').agg(n_rows=('bucket', 'size'))  \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use a missing indicator to cope with missing values\nunit_filled = pd.Series([0, 1, np.nan]).to_frame('val').assign(\n    val_miss_ind=lambda df: df.val.isna() + 0,\n    val=lambda df: df.val.fillna(0),\n).pipe(\n    divide_n, 'val', 3).pipe(\n    # Would be more natural to use all_levels() in this case\n    divide_n, 'val_miss_ind', 2, 'bucket_miss_ind'  \n)\ndisplay(unit_filled)\nunit_filled.groupby(['bucket_miss_ind', 'bucket']).agg(\n    n_rows=('bucket', 'size')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n## custom_width"},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_width(\n    df, bucket_var,\n    width, boundary=0,\n    first_break=None, last_break=None,\n    bucket_col='bucket'\n):\n    \"\"\"\n    Assign each row of `df` to a bucket by dividing the range of the \n    `bucket_var` column into `n_bins` number of equal width intervals.\n    \n    df: DataFrame\n    bucket_var: Name of the column of df to use for dividing.\n    width: Positive width of the buckets\n    boundary: Edge of one of the buckets, if the data extended that far\n    first_break: All values below this (if any) are grouped into one bucket\n    last_break: All values above this (if any) are grouped into one bucket\n    bucket_col: Name of the resulting `bucket` column\n    \n    Returns: df with the additional `bucket` column \n        The `bucket` column is Categorical data type consisting of Intervals\n        that partition the interval from just below min(bucket_var) to \n        max(bucket_var).\n    \"\"\"\n    var_min, var_max = df[bucket_var].min(), df[bucket_var].max()\n    extended_min = var_min - 0.001 * np.min([(var_max - var_min), width])\n\n    # Set bucket edges\n    start = np.floor((extended_min - boundary) / width) * width + boundary\n    stop = np.ceil((var_max - boundary) / width) * width + boundary\n    num = int((stop - start) / width) + 1\n    breaks_all = np.array([\n        extended_min,\n        *np.linspace(start, stop, num)[1:-1],\n        var_max,\n    ])\n    \n    # Clip lower and upper buckets\n    breaks_clipped = breaks_all\n    if first_break is not None or last_break is not None:\n        breaks_clipped = np.unique(np.array([\n            breaks_all.min(),\n            *np.clip(breaks_all, first_break, last_break),\n            breaks_all.max(),\n        ]))\n    \n    df_w_buckets = df.assign(**{bucket_col: (\n        lambda df: pd.cut(df[bucket_var], bins=breaks_clipped)\n    )})\n    return(df_w_buckets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bucket_var, width, boundary, first_break, last_break = 'DrivAge', 3, 17.5, None, None\n# bucket_var, width, boundary, first_break, last_break = 'DrivAge', 3, 0.5, 30, 70\n# bucket_var, width, boundary, first_break, last_break = 'DrivAge', 100, 0.5, None, None\nbucket_var, width, boundary, first_break, last_break = 'Density', 100, 0.5, None, 1500.5\ntmp6 = custom_width(df, bucket_var, width, boundary, first_break, last_break)\ntmp6_grpd = tmp6.groupby('bucket').agg(\n    n_obs=('bucket', 'size'),\n    stat_wgt_sum=('Exposure', 'sum'),\n    stat_sum=('ClaimNb', 'sum'),\n    x_min=(bucket_var, 'min'),\n    x_max=(bucket_var, 'max'),\n    x_nunique=(bucket_var, 'nunique'),\n).assign(\n    bucket_width=lambda df: df.index.categories.length\n)\ntmp6_grpd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n## weighted_quantiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_quantiles(df, bucket_var, n_bins=10, bucket_wgt=None, bucket_col='bucket'):\n    \"\"\"\n    Assign each row of `df` to a bucket by splitting column `bucket_var`\n    into `n_bins` weighted quantiles, weighted by `bucket_wgt`.\n    \n    bucket_var: Column name of the values to find the quantiles.\n        Must not be constant (i.e. just one value for all rows).\n    n_bins: Target number of quantiles, but could end up with fewer because\n        there are only a finite number of potential cut points.\n    bucket_wgt: Weights to use to calculate the weighted quantiles.\n        If None (default) or 'const' then equal weights are used for all rows.\n        Must be non-negative with at least one postive value.\n    bucket_col: Name of the resulting `bucket` column.\n    \n    Returns: df with the additional `bucket` column \n        The `bucket` column is Categorical data type consisting of Intervals\n        that partition the interval from 0 to sum(bucket_wgt).\n    \"\"\"\n    if bucket_wgt is None:\n        bucket_wgt = 'const'\n    if bucket_wgt == 'const' and 'const' not in df.columns:\n        df = df.assign(const = 1)\n\n    res = df.sort_values(bucket_var).assign(**{\n        'cum_rows_' + bucket_wgt: lambda df: (\n            df[bucket_wgt].cumsum()\n        ),\n        # Ensure that the quantiles cannot split rows with the same value of bucket_var\n        'cum_' + bucket_wgt: lambda df: (\n            df.groupby(bucket_var)['cum_rows_' + bucket_wgt].transform('max')\n        ),\n        bucket_col: (\n            lambda df: pd.cut(df['cum_' + bucket_wgt], bins=n_bins)\n        )\n    })\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bucket_var, bucket_wgt = 'Density', 'const'\n# bucket_var, bucket_wgt = 'Density', 'Exposure'\n# bucket_var, bucket_wgt = 'Density', 'Frequency'\n# bucket_var, bucket_wgt = 'DrivAge', 'Exposure'\n# bucket_var, bucket_wgt = 'Region', 'Exposure'  # Does *not* make sense to order by nominal variable 'Region'\n# bucket_var, bucket_wgt = 'Freq_pred_mean', 'Exposure'  # Does *not* make sense for bucket_var to be constant\nbucket_var, bucket_wgt = 'Freq_pred_veh', 'Exposure'  # Example for lift chart\ntmp2 = weighted_quantiles(df, bucket_var, 8, bucket_wgt)\ntmp2_grpd = tmp2.groupby('bucket').agg(\n    n_obs=('bucket', 'size'),\n    stat_wgt_sum=('Exposure', 'sum'),\n    stat_sum=('ClaimNb', 'sum'),\n    x_min=(bucket_var, 'min'),\n    x_max=(bucket_var, 'max'),\n    x_nunique=(bucket_var, 'nunique'),\n)\ntmp2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cases\n# It is still possible to end up with no rows in a bucket\npd.Series([2, 2, 3, 3]).to_frame('val').assign(\n    bucket=lambda df: pd.cut(df['val'], bins=5)\n).groupby('bucket').agg(n_rows=('bucket', 'size'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Illustration of why we don't want to split rows that have the same value of bucket_var\npd.DataFrame({\n    'bucket_var': [0, 0, 1],\n    'bucket_wgt': [1, 1, 1],\n}).sort_values('bucket_wgt').assign(\n    cum_wgt_rows=lambda df: df['bucket_wgt'].cumsum(),\n    bucket_rows=lambda df: pd.cut(df['cum_wgt_rows'], bins=3),\n    cum_wgt=lambda df: df.groupby('bucket_var')['cum_wgt_rows'].transform('max'),\n    bucket=lambda df: pd.cut(df['cum_wgt'], bins=3),\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n## all_levels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def all_levels(df, bucket_var, include_levels=None, ret_map=False, bucket_col='bucket'):\n    \"\"\"\n    Assign each row of `df` to a bucket according to the unique \n    values of `bucket_var`.\n    \n    bucket_var: Column name of the values to split on.\n        Missing values will not be assigned to an interval.\n    include_levels: Level values to guarantee to include \n        even if they do not appear in the values of bucket_var.\n        Missing values are ignored.\n    ret_map: Whether to also return the bucket_map Series.\n    bucket_col: Name of the resulting `bucket` column.\n    \n    Returns: \n        df with the additional `bucket` column\n            The `bucket` column is Categorical data type consisting of \n            Intervals that partition a range, plus possible NaN.\n        If ret_map is True, also return a Series mapping bucket values\n            to bucket intervals.\n    \"\"\"\n    # Format inputs\n    if include_levels is not None:\n        if not isinstance(include_levels, pd.Series):\n            include_levels = pd.Series(include_levels)\n    \n    # Get the mapping from level value to an appropriate interval\n    buckets_vals = pd.concat([\n        df[bucket_var], include_levels\n    ]).drop_duplicates().sort_values(\n    ).reset_index(drop=True).dropna().to_frame('val')\n    \n    # Add a column of intervals (there may be some intervals with no rows)\n    if np.issubdtype(df[bucket_var].dtype, np.number):\n        # If the values are numeric then take the smallest width\n        min_diff = np.min(np.diff(buckets_vals['val']))\n        buckets_map = buckets_vals.assign(\n            interval=lambda df: pd.cut(df['val'], pd.interval_range(\n                start=df['val'].min() - min_diff/2,\n                end=df['val'].max() + min_diff/2,\n                freq=min_diff\n            ))\n        )\n    else:\n        # If the values are not numeric then take unit intervals\n        buckets_map = buckets_vals.assign(\n            interval=lambda df: pd.interval_range(start=0., periods=df.shape[0], freq=1.)\n        )\n    \n    # Convert to a Series\n    buckets_map = buckets_map.reset_index(drop=True)\n    \n    # Assign buckets and map to intervals\n    res = df.assign(**{bucket_col: lambda df: (\n        df[bucket_var].astype(\n            # Cast the bucket variable as Categorical\n            pd.CategoricalDtype(buckets_map['val'], ordered=True)\n        ).cat.rename_categories(\n            # Swap the bucket levels with the bucket intervals\n            buckets_map.set_index('val')['interval']\n        )\n    )})\n    \n    if ret_map:\n        return(res, buckets_map)\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bucket_var, include_levels = 'DrivAge', None  # Discrete all levels\nbucket_var, include_levels = 'Area', 'X'  # Categorical all levels\n# bucket_var, include_levels = 'DrivAge', pd.Series([18.5])\n# bucket_var, include_levels = 'Area', np.nan  # With missing vals\n# bucket_var, include_levels = 'Area', None  # Slightly different\ntmp3, tmp3_bucket_map = df.pipe(all_levels, bucket_var, include_levels, ret_map=True)\ntmp3_grpd = tmp3.groupby('bucket').agg(\n    n_obs=('bucket', 'size'),\n    stat_wgt_sum=('Exposure', 'sum'),\n    stat_sum=('ClaimNb', 'sum'),\n    x_min=(bucket_var, 'min'),\n    x_max=(bucket_var, 'max'),\n    x_nunique=(bucket_var, 'nunique'),\n)\n# Use the bucket_map to assign labels to each bucket interval\ntmp3_grpd.assign(\n    x_label=lambda df: pd.Categorical(df.index).rename_categories(\n        tmp3_bucket_map.set_index('interval')['val']\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing vals\nunit_w_miss, bucket_map = pd.Series([0, 1, np.nan]).to_frame('val').pipe(\n    lambda df: all_levels(df, 'val', ret_map=True)\n)\ndisplay(unit_w_miss)\ndisplay(bucket_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use a missing indicator to cope with missing values\nunit_filled, b_map = pd.Series([0, 1, np.nan]).to_frame('val').assign(\n    val_miss_ind=lambda df: df.val.isna(),\n    val=lambda df: df.val.fillna(0),\n).pipe(divide_n, 'val', 3).pipe(\n    all_levels, 'val_miss_ind', bucket_col='bucket_miss_ind', ret_map=True \n)\ndisplay(unit_filled)\nunit_filled.groupby(['bucket_miss_ind', 'bucket']).agg(\n    n_rows=('bucket', 'size')\n).assign(y_label=lambda df: (\n    df.index.get_level_values('bucket_miss_ind').rename_categories(\n        b_map.set_index('interval')['val'].to_dict()\n    )\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interesting case: We can now group a nominal variable first by all_levels\n# and then by weighted_quantiles, to group the levels in order of increasing\n# stat_wgt_av. This is a possible way to group levels of a nominal variable\n# that makes sense.\nbucket_var, include_levels = 'Region', None\ntmp4_grpd = df.pipe(\n    all_levels, bucket_var, include_levels\n).groupby('bucket').agg(\n    n_obs=('bucket', 'size'),\n    stat_wgt_sum=('Exposure', 'sum'),\n    stat_sum=('ClaimNb', 'sum'),\n    x_min=(bucket_var, 'min'),\n    x_max=(bucket_var, 'max'),\n    x_nunique=(bucket_var, 'nunique'),\n).assign(\n    stat_wgt_av=lambda df: df['stat_sum'] / df['stat_wgt_sum']\n)\ntmp4_grpd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp4_grpd.rename_axis(index='index').pipe(\n    weighted_quantiles, 'stat_wgt_av', 8, 'stat_wgt_sum'\n).groupby('bucket').agg(\n    n_obs=('bucket', 'size'),\n    stat_wgt_sum=('stat_wgt_sum', 'sum'),\n    stat_sum=('stat_sum', 'sum'),\n    x_min=('x_min', 'min'),\n    x_max=('x_min', 'max'),\n    x_nunique=('x_min', 'nunique'),\n).assign(\n    stat_wgt_av=lambda df: df['stat_sum'] / df['stat_wgt_sum']\n).style.bar(subset='stat_wgt_av')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Group and aggregate\nGrouping and aggregating almost certainly results in a much smaller DataFrame, so do that *first* in one function and *then* add additional columns for plotting in a subsequent function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_wgt_av(\n    df_w_buckets, stat_wgt=None,\n    x_var=None, stat_vars=None,\n    bucket_col=None, split_col_val=None,\n):\n    \"\"\"\n    Group by bucket and calculate aggregate values in each bucket\n    \n    df_w_buckets: Result of an 'assign_buckets' function.\n        i.e. a DataFrame with a `bucket` column the is Categorical\n        with Interval categories that partition a range.\n        Rows with missing `bucket` value are excluded from the grouping.\n    stat_wgt: Weights for the weighted distributions of stat_vars.\n        If None (default) then it is set to 'const' and equal weights are used\n        for all rows. Must be non-negative with at least one postive value.\n    x_var: Column name of variable that will be plotted on the x axis.\n        If None, no x axis variables are calculated.\n    stat_vars: \n        If None (default) or empty list, no values are calculated.\n    bucket_col: Name of bucket column to group by.\n        Must be in df_w_buckets. Default 'bucket'.\n    split_col_val:\n        None (default): Do not split buckets.\n        str: Name of column to split buckets by.\n        tuple: Name of column, and constant value to assign (default '__all__').\n    \n    Returns: DataFrame with one row per group and aggregate statistics.\n    \"\"\"\n    # Set defaults\n    if x_var is None:\n        x_var_lst = []\n    else:\n        x_var_lst = [x_var]\n    if stat_wgt is None:\n        stat_wgt = 'const'\n        df_w_buckets = df_w_buckets.assign(**{stat_wgt: 1})\n    if stat_vars is None:\n        stat_vars = []\n    if bucket_col is None:\n        bucket_col = 'bucket'\n    if split_col_val is None:\n        split_col_lst = []\n    \n    # Format inputs\n    if not isinstance(stat_vars, list):\n        stat_vars = [stat_vars]\n    if isinstance(split_col_val, str):\n        split_col_lst = [split_col_val]\n    if isinstance(split_col_val, tuple):\n        if len(split_col_val) == 1:\n            split_col_val = split_col_val[0], '__all__'\n        df_w_buckets = df_w_buckets.assign(**{split_col_val[0]: split_col_val[1]})\n        split_col_lst = [split_col_val[0]]\n    \n    # Variables for which we want the (weighted) distribution in each bucket\n    agg_vars_all = stat_vars\n    if x_var is not None and np.issubdtype(df_w_buckets[x_var].dtype, np.number):\n        agg_vars_all = [x_var] + agg_vars_all\n    # Ensure they are unique (and maintain order)\n    agg_vars = pd.Series(agg_vars_all).drop_duplicates()\n    \n    df_agg = df_w_buckets.assign(\n        **{col + '_x_wgt': (\n            lambda df, col=col: df[col] * df[stat_wgt]\n        ) for col in agg_vars},\n    ).groupby(\n        # Group by the buckets\n        [bucket_col] + split_col_lst, sort=False\n    ).agg(\n        # Aggregate calculation for rows in each bucket\n        n_obs=(bucket_col, 'size'),  # It is possible that a bucket contains zero rows\n        **{col: (col, 'sum') for col in [stat_wgt]},\n        **{stat_var + '_wgt_sum': (\n            stat_var + '_x_wgt', 'sum'\n        ) for stat_var in agg_vars},\n        **{\"x_\" + func: (x_var, func) \n           for func in ['min', 'max'] for x_var in x_var_lst}\n    ).sort_index().assign(\n        # Calculate the weighted average of the stats\n        **{stat_var + '_wgt_av': (\n            lambda df, stat_var=stat_var: df[stat_var + '_wgt_sum'] / df[stat_wgt]\n        ) for stat_var in agg_vars},\n    )\n    \n    return(df_agg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_wgt_av(\n    df_w_buckets, stat_wgt=None,\n    x_var=None, stat_vars=None,\n    bucket_col=None, split_col=None,\n):\n    \"\"\"\n    Group by bucket and calculate aggregate values in each bucket\n    \n    df_w_buckets: Result of an 'assign_buckets' function.\n        i.e. a DataFrame with a `bucket` column the is Categorical\n        with Interval categories that partition a range.\n        Rows with missing `bucket` value are excluded from the grouping.\n    stat_wgt: Weights for the weighted distributions of stat_vars.\n        If None (default) then it is set to 'const' and equal weights are used\n        for all rows. Must be non-negative with at least one postive value.\n    x_var: Column name of variable that will be plotted on the x axis.\n        If None, no x axis variables are calculated.\n    stat_vars: \n        If None (default) or empty list, no values are calculated.\n    bucket_col: Name of bucket column to group by.\n        Must be in df_w_buckets. Default 'bucket'.\n    split_col:\n        None (default): Do not split buckets.\n        str: Name of column to split buckets by.\n    \n    Returns: DataFrame with one row per group and aggregate statistics.\n    \"\"\"\n    # Set defaults\n    if x_var is None:\n        x_var_lst = []\n    else:\n        x_var_lst = [x_var]\n    if stat_wgt is None:\n        stat_wgt = 'const'\n        df_w_buckets = df_w_buckets.assign(**{stat_wgt: 1})\n    if stat_vars is None:\n        stat_vars = []\n    if bucket_col is None:\n        bucket_col = 'bucket'\n    if split_col is None:\n        split_col_lst = []\n    else:\n        split_col_lst = [split_col]\n    \n    # Format inputs\n    if not isinstance(stat_vars, list):\n        stat_vars = [stat_vars]\n    \n    # Variables for which we want the (weighted) distribution in each bucket\n    agg_vars_all = stat_vars\n    if x_var is not None and np.issubdtype(df_w_buckets[x_var].dtype, np.number):\n        agg_vars_all = [x_var] + agg_vars_all\n    # Ensure they are unique (and maintain order)\n    agg_vars = pd.Series(agg_vars_all).drop_duplicates()\n    \n    df_agg = df_w_buckets.assign(\n        **{col + '_x_wgt': (\n            lambda df, col=col: df[col] * df[stat_wgt]\n        ) for col in agg_vars},\n    ).groupby(\n        # Group by the buckets\n        [bucket_col] + split_col_lst, sort=False\n    ).agg(\n        # Aggregate calculation for rows in each bucket\n        n_obs=(bucket_col, 'size'),  # It is possible that a bucket contains zero rows\n        **{col: (col, 'sum') for col in [stat_wgt]},\n        **{stat_var + '_wgt_sum': (\n            stat_var + '_x_wgt', 'sum'\n        ) for stat_var in agg_vars},\n        **{\"x_\" + func: (x_var, func) \n           for func in ['min', 'max'] for x_var in x_var_lst}\n    ).sort_index().assign(\n        # Calculate the weighted average of the stats\n        **{stat_var + '_wgt_av': (\n            lambda df, stat_var=stat_var: df[stat_var + '_wgt_sum'] / df[stat_wgt]\n        ) for stat_var in agg_vars},\n    )\n    \n    return(df_agg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example for lift chart\nbucket_var, bucket_wgt = 'Freq_pred_simple', 'Exposure'\nx_var, stat_wgt, stat_vars = 'cum_' + bucket_wgt, bucket_wgt, ['Frequency', 'Freq_pred_simple']\ntmp7_w_buckets = df.pipe(weighted_quantiles, bucket_var, 4, bucket_wgt)\ntmp7_agg_all = tmp7_w_buckets.pipe(agg_wgt_av, stat_wgt, x_var, stat_vars)\ntmp7_agg_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp7_agg_split = agg_wgt_av(tmp7_w_buckets, stat_wgt, x_var, stat_vars, split_col='split')\ntmp7_agg_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use a missing indicator to cope with missing values\nunit_filled, b_map = pd.Series([0, 1, np.nan]).to_frame('val').assign(\n    val_miss_ind=lambda df: df.val.isna(),\n    val=lambda df: df.val.fillna(-1),\n).pipe(divide_n, 'val', 2).pipe(\n    all_levels, 'val_miss_ind', bucket_col='bucket_miss_ind', ret_map=True \n)\ndisplay(unit_filled)\nunit_agg_all = unit_filled.pipe(agg_wgt_av, x_var='val')\ndisplay(unit_agg_all)\nunit_agg_split = unit_filled.pipe(agg_wgt_av, stat_vars='val', split_col='val_miss_ind')\ndisplay(unit_agg_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Other examples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Plot\nFor plotting, we switch away from `matplotlib` to `bokeh` because:\n- It is cumbersome to use `matplotlib` for plotting both a histogram and line plot on the same axes\n- It would be nice to have interactivity (e.g. zooming) in our resulting plots\n\nOn the downside, it does require a bit of code to produce the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Move to util functions\n\ndef expand_lims(df, pct_buffer_below=0.05, pct_buffer_above=0.05, include_vals=None):\n    \"\"\"\n    Find the range over all columns of df. Then expand these \n    below and above by a percentage of the total range.\n    \n    df: Consider all values in all columns\n    include_vals: Additional values to consider\n    \n    Returns: Series with rows 'start' and 'end' of the expanded range\n    \"\"\"\n    # If a Series is passed, convert it to a DataFrame\n    try:\n        df = df.to_frame()\n    except:\n        pass\n    # Case where df has no columns, just fill in default vals\n    if df.shape[1] == 0:\n        res_range = pd.Series({'start': 0, 'end': 1})\n        return(res_range)\n    if include_vals is None:\n        include_vals = []\n    if not isinstance(include_vals, list):\n        include_vals = [include_vals]\n    \n    res_range = pd.concat([\n        df.reset_index(drop=True),\n        # Add a column of extra values to the DataFrame to take these into account\n        pd.DataFrame({'_extra_vals': include_vals}),\n    ], axis=1).apply(\n        # Get the range (min and max) over the DataFrame\n        ['min', 'max']).agg({'min': 'min', 'max': 'max'}, axis=1).agg({\n        # Expanded range\n        'start': lambda c: c['max'] - (1 + pct_buffer_below) * (c['max'] - c['min']),\n        'end': lambda c: c['min'] + (1 + pct_buffer_above) * (c['max'] - c['min']),\n    })\n    return(res_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example lift chart\nbucket_var, bucket_wgt = 'Freq_pred_simple', 'Exposure'\nx_var, stat_wgt, stat_vars = 'cum_' + bucket_wgt, bucket_wgt, ['Frequency', 'Freq_pred_simple']\ntmp8_agg = df.pipe(\n    weighted_quantiles, bucket_var, 10, bucket_wgt\n).pipe(\n    agg_wgt_av, stat_wgt, x_var, stat_vars\n)\ntmp8_agg.assign(\n    # Get the coordinates for plot: interval edges\n    x_left=lambda df: df.index.categories.left,\n    x_right=lambda df: df.index.categories.right,\n    x_point=lambda df: (df['x_right'] + df['x_left'])/2.,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to set the x-axis edges `x_left` and `x_right`\ndef x_edges_min_max(df_agg):\n    \"\"\"\n    Set the x-axis edges to be the min and max values of `x_var`.\n    Does not make sense to use this option when min and max are not numeric.\n    This might result in zero width intervals, in which case a warning is given.\n    \"\"\"\n    if not np.issubdtype(df_agg['x_min'].dtype, np.number):\n        raise ValueError(\n            \"\\n\\tx_edges_min_max: This method can only be used when\"\n            \"\\n\\tx_min and x_max are numeric data types.\"\n        )\n        \n    if (df_agg['x_min'] == df_agg['x_max']).any():\n        warning(\n            \"x_edges_min_max: At least one bucket has x_min == x_max, \"\n            \"so using this method will result in zero width intervals.\"\n        )\n    \n    res = df_agg.assign(\n        # Get the coordinates for plot: interval edges\n        x_left=lambda df: df['x_min'],\n        x_right=lambda df: df['x_max'],\n    )\n    return(res)\n\n\ndef x_edges_interval(df_agg, bucket_col='bucket'):\n    \"\"\"Set the x-axis edges to be the edges of the bucket interval\"\"\"\n    res = df_agg.assign(\n        x_left=lambda df: [intval.left for intval in df.index.get_level_values(bucket_col)],\n        x_right=lambda df: [intval.right for intval in df.index.get_level_values(bucket_col)],\n    )\n    return(res)\n\n\ndef x_edges_unit(df_agg, bucket_col='bucket'):\n    \"\"\"\n    Set the x-axis edges to be the edges of equally spaced intervals\n    of width 1.\n    \"\"\"\n    res = df_agg.assign(\n        x_left=lambda df: pd.Categorical(df.index.get_level_values(bucket_col)).codes,\n        x_right=lambda df: df['x_left'] + 1,\n    )\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to set the x-axis point\ndef x_point_mid(df_agg):\n    \"\"\"Set the x_point to be mid-way between x_left and x_right\"\"\"\n    res = df_agg.assign(\n        x_point=lambda df: (df['x_left'] + df['x_right']) / 2.\n    )\n    return(res)\n\ndef x_point_wgt_av(df_agg, x_var):\n    \"\"\"\n    Set the x_point to be the weighted average of x_var within the bucket,\n    weighted by stat_wgt.\n    \"\"\"\n    if not (x_var + '_wgt_av') in df_agg.columns:\n        raise ValueError(\n            \"\\n\\tx_point_wgt_av: This method can only be used when\"\n            \"\\n\\tthe weighted average has already been calculated.\"\n        )\n    \n    res = df_agg.assign(\n        x_point=lambda df: df[x_var + '_wgt_av']\n    )\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def x_label_none(df_agg):\n    res = df_agg.copy()\n    if 'x_label' in df_agg.columns:\n        res = res.drop(columns='x_label')\n    return(res)\n\ndef x_label_map(df_agg, bucket_map, bucket_col='bucket'):\n    res = df_agg.assign(\n        x_label=lambda df: pd.Categorical(\n            df.index.get_level_values(bucket_col)\n        ).rename_categories(\n            bucket_map.set_index('interval')['val']\n        )\n    )\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Fill missing values in these functions\n# def y_quad_cumulative(df_agg, stat_wgt, bucket_col='bucket'):\n#     res = df_agg.assign(\n#         quad_upr=lambda df: df.groupby([bucket_col])[stat_wgt].transform('cumsum'),\n#         quad_lwr=lambda df: df.groupby([bucket_col])['quad_upr'].shift(fill_value=0).fillna(method='ffill'),\n#     )\n#     return(res)\n\ndef y_quad_cumulative(df_agg, stat_wgt, bucket_col='bucket'):\n    res = df_agg.assign(\n        quad_upr=lambda df: df.groupby([bucket_col])[stat_wgt].transform('cumsum'),\n        quad_lwr=lambda df: df.groupby([bucket_col])['quad_upr'].shift(fill_value=0),\n    )\n    return(res)\n\ndef y_quad_area(df_agg, stat_wgt, bucket_col='bucket'):\n    res = df_agg.assign(\n        x_width=lambda df: df['x_right'] - df['x_left'],\n        quad_upr=lambda df: df.groupby([bucket_col])[stat_wgt].transform('cumsum') / df['x_width'],\n        quad_lwr=lambda df: df.groupby([bucket_col])['quad_upr'].shift(fill_value=0),\n    )\n    return(res)\n\ndef y_quad_proportion(df_agg, stat_wgt, bucket_col='bucket'):\n    res = df_agg.assign(\n        quad_upr=lambda df: (\n            df.groupby([bucket_col])[stat_wgt].transform('cumsum') / \n            df.groupby([bucket_col])[stat_wgt].transform('sum')\n        ),\n        quad_lwr=lambda df: df.groupby([bucket_col])['quad_upr'].shift(fill_value=0),\n    )\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examples\nstat_wgt='Exposure'\ntmp7_agg_split.pipe(\n    x_edges_interval\n#     x_edges_min_max\n#     x_edges_min_max\n).pipe(\n#     y_quad_cumulative, stat_wgt\n    y_quad_area, stat_wgt\n#     y_quad_proportion, stat_wgt\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_funcs_df = pd.DataFrame(\n    columns=['task', 'func', 'alias'],\n    data = [\n        ('x_edges', x_edges_interval, ['interval']),\n        ('x_edges', x_edges_min_max, ['min_max']),\n        ('x_edges', x_edges_unit, ['unit']),\n        ('x_point', x_point_mid, ['mid']),\n        ('x_point', x_point_wgt_av, ['wgt_av']),\n        ('x_label', x_label_none, ['none']),\n        ('x_label', x_label_map, ['map']),\n        ('y_quad', y_quad_cumulative, ['cum']),\n        ('y_quad', y_quad_area, ['area']),\n        ('y_quad', y_quad_proportion, ['prop']),\n    ],\n).assign(\n    name=lambda df: df['func'].apply(lambda f: f.__name__),\n    arg_names=lambda df: df['func'].apply(\n        lambda f: [\n            arg_name for arg_name, val \n            in inspect.signature(f).parameters.items()\n        ][1:]  # Not the \"df\" argument\n    ),\n    req_arg_names=lambda df: df['func'].apply(\n        lambda f: [\n            arg_name for arg_name, val \n            in inspect.signature(f).parameters.items()\n            if val.default == inspect.Parameter.empty\n        ][1:]  # Not the \"df\" argument\n    ),\n).set_index(['task', 'name'])\n\npipe_funcs_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pipeline_func(\n    task, search_term,\n    kwarg_keys=None, calling_func='',\n    pipe_funcs_df=pipe_funcs_df\n):\n    \"\"\"\n    TODO: Write docstring <<<<<<<<<<<<<\n    \"\"\"\n    # Set defaults\n    if kwarg_keys is None:\n        kwarg_keys = []\n    \n    # Find function row\n    task_df = pipe_funcs_df.loc[task,:]\n    func_row = task_df.loc[task_df.index == search_term, :]    \n    if func_row.shape[0] != 1:\n        func_row = task_df.loc[[search_term in ali for ali in task_df.alias], :]\n    if func_row.shape[0] != 1:\n        raise ValueError(\n            f\"\\n\\t{calling_func}: Cannot find '{search_term}' within the\"\n            f\"\\n\\tavailable '{task}' pipeline functions.\"\n        )\n        \n    # Check required arguments are supplied\n    for req_arg in func_row['req_arg_names'][0]:\n        if not req_arg in kwarg_keys:\n            raise ValueError(\n                f\"\\n\\t{calling_func}: To use the '{search_term}' as a '{task}' pipeline\"\n                f\"\\n\\tfunction, you must specify '{req_arg}' as a keyword argument.\"\n            )\n    return(func_row['func'][0], func_row['arg_names'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examples\n# get_pipeline_func('x_edges', 'min_max')\n# get_pipeline_func('x_edges', 'x_edges_interval')\n# get_pipeline_func('x_point', 'foo', calling_func='from_here')  # Throws an error\nget_pipeline_func('x_point', 'wgt_av', ['x_var'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_coords(\n    df_agg_all,\n    x_edges=None, x_point=None, x_label=None,\n    y_quad=None,\n    **kwargs,\n):\n    \"\"\"\n    Given a DataFrame where each row is a bucket, add x-axis \n    properties to be used for plotting. See pipe_funcs_df for \n    available options.\n    \n    x_edges: How to position the x-axis edges.\n        Default: 'interval'\n    x_point: Where to position each bucket point on the x-axis.\n        Default: 'mid'\n    x_label: Option for x-axis label.\n        Default: 'none'\n    y_quad: How to plot the histogram quads.\n        Default: 'cum'\n    **kwargs: Additional arguments to pass to the functions.\n    \"\"\"\n    # Set variables for use throughout the function\n    calling_func = 'add_coords'\n    kwarg_keys = list(kwargs.keys())\n    \n    # Set defaults\n    if x_edges is None:\n        x_edges = 'interval'\n    if x_point is None:\n        x_point = 'mid'\n    if x_label is None:\n        x_label = 'none'\n    if y_quad is None:\n        y_quad = 'cum'\n    \n    # Get pipeline functions\n    full_func, arg_names = get_pipeline_func('x_edges', x_edges, kwarg_keys, calling_func)\n    x_edges_func = functools.partial(full_func, **{\n        arg_name: kwargs[arg_name] for arg_name in set(arg_names).intersection(set(kwarg_keys))\n    })\n    \n    full_func, arg_names = get_pipeline_func('x_point', x_point, kwarg_keys, calling_func)\n    x_point_func = functools.partial(full_func, **{\n        arg_name: kwargs[arg_name] for arg_name in set(arg_names).intersection(set(kwarg_keys))\n    })\n\n    full_func, arg_names = get_pipeline_func('x_label', x_label, kwarg_keys, calling_func)\n    x_label_func = functools.partial(full_func, **{\n        arg_name: kwargs[arg_name] for arg_name in set(arg_names).intersection(set(kwarg_keys))\n    })\n    \n    full_func, arg_names = get_pipeline_func('y_quad', y_quad, kwarg_keys, calling_func)\n    y_quad_func = functools.partial(full_func, **{\n        arg_name: kwargs[arg_name] for arg_name in set(arg_names).intersection(set(kwarg_keys))\n    })\n    \n    # Apply the functions\n    res = df_agg_all.pipe(\n        lambda df: x_edges_func(df)\n    ).pipe(\n        lambda df: x_point_func(df)\n    ).pipe(\n        lambda df: x_label_func(df)\n    ).pipe(\n        lambda df: y_quad_func(df)\n    )\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_funcs_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_bplot(\n    df_for_plt, stat_wgt, stat_vars,\n    cols=bokeh.palettes.Dark2[8],\n):\n    \"\"\"Create bucket plot object from aggregated data\"\"\"\n    # Add a second index level, if it does not have one already\n    if len(df_for_plt.index.names) == 1:\n        df_for_plt = df_for_plt.assign(\n            split='__all__'\n        ).set_index([df_for_plt.index, 'split'])\n    \n    # Set up the figure\n    bkp = bokeh.plotting.figure(\n        title=\"Bucket plot\", x_axis_label=\"X-axis name\", y_axis_label=stat_wgt, \n        tools=\"reset,box_zoom,pan,wheel_zoom,save\", background_fill_color=\"#fafafa\",\n        plot_width=800, plot_height=500\n    )\n\n    # Plot the histogram squares...\n    bkp.quad(\n        top=df_for_plt['quad_upr'], bottom=df_for_plt['quad_lwr'],\n        left=df_for_plt['x_left'], right=df_for_plt['x_right'],\n        fill_color=\"khaki\", line_color=\"white\", legend_label=\"Weight\"\n    )\n    # ...at the bottom of the graph\n    bkp.y_range = bokeh.models.ranges.Range1d(\n        **expand_lims(df_for_plt[['quad_upr', 'quad_lwr']], 0, 1.2)\n    )\n\n    bkp.legend.location = \"top_left\"\n    bkp.legend.click_policy=\"hide\"\n\n    # Plot the weight average statistic points joined by straight lines\n    # Set up the secondary axis\n    bkp.extra_y_ranges['y_range_2'] = bokeh.models.ranges.Range1d(\n        **expand_lims(df_for_plt[[stat_var + '_wgt_av' for stat_var in stat_vars]])\n    )\n    bkp.add_layout(bokeh.models.axes.LinearAxis(\n        y_range_name='y_range_2',\n        axis_label=\"Weighted average statistic\"\n    ), 'right')\n\n    for var_num, stat_var in enumerate(stat_vars):\n        for split_level in df_for_plt.index.levels[1]:\n            # The following parameters need to be passed to both circle() and line()\n            stat_line_args = {\n                'x': df_for_plt.xs(split_level, level=1)['x_point'],\n                'y': df_for_plt.xs(split_level, level=1)[stat_var + '_wgt_av'],\n                'y_range_name': 'y_range_2',\n                'color': cols[var_num],\n                'legend_label': stat_var,\n            }\n            bkp.circle(**stat_line_args, size=4)\n            bkp.line(**stat_line_args)\n    \n    return(bkp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example lift chart\nbucket_var, bucket_wgt = 'Freq_pred_simple', 'Exposure'\nx_var, stat_wgt, stat_vars = 'cum_' + bucket_wgt, bucket_wgt, ['Frequency', 'Freq_pred_simple']\ntmp8_for_plt = df.pipe(\n    weighted_quantiles, bucket_var, 10, bucket_wgt\n).pipe(\n    agg_wgt_av, stat_wgt, x_var, stat_vars, \n    split_col='split'\n).pipe(\n    add_coords, stat_wgt=stat_wgt, y_quad='prop', x_edges='min_max'\n)\nbkp = create_bplot(tmp8_for_plt, stat_wgt, stat_vars)\nbokeh.plotting.show(bkp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Worked examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"help(add_coords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DrivAge"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_var, stat_wgt, stat_vars = 'DrivAge', 'Exposure', ['Frequency', 'Freq_pred_simple', 'Freq_pred_veh']\nbucket_var = 'DrivAge'\ndf_for_plt = df.pipe(\n#     divide_n, bucket_var, 10\n#     all_levels, bucket_var\n#     custom_width, bucket_var, 3, 17.5\n    custom_width, bucket_var, 3, 17.5, None, 68.5\n).pipe(\n    agg_wgt_av, stat_wgt, x_var, stat_vars,\n    # split_col='split'\n).pipe(\n    add_coords, stat_wgt=stat_wgt, bucket_col='bucket',\n    y_quad='area', \n    # x_edges='unit'\n)\nbkp = create_bplot(df_for_plt, stat_wgt, stat_vars)\nbkp.legend.location = \"top_right\"\nbokeh.plotting.show(bkp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Density"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_var, stat_wgt, stat_vars = 'Density', 'Exposure', ['Frequency', 'Freq_pred_simple', 'Freq_pred_veh']\nbucket_var, bucket_wgt = x_var, stat_wgt\ndf_for_plt = df.pipe(\n#     divide_n, bucket_var, 10\n    custom_width, bucket_var, 100, 0.5, None, 1000\n#     weighted_quantiles, bucket_var, 10, bucket_wgt\n).pipe(\n    agg_wgt_av, stat_wgt, x_var, stat_vars,\n    split_col='split'\n).pipe(\n    add_coords, stat_wgt=stat_wgt, bucket_col='bucket',\n#     x_edges='min_max', x_point='wgt_av', x_var=x_var,\n#     y_quad='area',\n    x_edges='unit',\n)\nbkp = create_bplot(df_for_plt, stat_wgt, stat_vars)\n#bkp.legend.location = \"top_right\"\nbokeh.plotting.show(bkp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Area and Region"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_var, stat_wgt, stat_vars = 'Area', 'Exposure', ['Frequency', 'Freq_pred_simple', 'Freq_pred_veh']\nbucket_var, bucket_wgt = x_var, stat_wgt\ndf_for_plt = df.pipe(\n    all_levels, bucket_var\n).pipe(\n    agg_wgt_av, stat_wgt, x_var, stat_vars,\n    split_col='split',\n).pipe(\n    add_coords, stat_wgt=stat_wgt,\n)\nbkp = create_bplot(df_for_plt, stat_wgt, stat_vars)\nbokeh.plotting.show(bkp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interesting case: Group a nominal variable first by all_levels and then by \n# weighted_quantiles, to group the levels in order of increasing stat_wgt_av.\nx_var, stat_wgt, stat_vars = 'Region', 'Exposure', ['Frequency', 'Freq_pred_simple', 'Freq_pred_veh']\nbucket_var, bucket_wgt = x_var, stat_wgt\ndf_for_plt = df.pipe(\n    all_levels, bucket_var, bucket_col='split'\n).pipe(\n    agg_wgt_av, stat_wgt, x_var, stat_vars, bucket_col='split'\n).pipe(\n    weighted_quantiles, 'Frequency_wgt_av', 5, stat_wgt\n).pipe(\n    agg_wgt_av, stat_wgt, 'Frequency_wgt_av',\n    stat_vars=['Frequency_wgt_av', 'Freq_pred_simple_wgt_av'],\n    # split_col='split' # NOT CURRENTLY WORKING PROPERLY\n).pipe(\n    add_coords, stat_wgt=stat_wgt, bucket_col='bucket',\n)\nbkp = create_bplot(df_for_plt, stat_wgt, stat_vars=['Frequency_wgt_av', 'Freq_pred_simple_wgt_av'])\nbokeh.plotting.show(bkp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>\n\n# Rough work"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of allowing an additional var for grouping as missing_ind\nmiss_ind_grpd = pd.Series([0, 1, np.nan]).to_frame('val').assign(\n    missing_ind=lambda df: df['val'].isna(),  # Get missing_ind\n    val_filled=lambda df: df['val'].fillna(df['val'].min()),   # Fill missing vals\n).pipe(\n    lambda df: divide_n(df, 'val_filled', 3)\n).groupby(['bucket', 'missing_ind']).agg(\n    n_rows=('bucket', 'size'),\n)\ndisplay(miss_ind_grpd)\nmiss_ind_grpd.unstack(fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unfinished functions to merge and split df_agg_all and df_agg_split\ndef get_agg_all(df_agg, split_col_val=None):\n    if split_col_val is None:\n        split_col_val = ('split', '__all__')\n    df_agg_all = df_agg.xs(\n        split_col_val[1], level=split_col_val[0]\n    ).dropna(axis=1, how='all')\n    return(df_agg_all)\n\ndef get_agg_splits(df_agg, split_col_val=None):\n    if split_col_val is None:\n        split_col_val = ('split', '__all__')\n    df_agg_splits = df_agg.loc[\n        df_agg.index.get_level_values(split_col_val[0]) != split_col_val[1],:\n    ].dropna(axis=1, how='all')\n    return(df_agg_splits)\n\n# NOT COMPLETE\n\ndef agg_split_merge(\n    df_w_buckets, stat_wgt=None,\n    x_var=None, stat_vars=None,\n    bucket_var=None, split_var=None,\n):\n    if split_var is None:\n        df_agg_all = df_w_buckets.pipe(\n            agg_wgt_av, stat_wgt, x_var, stat_vars, bucket_var, split_var\n        )\n    df_agg_split = df_w_buckets.pipe(\n        agg_wgt_av, stat_wgt, None, stat_vars, bucket_var, split_var\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Not longer used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Previous attempt involved passing arguments between functions\nBARGS_DEFAULT = {\n    'stat_wgt': 'const',\n    'bucket_wgt': 'stat_wgt',\n    'n_bins': 10,\n    'order_by': 'NA',\n    'stat_vars': [],\n}\n\ndef update_bargs(bargs_new, bargs_prev, func_name, bargs_default=BARGS_DEFAULT):\n    \"\"\"\n    Update the Bucket arguments so that they can be tracked and passed between functions.\n    \n    bargs_new: Items to add to bargs, or overwrite previous values\n    bargs_prev: Current items in bargs\n    bargs_default: Values to take if the corresponding bargs_new value is None\n    func_name: To include in the error message.\n    \n    Specifically, return a dictionary of bargs with the following items:\n    Items from bargs_new with non-None values take precedence.\n    For items with None value in bargs_new:\n        If the key exists in bargs_prev, take that item.\n        Else if the key exists in bargs_default, that that item.\n        Else throw an error, i.e. every key from bargs_new must be in the output.\n    For keys in bargs_prev but not in bargs_new, take that item.\n    \"\"\"\n    # Convert input data types\n    if bargs_prev is None:\n        bargs_prev = dict()\n    # Allocate a non-None value for every key in bargs_new or bargs_prev\n    res = dict()\n    for key in {**bargs_new, **bargs_prev}.keys():\n        if key in bargs_new.keys() and bargs_new[key] is not None:\n            res[key] = bargs_new[key]\n        elif key in bargs_prev.keys():\n            res[key] = bargs_prev[key]\n        elif key in bargs_default.keys():\n            res[key] = bargs_default[key]\n        else: \n            raise ValueError(\n                f\"{func_name}: '{key}' is required but has not been supplied\"\n            )\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></div>"}],"metadata":{"jupytext":{"formats":"md,ipynb","text_representation":{"extension":".md","format_name":"markdown","format_version":"1.2","jupytext_version":"1.4.2"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}