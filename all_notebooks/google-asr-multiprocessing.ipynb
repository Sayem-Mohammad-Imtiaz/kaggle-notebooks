{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.io import wavfile\nfrom scipy.io import wavfile\nimport pandas as pd\nfrom joblib import Parallel, delayed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequency_sampling, audio_signal  = wavfile.read(\"../input/automatic-speech-recognition-in-wolof/audio_wav_16000/tmp/WOLOF_ASR_dataset/audio_wav_16000/0047cd812e3db0cf28588f86b5c218822b556e6f9bc8c986919296da4b11e60080938fa69151bcb119b287b51db5e8bf6f6a82087f629cf03cfe86efab2c37af.wav\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nSignal shape:', audio_signal.shape)\nprint('Signal Datatype:', audio_signal.dtype)\nprint('Signal duration:', round(audio_signal.shape[0] / \nfloat(frequency_sampling), 2), 'seconds')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_signal = audio_signal / np.power(2, 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_signal = audio_signal [:1000]\ntime_axis = 1000 * np.arange(0, len(audio_signal), 1) / float(frequency_sampling)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(time_axis, audio_signal, color='blue')\nplt.xlabel('Time (milliseconds)')\nplt.ylabel('Amplitude')\nplt.title('Input audio signal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Characterizing the audio signal\nConverting the time domain signal into frequency domain. This is important because it gives a lot of information about the signal. A mathematical tool like fourier transformation can be used for this.","metadata":{}},{"cell_type":"code","source":"length_signal = len(audio_signal)\nhalf_length = np.ceil((length_signal + 1) / 2.0).astype(np.int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal_frequency = np.fft.fft(audio_signal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal_frequency = abs(signal_frequency[0:half_length]) / length_signal\nsignal_frequency **= 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_fts = len(signal_frequency)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if length_signal % 2:\n   signal_frequency[1:len_fts] *= 2\nelse:\n   signal_frequency[1:len_fts-1] *= 2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"signal_power = 10 * np.log10(signal_frequency)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis = np.arange(0, half_length, 1) * (frequency_sampling / length_signal) / 1000.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(x_axis, signal_power, color='red')\nplt.xlabel('Frequency (kHz)')\nplt.ylabel('Signal power (dB)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installing libraries","metadata":{}},{"cell_type":"code","source":"!pip install SpeechRecognition","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import speech_recognition as sr\nsr.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recognizer class","metadata":{}},{"cell_type":"code","source":"r = sr.Recognizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading an audio file","metadata":{}},{"cell_type":"code","source":"bangla1 = sr.AudioFile('../input/automatic-speech-recognition-in-wolof/audio_wav_16000/tmp/WOLOF_ASR_dataset/audio_wav_16000/0047cd812e3db0cf28588f86b5c218822b556e6f9bc8c986919296da4b11e60080938fa69151bcb119b287b51db5e8bf6f6a82087f629cf03cfe86efab2c37af.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with bangla1 as source:\n    audio = r.record(source)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(audio)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using google web speech API\nr.recognize_google(audio)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pydub import AudioSegment\n\npath = \"../input/automatic-speech-recognition-in-wolof/audio_wav_16000/tmp/WOLOF_ASR_dataset/audio_wav_16000/\"\n\n#Change working directory\n# os.chdir(path)\n\naudio_files = os.listdir(path )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bangla2 = sr.AudioFile('../input/automatic-speech-recognition-in-wolof/audio_wav_16000/tmp/WOLOF_ASR_dataset/audio_wav_16000/005cea38ff6e71a7d1047aba774790ad04336b73fed6c02ebd144060725ada6861acf7821c9abfa408067dd345078c2eb74e34535e690957ade04ea985a13bff.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with bangla2 as source:\n    audio2 = r.record(source)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(audio2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.recognize_google(audio2,language = \"FR\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root=\"../input/automatic-speech-recognition-in-wolof/audio_wav_16000/tmp/WOLOF_ASR_dataset/audio_wav_16000\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_fn(filename):\n    \n    path = f\"{root}/{filename}.wav\"\n    if os.path.exists(path):\n        try:\n            wolof = sr.AudioFile(path)\n            with wolof as source:\n                audio2 = r.record(source)\n            transcript = r.recognize_google(audio2,language = \"fr\")\n        \n            return transcript\n        except:\n            print(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wolof_trans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nTest = pd.read_csv(\"../input/automatic-speech-recognition-in-wolof/Test.csv\")\ntest_list = list(Test.ID.values)\nwolof_trans = Parallel(n_jobs=8, backend=\"multiprocessing\")(\n    delayed(save_fn)(filename) for filename in tqdm(test_list)\n)  \n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nSub = pd.read_csv(\"../input/automatic-speech-recognition-in-wolof/SampleSubmission.csv\")\nSub[\"transcription\"] = wolof_trans\nSub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sub.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}