{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### <center>The Art of Cleaning Data <center>\n    \nData cleaning and preprocessing is a very important process in  data science and data analysis projects. You cannot go directly to modeling with raw data in your hands. As data scientists commonly say: garbage in garbage out. So it is a very indispensable task to clean your data, handle missing values, and deal with outliers before build you machine learning models. \n\nIn this tutorial we will go through several techniques to handle missing data by customizing the missing values and imputing the missing data values using different methods such as  mean, median, mode, a constant value, forward fill, backward fill and polynomial interpolation. After that, we will see how to implement some common methods to detect and remove outliers from the dataset. ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd \nimport numpy as np \nimport seaborn as sb \nfrom matplotlib import pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:35:18.581447Z","iopub.execute_input":"2021-07-09T13:35:18.5819Z","iopub.status.idle":"2021-07-09T13:35:18.595003Z","shell.execute_reply.started":"2021-07-09T13:35:18.58186Z","shell.execute_reply":"2021-07-09T13:35:18.593934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#choose columns for the dataframe \ncols = ['Emp ID','First Name', 'Last Name','Gender','E Mail','Date of Birth','Age in Yrs.',\n       'Weight in Kgs.','Year of Joining','Age in Company (Years)','Salary','State', 'Zip']\n#get the data \nemployees = pd.read_csv('/kaggle/input/employees/employees.csv',usecols= cols)\nheart = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:25:52.016021Z","iopub.execute_input":"2021-07-09T13:25:52.01643Z","iopub.status.idle":"2021-07-09T13:25:52.064401Z","shell.execute_reply.started":"2021-07-09T13:25:52.016393Z","shell.execute_reply":"2021-07-09T13:25:52.063201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#copy the employees dataset for further use\ndf = employees.copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:25:56.12299Z","iopub.execute_input":"2021-07-09T13:25:56.123335Z","iopub.status.idle":"2021-07-09T13:25:56.129434Z","shell.execute_reply.started":"2021-07-09T13:25:56.123306Z","shell.execute_reply":"2021-07-09T13:25:56.128277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rename some columns \ndf.rename(columns={'Emp ID':'ID','E Mail':'Email','Date of Birth':'Birth Date','Age in Yrs.':'Age'\n          ,'Weight in Kgs.':'Weight','Age in Company (Years)':'Experience'},inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:25:59.258036Z","iopub.execute_input":"2021-07-09T13:25:59.258384Z","iopub.status.idle":"2021-07-09T13:25:59.304133Z","shell.execute_reply.started":"2021-07-09T13:25:59.258354Z","shell.execute_reply":"2021-07-09T13:25:59.302908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#round Age and Experiene columns \ndf.Age = df.Age.apply(lambda x:int(round(x,0)))\ndf.Experience = df.Experience.apply(lambda x:int(round(x,0)))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:34:27.309089Z","iopub.execute_input":"2021-07-09T13:34:27.309455Z","iopub.status.idle":"2021-07-09T13:34:27.347647Z","shell.execute_reply.started":"2021-07-09T13:34:27.309425Z","shell.execute_reply":"2021-07-09T13:34:27.346904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:34:30.128655Z","iopub.execute_input":"2021-07-09T13:34:30.129288Z","iopub.status.idle":"2021-07-09T13:34:30.138423Z","shell.execute_reply.started":"2021-07-09T13:34:30.129232Z","shell.execute_reply":"2021-07-09T13:34:30.136966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#randomly put null values in some columns of the dataframe\ndf.loc[df.sample(frac=0.01).index, 'Gender'] = pd.np.nan\ndf.loc[df.sample(frac=0.115).index, 'Age'] = pd.np.nan\ndf.loc[df.sample(frac=0.16).index, 'Weight'] = pd.np.nan\ndf.loc[df.sample(frac=0.05).index, 'Salary'] = pd.np.nan\ndf.loc[df.sample(frac=0.23).index, 'Experience'] = pd.np.nan\ndf.loc[df.sample(frac=0.056).index, 'Birth Date'] = pd.np.nan\n#check out qgqin \ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:35:26.247362Z","iopub.execute_input":"2021-07-09T13:35:26.248024Z","iopub.status.idle":"2021-07-09T13:35:26.276197Z","shell.execute_reply.started":"2021-07-09T13:35:26.247966Z","shell.execute_reply":"2021-07-09T13:35:26.274991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mess with data by changing some row values \ndf.loc[df.query('Gender== \"F\"').sample(frac=0.021).index,'Gender'] = 'Femme'\ndf.loc[df.query('Gender ==\"M\"').sample(frac=0.011).index,'Gender'] = 'Homme'\ndf.loc[df.query('Age == 34').sample(frac=0.1).index,'Age'] = 99999","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:44:59.543014Z","iopub.execute_input":"2021-07-09T13:44:59.543378Z","iopub.status.idle":"2021-07-09T13:44:59.560466Z","shell.execute_reply.started":"2021-07-09T13:44:59.543347Z","shell.execute_reply":"2021-07-09T13:44:59.559117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping rows or columns with NaN\n\nOne approach would be removing all the rows or columns which contain missing values. This can easily be done with the *dropna()* assigned to whole the dataframe \n","metadata":{}},{"cell_type":"code","source":"# Drops all rows with NaN values\ndf.dropna(axis=0,inplace=True)\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:35:32.420079Z","iopub.execute_input":"2021-07-09T13:35:32.420612Z","iopub.status.idle":"2021-07-09T13:35:32.442903Z","shell.execute_reply.started":"2021-07-09T13:35:32.420577Z","shell.execute_reply":"2021-07-09T13:35:32.441242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look into the function parameters: \n- inplace : if it's True , that makes all the changes in the existing DataFrame without returning a new one. Without it, you'd have to re-assign the DataFrame to itself.\n\n- axis : this specifies if you're working with rows(with value 0) or columns(with value 1)\n\n- how : we can control whether you want to remove the rows containing at least 1 NaN (any) or all NaN values (all) by setting the how parameter in the dropna method.\n- thresh : we can specify the percentage in which the column or the row will be deleted\n- subset : choose columns which you want to delete if they contain NaN values. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Imputing Missing Values ","metadata":{}},{"cell_type":"markdown","source":"Dropping data isn't always the best way to deal with NaNs. these rows or columns might contain valuable data  and we don't want to skew the data towards an inaccurate state. In this case we could use another approach to handle missing data by imputing the empty values using other non null values. So we can either:\n1. Fill NaN with Mean, Median or Mode of the data\n2. Fill NaN with a constant value\n3. Forward Fill or Backward Fill NaN\n4. Interpolate Data and Fill NaN\n","metadata":{}},{"cell_type":"markdown","source":"#### 1. Fill NaN values with median, mode,  mean, or with a constant value","metadata":{}},{"cell_type":"code","source":"# Using median\ndf['Salary'].fillna(df['Salary'].mean(), inplace=True)\n  \n# Using mean\ndf['Age'].fillna(int(df['Age'].median()), inplace=True)\n  \n# Using mode\ndf['Gender'].fillna(str(df['Gender'].mode()), inplace=True)\n\n #check again \ndf[['Age','Gender','Salary']].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:35:38.967963Z","iopub.execute_input":"2021-07-09T13:35:38.968422Z","iopub.status.idle":"2021-07-09T13:35:38.98698Z","shell.execute_reply.started":"2021-07-09T13:35:38.968384Z","shell.execute_reply":"2021-07-09T13:35:38.985679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we could also fill null data in a certain column with a constant \ndf['Weight'].fillna(67, inplace=True)\n# check \ndf.Weight.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:35:42.450451Z","iopub.execute_input":"2021-07-09T13:35:42.450871Z","iopub.status.idle":"2021-07-09T13:35:42.459479Z","shell.execute_reply.started":"2021-07-09T13:35:42.450834Z","shell.execute_reply":"2021-07-09T13:35:42.458208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's mess again with salaries \ndf.loc[df.sample(frac=0.05).index, 'Salary'] = pd.np.nan\n# the forward fill (ffil) method would fill the missing values with first non-missing value that occurs before it:\ndf['Salary'].fillna(method='ffill', inplace=True)\n#same with the backward fill  method would fill the missing values with first non-missing value that occurs after it\ndf['Salary'].fillna(method='bfill', inplace=True)\n#or filling data using the polynomial interpolation \ndf['Salary'].interpolate(method ='linear', limit_direction ='forward')\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:43:01.962405Z","iopub.execute_input":"2021-07-09T13:43:01.962771Z","iopub.status.idle":"2021-07-09T13:43:01.975837Z","shell.execute_reply.started":"2021-07-09T13:43:01.96274Z","shell.execute_reply":"2021-07-09T13:43:01.974749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill the null dates with one of the frequent dates \ndf['Birth Date'].fillna('11/18/1965',inplace=True)\n#check \ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:43:57.13829Z","iopub.execute_input":"2021-07-09T13:43:57.138661Z","iopub.status.idle":"2021-07-09T13:43:57.15034Z","shell.execute_reply.started":"2021-07-09T13:43:57.138628Z","shell.execute_reply":"2021-07-09T13:43:57.148561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fill Eperience with mean of all experiences \ndf['Experience'].fillna(round(df.Experience.mean(),0),inplace=True)\n#check \ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:44:05.67711Z","iopub.execute_input":"2021-07-09T13:44:05.677672Z","iopub.status.idle":"2021-07-09T13:44:05.691121Z","shell.execute_reply.started":"2021-07-09T13:44:05.677625Z","shell.execute_reply":"2021-07-09T13:44:05.689729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Replace some irrelevant column values using a map dict","metadata":{}},{"cell_type":"code","source":"#let's have a looke at Gender columns values \ndf.Gender.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:45:14.988468Z","iopub.execute_input":"2021-07-09T13:45:14.988831Z","iopub.status.idle":"2021-07-09T13:45:14.998535Z","shell.execute_reply.started":"2021-07-09T13:45:14.988799Z","shell.execute_reply":"2021-07-09T13:45:14.997232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's replace Homme with M\ndf.Gender.replace(to_replace='Homme',value='M',inplace=True)\ndf.Gender.replace(to_replace='Femme',value='F',inplace=True)\ndf.Gender.replace(to_replace='0    M\\ndtype: object',value='M',inplace=True)\n#check \ndf.Gender.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:45:19.384383Z","iopub.execute_input":"2021-07-09T13:45:19.384755Z","iopub.status.idle":"2021-07-09T13:45:19.397889Z","shell.execute_reply.started":"2021-07-09T13:45:19.384725Z","shell.execute_reply":"2021-07-09T13:45:19.39658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we can also use a map dictionnary \ngender_dict ={'M':'Homme','F':'Femme','0    M\\ndtype: object':'M'}\n#replace old values with the right values \ndf.Gender.map(gender_dict)\n#check \ndf.Gender.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:45:33.599323Z","iopub.execute_input":"2021-07-09T13:45:33.599685Z","iopub.status.idle":"2021-07-09T13:45:33.610598Z","shell.execute_reply.started":"2021-07-09T13:45:33.599656Z","shell.execute_reply":"2021-07-09T13:45:33.609194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing outliers \nIn machine learning projects, during model building it is important to remove  outliers, because their presence can mislead the model. Existance of outliers may change the mean and standard deviation of the whole dataset that can badly affect the performance of the model. Outliers also increases the variance error and reduces the power of statistical test. We find outliers in due to data entry errors, Experimental measurement error, Measurement error(Instrument error), and\nSampling error. <br>\n\nDetecting outliers is one of the challenging job in data cleaning. There is no any precise way to detect and remove outliers due to specific of datasets. Yet, raw assumption and observation must be made to remove those outliers that seems to be unusual among all other data. The two ways for detection of outliers are:\n\n- Visualization method: using Box plots and scatter plots \n- Statistical method: using inter quartile method, standard deviation method ..<br>\n\nWe will some methods, either visual or statistical, to handle outliers. ","metadata":{}},{"cell_type":"code","source":"#let's define a function to get the outliers \ndef outlier_iqr(df, col):\n    Q1= df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    upper_limit = Q3 + 1.5 * IQR\n    lower_limit = Q1 - 1.5 * IQR\n    return upper_limit, lower_limit\n#using standard deviation \ndef outlier_std(df, col,cutoff=2):\n    upper_limit = df[col].mean() + cutoff * df[col].std()\n    lower_limit = df[col].mean() - cutoff * df[col].std()\n    return upper_limit, lower_limit\n# define a function for plotting a boxplot\ndef boxplot(df,col):\n    fig = plt.figure(figsize=(12,8))\n    sb.set_style('darkgrid')\n    sb.boxplot(data = df,x = col)\n    plt.title(\"Ages range\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:45:39.206386Z","iopub.execute_input":"2021-07-09T13:45:39.206777Z","iopub.status.idle":"2021-07-09T13:45:39.217806Z","shell.execute_reply.started":"2021-07-09T13:45:39.206744Z","shell.execute_reply":"2021-07-09T13:45:39.21563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as the plot shows above don't have any outliers. \n# we will work with an other dataset which has some to deal with. \nheart.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:45:56.597668Z","iopub.execute_input":"2021-07-09T13:45:56.598082Z","iopub.status.idle":"2021-07-09T13:45:56.614786Z","shell.execute_reply.started":"2021-07-09T13:45:56.598048Z","shell.execute_reply":"2021-07-09T13:45:56.613746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### IQR Method","metadata":{}},{"cell_type":"code","source":"boxplot(heart,'chol')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:02.252375Z","iopub.execute_input":"2021-07-09T13:46:02.252825Z","iopub.status.idle":"2021-07-09T13:46:02.472949Z","shell.execute_reply.started":"2021-07-09T13:46:02.252787Z","shell.execute_reply":"2021-07-09T13:46:02.471823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as you can see there are some outliers beyond the range upper limit \n# let's deal with them \nupper, lower = outlier_iqr(heart,'chol')\n#print \nprint(f'The upper limit is: {upper}')\nprint(f'The lower limit is: {lower}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:26.899974Z","iopub.execute_input":"2021-07-09T13:46:26.900549Z","iopub.status.idle":"2021-07-09T13:46:26.909922Z","shell.execute_reply.started":"2021-07-09T13:46:26.900513Z","shell.execute_reply":"2021-07-09T13:46:26.908526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now that we have upper and lower limits, we will filter out the column for outliers removal \noutliers = heart[(heart['chol'] < lower) | (heart['chol'] > upper)]['chol']\n# print the outliers \nprint(outliers)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:31.429909Z","iopub.execute_input":"2021-07-09T13:46:31.43032Z","iopub.status.idle":"2021-07-09T13:46:31.442277Z","shell.execute_reply.started":"2021-07-09T13:46:31.430286Z","shell.execute_reply":"2021-07-09T13:46:31.440771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's remove the outliers \nheart = heart[(heart['chol'] > lower) & (heart['chol'] < upper)]\n#plot again\nboxplot(heart,'chol')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:36.542665Z","iopub.execute_input":"2021-07-09T13:46:36.543117Z","iopub.status.idle":"2021-07-09T13:46:36.722868Z","shell.execute_reply.started":"2021-07-09T13:46:36.54308Z","shell.execute_reply":"2021-07-09T13:46:36.72151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Standad Deviation Method","metadata":{}},{"cell_type":"code","source":"#let's plot this variable from the dataset \nboxplot(heart,'trestbps')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:40.993966Z","iopub.execute_input":"2021-07-09T13:46:40.994357Z","iopub.status.idle":"2021-07-09T13:46:41.176448Z","shell.execute_reply.started":"2021-07-09T13:46:40.994325Z","shell.execute_reply":"2021-07-09T13:46:41.175395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Appears to have some outliers, get them \nupper, lower = outlier_std(heart,'trestbps')\n#print \nprint(f'The upper limit is: {upper}')\nprint(f'The lower limit is: {lower}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:45.192094Z","iopub.execute_input":"2021-07-09T13:46:45.19249Z","iopub.status.idle":"2021-07-09T13:46:45.19998Z","shell.execute_reply.started":"2021-07-09T13:46:45.192457Z","shell.execute_reply":"2021-07-09T13:46:45.198884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now that we have upper and lower limits, we will filter out the column for outliers removal \noutliers = heart[(heart['trestbps'] < lower) | (heart['trestbps'] > upper)]['trestbps']\n# print the outliers \nprint(outliers)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:48.95519Z","iopub.execute_input":"2021-07-09T13:46:48.955623Z","iopub.status.idle":"2021-07-09T13:46:48.965317Z","shell.execute_reply.started":"2021-07-09T13:46:48.95558Z","shell.execute_reply":"2021-07-09T13:46:48.963572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now let us remove them and check \nheart = heart[(heart['trestbps'] > lower) & (heart['trestbps'] < upper)]\n#plot \nboxplot(heart,'trestbps')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T13:46:54.523006Z","iopub.execute_input":"2021-07-09T13:46:54.523376Z","iopub.status.idle":"2021-07-09T13:46:54.713348Z","shell.execute_reply.started":"2021-07-09T13:46:54.523346Z","shell.execute_reply":"2021-07-09T13:46:54.712083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### And Don't Forget to Upvote if you Like the Notebook!!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}