{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Highlights \n## Used Pandas profiling for the first time and just amazed with it's utility, used np.log1p()[Explained in detail below], also explored fancyimpute"},{"metadata":{},"cell_type":"markdown","source":"## # Suggestions and views are most welcomed"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/hackerearth-machine-learning-exhibit-art/dataset/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/hackerearth-machine-learning-exhibit-art/dataset/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By problem statement I observed that score is calculated using MEAN-SQUARE-LOG-ERROR. It is so because the target has very large standard deviation(std)(shown below). So for prediction purpose I wanted to reduce std. This is done by taking[numpy.log1p] of the target variable. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y = train['Cost']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev1 = np.std(train_Y)\ndev1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y = log1p(train_Y)\ntrain_X = train.drop(['Cost'], axis=1)\ntest_X = test\ntrain_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev2 = np.std(train_Y)\ndev2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Pandas Profiling for EDA "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X.drop(['Customer Id', 'Artist Name', 'Artist Reputation', 'Transport'], axis = 1)\ntest_X = test.drop(['Customer Id', 'Artist Name', 'Artist Reputation', 'Transport'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X['State'] = train_X['Customer Location'].map(lambda x:x.split()[-2])\ntrain_X.drop(['Customer Location'], inplace = True, axis = 1)\ntest_X['State'] = test_X['Customer Location'].map(lambda x:x.split()[-2])\ntest_X.drop(['Customer Location'], inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X['Scheduled Date'] = pd.to_datetime(train_X['Scheduled Date'])\ntrain_X['Delivery Date'] = pd.to_datetime(train_X['Delivery Date'])\ntrain_X['scheduleDiff'] = (train_X['Scheduled Date'] - train_X['Delivery Date']).map(lambda x:str(x).split()[0])\ntrain_X['scheduleDiff'] = pd.to_numeric(train_X['scheduleDiff'])\ntest_X['Scheduled Date'] = pd.to_datetime(test_X['Scheduled Date'])\ntest_X['Delivery Date'] = pd.to_datetime(test_X['Delivery Date'])\ntest_X['scheduleDiff'] = (test_X['Scheduled Date'] - test_X['Delivery Date']).map(lambda x:str(x).split()[0])\ntest_X['scheduleDiff'] = pd.to_numeric(test_X['scheduleDiff'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X['dday'] = train_X['Delivery Date'].dt.day\ntrain_X['dmonth'] = train_X['Delivery Date'].dt.month\ntrain_X['dyear'] = train_X['Delivery Date'].dt.year\ntrain_X['ddayofweek'] = train_X['Delivery Date'].dt.dayofweek\n\ntest_X['dday'] = test_X['Delivery Date'].dt.day\ntest_X['dmonth'] = test_X['Delivery Date'].dt.month\ntest_X['dyear'] = test_X['Delivery Date'].dt.year\ntest_X['ddayofweek'] = test_X['Delivery Date'].dt.dayofweek\n\ntest_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.drop(['Delivery Date', 'Scheduled Date'], inplace=True, axis=1)\ntest_X.drop(['Delivery Date', 'Scheduled Date'], inplace=True, axis=1)\ntrain_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dealing with categorical columns having missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. Function to replace NAN values with mode value\ndef impute_nan_most_frequent_category(DataFrame,ColName):\n    # .mode()[0] - gives first category name\n     most_frequent_category=DataFrame[ColName].mode()[0]\n    \n    # replace nan values with most occured category\n     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n#2. Call function to impute most occured category\nfor Columns in ['Material', 'Remote Location']:\n    impute_nan_most_frequent_category(train_X,Columns)\n    impute_nan_most_frequent_category(test_X,Columns)\n    \n# Display imputed result\ntrain_X[['Material','Material_Imputed','Remote Location','Remote Location_Imputed']].head(10)\n#3. Drop actual columns\ntrain_X = train_X.drop(['Material', 'Remote Location'], axis = 1)\ntest_X = test_X.drop(['Material', 'Remote Location'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_X.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get list of categorical variables\ns = (train_X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One-Hot Encoding Categorical Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nonehot_X_train = one_hot_encoder.fit_transform(train_X[object_cols])\nonehot_X_test = one_hot_encoder.fit_transform(test_X[object_cols])\nonehot_X_train = pd.DataFrame(onehot_X_train)\nonehot_X_test = pd.DataFrame(onehot_X_test)\n\nnum_X_train = train_X.drop(object_cols, axis=1)\nnum_X_valid = test_X.drop(object_cols, axis=1)\n\nonehot_X_train = pd.concat([num_X_train, onehot_X_train], axis=1)\nonehot_X_test = pd.concat([num_X_valid, onehot_X_test], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Imputing missing values using fancyimpute**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fancyimpute import KNN \nfrom fancyimpute import IterativeImputer\n#imputer = KNN(k=5)\nimputer = IterativeImputer()\n#train_ = train[columns]\nfullset = pd.concat([onehot_X_train, onehot_X_test]) \n#fancy impute removes column names\nfullset = pd.DataFrame(imputer.fit_transform(fullset))\n#fullset.columns = columns\nfullset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = fullset[:onehot_X_train.shape[0]]\ntest_df = fullset[onehot_X_train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Normalisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nscaler = sklearn.preprocessing.StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\nX_test = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RandomForest Regressor**"},{"metadata":{},"cell_type":"markdown","source":"Earlier I tried this model and procedure"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction_rf = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prediction_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tuning RF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n{'n_estimators': [50,100,250,500], \n 'max_depth': [10, 50, 100], 'bootstrap': [True, False]}\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_forest = RandomizedSearchCV(rf, param_grid, cv=10)\nrandom_search_forest.fit(X_train, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_rf_best_random = random_search_forest.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_rf_best_random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_rf_best_random.fit(X_train, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_rf_random_pred = tuned_rf_best_random.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_rf_random_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LGBM**"},{"metadata":{},"cell_type":"markdown","source":"Nextly used this model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMRegressor()\nlgbm.fit(X_train, train_Y)\ny_test_pred_lgbm = lgbm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_lgbm2 = expm1(y_test_pred_lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_lgbm2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Customer Id'] = test['Customer Id']\ndf['Cost'] = y_test_pred_lgbm2\ndf['Cost'] = df['Cost'].abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('prediction1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}