{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset location: https://www.kaggle.com/itssuru/bike-sharing-system-washington-dc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime as dt\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/bike-sharing-system-washington-dc/train_bikes.csv',parse_dates=['datetime'])\ntest = pd.read_csv('/kaggle/input/bike-sharing-system-washington-dc/test_bikes.csv',parse_dates=['datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\n## Checking if there are null values\nThere are no null values in the training or test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the Data Types\nAll of the data is numerical, which means I won't need to do any label encoding or one hot encoding. The datetime, which is the index, can be modified to reflect time of the year. This is most likely going to be cyclical and dependent on the time of year"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspecting the data\nI will be predicting the 'count' feature. I will also most likely want to remove the 'registered' feature since it looks like this feature contains information about the count that the AI model shouldn't have access to when making predictions since that would be an example of target leakage. I'm not quite sure what the 'casual' column is, but it may also be a source of leakage.\n\nAlso, I can see that there is a season feature, which will be helpful. Having features on the season and the time of year could be useful. \n\nI'm going to normalize the data for preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_columns = ['datetime','season','holiday','workingday','weather','temp','atemp','humidity','windspeed']\n\ntarget = train['count']\ntrain = train[useful_columns]\ntest = test[useful_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_time = train.pop('datetime')\ndate_time_test = test.pop('datetime')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timestamp_s = date_time.map(dt.datetime.timestamp)\ntimestamp_test = date_time_test.map(dt.datetime.timestamp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timestamp_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day = 24*60*60\nyear = (365.2425)*day\ntrain.loc[:,'Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\ntrain.loc[:,'Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n\ntrain.loc[:,'Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\ntrain.loc[:,'Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n\ntest.loc[:,'Day sin'] = np.sin(timestamp_test * (2 * np.pi / day))\ntest.loc[:,'Day cos'] = np.cos(timestamp_test * (2 * np.pi / day))\n\ntest.loc[:,'Year sin'] = np.sin(timestamp_test * (2 * np.pi / year))\ntest.loc[:,'Year cos'] = np.cos(timestamp_test * (2 * np.pi / year))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.array(train['Day sin'])[:25])\nplt.plot(np.array(train['Day cos'])[:25])\nplt.xlabel('Time [h]')\nplt.title('Time of day signal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_indices = {name: i for i, name in enumerate(train.columns)}\n\nn = len(train)\ntrain_df = train[0:int(n*0.8)]\ntrain_target = target[0:int(n*0.8)]\n\nval_df = train[int(n*0.8):]\nval_target = target[int(n*0.8):]\n\n\nnum_features = train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize the data\nIt is important to scale features before training an AI algorithm. Normalization is a common way of doing this scaling. Subtract the mean and divide by the standard deviation of each feature.\n\nThe mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) / train_std\nval_df = (val_df - train_mean) / train_std\ntest_df = (test_df - train_mean) / train_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feed into AI algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\nimport sklearn.metrics as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Regressor\nrandom_forest = RandomForestRegressor()\n\n# K Nearest Neighbors Regressor\nkneighbors = KNeighborsRegressor()\n\n# SGD Regressor\nsgd = SGDRegressor()\n\n# xgb regressor\nxgb = XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressors_dict = {'random_forest':random_forest,'kneighbors':kneighbors,'sgd':sgd,'xgb':xgb}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without tuning the hyperparameters, the random_forest regressor seems to be performing the best"},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_df = pd.DataFrame(columns = [\"regressor\",\"score\"])\nmse_df = pd.DataFrame(columns=[\"regressor\",\"score\"])\n\nfor regressor_name in regressors_dict.keys():\n    regressor = regressors_dict[regressor_name]\n    regressor.fit(train_df, train_target)\n    predictions = regressor.predict(val_df)\n    \n    mae =  round(sm.mean_absolute_error(predictions, val_target), 2)\n    mse = round(sm.mean_squared_error(predictions, val_target), 2)\n    \n    mae_row = pd.DataFrame({'regressor':regressor_name,'score':mae}, index=[0])\n    mae_df = mae_df.append(mae_row)\n    \n    mse_row = pd.DataFrame({'regressor':regressor_name,'score':mse}, index=[0])\n    mse_df = mse_df.append(mse_row)\n    \n    print(\"Mean absolute error \" + regressor_name + \" =\", mae) \n    print(\"Mean squared error \" + regressor_name + \" =\", mse)\n    print(\"-\"*20)\n    \nmae_df = mae_df.reset_index(drop=True)\nmse_df = mse_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('MAE Classifier Comparisons')\nsns.set_color_codes(\"muted\")\nsns.barplot(x='score', y='regressor', data=mae_df, color=\"b\")\nplt.xlabel('MAE Score')\nplt.ylabel('Regressor')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('MSE Classifier Comparisons')\nsns.set_color_codes(\"muted\")\nsns.barplot(x='score', y='regressor', data=mse_df, color=\"b\")\nplt.xlabel('MSE Score')\nplt.ylabel('Regressor')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to use random_forest since it performs the best"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest.fit(train_df, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = random_forest.predict(val_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Final Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = random_forest.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = pd.DataFrame(test_predictions)\ntest_predictions.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}