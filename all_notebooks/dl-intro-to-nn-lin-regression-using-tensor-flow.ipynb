{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference Links - Evolution of Deep Neural Networks\n\n* **McCulloch-Pitts Neuron — Mankind’s First Mathematical Model Of A Biological Neuron -** https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1\n* **Perceptron Learning Algorithm: A Graphical Explanation Of Why It Works -** https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975\n\n* **How neural networks build up their understanding of images -** https://distill.pub/2017/feature-visualization/\n\n* **TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines -** https://www.tensorflow.org/tfx","metadata":{"id":"TcRuZARe0Ov9"}},{"cell_type":"markdown","source":"# <center> Linear Regression using TensorFlow\n\nIn this session, we will have a look at creating a linear regression model using tensorflow 2.0. Note that we already know the basics of linear regression and understand the implementation through sklearn. We will try to figure out how to do it using tensorflow tools that we have learnt.   \n\n\n<i>Note that to keep this exercise simple and focused on tensorflow and its relevant functions, we will make a very simple model with very basic preprocessing.</i> ","metadata":{"id":"5IIstFzwcpJE"}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"id":"ry8J8nA4ocet","outputId":"75b39cb2-6931-436e-d408-c971916c1927","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now Let us import out data and get it ready for modelling.","metadata":{"id":"ycFapGk6GTSi"}},{"cell_type":"code","source":"cars_data = pd.read_csv('/kaggle/input/dl-intro-to-nn/usedcars.csv')\ncars_data","metadata":{"id":"KGd9DbsKmQe2","outputId":"4ccb061c-6e61-4ea9-ee1e-e7ce817b0b5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cars_data.isna().sum()","metadata":{"id":"nX1zbyI75RMi","outputId":"bbf2144a-7ded-4520-d145-7639542da5aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#just some basic preprocessing\ncars_data.drop_duplicates(inplace=True)","metadata":{"id":"KuU03qQk1WfW","outputId":"3be576f2-50ee-4482-aac0-c65ea0424c21","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating dummy variables for the categorical features\ncars_data = pd.get_dummies(cars_data)\ncars_data = cars_data.astype('float32') # we will need to convert the dataset to float in order to be able to convert it into tensors later.\ncars_data","metadata":{"id":"MDWmpQY9mWi4","outputId":"9642f91a-70c5-4541-fa7e-b9ba1c2ae454","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#exploring column names\ncars_data.columns","metadata":{"id":"y4YBIUaZ5zG4","outputId":"9004fc2f-ada7-4770-b21d-ee70652e57b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the features and labels and finally splitting the test and train data.\n\n\nfrom sklearn.model_selection import train_test_split\nX = cars_data[['year','mileage', 'model_SE', 'model_SEL', 'model_SES',\n       'color_Black', 'color_Blue', 'color_Gold', 'color_Gray', 'color_Green',\n       'color_Red', 'color_Silver', 'color_White', 'color_Yellow',\n       'transmission_AUTO', 'transmission_MANUAL']]\nY = cars_data['price']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)","metadata":{"id":"xhihQaPY6LfN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let us scale the data as features are on different scales which might be a problem while modelling\nfrom sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\n# MinMaxScalar has been used here. You can go ahead and use the other scalars available and chcek the effect on the results.\n#fitting the transform on test and train separately\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train","metadata":{"id":"AJYtiMZfmiYx","outputId":"68b3f046-2c1d-42ec-e4f7-4eb499c60e8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# let us now convert the data elements into tensors as we need tensors to be fed into different tensorflow based operations\n#X-train and X_test were converted to numpy arrays while transformations while the other two need to be transformed into numpy arrays.\nX_train=tf.convert_to_tensor(X_train)\ny_train=tf.convert_to_tensor(y_train.values)\nX_test=tf.convert_to_tensor(X_test)\ny_test=tf.convert_to_tensor(y_test.values)","metadata":{"id":"PHjzg7vF5z0T","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"6ZEQAKqs66EO","outputId":"f9cb7672-0c6e-49aa-f264-d607d3b76cb3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let us try modelling now. We will use a few concepts covered in the practice exercise shared with the course material.","metadata":{"id":"mecQN830IGb9"}},{"cell_type":"code","source":"input_dim = X_train.shape[1]\noutput_dim = 1\nlearning_rate = 0.01\n\n# Let us initialize the weights and bias variables. \nweights = tf.Variable(tf.zeros(shape=(input_dim, output_dim), dtype= tf.float32))\nbias = tf.Variable(tf.ones(shape=(output_dim,), dtype= tf.float32))","metadata":{"id":"VVX-LUM8A-VW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights.shape","metadata":{"id":"vh2blCxhBBtM","outputId":"7bd542b4-2b1d-4997-8e7e-c9e396089946","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bias.shape","metadata":{"id":"AoR9D90mBFQS","outputId":"32cb519d-512e-4534-e467-1078747f408e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(features):\n    return tf.matmul(features, weights) + bias # note that the matmul is matrix multiplication and is needed for calculating predictions\n\ndef compute_loss(y_true, predictions):\n    return tf.reduce_mean(tf.square(y_true - predictions)) # mean square error\n\n# Let us now define a function to train the model. We will call the other functions in function definition.\ndef train(x, y):\n    with tf.GradientTape() as tape:\n        predictions = predict(x)\n        loss = compute_loss(y, predictions)\n        dloss_dw, dloss_db = tape.gradient(loss, [weights, bias]) #note that we can pass lists as well here.\n    weights.assign_sub(learning_rate * dloss_dw)\n    bias.assign_sub(learning_rate * dloss_db)\n    return loss","metadata":{"id":"Uhitqoj2FH8U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let us now, call the train function with 50 epochs","metadata":{"id":"pcVzQimxJJe2"}},{"cell_type":"code","source":"for epoch in range(50):\n    loss = train(X_train, y_train)\n    print('Epoch %d: Loss = %.4f' % (epoch, float(loss)))","metadata":{"id":"rWFBawhVoM4j","outputId":"269baacb-d426-49ec-e0d7-311177a62424","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Final Weights after 50 epochs:')\nprint(weights)","metadata":{"id":"5VHpA4MVu7Od","outputId":"e36873c3-2f05-4d74-ffe7-65a0a606c8e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Final Bias after 50 epochs:')\nprint(bias)","metadata":{"id":"uJAiiz7Yu9jP","outputId":"a7915439-a98d-4ef5-ba2e-2ce7aa3d328d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let us now test our model on the test data and predict on the test data.","metadata":{"id":"bdgve0a7Jwwq"}},{"cell_type":"code","source":"test_predictions = tf.matmul(X_test, weights) + bias\nprint(compute_loss(y_test, test_predictions))","metadata":{"id":"zyXnEJiyqNjv","outputId":"706ec173-3da6-4079-de41-07aafa9182fe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We learnt creating a very simple linear regression model on cars data and predicted prices. ","metadata":{"id":"X8xCBIesKGbY"}},{"cell_type":"markdown","source":"## <center> ML vs DL\n\nDeep Learning is considered as a subset of machine learning as it achieves the same goal, making machines learn from the data to gather meaningful insights. However, a common question that arises is - Why move to neural networks when we had the traditional machine learning algorithms? So let us have a look at the differences now.\n\nWe will see the differences between the two techniques based on the following parameters-\n\n* **1. Feature Engineering**\n\nMachine Learning - requires extensive feature engineering for the model to perform better.\nDeep Learning - learns to extract the relevant features from the dataset \n\n* **2. Performance with data**\n\nDeep Learning - performs poorly with lesser data but outshines the traditional machine learning algorithms with huge amounts of data. Since we are generating huge amounts of data every day, deep learning algorithms definitely perform better.\n\n* **3. Computation Requirements**\n\nTraditional machine learning algorithms generally have a low computational requirement. However, since there are not that much of matrix multiplications required in the implementation unlike neural networks based learning, therefore, these algorithms are computationally extensive.\n    \nNow, with the huge amounts of data being generated and extensive computation abilities being present, organisations are stressing on deep learning research work and most of the AI-based applications are finding their foundations in Neural Networks.\n\nHappy Learning!","metadata":{}}]}