{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T04:36:28.873079Z","iopub.execute_input":"2021-07-26T04:36:28.873988Z","iopub.status.idle":"2021-07-26T04:36:28.904556Z","shell.execute_reply.started":"2021-07-26T04:36:28.873822Z","shell.execute_reply":"2021-07-26T04:36:28.902802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"doctovec的用法,embeding如何嵌入keras,lstm  \ncopy from :https://www.kaggle.com/khotijahs1/using-lstm-for-nlp-text-classification","metadata":{}},{"cell_type":"markdown","source":"## 一、数据清洗","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:28.907103Z","iopub.execute_input":"2021-07-26T04:36:28.908341Z","iopub.status.idle":"2021-07-26T04:36:31.539981Z","shell.execute_reply.started":"2021-07-26T04:36:28.908249Z","shell.execute_reply":"2021-07-26T04:36:31.53909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv',delimiter=',',encoding='latin-1')\ndf = df[['Category','Message']]\ndf = df[pd.notnull(df['Message'])]\ndf.rename(columns = {'Message':'Message'}, inplace = True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:31.541755Z","iopub.execute_input":"2021-07-26T04:36:31.542135Z","iopub.status.idle":"2021-07-26T04:36:31.583958Z","shell.execute_reply.started":"2021-07-26T04:36:31.542099Z","shell.execute_reply":"2021-07-26T04:36:31.582783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:31.585837Z","iopub.execute_input":"2021-07-26T04:36:31.586268Z","iopub.status.idle":"2021-07-26T04:36:31.594929Z","shell.execute_reply.started":"2021-07-26T04:36:31.586233Z","shell.execute_reply":"2021-07-26T04:36:31.593445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Message'][0]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:31.596743Z","iopub.execute_input":"2021-07-26T04:36:31.597141Z","iopub.status.idle":"2021-07-26T04:36:31.609562Z","shell.execute_reply.started":"2021-07-26T04:36:31.597104Z","shell.execute_reply":"2021-07-26T04:36:31.608134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Message'].apply(lambda x: len(x.split(' '))).sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:31.61127Z","iopub.execute_input":"2021-07-26T04:36:31.611612Z","iopub.status.idle":"2021-07-26T04:36:31.637734Z","shell.execute_reply.started":"2021-07-26T04:36:31.611577Z","shell.execute_reply":"2021-07-26T04:36:31.636327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Message'].apply(lambda x: len(x.split(' '))).max()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:31.639347Z","iopub.execute_input":"2021-07-26T04:36:31.639677Z","iopub.status.idle":"2021-07-26T04:36:31.659406Z","shell.execute_reply.started":"2021-07-26T04:36:31.639646Z","shell.execute_reply":"2021-07-26T04:36:31.657899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#正负样本\ncnt_pro = df['Category'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:31.663851Z","iopub.execute_input":"2021-07-26T04:36:31.664384Z","iopub.status.idle":"2021-07-26T04:36:32.013787Z","shell.execute_reply.started":"2021-07-26T04:36:31.664331Z","shell.execute_reply":"2021-07-26T04:36:32.011757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#数据清洗\nfrom bs4 import BeautifulSoup\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['Message'] = df['Message'].apply(cleanText)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:32.016801Z","iopub.execute_input":"2021-07-26T04:36:32.017326Z","iopub.status.idle":"2021-07-26T04:36:33.368151Z","shell.execute_reply.started":"2021-07-26T04:36:32.017274Z","shell.execute_reply":"2021-07-26T04:36:33.366961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#转成wordtovec训练数据格式\nimport nltk\nfrom nltk.corpus import stopwords\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            #if len(word) < 0:\n            if len(word) <= 0:\n                continue\n            tokens.append(word.lower())\n    return tokens\ntrain_tagged = df.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.Category]), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:33.371939Z","iopub.execute_input":"2021-07-26T04:36:33.37228Z","iopub.status.idle":"2021-07-26T04:36:36.05811Z","shell.execute_reply.started":"2021-07-26T04:36:33.372249Z","shell.execute_reply":"2021-07-26T04:36:36.057194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nmax_fatures = 500000\n\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n\n#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Message'].values)\nX = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:36.059578Z","iopub.execute_input":"2021-07-26T04:36:36.060193Z","iopub.status.idle":"2021-07-26T04:36:36.365864Z","shell.execute_reply.started":"2021-07-26T04:36:36.060157Z","shell.execute_reply":"2021-07-26T04:36:36.364375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tagged.values","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:36.367366Z","iopub.execute_input":"2021-07-26T04:36:36.367701Z","iopub.status.idle":"2021-07-26T04:36:36.375303Z","shell.execute_reply.started":"2021-07-26T04:36:36.367669Z","shell.execute_reply":"2021-07-26T04:36:36.374325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#训练模型\nd2v_model = Doc2Vec(dm=1, dm_mean=1, vector_size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\nd2v_model.build_vocab([x for x in tqdm(train_tagged.values)])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:36.376604Z","iopub.execute_input":"2021-07-26T04:36:36.377102Z","iopub.status.idle":"2021-07-26T04:36:36.772602Z","shell.execute_reply.started":"2021-07-26T04:36:36.377065Z","shell.execute_reply":"2021-07-26T04:36:36.771359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor epoch in range(30):\n    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    d2v_model.alpha -= 0.002\n    d2v_model.min_alpha = d2v_model.alpha","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:36.774497Z","iopub.execute_input":"2021-07-26T04:36:36.774843Z","iopub.status.idle":"2021-07-26T04:36:50.520349Z","shell.execute_reply.started":"2021-07-26T04:36:36.77481Z","shell.execute_reply":"2021-07-26T04:36:50.519052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Doc2Vec参数：**\n\nsize 是特征向量的纬度。\n\nwindow 是要预测的词和文档中用来预测的上下文词之间的最大距离。\n\nalpha 是初始化的学习速率，会随着训练过程线性下降。\n\nseed 是随机数生成器。.需要注意的是，对于一个完全明确的重复运行（fully deterministically-reproducible run），你必须同时限制模型单线程工作以消除操作系统线程调度中的有序抖动。（在python3中，解释器启动的再现要求使用PYTHONHASHSEED环境变量来控制散列随机化）\n\nmin_count 忽略总频数小于此的所有的词。\n\nmax_vocab_size 在词汇累积的时候限制内存。如果有很多独特的词多于此，则将频率低的删去。每一千万词类大概需要1G的内存，设为None以不限制（默认）。\n\nsample 高频词被随机地降低采样的阈值。默认为0（不降低采样），较为常用的事1e-5。\n\ndm 定义了训练的算法。默认是dm=1,使用 ‘distributed memory’ (PV-DM)，否则 distributed bag of words (PV-DBOW)。\n\nworkers 使用多少现成来训练模型（越快的训练需要越多核的机器）。\n\niter 语料库的迭代次数。从Word2Vec中继承得到的默认是5，但在已经发布的‘Paragraph Vector’中，设为10或者20是很正常的。\n\nhs 如果为1 (默认)，分层采样将被用于模型训练（否则设为0）。\n\nnegative 如果 > 0，将使用负采样，它的值决定干扰词的个数（通常为5-20）。\n\ndm_mean 如果为0（默认），使用上下文词向量的和；如果为1，使用均值。（仅在dm被用在非拼接模型时使用）\n\ndm_concat 如果为1，使用上下文词向量的拼接，默认是0。注意，拼接的结果是一个更大的模型，输入的大小不再是一个词向量（采样或算术结合），而是标签和上下文中所有词结合在一起的大小。\n\ndm_tag_count 每个文件期望的文本标签数，在使用dm_concat模式时默认为1。\n\ndbow_words 如果设为1，训练word-vectors (in skip-gram fashion) 的同时训练 DBOW doc-vector。默认是0 (仅训练doc-vectors时更快)。","metadata":{}},{"cell_type":"code","source":"word_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:50.521861Z","iopub.execute_input":"2021-07-26T04:36:50.522187Z","iopub.status.idle":"2021-07-26T04:36:50.527013Z","shell.execute_reply.started":"2021-07-26T04:36:50.522149Z","shell.execute_reply":"2021-07-26T04:36:50.525833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:50.528274Z","iopub.execute_input":"2021-07-26T04:36:50.5286Z","iopub.status.idle":"2021-07-26T04:36:50.577012Z","shell.execute_reply.started":"2021-07-26T04:36:50.528567Z","shell.execute_reply":"2021-07-26T04:36:50.575989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d2v_model.wv.key_to_index","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:50.578455Z","iopub.execute_input":"2021-07-26T04:36:50.578774Z","iopub.status.idle":"2021-07-26T04:36:50.621958Z","shell.execute_reply.started":"2021-07-26T04:36:50.578742Z","shell.execute_reply":"2021-07-26T04:36:50.620791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#词向量矩阵,注意对应关系！！\nembedding_matrix = np.zeros((len(word_index) + 1, 20))\nfor word, i in word_index.items():\n    if word in d2v_model.wv.key_to_index.keys():\n        embedding_vector =  d2v_model.wv[word]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:50.623281Z","iopub.execute_input":"2021-07-26T04:36:50.623582Z","iopub.status.idle":"2021-07-26T04:36:50.667153Z","shell.execute_reply.started":"2021-07-26T04:36:50.623551Z","shell.execute_reply":"2021-07-26T04:36:50.665964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix ","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:50.668761Z","iopub.execute_input":"2021-07-26T04:36:50.669092Z","iopub.status.idle":"2021-07-26T04:36:50.676685Z","shell.execute_reply.started":"2021-07-26T04:36:50.669059Z","shell.execute_reply":"2021-07-26T04:36:50.675565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#段落向量\nsents = train_tagged.values\ntargets, regressors = zip(*[(doc.tags[0], d2v_model.infer_vector(doc.words, steps=20)) for doc in sents])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:50.679247Z","iopub.execute_input":"2021-07-26T04:36:50.679583Z","iopub.status.idle":"2021-07-26T04:36:51.770675Z","shell.execute_reply.started":"2021-07-26T04:36:50.67955Z","shell.execute_reply":"2021-07-26T04:36:51.769552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.shape(regressors)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:51.772474Z","iopub.execute_input":"2021-07-26T04:36:51.772938Z","iopub.status.idle":"2021-07-26T04:36:51.787277Z","shell.execute_reply.started":"2021-07-26T04:36:51.77289Z","shell.execute_reply":"2021-07-26T04:36:51.785689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" d2v_model['good']","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:51.789135Z","iopub.execute_input":"2021-07-26T04:36:51.789561Z","iopub.status.idle":"2021-07-26T04:36:51.798129Z","shell.execute_reply.started":"2021-07-26T04:36:51.789513Z","shell.execute_reply":"2021-07-26T04:36:51.79691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d2v_model.infer_vector(['he' ,'is','a','good','men'], steps=20) ","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:51.802266Z","iopub.execute_input":"2021-07-26T04:36:51.802599Z","iopub.status.idle":"2021-07-26T04:36:51.814246Z","shell.execute_reply.started":"2021-07-26T04:36:51.802569Z","shell.execute_reply":"2021-07-26T04:36:51.813059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 二、wordtovec lstm","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding\n\n\n# init layer\nmodel = Sequential()\n\n# emmbed word vectors\nmodel.add(Embedding(len(word_index)+1,20,input_length=X.shape[1],trainable=False,weights=[embedding_matrix]))\nmodel.add(LSTM(50,return_sequences=False))\nmodel.add(Dense(2,activation=\"softmax\"))\n\n# output model skeleton\nmodel.summary()\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:51.816296Z","iopub.execute_input":"2021-07-26T04:36:51.816625Z","iopub.status.idle":"2021-07-26T04:36:52.147517Z","shell.execute_reply.started":"2021-07-26T04:36:51.816593Z","shell.execute_reply":"2021-07-26T04:36:52.146136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(len(word_index)+1)*20","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:52.149333Z","iopub.execute_input":"2021-07-26T04:36:52.149658Z","iopub.status.idle":"2021-07-26T04:36:52.157751Z","shell.execute_reply.started":"2021-07-26T04:36:52.149627Z","shell.execute_reply":"2021-07-26T04:36:52.156417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Non-trainable params:(len(word_index)+1)*20","metadata":{}},{"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:52.159351Z","iopub.execute_input":"2021-07-26T04:36:52.159661Z","iopub.status.idle":"2021-07-26T04:36:52.285968Z","shell.execute_reply.started":"2021-07-26T04:36:52.159631Z","shell.execute_reply":"2021-07-26T04:36:52.284751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = pd.get_dummies(df['Category']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:52.287574Z","iopub.execute_input":"2021-07-26T04:36:52.287974Z","iopub.status.idle":"2021-07-26T04:36:52.304634Z","shell.execute_reply.started":"2021-07-26T04:36:52.287915Z","shell.execute_reply":"2021-07-26T04:36:52.303368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nhistory=model.fit(X_train, Y_train, epochs =50, batch_size=batch_size, verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:36:52.306325Z","iopub.execute_input":"2021-07-26T04:36:52.306658Z","iopub.status.idle":"2021-07-26T04:39:12.099003Z","shell.execute_reply.started":"2021-07-26T04:36:52.306625Z","shell.execute_reply":"2021-07-26T04:39:12.097493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_accuracy.png')\n\n# summarize history for loss\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_loss.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:12.101202Z","iopub.execute_input":"2021-07-26T04:39:12.1016Z","iopub.status.idle":"2021-07-26T04:39:12.470506Z","shell.execute_reply.started":"2021-07-26T04:39:12.10156Z","shell.execute_reply":"2021-07-26T04:39:12.469208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\n_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint('Train: %.3f, Test: %.4f' % (train_acc, test_acc))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:12.472314Z","iopub.execute_input":"2021-07-26T04:39:12.47267Z","iopub.status.idle":"2021-07-26T04:39:15.537381Z","shell.execute_reply.started":"2021-07-26T04:39:12.472637Z","shell.execute_reply":"2021-07-26T04:39:15.535505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities for test set\nyhat_probs = model.predict(X_test, verbose=0)\nprint(yhat_probs)\n# predict crisp classes for test set\nyhat_classes = model.predict_classes(X_test, verbose=0)\nprint(yhat_classes)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n#yhat_classes = yhat_classes[:, 1","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:15.539556Z","iopub.execute_input":"2021-07-26T04:39:15.540128Z","iopub.status.idle":"2021-07-26T04:39:16.468232Z","shell.execute_reply.started":"2021-07-26T04:39:15.540078Z","shell.execute_reply":"2021-07-26T04:39:16.466909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nrounded_labels=np.argmax(Y_test, axis=1)\nrounded_labels","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:16.470032Z","iopub.execute_input":"2021-07-26T04:39:16.470388Z","iopub.status.idle":"2021-07-26T04:39:16.479794Z","shell.execute_reply.started":"2021-07-26T04:39:16.470354Z","shell.execute_reply":"2021-07-26T04:39:16.478855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(rounded_labels, yhat_classes)\ncm","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:16.481065Z","iopub.execute_input":"2021-07-26T04:39:16.481501Z","iopub.status.idle":"2021-07-26T04:39:16.501347Z","shell.execute_reply.started":"2021-07-26T04:39:16.481467Z","shell.execute_reply":"2021-07-26T04:39:16.500057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlstm_val = confusion_matrix(rounded_labels, yhat_classes)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(lstm_val, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('LSTM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:16.503513Z","iopub.execute_input":"2021-07-26T04:39:16.504555Z","iopub.status.idle":"2021-07-26T04:39:16.776473Z","shell.execute_reply.started":"2021-07-26T04:39:16.504487Z","shell.execute_reply":"2021-07-26T04:39:16.775463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_size = 200\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:16.777801Z","iopub.execute_input":"2021-07-26T04:39:16.778324Z","iopub.status.idle":"2021-07-26T04:39:17.013355Z","shell.execute_reply.started":"2021-07-26T04:39:16.778272Z","shell.execute_reply":"2021-07-26T04:39:17.01201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message = ['Congratulations! you have won a $1,000 Walmart gift card. Go to http://bit.ly/123456 to claim now.']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['ham','spam']\nprint(pred, labels[np.argmax(pred)])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:17.015132Z","iopub.execute_input":"2021-07-26T04:39:17.015514Z","iopub.status.idle":"2021-07-26T04:39:17.083784Z","shell.execute_reply.started":"2021-07-26T04:39:17.015477Z","shell.execute_reply":"2021-07-26T04:39:17.082339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 三、dv-lr","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\n\nX_train, X_test, y_train, y_test = train_test_split(np.array(regressors),np.array(targets), test_size = 0.15, random_state = 42)\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Training accuracy %s' % accuracy_score(y_train,logreg.predict(X_train)))\nprint('Training F1 score: {}'.format(f1_score(y_train, logreg.predict(X_train), average='weighted')))\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T04:39:17.085703Z","iopub.execute_input":"2021-07-26T04:39:17.086234Z","iopub.status.idle":"2021-07-26T04:39:17.218362Z","shell.execute_reply.started":"2021-07-26T04:39:17.086178Z","shell.execute_reply":"2021-07-26T04:39:17.217086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}