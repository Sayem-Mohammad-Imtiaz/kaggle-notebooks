{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing in Washington D.C. with Dask\n\nTwo datasets from [Bike Sharing in Washington D.C.](https://www.kaggle.com/marklvl/bike-sharing-dataset/home) containing information about the Bike Sharing service in Washington D.C. \"Capital Bikeshare\" are provided.\n\nOne dataset contains hourly data and the other one has daily data from the years 2011 and 2012.\n\nThe following variables are included in the data:\n\n* instant: Record index\n* dteday: Date\n* season: Season (1:springer, 2:summer, 3:fall, 4:winter)\n* yr: Year (0: 2011, 1:2012)\n* mnth: Month (1 to 12)\n* hr: Hour (0 to 23, only available in the hourly dataset)\n* holiday: whether day is holiday or not (extracted from Holiday Schedule)\n* weekday: Day of the week\n* workingday: If day is neither weekend nor holiday is 1, otherwise is 0.\n* weathersit: (extracted from Freemeteo)\n    1: Clear, Few clouds, Partly cloudy, Partly cloudy\n    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n* temp: Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n* atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n* hum: Normalized humidity. The values are divided to 100 (max)\n* windspeed: Normalized wind speed. The values are divided to 67 (max)\n* casual: count of casual users\n* registered: count of registered users\n* cnt: count of total rental bikes including both casual and registered (Our target variable)\n\nWe will build a predictive model that can determine how many people will use the service on an hourly basis. We will use the first 5 quarters of the data for our training dataset and the last quarter of 2012 will be the holdout against which we perform our validation. Since that data was not used for training, we are sure that the evaluation metric that we get for it (R2 score) is an objective measurement of its predictive power.\n\n### Outline\n\nWe separate the project in 3 steps:\n\nData Loading and Exploratory Data Analysis: Load the data and analyze it to obtain an accurate picture of it, its features, its values (and whether they are incomplete or wrong), its data types among others. Also, the creation of different types of plots in order to help us understand the data and make the model creation easier.\n\nFeature Engineering / Pipeline and Hyperparameter Tuning: Once we have the data, we create some features and then create a pipeline with different transformers, we will hopefully produce a model that fits our expectations of performance. Once we have that model, a process of tuning it to the training data would be performed.\n\nResults and Conclusions: Finally, with our tuned model, we  predict against the test set we decided to separate initially, then we review those results against their actual values to determine the performance of the model, and finally, outlining our conclusions."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport dask.dataframe as dd\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom dask.distributed import Client\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom dask_ml.model_selection import RandomizedSearchCV\nfrom dask_ml.preprocessing import OneHotEncoder, PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting Key Values\n\nThe following values are used throught the code, this cell gives a central source where they can be managed."},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/bike-sharing-dataset/hour.csv\"\nSPLITS = 4\nMETRIC = \"r2\"\nSEED = 1\nTARGET = \"cnt\"","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we create the necessary Dask distributed client, you may click on the dashboard link to see the task stream and additional information."},{"metadata":{"trusted":true},"cell_type":"code","source":"client = Client()\nclient","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"<Client: scheduler='tcp://127.0.0.1:44945' processes=4 cores=4>","text/html":"<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3>Client</h3>\n<ul>\n  <li><b>Scheduler: </b>tcp://127.0.0.1:44945\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3>Cluster</h3>\n<ul>\n  <li><b>Workers: </b>4</li>\n  <li><b>Cores: </b>4</li>\n  <li><b>Memory: </b>27.40 GB</li>\n</ul>\n</td>\n</tr>\n</table>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Data Loading\n\nHere we load the necessary data, print its first rows and describe its contents.\n\nSince our dataset is small and we want to take advantage of the distributed capabilities of Dask, we set a blocksize of 300KB which distributes our code in 4 partitions."},{"metadata":{"trusted":true},"cell_type":"code","source":"types = {\n    \"season\": \"category\",\n    \"yr\": \"category\",\n    \"mnth\": \"category\",\n    \"holiday\": \"bool\",\n    \"weekday\": \"category\",\n    \"workingday\": \"bool\",\n    \"weathersit\": \"category\",\n}\n\ndf = dd.read_csv(PATH, parse_dates=[1], dtype=types, blocksize=\"300KB\")\ndf.npartitions","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"4"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Precipitation Data and Preparation\nWe will add precipitation data obtained from the [National Climatic Data Center.](https://www.ncdc.noaa.gov/cdo-web/datasets)\n\nHowever, since most of the values are 0, we will convert them to a boolean that determines if rain was present or not at that specific hour.\n\nWe also categorize the categorical features and set the date as the index."},{"metadata":{"trusted":true},"cell_type":"code","source":"precipitation = dd.read_csv(\n    \"https://gist.githubusercontent.com/akoury/6fb1897e44aec81cced8843b920bad78/raw/b1161d2c8989d013d6812b224f028587a327c86d/precipitation.csv\",\n    parse_dates=[1],\n)\ndf = dd.merge(df, precipitation, how=\"left\", on=[\"dteday\", \"hr\"])\ndf[\"precipitation\"] = (\n    df[\"precipitation\"]\n    .mask(df[\"precipitation\"].isnull(), 0)\n    .mask(df[\"precipitation\"] > 0, 1)\n    .astype(bool)\n)\ndf = df.set_index(\"dteday\")\ndf.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"            instant season yr      ...       registered  cnt  precipitation\ndteday                             ...                                     \n2011-01-01        1      1  0      ...               13   16           True\n2011-01-01        2      1  0      ...               32   40           True\n2011-01-01        3      1  0      ...               27   32           True\n2011-01-01        4      1  0      ...               10   13           True\n2011-01-01        5      1  0      ...                1    1           True\n\n[5 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instant</th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>casual</th>\n      <th>registered</th>\n      <th>cnt</th>\n      <th>precipitation</th>\n    </tr>\n    <tr>\n      <th>dteday</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2011-01-01</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.81</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>13</td>\n      <td>16</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2011-01-01</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>32</td>\n      <td>40</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2011-01-01</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>27</td>\n      <td>32</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2011-01-01</th>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>10</td>\n      <td>13</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2011-01-01</th>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Note:** head() only grabs values from the first partition, so it is not an expensive operation\n\nSince we set the date as the index for the Dask Dataframe, each division will be sorted according to its date"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.divisions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Types\n\nWe review the data types for each column."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis\n\nHere we will perform all of the necessary data analysis, with different plots that will help us understand the data and therefore, create a better model.\n\nBecause we are working in a distributed way with a supposedly large dataset, we will take a random sample of 15% of the dataset to visualize it."},{"metadata":{"trusted":false},"cell_type":"code","source":"sample = df.sample(frac=0.15, replace=True, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall distribution of the target variable\n\nHere we see the distribution in the number of users in the sample per hour"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsns.distplot(sample[TARGET])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Usage per hour of the day by registered and casual users\n\nHere we can see usage per hour differing between registered users and casual users.\n\nWe can assume that most of the registered users use the service to get to work/school, therefore peak hours are in the morning and in the afternoon, meanwhile, casual users do not have any big peaks."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\ngrouped = (\n    sample.groupby(\"hr\")\n    .agg({\"registered\": \"mean\", \"casual\": \"mean\"})\n    .reset_index()\n    .compute()\n)\nsns.lineplot(data=grouped, x=\"hr\", y=\"registered\", palette=\"husl\", label=\"registered\")\nsns.lineplot(data=grouped, x=\"hr\", y=\"casual\", palette=\"husl\", label=\"casual\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Users\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Usage per month by registered and casual users\n\nHere we can see usage per month, the colder months are the ones with least usage and the summer months have the most usage."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsample[\"mnth\"] = sample[\"mnth\"].astype(\"int\")\ngrouped = (\n    sample.groupby(\"mnth\")\n    .agg({\"registered\": \"mean\", \"casual\": \"mean\"})\n    .reset_index()\n    .compute()\n)\nsns.lineplot(data=grouped, x=\"mnth\", y=\"registered\", palette=\"husl\", label=\"registered\")\nsns.lineplot(data=grouped, x=\"mnth\", y=\"casual\", palette=\"husl\", label=\"casual\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Users\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Usage per day of the week by registered and casual users\n\nHere we can see usage per day of the week.\n\nFor registered users, usage goes down during the weekend days and up during the working days, while for casual users is the contrary"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsample[\"weekday\"] = sample[\"weekday\"].astype(\"int\")\ngrouped = (\n    sample.groupby(\"weekday\")\n    .agg({\"registered\": \"mean\", \"casual\": \"mean\"})\n    .reset_index()\n    .compute()\n)\nsns.lineplot(\n    data=grouped, x=\"weekday\", y=\"registered\", palette=\"husl\", label=\"registered\"\n)\nsns.lineplot(data=grouped, x=\"weekday\", y=\"casual\", palette=\"husl\", label=\"casual\")\nplt.xlabel(\"Weekday\")\nplt.ylabel(\"Users\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplot of Numerical Variables\n\nWe review the distribution of numerical data through a boxplot for each variable.\n\nSome features have many outliers, therefore some sort of scaling and skewness fixing may be of use."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 10))\nnumeric = sample[\n    [\"instant\", \"hum\", \"atemp\", \"temp\", \"windspeed\", \"casual\", \"registered\", \"cnt\"]\n].compute()\nnumeric = (numeric - numeric.mean()) / numeric.std()\nsns.boxplot(data=numeric, orient=\"h\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Correlation\n\nNow we will analyze correlation in the data for all variables.\n\nFrom this we see that 'temp' and 'atemp' are highly correlated as well as 'season' and 'month' therefore we will remove one variable of each group"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(13, 13))\nsns.heatmap(\n    sample.astype(float).corr(),\n    cmap=\"coolwarm\",\n    center=0,\n    square=True,\n    annot=True,\n    xticklabels=sample.columns,\n    yticklabels=sample.columns,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n\n#### Is Late\nWe add a boolean to determine if it is late during the day or not, since we see from the visualizations that usage varies greatly"},{"metadata":{"trusted":false},"cell_type":"code","source":"df[\"is_late\"] = (df[\"hr\"] > 20) | (df[\"hr\"] < 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test Split\nNow we take the last quarter of the data as our testing set and the remaining rows as our training set.\n\nWe also drop the columns dteday because we do not need the date, casual and registered since they make up the target variable and correlated columns."},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop([\"season\", \"atemp\", \"casual\", \"registered\"], axis=1)\ndf[\"hr\"] = df[\"hr\"].astype(\"category\")\ndf = df.categorize()\ntrain_df = df.loc[:\"2012-09-30\"]\nholdout = df.loc[\"2012-10-01\":]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline Creation\nWe create the pipeline that will be used for our data.\n\nInitially it standardizes the data and fixes its skewness, then it one-hot encodes the categorical variables and finally it runs it through a model.\n\nThe models from the Dask library did not work in the pipeline see relevant issues for [logistic regression](https://github.com/dask/dask-ml/issues/84) and [xgboost](https://github.com/dask/dask-xgboost/issues/31) therefore only models from sklearn and xgboost directly were used. The RandomizedSearchCV does come from Dask, which is the important part for the distribution of tasks."},{"metadata":{"trusted":false},"cell_type":"code","source":"num_pipeline = Pipeline([(\"power_transformer\", PowerTransformer(method=\"yeo-johnson\", standardize=True))])\n\ncategorical_pipeline = Pipeline([(\"one_hot\", OneHotEncoder())])\n\npipe = Pipeline([\n    (\"column_transformer\", ColumnTransformer([\n        (\"numerical_pipeline\", num_pipeline, [\"instant\", \"hum\", \"temp\", \"windspeed\"]),\n        (\"categorical_pipeline\", categorical_pipeline, [\"yr\", \"mnth\", \"hr\", \"weekday\", \"weathersit\"]),\n    ], remainder=\"passthrough\")),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scoring with Hyperparameter Tuning\nA model is added to the pipeline which is then inserted to a RandomizedSearchCV which cross validates a grid of parameters with a time series split, in order to choose the best ones.\n\nTo begin we split X and y."},{"metadata":{"trusted":false},"cell_type":"code","source":"X = train_df.drop([TARGET], axis=1)\ny = train_df[TARGET]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit function\nThis function will be used by the different pipelines to see the best performing one, it acceps the data, the pipe, the final model and its grid"},{"metadata":{"trusted":false},"cell_type":"code","source":"def fit(X, y, pipe, model, grid):\n    pipe = clone(pipe)\n    pipe.steps.append(model)\n    gridpipe = RandomizedSearchCV(\n        pipe,\n        grid,\n        n_iter=100,\n        cv=TimeSeriesSplit(n_splits=SPLITS),\n        scoring=METRIC,\n        random_state=SEED,\n    )\n\n    gridpipe.fit(X, y)\n\n    print(\"Model: \" + str(model[0]))\n\n    print(\"Best Parameters: \" + str(gridpipe.best_params_))\n    print(\"Best Fold Score: \" + str(gridpipe.best_score_))\n\n    return gridpipe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = (\"linear_reg\", LinearRegression())\n\ngrid = {\n    \"linear_reg__normalize\": [True, False],\n    \"linear_reg__fit_intercept\": [True, False],\n}\n\nlr_pipe = fit(X, y, pipe, model, grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = (\"xgb\", XGBRegressor(random_state=SEED))\n\ngrid = {\n    \"xgb__max_depth\": [3, 5],\n    \"xgb__learning_rate\": [0.1, 0.2],\n    \"xgb__n_estimators\": [100, 200],\n}\n\nxgb_gridpipe = fit(X, y, pipe, model, grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = (\"random_forest\", RandomForestRegressor(n_estimators=100, random_state=SEED))\n\ngrid = {\n    \"random_forest__max_depth\": [80, 100],\n    \"random_forest__min_samples_leaf\": [3, 5],\n    \"random_forest__min_samples_split\": [5, 10],\n    \"random_forest__max_leaf_nodes\": [None, 30],\n}\n\nrf_gridpipe = fit(X, y, pipe, model, grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results\nNow we select the best tuned model which is XGB, and with it, make a prediction on the holdout data and obtain its metrics."},{"metadata":{"trusted":false},"cell_type":"code","source":"final_pipe = xgb_gridpipe\nX_test = holdout.drop([TARGET], axis=1)\ny_test = holdout[TARGET]\n\npredicted = final_pipe.predict(X_test)\nscores = {}\nscores[\"R2\"] = r2_score(y_test, predicted)\nscores[\"MAE\"] = mean_absolute_error(y_test, predicted)\nscores[\"MSE\"] = mean_squared_error(y_test, predicted)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots of Predictions\n\nHere we plot the different results obtained.\n\nFor this scatter plot, the straighter the diagonal line is, the better the predictions since they are closer to the actual values."},{"metadata":{"trusted":false},"cell_type":"code","source":"y_test = y_test.compute()\nplt.figure(figsize=(11, 9))\nplt.scatter(y_test, predicted, alpha=0.3)\nplt.ylabel(\"Predicted\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Entire daily predictions vs. reality plot\n\nDoing dictionary comprehension to avoid adding numpy or pandas as dependencies"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_test = y_test.reset_index(drop=True)\npredicted = {\n    i: predicted[24 * i : (24 * i) + 24].sum() for i in range(len(predicted) // 24)\n}\n\nplt.figure(figsize=(19, 9))\nax = sns.lineplot(\n    data=y_test.groupby(y_test.index // 24).sum(), color=\"red\", label=\"Actual\"\n)\nax = sns.lineplot(\n    list(predicted.keys()), list(predicted.values()), color=\"blue\", label=\"Predicted\"\n)\nplt.xlabel(\"Day\")\nplt.ylabel(\"Users\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nWe created a model that, based on certain parameters, determine bike usage on an hourly basis, with these results we can provide an estimation of usage which can be of great importance for all of the involved parties.\n\nOne of the key findings is that there is a great difference in usage from weekends to normal working days and in usage throught the day depending on registered and casual users, this situation needs to be considered by the company to supply the correct amount of bicicles depending on the day and time of the week, since the demand changes drastically. Then, as can be guessed, temperature plays a big role in usage, although it is more significant in casual users.\n\nAfter performing multiple data preparation steps and transformations with different tuned models, we finally choose the best performing one on trained data and we obtain our predictions, we can see from them that the model follows along many of the peaks and valleys of the real data.\n\nMany different bike-sharing companies accross the world could use this model to estimate bike usage, planify better for expected demand and even help their governments transportation requirements. Measuring the impact of new bike infrastructure on cycling traffic and behavior is top of mind for many planners and advocacy groups.\n\nAs for Dask usage, we see great improvements in terms of speed, which make working in a distributed fashion very easy. Furthermore, it seems like the library is continually improving, currently there are multiple issues that need to be tackled in order to ensure correct funtionality across the board but its future is promising as a good distributed and familiar alternative to pandas, without having to tackle Spark."},{"metadata":{"trusted":false},"cell_type":"code","source":"client.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}