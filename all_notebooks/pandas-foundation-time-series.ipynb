{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"27\"></a> <br>\n### BUILDING DATA FRAMES FROM SCRATCH\n* We can build data frames from csv as we did earlier.\n* Also we can build dataframe from dictionaries\n    * zip() method: This function returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.\n* Adding new column\n* Broadcasting: Create new column and assign a value to entire column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some Exercises of Pandas Library\ndepartments=[\"Electrical Engineering\",\"Computer Engineering\",\"Civil Engineering\"]\nmale_pop=[75,60,120]\nfemale_pop=[75,90,30]\nlabels=[\"Departments\",\"Male Pop\",\"Female Pop\"]\nlistcol=[departments,male_pop,female_pop]\nlistcol\nzipped=list(zip(labels,listcol))#Zip List and convert to List\nzipped\nzipped_dict=dict(zipped)#Convert to dictionary\nzipped_dict\ndataFrame=pd.DataFrame(zipped_dict)#Pandas Data Frame\ndataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add new columns\ndataFrame[\"Exam Score\"]=[355,365,345]\ndataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Broadcasting \ndataFrame[\"University\"]=\"ITU\"\ndataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"28\"></a> <br>\n### VISUAL EXPLORATORY DATA ANALYSIS\n* Plot\n* Subplot\n* Histogram:\n    * bins: number of bins\n    * range(tuble): min and max values of bins\n    * normed(boolean): normalize or not\n    * cumulative(boolean): compute cumulative distribution\n    "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Get .csv files\ndata=pd.read_csv(\"/kaggle/input/retail-data-customer-summary-learn-pandas-basics/1_Sales.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show data head\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show Correlation \ndata.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Numerical Data\nimport matplotlib.pyplot as plt\ndataplt=data.loc[:,[\"Sales\",\"Units Sold\"]]\ndataplt.plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SubPlots\ndataplt.plot(subplots=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scatter Plots\ndataplt.plot(kind=\"scatter\",x=\"Sales\",y=\"Units Sold\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram\ndataplt.plot(kind = \"hist\",y = \"Units Sold\",bins = 50,range= (0,250))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram Subplot with non cumulative and cumulative\nfig,axes = plt.subplots(nrows=2,ncols=1)\ndataplt.plot(kind = \"hist\",y = \"Units Sold\",bins = 50,range= (0,250),ax = axes[0])\ndataplt.plot(kind = \"hist\",y = \"Units Sold\",bins = 50,range= (0,250),ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"29\"></a> <br>\n### STATISTICAL EXPLORATORY DATA ANALYSIS\nI already explained it at previous parts. However lets look at one more time.\n* count: number of entries\n* mean: average of entries\n* std: standart deviation\n* min: minimum entry\n* 25%: first quantile\n* 50%: median or second quantile\n* 75%: third quantile\n* max: maximum entry"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get Sample first 25 rows of data\ndata_samp=data.head(25)\ndata_samp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show Empty data\ndata_samp.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mean of Sales\nmean_sales=data_samp[\"Sales\"].mean()\nmean_sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill data sales of its mean \ndata_samp[\"Sales\"]=data_samp[\"Sales\"].fillna(mean_sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_samp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mean of Units Sold\nmean_sold=data_samp[\"Units Sold\"].mean()\nmean_sold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill empty data Units Sold of its mean \ndata_samp[\"Units Sold\"]=data_samp[\"Units Sold\"].fillna(mean_sold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_samp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add time list\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndata_timelist=data_samp.head()\ndate_list=[\"1992-01-10\",\"1992-02-10\",\"1992-03-10\",\"1993-03-15\",\"1993-03-16\"]\ndate_object=pd.to_datetime(date_list)\ndate_object\ndata_timelist[\"Date\"]=date_object\ndata_timelist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Date is index\ndata_timelist=data_timelist.set_index(\"Date\")\ndata_timelist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can select according to our date index\nprint(data_timelist.loc[\"1993-03-16\"])\nprint(data_timelist.loc[\"1992-03-10\":\"1993-03-16\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"31\"></a> <br>\n### RESAMPLING PANDAS TIME SERIES\n* Resampling: statistical method over different time intervals\n    * Needs string to specify frequency like \"M\" = month or \"A\" = year\n* Downsampling: reduce date time rows to slower frequency like from daily to weekly\n* Upsampling: increase date time rows to faster frequency like from daily to hourly\n* Interpolate: Interpolate values according to different methods like ‘linear’, ‘time’ or index’ \n    * https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_timelist.resample(\"A\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_timelist.resample(\"A\").median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_timelist.resample(\"M\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In real life (data is real. Not created from us like data2) we can solve this problem with interpolate\n# We can interpolete from first value\ndata_timelist.resample(\"M\").first().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In real life (data is real. Not created from us like data2) we can solve this problem with interpolate\n# We can interpolete from first value\ndata_timelist.resample(\"M\").mean().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}