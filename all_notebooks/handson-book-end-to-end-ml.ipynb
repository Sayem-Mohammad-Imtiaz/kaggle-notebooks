{"cells":[{"metadata":{},"cell_type":"markdown","source":"> The example and explanations in this kernel is taken from \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by **Aurélien Géron**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nhousing = pd.read_csv(\"../input/california-housing-prices/housing.csv\")\nhousing.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Observations"},{"metadata":{},"cell_type":"markdown","source":"1. First, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the **data has been scaled and capped ** at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is** not necessarily a problem, but you should try to understand how the data was computed**.\n\n1. The **housing median age and the median house value were also capped**. The **latter may be a serious problem since it is your target attribute** (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:\n\n * Collect proper labels for the districts whose labels were capped.\n\n * Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\n\n1. These attributes have very different scales. We will discuss this later in this chapter, when we explore feature scaling.\n\n1. Finally, many histograms are tail-heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions."},{"metadata":{},"cell_type":"markdown","source":"# Create A Test Set\nYou shoud split your data for training vs testing. Scikit provides a function **train_test_split** that you can use to split your data into training vs testing\n`from sklearn.model_selection import train_test_split`\n\nTo ensure same split for training data and test data , we need an index on some static field in the data.\n\n#### We can either create an index based on row (only works if you add data to the file in append only mode) like below\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head(5)  # Lets examine the modified data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.size/housing.size # Training set now contains 80% of housing data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.size/housing.size # Test set now contains 20% of housing data. We will use this for generalization accuracy of our model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population. For example, the US population is 51.3% females and 48.7% males, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called **stratified sampling**: the population is divided into homogeneous subgroups called **strata**, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population\n\nSuppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely (back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000), but some median incomes go far beyond 6. It is **important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of a stratum’s importance may be biased**. This means that **you should not have too many strata**, and **each stratum should be large enough**. The following code uses the pd.cut() function to create an income category attribute with five categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].hist()  # Observe that we have enough samples in each stratum , so this division looks good.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts()/housing.size  # These are our original percentages in samples by the income category","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For stratified sampling, you can use Scikit-Learn’s StratifiedShuffleSplit class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets verify if our strat_train_set has the same percentages of samples as in the original"},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set[\"income_cat\"].value_counts()/strat_train_set.size # These look very close to the percentages in the entire dataset population","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets drop the income_cat as we computed this only for acheiving our splits not as a feature to be considered for machine learning\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discover and Visualize the Data to Gain Insights\nSo far you have only taken a quick glance at the data to get a general understanding of the kind of data you are manipulating. Now the goal is to go into a little more depth.\n\nFirst, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast. In our case, the set is quite small, so you can just work directly on the full set. Let’s create a copy so that you can play with it without harming the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing geographical data\nSince there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks like California all right, but other than that it is hard to see any particular pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points "},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that’s much better: you can clearly see the high-density areas, namely the **Bay Area and around Los Angeles and San Diego**, plus a long line of fairly high density in the Central Valley, in particular around **Sacramento and Fresno**.\n\nOur brains are very good at spotting patterns in pictures, but you may need to play around with visualization parameters to make the patterns stand out.\n\nNow let’s look at the housing prices. The radius of each circle represents the district’s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices)"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking for Correlations\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"total_rooms\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to check for correlation between attributes is to use the pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 112 = 121 plots, which would not fit on a page—so let’s just focus on a few promising attributes that seem most correlated with the median housing value "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot reveals a few things. First, the correlation is indeed very strong; you can clearly see the upward trend, and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500,000. But this plot reveals other less obvious straight lines: a horizontal line around 450,000, another around 350,000 and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks."},{"metadata":{},"cell_type":"markdown","source":"# Experimenting with attribute combinations\nOne last thing you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now let’s look at the correlation matrix again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hey, not bad! **The new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms**. Apparently houses with a lower bedroom/room ratio tend to be more expensive.** The number of rooms_per_household is also more correlated than the total_rooms in a district—obviously the larger the houses**, the more expensive they are.\n\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step."},{"metadata":{},"cell_type":"markdown","source":"# Prepare the Data for Machine Learning Algorithms\n\nLet’s revert to a clean training set (by copying strat_train_set once again). Let’s also separate the predictors and the labels, since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set):"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\n\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. We saw earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:\n\n* Get rid of the corresponding districts.\n\n* Get rid of the whole attribute.\n\n* Set the values to some value (zero, the mean, the median, etc.).\n\nYou can accomplish these easily using DataFrame’s dropna(), drop(), and fillna() methods:\n\n`\nhousing.dropna(subset=[\"total_bedrooms\"])    # option 1\nhousing.drop(\"total_bedrooms\", axis=1)       # option 2\nmedian = housing[\"total_bedrooms\"].median()  # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n`\n\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer. Here is how to use it. First, you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the median can only be computed on numerical attributes, you need to create a copy of the data without the text attribute ocean_proximity:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num = housing.drop(\"ocean_proximity\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you can fit the imputer instance to the training data using the fit() method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(housing_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num.median().values  # Check above by calculating medians yourself","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = imputer.transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is a plain NumPy array containing the transformed features. If you want to put it back into a pandas DataFrame, it’s simple:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Text and Categorical Attributes\n\nSo far we have only dealt with numerical attributes, but now let’s look at text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first 10 instances:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It’s not arbitrary text: there are a limited number of possible values, each of which represents a category. So this attribute is a categorical attribute. Most Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from sklearn.preprocessing import OrdinalEncoder\n>>> ordinal_encoder = OrdinalEncoder()\n>>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n>>> housing_cat_encoded[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can get the list of categories using the categories_ instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> ordinal_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from sklearn.preprocessing import OneHotEncoder\n>>> cat_encoder = OneHotEncoder()\n>>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n>>> housing_cat_1hot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. You can use it mostly like a normal 2D array, but if you really want to convert it to a (dense) NumPy array, just call the toarray() method:"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> housing_cat_1hot.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, you can get the list of categories using the encoder’s categories_ instance variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tip\nIf a categorical attribute has a large number of possible categories (e.g., country code, profession, species), then one-hot encoding will result in a large number of input features. This may slow down training and degrade performance. If this happens, you may want to replace the categorical input with useful numerical features related to the categories: for example, you could replace the ocean_proximity feature with the distance to the ocean (similarly, a country code could be replaced with the country’s population and GDP per capita). Alternatively, you could replace each category with a learnable, low-dimensional vector called an embedding. Each category’s representation would be learned during training. This is an example of representation learning (see Chapters 13 and 17 for more details)."},{"metadata":{},"cell_type":"markdown","source":"## Custom Transformers\n You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods: fit() (returning self), transform(), and fit_transform().\n\nYou can get the last one for free by simply adding **TransformerMixin** as a base class. If you add **BaseEstimator** as a base class (and avoid *args and \\**kargs in your constructor), you will also get two extra methods (get_params() and set_params()) that will be useful for automatic hyperparameter tuning.\n\nFor example, here is a small transformer class that adds the combined attributes we discussed earlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this example the transformer has one hyperparameter, add_bedrooms_per_room, set to True by default (it is often helpful to provide sensible defaults). This hyperparameter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and saving you a lot of time)."},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required.\n\nThere are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n\n**Min-max scaling** (many people call this normalization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called **MinMaxScaler** for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1.\n\n**Standardization ** is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called **StandardScaler** for standardization."},{"metadata":{},"cell_type":"markdown","source":"# Transformation Pipelines\nAs you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Here is a small pipeline for the numerical attributes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news is that it works great with pandas DataFrames. Let’s use it to apply all the transformations to the housing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Evaluating on the Training Set\nThe good news is that thanks to all these previous steps, things are now going to be much simpler than you might think. Let’s first train a Linear Regression model, like we did in the previous chapter:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done! You now have a working Linear Regression model. Let’s try it out on a few instances from the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> some_data = housing.iloc[:5]\n>>> some_labels = housing_labels.iloc[:5]\n>>> some_data_prepared = full_pipeline.transform(some_data)\n>>> print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n>>> print(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It works, although the predictions are not exactly accurate (e.g., the first prediction is off by close to 40%!). Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error() function:"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from sklearn.metrics import mean_squared_error\n>>> housing_predictions = lin_reg.predict(housing_prepared)\n>>> lin_mse = mean_squared_error(housing_labels, housing_predictions)\n>>> lin_rmse = np.sqrt(lin_mse)\n>>> lin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is better than nothing, but clearly not a great score: most districts’ median_housing_values range between $120,000 and $265,000, so a typical prediction error of $68,628 is not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features (e.g., the log of the population), but first let’s try a more complex model to see how it does.\n\nLet’s train a **DecisionTreeRegressor**. This is a powerful model, capable of finding complex nonlinear relationships in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the model is trained, let’s evaluate it on the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> housing_predictions = tree_reg.predict(housing_prepared)\n>>> tree_mse = mean_squared_error(housing_labels, housing_predictions)\n>>> tree_rmse = np.sqrt(tree_mse)\n>>> tree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wait, what!? No error at all? Could this model really be absolutely perfect? Of course, it is much more likely that the **model has badly overfit the data**. How can you be sure? As we saw earlier, **you don’t want to touch the test set until you are ready to launch a model you are confident about**, so **you need to use part of the training set for training and part of it for model validation**."},{"metadata":{},"cell_type":"markdown","source":"## Better Evaluation Using Cross-Validation\nOne way to evaluate the Decision Tree model would be to use the train_test_split() function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. It’s a bit of work, but nothing too difficult, and it would work fairly well.\n\nA great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets look at the results"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> def display_scores(scores):\n...     print(\"Scores:\", scores)\n...     print(\"Mean:\", scores.mean())\n...     print(\"Standard deviation:\", scores.std())\n...\n>>> display_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to perform worse than the Linear Regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 71,407, generally ±2,439. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always possible.\n\nLet’s compute the same scores for the Linear Regression model just to be sure:"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n...                              scoring=\"neg_mean_squared_error\", cv=10)\n...\n>>> lin_rmse_scores = np.sqrt(-lin_scores)\n>>> display_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s try one last model now: the RandomForestRegressor. As we will see in Chapter 7, Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further. We will skip most of the code since it is essentially the same as for the other models"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> from sklearn.ensemble import RandomForestRegressor\n>>> forest_reg = RandomForestRegressor()\n>>> forest_reg_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n...                              scoring=\"neg_mean_squared_error\", cv=10)\n...\n>>> forest_reg_rmse_scores = np.sqrt(-forest_reg_scores)\n>>> display_scores(forest_reg_rmse_scores)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, this is much better: Random Forests look very promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into Random Forests, however, you should try out many other models from various categories of Machine Learning algorithms (e.g., several Support Vector Machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models."},{"metadata":{},"cell_type":"markdown","source":"# Fine-Tune Your Model\nLet’s assume that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that.\n\n## Grid Search\nOne option would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\n\nInstead, you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor:"},{"metadata":{},"cell_type":"markdown","source":"**Be Patient as this will be a bit slow**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict (don’t worry about what these hyperparameters mean for now; they will be explained later), then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True (which is the default value for this hyperparameter).\n\nThe grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor hyperparameter values, and it will train each model 5 times (since we are using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":">>> grid_search.best_params_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}