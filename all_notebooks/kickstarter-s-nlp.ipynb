{"cells":[{"metadata":{"_uuid":"c9f40d255e943a2d9fe538fdfd71298b484955a9"},"cell_type":"markdown","source":"## Introduction:\n\n> Kickstarter is an American public-benefit corporation[2] based in Brooklyn, New York, that maintains a global crowdfunding platform focused on creativity and merchandising.[3] The company's stated mission is to \"help bring creative projects to life\".[4] Kickstarter has reportedly received more than $1.9 billion in pledges from 9.4 million backers to fund 257,000 creative projects, such as films, music, stage shows, comics, journalism, video games, technology and food-related projects.[5]\nPeople who back Kickstarter projects are offered tangible rewards or experiences in exchange for their pledges.[6] This model traces its roots to subscription model of arts patronage, where artists would go directly to their audiences to fund their work.[7][Wikipedia](https://en.wikipedia.org/wiki/Kickstarter)\n\nSo, with the help of the crawlers of [webrobots](https://webrobots.io/kickstarter-datasets/), we got all the data from the run along 2017 kickstater projects and keep just those written in english and finished either as \"successful\" or \"failed\", and two columns:\n\n- the one with the blurb or short description of the project [text]\n- the one with the final state: \"successful\" if the project got the money goal in time or \"failed\" if don't [factor]\n\nThis kernel will deploy a linguistic model with embeddings and RNN which predict from the blurb/short description of the project if will be \"successful\" or \"failed\".\n\nAnother works, like this from [Sawhney et al](https://stanford.edu/~kartiks2/kickstarter.pdf), propose to include other variables of the project and the NLP part to get a 0.7 accuracy, but we'll use here just the NLP part."},{"metadata":{"_uuid":"c00980fef6b428da8e16c68af312c593aa46ded5"},"cell_type":"markdown","source":"## Getting and preprocessing data:\n\nThe first is to get the dataset which have been disposed as public [here](https://www.kaggle.com/oscarvilla/kickstarter-nlp) on Kaggle with pandas, and to map the categories to numeric ones: 1 if \"successful\" and 0 if \"failed\"."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"../input/df_text_eng.csv\")\ndf[\"state\"] = df[\"state\"].map({'successful': 1, 'failed': 0})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca0dd1edf7f52941ad775d5fedaa095b5a44d9b1"},"cell_type":"markdown","source":"Here we need to pass from pandas dataframe to list of strings"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"texts = list(df[\"blurb\"].astype(str).str.split(\",\")) # Convert the column to string and to list... list of strings\nlabels = list(df[\"state\"].astype(str).str.split(\",\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54f49806db7afba8bf33e0b23df0f537a7b466b0"},"cell_type":"markdown","source":"Sanity checks of the texts and the labels"},{"metadata":{"trusted":true,"_uuid":"a6fcf593047f0123270564b4bfd0fadfbc09be74","collapsed":true},"cell_type":"code","source":"texts[1:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81ca59c78466cd4b70af92ef97a41e1c57861fd1","collapsed":true},"cell_type":"code","source":"labels[1:6]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81ab40ff8a1649494f0fe881b95cff658f94491e"},"cell_type":"markdown","source":"The preprocess needed here it's to convert the words sequences to a sequences of numbers, where each number represent a word. That's done with a tokenizer to which we've defined the more frequent words to take in account (20000).\nAlso, we have to define a fixed length for the sequences (25): truncating the larger and padding the shorters.\n\nLastly, we define how much of the sample will be used for training and how much for validation and test."},{"metadata":{"trusted":true,"_uuid":"c61d33ecc4435ac6eb6465889d92b8448d2a3b43","collapsed":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 25\ntraining_samples = 172410\nvalidation_samples = 21551\ntest_samples = 21551\nmax_words = 20000\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"278187c272d4781ac08c1a463621ec85cdd3b9a2"},"cell_type":"markdown","source":"Padding and check dimensions."},{"metadata":{"trusted":true,"_uuid":"8efa1707784d279d086d522ae0a2a8fd1828ab03","collapsed":true},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=maxlen)\nlabels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of labels tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e3ea123142a759f83484e5863cb1ff3aa1b3b2c"},"cell_type":"markdown","source":"Here we shuffle the data and create partitions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b99e0057ea0f6d0733fe77e8d97529dc30fe9794"},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ee003a23b6b23ec642a61ab66cdc86c1c976b98","collapsed":true},"cell_type":"code","source":"x_train = data[:training_samples]\ny_train = labels[:training_samples]\nk = training_samples + validation_samples\nx_val = data[training_samples: k]\ny_val = labels[training_samples: k]\nl = training_samples + validation_samples + test_samples\nx_test = data[k:l]\ny_test = labels[k:l]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62459b5e0acea795b845a1f1324f9234b9138eab","collapsed":true},"cell_type":"code","source":"print('Shape of x_train tensor:', x_train.shape)\nprint('Shape of x_val tensor:', x_val.shape)\nprint('Shape of x_test tensor:', x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3279835345575cf4aabbf9fb98a314d098564d7"},"cell_type":"markdown","source":"## The model:\n\nFinally we define the model, compile it and run for training"},{"metadata":{"trusted":true,"_uuid":"2fb642d5853309d32241dd71ba926ee2e3aad6c4","collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, LSTM\n\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(100, recurrent_dropout = 0.3, dropout = 0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce7b0ad2fd839dc32487d95306f2bbbd842a2876","collapsed":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['acc'])\nhistory = model.fit(x_train, y_train,\nepochs=3,\nbatch_size=64,\nvalidation_data=(x_val, y_val))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}