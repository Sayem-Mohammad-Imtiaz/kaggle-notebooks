{"cells":[{"metadata":{"_uuid":"bc65d74b886038781f65d7582ad59d86be435274"},"cell_type":"markdown","source":"# Sberbank Data exploration and Modelling"},{"metadata":{"trusted":false,"_uuid":"675001325f29dd0424719c90f669a6d837e38749"},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport math as m\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy import stats\nimport statsmodels.api as sm\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.model_selection import GridSearchCV\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nfrom sklearn.metrics import mean_absolute_error, r2_score , mean_squared_error\n\n#to randomly split data into train and test\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nseed=45","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4733700fa105cd4631574aadb3f1bc080a684d57"},"cell_type":"markdown","source":"## Importing the train and test datasets "},{"metadata":{"trusted":false,"_uuid":"d498894c17285d785c20745c63a6ee7a47996bd4"},"cell_type":"code","source":"traindf = pd.read_csv(path+\"train.csv\")\ntestdf = pd.read_csv(path+\"test.csv\")\n\nprint(traindf.shape)\nprint(testdf.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6c5955054a6add97b07eed635f75e6def6c3218"},"cell_type":"markdown","source":"THe test dataset will not be used in the course of this excercise and we will only submit our scores later on into the kaggle challenge using the test. Importing it now to check scores later"},{"metadata":{"_uuid":"d04a3b17ca0be402b0dc43e547771537ffb4c581"},"cell_type":"markdown","source":"### We will be trying to predict the price of houses using all the other variables in the dataset. "},{"metadata":{"_uuid":"7af2d0e31f6ee86f45296f36343762522b6dd1d5"},"cell_type":"markdown","source":"# Data Exploration and Feature Creation"},{"metadata":{"_uuid":"d4605001c5bb2dfcc333d8e529e0b17e69695f1e"},"cell_type":"markdown","source":"The dataset also has a timestamp component to it.Let's visualize the price variable."},{"metadata":{"trusted":false,"_uuid":"48e614c953bd5751f7e22446357bb01c7370c778"},"cell_type":"code","source":"sns.distplot(traindf[\"price_doc\"],bins = 100)\nprint(traindf[\"price_doc\"].describe().apply(lambda x : format(x,'10.0f')))\nprint(\"\\n\\n\\nPrice range skewness\",stats.skew(traindf[\"price_doc\"]))\nplt.figure(figsize=(8,6))\nplt.scatter(range(traindf.shape[0]), np.sort(traindf.price_doc.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"90b1dad07f3c1bfaedec8cb99717533311b1ac66"},"cell_type":"code","source":"list(np.percentile(traindf[\"price_doc\"], np.arange(0, 100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0e31673c9a65e805deeefd8792da56d33223bda"},"cell_type":"markdown","source":"We see that the price range is postively skewed with a long tail, with 98% of the prices less than 20 mil. We remove these outliers later on "},{"metadata":{"_uuid":"c45017f6c9cf98beb189b58dbd8037614e243d4f"},"cell_type":"markdown","source":"## The timestamp variable might be important in viewing the whole problem. Let's look at it"},{"metadata":{"trusted":false,"_uuid":"dfce231864fb7aad81d9e9496a74a63f4d3a4bcf"},"cell_type":"code","source":"print(\"NAs in Price Doc column\",traindf[\"price_doc\"].isnull().sum(),\"\\n\")\nprint(\"Train Data\")\nprint(\"min date:\",traindf[\"timestamp\"].min())\nprint(\"max date:\",traindf[\"timestamp\"].max())\nprint(\"number of nulls\",traindf[\"timestamp\"].isna().value_counts())\n\nprint(\"\\nTest Data\")\nprint(\"min date:\",testdf[\"timestamp\"].min())\nprint(\"max date:\",testdf[\"timestamp\"].max())\nprint(\"number of nulls\",testdf[\"timestamp\"].isna().value_counts())\n\nprint(\"\\nTime variable type before conversion: \",traindf['timestamp'].dtype)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"618b7a4f2b9dcf006554c6ed598d8c4a44de084a"},"cell_type":"markdown","source":"The test and the train datasets are split by time series. We have to learn from the past and predict the future"},{"metadata":{"trusted":false,"_uuid":"9a0cec2584731e27c56c7dbe3c9732a9b6df8c0c"},"cell_type":"code","source":"traindf[\"timestamp\"]=pd.to_datetime(traindf['timestamp'])\ntestdf[\"timestamp\"]=pd.to_datetime(testdf['timestamp'])\nprint(\"Time variable type : \",traindf['timestamp'].dtype)\n\ntraindf[\"year\"] = traindf[\"timestamp\"].dt.year\ntraindf[\"month\"] = traindf[\"timestamp\"].dt.month\ntestdf[\"year\"] = testdf[\"timestamp\"].dt.year\ntestdf[\"month\"] = testdf[\"timestamp\"].dt.month","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcef4fbb32a538d4990c4f9abf0df4fd6dc29894"},"cell_type":"markdown","source":"Visualizing the timestamp variable across all dimensions like month, date and year to understand its distribution"},{"metadata":{"trusted":false,"_uuid":"b934934299329558683b772c0604a9c252f7d1f5"},"cell_type":"code","source":"mean = pd.DataFrame(traindf.groupby(traindf[\"year\"])[\"price_doc\"].agg('mean').apply(lambda x : format(x,'10.0f')))\nmean[\"price_doc\"]=mean[\"price_doc\"].astype(int)\nmean.reset_index(level=0, inplace=True)\nsns.barplot(x=\"year\",y=\"price_doc\",data=mean, color=\"grey\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7b058a0612823da586db8a5bb3c46010a4dc95c4"},"cell_type":"code","source":"mean = pd.DataFrame(traindf.groupby([traindf[\"month\"]])[\"price_doc\"].agg('mean').apply(lambda x : format(x,'10.0f')))\nmean[\"price_doc\"]=mean[\"price_doc\"].astype(int)\nmean[\"month\"]= mean.index\n#mean.reset_index(level=0, inplace=True)\nplt.figure(figsize=(30,10))\nsns.barplot(x=\"month\",y=\"price_doc\",data=mean,color=\"grey\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"12849473c77b1be351ea17a28feacfb9b3168572"},"cell_type":"code","source":"mean = pd.DataFrame(traindf.groupby([traindf[\"year\"],traindf[\"month\"]])[\"price_doc\"].agg('mean').apply(lambda x : format(x,'10.0f')))\nmean[\"price_doc\"]=mean[\"price_doc\"].astype(int)\nmean[\"year\"]= mean.index\n#mean.reset_index(level=0, inplace=True)\nplt.figure(figsize=(40,20))\nsns.barplot(x=\"year\",y=\"price_doc\",data=mean, color=\"grey\")\nplt.xlabel('index', fontsize=50)\nplt.ylabel('price', fontsize=50)\nplt.show()\n\n#plt.bar(range(mean.shape[0]),mean.price_doc.values)\n#plt.xlabel('index', fontsize=50)\n#plt.ylabel('price', fontsize=50)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"744a4b30f2e07e285ed5360e623f8deea26900c8"},"cell_type":"markdown","source":"It seems like the average price is steadily increasing every year from the first chart. Specific months don't make a difference I guess, but there is a\nsteady growth in price after the first 18 months..So a combination of year and month might be useful inputs"},{"metadata":{"trusted":false,"_uuid":"bc0cc02a9e225eb227533e3a0e6b7e117e911c2a"},"cell_type":"code","source":"print(\"Train Shape\",traindf.shape)\nprint(\"Test Shape\",testdf.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"774bdd4e18f016fe1e40ed18bc234549b8cc78e3"},"cell_type":"markdown","source":"Okay, the dataset has 292 variables. We might not be able to visualize all variables, but we need ways to eliminate variables that do not significantly influence price. We could start by looking at \n\n1) Variables with high level of null values\n\n2) Correlation and multicolleniarity to remove redundant variables that duplicate information using VIF\n\n3) Significance of variables in predicting the outcome ; regression table output - but this might not be useful when we implement decision tree models\n\n4) I read about Box Cox transformation to convert non normal data in normal form at https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/ . Since regression is something we will be using a lot here, let us see if it is needed.\n\nLet's see how many of the above we can implement"},{"metadata":{"_uuid":"65db0c9768c277e4da13d52791eb9670c51f376b"},"cell_type":"markdown","source":"## 1) Eliminate variables with high levels of null values"},{"metadata":{"trusted":false,"_uuid":"1949b34170f33f4c01066790e80654c4af1bca97"},"cell_type":"code","source":"nulltable = pd.DataFrame(traindf.isnull().sum()/traindf.shape[0]).reset_index()\nnulltable.columns = ['column_name', 'missing_count']\nnon_nullcolumns = nulltable[nulltable['missing_count']==0]\nnulltable.sort_values(by=\"missing_count\",ascending = 0).head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6232eb53a7b40dfbe3039284ef8b3a76f1e70df3"},"cell_type":"markdown","source":"Highest degree of missing values is the hospital_beds_raion column with 47% . Let me remove variables with more than 10% missing values and impute -1 into the others."},{"metadata":{"trusted":false,"_uuid":"70ece75ad7b8b4e67385ac4faa5821687135fb26"},"cell_type":"code","source":"null_columns_remove= list(nulltable[nulltable['missing_count']>.10][\"column_name\"])\nprint(traindf.shape)\nprint(testdf.shape)\ntraindf1 = traindf.drop(null_columns_remove,axis=1)\ntestdf1 = testdf.drop(null_columns_remove,axis=1)\nprint(traindf1.shape)\nprint(testdf1.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"633136a42e24c6645aa4cf18b6261f43d8f32811"},"cell_type":"markdown","source":"There are now 259 variables in the data after removing variables with >10% null values"},{"metadata":{"trusted":false,"_uuid":"f955dcfd46f2f9d8d672b581a37174aea7960fb6"},"cell_type":"code","source":"datatype = traindf1.dtypes.reset_index()\ndatatype.columns = ['column_name', 'datatype']\ndatatype[\"datatype\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc536859bb157da5acc1aeb9167937bc9c535715"},"cell_type":"code","source":"table = pd.merge(nulltable,datatype, how=\"inner\",on=\"column_name\")\ntable[\"Null\"] = np.where(table[\"missing_count\"]==0,\"No\",\"Yes\")\ntable[\"type\"] = np.where(table[\"datatype\"]==\"object\",\"Category\",\"Number\")\nprint(\"There are \",non_nullcolumns.shape[0],\"columns with no null values and \",\n     traindf1.shape[1]-non_nullcolumns.shape[0], \"columns with null values that have been filled with -1 \\n\")\nprint(pd.crosstab(table.type,table[\"Null\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5437a88f935d237306d096252c3eb18d46b0eba7"},"cell_type":"markdown","source":"### Imputing null values with -1"},{"metadata":{"trusted":false,"_uuid":"57e34042cd0d823707f9213f266f2832d5f7306e"},"cell_type":"code","source":"traindf1.fillna(-1,inplace=True)\ntestdf1.fillna(-1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5606ef40856637e50884dd03b218f394ace35667"},"cell_type":"markdown","source":"## Encoding Categorical Variables"},{"metadata":{"_uuid":"a2c57574d96333a1cdffdff8357886091a321374"},"cell_type":"markdown","source":"Below I will be looking at all the categorical variables to see how big the dataset will become once I encode it."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"6a85cc2985045ebc492c5bb56b7d68f412919b9f"},"cell_type":"code","source":"for i in table[table[\"datatype\"]==\"object\"][\"column_name\"] :\n    print(\"\\n\",i)\n    print(traindf1[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a27951765f65f9bd985fd09bd20aa164d1e8bb19"},"cell_type":"markdown","source":"We will be dropping sub_area for now. While it might be useful, it has too many categories and might require a lot of time to weed through once encoded. Other columns have relatively lower categories and will be easy to look at."},{"metadata":{"trusted":false,"_uuid":"a0adc54e3ef7f2daa886aa6ec9550c8713f0f87a"},"cell_type":"code","source":"traindf2 = traindf1.drop(\"sub_area\",axis=1)\ntestdf2 = testdf1.drop(\"sub_area\",axis=1)\nprint(\"After removing sub_area, shape reduced from \", traindf1.shape,\"to\",traindf2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d47c6357bfb6b68e9ec1ebaa7fcdd10f3b7e137"},"cell_type":"code","source":"traindf2 = pd.get_dummies(traindf2)\ntestdf2 = pd.get_dummies(testdf2)\nprint(traindf2.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e50ec073f6470226e64e7edf7cde182a65033c8c"},"cell_type":"markdown","source":"So all categorical columns have now been converted into numerical columns and the data is more or less fit for any algorithm to parse"},{"metadata":{"_uuid":"2e9b6377a7b28029043036c7657d8e7271406692"},"cell_type":"markdown","source":"## 2)Checking multicolleniarity to remove redundant variables and reduce variance in prediction using VIF"},{"metadata":{"_uuid":"ca418c69ce834fdf2fd971dd107ca07e3a9c4222"},"cell_type":"markdown","source":"How does multicollinearity affect the coeffcients in a regression problem and the final predictions?Why should you solve for multicollinearity?\n\nSo in a multiple linear regression problem, we equate y = Ax1+bx2 +c . \n\ny- dependant\n\nx1 and x2 being the independant variables that are correlated\n\nc being the intercept or constant.\n\nWhen you say y = ax1 + bx2 , you mean that for a unit increase in x1, y increase by a, all else being constant. And this should be a true relationship for y and x1 irrespective of what other variables we use. But if x2 is correlated with x1, the constant a seems to be decided by the order of variables fed into the regressor and also by the number of predictors. Slight multicollinearity is tolerable as the coefficients are not strongly influnced, but high multicollinearity makes the coeffs erratic and unreliable. So the prediction might fluctuate based on the number and order of variables, which shouldn't be the case.\n\nHence, removing colinear variable is a need for a stable regression model.\n VIF seems to be the goto way of detecting multicollinearity. \n VIF is calculated by 1/(1-R^2), R^2 coming out of every variable being predicted by the others using a regression model. So, R2 tells you how much of the variation in a variable is explained by the other variables, and the higher the R2, bigger the relationhip between independant variables, and ergo higher is the value of 1/(1-R2). Typical cutoff seems to be 5 for removing variables \n on account of multicollinearity, 1-(1-.8) would give you 5, which means if a 80% of variation in a variable can be explained by other variables, it needs to be removed and not fed into a model"},{"metadata":{"trusted":false,"_uuid":"bbb1342ecb4c2e74760722de40164f4efb4c950b"},"cell_type":"code","source":"X = traindf2.drop(['id','timestamp','price_doc'], axis =1)\nX = add_constant(X)\nviftab = pd.Series([variance_inflation_factor(X.values, i) \n               for i in range(X.shape[1])], \n              index=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d48829c5b1b9962038cc47a3d7b13d34079f8345"},"cell_type":"code","source":"vifdf=pd.DataFrame(viftab).reset_index()\nvifdf.columns = [\"Name\",\"VIF\"]\nvifdf.sort_values(\"VIF\",ascending=False)\n#vifdf[\"VIF\"] = vifdf[\"VIF\"]\n#vifdf[\"VIF\"] = vifdf[\"VIF\"].apply(lambda x : x.strip())\n#vifdf1=vifdf[vifdf[\"VIF\"] != 'inf']\n#vifdf1[\"VIF\"] =pd.to_numeric(vifdf1[\"VIF\"])\n#vifdf1.sort_values(\"VIF\",ascending=False)\n\nnoncollinearvar=list(vifdf[vifdf[\"VIF\"]<10][\"Name\"])\nnoncollinearvar.remove('const')\nprint(noncollinearvar)\nvifdf.dtypes\n\nnoncollinearvar = ['full_sq', 'floor', 'green_zone_part', 'indust_part', 'school_education_centers_top_20_raion', 'healthcare_centers_raion', 'university_top_20_raion', 'ID_metro', 'green_zone_km', 'industrial_km', 'cemetery_km', 'ID_railroad_station_walk', 'ID_railroad_station_avto', 'water_km', 'big_road1_km', 'ID_big_road1', 'ID_big_road2', 'ID_bus_terminal', 'church_synagogue_km', 'catering_km', 'green_part_500', 'prom_part_500', 'office_sqm_500', 'trc_count_500', 'trc_sqm_500', 'mosque_count_500', 'leisure_count_500', 'sport_count_500', 'market_count_500', 'trc_sqm_1000', 'mosque_count_1000', 'sport_count_1000', 'market_count_1000', 'trc_sqm_1500', 'mosque_count_1500', 'market_count_1500', 'trc_sqm_2000', 'mosque_count_2000', 'market_count_2000', 'mosque_count_3000', 'mosque_count_5000', 'year', 'month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a1070949d7076708882286c6ca56079dc5c9dded"},"cell_type":"code","source":"noncollinearvar = ['full_sq', 'floor', 'green_zone_part', 'indust_part', 'school_education_centers_top_20_raion', 'healthcare_centers_raion', 'university_top_20_raion', 'ID_metro', 'green_zone_km', 'industrial_km', 'cemetery_km', 'ID_railroad_station_walk', 'ID_railroad_station_avto', 'water_km', 'big_road1_km', 'ID_big_road1', 'ID_big_road2', 'ID_bus_terminal', 'church_synagogue_km', 'catering_km', 'green_part_500', 'prom_part_500', 'office_sqm_500', 'trc_count_500', 'trc_sqm_500', 'mosque_count_500', 'leisure_count_500', 'sport_count_500', 'market_count_500', 'trc_sqm_1000', 'mosque_count_1000', 'sport_count_1000', 'market_count_1000', 'trc_sqm_1500', 'mosque_count_1500', 'market_count_1500', 'trc_sqm_2000', 'mosque_count_2000', 'market_count_2000', 'mosque_count_3000', 'mosque_count_5000', 'year', 'month']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ff882e6f7e764e3f8ef4edd6e6694a657f5890"},"cell_type":"markdown","source":"Note: There are some infinity values in the dataset.Need to be careful as sometimes the column is a string and other times it is float"},{"metadata":{"_uuid":"0268918dd91ef9faab8b5e78ed2d4d1143e7fbe5"},"cell_type":"markdown","source":"In the above code, I am essentially trying to remove variables with VIF over 10.\n\nTHe following are the variables with VIF value less than 6.\n['full_sq', 'floor', 'green_zone_part', 'indust_part', 'school_education_centers_top_20_raion', 'healthcare_centers_raion', 'university_top_20_raion', 'ID_metro', 'green_zone_km', 'industrial_km', 'cemetery_km', 'ID_railroad_station_walk', 'ID_railroad_station_avto', 'water_km', 'big_road1_km', 'ID_big_road1', 'ID_big_road2', 'ID_bus_terminal', 'church_synagogue_km', 'catering_km', 'green_part_500', 'prom_part_500', 'office_sqm_500', 'trc_count_500', 'trc_sqm_500', 'mosque_count_500', 'leisure_count_500', 'sport_count_500', 'market_count_500', 'trc_sqm_1000', 'mosque_count_1000', 'sport_count_1000', 'market_count_1000', 'trc_sqm_1500', 'mosque_count_1500', 'market_count_1500', 'trc_sqm_2000', 'mosque_count_2000', 'market_count_2000', 'mosque_count_3000', 'mosque_count_5000', 'year', 'month']"},{"metadata":{"_uuid":"81416c00935e4b96543349b666b88ba8880f525a"},"cell_type":"markdown","source":"Is correlation/mulitcollinearity only important for regression problems?\n\nhttps://datascience.stackexchange.com/questions/31402/multicollinearity-in-decision-tree\n\nFound answers in the above link. Since decision trees anyway split based on one variable at a time and then looks at importance of the next based on gini or entropy, it is said that the correlated variables will anyway be ignored for non correlated ones, because if a correlated variable contains similar information, a better split is not achieved and there will be less info gain from that variable."},{"metadata":{"_uuid":"931d861b039570ed050f1f5a671cf649fc17c553"},"cell_type":"markdown","source":" "},{"metadata":{"_uuid":"dced861c943b9b1bc09359ef0dd755c63c30e47d"},"cell_type":"markdown","source":"# Feature importance check\n\nIn the below code, I am checking feature importance just to understand how a decision tree views these variables "},{"metadata":{"trusted":false,"_uuid":"635a83228c330933c1149737153175c603f0da96"},"cell_type":"code","source":"X = traindf2[noncollinearvar]\nY = traindf2['price_doc']\nparam_grid = [{\"max_depth\":[5,8,10,12,15], \"max_features\":[\"sqrt\",\"log2\",\"auto\"]}]\ngrid = GridSearchCV(GradientBoostingRegressor(),param_grid, cv=3, n_jobs =-1)\ngrid.fit(X,Y)\ngb = grid.best_estimator_\ngb.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e26079a1596bf876fb596dcdd373433519b77a22"},"cell_type":"code","source":"plt.figure(figsize=(10,8))\npd.Series(gb.feature_importances_, index=X.columns).nlargest(30).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"280a74a800a94b01e037df0dbd3d183995745a71"},"cell_type":"markdown","source":"#### The above table gives the order of important features based on the Decision Tree. It seems sensible. For instance, any person who starts looking for a house would first ask how big it is for the price. Followed by locality and amenities which is explained by variables like metro or not, school facility, water availability,religious building availability etc..."},{"metadata":{"trusted":false,"_uuid":"5edd8f2e6f61411434810b67a64a744b7ffa7c51"},"cell_type":"code","source":"importantfeatures = pd.Series(gb.feature_importances_, index=X.columns).nlargest(20).reset_index()\nimportantfeatures.columns = [\"columnname\",\"importance\"]\n#columnlist= importantfeatures[\"columnname\"]\ncolumnlist = list(importantfeatures[\"columnname\"])\ncolumnlist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c6ad002a38bd1d7442954222d47bfc6e5c2abbf"},"cell_type":"markdown","source":"Note to self : There was a strong overlap between the variables remaining after checking for multicollinearity and the variables predicted as most important by the gradient boosted model on the original dataset. This probably means that the Decison Tree behind gradientboosting prioritizes variables that provide new information for a better split over variables that have redundant information( and hence are collinear)"},{"metadata":{"_uuid":"a3ad0e64bec0351d3522c5eb8d2f46d6d3cb5725"},"cell_type":"markdown","source":"# Checking Correlation"},{"metadata":{"trusted":false,"_uuid":"24301f78781f532132ce2f22697e2dae818c1bc3"},"cell_type":"code","source":"for i in noncollinearvar:\n    print(stats.pearsonr(traindf2[i], traindf2[\"price_doc\"]))\n  #  print(stats.spearmanr(traindf1[\"full_sq\"], traindf1[\"price_doc\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83d01376c1373577d0493b2b950dd76c32dacf85"},"cell_type":"markdown","source":"While correlation of most columns in weak, they all seem significant as e-06 is the lowest value"},{"metadata":{"_uuid":"557fbdf6c01a6fd8e1bd89077a302dfd33f5613a"},"cell_type":"markdown","source":"# Let's visualize the relationship between all these vaiables and price"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"57c66ca1cfdd849cf9139e4fa379b081485a4ee7"},"cell_type":"code","source":"plotdf=traindf2[(traindf2[\"full_sq\"]<175) & (traindf2[\"price_doc\"]<20000000)]\nfor i in noncollinearvar:\n    sns.lmplot(i,\"price_doc\",data=plotdf,fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19eb316f4a887b3af9bd438998e24776fe34bc6c"},"cell_type":"markdown","source":"# Outlier removal on specific variables based on the above charts and prior analysis "},{"metadata":{"_uuid":"0add786d370d22910d259e5cc05f3b6f8efd2a8b"},"cell_type":"markdown","source":"### 1) Removing outliers in the price_doc and full_sq variables. 2% of the data has price above 20 mil  and bring in a big variance to the data. We do not want this 2% to influnce the model scores and therefore the final results. Hence, this data is removed. "},{"metadata":{"_uuid":"d4e49f79d50974235a283204acb15c95f4936ef0"},"cell_type":"markdown","source":"### 2)We are also removing full_sq >175 as this seems like the most important variable and outliers in this variable again might affect final predictions."},{"metadata":{"trusted":false,"_uuid":"16ebc45b5fba5cc531dcf339a843e7eae7045b6e"},"cell_type":"code","source":"traindf3 = traindf2[(traindf2[\"full_sq\"]<175) & (traindf2[\"price_doc\"]<20000000)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f73aac0340d9f9e6c836f3b7178d17b2b4a561ee"},"cell_type":"markdown","source":"Extracting only the variables with low VIF scores as features"},{"metadata":{"trusted":false,"_uuid":"14f2a48b980fe3096a4b98eabfd62412d6b3f5e3"},"cell_type":"code","source":"featuredata=traindf3[noncollinearvar]\ntestdf3=testdf2[noncollinearvar]\nY=traindf3[\"price_doc\"]\n#featuredata.drop(\"water_km\",axis=1,inplace=True)\nfeaturedata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07128bdbee2524797b074ad3b28be605160fa928"},"cell_type":"markdown","source":"# Data Sampling. Test Train split creation from the train dataset given"},{"metadata":{"trusted":false,"_uuid":"27c779a410ae136a184d1f20452a20659e45a4a0"},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(featuredata,Y, test_size=0.3,random_state=seed)\nprint (xtrain.shape, ytrain.shape)\nprint (xtest.shape, ytest.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77bccc882ca8d6bed7dad60b8c6774260cf3d650"},"cell_type":"markdown","source":"# Model Evaluation Metrics:\nDefining metrics that the model will be evaluated on:\n\nWe are evaluating the model on three metrics:\n\nRoot mean Square Log error:\n\nIn case of price predictions, the RMSLE is used when you do not want to penalize the model for high differences. As in when the actual and the prediction values are very large, relative small differences might be amplified when taking absolute mean square. Hence Log of the values are used to not penalize large differences.\n\n\n\nRoot mean Square error:\n\nWe are also looking at the residual errors to compare models.\n\n\n\nR2 score:\n\nR2 can be calculated in two ways. SSR/SST or 1 - SSE/SST.Both \nthese options should yield that same value, provided the regression line \npredicts better than mean. But in this case, the prediction is so bad that \nSSE is higher than SST, meaning the regression line residuals are greater than\ndifference between actual points and their mean.\n\nI am assuming the r2score function uses 1- SSE/SST, and therefore the R2 is -ve."},{"metadata":{"trusted":false,"_uuid":"af70fcf4dccacdbc5715847a4a14ee892ed7e390"},"cell_type":"code","source":"def RMSLE(y, y0):\n    return np.sqrt(np.mean(np.square(np.log1p(y) - np.log1p(y0))))\n    \ndef RMSE(y, y0):\n    return np.sqrt(np.mean(np.square(y- y0)))\n    \ndef r2(y,y0):\n    return r2_score(y, y0)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae4394f09bfd48a943f7d9ea3b007156496b5054"},"cell_type":"markdown","source":"## A basline OLS model "},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"abf2d0a2f863c30cf99eec5fd9d4208309a0f53b"},"cell_type":"code","source":"model = sm.OLS(ytrain,xtrain)\nresults = model.fit()\nprint(results.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d40fb2f00f8a1d61e8c2a423c2c37110a8d5a9d3"},"cell_type":"markdown","source":"# Inference:\n    \n    Since the variables are on different scales, the coefficients are not directly comparable to each other. But , looking at the most important variables in the model from the feature importance set, we infer  that :\n    \n    For 1 unit increase in full sq,the price increases by 541 units( 9.92 * e^4), all else being constant\n    For 1 unit increase in floor,the price increase by 247 units( 4.50 * e^4), all else being constant\n    For 1 unit increase in full sq,the price increase  by 175 units( 1.176e+05), all else being constant\n    For 1 unit increase in big road,the price decreases by 235 units( -1.584e+05), all else being constant.\n   \nIt is interesting to note that bigger the road, lower in the price of the house, which would not be your initial assumption. \n\nNow the reason for removing highly correlated variables and checking for multicollinearity is that, now these coefficients are reliable and will not vary much based on other variables being added, as there is no overlap of information in those variables. This helps with a clean inference"},{"metadata":{"trusted":false,"_uuid":"627c992db6cfa81660e4de9c4e2add9c96431be2"},"cell_type":"code","source":"print(stats.pearsonr(traindf3[\"big_road1_km\"], traindf3[\"price_doc\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e7412730804c7405821619c73d8a9abbf4c002f"},"cell_type":"markdown","source":"Whilst not strong, there is indeed a negative correlation between the two variables"},{"metadata":{"trusted":false,"_uuid":"20a90ed27c52e470c38be2784ee8632236dc04a6"},"cell_type":"code","source":"predictions = results.predict(xtest)\npredictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a4a165500c2300dbd3f5442275f8a8099ac22147"},"cell_type":"code","source":"LR_RMSLE = RMSLE(ytest,predictions)\nLR_RMSE = RMSE(ytest,predictions)\nLR_R2 = r2(ytest,predictions)\n\nprint(\"RMSLE :\",LR_RMSLE,\"\\nRMSE:\",LR_RMSE,\"\\nR2:\",LR_R2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"400170d0ea5e3d3fc5f09df9e0c35b3b9bde2767"},"cell_type":"markdown","source":"Trying out ridge regression and Lasso regression here to reduce effect of coefficients of any one predictor to better generalize the model and not have it leaning on a single predictor"},{"metadata":{"_uuid":"f86b57aa448c66927583043bb4ecd272153e2956"},"cell_type":"markdown","source":"# Ridge Regression"},{"metadata":{"trusted":false,"_uuid":"8273c138f871947099a8f2a94198f27a950d2ddf"},"cell_type":"code","source":"ridgereg = Ridge(alpha=.1,normalize=True, max_iter=200)\nridgereg.fit(xtrain,ytrain)\nridgereg_pred = ridgereg.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a54a0136046ca9afe002e6d5288602295d43aacc"},"cell_type":"code","source":"RR_RMSLE = RMSLE(ytest,ridgereg_pred)\nRR_RMSE = RMSE(ytest,ridgereg_pred)\nRR_R2 = r2(ytest,ridgereg_pred)\n\nprint(\"RMSLE :\",RR_RMSLE,\"\\nRMSE:\",RR_RMSE,\"\\nR2:\",RR_R2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94e644db1bc3e115dbcf3bbbd84998aa0eddebad"},"cell_type":"markdown","source":"# Lasso Regression"},{"metadata":{"trusted":false,"_uuid":"889ea92d0e620baadc1b1df6dedc1847b2e49f24"},"cell_type":"code","source":"lassoreg = Lasso(alpha=.1,normalize=True, max_iter=200)\nlassoreg.fit(xtrain,ytrain)\nlassoreg_pred = lassoreg.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"00e2796fffdbd1c7ba51153b6136ea70fa5f41a8"},"cell_type":"code","source":"LAR_RMSLE = RMSLE(ytest,lassoreg_pred)\nLAR_RMSE = RMSE(ytest,lassoreg_pred)\nLAR_R2 = r2(ytest,lassoreg_pred)\n\nprint(\"RMSLE :\",LAR_RMSLE,\"\\nRMSE:\",LAR_RMSE,\"\\nR2:\",LAR_R2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07b9dcd1e1fa5a67cc7cf4aa6ccc920d5eadf0f"},"cell_type":"markdown","source":"Most variables don't seem to have a linear relationship with price_doc. We\neither have to transform them to check for linearity or use a non paramteric method. Let's try decision trees as they do not make assumptions about the distribution of the data."},{"metadata":{"_uuid":"94d95a0b8a73d7212c5ab70d27bb06cb12d1d700"},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":false,"_uuid":"b6f544ca11989757c7d60016cf7997c636bf65da"},"cell_type":"code","source":"param_grid = [{\"max_depth\":[5,8,10], \"max_features\":[\"sqrt\",\"log2\",\"auto\"]}]\ngrid = GridSearchCV(RandomForestRegressor(),param_grid, cv=3, n_jobs =-1)\ngrid.fit(xtrain,ytrain)\nrf = grid.best_estimator_\nrf.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"829f2cc9f5578298dd30a8626acfdfbc7388e969"},"cell_type":"code","source":"print(\"The Best hyperparamters of the model are\",grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ae04ee67bee7f1b7623ea65ede90b9d6d17a28b"},"cell_type":"code","source":"rf_pred = rf.predict(xtest)\nrf_RMSLE = RMSLE(ytest,rf_pred)\nrf_RMSE = RMSE(ytest,rf_pred)\nrf_R2 = r2(ytest,rf_pred)\n\nprint(\"RMSLE :\",rf_RMSLE,\"\\nRMSE:\",rf_RMSE,\"\\nR2:\",rf_R2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b19aa9a371f0ac129b07dd12e5291edbd3a5b6a"},"cell_type":"markdown","source":"### Feature Importance chart for Random Forest"},{"metadata":{"trusted":false,"_uuid":"40ed1eeaa181d5ca5033677186e0575d28e8f69f"},"cell_type":"code","source":"plt.figure(figsize=(10,8))\npd.Series(rf.feature_importances_, index=xtrain.columns).nlargest(30).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c5b99d5fadb5486fe11c035a8504d0d54817f40"},"cell_type":"markdown","source":"# Gradient Boosting"},{"metadata":{"_uuid":"0bd18e0f649af29dae21e170de74058cfbfa7113"},"cell_type":"markdown","source":"Using Grid Search to tune hyperparameters of the model"},{"metadata":{"trusted":false,"_uuid":"4d6a7785566606782976b9df1999e802c7d3126c"},"cell_type":"code","source":"param_grid = [{\"max_depth\":[5,8,10], \"max_features\":[\"sqrt\",\"log2\",\"auto\"],\"n_estimators\":[100,200,300]}]\ngrid = GridSearchCV(GradientBoostingRegressor(),param_grid, cv=3, n_jobs =-1)\ngrid.fit(xtrain,ytrain)\ngb = grid.best_estimator_\ngb.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d3b87f74ebf38ad60e56da5da36c52c12883b92a"},"cell_type":"code","source":"print(\"The Best hyperparamters of the model are\",grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"32e6279a0b6a67028cf8b385e722e742f632872a"},"cell_type":"code","source":"gb_pred = gb.predict(xtest)\ngb_RMSLE = RMSLE(ytest,gb_pred)\ngb_RMSE = RMSE(ytest,gb_pred)\ngb_R2 = r2(ytest,gb_pred)\n\nprint(\"RMSLE :\",gb_RMSLE,\"\\nRMSE:\",gb_RMSE,\"\\nR2:\",gb_R2)\n#testgb_pred = gb.predict(testdf3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c20899e14efe33c9963e537f090f994ab43726"},"cell_type":"markdown","source":"### Feature importance chart for GB"},{"metadata":{"trusted":false,"_uuid":"185ade4162a8bcf48261732bd1a1470cf1715a6a"},"cell_type":"code","source":"plt.figure(figsize=(10,8))\npd.Series(gb.feature_importances_, index=xtrain.columns).nlargest(30).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e674f59526f099b45e3bcaaee0d470388815369f"},"cell_type":"markdown","source":"# Model Comparison based on the evaluation metrics"},{"metadata":{"trusted":false,"_uuid":"5bb4c52dae6848600d65fa9d41add351e55c5f59"},"cell_type":"code","source":"model_compare=[{\"Name\":'Linear Reg',\"RMSLE\":LR_RMSLE,\"RMSE\":LR_RMSE,\"R2\":LR_R2},\n               {\"Name\":'Ridge Reg',\"RMSLE\":RR_RMSLE,\"RMSE\":RR_RMSE,\"R2\":RR_R2},\n               {\"Name\":'Lasso Reg',\"RMSLE\":LAR_RMSLE,\"RMSE\":LAR_RMSE,\"R2\":LAR_R2},\n               {\"Name\":'Random Forest',\"RMSLE\":rf_RMSLE,\"RMSE\":rf_RMSE,\"R2\":rf_R2},\n               {\"Name\":'Gradient Boosting',\"RMSLE\":gb_RMSLE,\"RMSE\":gb_RMSE,\"R2\":gb_R2}]\n\nmodel_comparedf = pd.DataFrame(model_compare)\nmodel_comparedf =model_comparedf.set_index('Name')\nprint(model_comparedf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b98f54568141a4b55a51c82ce0c7b661a3eb25c0"},"cell_type":"markdown","source":"## R2 Score Comparison"},{"metadata":{"trusted":false,"_uuid":"dae88ac4a79a64117562dae8c06f14244eacebdd"},"cell_type":"code","source":"model_comparedf[[\"R2\"]].plot(figsize=(7,3), xticks=range(0, 5)).legend(title='Name', bbox_to_anchor=(1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa4274dea7d695a2bc6edccdb2ef73fc024e28b8"},"cell_type":"markdown","source":"## RMSLE Comparison"},{"metadata":{"trusted":false,"_uuid":"833c93d05d178140e37eb67aeb4b00e9b477cbce"},"cell_type":"code","source":"model_comparedf[[\"RMSLE\"]].plot(figsize=(7,3), xticks=range(0, 5)).legend(title='Name', bbox_to_anchor=(1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"353088f3d5884af599c7cbb00e2f13368c9b4079"},"cell_type":"markdown","source":"## RMSE Comparison"},{"metadata":{"trusted":false,"_uuid":"7cbc74cfbc8490dd4eae8a20ee011d8011a09102"},"cell_type":"code","source":"model_comparedf[[\"RMSE\"]].plot(figsize=(7,3), xticks=range(0, 5)).legend(title='Name', bbox_to_anchor=(1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eadfcb3a6dd02ba450adc9347e5263138f1802b6"},"cell_type":"markdown","source":"### Clearly, the Gradient Boosting Model has the highest R2 score and the lowest RMSE and RMSLE scores of all the models and outperforms all other models. Hence, the final prediction would be the ones coming out of the Gradient Boosted Model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}