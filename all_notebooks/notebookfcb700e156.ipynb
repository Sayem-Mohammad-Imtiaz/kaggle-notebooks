{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1- Importing the libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score , classification_report\nimport seaborn as sns\nclasses=['healthy','Un-healthy']\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attriute Information\n1- flare Flare ID \n2- start.date Date (dd-mm-yy) when begin the flare \n3- start.time Hour (24hrs) (hh-mm-ss) when begin the flare 4- peak The time where the flare in peak 5- end End of current flare \n6- duration.s Duration / sec \n7- peak.c/s Count of peaks \n8- total.counts Total count of flares \n9- energy.kev The energy produced by the flare KiloElectronVolt \n10- x.pos.asec The x position by arc second","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/solar-flares-rhessi/hessi.solar.flare.2002to2016.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis (EDA)\nThe goal exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations\n\nWhat questions are you trying to solve?\nWhat kind of data do we have and how do we deal with the different types?\nWhat is missing data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change, or remove features to get more out of your data?","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(\"flare\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['flag.2']=df['flag.2'].map({'p1':1,'Gs':0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.radial.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our problem is balanced, with radiation emitted from solar energy at different times where different values form from this graph, we can see that there are different frequencies and values of radiation.\n\n\n# Visualize the relationship between our features","metadata":{}},{"cell_type":"code","source":"# plot out just the deffrent 5 variables (features)\nsns.pairplot(df,hue ='peak', vars = ['flag.5','flag.2','flag.1','flag.3','flag.4'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for messing values\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 Correlation Matrix¶\n","metadata":{}},{"cell_type":"code","source":"# this will describe the all statistical function of our data\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Correlation Matrix¶\n","metadata":{}},{"cell_type":"code","source":"features_mean= list(df.columns[1:17])\nprint(features_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let a graph be drawn so that multi-colinearity can be removed It means the columns are\n# dependent on each other so that we can avoid it, because it allows us to verify the connection between\n# the features using the same column twice\n# Now we're just going to do this analyze for functions, then we are doing it for others.\ncorr = df[features_mean].corr() \nplt.figure(figsize=(11,17))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size':15},\n           xticklabels= features_mean, yticklabels= features_mean,\n           cmap= 'coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 Applying machine learning algorithms","metadata":{}},{"cell_type":"code","source":"prediction_var = ['flag.3','flag.2','flag.1','duration.s','radial']\n# now these are the variables which will use for prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now split our data into train and test\ntrain, test = train_test_split(df, test_size = 0.3)\n# we can check their dimension \nprint(train.shape)\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = train[prediction_var]\ntrain_y=train.radial\ntest_X= test[prediction_var]\ntest_y =test.radial","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=0.4)\nmodel.fit(train_X,train_y) #new fit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_X,train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Data Predict\n# prediction contains the forecast value of the column of dignosis for test inputs according to our model.\nprediction=model.predict(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score , classification_report\nimport seaborn as sns\nclasses=['healthy','Un-healthy']\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_train, pred) * 100:.2f}%\")\n       # recall=recall_score(y_train, pred) \n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_train, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        sns.heatmap(confusion_matrix(y_test, pred), annot= True, cmap='YlGnBu',fmt = 'g')\n        print(classification_report(y_test,pred))\n        cm=(confusion_matrix(y_test,pred))\n       # ax.xaxis.set_label_position('top')\n        plt.tight_layout()\n        plt.title('Confusion matrix for Decision Tree Model', y = 1.1)\n        plt.ylabel('Actual label')\n        plt.xlabel('Predicted label')\n        plt.show()\n        total = sum(sum(cm))\n        acc = (cm[0, 0] + cm[1, 1]) / total\n        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n       # print(cm)\n\n        FP = cm.sum(axis=0) - np.diag(cm)  \n        FN = cm.sum(axis=1) - np.diag(cm)\n        TP = np.diag(cm)\n        TN = cm.sum() - (FP + FN + TP)\n        FP = FP.astype(float)\n        FN = FN.astype(float)\n        TP = TP.astype(float)\n        TN = TN.astype(float)\n\n        # Sensitivity, hit rate, recall, or true positive rate\n        TPR = TP/(TP+FN)\n        print('Sensitivity (TPR) : ',TPR)\n        # Specificity or true negative rate\n        TNR = TN/(TN+FP) \n        print('Specificity (TNR) : ',TNR)\n        # Overall accuracy\n        print(\" Overall accuracy\")\n        ACC = (TP+TN)/(TP+FP+FN+TN)\n        print('Accuracy : ',ACC)\n        print(\"Accuracy: {:.4f}\".format(acc))\n        print(\"Average Sensitivity: {:.4f}\".format(sensitivity))\n        print(\"Average Specificity: {:.4f}\".format(specificity))\n        print('\\n')\n        \n        conf_matrix=cm\n        print(\"=========================================\")\n        # save confusion matrix and slice into four pieces\n        TP = conf_matrix[1][1]\n        TN = conf_matrix[0][0]\n        FP = conf_matrix[0][1]\n        FN = conf_matrix[1][0]\n        print('True Positives:', TP)\n        print('True Negatives:', TN)\n        print('False Positives:', FP)\n        print('False Negatives:', FN)\n\n        # calculate accuracy\n        conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n\n        # calculate mis-classification\n        conf_misclassification = 1- conf_accuracy\n\n        # calculate the sensitivity\n        conf_sensitivity = (TP / float(TP + FN))\n        # calculate the specificity\n        conf_specificity = (TN / float(TN + FP))\n\n        # calculate precision\n        conf_precision =(TN / float(TN + FP))\n        # calculate f_1 score\n        conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n        print('-'*50)\n        print(f'Accuracy: {round(conf_accuracy,2)}') \n        print(f'Mis-Classification: {round(conf_misclassification,2)}') \n        print(f'Sensitivity: {round(conf_sensitivity,2)}') \n        print(f'Specificity: {round(conf_specificity,2)}') \n        print(f'Precision: {round(conf_precision,2)}')\n        print(f'f_1 Score: {round(conf_f1,2)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to plot ROC and Precision Recall Curve for combination of all models","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\ndef plotting(true,pred):\n    fig,ax=plt.subplots(1,2,figsize=(15,5))\n    precision,recall,threshold = precision_recall_curve(true,pred[:,1])\n    ax[0].plot(recall,precision,'g--')\n    ax[0].set_xlabel('Recall')\n    ax[0].set_ylabel('Precision')\n    ax[0].set_title(\"Average Precision Score : {}\".format(average_precision_score(true,pred[:,1])))\n    fpr,tpr,threshold = roc_curve(true,pred[:,1])\n    ax[1].plot(fpr,tpr)\n    ax[1].set_title(\"AUC Score is: {}\".format(auc(fpr,tpr)))\n    ax[1].plot([0,1],[0,1],'k--')\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop('flag.2',axis=1)\ny = df.peak\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is time to construct a machine learning model, and now we have our data divided into training and test sets.\nWe'll train it (find the patterns) on the training set.\n\nAnd we'll test it (use the patterns) on the test set.\n\nWe're going to try 3 different machine learning models:\n\n* Logistic Regression\n* K-Nearest Neighbours Classifier\n* Support Vector machine\n* Decision Tree Classifier\n* Random Forest Classifier\n* XGBoost Classifier\n* ROC","metadata":{}},{"cell_type":"markdown","source":"# ****ROC","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\ndef plotting(true,pred):\n    fig,ax=plt.subplots(1,2,figsize=(15,5))\n    precision,recall,threshold = precision_recall_curve(true,pred[:,1])\n    ax[0].plot(recall,precision,'g--')\n    ax[0].set_xlabel('Recall')\n    ax[0].set_ylabel('Precision')\n    ax[0].set_title(\"Average Precision Score : {}\".format(average_precision_score(true,pred[:,1])))\n    fpr,tpr,threshold = roc_curve(true,pred[:,1])\n    ax[1].plot(fpr,tpr)\n    ax[1].set_title(\"AUC Score is: {}\".format(auc(fpr,tpr)))\n    ax[1].plot([0,1],[0,1],'k--')\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n\n\n# **KNN**","metadata":{}},{"cell_type":"code","source":"knn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train, y_train)\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DT Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DT Classifier","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier(random_state=42)\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM\n","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n\nsvm_model = SVC(kernel='rbf', gamma=0.1, C=1.0, probability=True)\nsvm_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_score(svm_model, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_model, X_train, y_train, X_test, y_test, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrand_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\nrand_forest.fit(X_train, y_train)\n\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrand_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\nrand_forest.fit(X_train, y_train)\n\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Classifer","metadata":{}},{"cell_type":"code","source":"rom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\nprint_score(xgb, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb, X_train, y_train, X_test, y_test, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# naive_bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\nprint_score(nb, X_train, y_train, X_test, y_test, train=True)\nprint_score(nb, X_train, y_train, X_test, y_test, train=False) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['r', 'g', 'b', 'y', 'k', 'c', 'm', 'brown', 'r']\nlw = 1\nCs = [1e-6, 1e-4, 1e0]\n\nplt.figure(figsize=(12,6))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for different classifiers')\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\nlabels = []\nfor idx, C in enumerate(Cs):\n    clf = LogisticRegression(C = C)\n    clf.fit(X_train, y_train)\n    print(\"C: {}, parameters {} and intercept {}\".format(C, clf.coef_, clf.intercept_))\n    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, color=colors[idx])\n    labels.append(\"C: {}, AUC = {}\".format(C, np.round(roc_auc, 4)))\n\nplt.legend(['random AUC = 0.5'] + labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}