{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import SGDClassifier as SGDC\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.naive_bayes import GaussianNB as GNB\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/wisconsin-breast-cancer-cytology-features/wisconsin_breast_cancer.csv\")\ndata.head()\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"removing the null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the features and class"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_features = data.iloc[:,1:10]\ndata_class = data.iloc[:,10:11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"generating the models for each classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD_model = SGDC(random_state=1502775)\nRFC_model = RFC(n_estimators=30,random_state=1502775)\nGNB_model = GNB()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"getting the prediction scores for each model on the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD_scores = cross_val_predict(SGD_model,data_features,data_class.values.ravel(),cv=10)\nRFC_scores = cross_val_predict(RFC_model,data_features,data_class.values.ravel(),cv=10)\nGNB_scores = cross_val_predict(GNB_model,data_features,data_class.values.ravel(),cv=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"function for plotting the ROC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr,tpr,line_sty,label):\n    plt.plot(fpr,tpr,line_sty,linewidth=1,label=label)\n    plt.plot([0,1],[0,1],\"k--\",label=None)\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plotting the ROC curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr_sgd,tpr_sgd,threshold = roc_curve(data_class,SGD_scores)\nfpr_rf,tpr_rf,threshold = roc_curve(data_class,RFC_scores)\nfpr_gnb,tpr_gnb,threshold = roc_curve(data_class,GNB_scores)\nplot_roc_curve(fpr_sgd,tpr_sgd,\"-r\",label=\"SGDC\")\nplot_roc_curve(fpr_rf,tpr_rf,\"-b\",label=\"RFC\")\nplot_roc_curve(fpr_gnb,tpr_gnb,\"-\",label=\"GNB\")\nplt.xlabel(\"False Positive Results\")\nplt.ylabel(\"True Positive Results\")\nplt.title(\"ROC-curves for\\nSGDC, RFC, GNB\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we can see that all three models gave excellent results where the random forest and SGD models seem to give a better result than the Gaussian NB to look at the reslts in a better fashion we can calculate the AUC values of these curves."},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD_auc = roc_auc_score(data_class,SGD_scores)\nRFC_auc = roc_auc_score(data_class,RFC_scores)\nGNB_auc = roc_auc_score(data_class,GNB_scores)\nprint(\"SGDClassifier AUC Value :\\n\", SGD_auc,\"\\n\")\nprint(\"RandomForestClassifier AUC Value :\\n\",RFC_auc,\"\\n\")\nprint(\"GassianNB AUC value :\\n\",GNB_auc,\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above AUC values we can see that the SGDClassifier works the best of the three with just a slight difference.\nThese values can be represented in the form of fill areas on a graph ."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}