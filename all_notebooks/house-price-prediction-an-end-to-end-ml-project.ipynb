{"cells":[{"metadata":{"tags":["hideInput"],"trusted":false,"_uuid":"085e99c41fb7d55f165fa004f0f11ce209f43174"},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'\nsns.set(style=\"ticks\")\nplt.rc('figure', figsize=(6, 3.7), dpi=100)\nplt.rc('axes', labelpad=20, facecolor=\"#ffffff\", \n       linewidth=0.4, grid=True, labelsize=10)\nplt.rc('patch', linewidth=0)\nplt.rc('xtick.major', width=0.2)\nplt.rc('ytick.major', width=0.2)\nplt.rc('grid', color='#EEEEEE', linewidth=0.25)\nplt.rc('font', family='Arial', weight='400', size=10)\nplt.rc('text', color='#282828')\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nplt.rc('savefig', pad_inches=0.3, dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a8738b6d4ec4f77c64ae9be1b3e8be99b19a368"},"cell_type":"markdown","source":"## Reading the Dataset\n\nThe first step is reading the dataset from the csv file we downloaded. We will use the `read_csv()` function from `Pandas` Python package:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"59896529d57a3d42950697a2943bd769d04105e5"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndataset = pd.read_csv(\"../input/AmesHousing.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0119da62b47d32864d06717bec1c24a5f8c618b6"},"cell_type":"markdown","source":"## Getting A Feel of the Dataset\n\nLet's display the first few rows of the dataset to get a feel of it:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"5f3c048591ff28a61a84631c67b6ab84f89a5778"},"cell_type":"code","source":"# Configuring float numbers format\npd.options.display.float_format = '{:20.2f}'.format\ndataset.head(n=5)","execution_count":null,"outputs":[]},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"1db15bab500eef551812e8a8c8cd7ddee198b1d9"},"cell_type":"code","source":"dataset.describe(include=[np.number], percentiles=[.5]) \\\n    .transpose().drop(\"count\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"19f37c0edd131ded185a055e459a87a0f1ea903d"},"cell_type":"code","source":"dataset.describe(include=[np.object]).transpose() \\\n    .drop(\"count\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b60a99bd63faeb44b1bf2b3281dc7656fc0e392f"},"cell_type":"markdown","source":"\n### Solving The Missing Values\n\nWe should deal with the problem of missing values because some machine learning models don't accept data with missing values. Firstly, let's see the number of missing values in our dataset. We want to see the number and the percentage of missing values for each column that actually contains missing values.","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"8afdfebf4c8d0c847da52d5d9df6564cc2b2f2e5"},"cell_type":"code","source":"# Getting the number of missing values in each column\nnum_missing = dataset.isna().sum()\n# Excluding columns that contains 0 missing values\nnum_missing = num_missing[num_missing > 0]\n# Getting the percentages of missing values\npercent_missing = num_missing * 100 / dataset.shape[0]\n# Concatenating the number and perecentage of missing values \n# into one dataframe and sorting it\npd.concat([num_missing, percent_missing], axis=1, \n          keys=['Missing Values', 'Percentage']).\\\n          sort_values(by=\"Missing Values\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62318b69d7af08e3a4a9ef57c6774b74ee044706"},"cell_type":"markdown","source":"Now we start dealing with these missing values.\n\n##### Pool QC\n\nThe percentage of missing values in `Pool QC` column is 99.56% which is very high. We think that a missing value in this column denotes that the corresponding house doesn't have a pool. To verify this, let's take a look at the values of `Pool Area` column:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"6ea529e2f8c98670e66f7ff09a0641e27f25934e"},"cell_type":"code","source":"dataset[\"Pool Area\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"70856c63e3233ce7b1b3cf6eafbdbc5b11524fe5"},"cell_type":"code","source":"dataset[\"Pool QC\"].fillna(\"No Pool\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cc6e583c0b3122657c861f3d06f2562dd993211"},"cell_type":"markdown","source":"##### Misc Feature\n\nThe percentage of missing values in Pool QC column is 96.38% which is very high also. Let's take a look at the values of `Misc Val` column:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"c4c5ba3a91f6bd0dbcbb142fd2936f048e17b26e"},"cell_type":"code","source":"dataset[\"Misc Val\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9377d756640fd797d95618ebdd70a365ef317d75"},"cell_type":"markdown","source":"We can see that `Misc Val` column has 2827 entries with a value of 0. `Misc Feature` has 2824 missing values. Then, as with `Pool QC`, we can say that each house without a \"miscellaneous feature\" has a missing value in `Misc Feature` column and a value of 0 in `Misc Val` column. So let's fill the missing values in `Misc Feature` column with `\"No Feature\"`:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"3aab1df96559331eeeaef49687a59a086bf95337"},"cell_type":"code","source":"dataset['Misc Feature'].fillna('No feature', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed1fb5d8885833961d13eefa2f6184437589f694"},"cell_type":"markdown","source":"##### Alley,  Fence, and Fireplace Qu\n\nAccording to the dataset documentation, `NA` in `Alley`, `Fence`, and `Fireplace Qu` columns denotes that the house doesn't have an alley, fence, or fireplace. So we fill in the missing values in these columns with `\"No Alley\"`, `\"No Fence\"`, and `\"No Fireplace\"` accordingly:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"ede465d77e7081313ad69e169e653bf7882f3be6"},"cell_type":"code","source":"dataset['Alley'].fillna('No Alley', inplace=True)\ndataset['Fence'].fillna('No Fence', inplace=True)\ndataset['Fireplace Qu'].fillna('No Fireplace', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"195a26537342ab271329a2d279c2a4d6246b9fc4"},"cell_type":"markdown","source":"##### Lot Frontage\n\nAs we saw previously, `Lot Frontage` represents the linear feet of street connected to the house. So we assume that the missing values in this column indicates that the house is not connected to any street, and we fill in the missing values with 0:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"3421cc13bc829170d8425bce7db91d225717302e"},"cell_type":"code","source":"dataset['Lot Frontage'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1e988ba5acf82b9a0eac8c7c4a39a47c0f936b4"},"cell_type":"markdown","source":"##### Garage Cond, Garage Qual, Garage Finish, Garage Yr Blt, Garage Type, Garage Cars, and Garage Area\n\nAccording to the dataset documentation, `NA` in `Garage Cond`, `Garage Qual`, `Garage Finish`, and `Garage Type` indicates that there is no garage in the house. So we fill in the missing values in these columns with `\"No Garage\"`. We notice that `Garage Cond`, `Garage Qual`, `Garage Finish`, `Garage Yr Blt` columns have 159 missing values, but `Garage Type` has 157 and both `Garage Cars` and `Garage Area` have one missing value. Let's take a look at the row that contains the missing value in `Garage Cars`:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"ae6a88dfe52784e01bb7b5446f3218dd40c8f3a7"},"cell_type":"code","source":"garage_columns = [col for col in dataset.columns if col.startswith(\"Garage\")]\ndataset[dataset['Garage Cars'].isna()][garage_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bed6d394efd8e979b512df4d286bd9301e5180a2"},"cell_type":"markdown","source":"We can see that this is the same row that contains the missing value in `Garage Area`, and that all garage columns except `Garage Type` are null in this row, so we will fill the missing values in `Garage Cars` and `Garage Area` with 0.","execution_count":null},{"metadata":{"_uuid":"b1d03bfb8da69349b6bc17dfadd5b02560d51df9"},"cell_type":"markdown","source":"We saw that there are 2 rows where `Garage Type` is not null while `Garage Cond`, `Garage Qual`, `Garage Finish`, and `Garage Yr Blt` columns are null. Let's take a look at these two rows:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"1921da3e01f059fd6cd4bb07ebc97793594d610e"},"cell_type":"code","source":"dataset[~pd.isna(dataset['Garage Type']) & \n        pd.isna(dataset['Garage Qual'])][garage_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29be2be21f31da7d26821a2408a8e95326c227d3"},"cell_type":"markdown","source":"We will replace the values of `Garage Type` with `\"No Garage\"` in these two rows also.\n\nFor `Garage Yr Blt`, we will fill in missing values with 0 since this is a numerical column:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"cb8934e303a0982aca037320ab744484da7f2b1e"},"cell_type":"code","source":"dataset['Garage Cars'].fillna(0, inplace=True)\ndataset['Garage Area'].fillna(0, inplace=True)\n\ndataset.loc[~pd.isna(dataset['Garage Type']) & \n            pd.isna(dataset['Garage Qual']), \"Garage Type\"] = \"No Garage\"\n\nfor col in ['Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond']:\n    dataset[col].fillna('No Garage', inplace=True)\n    \ndataset['Garage Yr Blt'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61b1eac8d781cbbaf8a3129daf6a505030174ccb"},"cell_type":"markdown","source":"##### Bsmt Exposure, BsmtFin Type 2, BsmtFin Type 1, Bsmt Qual, Bsmt Cond, Bsmt Half Bath, Bsmt Full Bath, Total Bsmt SF, Bsmt Unf SF, BsmtFin SF 2, and BsmtFin SF 1\n\n\nAccording to the dataset documentation, `NA` in any of the first five of these columns indicates that there is no basement in the house. So we fill in the missing values in these columns with `\"No Basement\"`. We notice that the first five of these columns have 80 missing values, but `BsmtFin Type 2` has 81, `Bsmt Exposure` has 83, `Bsmt Half Bath` and `Bsmt Full Bath` each has 2, and each of the others has 1. Let's take a look at the rows where `Bsmt Half Bath` is null:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"20d7f6799d715e746897c4c34c5b7f5bfc736992"},"cell_type":"code","source":"bsmt_columns = [col for col in dataset.columns if \"Bsmt\" in col]\ndataset[dataset['Bsmt Half Bath'].isna()][bsmt_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e0eac1b44ad681fc916f0e8f8ee8b8157307749"},"cell_type":"markdown","source":"We can see that these are the same rows that contain the missing values in `Bsmt Full Bath`, and that one of these two rows is contains the missing value in each of `Total Bsmt SF`, `Bsmt Unf SF`, `BsmtFin SF 2`, and `BsmtFin SF 1` columns. We notice also that `Bsmt Exposure`, `BsmtFin Type 2`, `BsmtFin Type 1`, `Bsmt Qual`, and `Bsmt Cond` are null in these rows, so we will fill the missing values in `Bsmt Half Bath`, `Bsmt Full Bath`, `Total Bsmt SF`, `Bsmt Unf SF`, `BsmtFin SF 2`, and `BsmtFin SF 1` columns with 0.\n\nWe saw that there are 3 rows where `Bsmt Exposure` is null while `BsmtFin Type 1`, `Bsmt Qual`, and `Bsmt Cond` are not null. Let's take a look at these three rows:","execution_count":null},{"metadata":{"scrolled":true,"tags":["hideOutput"],"trusted":false,"_uuid":"b2f9b65ca516f481f9729251f58a806c3fb77081"},"cell_type":"code","source":"dataset[~pd.isna(dataset['Bsmt Cond']) & \n        pd.isna(dataset['Bsmt Exposure'])][bsmt_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d622db086e56442c6bb5a4b308e6487b29c55ee"},"cell_type":"markdown","source":"We will fill in the missing values in `Bsmt Exposure` for these three rows with `\"No\"`. According to the dataset documentation, `\"No\"` for `Bsmt Exposure` means \"No Exposure\":","execution_count":null},{"metadata":{"_uuid":"ade37df141a150e5a443f4e84046412ba76ec841"},"cell_type":"markdown","source":"Let's now take a look at the row where `BsmtFin Type 2` is null while `BsmtFin Type 1`, `Bsmt Qual`, and `Bsmt Cond` are not null:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"c898d81c3349d8a47fa331de6015f4c00135fcf6"},"cell_type":"code","source":"dataset[~pd.isna(dataset['Bsmt Cond']) & \n        pd.isna(dataset['BsmtFin Type 2'])][bsmt_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"585eaff7947c260ea583be47570f7d0e59a9b81b"},"cell_type":"markdown","source":"We will fill in the missing value in `BsmtFin Type 2` for this row with `\"Unf\"`. According to the dataset documentation, `\"Unf\"` for `BsmtFin Type 2` means \"Unfinished\":","execution_count":null},{"metadata":{"trusted":false,"_uuid":"92ffa151620d86a5fae901279d14002447a1b3ac"},"cell_type":"code","source":"for col in [\"Bsmt Half Bath\", \"Bsmt Full Bath\", \"Total Bsmt SF\", \n            \"Bsmt Unf SF\", \"BsmtFin SF 2\", \"BsmtFin SF 1\"]:\n    dataset[col].fillna(0, inplace=True)\n\ndataset.loc[~pd.isna(dataset['Bsmt Cond']) & \n            pd.isna(dataset['Bsmt Exposure']), \"Bsmt Exposure\"] = \"No\"\ndataset.loc[~pd.isna(dataset['Bsmt Cond']) & \n            pd.isna(dataset['BsmtFin Type 2']), \"BsmtFin Type 2\"] = \"Unf\"\n\nfor col in [\"Bsmt Exposure\", \"BsmtFin Type 2\", \n            \"BsmtFin Type 1\", \"Bsmt Qual\", \"Bsmt Cond\"]:\n    dataset[col].fillna(\"No Basement\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d91aa040698fc3ce5a48d5411ad867c7b296090"},"cell_type":"markdown","source":"##### Mas Vnr Area and Mas Vnr Type\n\nEach of these two columns have 23 missing values. We will fill in these missing values with `\"None\"` for `Mas Vnr Type` and with 0 for `Mas Vnr Area`. We use `\"None\"` for `Mas Vnr Type` because in the dataset documentation, `\"None\"` for `Mas Vnr Type` means \"None\" (i.e. no masonry veneer):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"07eabe20173f0f9f216c5d2e133d2a2e6aa80774"},"cell_type":"code","source":"dataset['Mas Vnr Area'].fillna(0, inplace=True)\ndataset['Mas Vnr Type'].fillna(\"None\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c7ae9115c4e35e6079c0837193d8917390cebb1"},"cell_type":"markdown","source":"##### Electrical\n\nThis column has one missing value. We will fill in this value with the mode of this column:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"f5a486a6a785056a5995bd05a0ee03ef6b86e71a"},"cell_type":"code","source":"dataset['Electrical'].fillna(dataset['Electrical'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"323b1534d6787951817bdd0da93ef2ef3fe355ea"},"cell_type":"markdown","source":"Now let's check if there is any remaining missing value in our dataset:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"b369ecbe67d774cfcc0e716a9315e1b88f142da6"},"cell_type":"code","source":"dataset.isna().values.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f64babdf993e3133d8530a04e1683bd4126b9775"},"cell_type":"markdown","source":"This means that our dataset is now complete; it doesn't contain any missing value anymore.\n\n## Outlier Removal\n\nIn the paper in which our dataset was introduced by De Cock (2011), the author states that there are five unusual values and outliers in the dataset, and encourages the removal of these outliars. He suggested plotting `SalePrice` against `Gr Liv Area` to spot the outliers. We will do that now:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"0bb0de396732fb5c7f1adebf26a21efaf251145f"},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\nplt.scatter(x=dataset['Gr Liv Area'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Gr Liv Area\"); plt.ylabel(\"SalePrice\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03d7687660d641769d6b1a62b8116130cadb881a"},"cell_type":"markdown","source":"We can clearly see the five values meant by the authour in the plot above. Now, we will remove them from our dataset. We can do so by keeping data points that have `Gr Liv Area` less than 4,000. But first we take a look at the dataset rows that correspond to these unusual values:","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"224b23eb7f6937e14923305b33117327861b47fb"},"cell_type":"code","source":"outlirt_columns = [\"Gr Liv Area\"] + \\\n                  [col for col in dataset.columns if \"Sale\" in col]\ndataset[dataset[\"Gr Liv Area\"] > 4000][outlirt_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"758786b8ea1bc351ed15feb69f99705ea4245f08"},"cell_type":"markdown","source":"Now we remove them:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"6145c06916521ef9519a767925281e676de99cf1"},"cell_type":"code","source":"dataset = dataset[dataset[\"Gr Liv Area\"] < 4000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dc980de3e047ed5c386546bcccdeab7620eda87a"},"cell_type":"code","source":"plt.scatter(x=dataset['Gr Liv Area'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Gr Liv Area\"); plt.ylabel(\"SalePrice\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46a681450d329d14e472ac57d4b6ad9ba4fab2e4"},"cell_type":"markdown","source":"To avoid problems in modeling later, we will reset our dataset index after removing the outlier rows, so no gaps remain in our dataset index:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"ea60a12e0fa0dcf3298bd24a4493844f5808209f"},"cell_type":"code","source":"dataset.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15fe7543c5e369907836564b43b9624448e97a8e"},"cell_type":"markdown","source":"## Deleting Some Unimportant Columns\n\nWe will delete columns that are not useful in our analysis. The columns to be deleted are `Order` and `PID`:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"82a8ebcbff188d9009c41fe2bb2bad06bfb66325"},"cell_type":"code","source":"dataset.drop(['Order', 'PID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"206d656dc243b7155faa0ea1d27338bee26b36f8"},"cell_type":"markdown","source":"<h1 id=\"eda\">Exploratory Data Analysis</h1>\n\nIn this section, we will explore the data using visualizations. This will allow us to understand the data and the relationships between variables better, which will help us build a better model.\n\n## Target Variable Distribution\n\nOur dataset contains a lot of variables, but the most important one for us to explore is the target variable. We need to understand its distribution. First, we start by plotting the violin plot for the target variable. ","execution_count":null},{"metadata":{"trusted":false,"_uuid":"a5697a28ec50aa520b207fddd653364d0661c010"},"cell_type":"code","source":"sns.violinplot(x=dataset['SalePrice'], inner=\"quartile\", color=\"#36B37E\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52a6b9ecc38d3b282b9dc5fcb3515f77a363a295"},"cell_type":"markdown","source":"We can see from the plot that most house prices fall between 100,000 and 250,000. The dashed lines represent the locations of the three quartiles Q1, Q2 (the median), and Q3. Now let's see the box plot of `SalePrice`:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"3663ffe68c6e4384fea4414b813ae9075d0f2b4b"},"cell_type":"code","source":"sns.boxplot(dataset['SalePrice'], whis=10, color=\"#00B8D9\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b99f1646aefccfbb1501bf7176f4236f1b57069"},"cell_type":"markdown","source":"This shows us the minimum and maximum values of `SalePrice`. It shows us also the three quartiles represented by the box and the vertical line inside of it. Lastly, we plot the histogram of the variable to see a more detailed view of the distribution:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"b04904c6303738fdaffc71eff84c87506987ccb9"},"cell_type":"code","source":"sns.distplot(dataset['SalePrice'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a78c4a7f9c4f1257e7353e46df371902c2c808bb"},"cell_type":"markdown","source":"## Correlation Between Variables\n\n","execution_count":null},{"metadata":{"trusted":false,"_uuid":"ae332817bf98bb6379222fd48962ae4bf06b0efe"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(dataset.corr(), ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a3a1b770d58bf617a9ea40ecfcbc2374ce3c952b"},"cell_type":"code","source":"sns.distplot(dataset['SalePrice'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b279ff06d34c54d65edb86f8bfc7bb0d7997838e"},"cell_type":"markdown","source":"We can see that most house prices fall between 100,000 and 200,000. We see also that there is a number of expensive houses to the right of the plot. Now, we move to see the distribution of `Overall Qual` variable:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"84df4417919d94e26917cd227232258759b302ee"},"cell_type":"code","source":"sns.distplot(dataset['Overall Qual'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 1});\nplt.ylabel(\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"043624f25e69292db3deff06d7050ad89b5982c1"},"cell_type":"markdown","source":"We see that `Overall Qual` takes an integer value between 1 and 10, and that most houses have an overall quality between 5 and 7. Now we plot the scatter plot of `SalePrice` and `Overall Qual` to see the relationship between them:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"7d2e0d90144d0577bd9d2532d4761fd05003e4a3"},"cell_type":"code","source":"plt.scatter(x=dataset['Overall Qual'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Overall Qual\"); plt.ylabel(\"SalePrice\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c4a19df9e1b050c7739989751da2af63c2586ee"},"cell_type":"markdown","source":"We can see that they are truly positively correlated; generally, as the overall quality increases, the sale price increases too. This verfies what we got from the heatmap above.\n\nNow, we want to see the relationship between the target variable and `Gr Liv Area` variable which represents the living area above ground. Let us first see the distribution of `Gr Liv Area`:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"8c8da863eb37fe8935fa10dca30e3c708589c494"},"cell_type":"code","source":"sns.distplot(dataset['Gr Liv Area'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"441af5298264acb68b1948a16d9571ccca7cab33"},"cell_type":"markdown","source":"We can see that the above-ground living area falls approximately between 800 and 1800 ft<sup>2</sup>. Now, let us see the relationship between `Gr Liv Area` and the target variable:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"1b98deae85fbda7647c0928d1b32fe0490103ae0"},"cell_type":"code","source":"plt.scatter(x=dataset['Gr Liv Area'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Gr Liv Area\"); plt.ylabel(\"SalePrice\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fdb4db087b249dd18e4bb5c72bc8c6d4b0e3e3a"},"cell_type":"markdown","source":"The scatter plot above shows clearly the strong positive correlation between `Gr Liv Area` and `SalePrice` verifying what we found with the heatmap.\n\n#### Moderate Positive Correlation\n\nNext, we want to visualize the relationship between the target variable and the variables that are positively correlated with it, but the correlation is not very strong. Namely, these variables are `Year Built`, `Year Remod/Add`, `Mas Vnr Area`, `Total Bsmt SF`, `1st Flr SF`, `Full Bath`, `Garage Cars`, and `Garage Area`. We start with the first four. Let us see the distribution of each of them:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"0b6dda485f5342cb0fa5e2df0b53ce36da6a40ec"},"cell_type":"code","source":"fig, axes = plt.subplots(1, 4, figsize=(18,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.6)\nfor ax, v in zip(axes.flat, [\"Year Built\", \"Year Remod/Add\", \n                             \"Mas Vnr Area\", \"Total Bsmt SF\"]):\n    sns.distplot(dataset[v], kde=False, color=\"#172B4D\", \n                 hist_kws={\"alpha\": 0.8}, ax=ax)\n    ax.set(ylabel=\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fecabca761e2f746e8cb0f1835e0da3be65a82e"},"cell_type":"markdown","source":"Now let us see their relationships with the target variable using scatter plots:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"9632da7928d5b04a3c7ee421c910384809609d5d"},"cell_type":"code","source":"x_vars = [\"Year Built\", \"Year Remod/Add\", \"Mas Vnr Area\", \"Total Bsmt SF\"]\ng = sns.PairGrid(dataset, y_vars=[\"SalePrice\"], x_vars=x_vars);\ng.map(plt.scatter, color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a34cb6a43901e4d3b1edb6e86aef451a33172640"},"cell_type":"markdown","source":"Next, we move to the last four. Let us see the distribution of each of them:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"24b97613ca4a64bd2996ba3f08b148147b626aa4"},"cell_type":"code","source":"fig, axes = plt.subplots(1, 4, figsize=(18,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.6)\nfor ax, v in zip(axes.flat, [\"1st Flr SF\", \"Full Bath\", \n                             \"Garage Cars\", \"Garage Area\"]):\n    sns.distplot(dataset[v], kde=False, color=\"#172B4D\", \n                 hist_kws={\"alpha\": 0.8}, ax=ax);\n    ax.set(ylabel=\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2794a272a998912c8a3c3736f811bd58617a8a3"},"cell_type":"markdown","source":"And now let us see their relationships with the target variable:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"8d98014ec8ff6ab86dee7fffb7093649b1f4b702"},"cell_type":"code","source":"x_vars = [\"1st Flr SF\", \"Full Bath\", \"Garage Cars\", \"Garage Area\"]\ng = sns.PairGrid(dataset, y_vars=[\"SalePrice\"], x_vars=x_vars);\ng.map(plt.scatter, color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3940c5357b6b2884a7f3d3a93e649bbcbc135f4"},"cell_type":"markdown","source":"From the plots above, we can see that these eight variables are truly positively correlated with the target variable. However, it's apparent that they are not as highly correlated as `Overall Qual` and `Gr Liv Area`.\n\n### Relatioships Between Predictor Variables\n\n#### Positive Correlation\n\nApart from the target variable, when we plotted the heatmap, we discovered a high positive correlation between `Garage Cars` and `Garage Area` and between `Gr Liv Area` and `TotRms AbvGrd`. We want to visualize these correlations also. We've already seen the distribution of each of them except for `TotRms AbvGrd`. Let us see the distribution of `TotRms AbvGrd` first:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"911dc5b15d51ac4a2da9310229f32893cd808534"},"cell_type":"code","source":"sns.distplot(dataset['TotRms AbvGrd'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6600fde357db9c4e87766ca0cdd5583dc580bb87"},"cell_type":"markdown","source":"Now, we visualize the relationship between `Garage Cars` and `Garage Area` and between `Gr Liv Area` and `TotRms AbvGrd`:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"15add0d24a55bf4be30a9b4cd6c6fedd2b02c9b6"},"cell_type":"code","source":"plt.rc(\"grid\", linewidth=0.05)\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\nh1 = axes[0].hist2d(dataset[\"Garage Cars\"], \n                    dataset[\"Garage Area\"],\n                    cmap=\"viridis\");\naxes[0].set(xlabel=\"Garage Cars\", ylabel=\"Garage Area\")\nplt.colorbar(h1[3], ax=axes[0]);\nh2 = axes[1].hist2d(dataset[\"Gr Liv Area\"], \n                    dataset[\"TotRms AbvGrd\"],\n                    cmap=\"viridis\");\naxes[1].set(xlabel=\"Gr Liv Area\", ylabel=\"TotRms AbvGrd\")\nplt.colorbar(h1[3], ax=axes[1]);\nplt.rc(\"grid\", linewidth=0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"550f8b2ea62a21e8171398056e24c80b94283b91"},"cell_type":"markdown","source":"We can see the strong correlation between each pair. For `Garage Cars` and `Garage Area`, we see that the highest concentration of data is when `Garage Cars` is 2 and `Garage Area` is approximately between 450 and 600 ft<sup>2</sup>. For `Gr Liv Area` and `TotRms AbvGrd`, we notice that the highest concentration is when `Garage Liv Area` is roughly between 800 and 2000 ft<sup>2</sup> and `TotRms AbvGrd` is 6.\n\n#### Negative Correlation\n\nWhen we plotted the heatmap, we also discovered a significant negative correlation between `Bsmt Unf SF` and `BsmtFin SF 1`, and between `Bsmt Unf SF` and `Bsmt Full Bath`. We also want to visualize these correlations. Let us see the distribution of these variables first:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"c55e8e2c5b8b4e2dac57cdef03dea9b16810da20"},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(16,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.6)\nfor ax, v in zip(axes.flat, [\"Bsmt Unf SF\", \"BsmtFin SF 1\", \"Bsmt Full Bath\"]):\n    sns.distplot(dataset[v], kde=False, color=\"#172B4D\", \n                 hist_kws={\"alpha\": 0.8}, ax=ax);\n    ax.set(ylabel=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e1900e16e157a658aed99daaa89cfc139d97bff"},"cell_type":"markdown","source":"Now, we visualize the relationship between each pair using scatter plots:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"989912ce950104476bb532d809077e7ad80de540"},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\naxes[0].scatter(dataset[\"Bsmt Unf SF\"], dataset[\"BsmtFin SF 1\"],\n                color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\naxes[0].set(xlabel=\"Bsmt Unf SF\", ylabel=\"BsmtFin SF 1\");\naxes[1].scatter(dataset[\"Bsmt Unf SF\"], dataset[\"Bsmt Full Bath\"],\n                color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\naxes[1].set(xlabel=\"Bsmt Unf SF\", ylabel=\"Bsmt Full Bath\");","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"b419a2961e1a585156b7e34031cac0e0a9241da4"},"cell_type":"code","source":"for f in [\"Overall Qual\", \"Gr Liv Area\"]:\n    dataset[f + \"_p2\"] = dataset[f] ** 2\n    dataset[f + \"_p3\"] = dataset[f] ** 3\ndataset[\"OverallQual_GrLivArea\"] = \\\n    dataset[\"Overall Qual\"] * dataset[\"Gr Liv Area\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a19398024f090d428524c7beb7e1ffcd8366fb87"},"cell_type":"code","source":"dataset.drop([\"Garage Cars\", \"TotRms AbvGrd\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fed4b782a9360bbeff2b219dcc51a5be65877208"},"cell_type":"markdown","source":"### Dealing with Ordinal Variables\n\nThere are some ordinal features in our dataset. For example, the `Bsmt Cond` feature has the following possible values: ","execution_count":null},{"metadata":{"trusted":false,"_uuid":"3afc262294e76a56a35c1fed35c9e336e09d56e1"},"cell_type":"code","source":"print(\"Unique values in 'Bsmt Cond' column:\")\nprint(dataset['Bsmt Cond'].unique().tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"77a0c5e2995206901b66ddd122091aa5077146c2"},"cell_type":"code","source":"mp = {'Ex':4,'Gd':3,'TA':2,'Fa':1,'Po':0}\ndataset['Exter Qual'] = dataset['Exter Qual'].map(mp)\ndataset['Exter Cond'] = dataset['Exter Cond'].map(mp)\ndataset['Heating QC'] = dataset['Heating QC'].map(mp)\ndataset['Kitchen Qual'] = dataset['Kitchen Qual'].map(mp)\n\nmp = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Basement':0}\ndataset['Bsmt Qual'] = dataset['Bsmt Qual'].map(mp)\ndataset['Bsmt Cond'] = dataset['Bsmt Cond'].map(mp)\ndataset['Bsmt Exposure'] = dataset['Bsmt Exposure'].map(\n    {'Gd':4,'Av':3,'Mn':2,'No':1,'No Basement':0})\n\nmp = {'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'No Basement':0}\ndataset['BsmtFin Type 1'] = dataset['BsmtFin Type 1'].map(mp)\ndataset['BsmtFin Type 2'] = dataset['BsmtFin Type 2'].map(mp)\n\ndataset['Central Air'] = dataset['Central Air'].map({'Y':1,'N':0})\ndataset['Functional'] = dataset['Functional'].map(\n    {'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,\n     'Maj2':2,'Sev':1,'Sal':0})\ndataset['Fireplace Qu'] = dataset['Fireplace Qu'].map(\n    {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Fireplace':0})\ndataset['Garage Finish'] = dataset['Garage Finish'].map(\n    {'Fin':3,'RFn':2,'Unf':1,'No Garage':0})\ndataset['Garage Qual'] = dataset['Garage Qual'].map(\n    {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\ndataset['Garage Cond'] = dataset['Garage Cond'].map(\n    {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\ndataset['Pool QC'] = dataset['Pool QC'].map(\n    {'Ex':4,'Gd':3,'TA':2,'Fa':1,'No Pool':0})\ndataset['Land Slope'] = dataset['Land Slope'].map(\n    {'Sev': 2, 'Mod': 1, 'Gtl': 0})\ndataset['Fence'] = dataset['Fence'].map(\n    {'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'No Fence':0})","execution_count":null,"outputs":[]},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"9dad5b17714b5ed742efaba2b59f77f8c21ef229"},"cell_type":"code","source":"dataset[['Paved Drive']].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b424e43f7fed472583c61f1db40263dcebfa41cd"},"cell_type":"markdown","source":"Now, we perform one-hot encoding:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"62c2f187d357530110b87504b7ebc2e88fe15e45"},"cell_type":"code","source":"dataset = pd.get_dummies(dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf8b97a6c7a76c1efd86ffe2e9d3929bf0f3f5ce"},"cell_type":"markdown","source":"Let us see what has happened to the `Paved Drive` variable by looking at the same rows above: ","execution_count":null},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"f847bf3a7b4e6212263f4f6e148bbc9c81b4e18d"},"cell_type":"code","source":"pavedDrive_oneHot = [c for c in dataset.columns if c.startswith(\"Paved\")]\ndataset[pavedDrive_oneHot].head()","execution_count":null,"outputs":[]},{"metadata":{"tags":["hideOutput"],"trusted":false,"_uuid":"a487ae5a7f91452c2a4922981e4046e998e9817c"},"cell_type":"code","source":"dataset[['SalePrice']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dc0bf6d4c20f53a5e1e53ae1712ae7376d8bbdd6"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# We need to fit the scaler to our data before transformation\ndataset.loc[:, dataset.columns != 'SalePrice'] = scaler.fit_transform(\n    dataset.loc[:, dataset.columns != 'SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"773780549319f11649c225ab855236c61f2c0377"},"cell_type":"markdown","source":"## Splitting the Dataset\n\nAs usual for supervised machine learning problems, we need a training dataset to train our model and a test dataset to evaluate the model. So we will split our dataset randomly into two parts, one for training and the other for testing. For that, we will use another function from Scikit-Learn called `train_test_split()`:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"f5fbe254e8925ef47851db2dbc7d33a99fdb2411"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset.drop('SalePrice', axis=1), dataset[['SalePrice']], \n    test_size=0.25, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5083cb19ed132f9cb935892436083761c79b3561"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nparameter_space = {\n    \"alpha\": [1, 10, 100, 290, 500],\n    \"fit_intercept\": [True, False],\n    \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n}\n\nclf = GridSearchCV(Ridge(random_state=3), parameter_space, n_jobs=4,\n                   cv=3, scoring=\"neg_mean_absolute_error\")\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbc7aca13d770c94aa8c79149185fe56342448c7"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Ridge model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"5731114f2e0d2f57765b1f5806e22427ff05e509"},"cell_type":"code","source":"ridge_model = Ridge(random_state=3, **clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc64f8375606e3c5eceeaeca773d37ab401c37f7"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"429c4a22e366cd9508222678c8c3af95c4ab922c"},"cell_type":"code","source":"ridge_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec6c33e25d0901edbf7a408dc5cf9194b8f0ae41"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"ec753d55c0782bbdecf624f622e0d4e92c202f4d"},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\ny_pred = ridge_model.predict(X_test)\nridge_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Ridge MAE =\", ridge_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d0722513ba9d6476291f18a1db1f140581acf91"},"cell_type":"markdown","source":"#### 2. Elastic Net\n\nThis model has the following syntax:\n\n```py\nElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, \n           precompute=False, max_iter=1000, copy_X=True, tol=0.0001, \n           warm_start=False, positive=False, random_state=None, selection=’cyclic’)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `alpha` is a constant that multiplies the penalty terms, `l1_ratio` determines the amount of L1 and L2 regularizations, `fit_intercept` is the same as Ridge's.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"169fc24fd1b1369d7ad49ac312243710a27849ff"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nparameter_space = {\n    \"alpha\": [1, 10, 100, 280, 500],\n    \"l1_ratio\": [0.5, 1],\n    \"fit_intercept\": [True, False],\n}\n\nclf = GridSearchCV(ElasticNet(random_state=3), parameter_space, \n                   n_jobs=4, cv=3, scoring=\"neg_mean_absolute_error\")\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7448ecd3e1d04304afbe8d5be038826095df03ff"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Ridge model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"b54a8fd4f1475aec665c7043dc020c3d5b6f104f"},"cell_type":"code","source":"elasticNet_model = ElasticNet(random_state=3, **clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ad220daf8aa2ce220612a178cc0fa54f3a4379b"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"212c028929a900f34c0654dc4ea4942ab1159b7f"},"cell_type":"code","source":"elasticNet_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbfdcc6bb28820f61bff12239933831eca40cd9b"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"04d4c4bd99fe62000a8b4b4569c7717559dd7c40"},"cell_type":"code","source":"y_pred = elasticNet_model.predict(X_test)\nelasticNet_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Elastic Net MAE =\", elasticNet_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9558f31927cf822518b76bc5ede0eaec7779d666"},"cell_type":"markdown","source":"### Nearest Neighbors\n\nFor Nearest Neighbors, we will use an implementation of the k-nearest neighbors (KNN) algorithm provided by Scikit-Learn package.\n\nThe KNN model has the following syntax:\n\n```py\nKNeighborsRegressor(n_neighbors=5, weights=’uniform’, algorithm=’auto’, \n                    leaf_size=30, p=2, metric=’minkowski’, metric_params=None, \n                    n_jobs=None, **kwargs)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `n_neighbors` represents `k` which is the number of neighbors to use, `weights` determines the weight function used in prediction: `uniform` or `distance`, `algorithm` specifies the algorithm used to compute the nearest neighbors, `leaf_size` is passed to `BallTree` or `KDTree` algorithm. It can affect the speed of the construction and query, as well as the memory required to store the tree.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"db7f0b5603cbd23f2876be10e20b9ab8b9ddd672"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nparameter_space = {\n    \"n_neighbors\": [9, 10, 11,50],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n    \"leaf_size\": [1,2,20,50,200]\n}\n\nclf = GridSearchCV(KNeighborsRegressor(), parameter_space, cv=3, \n                   scoring=\"neg_mean_absolute_error\", n_jobs=4)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c8ea886c554b6391ea847d59bd2ce190bbb4e2b"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Ridge model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"5f7ef9d545024ae0cedc8a65a430527a2c02b577"},"cell_type":"code","source":"knn_model = KNeighborsRegressor(**clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cf4739fd211afc074b8910648573cf50d1893e0"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"c6c4e5306d986c1954f24ab8419d985a81e23fb4"},"cell_type":"code","source":"knn_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"001641cd3374f397a188e6935646198e8e7231ce"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"72b7ddbabd6f010c137eada0c58d9a91d8106213"},"cell_type":"code","source":"y_pred = knn_model.predict(X_test)\nknn_mae = mean_absolute_error(y_test, y_pred)\nprint(\"K-Nearest Neighbors MAE =\", knn_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a816cd651873e13278773feca2c692b7e8d8ac3"},"cell_type":"markdown","source":"### Support Vector Regression\n\nFor Support Vector Regression (SVR), we will use one of three implementations provided by the Scikit-Learn package.\n\nThe SVR model has the following syntax:\n\n```py\nSVR(kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, tol=0.001, \n    C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `kernel` specifies the kernel type to be used in the algorithm, `degree` represents the degree of the polynomial kernel `poly`, `gamma` is the kernel coefficient for `rbf`, `poly` and `sigmoid` kernels, `coef0` is independent term in kernel function, and `C` is the penalty parameter of the error term.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"bc4ad5a528de1b9224e01450c83c386f86df4d04"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVR\n\nparameter_space = \\\n    {\n        \"kernel\": [\"poly\", \"linear\", \"rbf\", \"sigmoid\"],\n        \"degree\": [3, 5],\n        \"coef0\": [0, 3, 7],\n        \"gamma\":[1e-3, 1e-1, 1/X_train.shape[1]],\n        \"C\": [1, 10, 100],\n    }\n\nclf = GridSearchCV(SVR(), parameter_space, cv=3, n_jobs=4,\n                   scoring=\"neg_mean_absolute_error\")\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f8cbf709521d3f1a44fbd31c3173da26a1c66bc"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Support Vector Regression model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"a20171fd9cf07c24bd9b27819279bcd3a84e3116"},"cell_type":"code","source":"svr_model = SVR(**clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3f2eb38d2d6b1d7e4c29c48de1d387c8d784f86"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"5b145492a0b2ab03b8e6cd919be5c57be2ce332d"},"cell_type":"code","source":"svr_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0017ee29b1a32b408e9dc06f65660cc73b8dfafc"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"b632b4ed79e210806fea4d9d9898a6e8344d2cb4"},"cell_type":"code","source":"y_pred = svr_model.predict(X_test)\nsvr_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Support Vector Regression MAE =\", svr_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f62107925cc8f3aa177cd671183cccaa1a3a2060"},"cell_type":"markdown","source":"### Decision Tree\n\nFor Decision Tree (DT), we will use an implementations provided by the Scikit-Learn package.\n\nThe Decision Tree model has the following syntax:\n\n```py\nDecisionTreeRegressor(criterion=’mse’, splitter=’best’, max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features=None, \n                      random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, presort=False)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `criterion` specifies the function used to measure the quality of a split, `min_samples_split` determines the minimum number of samples required to split an internal node, `min_samples_leaf` determines the minimum number of samples required to be at a leaf node, and `max_features` controls the number of features to consider when looking for the best split.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"0b89cfcf6f3fa0ef79bcb25a9bb711b50a19fa49"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nparameter_space = \\\n    {\n        \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n        \"min_samples_split\": [5, 18, 29, 50],\n        \"min_samples_leaf\": [3, 7, 15, 25],\n        \"max_features\": [20, 50, 150, 200, X_train.shape[1]],\n    }\n\nclf = GridSearchCV(DecisionTreeRegressor(random_state=3), parameter_space, \n                   cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=4)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b69ee71e337958af9b0268ca83d261b407ac25"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Decision Tree model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"0e3c5321d64335de53f33adc0dd0ef79623a2258"},"cell_type":"code","source":"dt_model = DecisionTreeRegressor(**clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24def1c2d6e57c1618c9e38986c991d007387294"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"d1872a83b56ad8abbc0b7d3c22ae55f14490f348"},"cell_type":"code","source":"dt_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03eb3b1d88c69b80873000900fa7fd695daf09d1"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"4c989cef7372ce94aff4c7c9cb872d1225ed94dd"},"cell_type":"code","source":"y_pred = dt_model.predict(X_test)\ndt_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Decision Tree MAE =\", dt_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b879fde0223a61cf00f6624d8c99a64df3623a2"},"cell_type":"markdown","source":"### Neural Network\n\nFor Neural Network (NN), we will use an implementations provided by the Scikit-Learn package.\n\nThe Neural Network model has the following syntax:\n\n```py\nMLPRegressor(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, \n             alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, \n             learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n             random_state=None, tol=0.0001, verbose=False, warm_start=False, \n             momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n             validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n             n_iter_no_change=10)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `hidden_layer_sizes` is a list where its ith element represents the number of neurons in the ith hidden layer, `activation` specifies the activation function for the hidden layer, `solver` determines the solver for weight optimization, and `alpha` represents L2 regularization penalty.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"e7593355cb8c71281d658992e4227797186abe6b"},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\n\nparameter_space = \\\n    {\n        \"hidden_layer_sizes\": [(7,)*3, (19,), (100,), (154,)],\n        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n        \"solver\": [\"lbfgs\"],\n        \"alpha\": [1, 10, 100],\n    }\n\nclf = GridSearchCV(MLPRegressor(random_state=3), parameter_space, \n                   cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=4)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f3e855c673d1d54040310086104456ed93249d"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Neural Network model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"66b5a633d4a35609722c0c89a63feb1394e74a16"},"cell_type":"code","source":"nn_model = MLPRegressor(**clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8af916a73b92056996839d89beec7ff41eb0e9e9"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"70bda8397dcbe91b6eaf8b143f6a0b183c02ebe2"},"cell_type":"code","source":"nn_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80a843c2f9654e666f6d3340de356cff38763900"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"2c43e26f24dc136f9d0d7c12336d2f658a7162cd"},"cell_type":"code","source":"y_pred = nn_model.predict(X_test)\nnn_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Neural Network MAE =\", nn_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f792359c04844082c1786b67dbcace8138bed21"},"cell_type":"markdown","source":"### Random Forest\n\nFor Random Forest (RF), we will use an implementations provided by the Scikit-Learn package.\n\nThe Random Forest model has the following syntax:\n\n```py\nRandomForestRegressor(n_estimators=’warn’, criterion=’mse’, max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features=’auto’, \n                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, bootstrap=True, oob_score=False, \n                      n_jobs=None, random_state=None, verbose=0, warm_start=False)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `n_estimators` specifies the number of trees in the forest, `bootstrap` determines whether bootstrap samples are used when building trees. `criterion`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features` are the same as those of the decision tree model.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"d97de957d8dd45bc55995cfc8add33cd456f0a69"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nparameter_space = \\\n    {\n        \"n_estimators\": [10, 100, 300, 600],\n        \"criterion\": [\"mse\", \"mae\"],\n        \"max_depth\": [7, 50, 254],\n        \"min_samples_split\": [2, 5],\n        \"min_samples_leaf\": [1, 5],\n        \"max_features\": [19, 100, X_train.shape[1]],\n        \"bootstrap\": [True, False],\n    }\n\nclf = RandomizedSearchCV(RandomForestRegressor(random_state=3), \n                         parameter_space, cv=3, n_jobs=4,\n                         scoring=\"neg_mean_absolute_error\", \n                         n_iter=10, random_state=3)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d621af1fd6e5702cb79e7bb4d5dd58937895ae8a"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Random Forest model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"73e25cf56333d0efc3ab28e1bb6a1edf828c77f1"},"cell_type":"code","source":"rf_model = RandomForestRegressor(**clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c38c8da96a21f4f300a3fb56c19cba267269a1"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"6dd34afc36e723a27ae5a87a108d6d63ed84ea37"},"cell_type":"code","source":"rf_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbe0bea89b194c3d1eec8b365c98dfebd8a6b9ed"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"d2005f32d92a6e6fa3f4d4f152c835085dfc7871"},"cell_type":"code","source":"y_pred = rf_model.predict(X_test)\nrf_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Random Forest MAE =\", rf_mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c14a3aa4ab6bd85dfb5864971dec295cf02e1f0"},"cell_type":"markdown","source":"### Gradient Boosting\n\nFor Gradient Boosting (GB), we will use the renowned XGBoost implementations.\n\nXGBoost model has the following syntax:\n\n```py\nXGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n             objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, \n             gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, \n             colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n             scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n             missing=None, importance_type='gain', **kwargs)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `max_depth` sets the maximum depth of a tree, `learning_rate` represents the step size shrinkage used in updating weights, `n_estimators` specifies the number of boosted trees to fit, `booster` determines which booster to use, `gamma` specifies the minimum loss reduction required to make a further partition on a leaf node of the tree, `subsample` is subsample ratio of the training instances; this subsampling will occur once in every boosting iteration, `colsample_bytree` specifies the subsample ratio of columns when constructing each tree, `colsample_bylevel` specifies the subsample ratio of columns for each split, in each level, `reg_alpha` is L1 regularization term, and `reg_lambda` is L2 regularization term.","execution_count":null},{"metadata":{"trusted":false,"_uuid":"02f09508f527ebe8ca435d0460de245716befda7"},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nparameter_space = \\\n    {\n        \"max_depth\": [4, 5, 6],\n        \"learning_rate\": [0.005, 0.009, 0.01],\n        \"n_estimators\": [700, 1000, 2500],\n        \"booster\": [\"gbtree\",],\n        \"gamma\": [7, 25, 100],\n        \"subsample\": [0.3, 0.6],\n        \"colsample_bytree\": [0.5, 0.7],\n        \"colsample_bylevel\": [0.5, 0.7,],\n        \"reg_alpha\": [1, 10, 33],\n        \"reg_lambda\": [1, 3, 10],\n    }\n\nclf = RandomizedSearchCV(XGBRegressor(random_state=3), \n                         parameter_space, cv=3, n_jobs=4,\n                         scoring=\"neg_mean_absolute_error\", \n                         random_state=3, n_iter=10)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7f04b8186173be556bf2a1b6ad1d47a9d6589ac"},"cell_type":"markdown","source":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our XGBoost model with the best parameters found:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"42a829335b56cd697134d68635d06b7d5040072d"},"cell_type":"code","source":"xgb_model = XGBRegressor(**clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68a1ff7b738e7c640343c7356ef70360c5ad15af"},"cell_type":"markdown","source":"Then we train our model using our training set (`X_train` and `y_train`):","execution_count":null},{"metadata":{"trusted":false,"_uuid":"61bce49e282b837a654129f5ca116c49638b3977"},"cell_type":"code","source":"xgb_model.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"167e6d86247586bcbe53967f0dd330c7d8c08f3d"},"cell_type":"markdown","source":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","execution_count":null},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"992779d82ae7886508ea6c59b9b101e3894dee09"},"cell_type":"code","source":"y_pred = xgb_model.predict(X_test)\nxgb_mae = mean_absolute_error(y_test, y_pred)\nprint(\"XGBoost MAE =\", xgb_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0185ea61d5066fba3c8d1a27d26932e10bd4917b"},"cell_type":"code","source":"x = ['KNN', 'Decision Tree', 'Neural Network', 'Ridge', \n     'Elastic Net', 'Random Forest', 'SVR', 'XGBoost']\ny = [22780.14, 20873.95, 15656.38, 15270.46, 14767.91,\n     14506.46, 12874.93, 12556.68]\ncolors = [\"#392834\", \"#5a3244\", \"#7e3c4d\", \"#a1484f\", \n          \"#c05949\", \"#d86f3d\", \"#e88b2b\", \"#edab06\"]\nfig, ax = plt.subplots()\nplt.barh(y=range(len(x)), tick_label=x, width=y, height=0.4, color=colors);\nax.set(xlabel=\"MAE (smaller is better)\", ylabel=\"Model\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc67011176f42f58083b94d8e9a890972586be5b"},"cell_type":"code","source":"sns.violinplot(x=dataset['SalePrice'], inner=\"quartile\", color=\"#36B37E\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f364129134fd172965780a41082c1fe83c8d4370"},"cell_type":"code","source":"sns.boxplot(dataset['SalePrice'], whis=10, color=\"#00B8D9\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc10506b1f1eba052e8e58905f98f3a14211e532"},"cell_type":"code","source":"sns.distplot(dataset['SalePrice'], kde=False,\n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dbf43903e2808a68d80f3f6163c71858cbc3d4e"},"cell_type":"markdown","source":"From the three plots above, we can understand the distribution of `SalePrice`. Now let's get some numerical statistical information about it:","execution_count":null},{"metadata":{"scrolled":true,"tags":["hideOutput"],"trusted":false,"_uuid":"0a8029b0d98e74714e311175103e0bb1efe80502"},"cell_type":"code","source":"y_train.describe(include=[np.number])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d8dd98c61aadbc1429df6a736ac7514da350448"},"cell_type":"markdown","source":"We can see that the mean is `179,846.69` and the median is `159,895`. We can see also that the first quartile is `128,500`; this means that 75% of the data is larger than this number. Now looking at XGBoost error of `12,556.68`, we can say that an error of about `12,000` is good for data whose mean is `159,895` and whose 75% of it is larger than `128,500`.","execution_count":null},{"metadata":{"_uuid":"c3d7281ca7f0653ff52d7b523e3f3e7627f41d9a"},"cell_type":"markdown","source":"## Feature Importances\n\nSome of the models we used provide the ability to see the importance of each feature in the dataset after fitting the model. We will look at the feature importances provided by both XGBoost and Random Forest models. We have 242 features in our data which is a big number, so we will take a look at the 15 most important features.\n\n### XGBoost\n\nLet's discover the most important features as determined by XGBoost model:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"1cef1501917aab710e821c086137b20385e8ad96"},"cell_type":"code","source":"xgb_feature_importances = xgb_model.feature_importances_\nxgb_feature_importances = pd.Series(\n    xgb_feature_importances, index=X_train.columns.values\n    ).sort_values(ascending=False).head(15)\n\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.barplot(x=xgb_feature_importances, \n            y=xgb_feature_importances.index, \n            color=\"#003f5c\");\nplt.xlabel('Feature Importance');\nplt.ylabel('Feature');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd1eb81f34057692839bae65e9ec9321ef6be643"},"cell_type":"markdown","source":"### Random Forest\n\nNow, let's see the most important features as for Random Forest model:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"13b5b1a36994299485d9d9ad88ce7bd1af5c3790"},"cell_type":"code","source":"rf_feature_importances = rf_model.feature_importances_\nrf_feature_importances = pd.Series(\n    rf_feature_importances, index=X_train.columns.values\n    ).sort_values(ascending=False).head(15)\n\nfig, ax = plt.subplots(figsize=(7,5))\nsns.barplot(x=rf_feature_importances, \n            y=rf_feature_importances.index, \n            color=\"#ffa600\");\nplt.xlabel('Feature Importance');\nplt.ylabel('Feature');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e79c3fb04e89f3e6ac37efed050e7c9b27a1c21"},"cell_type":"markdown","source":"### Common Important Features\n\nNow, let us see which features are among the most important features for both XGBoost and Random Forest models, and let's find out the difference in their importance regarding the two models:","execution_count":null},{"metadata":{"trusted":false,"_uuid":"5d35926281ee98401dadd1b6aaadb9c71f5965c8"},"cell_type":"code","source":"common_imp_feat = [x for x in xgb_feature_importances.index \n                   if x in rf_feature_importances.index]\ncommImpFeat_xgb_scores = [xgb_feature_importances[x] \n                          for x in common_imp_feat]\ncommImpFeat_rf_scores = [rf_feature_importances[x] \n                         for x in common_imp_feat]\n\nind = np.arange(len(commImpFeat_xgb_scores))\nwidth = 0.35\n\nfig, ax = plt.subplots()\nax.bar(ind - width/2, commImpFeat_xgb_scores, width,\n       color='#003f5c', label='XGBoost');\nax.bar(ind + width/2, commImpFeat_rf_scores, width, \n       color='#ffa600', label='Random Forest')\nax.set_xticks(ind);\nax.set_xticklabels(common_imp_feat);\nax.legend();\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"81a30806d63932b4ec01ef201e73a461e100f9ca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}