{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Before everything\nthe task 1 is to find the top 10 teams, which i think is a prediction problem, so first i will just do a small test without cross validation, so i will just use cbb15-17 as training set and cbb18 as testing set, as i noticed that cbb19 has 2 more columns which could be 2 more teams."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Review of normal procedure\n1. understand all features and categorize them into numerical values and categoritical values\n2. feature engineering: analyse features by visualizing them and drop the most unrelevant features and generate new representative features based on old features\n3. feature engineering: 7C methods to pre-perpare datasets(cleanse, convert, clear...)\n4. create basemodel to make first-level prediction\n5. ensemble and stack: use the output of first-level prediction to make second-level prediction\n6. score the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries \nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\n\n#ignore warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n#use xgb for the second classifier\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read dataset\ncbb = pd.read_csv('../input/college-basketball-dataset/cbb.csv')\ncbb.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get all rows without year == 2019\ncbb_ = cbb[~(cbb.YEAR == 2019)]\ncbb_.head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understand all features\njust copied it from the description\n\n\nTEAM: Name of all schools participated into the contest 学校的名字\n**looks like irrelevant from the first sight but\n1. maybe the school names also indicate the location and has some correlation with CONF\n**\n\nCONF: The Athletic Conference in which the school participates in (A10 = Atlantic 10, ACC = Atlantic Coast Conference, AE = America East, Amer = American, ASun = ASUN, B10 = Big Ten, B12 = Big 12, BE = Big East, BSky = Big Sky, BSth = Big South, BW = Big West, CAA = Colonial Athletic Association, CUSA = Conference USA, Horz = Horizon League, Ivy = Ivy League, MAAC = Metro Atlantic Athletic Conference, MAC = Mid-American Conference, MEAC = Mid-Eastern Athletic Conference, MVC = Missouri Valley Conference, MWC = Mountain West, NEC = Northeast Conference, OVC = Ohio Valley Conference, P12 = Pac-12, Pat = Patriot League, SB = Sun Belt, SC = Southern Conference, SEC = South Eastern Conference, Slnd = Southland Conference, Sum = Summit League, SWAC = Southwestern Athletic Conference, WAC = Western Athletic Conference, WCC = West Coast Conference) \n**looks like this CONF is geographically related and symbolize which sector does which school plays in**\n\nG: Number of games played\n**the more games played, the more skilled**\n\nW: Number of games won\n**the more games won, the more energetic players are and higher the spirit**\n\nADJOE: Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions) a team would have against the average Division I defense)\n**seems to me its 进攻效率，每100次控球进攻得分，对于平均defense 来说\nso the more points scored, the more likely the team to be categorized as a offensive team, which means they play more aggresive\n**\n\nADJDE: Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions) a team would have against the average Division I offense)\n**seems to me its 防守效率，每100次(对方)控球我们被得的分，对于平均offense 来说\n(here im not sure if points allowed means the points scored by offense or the points we managed to prevent, let me assue its the first situation) so the higher ADJDE, the worse we play defense\nso the more points scored, the more likely the team to be categorized as a defensive team, which means they play more defensive\n**\n\nBARTHAG: Power Rating (Chance of beating an average Division I team)\n**general score of power**\n\nEFG_O: Effective Field Goal Percentage Shot\n**有效场均命中率**\n\nEFG_D: Effective Field Goal Percentage Allowed\n**对方有效场均命中率**\n\nTOR: Turnover Percentage Allowed (Turnover Rate)\n**失误率**\n\nTORD: Turnover Percentage Committed (Steal Rate)\n**抢断率**\n\nORB: Offensive Rebound Percentage\n**场均进攻篮板**\n\nDRB: Defensive Rebound Percentage\n**场均防守篮板**\n\nFTR : Free Throw Rate (How often the given team shoots Free Throws)\n**罚球率**\n\nFTRD: Free Throw Rate Allowed\n**对方罚球率**\n\n2P_O: Two-Point Shooting Percentage\n**2分命中率**\n\n2P_D: Two-Point Shooting Percentage Allowed\n**对方2分出手率**\n\n3P_O: Three-Point Shooting Percentage\n**3分命中率**\n\n3P_D: Three-Point Shooting Percentage Allowed\n**对方3分出手率**\n\nADJ_T: Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team that wants to play at an average Division I tempo)\n**控球率**\n\nWAB: Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)\n**胜利基于现场燃爆！**\n\nPOSTSEASON: Round where the given team was eliminated or where their season ended (R68 = First Four, R64 = Round of 64, R32 = Round of 32, S16 = Sweet Sixteen, E8 = Elite Eight, F4 = Final Four, 2ND = Runner-up, Champion = Winner of the NCAA March Madness Tournament for that given year)\n**总决赛第几强**\n\nSEED: Seed in the NCAA March Madness Tournament\n**种子选手**\n\nYEAR: Season\n**赛季**"},{"metadata":{},"cell_type":"markdown","source":"After overviewing the meaning of each feature, lets take a look into details of the parameters of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,el in enumerate(cbb_):\n    print(el +'   is')\n    print(pd.unique(cbb_[el]))\n    print('-'*30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization work maybe for the version 0.2 here i will just cleanse, mapping and generate new features\n\ndecision:\n1. mapping all string elements with integer\n2. fill nan with differnt number \n3. delete 'G' and 'W' generate new feature winrate\n\n\np.s. from the visualization\n1. here i fill the nan with big numbers, later should be changed to mean of other values\n2. mapping conf may be first sorted by the win rate\n3. calculate win rate= win times/game times"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# generate winrate\ncbb_['WINRATE'] = cbb_['W']/cbb_['G']\n# generate winrate above bubble\ncbb_['WINRATE_BUBBLE'] = cbb_['WAB']/cbb_['G']\n#drop W ,G ,team name, Winsabovebubble\ncbb_.drop(columns = ['W','G','TEAM','WAB'],inplace = True)\ncbb_.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mapping POSTSEASON\npos_mapping = {'Champions':0,'2ND':1,'F4':2,'E8':3,'S16':4,'R32':5,'R64':6,'R68':7}\ncbb_['POSTSEASON'] = cbb_['POSTSEASON'].map(pos_mapping)\ncbb_['POSTSEASON'] = cbb_['POSTSEASON'].fillna(8)\n\n#mapping SEED, punish team without seed\ncbb_['SEED'] = cbb_['SEED'].fillna(30)\n\n#mapping CONF\nconf_mapping = {'WCC':0,'ACC':1 ,'B10':2 ,'SEC':3, 'B12' :4,'Amer':5, 'BE' :6,'MAC':7, 'SC':8, 'MWC':9, 'A10':10, 'P12':11,\n 'OVC': 12,'WAC':13, 'BSth':14, 'BW':15, 'AE': 16,'CAA':17, 'Ivy':18, 'Horz':19,'SB':20, 'CUSA':21, 'Pat':22, 'BSky':23,\n 'MVC': 24,'Slnd':25, 'Sum' :26,'MAAC': 27,'SWAC':28, 'NEC':29 ,'MEAC':30, 'ASun':31}\ncbb_['CONF'] = cbb_['CONF'].map(conf_mapping)\ncbb_['CONF'] = cbb_['CONF'].fillna(100)\n\n#no need to map school name cuz too big and corupt the correlation just drop it\n\n#after preparation finally to split data\ntrain_set = cbb_[~(cbb_.YEAR == 2018)]\ntest_set = cbb_[(cbb_.YEAR == 2018)]\n\n#feature selection in later version\ntrain_set.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#just for test visualizing\ntest_cbb = cbb\n\n#plot a multiplot \n\ndf = pd.DataFrame({'g':test_cbb['G'],'P':test_cbb['POSTSEASON']})\nsns.jointplot(x='g',y='P',data = df)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#see heatmap the correlation between features\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(24,22))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_set.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#just copied\n# Some useful parameters which will come in handy later on\nntrain = train_set.shape[0]#number of train sets\nntest = test_set.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits = NFOLDS, shuffle = False, random_state=None)\n\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create Out of Folder function\ndef get_oof(clf, x_train, y_train, x_test):\n    #训练集的样本数\n    oof_train = np.zeros((ntrain,))\n    #测试集的样本数\n    oof_test = np.zeros((ntest,))\n    #(5，测试集的样本数)\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    #对于大训练集分成5分，细分成为4个小训练集和1个小测试集\n    for i,(train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]#训练集x\n        y_tr = y_train[train_index]#训练集y\n        x_te = x_train[test_index]#测试集x\n\n        clf.train(x_tr, y_tr)#训练 小训练集\n\n        oof_train[test_index] = clf.predict(x_te)#对小测试集 predict\n        oof_test_skf[i, :] = clf.predict(x_test)#对大测试集 predict\n\n    oof_test[:] = oof_test_skf.mean(axis=0)#kfold 之后对于大测试集的 predict 求mean\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)#only one column and all values are in one column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create 5 objects that represent our 4 models\n#rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train_set['POSTSEASON'].ravel()#展开成1d\ntrain_set = train_set.drop(['POSTSEASON'], axis=1)\nx_train = train_set.values # Creates an array of the train data\n\ny_test = test_set['POSTSEASON'].ravel()\ntest_set = test_set.drop(['POSTSEASON'], axis=1)\nx_test = test_set.values # Creats an array of the test data\n\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n#rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n#svc_feature = svc.feature_importances(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.concatenate(( et_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(y_test)\nscore = metrics.accuracy_score(y_test, predictions)\nprint(f\"Test score: {score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create output csv\n\"\"\"\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\"\"\"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}