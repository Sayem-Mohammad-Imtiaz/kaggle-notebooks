{"cells":[{"metadata":{"_uuid":"2228abcaed691cd62392df24e658319954a7949e"},"cell_type":"markdown","source":"# Sonar Mines vs Rocks with explanations"},{"metadata":{"_uuid":"0197b4db10fad756283d46ac06ddb4d29a41c70f"},"cell_type":"markdown","source":"### Contents \n\n1. Looking at the data\n - Loading the data and checking some basic statistics \n - Checking correlation between the variables\n - Checking the skewness of the data\n - Checking the outliers\n2. Feature Selection\n - Using Lars\n - Using sklearn's FeatureSelection\n3. Analysing the preformance \n - Analysing six different models\n - Tuning the best model\n4. Results"},{"metadata":{"trusted":true,"_uuid":"b0f6e3f0440a331a27699dbdc59f9bddaa5c91b8"},"cell_type":"code","source":"import csv\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import  LogisticRegression, lars_path\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,power_transform\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectPercentile \nimport warnings\nwarnings.filterwarnings(\"ignore\",category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning) \nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.plotting import gmap\nfrom bokeh.models import WMTSTileSource, LinearColorMapper, LogColorMapper,ColumnDataSource, HoverTool, CustomJS, Slider, ColorBar, FixedTicker\nfrom bokeh.transform import linear_cmap, factor_cmap\nfrom bokeh.layouts import row, column\nfrom matplotlib import colors as mcolors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7bf98aa7eb246ffdf53b8a1eff0d62ced0f5442"},"cell_type":"markdown","source":"## 1. Looking at the data \n### Loading the data and checking some basic statistics "},{"metadata":{"trusted":true,"_uuid":"3c348df608cb390257896d6db20ff57b30e534e5"},"cell_type":"code","source":"data = pd.read_csv(\"../input/sonar.all-data.csv\",header = None,prefix='V')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6ec16f063f99b85d1537875002edc16e463f682"},"cell_type":"markdown","source":"The data is chirped signal (i.e. signal of increasing frequency) taken in 60 different times (Variables V0-V59) and the target variable (V60) is either a Rock (R) or a mine (M)"},{"metadata":{"trusted":true,"_uuid":"d209ea2e4e466de9a861291de9b5533ab66fea94"},"cell_type":"code","source":"print (\"Data size is: {}\".format(data.shape))\nprint (\"Variable types: \\n{}\".format(data.dtypes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d1e170dce3bb5a2ef833194bab21069ee848d4"},"cell_type":"code","source":"data.describe()  # does not 'describe' the last qualitative column","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9359850ce1950d3ece6796531c1f9d894513c98d"},"cell_type":"markdown","source":"Checking if there is some missing data "},{"metadata":{"trusted":true,"_uuid":"6927d73fb85f8117a9474ab3b67364bf4dc02641"},"cell_type":"code","source":"data[data.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9613e7a0a4df9c8ef0524ba9a949e755731f7b8c"},"cell_type":"markdown","source":"Checking the distribution of the target variable"},{"metadata":{"trusted":true,"_uuid":"f2157c8d06f066ca0d1721c06989b4f33ba2d02e"},"cell_type":"code","source":"data['V60'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83f4d5a0443fa4de2141820e6b8ec6b1f469bdc6"},"cell_type":"markdown","source":"Lets convert the M and R into 1 and 0 "},{"metadata":{"trusted":true,"_uuid":"ee3c6ab3e1557332333ff88f177c105afce34d32"},"cell_type":"code","source":"def conv(x):\n    if x == 'M':\n        return 1\n    if x == 'R':\n        return 0\n    \ndata['V60'] = data['V60'].apply(lambda x : conv(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80e4e1298e420923bc08208e7a945ca473f7e1f1"},"cell_type":"markdown","source":"### Checking correlation "},{"metadata":{"_uuid":"aef17bbd48cc00d7bfd56ccc1071a629b7227f98"},"cell_type":"markdown","source":"Bescause the way the data is collected I expect higher correlation between adjacent variables. For example, for the first 20 variables, as shown further, the correlation first decreases and then stabilises around very low values. "},{"metadata":{"trusted":true,"_uuid":"851b88fd5823ef31fdadc630885df4ac99c799c2"},"cell_type":"code","source":"num_cols = data.shape[1]\ndata_corr = data.corr(method = 'pearson') \nfig,axes = plt.subplots(figsize=(15,5))\nfor i in range (0,20):\n    plt.plot(data_corr.iloc[i:num_cols,i], label=str(i))\nplt.xticks(rotation='vertical')\nplt.legend(bbox_to_anchor=(1.1, 1.05))\nplt.axhline(y=0,color='k')\nplt.ylabel(\"Pearson correlation coefficient\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"460b0c5f39229527f0e21dff21e292f64d1dfad0"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nplt.pcolor(data_corr)\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d6e1201d4884f571a344da9a4f420484bcb3af"},"cell_type":"markdown","source":"### Checking the skewness of the data"},{"metadata":{"trusted":true,"_uuid":"ea68a01043db2c4de5489af1c6c03e50577958c5"},"cell_type":"code","source":"data.hist(xlabelsize = 0, figsize=(20,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68e5791e301894d23cb525fabbd63b25fdc4aeeb"},"cell_type":"markdown","source":"The data do not look too bad, but most variables are not noramlly distributed. I will try squre root tranformation and the Yeo-Johnson transformation"},{"metadata":{"trusted":true,"_uuid":"aca4754cecb64df9058dd0e0d2e6acd36ddcac11"},"cell_type":"code","source":"skewness = []\nskewness_sqrt = []\nskewness_yj = []\n\ndata_sqrt = data.apply(np.sqrt) \n\ntemp = power_transform(data.iloc[:,:-1],method = \"yeo-johnson\")\ndata_yj = pd.DataFrame(temp,columns=data.iloc[:,:-1].columns.tolist())\n\nfor var in data.iloc[:,:-1].columns:\n        skewness.append(data[var].skew())\nfor var in data_sqrt.iloc[:,:-1].columns:\n        skewness_sqrt.append(data_sqrt[var].skew())\nfor var in data_yj.iloc[:,:-1].columns:\n        skewness_yj.append(data_yj[var].skew())\n        \nfig,ax = plt.subplots(3,1, figsize=(30,6),sharex=True)\nbins = 208\nfontsize = 15\nax[0].hist(skewness,bins=bins)\nax[1].hist(skewness_sqrt,bins=bins)\nax[2].hist(skewness_yj,bins=bins)\nax[0].text(3,2.5,\"no transform\",fontsize = fontsize)\nax[1].text(3,2.5,\"sqrt\",fontsize = fontsize)\nax[2].text(3,2.5,\"yeo-johnson\",fontsize = fontsize)\n\nfig.subplots_adjust(hspace=0)\nfor ax in ax:\n    ax.label_outer()\n    ax.axvline(x=0,color='r')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"391d46dcf9a6209fd5f27db92393a32738fecb9d"},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,6))\nplt.plot(skewness, label = \"No transformation\")\nplt.plot(skewness_sqrt, color='r', label = \"Sqrt transformation\")\nplt.plot(skewness_yj, color='g', label = \"Yeo-Johnson transformation\")\nplt.axhline(y=0,color='k')\nplt.xlabel(\"Variable/Column\")\nplt.ylabel(\"Skewness\")\nplt.legend(loc=\"best\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ad3e6fe57c0a2254b8d3ba491654a3a5df3f91d"},"cell_type":"markdown","source":"It seems that the Yeo-Johnson transformation is the best."},{"metadata":{"trusted":true,"_uuid":"b996df6a67a91a40102342a3e22c118d82b4d969"},"cell_type":"code","source":"data_yj.hist(xlabelsize = 0, figsize=(20,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78479ce1a9ddd9bc70c82262dd8c8c38157fc98a"},"cell_type":"markdown","source":"### Checking and removing the outliers"},{"metadata":{"trusted":true,"_uuid":"85ead9f5992683fcfa6248254f84ae2f88a1929e"},"cell_type":"code","source":"data.plot(kind = 'box',figsize=(20,10))\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"125d1660a13f0299f1f90be79f9374439b1715e7"},"cell_type":"markdown","source":"Next, I will plot a graph that given a particular number of outliers shows how many observations (rows) have this amount of outliers. It is an interactive graph. "},{"metadata":{"trusted":true,"_uuid":"1fe2e06ef78555f56b061529576a445fb9e39ab5"},"cell_type":"code","source":"outlier_idx = []\nfor col in data.columns.tolist():\n    Q1 = np.percentile(data[col],25)\n    Q3 = np.percentile(data[col],75)\n    outlier = 1.5*(Q3-Q1)\n    outlier_list = data[(data[col]<Q1-outlier) | (data[col]>Q3+outlier)].index\n    outlier_idx.extend(outlier_list)\n\namount_of_rows_with_outliers=[]\nfor i in range(1,data.shape[1]-1):\n    idx_mult_outliers = set([x for x in outlier_idx if outlier_idx.count(x)>i])\n    amount_of_rows_with_outliers.append(len(idx_mult_outliers))\n    \noutput_notebook()\nTOOLS = \"pan,wheel_zoom,reset,hover,save\"\nmy_dict = dict(amount_of_rows_with_outliers = amount_of_rows_with_outliers, number_of_outliers = list(range(1,data.shape[1]-1)))\nsource2 = ColumnDataSource(my_dict)\np = figure(plot_width=600, plot_height=400, tools = TOOLS, tooltips=[(\"Number of outliers\",\"@number_of_outliers\"),(\"Amount of rows with such amount of outliers\",\"@amount_of_rows_with_outliers\")])\np.line(x=\"number_of_outliers\",y=\"amount_of_rows_with_outliers\", source = source2, line_width  = 2)\np.xaxis.axis_label = 'Number of outliers'\np.yaxis.axis_label = 'Amount of rows with outliers'\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3accfbf24a7166b69fa3180cee1f263e34ae36"},"cell_type":"markdown","source":"To put somewhere a threshold, I will remove all the rows with more than 7 outliers. There are 13 of them as can be checked above. It is sort of a compormise, because out dataset is small and removing more rows may affect the results."},{"metadata":{"trusted":true,"_uuid":"9227576592b8f994e958970ff1e0d49010bd1ae4"},"cell_type":"code","source":"rows_to_remove = set(list([x for x in outlier_idx if outlier_idx.count(x)>7]))\ndata_new = data.drop(rows_to_remove,axis=0)\nprint(\"The expected number of rows in the new dataset is 208 - 13 = {}\".format(data_new.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce50712c1d673a31781b333ab7e7176ac8313448"},"cell_type":"markdown","source":"Let check the skewness against the best transformation again after the removal of the 13 rows"},{"metadata":{"trusted":true,"_uuid":"e0bd3618eaa98ae403e796a64561a2e49d96f072"},"cell_type":"code","source":"skewness_new = []\n\nfor var in data_new.iloc[:,:-1].columns:\n        skewness_new.append(data_new[var].skew())\n\nfig,ax = plt.subplots(2,1, figsize=(20,6),sharex=True)\nbins = 208\nfontsize = 15\nax[0].hist(skewness_yj,bins=bins)\nax[1].hist(skewness_new,bins=bins)\nax[0].text(3,2.5,\"Yeo-Johnson\",fontsize = fontsize)\nax[1].text(3,2.5,\"Original with removal\",fontsize = fontsize)\n\nfig.subplots_adjust(hspace=0)\nfor ax in ax:\n    ax.label_outer()\n    ax.axvline(x=0,color='r')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7b6f9087869b883a73e507f636558398afa1e16"},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(10,6))\nplt.plot(skewness_yj, label = \"Yeo-Johnson removal\")\nplt.plot(skewness_new,color='r', label = \"Original with removal\")\nplt.xlabel(\"Column\")\nplt.ylabel(\"Skewness\")\nplt.legend(loc=\"best\")\nax.axhline(y=0,color='k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d34d57d99a7cf01c9c75888fd1a55555870dd8f6"},"cell_type":"markdown","source":"The Yeo-Johnson is still much better."},{"metadata":{"_uuid":"98107ae072baba9a237e319c87fde8e06acd642d"},"cell_type":"markdown","source":"## 2. Feature selection "},{"metadata":{"_uuid":"2ded5394aefa7097e4e9bbe2cfcdb3780cdfffdd"},"cell_type":"markdown","source":"### 2.1. LARS"},{"metadata":{"_uuid":"1313973a3bcda60b7e2e97e223bed81d5f80419f"},"cell_type":"markdown","source":"First, the data need to be normalised"},{"metadata":{"trusted":true,"_uuid":"667d252d83f7efe9d3a2cffa2a8dffe54ba97100"},"cell_type":"code","source":"transformer_data = StandardScaler().fit(data)\ndata_norm = pd.DataFrame(transformer_data.transform(data),columns = data.columns.tolist())\ndata_norm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1a59ecdd191e91b64f40c6ac27af43facbb073"},"cell_type":"code","source":"data_norm.plot(kind = 'box',figsize=(20,10))\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f619302cfbaf4f5703a35f2fac64aaa3385108f"},"cell_type":"code","source":"X = data_norm.drop('V60',axis=1).values\ny = data_norm['V60'].values\n\nalphas, _, coefs = lars_path(X, y, verbose=True)\ncoefs_df = pd.DataFrame(coefs.T, columns =  data_norm.drop('V60',axis=1).columns.tolist())\n\ncolors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\nlisty = []\nfor item in colors.keys():\n    listy.append(item)\nrandom_color = listy[0:data.shape[1]-1]\n\nlist_of_alphas = [alphas]*(data.shape[1]-1)\nlist_of_data = coefs_df.T.values.tolist()\nmy_dictionary = dict(alpha = list_of_alphas,dat = list_of_data,var_names=data.iloc[:,:-1].columns.tolist(),c=random_color)\nsource3=ColumnDataSource(my_dictionary)\n\np = figure(plot_width=800, plot_height=600, tools = TOOLS, tooltips=[(\"Variable\",\"@var_names\")])\np.multi_line(xs = 'alpha',ys ='dat',source=source3, line_width  = 2,line_color='c')\n\np.xaxis.axis_label=\"alpha\"\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8eb22f6075615193d9eb11b08911aa193cbea4"},"cell_type":"markdown","source":"By zooming in it shows that the 6 most important variables are V10, V11, V35, V44, V48 and V51"},{"metadata":{"_uuid":"2303ea1e3e7c7f5255dfa1eccd0ca8d18727924e"},"cell_type":"markdown","source":"### 2.2. Using FeatureSelection (no model required) "},{"metadata":{"trusted":true,"_uuid":"f777dae670f3bea4c2307b88d4f569bc1579b249"},"cell_type":"code","source":"select  = SelectPercentile(percentile=10)\nselect.fit(X,y)\nmask = select.get_support()\nprint (pd.DataFrame(mask))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38cf9461731e0ab39cba37a1febee4d807f6e565"},"cell_type":"markdown","source":"Here the variables are a bit different: V9, V10, V11, V44, V47, V48"},{"metadata":{"_uuid":"aed9acf49c2acea613315cf5eee44115f3c27534"},"cell_type":"markdown","source":"## 3. Performance evaluation"},{"metadata":{"_uuid":"ea40293ec1f969bf65ef63ceb71745ac1d60de18"},"cell_type":"markdown","source":"Six models are used:\n- Support Vector Classifier\n- Linear Support Vector Classifier\n- K-Neighbors Classifier\n- Linear Regression\n- Random Forest \n- Gradient Boosting Classifier\n\nInstead of splitting the data into train-test set only once, it was decided to average it on 10 random splittings for more robust perfromance. \nThere are three datasets to assess:\n- the original dataset (data)\n- the Yeo-Johnson-transformed dataset (data_yj)\n- the reduced dataset with only the chosen variables according to the previous feature selection"},{"metadata":{"trusted":true,"_uuid":"a2d36d539031100ce6e8a04414fd79415f299e33"},"cell_type":"code","source":"# The orignal dataset\nX = data.drop('V60',axis=1)\ny = data['V60']\n\nnumber_of_models = 6\n\nresult_matrix_grid = [[] for _ in range(number_of_models)] \nresult_matrix_test = [[] for _ in range(number_of_models)]\nnames=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    grid_search_pipelines=[]\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    \n    linsvc_param = {'linsvc__C': [0.001, 0.01, 0.1, 1, 10, 100], 'linsvc__penalty': [\"l1\",\"l2\"]}\n    grid_search_pipelines.append((\"LINSVC\",linsvc_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"linsvc\",LinearSVC(dual=False,max_iter=3000))])))\n    \n    knn_param = {'knn__n_neighbors':[2,5,10]}\n    grid_search_pipelines.append((\"KNN\",knn_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"knn\",KNeighborsClassifier())])))\n    \n    lr_param = {'lr__C':[0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"LR\",lr_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"lr\",LogisticRegression())])))\n    \n    rfc_param = {'rfc__max_features':['auto',None]}\n    grid_search_pipelines.append((\"RFC\",rfc_param,Pipeline([(\"rfc\",RandomForestClassifier(n_estimators=2000))])))\n    \n    gbc_param = {'gbc__loss':[\"deviance\",\"exponential\"],'gbc__max_depth':[1,3],'gbc__learning_rate':[0.01,0.1,0.2,0.3]}\n    grid_search_pipelines.append((\"GBC\",gbc_param,Pipeline([(\"gbc\",GradientBoostingClassifier(n_estimators=2000))])))\n\n    \n    i = 0\n    for name,param,model in grid_search_pipelines:\n        names.append(name)\n        grid = GridSearchCV(estimator=model,param_grid=param,cv=10,n_jobs=-1)\n        grid.fit(X_train,y_train)\n        result_matrix_grid[i].append(grid.best_score_)\n        result_matrix_test[i].append(grid.score(X_test,y_test))\n        i +=1\n\nprint (\"Elapsed time is: \", time.time()-start, \"seconds\")\n\ngrid_results = pd.DataFrame(np.array(result_matrix_grid).T,columns=names[0:number_of_models])\nprint (\"\\nGrid results:\")\nfor i in grid_results.columns:\n    print (\"{}: {:.2f} ± {:.2f}\".format(i,grid_results[i].mean(),grid_results[i].std()))\n    \ntest_results = pd.DataFrame(np.array(result_matrix_test).T,columns=names[0:number_of_models])\nprint(\"\\nTest results:\")\nfor i in test_results.columns:\n    print (\"{}: {:.2f} ± {:.2f}\".format(i,test_results[i].mean(),test_results[i].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"023d7a9209b2c29e9995b49f4348f99a66182b79"},"cell_type":"code","source":"# The yj dataset\nX = data_yj#.drop('V60',axis=1)\ny = data['V60']\n\nnumber_of_models = 6\n\nresult_matrix_grid = [[] for _ in range(number_of_models)] \nresult_matrix_test = [[] for _ in range(number_of_models)]\nnames=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    grid_search_pipelines=[]\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    \n    linsvc_param = {'linsvc__C': [0.001, 0.01, 0.1, 1, 10, 100], 'linsvc__penalty': [\"l1\",\"l2\"]}\n    grid_search_pipelines.append((\"LINSVC\",linsvc_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"linsvc\",LinearSVC(dual=False))])))\n    \n    knn_param = {'knn__n_neighbors':[5,10],'knn__algorithm':['auto','brute']}\n    grid_search_pipelines.append((\"KNN\",knn_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"knn\",KNeighborsClassifier())])))\n    \n    lr_param = {'lr__C':[0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"LR\",lr_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"lr\",LogisticRegression())])))\n    \n    rfc_param = {'rfc__max_features':['auto',None]}\n    grid_search_pipelines.append((\"RFC\",rfc_param,Pipeline([(\"rfc\",RandomForestClassifier(n_estimators=2000))])))\n    \n    gbc_param = {'gbc__loss':[\"deviance\",\"exponential\"],'gbc__max_depth':[1,3],'gbc__learning_rate':[0.01,0.1,0.2,0.3]}\n    grid_search_pipelines.append((\"GBC\",gbc_param,Pipeline([(\"gbc\",GradientBoostingClassifier(n_estimators=2000))])))\n\n    \n    i = 0\n    for name,param,model in grid_search_pipelines:\n        names.append(name)\n        grid = GridSearchCV(estimator=model,param_grid=param,cv=10,n_jobs=-1)\n        grid.fit(X_train,y_train)\n        result_matrix_grid[i].append(grid.best_score_)\n        #print(name, grid.best_score_)\n        result_matrix_test[i].append(grid.score(X_test,y_test))\n        i +=1\n\nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\ngrid_results = pd.DataFrame(np.array(result_matrix_grid).T,columns=names[0:number_of_models])\nprint (\"\\nGrid results:\")\nfor i in grid_results.columns:\n    print (\"{}: {:.2f} ± {:.2f}\".format(i,grid_results[i].mean(),grid_results[i].std()))\n    \ntest_results = pd.DataFrame(np.array(result_matrix_test).T,columns=names[0:number_of_models])\nprint(\"\\nTest results:\")\nfor i in test_results.columns:\n    print (\"{}: {:.2f} ± {:.2f}\".format(i,test_results[i].mean(),test_results[i].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50475ac6ef78735b9ca9adbad058b1334d3bcd8e"},"cell_type":"code","source":"# Reduced dataset\ndata_red = data[[\"V9\",\"V10\",\"V11\",\"V35\",\"V44\",\"V47\",\"V48\",\"V51\",\"V60\"]]\n\nX = data_red.drop('V60',axis=1)\ny = data_red['V60']\n\nnumber_of_models = 6\n\nresult_matrix_grid = [[] for _ in range(number_of_models)] \nresult_matrix_test = [[] for _ in range(number_of_models)]\nnames=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    grid_search_pipelines=[]\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    \n    linsvc_param = {'linsvc__C': [0.001, 0.01, 0.1, 1, 10, 100], 'linsvc__penalty': [\"l1\",\"l2\"]}\n    grid_search_pipelines.append((\"LINSVC\",linsvc_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"linsvc\",LinearSVC(dual=False))])))\n    \n    knn_param = {'knn__n_neighbors':[5,10],'knn__algorithm':['auto','brute']}\n    grid_search_pipelines.append((\"KNN\",knn_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"knn\",KNeighborsClassifier())])))\n    \n    lr_param = {'lr__C':[0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"LR\",lr_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"lr\",LogisticRegression())])))\n    \n    rfc_param = {'rfc__max_features':['auto',None]}\n    grid_search_pipelines.append((\"RFC\",rfc_param,Pipeline([(\"rfc\",RandomForestClassifier(n_estimators=2000))])))\n    \n    gbc_param = {'gbc__loss':[\"deviance\",\"exponential\"],'gbc__max_depth':[1,3],'gbc__learning_rate':[0.01,0.1,0.2,0.3]}\n    grid_search_pipelines.append((\"GBC\",gbc_param,Pipeline([(\"gbc\",GradientBoostingClassifier(n_estimators=2000))])))\n    \n\n        \n    i = 0\n    for name,param,model in grid_search_pipelines:\n        names.append(name)\n        grid = GridSearchCV(estimator=model,param_grid=param,cv=10,n_jobs=-1)\n        grid.fit(X_train,y_train)\n        result_matrix_grid[i].append(grid.best_score_)\n        #print(name, grid.best_score_)\n        result_matrix_test[i].append(grid.score(X_test,y_test))\n        i +=1\n\nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\ngrid_results = pd.DataFrame(np.array(result_matrix_grid).T,columns=names[0:number_of_models])\nprint (\"\\nGrid results:\")\nfor i in grid_results.columns:\n    print (\"{}: {:.2f} ± {:.2f}\".format(i,grid_results[i].mean(),grid_results[i].std()))\n    \ntest_results = pd.DataFrame(np.array(result_matrix_test).T,columns=names[0:number_of_models])\nprint(\"\\nTest results:\")\nfor i in test_results.columns:\n    print (\"{}: {:.2f} ± {:.2f}\".format(i,test_results[i].mean(),test_results[i].std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d065f0940006db5cfdc3295483b420ce65c5447"},"cell_type":"markdown","source":"It seems that the Yeo-Johnson-transformed dataset gives the best results, but the difference from the original dataset is within statistical error. Nevertheless, I will continue with the Yeo-Johnson-transformed dataset. "},{"metadata":{"_uuid":"e1760bf9abd606535ed59be188e20acad9d846c7"},"cell_type":"markdown","source":"So now it is time to tune the best two model: SVC. First, lets expand the search by adding also different kernels:"},{"metadata":{"trusted":true,"_uuid":"cbc2fee55cf8d97bcda6cf3d23e679e617eae5e7"},"cell_type":"code","source":"X = data_yj\ny = data['V60']\n\nresult_matrix_grid = [] \nresult_matrix_test = []\nbest_param=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100],'svm__kernel':['rbf','poly','sigmoid'],'svm__degree':[3,4,5]}\n    #grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    pipe = Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])\n    grid = GridSearchCV(estimator=pipe,param_grid=svm_param,cv=10,n_jobs=-1)\n    grid.fit(X_train,y_train)\n    result_matrix_grid.append(grid.best_score_)\n    result_matrix_test.append(grid.score(X_test,y_test))\n    best_param.append(grid.best_params_)\n\n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\nprint (\"\\nGrid results: {:.2f} ± {:.2f}\".format(np.array(result_matrix_grid).mean(),np.array(result_matrix_grid).std()))\nprint (\"\\nTest results: {:.2f} ± {:.2f}\".format(np.array(result_matrix_test).mean(),np.array(result_matrix_test).std()))\nprint(\"\\nBest parameters: {}\".format(best_param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b903c799514dc9f0e11abe5da473fe385d3e1b2"},"cell_type":"code","source":"pd.DataFrame.from_dict(best_param)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bbca82e70756ba74cf1cf8994081e8d051265e4"},"cell_type":"markdown","source":"Lets narrow down the parameters a bit more given the results above "},{"metadata":{"trusted":true,"_uuid":"311cc5b9e5e69518573b3b105b4cfed635509958"},"cell_type":"code","source":"X = data_yj\ny = data['V60']\n\nresult_matrix_grid = [] \nresult_matrix_test = []\nbest_param=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    \n    svm_param = {'svm__C': [3,5,10,20,30,40,50,60,70], 'svm__gamma': [0.3,0.5,1,2,3,4,5,6,7]}\n    #grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    pipe = Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])\n    grid = GridSearchCV(estimator=pipe,param_grid=svm_param,cv=10,n_jobs=-1)\n    grid.fit(X_train,y_train)\n    result_matrix_grid.append(grid.best_score_)\n    result_matrix_test.append(grid.score(X_test,y_test))\n    best_param.append(grid.best_params_)\n\n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\nprint (\"Grid results: {:.2f} ± {:.2f}\".format(np.array(result_matrix_grid).mean(),np.array(result_matrix_grid).std()))\nprint (\"Test results: {:.2f} ± {:.2f}\".format(np.array(result_matrix_test).mean(),np.array(result_matrix_test).std()))\nprint(\"Best parameters: {}\".format(best_param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26dbe4aabfefeacbeb10f3f7b8110e7fd6d7ac1b"},"cell_type":"code","source":"pd.DataFrame.from_dict(best_param)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"feda7303a29e76c6502f0a34faf5fe44f9bf7b62"},"cell_type":"markdown","source":"and one last time..."},{"metadata":{"trusted":true,"_uuid":"2db0400207d65d7f5ab55be2bdfb46b0af497a92"},"cell_type":"code","source":"X = data_yj\ny = data['V60']\n\nresult_matrix_grid = [] \nresult_matrix_test = []\nbest_param=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    \n    svm_param = {'svm__C': [2,3,4,5,6,7], 'svm__gamma': [0,2,0.3,0.4,0.5,0.6,0.7]}\n    #grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    pipe = Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])\n    grid = GridSearchCV(estimator=pipe,param_grid=svm_param,cv=10,n_jobs=-1)\n    grid.fit(X_train,y_train)\n    result_matrix_grid.append(grid.best_score_)\n    result_matrix_test.append(grid.score(X_test,y_test))\n    best_param.append(grid.best_params_)\n\n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\nprint (\"Grid results: {:.2f} ± {:.2f}\".format(np.array(result_matrix_grid).mean(),np.array(result_matrix_grid).std()))\nprint (\"Test results: {:.2f} ± {:.2f}\".format(np.array(result_matrix_test).mean(),np.array(result_matrix_test).std()))\nprint(\"Best parameters: {}\".format(best_param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14ea797e59c60900a19fac4af013a907699fe408"},"cell_type":"code","source":"pd.DataFrame.from_dict(best_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d856a39d045bc660f0b8608562b334f55dc7733"},"cell_type":"code","source":"X = data.drop('V60',axis=1)\ny = data['V60']\n\nfinal_test = []\n\nstart = time.time()\n\nfor _ in range(0,100):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    svm = SVC(C=3,gamma=0.5)\n    svm.fit(X_train,y_train)\n    final_test.append(svm.score(X_test,y_test))\n    \n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\nprint (\"\\nTest results: {:.2f} ± {:.2f}\".format(np.array(final_test).mean(),np.array(final_test).std())) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"078c91c1c73f73fa97dcebbb8615c0edeb7ad084"},"cell_type":"markdown","source":"## Result\nThe best accuracy was achived using SVC and the following parameters:\nC: 2-5\ngamma: 0.3-0.5\nand the rest are default. "},{"metadata":{"trusted":true,"_uuid":"62dfef48671be56c0d54b7b69db5c4451c0c3f1e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}