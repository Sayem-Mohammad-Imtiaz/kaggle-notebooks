{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-gpu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import LSTM, Dense, RepeatVector, Dropout, TimeDistributed\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sp500-daily-19862018/spx.csv\", parse_dates=['date'], index_col='date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(figsize=(14,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Using 95% as training data\n\n# We'll look back 30 days of historical data to learn past trend. \n# Setting shuffle to False to retain the time series\nTIMESTEPS = 30            \n\ntrain_data, test_data = train_test_split(df, train_size=0.95, shuffle=False)\ntrain_data.sort_index(inplace=True)\ntest_data.sort_index(inplace=True)\ntrain_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getScaledData(method='standard', train_df=None, test_df=None, feature_col='feature'):\n    if method == 'standard':\n        scaler = StandardScaler()\n    else:\n        scaler = MinMaxScaler()\n    scaler = scaler.fit(train_df[[feature_col]])\n    train_df['scaled_'+feature_col] = scaler.transform(train_df[[feature_col]])\n    test_df['scaled_'+feature_col] = scaler.transform(test_df[[feature_col]])\n    return train_df, test_df, scaler\n    \ndef createDataset(df, lookback=30, feature_col=None):\n    data_x, data_y = [], []\n    for i in range(lookback, len(df)):\n        data_x.append(df.iloc[i-lookback:i][[feature_col]].values)\n        data_y.append(df.iloc[i][feature_col])\n    data_x = np.array(data_x)\n    data_y = np.array(data_y)\n    return data_x, data_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will fit a separate scaler for training (& validation set) and test set.\nWe are assuming that the data used in training is normal with no anomalies and hence will fit a scaler from training dataset and will extract a subset of data as validation. Since validation data is also normal, this will be used for validation during training process."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df, scaler = getScaledData('standard', train_data, test_data, 'close')\ntrain_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['scaled_close'].plot(figsize=(14,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, train_y = createDataset(train_df, TIMESTEPS, 'scaled_close')\ntest_x, test_y = createDataset(test_df, TIMESTEPS, 'scaled_close')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape, train_y.shape, test_x.shape, test_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LSTM autoencoder will get train_x as input and will return an output with the same shape that will be compared with this input."},{"metadata":{},"cell_type":"markdown","source":"1. ### Model configuration & training \n\nEach LSTM unit cell has an internal state called as cell state and an output called as hidden state.\nWe set return sequences to true to return hidden state for each timestep. This is set to true when \nstacking multiple LSTM layers where each LSTM layer receives a 3-dimensional input sequence or while returning a sequence of outputs.\nWe'll be using Timedistributed Layer to wrap output of dense layer for every timestep to return an output sequence.\n\nWe use RepeatVector to repeat our vector output returned by last layer in encoder LSTM. This vector is repeated TIMESTEPS time since the 1st layer in the decoder - decoder_lstm requires a 3-D input compressed sequence.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nLSTM_units = 64\nmodel = keras.Sequential()\nmodel.add(LSTM(LSTM_units, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=False,name='encoder_lstm'\n              ))\nmodel.add(Dropout(0.2, name='encoder_dropout'))\nmodel.add(RepeatVector(train_x.shape[1], name='decoder_repeater'))\nmodel.add(LSTM(LSTM_units, return_sequences=True, name='decoder_lstm'))\nmodel.add(Dropout(rate=0.2, name='decoder_dropout'))\nmodel.add(TimeDistributed(Dense(train_x.shape[2],name='decoder_dense_output')))\n\nmodel.compile(loss='mae', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time history = model.fit(train_x, train_x, epochs=10, batch_size=32, validation_split=0.1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='training_loss')\nplt.plot(history.history['val_loss'], label='validation_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding Anomalies\n\nPlotting the distribution of error for train set to set a threshold for reconstruction error beyond which the input record will be labelled as anomaly."},{"metadata":{"trusted":true},"cell_type":"code","source":"reconstructed = model.predict(train_x)\nreconstructed.shape, train_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reconstruction error - MAE for each sample\n\nmae_loss = np.mean(np.abs(reconstructed - train_x), axis=1)\nmae_loss.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(mae_loss[:,0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting a threshold to label anomalies"},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.65","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_reconstruction = model.predict(test_x)\ntest_reconstruction.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAE for reconstruction on test data\ntest_mae_loss = np.mean(np.abs(test_x - test_reconstruction), axis=1)\ntest_mae_loss.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observing the anomalies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting index after N timesteps from past in test_df\nanomaly_results_df = test_df[TIMESTEPS:][['close', 'scaled_close']].copy()\nanomaly_results_df.index = test_df[TIMESTEPS:].index\n\n# Including reconstructed predictions\nanomaly_results_df['deviation'] = test_mae_loss\nanomaly_results_df['threshold'] = THRESHOLD\nanomaly_results_df['anomaly'] = anomaly_results_df['deviation'].apply(lambda dev: 1 if dev > THRESHOLD else 0)\n\n\nanomalies = anomaly_results_df[anomaly_results_df['anomaly'] == 1]\nanomalies.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomaly_results_df['anomaly'].plot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomaly_results_df[['deviation', 'threshold']].plot(figsize=(14, 6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anomaly_results_df[['close']].plot(figsize=(14, 6))\nsns.scatterplot(anomalies.index, anomalies['close'],label='anomaly',color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### References\n\n* https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n* https://www.curiousily.com/posts/anomaly-detection-in-time-series-with-lstms-using-keras-in-python/\n* https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}