{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"boston=pd.read_csv('../input/boston-housing/boston_housing.csv')\nboston.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show missing variable\nmsno.matrix(boston);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Descriptive Statistics of Each Features***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def MissingUniqueStatistics(df):\n    variable_name_list = []\n    total_entry_list = []\n    data_type_list = []\n    unique_values_list = []\n    number_of_unique_values_list = []\n    missing_value_number_list = []\n    missing_value_ratio_list = []\n    mean_list=[]\n    std_list=[]\n    min_list=[]\n    Q1_list=[]\n    Q2_list=[]\n    Q3_list=[]\n    max_list=[]\n\n    df_statistics = boston.describe().copy()\n\n    for col in boston.columns:\n        variable_name_list.append(col)\n        total_entry_list.append(boston.loc[:,col].shape[0])\n        data_type_list.append(boston.loc[:,col].dtype)\n        unique_values_list.append(list(boston.loc[:,col].unique()))\n        number_of_unique_values_list.append(len(list(boston.loc[:,col].unique())))\n        missing_value_number_list.append(boston.loc[:,col].isna().sum())\n        missing_value_ratio_list.append(round((boston.loc[:,col].isna().sum()/boston.loc[:,col].shape[0]),4))\n        \n        try:\n            mean_list.append(df_statistics.loc[:,col][1])\n            std_list.append(df_statistics.loc[:,col][2])\n            min_list.append(df_statistics.loc[:,col][3])\n            Q1_list.append(df_statistics.loc[:,col][4])\n            Q2_list.append(df_statistics.loc[:,col][5])\n            Q3_list.append(df_statistics.loc[:,col][6])\n            max_list.append(df_statistics.loc[:,col][7])\n    \n        except:\n            mean_list.append('NaN')\n            std_list.append('NaN')\n            min_list.append('NaN')\n            Q1_list.append('NaN')\n            Q2_list.append('NaN')\n            Q3_list.append('NaN')\n            max_list.append('NaN')\n\n\n    data_info_df = pd.DataFrame({'Variable': variable_name_list, \n                               '#_Total_Entry':total_entry_list,\n                               '#_Missing_Value': missing_value_number_list,\n                               '%_Missing_Value':missing_value_ratio_list,\n                               'Data_Type': data_type_list, \n                               'Unique_Values': unique_values_list,\n                               '#_Unique_Values':number_of_unique_values_list,\n                               'Mean':mean_list,\n                               'STD':std_list,\n                               'Min':min_list,\n                               'Q1':Q1_list,\n                               'Q2':Q2_list,\n                               'Q3':Q3_list,\n                               'Max':max_list\n                               })\n\n    data_info_df = data_info_df.set_index(\"Variable\", inplace=False)\n\n    \n    return data_info_df.sort_values(by='%_Missing_Value', ascending=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_info = MissingUniqueStatistics(boston)\ndata_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target Value Distribution\nplt.subplots(figsize=(12, 9))\nsns.distplot(boston['medv'], fit = stats.norm)\n\n(mu, sigma) = stats.norm.fit(boston['medv'])\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma = $ {: .2f})'.format(mu, sigma)], loc = 'best')\nplt.ylabel('Frekans')\n\n#Probability Plot\nfig = plt.figure()\nstats.probplot(boston['medv'], plot = plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#High Correlation between features\ncorr_matrix = boston.corr().abs()\nhigh_corr_var=np.where(corr_matrix>0.8)\nhigh_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\nhigh_corr_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#High Correlation with Dependent Value\ncorr = boston.corr().abs()\nk = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'medv')['medv'].index\ncm = np.corrcoef(boston[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(10,10))       \nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values,cmap='RdYlGn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#variables that are highly correlated with each other except the dependent variable\ncorrelated_features = set()\ncorrelation_matrix = boston.loc[:, boston.columns != 'medv'].corr()\n\nfor i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n            \ncorrelated_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with dependent variable\ncor_target = abs(boston.corr()[\"medv\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.7]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will drop the variable 'RAD' according to the above situations. ('TAX' affects the target variable more than the 'RAD' variable.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(boston,palette='coolwarm',height=1.5,corner=True,plot_kws=dict(marker=\"+\", linewidth=1),diag_kws=dict(fill=False));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp = sns.pairplot(data=boston,\n                  y_vars=['medv'],\n                  x_vars=['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat'],\n                  plot_kws=dict(marker=\"D\", linewidth=1))\npp.fig.set_size_inches(20,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, there is a strong relationship between explanatory variables. Multiple linear linkage can be reduced by standardizing the data."},{"metadata":{},"cell_type":"markdown","source":"# ***Detecting Multicollinearity with VIF***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom statsmodels.tools.tools import add_constant\n\n\n# the independent variables set \nX = boston.iloc[:,:-1]\nX = add_constant(X)\n# VIF dataframe \nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = X.columns \n  \n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n                          for i in range(len(X.columns))] \n  \nprint(vif_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no variable with VIF value greater than 10. But 'rad' and 'tax' s VIF value >5."},{"metadata":{},"cell_type":"markdown","source":"We can use ridge regression or principal components to solve the multicollinearity problem. Since the values of multicollinearity are low, there is no need to subtract variables."},{"metadata":{},"cell_type":"markdown","source":"# ***Outlier Plotting***"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Box Plot Each Numeric Features in Data\nfor col in boston.columns:\n    sns.boxplot(data = [boston[col]], linewidth = 1, width = 0.5) \n    plt.ylabel(col)\n    plt.title(\"IQR\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX=boston.iloc[:,:-1]\ny=boston.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Variable Definitions and OLS Regression Results***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom scipy import stats\n\nX2 = sm.add_constant(X)\nest = sm.OLS(y, X2)\nest2 = est.fit()\nprint(est2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we put it into the basic multiple regression model without any transformation etc.;\n* our adjusted R^2 is 0.73. (The regression result of the given model shows that 73% of the change in the medv rate is explained together by these explanatory variables.)\n* F statistic is 108.1 \n* indus and age features p_value is >0.05\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Partial Regression Plots\nfig = sm.graphics.plot_partregress_grid(est2)\nfig.set_size_inches(15.5, 18.5)\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Multiple Linear Regression***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\n\nregressor.fit(X_train,y_train)\ny_pred= regressor.predict(X_test)\n\nprint(y_pred[0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('coefficients of all features (ß1,ß2,...): ' + str(regressor.coef_))\nprint('intercept of model (ß0): ' + str(regressor.intercept_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\ntest_set_rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\ntest_set_r2 = r2_score(y_test, y_pred)\n\nprint(test_set_rmse)\nprint(test_set_r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_squared = 0.71\nplt.scatter(y_test,y_pred)\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\n\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_pred, 1))(np.unique(y_test)))\n\nplt.text(7,0.5, 'R-squared = %0.2f' % r_squared)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see here, there are situations that break normality in data (such as Influence or leverage points etc.)."},{"metadata":{},"cell_type":"markdown","source":"# *Generate Prediction Intervals*"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression()\nfit_model = model.fit(X_train, y_train)\npredictions = fit_model.predict(X_test)\n\ndef get_prediction_interval(prediction, y_test, test_predictions, pi=.95):    \n#get standard deviation of y_test\n    sum_errs = np.sum((y_test - test_predictions)**2)\n    stdev = np.sqrt(1 / (len(y_test) - 2) * sum_errs)\n#get interval from standard deviation\n    one_minus_pi = 1 - pi\n    ppf_lookup = 1 - (one_minus_pi / 2)\n    z_score = stats.norm.ppf(ppf_lookup)\n    interval = z_score * stdev\n#generate prediction interval lower and upper bound\n    lower, upper = prediction - interval, prediction + interval\n    return lower, prediction, upper\nprint('prediction interval of first value :')\nget_prediction_interval(predictions[0], y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Residual Plotting*\nTo analyze the variance of the error of the regressor. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#1-\nresiduals = y_test-y_pred\nplt.plot(X_test,residuals, 'o', color='darkblue')\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Independent Variable\")\nplt.ylabel(\"Residual\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2-\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.linear_model import Ridge\n\nmodel = Ridge()\nvisualizer = ResidualsPlot(model)\n\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \nvisualizer.show();                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the points are randomly dispersed around the horizontal axis, a linear regression model is usually appropriate for the data; otherwise, a non-linear model is more appropriate. "},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(model, hist=False, qqplot=True)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q-Q plot which is a common way to check that residuals are normally distributed. We can see that there are outliers with the Q-Q plot."},{"metadata":{},"cell_type":"markdown","source":"# ***Influence plots***"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sm.graphics.influence_plot(est2, criterion=\"cooks\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there are a few worrisome observations. 380,418,405,410 have high leverage but a low residual. 364,368,372,371,369,370 has high residual and small leverage."},{"metadata":{},"cell_type":"markdown","source":"# *Heteroscedasticity*\n* In regression analysis, heteroscedasticity refers to the unequal scatter of residuals.\n* Heteroscedasticity is a problem because ordinary least squares (OLS) regression assumes that the residuals come from a population that has homoscedasticity (constant variance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as smf\n\n#fit regression model\nfit = smf.ols('medv ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat', data=boston).fit()\n\n#view model summary\nprint(fit.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\n\n#perform Bresuch-Pagan test\nnames = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(fit.resid, fit.model.exog)\n\nlzip(names, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"p-value is less than 0.05,we have to reject the null hypothesis.(The null hypothesis (H0): Homoscedasticity is present.).\n\nVariable transformations can be done, but we will use the standardize method to minimize variance while setting up the final model."},{"metadata":{},"cell_type":"markdown","source":"# ***Feature Selection and Modelling***"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop RAD\nboston.drop(columns=['rad'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop Outlier according to LOF\nfrom sklearn.neighbors import LocalOutlierFactor\nclf=LocalOutlierFactor(n_neighbors=20)\n\npred=clf.fit_predict(boston)\npred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 1:Normal observation\n* -1: Anomaly observation"},{"metadata":{"trusted":true},"cell_type":"code","source":"boston=boston[pred==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Min-Max Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\n\nboston[['crim', 'zn', 'indus', 'chas',\n      'nox', 'rm', 'age', 'dis', 'tax',\n       'ptratio', 'black', 'lstat']] = mms.fit_transform(boston[['crim', 'zn', 'indus', 'chas',\n                                                                           'nox', 'rm', 'age', 'dis', 'tax',\n                                                                           'ptratio', 'black', 'lstat']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"#Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nboston[['crim', 'zn', 'indus', 'chas',\n      'nox', 'rm', 'age', 'dis', 'tax',\n       'ptratio', 'black', 'lstat', 'medv']] = scaler.fit_transform(boston[['crim', 'zn', 'indus', 'chas',\n                                                                           'nox', 'rm', 'age', 'dis', 'tax',\n                                                                           'ptratio', 'black', 'lstat', 'medv']])\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boston.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=boston.iloc[:,:-1]\nY=boston[['medv']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LassoCV Feature Selection\nfrom sklearn.linear_model import LassoCV\n\nreg=LassoCV(cv=10)\nreg.fit(X,Y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,Y))\ncoef = pd.Series(reg.coef_, index = X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here Lasso model has taken all the features except CRIM. So I will drop CRIM."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(columns=['crim'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, X_test, y, y_test = train_test_split(X, Y, train_size=0.8,test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(x, y, train_size=0.75,test_size=0.25, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ntrain_errors = []\nvalid_errors = []\nparam_range = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n\nfor max_depth in param_range:\n    random_forest = RandomForestRegressor(max_depth=max_depth, n_estimators=100, random_state=1)\n    random_forest.fit(X_train, y_train)\n    \n    train_errors.append(np.sqrt(mean_squared_error(y_train, random_forest.predict(X_train))))\n    valid_errors.append(np.sqrt(mean_squared_error(y_valid, random_forest.predict(X_valid))))\n    \n\nplt.xlabel('max_depth')\nplt.ylabel('root mean_squared_error')\nplt.plot(param_range, train_errors, label=\"train rmse\")\nplt.plot(param_range, valid_errors, label=\"validation rmse\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestRegressor(max_depth=4, n_estimators=100, random_state=1)\nrandom_forest.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_mean_squared_error = np.sqrt(mean_squared_error(y_train, random_forest.predict(X_train)))\nprint(root_mean_squared_error)\n\ntrain_set_r2 = r2_score(y_train, random_forest.predict(X_train))\nprint(train_set_r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_mean_squared_error = np.sqrt(mean_squared_error(y_valid, random_forest.predict(X_valid)))\nprint(root_mean_squared_error)\n\nvalid_set_r2 = r2_score(y_valid, random_forest.predict(X_valid))\nprint(valid_set_r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"root_mean_squared_error = np.sqrt(mean_squared_error(y_test, random_forest.predict(X_test)))\nprint(root_mean_squared_error)\n\ntest_set_r2 = r2_score(y_test, random_forest.predict(X_test))\nprint(test_set_r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Regressor with CV\nfrom sklearn.model_selection import cross_val_score\ncross_val_scores = cross_val_score(RandomForestRegressor(max_depth=4, n_estimators=100, random_state=1),\\\n                                   X_test, y_test, scoring='neg_mean_squared_error', cv=5)\ncross_val_scores = np.sqrt(np.abs(cross_val_scores)) \nprint(cross_val_scores)\nprint(\"mean:\", np.mean(cross_val_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\ntrain_scores, valid_scores = validation_curve(RandomForestRegressor(n_estimators=100, random_state=1), X_train, y_train, \"max_depth\",\n                                               param_range, scoring='neg_mean_squared_error', cv=5)\ntrain_scores = np.sqrt(np.abs(train_scores))\nvalid_scores = np.sqrt(np.abs(valid_scores))\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\n\nplt.title(\"Validation Curve with Random Forest\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"RMSE\")\nplt.plot(param_range, train_scores_mean, label=\"train rmse\")\nplt.plot(param_range, valid_scores_mean, label=\"validation rmse\")\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge \nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor \nimport xgboost as xgb \nimport lightgbm as lgb\n\nmods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n  RandomForestRegressor(),BaggingRegressor(),\n  xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nfitted = [mod.fit(X_train,y_train) for mod in mods]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in fitted],\n    'Score': [i.score(X_train,y_train) for i in fitted]\n    })\n\nmodel_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n  RandomForestRegressor(),BaggingRegressor(),\n  xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nfitted = [mod.fit(X_train,y_train) for mod in mods]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in fitted],\n    'Score': [i.score(X_valid,y_valid) for i in fitted]\n    })\n\nmodel_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n  RandomForestRegressor(),BaggingRegressor(),\n  xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nfitted = [mod.fit(X_train,y_train) for mod in mods]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in fitted],\n    'Score': [i.score(X_test,y_test) for i in fitted]\n    })\n\nmodel_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(model_df['Model'], model_df['Score'], color = (0.5,0.1,0.5,0.6))\nplt.title('Performance Compare')\nplt.xlabel('Algorithms')\nplt.ylabel('Values')\nplt.ylim(0.50,0.95)\nplt.xticks(model_df['Model'],rotation='vertical');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}