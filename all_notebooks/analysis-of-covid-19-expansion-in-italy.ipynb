{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>Covid-19 in Italy</center>\n\n#### <center> 29 March 2020 </center> \n#####  <center> A.P. </center> \n\n\n\n# Introduction \nWith this exercise I tried to perform some consideration about data provided in \"covid19-in-italy\" database (only covid19_italy_region table). At first I have reported some considerations to remove redundant information. I have splitted data in a train and a test set. I have reported \"time\" and \"space\" analysis. Finally I have tried to predict \"NewPositiveCases\" with two supervised learning algorithms: Linear Regression and Random Forest. The performance measurement chosen is RMSE.\nX train matrix is build with  Test Performed and Home Confinement data. These provision, which are the strongest weapons against virus.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas.plotting import scatter_matrix\nimport sys\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_data(data_path,data_fname):\n    csv_path = os.path.join(data_path, data_fname)\n    return pd.read_csv(csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename=filenames[0]\nif (filenames[1].find('region')>0):\n    filename=filenames[1]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg = load_data(dirname,filename)\ndf_reg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Country\" column can be dropped because do not add any relevant information"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg=df_reg.drop('Country',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"SNo\" can also be dropped cause it seems to be a Serial Number, counting numbers of row and therefore redundant w.r.t. dataframe index information "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg=df_reg.drop('SNo',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.Date.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data related to time period from end of february to the end of march. It has to be converted to datetime object."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.Date=pd.to_datetime(df_reg.Date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last object type remained is \"Region Name\", which I suppose has been already coded by column \"Region Code and therefore irrelevant. Let's see:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg_conv=df_reg.loc[:,['RegionCode','RegionName']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_devstd=0\nfor reg in df_reg_conv.RegionName.value_counts().index:\n    print (reg)\n    print ('Coded as:')\n    print(str(df_reg_conv.loc[df_reg_conv.RegionName==reg].mean()[0]))\n    sum_dev_std=+df_reg_conv.loc[df_reg_conv.RegionName==reg].std()[0]\nprint ('Total Dev Std:')\nprint(str(sum_dev_std))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total sum of Standard Deviation is 0. Coding has been displayed before. Therefore \"RegionName\" column can be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg=df_reg.drop('RegionName',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is convenient to set the index to Date parameter:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown hereafter the increment of Total Positive Cases vs Time is exponential:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.TotalPositiveCases.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train Test Split and Visual Analysis of Train Test Set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set,test_set=train_test_split(df_reg,test_size=0.2, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing Geographical Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.plot(kind='scatter',x='Longitude',y='Latitude')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like Italy. Each dot is related to a Region. E.G. the dot in the middle lower part of the plot is Sicily:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.loc[df_reg_conv.RegionCode==19].loc[:,['Latitude','Longitude']].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets see "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.4,\n    s=train_set['TotalPositiveCases'], figsize=(10,13),\n    c='TestsPerformed', cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The radius of each circle represent the Total Positive Cases and the color represents the Number of Test Performed.\nThe greatest number of test performed and positive cases are concentrated in Northern Italy"},{"metadata":{},"cell_type":"markdown","source":"Looking for correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mx=train_set.iloc[:,1:].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=train_set.iloc[:,1:].columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.5)\nhm=sns.heatmap(corr_mx,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size':6},yticklabels=col,xticklabels=col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above heatmap shows uncorrelation between geographical data and medical ones. Therefore these data can be removed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mx=train_set.iloc[:,5:].corr()\ncol=train_set.iloc[:,5:].columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hm=sns.heatmap(corr_mx,cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size':9},yticklabels=col,xticklabels=col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hereafter the strongest correlations found (scc>0.9):\n1. TotalHospitalizedPatients w.r.t. IntensiveCarePatients (scc=0.99)\n2. TotalHospitalizedPatients w.r.t. CurrentPositiveCases (scc=0.99)\n2. TotalHospitalizedPatients w.r.t. TotalPositiveCases (scc=0.98)\n2. TotalHospitalizedPatients w.r.t. HomeConfinement (scc=0.93)\n2. TotalHospitalizedPatients w.r.t. Recovered (scc=0.93)\n2. TotalHospitalizedPatients w.r.t. Deaths (scc=0.95)\n3. IntensiveCarePatients w.r.t. CurrentPositiveCases (scc=0.98)\n4. IntensiveCarePatients w.r.t. HomeConfinement (scc=0.93)\n4. IntensiveCarePatients w.r.t. Deaths (scc=0.91)\n4. HomeConfinement w.r.t. Deaths (scc=0.92)\n4. HomeConfinement w.r.t. TestsPerformed (scc=0.93)\n4. Recovered w.r.t. CurrentPositiveCases (scc=0.92)\n4. Recovered w.r.t. TotalPositiveCases (scc=0.96)\n4. Recovered w.r.t. Deaths (scc=0.99)\n4. Death w.r.t. CurrentPositiveCases (scc=0.95)\n5. TotalPositiveCases w.r.t. Deaths (scc=0.98)"},{"metadata":{},"cell_type":"markdown","source":"And hereafter represented the scatter matrix of these parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"col=['TotalHospitalizedPatients','IntensiveCarePatients','CurrentPositiveCases','TotalPositiveCases','HomeConfinement','Recovered','Deaths','TestsPerformed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"axes =scatter_matrix(train_set[col],figsize=(14,10))\n\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(90)\n    ax.yaxis.label.set_rotation(0)\n    ax.yaxis.label.set_ha('right')\n\nplt.tight_layout()\nplt.gcf().subplots_adjust(wspace=0, hspace=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An interesting parameter to be predicted is the number of new positive cases.\nA Strong \nI try to build a model "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mx.NewPositiveCases.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To predict it I will use parameters with scc >0.8, with exception of Test Performed which, togheter with Home Confinement,are the strongest weapons against virus."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_m=train_set.iloc[:,4:]\ntest_set_m=test_set.iloc[:,4:]\ntrain_set_m=train_set_m.drop('Deaths',axis=1)\ntrain_set_m=train_set_m.drop('Recovered',axis=1)\ntest_set_m=test_set_m.drop('Deaths',axis=1)\ntest_set_m=test_set_m.drop('Recovered',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_set_m=train_set_m.iloc[:,5]\ny_test_set_m=test_set_m.iloc[:,5]\nX_train_set_m=train_set_m.drop('NewPositiveCases',axis=1)\nX_test_set_m=test_set_m.drop('NewPositiveCases',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_set_m.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try to predict \"NewPositiveCases\" with two supervised learning algorithms: Linear Regression and Random Forest. The performance measurement chosen is RMSE."},{"metadata":{},"cell_type":"markdown","source":"Linear Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg=LinearRegression()\nlin_reg.fit(X_train_set_m_std,y_train_set_m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_predict=lin_reg.predict(X_test_set_m)\nlin_rmse=np.sqrt(mean_squared_error(y_test_set_m,y_test_predict))\nprint(lin_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Regressor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_clf=RandomForestRegressor(n_estimators=30)\nrnd_clf.fit(X_train_set_m,y_train_set_m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_predict_rf=rnd_clf.predict(X_test_set_m)\nlin_rmse_rf=np.sqrt(mean_squared_error(y_test_set_m,y_test_predict_rf))\nprint(lin_rmse_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model is Linear Regression one (less RMSE)! Let's try to tune the Random Forest model and cross validate it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid=[{'n_estimators':[3,30,50,80,100]}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search=GridSearchCV(rnd_clf,param_grid,cv=5,scoring='neg_mean_squared_error',return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train_set_m,y_train_set_m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres=grid_search.cv_results_\nfor mean_score,params in zip(cvres[\"mean_test_score\"],cvres[\"params\"]):\n    print(np.sqrt(-mean_score),params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd_clf=RandomForestRegressor(n_estimators=80)\nrnd_clf.fit(X_train_set_m,y_train_set_m)\ny_test_predict_rf=rnd_clf.predict(X_test_set_m)\nlin_rmse_rf=np.sqrt(mean_squared_error(y_test_set_m,y_test_predict_rf))\nprint(lin_rmse_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores=cross_val_score(rnd_clf,X_train_set_m,y_train_set_m,scoring=\"neg_mean_squared_error\",cv=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_rmse=np.sqrt(-scores)\nprint(score_rmse)\nprint(score_rmse.mean())\nprint(score_rmse.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The best model is still Linear Regression one with 4.68 of rmse vs 91.82 (MEAN) of Random Forest one."},{"metadata":{},"cell_type":"markdown","source":"*Thanks for the attention. Any feedback are  welcome.*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}