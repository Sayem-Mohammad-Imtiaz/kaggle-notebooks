{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spam Classification Using SVM,Logistic regression, NN and Decision tree"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Reading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_spam=pd.read_csv('/kaggle/input/spam.csv',encoding='latin-1')\ndata_spam=data_spam.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data text Preprocess function"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef pre_process(text):    \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words += (stemmer.stem(i))+\" \"\n    return words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initiating models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression()\nsvm=SVC(kernel='rbf')\nnn=NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\ndt=tree.DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre processing Data and splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data_spam['v2']\ny=data_spam['v1'].map({'ham':0,'spam':1})\nX = X.apply(pre_process)\nvectorizer = TfidfVectorizer(\"english\")\nX = vectorizer.fit_transform(X)\nX_train,X_test,Y_train,Y_test=train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Data on all 4 models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train,Y_train)\nsvm.fit(X_train,Y_train)\nnn.fit(X_train,Y_train)\ndt.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction step"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_predicted=lr.predict(X_test)\nsvm_predicted=svm.predict(X_test)\nnn_predicted=nn.kneighbors(X_test)\ndt_predicted=dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification error metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_cls_report=classification_report(Y_test,lr_predicted)\nsvm_cls_report=classification_report(Y_test,svm_predicted)\n# nb_cls_report=classification_report(Y_test,nb_predicted)\ndt_cls_report=classification_report(Y_test,dt_predicted)\nprint(\"Logistic regression : \"+lr_cls_report)\nprint(\"SVM : \"+svm_cls_report)\n# print(\"Naive Bayes : \"+nb_cls_report)\nprint(\"Decision tree : \"+dt_cls_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The LR model has performed the best among the four algos. Ironically Naive bayes is supposed to perofrm the best as per literature. Initially I have run this on AWS sagemaker and found that NB algo requires more memory power. So, considering the computational, accuracy and memory requirements, LR performs the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}