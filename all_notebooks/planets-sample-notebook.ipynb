{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Title:\n\n### Planet: Understanding the Amazon from space -- *(a competition hosted on Kaggle)*\n#### Use satelite data to track the human footprint in the Amazon rainforest\n\n# Description:\n\nEvery minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively.\n\nPlanet, designer and builder of the worldâ€™s largest constellation of Earth-imaging satellites, will soon be collecting daily imagery of the entire land surface of the earth at 3-5 meter resolution. While considerable research has been devoted to tracking changes in forests, it typically depends on coarse-resolution imagery from Landsat (30 meter pixels) or MODIS (250 meter pixels). This limits its effectiveness in areas where small-scale deforestation or forest degradation dominate.\n\nFurthermore, these existing methods generally cannot differentiate between human causes of forest loss and natural causes. Higher resolution imagery has already been shown to be exceptionally good at this, but robust methods have not yet been developed for Planet imagery.\n\nIn this competition, Planet and its Brazilian partner SCCON are challenging Kagglers to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing useful libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom skimage import io\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd # checking current working directory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading in the training classes, it is a dataframe mapping of image name to tags\ntrain_classes_df = pd.read_csv(\"/kaggle/input/planets-dataset/planet/planet/train_classes.csv\")\nprint(train_classes_df.shape)\ntrain_classes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the color channels in a randomly selected image...say the image with image_name 'train_10.jpg'\ntrain_img10 = image.load_img(\"/kaggle/input/planets-dataset/planet/planet/train-jpg/train_10.jpg\")\ntrain_img10.mode # checking the color channels ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls \"/kaggle/input/planets-dataset/planet/planet/train-jpg/\" | wc -l # checking the total number of images in the\n                                                                     # training image folder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = !ls \"/kaggle/input/planets-dataset/planet/planet/test-jpg/\" | wc -l # checking total number of images in \n                                                                            # testing images folder\nfloat(test1[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking total number of images in the testing images additional folder\ntest_additional = !ls \"/kaggle/input/planets-dataset/test-jpg-additional/test-jpg-additional/\" | wc -l\nfloat(test_additional[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading and checking the sample submission dataframe\nsample_submission = pd.read_csv(\"/kaggle/input/planets-dataset/planet/planet/sample_submission.csv\")\nprint(sample_submission.shape)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's confirm that the sum of image files in the testing and testing-additional equals the number of images in\n# the sample submission dataframe\nassert sample_submission.shape[0] == float(test1[0]) + float(test_additional[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's find the unique tags in ```train_classes``` data\n- First, we create a function that adds elements of a list to a Set\n- Secondly, we apply this function to the ```tags``` column of ```train_classes``` after splitting its values to a list"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_labels = set()\ndef append_labels(tags):\n    for tag in tags.split():\n        unique_labels.add(tag)\n\ntrain_classes = train_classes_df.copy()\ntrain_classes['tags'].apply(append_labels)\nunique_labels = list(unique_labels) # casting 'unique_labels' as a list because set isn't an\n                                    # indexed data structure","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(unique_labels)\nlen(unique_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's vectorize (one hot encode) the ```tags``` in ```train_classes``` using ```unique_labels``` "},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's confirm that there is no image_name duplicate in the 'train_classes' dataframe\nassert len(train_classes['image_name'].unique()) == train_classes.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do one hot encoding (vectorize) the labels in 'train_classes'\nfor tag in unique_labels:\n    train_classes[tag] = train_classes['tags'].apply(lambda x: 1 if tag in x.split() else 0)\n    \n# adding '.jpg' extension to 'image_name'\ntrain_classes['image_name'] = train_classes['image_name'].apply(lambda x: '{}.jpg'.format(x)) \ntrain_classes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_classes[unique_labels].sum().sort_values().plot.bar() # an histogram of the number of tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a function that generates a concurrent matrix (a matrix that contains the number of overlaps of pairs\n# of tags)\ndef get_concurrent_matrix(tags):\n    concur_df = train_classes[tags]\n    concur_matrix = concur_df.T.dot(concur_df)\n    mask = np.triu(np.ones((len(tags), len(tags))))\n    sns.heatmap(concur_matrix, cmap=sns.cm.rocket_r, mask=mask)\n    \n    return concur_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifying the tags into the three categories of 'atmospheric condition', 'common land cover' and \n# 'rare land cover'\natm_condition_tags = ['clear', 'partly_cloudy','cloudy', 'haze']\ncommon_land_cover_tags = ['primary', 'water', 'habitation', 'agriculture', 'road', 'cultivation', 'bare_ground']\nrare_land_cover_tags = [tag for tag in unique_labels if (tag not in atm_condition_tags) and (tag not in \\\n                                                                                        common_land_cover_tags)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concurrent matrix of atmospheric condition tags\natm_tags_concurrent_matrix = get_concurrent_matrix(atm_condition_tags) \natm_tags_concurrent_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No overlap in atmospheric condition"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_concurrent_matrix(common_land_cover_tags) # concurrent matrix of common land cover tags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```'primary'``` and ```'agriculture'``` seems to have the most overlap amongst ```common_land_cover_tags```"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_concurrent_matrix(rare_land_cover_tags) # concurrent matrix of rare land cover","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fairly overlaps but ```'selective_logging'``` and ```'blooming'``` seems to have the most overlap amongst ```'rare_land_cover_tags'``` "},{"metadata":{"trusted":true},"cell_type":"code","source":"get_concurrent_matrix(unique_labels) # concurrent matrix of all tags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much overlaps amongst all tags, but ```'primary'``` and ```'clear'``` seems to have the most ovelap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check if indeed every image must have one atmospheric condition tag\ntotal_atm_tags = np.matmul(np.array(atm_tags_concurrent_matrix), (np.ones((4, 1)))).sum()\nprint(total_atm_tags)\ntotal_atm_tags == train_classes.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the above cell returned false, it seems only one image doesn't contain any atmospheric condition.\n# let's check it out\nimage_atm_tags_df = train_classes.loc[:, ['image_name']+atm_condition_tags] \nimage_without_atm_df = image_atm_tags_df.loc[image_atm_tags_df.sum(axis=1) == 0]\nimage_without_atm_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's view this image without any atmospheric condition\nimage_without_atm_name = image_without_atm_df.loc[24448, 'image_name']\nimage_without_atm = io.imread('/kaggle/input/planets-dataset/planet/planet/train-jpg/{}'.format( \\\n                                                                                    image_without_atm_name))\nplt.imshow(image_without_atm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's checkout the tags associated with this image above\ntrain_classes_df[train_classes_df['image_name'] == image_without_atm_name[:-4]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data says it is water, does it look like one? Perhaps it is a dirty water or some random noise!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's view a sample image say 'train_10.jpg' \nimage_number = 10\nsample_img = io.imread('/kaggle/input/planets-dataset/planet/planet/train-jpg/train_{}.jpg'.format(image_number))\nr, g, b = sample_img[:, :, 0], sample_img[:, :, 1], sample_img[:, :, 2]\nsample_img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.set_size_inches(12, 4)\nfor ind, (img, channel) in enumerate(((r, 'r'), (g, 'g'), (b, 'b'))):\n    a = fig.add_subplot(1, 4, ind+1)\n    a.set_title(channel)\n    plt.imshow(img)\n    \n# displaying the red, green and blue channels seperately","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(sample_img) # displaying all channels at once","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col = list(train_classes.columns[2:]) # storing the tags column names as a variable\n\n# initializing an image generator with some data augumentation\nimage_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=45, horizontal_flip=True, \\\n                                            vertical_flip=True, zoom_range=0.2)\n\n# loading images from dataframe\nX = image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='/kaggle/input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=1, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X is an iterable, It contains 317 batches, each batch contains 128 images and labels because \n#40479 / 128 is 316 remainder 31 each image is of shape (128, 128, 3), each label is of shape (17, )\n\n# let's abitrarily view an image\nx109 = X[0][0][109] # first batch, images, 109th image\ny109 = X[0][1][109] # first batch, labels, 109th label\nprint(\"each image's shape is {}\".format(x109.shape))\nprint(\"each label's shape is {}\".format(y109.shape))\nprint('we have {} batches'.format(len(X)))\nprint('each batch has {} images/labels'.format(X[0][0].shape[0]))\nprint('40479/128 is {:.2F}, so the last batch will have {} images/labels'.format(40479/128, X[316][0].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x109/255) # divided by 255 so the image can be displayed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing useful deep learning libraries\n\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a function to calculate fbeta score\n\ndef fbeta(ytrue, ypred, beta=2, threshold=0.2, epsilon=1e-7):\n    # threshold is set to 0.2 to maximize recall since f2 score is recall biased\n    # epsilon is set to 1e-7 to avoide Nan values due to zero division\n    \n    beta_squarred = float(beta)**2\n    \n    ytrue = tf.cast(ytrue, tf.float32) # casts ytrue as a float\n    # convert ypred to bool, then to float\n    ypred = tf.cast(tf.greater(tf.cast(ypred, tf.float32), tf.constant(threshold)), tf.float32) \n    \n    tp = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(3.0)), tf.float32), axis=1) \n    fp = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(1.0)), tf.float32), axis=1)\n    fn = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(2.0)), tf.float32), axis=1)\n\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    fb = (beta_squarred+1) * precision * recall / (precision*beta_squarred + recall + epsilon)\n  \n    return fb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a function to calculate multi-label accuracy \n\ndef multi_label_acc(ytrue, ypred, threshold=0.2, epsilon=1e-7):\n    # threshold is set to 0.2 to maximize recall since f2 score is recall biased\n    # epsilon is set to 1e-7 to avoide Nan values due to zero division\n    \n    ytrue = tf.cast(ytrue, tf.float32) # casts ytrue as a float\n    # convert ypred to bool, then to float\n    ypred = tf.cast(tf.greater(tf.cast(ypred, tf.float32), tf.constant(threshold)), tf.float32) \n    \n    tp = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(3.0)), tf.float32), axis=1) \n    fp = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(1.0)), tf.float32), axis=1)\n    fn = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(2.0)), tf.float32), axis=1)\n    tn = tf.reduce_sum(tf.cast(tf.equal((2.0*ytrue + ypred), tf.constant(0.0)), tf.float32), axis=1)\n    \n    acc = (tp+tn) / (tp+fp+fn+tn+epsilon)  \n    \n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a function to build a sequential model\n\ndef build_model():\n    base_model = VGG19(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(128, 128, 3)))\n    model.add(base_model)\n    model.add(Flatten())\n    model.add(Dense(17, activation='sigmoid'))\n    opt = Adam(lr=1e-4)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[multi_label_acc, fbeta])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing callbacks\n#early_stopping = EarlyStopping(monitor='val_fbeta', patience=15, mode='max', verbose=1)\nreduced_lr = ReduceLROnPlateau(monitor='val_fbeta', patience=3, cool_down=2, mode='max')\nsave_best_check_point = ModelCheckpoint(filepath='best_model3.hdf5', monitor='val_fbeta', \\\n                                        mode='max', save_best_only=True, save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing an image data generator object with a validation split of 80:20\ntrain_image_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=180, horizontal_flip=True, \\\n                                            vertical_flip=True, validation_split=0.2)\n\n# generating the 80% training image data\ntrain_gen = train_image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='/kaggle/input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=0, batch_size=128, subset='training')\n\n# generating the 20% validation image data\nval_gen = train_image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='/kaggle/input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=0, batch_size=128, subset='validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting step size for training and validation image data\nstep_train_size = int(np.ceil(train_gen.samples / train_gen.batch_size))\nstep_val_size = int(np.ceil(val_gen.samples / train_gen.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"model1 = build_model() # building a sequential model for training\n\n#model1.load_weights('../input/my-best-model2/best_model2.hdf5')\n# fitting the model\nmodel1.fit(x=train_gen, steps_per_epoch=step_train_size, validation_data=val_gen, validation_steps=step_val_size,\n         epochs=50, callbacks=[reduced_lr, save_best_check_point] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model training lasted for approximately 2hrs 30mins with a best val_fbeta score of 0.925. Early stopping was triggered after the 29th epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = build_model() # building a sequential model for testing\n\n#loading in the weights of the trained model\nmodel2.load_weights('best_model3.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding .jpg extension to 'image_name' in sample_submission data\nsample_submission['image_name'] = sample_submission['image_name'].apply(lambda x: '{}.jpg'.format(x))\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the first 40669 'image_name'(s) from the submission_sample dataframe to generate image data from \n# test.jpg folder\ntest1_df = sample_submission.iloc[:40669]['image_name'].reset_index().drop('index', axis=1)\ntest1_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing an image data generator object for the first 40669 images in the sample submission dataframe\ntest_image_gen1 = tf.keras.preprocessing.image.ImageDataGenerator()\n\n# generating the image data for the first 40669 images in the sample submission dataframe\ntest_gen1 = test_image_gen1.flow_from_dataframe(dataframe=test1_df, \\\n            directory='../input/planets-dataset/planet/planet/test-jpg/', x_col='image_name', y_col=None, \\\n            batch_size=128, shuffle=False, class_mode=None, target_size=(128, 128))\n\n# setting the step size for the testing set for the first 40669 images in the sample submission dataframe\nstep_test_size1 = int(np.ceil(test_gen1.samples / test_gen1.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_gen1.reset() # reseting the generator to be sure of avoiding shuffling\npred1 = model2.predict(test_gen1, steps=step_test_size1, verbose=1) # predicts the first 40669 images in the \n                                                                    # sample submission dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file_names1 = test_gen1.filenames # storing the filenames (images names) of the first 40669 images names in \n                                       # the sample submission dataframe as ordered in the prediction as a \n                                       # variable\n        \n# converting the predictions of the first 40669 to tag names\npred_tags1 = pd.DataFrame(pred1)\npred_tags1 = pred_tags1.apply(lambda x: ' '.join(np.array(unique_labels)[x > 0.2]), axis=1)\n\n# converting the predictions of the first 40669 to a dataframe\nresult1 = pd.DataFrame({'image_name': test_file_names1, 'tags': pred_tags1})\nresult1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the remaining 'image_name'(s) from the submission_sample dataframe to generate image data from \n# test-additional.jpg folder\ntest2_df = sample_submission.iloc[40669:]['image_name'].reset_index().drop('index', axis=1)\ntest2_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing an image data generator object for the remaining images in the sample submission dataframe\ntest_image_gen2 = tf.keras.preprocessing.image.ImageDataGenerator()\n\n# generating the image data for the remaining images in the sample submission dataframe\ntest_gen2 = test_image_gen2.flow_from_dataframe(dataframe=test2_df, \\\n            directory='../input/planets-dataset/test-jpg-additional/test-jpg-additional/', x_col='image_name', \\\n            y_col=None, batch_size=128, shuffle=False, class_mode=None, target_size=(128, 128))\n\n# setting the step size for the testing set for the remaining images in the sample submission dataframe\nstep_test_size2 = int(np.ceil(test_gen2.samples / test_gen2.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_gen2.reset() # reseting the generator to be sure of avoiding shuffling\npred2 = model2.predict(test_gen2, steps=step_test_size2, verbose=1) # predicts the remaining images in the \n                                                                    # sample submission dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file_names2 = test_gen2.filenames # storing the filenames (images names) of the remaining images names in \n                                       # the sample submission dataframe as ordered in the prediction as a \n                                       # variable\n        \n# converting the predictions of the remaining images to tag names\npred_tags2 = pd.DataFrame(pred2)\npred_tags2 = pred_tags2.apply(lambda x: ' '.join(np.array(unique_labels)[x > 0.2]), axis=1)\n\n# converting the predictions of the remaining to a dataframe\nresult2 = pd.DataFrame({'image_name': test_file_names2, 'tags': pred_tags2})\nresult2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.concat([result1, result2]) # concatenate the predictions of the test.jpg and \n                                             # test-additional.jpg into a single dataframe\n    \nfinal_result = final_result.reset_index().drop('index', axis=1) # reseting the index of the dataframe so it \n                                                                # matches that of sample submission datafarme\n\nprint(final_result.shape)\nfinal_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirming that the predicted images are ordered as in sample submission dataframe\nassert sum(sample_submission['image_name'] == final_result['image_name']) == 61191","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing the .jpg extension from 'iamge_name' column\nfinal_result['image_name'] = final_result['image_name'].apply(lambda x: x[:-4])\nfinal_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result.to_csv('sixth_submission.csv', index=False) # saving the predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}