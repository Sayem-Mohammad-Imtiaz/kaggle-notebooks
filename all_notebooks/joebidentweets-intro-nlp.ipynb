{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.corpus import stopwords\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load data\ndata = pd.read_csv('../input/joe-biden-tweets/JoeBidenTweets.csv')\ndata.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Dataset size:',data.shape)\nprint('Columns are:',data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df  = pd.DataFrame(data[['tweet']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\nfrom PIL import Image\n\ndef wordcloud(tweets):\n    tweet_All = \" \".join(review for review in tweets)\n\n    fig, ax = plt.subplots(1, 1, figsize  = (15,10))\n    mask = np.array(Image.open(\"../input/twitter/Twitter.png\"))\n    wordcloud_ALL = WordCloud(background_color=\"white\", max_words=1000,mask=mask,  mode=\"RGBA\").generate(tweet_All)\n\n    image_colors = ImageColorGenerator(mask)\n    ax.imshow(wordcloud_ALL.recolor(color_func=image_colors), interpolation=\"bilinear\")\n    ax.axis(\"off\")\n\n    # Display the generated image:\n#     ax.imshow(wordcloud_ALL, interpolation='bilinear')\n#     ax.set_title('Tweets', fontsize=30)\n#     ax.axis('off')\n\nwordcloud(df.tweet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing text \n\nstring.punctuation\n\ndef remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ndf['tweet_clean'] = df['tweet'].apply(lambda x: remove_punct(x))\ndf.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndf['tweet_clean'] = df['tweet_clean'].apply(lambda x: remove_urls(x))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud(df.tweet_clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['tweet_tokenized'] = df['tweet_clean'].apply(lambda x: tokenization(x.lower()))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(text):\n    text = [word for word in text if word not in stopwords_english]\n    return text\n    \ndf['tweet_nonstop'] = df['tweet_tokenized'].apply(lambda x: remove_stopwords(x))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = nltk.PorterStemmer()\n\ndef stemming(text):\n    text = [ps.stem(word) for word in text]\n    return text\n\ndf['tweet_stemmed'] = df['tweet_nonstop'].apply(lambda x: stemming(x))\n\nprint(df['tweet_nonstop'][2])\nprint(df['tweet_stemmed'][2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}