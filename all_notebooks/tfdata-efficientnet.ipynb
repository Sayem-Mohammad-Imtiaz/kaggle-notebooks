{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A highly efficient tf.data input pipeline with EfficientNets\n"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I am going to create a tf.data input pipeline, which is very efficient if done correctly. It not only speeds up preprocessing but alsospeeds up actual training by a bit. \nThis notebook is a reference for me. So, i have made it as explanatory and easy to revise as possible. \nSo, sit tight,relax and enjoy the ride.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import required libraries\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow_hub as hub\n#import cv2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you may note, I have included a new dataset of TFRecords. Credits to [@DimitreOliveira](https://www.kaggle.com/dimitreoliveira). The following functions are inspired by his notebbok itself.\n\nFollowing auxilliary functions are optimized using @tf.function decorator. Thus, many operations are from TensorFlow library and not native python. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Declaring constants and hyperparameters\n\nclass Hyperparameters:\n    TFRECORDS_FORMAT={'image': tf.io.FixedLenFeature([], tf.string),\n                      'image_name': tf.io.FixedLenFeature([], tf.string),\n                      'target': tf.io.FixedLenFeature([], tf.int64)}\n    BATCH_SIZE=32\n    AUTOTUNE=tf.data.experimental.AUTOTUNE\n    HEIGHT=224\n    WIDTH=224\n    WIDTH_FACTOR=0.2\n    HEIGHT_FACTOR=0.2\n    FILL_MODE='reflect'\n    TRAINING=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the TFRecords\n\nclass DataParser(Hyperparameters):\n    #All functions except tf\n    def __init__(self): #Take required hyperparameters\n        self.TFRECORDS_FORMAT=Hyperparameters.TFRECORDS_FORMAT\n        self.BATCH_SIZE=Hyperparameters.BATCH_SIZE\n        self.HEIGHT=Hyperparameters.HEIGHT\n        self.WIDTH=Hyperparameters.WIDTH\n     \n    @tf.function\n    def readTFRecs(self,dir_name): \n        #Read the TFRecords and make a Dataset iterator. \n        #Tensorflow has an object called tf.data.iterator which is created by calling iter() method\n        TFRecFiles=tf.constant(tf.io.gfile.listdir(dir_name))\n        TFRecFiles=tf.map_fn(lambda name:dir_name+'/'+name,TFRecFiles)\n        TFRecDataset=tf.data.TFRecordDataset(TFRecFiles)#.batch(self.BATCH_SIZE).prefetch(1)\n        #self.dataset_len=tf.data.experimental.cardinality(TFRecDataset).numpy()\n        Dataset = TFRecDataset.map(lambda example:tf.io.parse_example(example,self.TFRECORDS_FORMAT))\n        return Dataset\n    \n    @tf.function\n    def decode_image(self,entry):\n       return tf.image.decode_image(entry['image'],channels=3),tf.one_hot(entry['target'],5) #[batch_size,h,w,3]\n    \n    \n    #@tf.function\n    def makeDataset(self,TFRecDataset):\n        Dataset = TFRecDataset.map(lambda entry: self.decode_image(entry))\n        #Dataset = Dataset.map(lambda entry:(entry['image'],tf.one_hot(entry['target'],5)))\n        Dataset = Dataset.shuffle(4000)\n        #Dataset=Dataset.zip(TFRecDataset.map(lambda entry:entry['target']))\n        Dataset = Dataset.batch(self.BATCH_SIZE).prefetch(1)\n        return Dataset\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp=DataParser()\nTFRecDataset=dp.readTFRecs('../input/cassava-tfrecords-512x512')\nDataset=dp.makeDataset(TFRecDataset)\n#Dataset = Dataset.shuffle(600)\nvalDataset=Dataset.take(50).prefetch(dp.AUTOTUNE)\ntrainDataset=Dataset.skip(50).prefetch(dp.AUTOTUNE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Callbacks\n\nmodel_save=tf.keras.callbacks.ModelCheckpoint(\n    './inceptionresnet_{epoch:02d}.h5',\n    monitor=\"val_loss\",\n    verbose=1,\n    mode=\"auto\",\n    save_freq=\"epoch\",\n)\n\ntensorboard = tf.keras.callbacks.TensorBoard(\n  log_dir='./logs',\n  histogram_freq=1,\n)\n\ncallbacks=[model_save,tensorboard]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building model\npretrained=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n                   trainable=False)\n#pretrained.trainable=False\nmodel=tf.keras.Sequential()\nmodel.add(tf.keras.Input((512,512,3)))\n#Augmentation layers\nmodel.add(tf.keras.layers.experimental.preprocessing.Resizing(331,331))\nmodel.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255.))\n#model.add(tf.keras.layers.experimental.preprocessing.RandomFlip())\n# model.add(tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2,0.2))\n# model.add(tf.keras.layers.experimental.preprocessing.RandomRotation(0.2))\n#model.add(tf.keras.layers.experimental.preprocessing.RandomZoom(0.2,0.2))\n\n\nmodel.add(pretrained)\n\n#model.add(tf.keras.layers.GlobalMaxPool2D())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(1080,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(540,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(135,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(96,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(48,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(16,activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(5,activation='softmax'))\n\n#compile model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=[tf.keras.metrics.CategoricalAccuracy()])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(x=trainDataset,epochs=80,callbacks=callbacks,verbose=1,validation_data=valDataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_loss = history.history['loss']\nval_loss   = history.history['val_loss']\ntrain_acc  = history.history['categorical_accuracy']\nval_acc    = history.history['val_categorical_accuracy']\nxc         = range(80)\n\nplt.figure()\nplt.plot(xc, train_loss)\nplt.plot(xc, val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Augmenter(tf.keras.layers.Layer):\n    def __init__(self):\n        super(Augmenter,self).__init__()\n        self.resize=tf.keras.layers.experimental.preprocessing.Resizing(224,224)\n        self.rescale=tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)\n        #self.flip=tf.keras.layers.experimental.preprocessing.RandomFlip()\n        #self.translate=tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2,0.2)\n        self.rotate=tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n        self.zoom=tf.keras.layers.experimental.preprocessing.RandomZoom(0.2,0.2)\n    \n    def build(self,input_shape=(32,512,512,3)):\n        pass\n    \n    @tf.function\n    def call(self,inputs):\n        return self.zoom(\n               self.rotate(\n               self.rescale(\n               self.resize(inputs))))\n           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n#!unzip ngrok-stable-linux-amd64.zip\n\nimport os\nimport multiprocessing\n\n\npool = multiprocessing.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006 &\",\n                        \"./ngrok http 6006 &\"\n                        ]]\n\n! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}