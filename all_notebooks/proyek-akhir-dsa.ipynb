{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install PySastrawi\nimport numpy as np\nimport pandas as pd\n\nimport gc\ngc.enable()\ndef run_gc():\n    gc.collect() # avoid printing GC collect in output\n\nimport os\nimport multiprocessing\nprint(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Any results you write to the current directory are saved as output.\ndata = pd.read_csv('../input/rupiah-movement-prediction-using-news/dataset.csv')\nrun_gc()\nstopwords = pd.read_csv('../input/indonesian-stoplist/stopwordbahasa.csv', names=['stopword'])['stopword'].tolist()\nrun_gc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\n#create stopword removal\nfactory = StopWordRemoverFactory()\nstopword = StopWordRemover(ArrayDictionary(factory.get_stop_words() + stopwords))\nrun_gc()\n\n#create stemmer\nstem = StemmerFactory()\nstemmer = stem.create_stemmer()\nrun_gc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_processing(text):\n    clean_str = text.lower() # lowercase\n    clean_str = re.sub(r\"(?:\\@|#|https?\\://)\\S+\", \" \", clean_str) # eliminate username, url, hashtags\n    clean_str = re.sub(r'&amp;', '', clean_str) # remove &amp; as it equals &\n    clean_str = re.sub(r'[^\\w\\s]',' ', clean_str) # remove punctuation\n    clean_str = re.sub('[\\s\\n\\t\\r]+', ' ', clean_str) # remove extra space\n    clean_str = clean_str.strip() # trim\n    clean_str = \" \".join([stemmer.stem(word) for word in clean_str.split()]) # stem\n    clean_str = stopword.remove(clean_str) # remove stopwords\n    return clean_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"clean\"] = data[\"berita\"].map(pre_processing, na_action='ignore')\nrun_gc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_label(delta):\n    if not isinstance(delta, float) or delta == 0: return \"stabil\"\n    if delta < 0: return \"turun\"\n    return \"naik\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"label\"] = data[\"delta\"].map(convert_to_label)\nrun_gc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data[\"clean\"]\ny = data[\"label\"]\nrun_gc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_text(text):\n    tokens = []\n    for word in word_tokenize(text):\n        if len(word) < 2:\n            continue\n        tokens.append(word.lower())\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def doc2vec_transform(X, y, train_index, test_index):\n    model = Doc2Vec(dm=1, dm_mean=1, vector_size=300,\n                    window=10, negative=5, min_count=1,\n                    workers=multiprocessing.cpu_count(),\n                    alpha=0.065, min_alpha=0.065)\n    y_train = y[train_index].tolist()\n    train_tagged = [TaggedDocument(\n        words=tokenize_text(_d),\n        tags=[y_train[i]]\n    ) for i, _d in enumerate(X[train_index])]\n    model.build_vocab(train_tagged)\n    run_gc()\n    \n    y_test = y[test_index].tolist()\n    test_tagged = [TaggedDocument(\n        words=tokenize_text(_d),\n        tags=[y_test[i]]\n    ) for i, _d in enumerate(X[test_index])]\n    run_gc()\n    \n    return model, train_tagged, test_tagged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vec_for_learning(model, tagged_docs):\n    targets, classifiers = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in tagged_docs])\n    return np.array(targets), np.array(classifiers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.neural_network import MLPClassifier\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport statistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_classification(model, X_train, y_train, X_test, y_test):\n    print(model)\n    model.fit(X_train, y_train)\n    \n    result = model.predict(X_test)\n    print(\"Result:\", result)\n    run_gc()\n    \n    CM = confusion_matrix(y_test, result)\n    score = model.score(X_test, y_test)\n    print(\"Test Result:\")\n    print(\"Confusion Matrix:\")\n    print(CM)\n    print(\"Score:\", score)\n    test = score\n    run_gc()\n    \n    self_res = model.predict(X_train)\n    print(\"Self-Predict Result:\", self_res)\n    run_gc()\n    \n    CM = confusion_matrix(y_train, self_res)\n    score = model.score(X_train, y_train)\n    print(\"Self-Predict Test Result:\")\n    print(\"Confusion Matrix:\")\n    print(CM)\n    print(\"Score:\", score)\n    self_test = score\n    run_gc()\n    \n    return (test, self_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n \n# Function to create model, required for KerasClassifier\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(1000, input_dim=300, kernel_initializer='normal', activation='relu')) # batch size 1000, vector size 300\n    model.add(Dense(3, kernel_initializer='normal', activation='softmax')) # 3 classes: naik, turun, stabil\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=5)\nclassifiers = [{\"model\": x, \"self_tests\": [], \"tests\": []} for x in [\n    KerasClassifier(build_fn=create_model, epochs=500, batch_size=1000, verbose=0),\n    MLPClassifier(max_iter=1000, batch_size=1000, learning_rate='adaptive')\n]]\n\nfor train_index, test_index in tscv.split(X):\n    print(\"=\"*20)\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    d2v_model, train_tagged, test_tagged = doc2vec_transform(X, y, train_index, test_index)\n    run_gc()\n    y_train, X_train = vec_for_learning(d2v_model, train_tagged)\n    y_test, X_test = vec_for_learning(d2v_model, test_tagged)\n    run_gc()\n    \n    for classifier in classifiers:\n        print(\"-\"*20)\n        test, self_test = train_classification(classifier[\"model\"], X_train, y_train, X_test, y_test)\n        classifier[\"self_tests\"].append(self_test)\n        classifier[\"tests\"].append(test)\n        run_gc()\n    \nprint(\"=\"*20)\nprint(\"Final Result\")\nfor classifier in classifiers:\n    print(\"-\"*20)\n    print(classifier[\"model\"])\n    print(\"Self-Test Average Score:\", statistics.mean(classifier[\"self_tests\"]))\n    print(\"Test Average Score:\", statistics.mean(classifier[\"tests\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}