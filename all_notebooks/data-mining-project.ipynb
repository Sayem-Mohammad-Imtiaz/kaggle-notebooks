{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n!pip install apyori\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom apyori import apriori\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"csvdf=pd.read_csv(\"/kaggle/input/hotel-booking-demand/hotel_bookings.csv\")\ncsvdf = csvdf.drop(['company','agent'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvdf.corr()['is_canceled'][:-1].sort_values().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nf = csvdf.select_dtypes(exclude=['int','float'])\nnf = nf.dropna(subset=['country'])\nnf.columns = range(nf.shape[1])\nprint(nf.isna().sum())\ntransactions = []\nfor i in range(0,len(nf)):\n    transactions.append([str(nf.values[i,j]) for j in range(0,11) if str(nf.values[i,j])!='0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions[0]\nrules = apriori(transactions,min_support=0.1,min_confidance=0.3,min_lift=3,min_length=2)\nResults = list(rules)\ndf_results = pd.DataFrame(Results)\ndf_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"support = df_results.support","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_values = []\nsecond_values = []\nthird_values = []\nfourth_value = []\n\n# loop number of rows time and append 1 by 1 value in a separate list.. first and second element was frozenset which need to be converted in list..\nfor i in range(df_results.shape[0]):\n    single_list = df_results['ordered_statistics'][i][0]\n    first_values.append(list(single_list[0]))\n    second_values.append(list(single_list[1]))\n    third_values.append(single_list[2])\n    fourth_value.append(single_list[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert all four list into dataframe for further operation..\nlhs = pd.DataFrame(first_values)\nrhs= pd.DataFrame(second_values)\nconfidance=pd.DataFrame(third_values,columns=['Confidance'])\nlift=pd.DataFrame(fourth_value,columns=['lift'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concat all list together in a single dataframe\ndf_final = pd.concat([lhs,rhs,support,confidance,lift], axis=1)\ndf_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Nan in each columns\" , csvdf.isna().sum(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvdf['is_canceled'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.hist(csvdf['lead_time'].dropna(), bins=30,color = 'paleturquoise' )\n\nplt.ylabel('Count')\nplt.xlabel('Time (days)')\nplt.title(\"Lead time distribution \", fontdict=None, position= [0.48,1.05], size = 'xx-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = csvdf.corr()\nf, ax = plt.subplots(figsize=(15, 8))\ncmap = sns.diverging_palette(10, 10, as_cmap=True)\nsns.heatmap(corr, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = csvdf.corr().abs()\n\n#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\nsol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\nsol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = csvdf['arrival_date_month'].unique().tolist()\nplt.rcParams['figure.figsize'] = 15,8\n\nheight = csvdf['is_canceled'].value_counts().tolist()\nbars =  ['Not Cancel','Cancel']\ny_pos = np.arange(len(bars))\ncolor = ['lightgreen','salmon']\nplt.bar(y_pos, height , width=0.7 ,color= color)\nplt.xticks(y_pos, bars)\nplt.xticks(rotation=90)\nplt.title(\"How many booking was cancel\", fontdict=None, position= [0.48,1.05], size = 'xx-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvdf['arrival_date_month_p'] = csvdf['arrival_date_month'].map({'January':1, 'February': 2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7,'August':8, 'September':9, 'October':10, 'November':11, 'December':12})\ncsvdf = csvdf.sort_values(by=['arrival_date_month_p'])\nct = pd.crosstab(csvdf.arrival_date_month_p, csvdf.is_canceled)\nct.plot.bar(stacked=True)\nplt.legend(title='is_cancle')\nplt.title(\"How many booking was cancel per month\", fontdict=None, position= [0.48,1.05], size = 'xx-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf = csvdf.select_dtypes(exclude=['object'])\ndf[\"children\"].replace(np.nan,0,inplace=True)\nX = df.drop(['is_canceled','children'], axis = 1)\ny = df['is_canceled']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n#Create tree\ndecision_tree = DecisionTreeClassifier(criterion = 'entropy',max_depth = 4)\ndecision_tree.fit(X_train, y_train)\nprint('The accuracy of the Decision Tree classifier on test data is {:.2f}'.format(decision_tree.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.tree as tree\nfrom sklearn.externals.six import StringIO \nfrom IPython.display import Image\n\ndot_data = StringIO()\ntree.export_graphviz(decision_tree, \n out_file='tree_limited.dot', \n class_names=df['is_canceled'].map({0:'False',1:'True'}).unique().tolist(), # the target names.\n feature_names=X_train.columns.tolist(), # the feature names.\n filled=True, \n rounded=True, \n special_characters=True)\n!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\nImage(filename = 'tree_limited.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=csvdf.drop('reservation_status',axis=1)\ndf['is_canceled']=df['is_canceled'].replace([0,1],[\"no\",\"yes\"])\ncols = df.columns\nnum_cols = df._get_numeric_data().columns\ncat_cols=list(set(cols) - set(num_cols))\ndf_cat=df[cat_cols]\n\nX_cat = df_cat.drop(\"is_canceled\", axis=1)\ny_cat = df_cat[\"is_canceled\"].eq('yes').mul(1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat['country'].fillna(\"No Country\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndef labelencode(df):\n    le = LabelEncoder()\n    return df.apply(le.fit_transform)\n\ndef onehotencode(df):\n    onehot = OneHotEncoder()\n    return onehot.fit_transform(df).toarray()\n\nX_2 = labelencode(X_cat)\nonehotlabels = onehotencode(X_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_2.head().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the numerical feature columns one more time\ncols = csvdf.columns\nnum_cols = csvdf._get_numeric_data().columns\n\n#selecting numerical features\ndf_num=df[num_cols].drop('is_canceled',axis=1)\n\n#selecting target ('is_canceled' column)\ny_num=y_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num=df_num.fillna(df_num.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# Standardizing the features\ndf_num_standard = StandardScaler().fit_transform(df_num.values)\n\n#replacing the X_num dataframe with the standardized dataframe\ndf_num[:] = df_num_standard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concatenating numerically converted categorical and numerical feature arrays\nX_arr=np.concatenate((onehotlabels, df_num_standard), axis=1)\ny_arr = df['is_canceled'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_arr,y_arr,test_size=0.25,random_state=2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#adding dropout layers for improved learning\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=20,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=10,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# For a binary classification problem\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Putting early_stop in to prevent overfitting\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nmodel.fit(x=X_train, \n          y=y_train, \n          epochs=100,\n          validation_data=(X_test, y_test), verbose=1,callbacks=[early_stop]\n          )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}