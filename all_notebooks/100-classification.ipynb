{"cells":[{"metadata":{"_uuid":"459763fd2dd79d95f7bcc6cc2ef178e466941674"},"cell_type":"markdown","source":"******Is not a very difficult problem to solve...**\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56f3b25e7331d6395cf08c05e555d9e173b459ef"},"cell_type":"code","source":"train=pd.read_csv('../input/diabetes.csv') #[:4000]\ntrain.describe().T","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"def Klasseer(Mtrain,Mtest,Mlabel,klas,rank,start):\n    #data preparation\n    #print(Mtotal)\n    #Mtotal=Mtotal.fillna(-1)\n    #print(Mtotal)\n    #Mtrain=Mtotal[Mtotal[labelveld]!=-1]\n    #Mtest=Mtotal[Mtotal[labelveld]==-1]\n    #Mtest=Mtest.drop(labelveld,axis=1)\n    Mlabel=pd.DataFrame( Mlabel,columns=['label'] )  #[:len(Mtrain)]\n    #Mlabel=Mlabel.fillna(-1)  \n    labelveld='label'\n    print('shapes train',Mtrain.shape,'label',Mlabel.shape,'test',Mtest.shape)\n\n    \n    #totalA=Mtrain.append(Mtest)\n    totalA=np.concatenate((Mtrain,Mtest), axis=0)\n    predictionA=pd.DataFrame(Mlabel,columns=[labelveld])    \n    #totalA=totalA.drop(labelveld,axis=1)\n    #print(totalA.shape,predictionA.shape)\n    #print(prediction)\n    #faze 1\n    # dimmension reduction\n    from scipy.spatial.distance import cosine\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.decomposition import TruncatedSVD\n    from sklearn.preprocessing import Normalizer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, log_loss\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC, LinearSVC, NuSVC\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier,Perceptron\n\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    \n    for ira in range(rank-start,rank+1):\n        print('****20% sample test==',ira)\n        #Ulsa = lsa.fit_transform(Mtrain.values/255)  #train version\n        #print(total)\n        if ira!=0:\n            if ira<len(totalA.T):\n                print(\"lsa dimmension reduction\")                \n                svd = TruncatedSVD(ira)\n                normalizer = Normalizer(copy=False)\n                lsa = make_pipeline(svd, normalizer)\n                UlsA = lsa.fit_transform(totalA) #total version\n                explained_variance = svd.explained_variance_ratio_.sum()\n                print(\"Explained variance of the SVD step knowledge transfer: {}%\".format(\n                    int(explained_variance * 100)))            \n            else:\n                print(\"no reduction\")\n                UlsA=totalA\n        else:\n            print(\"3D-SVD dimmension reduction\")\n            u,s,vh=np.linalg.svd(totalA)\n            print(u.shape, s.shape, vh.shape)\n            UlsA=np.reshape(u, (len(totalA),28*28))            \n        #    UlsA = totalA\n        #    print(\"no LSA reduction\")\n        print('ulsa',UlsA.shape)\n\n\n        #faze2\n        #training model\n\n        #sample\n        samlen=int(len(Mlabel))\n        X_train, X_test, y_train, y_test = train_test_split(UlsA[:samlen], Mlabel[:samlen],stratify=Mlabel[:samlen], test_size=0.10)\n        print(\"test on 10% sample\")\n        \n        if klas=='Logi':\n            classifiers = [\n    #    SVC(kernel=\"rbf\", C=0.025, probability=True),  20%\n    #    NuSVC(probability=True),\n                LogisticRegression(),\n                 ]\n        if klas=='Quad':\n            classifiers = [\n                QuadraticDiscriminantAnalysis(),\n                 ]           \n        if klas=='Rand':\n            classifiers = [\n                RandomForestClassifier(84),\n                 ]               \n        if klas=='Extr':\n            classifiers = [\n                ExtraTreesClassifier(verbose=1,n_jobs=3),\n                 ]             \n        if klas=='Adab':\n            classifiers = [\n                AdaBoostClassifier(),\n                 ]            \n        if klas=='Deci':\n            classifiers = [\n                DecisionTreeClassifier(),\n                 ]\n        if klas=='Grad':\n            classifiers = [\n                GradientBoostingClassifier(),\n                 ]            \n        if klas=='KNN':\n            classifiers = [\n                KNeighborsClassifier(n_jobs=4),  \n                 ]            \n        if klas=='Line':\n            classifiers = [\n                LinearDiscriminantAnalysis(), \n                 ]  \n        if klas=='Gaus':\n            classifiers = [\n                GaussianNB(),\n                 ] \n        if klas=='Perc':\n            classifiers = [\n                Perceptron(),\n                 ]      \n        if klas=='Elas':\n            classifiers = [\n                ElasticNet(random_state=0),\n                 ]                 \n    # Logging for Visual Comparison\n        log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n        log = pd.DataFrame(columns=log_cols)\n    \n        for clf in classifiers:\n            clf.fit(X_train,y_train)\n            name = clf.__class__.__name__\n        \n            print(\"=\"*30)\n            print(name)\n            \n            #print('****Results****')\n            train_predictions = clf.predict(X_test)\n            acc = accuracy_score(y_test, train_predictions)\n            print(\"Accuracy: {:.4%}\".format(acc))\n        \n            train_predictions = clf.predict_proba(X_test)\n            ll = log_loss(y_test, train_predictions)\n            print(\"Log Loss: {}\".format(ll))\n            \n            log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n            log = log.append(log_entry)\n    \n        print(\"=\"*30)\n\n    print('*** train complete set==',UlsA[:len(Mlabel)].shape)\n     \n    clf.fit(UlsA[:len(Mlabel)],Mlabel)\n    #on complete trainset\n\n    #pr2=pd.DataFrame(clf.predict_proba(Ulsa),index=list(range(0,len(Ulsa),1)))\n\n    predictionA=pd.DataFrame(clf.predict(UlsA),columns=['pred'],index=range(0,len(UlsA)))\n    predictionA[labelveld]=Mlabel \n    print('predict',predictionA.shape)\n    predictionA.fillna(-1)\n    predictionA['diff']=0\n    predictionA['next']=Mlabel\n    #abs(prediction[labelveld]-prediction['pred\n    collist=sorted( Mlabel.label.unique() )\n\n    print(collist)\n    if klas=='Logi':\n        predictionA[collist] = pd.DataFrame(clf.predict_log_proba(UlsA))\n    if klas!='Logi':\n        print(UlsA.shape)\n        temp=pd.DataFrame(clf.predict_proba(UlsA))\n        print(temp.shape)\n        predictionA[collist]=temp\n    \n    from sklearn.metrics import classification_report, confusion_matrix\n    true_labels=predictionA[labelveld][:len(Mtrain)].values.astype('float32')\n    predicted_labels = predictionA['pred'][:len(Mtrain)].values.astype('float32')\n\n    cm = confusion_matrix(true_labels, predicted_labels,labels=collist)\n    print(classification_report(true_labels, predicted_labels))\n    print(\"Confusion matrix\")\n    print(cm)\n    \n    corr=predictionA.drop(['pred','diff'],axis=1).corr()\n    f, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(abs(corr), mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)\n    predictionA=predictionA.fillna('0')\n    #print('Prediction',prediction.head())\n    pred2=predictionA.drop(['pred',labelveld,'diff','next'],axis=1)\n    \n    print(predictionA.shape)\n\n\n    return predictionA #['next']","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"3be614bdf47a182e9e0ed06f79dbf10f97ce20a5"},"cell_type":"markdown","source":"# SOLUTION  3x 100%\n\n*  ExtraTrees 93%\n*  RandomForest 94%\n* Decisiontree 90%\n\n* Gradientboost 95%\n* Adaboost 95%\n"},{"metadata":{"trusted":true,"_uuid":"ff42e2c2ef843506fec78d84a138db7a9c1c41bb"},"cell_type":"code","source":"Xtrain=train.drop(['PatientID','Diabetic'], axis=1)\n\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Quad',8,0)\n\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Adab',8,0)\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Grad',8,0)\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Gaus',8,0)\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Extr',8,0)\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Rand',8,0)\nKlasseer(Xtrain,Xtrain,train['Diabetic'].values,'Deci',8,0)","execution_count":26,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}