{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load & Explore Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/esigning-of-loan-based-on-financial-history/financial_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check missing values\ndf.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove unwanted column\ndf_new = df.drop(columns=[\"entry_id\", \"e_signed\", \"pay_schedule\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram for every single column\nplt.figure(figsize=(15, 12))\nfor i in range(df_new.shape[1]):\n    plt.subplot(6, 3, i+1)\n    f = plt.gca()\n    f.set_title( df_new.columns.values[i])\n    bins = len(df_new.iloc[:, i].unique())\n    if bins >= 100:\n        bins = 100\n    plt.hist(df_new.iloc[:, i], bins=bins)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Correlation Histogram"},{"metadata":{},"cell_type":"markdown","source":"Correlation is very important factor. it show how your variables are connected to each other. High magnitude means high correlation. + sign means if one increases, other also increases. - sign means they are inversly proportional. that mean if one increases, other will decrease. But still its a negative relationship.\nPoint to remember:\n<ul>\n    <li>High Maginitude - High Correlation</li>\n    <li>+ sign means directly proportional.</li>\n    <li>- sign means inversely proportional</li>\n    </ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.corrwith(df.e_signed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.corrwith(df.e_signed).plot.bar(rot=60, figsize=(16, 13), title=\"Correlation with e-signed\", grid=True\n                                     , fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 12))\nsns.heatmap(df_new.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Feature engineering is a vast term. It means to do something in your feature so that we can get a smaller dimensions. It may include feature extraction, deleting a feature or creating a new feature with the combination of two or more features. So in our case we can see personal_account_m, personal_account_y are almost same one show months and other show years. So we can combine both columns in a single column called personal_account_months and drop both the columns. In this way we can reduce our dimension by one column. Not very big diffrence but still something is better than nothing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# personal_account_m & personal_account_y column can be changed to single column personal_Account_months\n\ndf[\"personal_account_month\"] = df[\"personal_account_m\"] + 12* df[\"personal_account_y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users = df[\"entry_id\"]\nresponse = df[\"e_signed\"]\ndf.drop(columns=[\"entry_id\", \"months_employed\", \"e_signed\",\"personal_account_m\", \"personal_account_y\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Categorical Features"},{"metadata":{},"cell_type":"markdown","source":"Dummy variables are for handling categorical variable. While creating dummy variable always use drop_first=True otherwise you have to drop one dummy variable column manually to avoid dummy variable trap. you can search this term on google. In short, it says never include all dummy variables, always leave one. e.g. if you have n dummy variables then you should take n-1 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.values\ny= response.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# SVM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators =10, max_features=10, random_state=0,  criterion='entropy')\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_test, y_predict)\n#sns.heatmap(cm, annot=True)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy\nclf.score(X_test, y_test) # we can see score is very bad. Lets apply gridsearchcv to fine tune our model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tuning using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams = {\n    \"n_estimators\" : [10, 100, 200 ],  \"criterion\":[\"entropy\", \"gini\"]\n}\ngs = GridSearchCV(estimator=clf, param_grid=params, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = gs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So we can see our best accuracy is 63%. Actually acocrding to our data it is a good accuracy and our case study is not that sensitive so it may compromise with some %of accuracy.\n# lets train and fit our final model with the best params.\nclf = RandomForestClassifier(n_estimators=200, criterion=\"entropy\")\nclf.fit(X_train, y_train)\ny_predict=clf.predict(X_test)\ncm = confusion_matrix(y_test, y_predict)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_report(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_n = Sequential([\n    Dense(activation=\"relu\", init=\"uniform\", input_dim=19, output_dim=10),\n    Dense(activation=\"relu\", init=\"uniform\", output_dim=10),\n    Dropout(0.5),\n    Dense(activation=\"relu\", init=\"uniform\", output_dim=10),\n    Dense(activation=\"sigmoid\", init=\"uniform\", output_dim=1)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_n.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_n.compile(optimizer=\"adam\", metrics=[\"accuracy\"], loss=\"binary_crossentropy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(np.array(y_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_n.fit((X_train), (y_train), batch_size=20, epochs=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So here we can see our deep learning network accuracy is very less. It may be because of data. As deep learning requires a large datasets. So for our case we will settle with\n# random forest classifier. You also try diffrent model to fine tune and let us know which one do you feel suits our case study.\n# Thanks... UPVOTE IF YOU LIKE THE KERNEL","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}