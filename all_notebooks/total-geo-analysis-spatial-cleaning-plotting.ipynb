{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1.Introduction"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to **prepare universal geographical data** with the use of huge spatial data set.\n\n* I clean the 11 million records data set approximating the values of elevation etc.\n* Second, I plot the created data set and check whether it is reasonable\n* I merge the data set with population information and sample on the basis of this\n\nAs a result I created big data set containing information about location (latitude, longitude), geogrphical elevation and denisty which can be used as auxiliary data set for any other analysis.\n\n**Majority of code was hidden for clarity. Click \"unhide\" to look in there !**"},{"metadata":{},"cell_type":"markdown","source":"# 2.Data preparation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mpl_toolkits.basemap import Basemap\nimport math\nfrom math import cos, asin, sqrt\nfrom numpy import nansum\nfrom numpy import nanmean\nimport netCDF4\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I define libraries. Second, the underlying data set with the list of all locations in the Earth is very big. This means ~11 million records. I decide to load take all 11 million rows. And it can be changed to other value below."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"n = 11061987\ns = 11061987\nskip = sorted(random.sample(range(n),n-s))\n        \ndf_path = \"../input/geonames-database/geonames.csv\"\ndf = pd.read_csv(df_path,index_col='geonameid',skiprows=skip)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I check the missing entries for whole the data set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nMissing_Percentage = (df.isnull().sum()).sum()/np.product(df.shape)*100\n\nprint(\"The number of missing entries: \" + str(round(Missing_Percentage,2)) + \" %\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"All_NaN = df.isnull().sum()\nRowsCount = len(df.index)\n\nprint(\"The percentage number of missing entries per variable: \", format(round(All_NaN/RowsCount * 100,5)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's list some cleaning decisions:\n* cc2 and admin codes higher than 1 are to be dropped\n* I will estimate elevation by surroudning areas\n* I drop alternatenames"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df=df.drop(['alternatenames','admin2 code','admin3 code','admin4 code','cc2'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I estimate elevation by its local neighbours. We define this function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(a))\n\nprint(\"The length is circa: \"+ format(round(distance(51,14,55,24))) + \" kilometers.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It lets us to calculate the distance between two points by use of Harversine function. Above I checked what is the distance between two points with latitutde and longitude different of Polish territory. It is how I expected around 800 kilometers difference between furthest points. The function works. I will use in the further part of analysis. For now, simple approximation will be enough.\n\nTo approximate elevation I am applying simplified net of latitude and logitude where each value is rounded to full grade (for example: 42.523432 E = ~43 E).\n\nWhat I find, round in that way is still not enough. Data for elevation is so bad, that I decide to round to even (for example: 42.523432 E = ~42 E)."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def round_up_to_even(f):\n    return math.ceil(f / 2.) * 2","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df[\"latitude_app\"] = df.apply(lambda row: round_up_to_even(row['latitude']),axis=1)\ndf[\"longitude_app\"] = df.apply(lambda row: round_up_to_even(row['longitude']),axis=1)\n\nelevation_table = df[['elevation','latitude_app','longitude_app']].groupby(['latitude_app',\n                'longitude_app']).agg({'elevation': lambda x: x.mean(skipna=True)}).sort_values(by=['latitude_app', \n                'longitude_app'], ascending=False).reset_index()\n\ndf = pd.merge(df,  elevation_table,  on =['latitude_app', 'longitude_app'],  how ='inner')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Still NAs in 'elevation': \"+ format(      round((df[['elevation_y']].isnull().sum()).sum()/np.product(df.shape[0])*100,2)     )  + \" %.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some regions (like desserts and polar areas are absolutely empty. I will apply there world elevation average - it is 840m, more than I expected."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"WorldAverageElevation = 840\ndf['elevation_y']=df['elevation_y'].fillna(WorldAverageElevation)\n\ndf=df.drop(['elevation_x'], axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is not very obvious (at least for me) what all these country shortcuts mean. But I identify them as *universal Alpha-2 code* used in whole the world. I decide to match to them to simple country names by use of translation data in standard ISO-3166, Alpha 2 digits code."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ISO = pd.read_csv('../input/alpha-country-codes/Alpha__2_and_3_country_codes.csv', sep=';')\n\nISO['Country'] = ISO.apply(lambda row: str.rstrip(row['Country']),axis=1)\n\nISO_toMerge = ISO.drop(['Alpha-3 code','Numeric'], axis=1)\nISO_toMerge=ISO_toMerge.rename(columns={\"Alpha-2 code\": \"country code\"})\ndf = pd.merge(df, ISO_toMerge,  on ='country code',  how ='inner')\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, my base spatial data is ready for use."},{"metadata":{},"cell_type":"markdown","source":"# 3.Data analysis\n\nFor the memory reasons, let's plot sample equals to 100.000 records."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_sample = df.sample(n=100000)\n\nplt.figure(1, figsize=(12,6))\nm1 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,\n             lat_ts=0,resolution='c')\n\nm1.fillcontinents(color='#191919',lake_color='#000000') \nm1.drawmapboundary(fill_color='#000000')                \nm1.drawcountries(linewidth=0.2, color=\"w\")              \n\n# Plot the data\nmxy = m1(df_sample[\"longitude\"].tolist(), df_sample[\"latitude\"].tolist())\nm1.scatter(mxy[0], mxy[1], s=3, c=\"#1292db\", lw=0, alpha=1, zorder=5)\n\nplt.title(\"Sample of 100.000 locations in the world\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, number of points more less reflect the population's world density as well. Similarly, I will zoom a bit for Europe."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"lon_min, lon_max = -10, 40\nlat_min, lat_max = 35, 65\n\nidx_europe = (df[\"longitude\"]>lon_min) &\\\n            (df[\"longitude\"]<lon_max) &\\\n            (df[\"latitude\"]>lat_min) &\\\n            (df[\"latitude\"]<lat_max)\n\ndf_europe = df[idx_europe].sample(n=100000)\n\nplt.figure(2, figsize=(12,6))\nm2 = Basemap(projection='merc',llcrnrlat=lat_min,urcrnrlat=lat_max,llcrnrlon=lon_min,\n             urcrnrlon=lon_max,lat_ts=35,resolution='c')\n\nm2.fillcontinents(color='#191919',lake_color='#000000') \nm2.drawmapboundary(fill_color='#000000')                \nm2.drawcountries(linewidth=0.2, color=\"w\")              \n\nmxy = m2(df_europe[\"longitude\"].tolist(), df_europe[\"latitude\"].tolist())\nm2.scatter(mxy[0], mxy[1], s=5, c=\"#1292db\", lw=0, alpha=0.05, zorder=5)\n\nplt.title(\"Sample of 100.000 locations in Europe\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, situation is different than expected. Countries like UK, Benelux are not dense enough. Surprisingly, Norway which has very low population received big number of points. I can see that these values do not reflect the population's density then."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Aggregated = df[['name','Country']]\nAggregated = Aggregated.groupby(['Country']).agg(['count']).sort_values([('name', 'count')], ascending=False)\nAggregated['Percentage'] = round(Aggregated[['name']] / df.shape[0],2)\nAggregated.columns = Aggregated.columns.get_level_values(0)\nAggregated.columns = [''.join(col).strip() for col in Aggregated.columns.values]\nAggregated","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"USA, China and India or even Mexico make sense being in the top. Norway is not expected. I have an idea: let's use this data set but apply sampling to the countries based on its population. In other words, I use the given points, but I sample it by density. So for example in the above table, both China and India will grow, USA and Mexico will remain high, Norway and other small countries will drop a lot.\n\nI merge the population data to our ISO data, it requires of course some corrections (due to differences between countries' naming). "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Population = pd.read_csv('../input/population-by-country-2020/population_by_country_2020.csv')\n\nPopulation = Population.rename(columns={\"Country (or dependency)\": \"Country\"})\n\nPopulation[['Country']] = Population[['Country']].replace(\"Czech Republic (Czechia)\", \"Czechia\")\nPopulation[['Country']] = Population[['Country']].replace(\"United States\", \"United States of America\")\nPopulation[['Country']] = Population[['Country']].replace(\"United Kingdom\", \"United Kingdom of Great Britain and Northern Ireland\")\nPopulation[['Country']] = Population[['Country']].replace(\"Vietnam\", \"Viet Nam\")\nPopulation[['Country']] = Population[['Country']].replace(\"Laos\", \"Lao People Democratic Republic\")\nPopulation[['Country']] = Population[['Country']].replace(\"State of Palestine\", \"Palestine\")\nPopulation[['Country']] = Population[['Country']].replace(\"North Macedonia\", \"Republic of North Macedonia\")\nPopulation[['Country']] = Population[['Country']].replace(\"Russia\", \"Russian Federation\")\nPopulation[['Country']] = Population[['Country']].replace(\"Syria\", \"Syrian Arab Republic\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I check how much population from one data set was successfully merged after corrections. Ok, 97.6% is enough for me."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Population_Merged = pd.merge(ISO_toMerge,Population,  on ='Country',  how ='inner')\n\nPopulation_Merged[['Population Perc']] = Population_Merged[['Population (2020)']]/Population_Merged[['Population (2020)']].sum()\n\nprint(   format(   round(Population_Merged[['Population (2020)']].sum()/Population[['Population (2020)']].sum() ,3) ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next steps:\n* For each country I divide its population by total one\n* I introduce sample size equals to 1000.000 and I multiply the percentage ratio. In that way every country receives the number of rows it should have\n* This ratio is applied to the main data set to assess how many points should be sampled for each country"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Sample_Size = 1000000\n\nPopulation_Merged = pd.merge(Population_Merged,Aggregated,  on ='Country',  how ='inner')\n\nPopulation_Merged[['Sample size']] = Population_Merged['Population Perc']  / Population_Merged['name']*Population_Merged['name'].sum()*Sample_Size\n\nPopulation_toMerge = Population_Merged.loc[:, Population_Merged.columns.intersection(['Country','Sample size'])]\n\ndf = pd.merge(df,Population_toMerge,  on ='Country',  how ='inner')\n\nTotal_Probability = df[['Sample size']].sum()\n\ndf[['Sample size']] = df[['Sample size']] / Total_Probability\n\nvec = df[['Sample size']]\n\ndf_sampled = df.sample(n=Sample_Size, weights='Sample size')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Aggregated = df_sampled[['name','Country']]\nAggregated = Aggregated.groupby(['Country']).agg(['count']).sort_values([('name', 'count')], ascending=False)\nAggregated['Sampled records'] = round(Aggregated[['name']] / df_sampled.shape[0],2)\nAggregated.columns = Aggregated.columns.get_level_values(0)\nAggregated.columns = [''.join(col).strip() for col in Aggregated.columns.values]\n\nPopulation_toMerge_2 = Population_Merged.loc[:, Population_Merged.columns.intersection(['Country','Population Perc'])]\n\nAggregated = pd.merge(Aggregated,Population_toMerge_2,  on ='Country',  how ='inner')\nAggregated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lat_min, lat_max = 35, 65\n\nidx_europe = (df_sampled[\"longitude\"]>lon_min) &\\\n            (df_sampled[\"longitude\"]<lon_max) &\\\n            (df_sampled[\"latitude\"]>lat_min) &\\\n            (df_sampled[\"latitude\"]<lat_max)\n\ndf_sampled_europe = df_sampled[idx_europe].sample(n=100000)\n\nplt.figure(2, figsize=(12,6))\nm2 = Basemap(projection='merc',llcrnrlat=lat_min,urcrnrlat=lat_max,llcrnrlon=lon_min,\n             urcrnrlon=lon_max,lat_ts=35,resolution='c')\n\nm2.fillcontinents(color='#191919',lake_color='#000000') \nm2.drawmapboundary(fill_color='#000000')                \nm2.drawcountries(linewidth=0.2, color=\"w\")              \n\nmxy = m2(df_sampled_europe[\"longitude\"].tolist(), df_sampled_europe[\"latitude\"].tolist())\nm2.scatter(mxy[0], mxy[1], s=5, c=\"#1292db\", lw=0, alpha=0.05, zorder=5)\n\nplt.title(\"Sample of 100.000 locations in Europe\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And this is it. The map reflects correctly the denisty thanks to the weighing vector. \n\n**Perfect stage to do further analysis!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}