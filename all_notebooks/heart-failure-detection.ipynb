{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 1. Introduction","metadata":{}},{"cell_type":"markdown","source":"![](http://www.mydr.com.au/wp-content/uploads/2019/04/heart_failure_750.jpeg)","metadata":{}},{"cell_type":"markdown","source":"![](http://)","metadata":{}},{"cell_type":"markdown","source":"Heart failure is a pathophysiological state in which cardiac output is insufficient to meet the needs of the body and lungs. The term \"congestive heart failure\" is often used, as one of the common symptoms is congestion, or build-up of fluid in a person's tissues and veins in the lungs or other parts of the body","metadata":{}},{"cell_type":"markdown","source":"in this study we want to analyse this small data and detect relation between each feature and target \nthen we visulate the important information of data \nfinnaly we build model for our data try to get better modelisation for our dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# 2. reading data and exploration ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\nprint(\"dimonsion of the data set is \",data.shape) \ndata.info()\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"data set already cleaned we don't have any missing values\")\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# 3. Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature= ['anaemia','diabetes','high_blood_pressure','sex','smoking']\nfor i in feature :\n    print(data[i].value_counts())\n    plt.figure(figsize=(6,6))\n    data[i].value_counts().plot(kind='pie',legend=True,autopct='%1.0f%%',colors=[\"g\",\"y\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('percentage of the most ages present in this stady')\nplt.figure(figsize=(16, 9))\ndata[\"age\"].value_counts().head(5).plot(kind=\"bar\",title='percentage of the most ages present in this stady',color='r')\n(data[\"age\"].value_counts()*100/len(data)).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 9))\ndata[\"age\"].describe()[1:].plot(kind=\"bar\",grid=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[['age','DEATH_EVENT']].groupby(\"age\").sum().plot(grid=True,figsize=(16, 9),title=\"number of death_events per age\",color=\"b\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we can see in this figure the highest number of death_event is on age of 60 and thier nearly \nmany people in this age retired i think this death_event is resultat of pression of thier carreer\nbut honstly i don't see relation between the deeth_event and age because in age of 47 we have 7 events and the same in the age of 80","metadata":{}},{"cell_type":"code","source":"for col in data.select_dtypes(\"float\"):\n    plt.figure()\n    sns.distplot(data[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**comment and mark for distrubtions**\n* data normalised for the three feature (age , platelates ,sercum_creatinine)","metadata":{}},{"cell_type":"code","source":"feauture = ['anaemia', 'diabetes', 'high_blood_pressure','sex', 'smoking']\nfor i in feauture :\n    plt.figure(figsize=(4,4))\n    sns.heatmap(pd.crosstab(data[\"DEATH_EVENT\"],data[i]),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**note and comment**\n* we have 83 person have an anemie but he don' died and we have 46 person have an anemie and died but with this results we can't link the death_event to the anemia perhaps it is an  fearture combined with some causes but its self alone can't provid heart faillure\n* the same way with the other feature (sex,blood_pression,diabtes....)  sometimes we detect a death_event in their presence but some times the opposites so in conclusion we can say that this variation of value of this features can detect a death_event but heart_faillure not linked directly to one feature(cause) ","metadata":{}},{"cell_type":"code","source":"data.corr().abs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.heatmap(data.corr().abs())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**mark and note**\n* bad correlation  beetween feature expect between  time and some other feature\n* so for this one reason in the next modelistaion we use all feature \n* in part of feature selection i use all of feature because we detect generally  the same correlation between each feature and the target ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# 2. machine learning part (building,training _testing  the model )","metadata":{}},{"cell_type":"markdown","source":"# fist one we build some naive models and we try to imporove it by changing thier parametrs","metadata":{}},{"cell_type":"markdown","source":"# *1/logitic regression model*","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\ntarget =['DEATH_EVENT']\nX = data[feature].values\ny = data[target].values.ravel()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state=42)\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nypredict = model.predict(X_test)\ncm = metrics.confusion_matrix(y_test, ypredict)\nprint(\"Accuracy:\",metrics.classification_report(y_test, ypredict))\nsns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 decision treeregression","metadata":{}},{"cell_type":"code","source":"model = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train,y_train)\nypredict = model.predict(X_test)\ncm = metrics.confusion_matrix(y_test, ypredict)\nprint(\"Accuracy:\",metrics.classification_report(y_test, ypredict))\nsns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# knn models","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(X_train,y_train)\nypredict = model.predict(X_test)\ncm = metrics.confusion_matrix(y_test, ypredict)\nprint(\"Accuracy:\",metrics.classification_report(y_test, ypredict))\nsns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# random forest models","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train,y_train)\nypredict = model.predict(X_test)\ncm = metrics.confusion_matrix(y_test, ypredict)\nprint(\"Accuracy:\",metrics.classification_report(y_test, ypredict))\nsns.heatmap(cm,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**model selection **\n\n1- decisiontreeclassifer","metadata":{}},{"cell_type":"code","source":"param_grid = { 'criterion': [\"gini\", \"entropy\"] ,'max_depth':[5, 10]}\ngrid = GridSearchCV(DecisionTreeClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)\ngrid.fit(X_train, y_train) \n# print best parameter after tuning \nprint(grid.best_params_) \ngrid_predictions = grid.predict(X_test) \n   \n# print classification report \nprint(metrics.classification_report(y_test, grid_predictions)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2/knn model","metadata":{}},{"cell_type":"code","source":"param_grid = {\"weights\":[\"uniform\", \"distance\"], \n              \"n_neighbors\":[2,3,5,7,9,11,15]}\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)\ngrid.fit(X_train, y_train) \n# print best parameter after tuning \nprint(grid.best_params_) \ngrid_predictions = grid.predict(X_test) \n   \n# print classification report \nprint(metrics.classification_report(y_test, grid_predictions)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3/random_foreset_models","metadata":{}},{"cell_type":"code","source":"param_grid = {'n_estimators': [100, 200, 500, 1000],  \n              'criterion': [\"gini\", \"entropy\"], \n              'max_depth':[5, 7]}\ngrid = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)\ngrid.fit(X_train, y_train) \n# print best parameter after tuning \nprint(grid.best_params_) \ngrid_predictions = grid.predict(X_test) \n   \n# print classification report \nprint(metrics.classification_report(y_test, grid_predictions)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**summury of resulats of part of builidin models**","metadata":{}},{"cell_type":"code","source":"accurency_before = [0.8,0.63,0.73,0.53]\nacuurency_after = [0.8,0.72,0.75,0.53]\nmodel_name = [\"logistic_regression\",\"tree_classifer\",'randomforestclassifer', 'knn_classifer' ]\ne = np.arange(4)\nwidth = 0.2\n  \n# plot data in grouped manner of bar type\nplt.figure(figsize=(16,10))\nplt.bar(e-0.2, accurency_before, width, color='b')\nplt.bar(e, acuurency_after, width, color='y')\nplt.xticks(e, model_name)\nplt.xlabel(\"models\")\nplt.ylabel(\"accurency\")\nplt.legend([\"before\", \"after\"])\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n# 5. conclusion","metadata":{}},{"cell_type":"markdown","source":"as we can see in these figure and based on these analysis the best accurency can get with this dataset is 80% \ni think that some people talk about feature selection here and use it and get accurency better to 80% but in my opinion you can't build your model and chose some fearture and leave the rest  beacause as we see in part of analysis correlation between feature and target are genarally the same , then these kinds  of these data need an expert from the field to say if we cant expect variable from the buildin model operation or not \nas resume i say you can't build mathematical model with an accurancy grandther to 80%  but you can't applied ti the real world \n","metadata":{}},{"cell_type":"markdown","source":"if you found my work awsome give an upvote please ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}