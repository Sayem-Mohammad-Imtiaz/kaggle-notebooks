{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the dataset and get rid of useless columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the data\ndata = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get rid of useless columns\ndata = data[['v1','v2']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let us check the distribution of classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.v1.value_counts()\nsns.countplot(data.v1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The dataset in imbalanced. Accuracy will not be a good metric. We need to evaluate the model using precision/recall, F1-score, cohen-kappa score etc. We might need to perform undersampling/oversampling. We shall look at it later. We change 'ham' to label 0, 'spam' to label 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_label(value):\n    return 0 if value == 'ham' else 1\ndata['v1'] = data['v1'].apply(set_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning:\n#### 1. remove urls\n#### 2. remove emails\n#### 3. remove tags\n#### 4. remove punctuations\n#### 5. remove stopwords\n#### 6. lemmatize/stem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(text): \n    translator = str.maketrans('', '', string.punctuation) \n    return text.translate(translator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_tags(text):\n  return re.sub('<.*?>',\" \",text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(text):\n  return re.sub('[0-9]+','number',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n  return re.sub('https?:\\S+','httpaddr',text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emails(text):\n    return re.sub('\\S+@\\S+','email',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(remove_urls)\ndata['v2'] = data['v2'].apply(remove_tags)\ndata['v2'] = data['v2'].apply(remove_emails)\ndata['v2'] = data['v2'].apply(remove_punctuation)\ndata['v2'] = data['v2'].apply(remove_numbers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(lambda word : word.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstops = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    cleaned = []\n    for word in text.split():\n        if word not in stops:\n            cleaned.append(word)\n    return \" \".join(cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_words(text):\n  lemmas = []\n  for word in text.split():\n    lemmas.append(lemmatizer.lemmatize(word))\n  return \" \".join(lemmas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatize and shuffle the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(lemmatize_words)\ndata = data.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['v2'].values\ny = data['v1'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize the data into sequence of tokens. Then pad/truncate the data so that every sequence is of same length. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer() \ntokenizer.fit_on_texts(X)\nword_to_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_to_index)\nmax_length = 50\nembedding_dim = 100\npadded_sequences = pad_sequences(sequences,maxlen=max_length,padding='post',truncating='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set up the embedding matrix. We shall use 100 dimensional glove vectors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {};\nwith open('glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embeddings_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,SpatialDropout1D,LSTM,Bidirectional,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define and compile the model. We shall use an LSTM network with dropouts to prevent overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(128,recurrent_dropout=0.2,dropout=0.2)),\n    Dense(32,activation='relu'),\n    Dense(1,activation='sigmoid')\n])\noptimizer = Adam(learning_rate=0.01)\ncallbacks = ReduceLROnPlateau(monitor='val_accuracy',patience=2,factor=0.5,min_lr=0.00001)\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(padded_sequences,y,test_size=0.3,random_state=1)\nprint('No. of training samples:',len(X_train))\nprint('No. of testing samples:',len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nhistory = model.fit(X_train,y_train,epochs=epochs,validation_data=(X_test,y_test),batch_size=64,callbacks=[callbacks])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,cohen_kappa_score\ntrain_stats = model.evaluate(X_train,y_train)\ntest_stats = model.evaluate(X_test,y_test)\nprint('training accuracy:',train_stats[1]*100)\nprint('testing accuracy:',test_stats[1]*100)\n\ny_pred = model.predict_classes(X_test)\nprint(classification_report(y_test,y_pred))\nprint('Confusion matix:\\n',confusion_matrix(y_test,y_pred))\nprint('Cohen-kappa score:',cohen_kappa_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\n### Good results provided that the dataset was imbalanced.\n### Cohen kappa score of 92.3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}