{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d94a3485-984a-0a04-158c-6605608f9051"},"source":"In my other script, I like at using EEG Power [bands][1]. Here, we use the Raw EEG data. There is more data, but it is completely unstructured. How will this tradeoff allow us to classify what activity the subject is engaged in?\n\n\n  [1]: http://support.neurosky.com/kb/development-2/eeg-band-power-values-units-amplitudes-and-meaning"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ff61fa1-fd3b-9b3e-8ea6-f19f7661eee6"},"source":"Imports:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9dd3afd1-ff3b-7096-306e-e06198115734"},"outputs":[],"source":"import json\nimport random\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\ndf = pd.read_csv(\"../input/eeg-data.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc363637-4b20-6ff8-a6af-6d15fa72a3ce"},"outputs":[],"source":"\ndef prepare_individual_data(df,individual):\n\t# drop unused features. just leave eeg_power and the label\n\tdf = df.drop('Unnamed: 0', 1)\n\tdf = df.drop('indra_time', 1)\n\tdf = df.drop('browser_latency', 1)\n\tdf = df.drop('reading_time', 1)\n\tdf = df.drop('attention_esense', 1)\n\tdf = df.drop('meditation_esense', 1)\n\tdf = df.drop('signal_quality', 1)\n\tdf = df.drop('createdAt', 1)\n\tdf = df.drop('updatedAt', 1)\n\tdf = df.drop('eeg_power',1)\n\tdf['raw_values'] = df.raw_values.map(json.loads) #must perform, or else we won't be able to split cell by commas \n\t# separate eeg power to multiple columns\n\tto_series = pd.Series(df['raw_values']) # df to series\n\traw_data=pd.DataFrame(to_series.tolist()) #series to list and then back to df\n\tdf = pd.concat([df,raw_data], axis=1, join='outer') # concatenate the create columns\n\tdf = df.drop('raw_values', 1) # drop comma separated cell\n\tdf=df.loc[df['id'] == individual]\n\treturn df\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8013132e-79a7-b524-edf4-bf4453542675"},"outputs":[],"source":"individual_data=prepare_individual_data(df,3)\nprint(individual_data.shape)\nprint(individual_data.head())\n# now we have all raw values for id 3. and labels"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc05ce1d-fcab-37f0-e0ef-771c45525b1f"},"source":"Create function to clean labels, so that labels like \"math1\" = \"math2\" = math..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3a7dddb-7550-4a17-b5d2-e06c4385e7ed"},"outputs":[],"source":"def clean_labels(dd):\n    #Thanks Alexandru\n\tdd[\"label\"] = dd[\"label\"].str.replace(\"\\d|Instruction|-|ver\", \"\")\n\treturn dd\n\ncleaned_individual_data = clean_labels(individual_data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cdaeb0c8-0d8a-4d06-72cd-ffcb8403ebab"},"source":"Drop labels that I don't fully understand and therefore don't care to classify. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c80a56ed-4f39-9b0e-2934-55407c74e616"},"outputs":[],"source":"def drop_useless_labels(df):\n\t# drop unlabeled and everyone paired.\n\tdf = df[df.label != 'unlabeled']\n\tdf = df[df.label != 'everyone paired']\n\treturn df\n\nfinal_individual_full_data= drop_useless_labels(cleaned_individual_data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca274c10-18ed-5bd6-9ff2-a63552c1e6d4"},"source":"We can see how many labels we have and their frequency."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"286b5871-1720-a69f-fc00-2281efa07855"},"outputs":[],"source":"print(final_individual_full_data['label'].value_counts())"},{"cell_type":"markdown","metadata":{"_cell_guid":"bccfb7c0-278c-68de-c38c-3e248a127a4a"},"source":"Set aside a test set:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9deb4cc-e2c8-f973-603a-35f0e4121383"},"outputs":[],"source":"# set aside training\ndef set_aside_test_data(d):\n\tlabel=d.pop(\"label\") # pop off labels to new group\n\tx_train,x_test,y_train,y_test = train_test_split(d,label,test_size=0.2)\n    \nx_train, x_test, y_train, y_test = set_aside_test_data(final_individual_full_data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e611f39-defe-315b-68ad-ce75768ca65b"},"source":"Check out test and train data after split."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ead429c9-f2c7-83c6-dd30-6b5d9f4be950"},"outputs":[],"source":"print(x_train.shape)\nprint(x_train.head())\nprint(y_test.shape)\nprint(y_test.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"f46b11ae-cb10-bd8f-b794-40ab0c246922"},"source":"Now, since we have limited training data, let's expand it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5c87520-d83d-dba0-88a4-bea76b69c7fc"},"outputs":[],"source":"full_train = x_train.append(y_train)\nprint(full_train.head())\n\n\n#for i in range(5):\n    # merge x_train and y_train back together\n    # copy with noise\n    # append\n#\tcopy = x_train\n#\tcopy[0]=copy[0]+random.gauss(1,.1) # add noise to mean freq var\n#\tfinal_individual_full_data=final_individual_full_data.append(copy,ignore_index=True) # make voice df 2x as big\n#\tprint(\"shape of df after {0}th intertion of this loop is {1}\".format(i,final_individual_full_data.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"45b23577-56a5-2e18-352e-efb3122ef676"},"source":"Split back into features and labels. "},{"cell_type":"markdown","metadata":{"_cell_guid":"18fa4030-c0e5-fab3-bca7-e929c4a1789a"},"source":"Now we need to convert these pd dataframes to np arrays for tensorflow. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b78e9e3e-ae60-8428-b9ef-451eade6a5d1"},"outputs":[],"source":"#\ttrain_labels = pd.get_dummies(label) #make labels into one hot vector\n#\ttrain_labels = train_labels.values # convert to np array\n#\tdf=individualdata.values # convert features to np array\n#\tx_train,x_test,y_train,y_test = train_test_split(df,train_labels,test_size=0.2)\n#\t#so now we have predictors and y values, separated into test and train\n#\tx_train,x_test,y_train,y_test = np.array(x_train,dtype='float32'), np.array(x_test,dtype='float32'),np.array(y_train,dtype='float32'),np.array(y_test,dtype='float32')\n#\treturn x_train, x_test, y_train, y_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"d808d97c-d288-f489-2e4a-073e3f601fe6"},"source":"Create mini batch creator function."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9585e95f-1cde-6cc2-9f85-2a8665ee633b"},"outputs":[],"source":"def get_mini_batch(x,y):\n\trows=np.random.choice(x.shape[0], 50)\n\treturn x[rows], y[rows]\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"00a00ff6-c5ff-0e74-0efa-954c644b420c"},"source":"Train NN. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f64e958-7811-e068-b891-85d042aedd31"},"outputs":[],"source":"def trainNN(x_train, y_train,x_test,y_test,number_trials):\n\t# there are 8 features\n\t# place holder for inputs. feed in later\n\tx = tf.placeholder(tf.float32, [None, x_train.shape[1]])\n\t# # # take 20 features  to 10 nodes in hidden layer\n\tw1 = tf.Variable(tf.random_normal([x_train.shape[1], 1000],stddev=.5,name='w1'))\n\t# # # add biases for each node\n\tb1 = tf.Variable(tf.zeros([1000]))\n\t# # calculate activations \n\thidden_output = tf.nn.softmax(tf.matmul(x, w1) + b1)\n\tw2 = tf.Variable(tf.random_normal([1000, y_train.shape[1]],stddev=.5,name='w2'))\n\tb2 = tf.Variable(tf.zeros([y_train.shape[1]]))\n\t# # placeholder for correct values \n\ty_ = tf.placeholder(\"float\", [None,y_train.shape[1]])\n\t# # #implement model. these are predicted ys\n\ty = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\n\t# loss and optimization \n\tloss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))\n\topt = tf.train.AdamOptimizer(learning_rate=.0009)\n\ttrain_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])\n\t# start session\n\tsess = tf.Session()\n\t# init all vars\n\tinit = tf.initialize_all_variables()\n\tsess.run(init)\n\tntrials = number_trials\n\tfor i in range(ntrials):\n\t    # get mini batch\n\t    a,b=get_mini_batch(x_train,y_train)\n\t    # run train step, feeding arrays of 100 rows each time\n\t    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n\t    if i%500 ==0:\n\t    \tprint(\"epoch is {0} and cost is {1}\".format(i,cost))\n\tcorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\tprint(\"test accuracy is {}\".format(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})))\n\n\n#trainNN(x_train,y_train,x_test,y_test,10000)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}