{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/hackerearth-how-not-to-lose-a-customer-in-10-days/train.csv')\ntest_data = pd.read_csv('/kaggle/input/hackerearth-how-not-to-lose-a-customer-in-10-days/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report_train = pandas_profiling.ProfileReport(train_data)\nreport_train.to_file(\"report_train.html\")\n\nreport_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report_test = pandas_profiling.ProfileReport(test_data)\nreport_test.to_file(\"report_test.html\")\n\nreport_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = pd.concat([train_data,test_data],axis = 0)\nmerged.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = merged.replace('Unknown',np.nan)\nmerged = merged.replace('?',np.nan)\nmerged = merged.replace(-999,np.nan)\nmerged = merged.replace('Error',np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_with_missing = (col for col in merged.columns \n                                 if merged[col].isnull().any())\n\nfor col in cols_with_missing:\n    merged[col + '_was_missing'] = merged[col].isnull()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['gender_was_missing','region_category_was_missing',\n           'joined_through_referral_was_missing','preferred_offer_types_was_missing',\n           'medium_of_operation_was_missing','days_since_last_login_was_missing',\n           'avg_frequency_login_days_was_missing', 'points_in_wallet_was_missing']\n\nfor col in columns:\n    result = merged[col].astype(int)\n    merged[col] = result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = merged.drop(['churn_risk_score_was_missing'],axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_mod = merged.iloc[:36992, :]\ntest_data_mod = merged.iloc[36992:, :].drop(columns = ['churn_risk_score'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_tf_filtered = train_data_mod.copy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_tf_filtered.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_by_median = ['days_since_last_login','avg_frequency_login_days', 'points_in_wallet']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols_by_median:\n    exp_tf_filtered[col].fillna(exp_tf_filtered[col].median(), inplace=True)\n    test_data_mod[col].fillna(exp_tf_filtered[col].median(), inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_by_mode = ['gender','region_category','joined_through_referral',\n                'preferred_offer_types','medium_of_operation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols_by_mode:\n    exp_tf_filtered[col].fillna(exp_tf_filtered[col].mode()[0], inplace=True)\n    test_data_mod[col].fillna(exp_tf_filtered[col].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_tf_filtered.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_mod.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = exp_tf_filtered.copy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = df.churn_risk_score\ndf.drop('churn_risk_score', axis = 1, inplace = True)\ncooking_data = pd.concat([df,test_data_mod],axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cooking_data.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cooking_data.drop(['customer_id', 'Name', 'security_no', 'referral_id'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data = cooking_data.copy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data['current_date'] = pd.Timestamp('2020-12-31')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_data1 = unencoded_data['current_date']\ndate_data2 = unencoded_data['joining_date']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_data1 = pd.to_datetime(date_data1)\ndate_data2 = pd.to_datetime(date_data2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days_diff = date_data1 - date_data2\ndays_diff = pd.to_numeric(days_diff)\ndays_diff = days_diff/(24*60*60*1000000000)\ndays_diff = days_diff.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data[[\"avg_frequency_login_days\"]] = unencoded_data[[\"avg_frequency_login_days\"]].apply(pd.to_numeric)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data['last_visit_time_sec'] = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('mode.chained_assignment', None)\nfor index, row in unencoded_data.iterrows():\n    hour = int(row['last_visit_time'][:2])\n    mnt = int(row['last_visit_time'][3:5])\n    sec = int(row['last_visit_time'][6:8])\n    total_sec = hour*60*60 + mnt*60 + sec\n    unencoded_data['last_visit_time_sec'][index] = total_sec\n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data['last_visit_time_sec'] = unencoded_data['last_visit_time_sec'].astype('int64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data.drop(['last_visit_time'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unencoded_data['avg_time_spent'] = unencoded_data['avg_time_spent'].abs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_data = unencoded_data[['gender', 'region_category', 'preferred_offer_types', 'medium_of_operation', 'internet_option', 'complaint_status', 'feedback', 'membership_category']]\none_hot_data = pd.get_dummies(one_hot_data, drop_first=True, prefix=['gender', 'region_category', 'preferred_offer_types', 'medium_of_operation', 'internet_option', 'complaint_status', 'feedback', 'membership_category'])\none_hot_data = one_hot_data.astype('int64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bool_data = unencoded_data[['joined_through_referral', 'used_special_discount', 'offer_application_preference', 'past_complaint']]\nbool_data.replace(value = [1, 0], to_replace = ['Yes', 'No'], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_data = unencoded_data.select_dtypes(include = ['int64', 'float64'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cooked_data = pd.concat([num_data, one_hot_data, bool_data, days_diff],axis = 1)\ncooked_data = cooked_data.rename(columns = {0:\"Days_diff\"}) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cooked_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_prepared =  cooked_data.iloc[:36992, :]\ntest_prepared = cooked_data.iloc[36992:, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = target.abs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ntransformer = StandardScaler().fit(train_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_train_data = transformer.transform(train_prepared)\nscaled_test_data = transformer.transform(test_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_train_data = pd.DataFrame(data = scaled_train_data, columns = train_prepared.columns, index = train_prepared.index)\nscaled_test_data = pd.DataFrame(data = scaled_test_data, columns = test_prepared.columns, index = test_prepared.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = scaled_train_data\ny = target  \ndata = pd.concat([X, y],axis = 1)\ntrain_labels_mod = target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances.nlargest(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 43\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC \nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(multi_class='ovr')\nlinsvc = LinearSVC(multi_class='ovr')\nmlp = MLPClassifier(random_state=seed, early_stopping=True)\nbnb = BernoulliNB()\ngnb = GaussianNB()\nlda = LinearDiscriminantAnalysis()\nqda = QuadraticDiscriminantAnalysis()\nridge = RidgeClassifier(random_state=seed)\ndt = DecisionTreeClassifier(random_state=seed)\net = ExtraTreeClassifier(random_state=seed)\nrf = RandomForestClassifier(random_state=seed, n_jobs=-1)\nets = ExtraTreesClassifier(random_state=seed, n_jobs=-1)\ngboost = GradientBoostingClassifier(random_state=seed)\nkn = KNeighborsClassifier(n_jobs=-1)\nnc = NearestCentroid()\nxgboost = XGBClassifier(random_state=seed, n_jobs=-1)\nlgbm = LGBMClassifier(random_state=seed, n_jobs=-1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split_score(model):\n    from sklearn.metrics import f1_score\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, Y_train, Y_test = train_test_split(scaled_train_data, train_labels_mod, test_size = 0.2, random_state = seed)\n    model.fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n    f1_score = f1_score(prediction, Y_test, average='macro')\n    return f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [lr, linsvc, mlp, bnb, gnb, lda, qda, ridge, dt, et, rf, ets, gboost, kn, nc, xgboost, lgbm]\ntrain_test_split_f1 = []\n\nfor model in models:\n    print(model)\n    train_test_split_f1.append(train_test_split_score(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_score = pd.DataFrame(data = train_test_split_f1, columns = ['Train_Test_F1'])\ntrain_test_score.index = ['Logistic Reg','LinearSVC', 'MLPClassifier', 'BernoulliNB', 'GaussianNB', 'LinearDiscriminantAnalysis',\n                          'QuadraticDiscriminantAnalysis', 'RidgeClassifier', 'DecisionTreeClassifier', 'ExtraTreeClassifier', \n                          'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', \n                         'NearestCentroid', 'XGBClassifier', 'LGBMClassifier']\nsns.scatterplot(train_test_score.index,train_test_score['Train_Test_F1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_score.Train_Test_F1.nlargest(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eclf1 = VotingClassifier(estimators=[\n         ('gb', gboost), ('lgbm', lgbm), ('xgb', xgboost), ('rf', rf)], voting='soft', verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_split_score(eclf1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eclf1.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = eclf1.predict(scaled_test_data)\nId = test_data['customer_id'].values\nd = {'customer_id': Id , 'churn_risk_score': preds}\nsubmission = pd.DataFrame(data=d)\nsubmission.to_csv('votingclf.csv', index = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_sub(model, i, full_train=False):\n    \n    if full_train==True:\n        model.fit(scaled_train_data, train_labels_mod)\n    preds = model.predict(scaled_test_data)\n    Id = test_data['customer_id'].values\n    d = {'customer_id': Id , 'churn_risk_score': preds}\n    submission = pd.DataFrame(data=d)\n    submission.to_csv(str(model)[:10]+str(i)+'.csv', index = False)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sub(lgbm, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validate(model):\n    from sklearn.model_selection import cross_val_score\n    f1_score = cross_val_score(model, scaled_train_data, train_labels_mod, cv = 10, n_jobs = -1, scoring = 'f1_macro')\n    f1_score_rounded = np.round(f1_score, 5)\n    return f1_score_rounded.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [lr, mlp, bnb, gnb, lda, qda, ridge, dt, et, rf, ets, gboost, kn, nc, xgboost, lgbm]\ncross_val_scores = []\nfor model in models:\n    print(model)\n    cross_val_scores.append(cross_validate(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_val_score = pd.DataFrame(data = cross_val_scores, columns = ['Cross Validation Scores (F1)'])\nx_val_score.index = ['Logistic Reg', 'MLPClassifier', 'BernoulliNB', 'GaussianNB', 'LinearDiscriminantAnalysis',\n                          'QuadraticDiscriminantAnalysis', 'RidgeClassifier', 'DecisionTreeClassifier', 'ExtraTreeClassifier', \n                          'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', \n                         'NearestCentroid', 'XGBClassifier', 'LGBMClassifier']\nx_val_score = x_val_score.round(5)\nx = x_val_score.index\ny = x_val_score['Cross Validation Scores (F1)']\nsns.scatterplot(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_val_score['Cross Validation Scores (F1)'].nlargest(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE()\nscaled_train_data_res, train_labels_mod_res = sm.fit_resample(scaled_train_data, train_labels_mod )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split_score(model):\n    from sklearn.metrics import f1_score\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, Y_train, Y_test = train_test_split(scaled_train_data, train_labels_mod, test_size = 0.2, stratify=train_labels_mod, random_state = seed)\n    X_train_res, Y_train_res = sm.fit_resample(X_train, Y_train )\n    model.fit(X_train_res, Y_train_res)\n    prediction = model.predict(X_test)\n    f1_score = f1_score(prediction, Y_test, average='macro')\n    return f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [lr, linsvc, mlp, bnb, gnb, lda, qda, ridge, dt, et, rf, ets, gboost, kn, nc, xgboost, lgbm]\ntrain_test_split_f1 = []\n\nfor model in models:\n    print(model)\n    train_test_split_f1.append(train_test_split_score(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_score = pd.DataFrame(data = train_test_split_f1, columns = ['Train_Test_F1'])\ntrain_test_score.index = ['Logistic Reg','LinearSVC', 'MLPClassifier', 'BernoulliNB', 'GaussianNB', 'LinearDiscriminantAnalysis',\n                          'QuadraticDiscriminantAnalysis', 'RidgeClassifier', 'DecisionTreeClassifier', 'ExtraTreeClassifier', \n                          'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', \n                         'NearestCentroid', 'XGBClassifier', 'LGBMClassifier']\nsns.scatterplot(train_test_score.index,train_test_score['Train_Test_F1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_score.Train_Test_F1.nlargest(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = rf.predict(scaled_test_data)\nId = test_data['customer_id'].values\nd = {'customer_id': Id , 'churn_risk_score': preds}\nsubmission = pd.DataFrame(data=d)\nsubmission.to_csv('rf1.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = feat_importances.nlargest(20).index\nscaled_train_data_10 = scaled_train_data[cols]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split_score(model):\n    from sklearn.metrics import f1_score\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, Y_train, Y_test = train_test_split(scaled_train_data_10, train_labels_mod, test_size = 0.2, random_state = seed)\n    model.fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n    f1_score = f1_score(prediction, Y_test, average='macro')\n    return f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [lr, linsvc, mlp, bnb, gnb, lda, qda, ridge, dt, et, rf, ets, gboost, kn, nc, xgboost, lgbm]\ntrain_test_split_f1 = []\n\nfor model in models:\n    print(model)\n    train_test_split_f1.append(train_test_split_score(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_score = pd.DataFrame(data = train_test_split_f1, columns = ['Train_Test_F1'])\ntrain_test_score.index = ['Logistic Reg','LinearSVC', 'MLPClassifier', 'BernoulliNB', 'GaussianNB', 'LinearDiscriminantAnalysis',\n                          'QuadraticDiscriminantAnalysis', 'RidgeClassifier', 'DecisionTreeClassifier', 'ExtraTreeClassifier', \n                          'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', \n                         'NearestCentroid', 'XGBClassifier', 'LGBMClassifier']\nsns.scatterplot(train_test_score.index,train_test_score['Train_Test_F1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_score.Train_Test_F1.nlargest(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(scaled_train_data, train_labels_mod, test_size = 0.2, random_state = seed)\nbaseline = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\nbaseline.fit(X_train,y_train)\npredictors=list(X_train)\nfeat_imp = pd.Series(baseline.feature_importances_, predictors).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Importance of Features')\nplt.ylabel('Feature Importance Score')\nprint('Accuracy of the GBM on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\npred=baseline.predict(X_test)\nprint(classification_report(y_test, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\np_test3 = {'learning_rate':[0.1,0.05,0.01,0.5], 'n_estimators':[100,200,250,500,1000]}\n\ntuning = GridSearchCV(estimator =GradientBoostingClassifier(max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n            param_grid = p_test3, scoring='f1_macro', cv=5, verbose=2)\ntuning.fit(X_train,y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuning.best_score_\ntuning.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\np_test2 = {'max_depth':[2,3,4,5,6,7] }\ntuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.05,n_estimators=200, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n            param_grid = p_test2, scoring='f1_macro', cv=5, verbose=2)\ntuning.fit(X_train,y_train)\ntuning.best_score_, tuning.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=200,max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\nmodel1.fit(X_train,y_train)\npredictors=list(X_train)\nfeat_imp = pd.Series(model1.feature_importances_, predictors).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Importance of Features')\nplt.ylabel('Feature Importance Score')\nprint('Accuracy of the GBM on test set: {:.3f}'.format(model1.score(X_test, y_test)))\npred=model1.predict(X_test)\nprint(classification_report(y_test, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_test4 = {'min_samples_split':[2,5,10,20,50,75,100], 'min_samples_leaf':[1,3,5,7,9]}\n\ntuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.05, n_estimators=200,max_depth=4, subsample=1,max_features='sqrt', random_state=10), \n            param_grid = p_test4, scoring='f1_macro', cv=5,verbose=2)\ntuning.fit(X_train,y_train)\ntuning.best_params_, tuning.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_test5 = {'max_features':[2,5,10,15,20,'auto',None]}\ntuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.05, n_estimators=200,max_depth=4, min_samples_split=2, min_samples_leaf=3, subsample=1, random_state=10), \nparam_grid = p_test5, scoring='f1_macro',verbose=2, cv=5)\ntuning.fit(X_train,y_train)\ntuning.best_params_, tuning.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_test6= {'subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1]}\n\ntuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.05, n_estimators=200,max_depth=4, min_samples_split=2, min_samples_leaf=3,max_features=15, random_state=10), \nparam_grid = p_test6, scoring='f1_macro',verbose=2, cv=5)\ntuning.fit(X_train,y_train)\ntuning.best_params_, tuning.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new=GradientBoostingClassifier(learning_rate=0.05, n_estimators=200,max_depth=4, min_samples_split=2, min_samples_leaf=3,max_features=15, subsample=1, random_state=10)\nnew.fit(X_train,y_train)\npredictors=list(X_train)\nfeat_imp = pd.Series(new.feature_importances_, predictors).sort_values(ascending=False)\nfeat_imp.plot(kind='bar', title='Importance of Features')\nplt.ylabel('Feature Importance Score')\nprint('Accuracy of the GBM on test set: {:.3f}'.format(new.score(X_test, y_test)))\npred=new.predict(X_test)\nprint(classification_report(y_test, pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new.fit(X_train, y_train)\npreds = new.predict(scaled_test_data)\nId = test_data['customer_id'].values\nd = {'customer_id': Id , 'churn_risk_score': preds}\nsubmission = pd.DataFrame(data=d)\nsubmission.to_csv('gb_tuned2.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRFClassifier\nxgbrf = XGBRFClassifier()\nxgbrf.fit(X_train, y_train)\npreds = new.predict(scaled_test_data)\nId = test_data['customer_id'].values\nd = {'customer_id': Id , 'churn_risk_score': preds}\nsubmission = pd.DataFrame(data=d)\nsubmission.to_csv('xgbrf.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}