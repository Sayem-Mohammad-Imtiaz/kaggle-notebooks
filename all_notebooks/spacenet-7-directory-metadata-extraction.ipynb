{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Creating and Packaging GeoDataFrame from Data"},{"metadata":{},"cell_type":"markdown","source":"In the [previous notebook](https://www.kaggle.com/amerii/spacenet-7-metadata-extraction/) we dealt with the raw data labels that were in a csv format. In this notebook we are going to extract the metadata of the rest of the files in the rest of the directories. This will hopefully make our data more accessible and summarizable and make it easier to explore.\n\nBefore we begin, let's begin with some note on the filenames:\n\n-     The format of a filename (as defined above for the footprint definition CSV file) is:\n   `global_monthly_<time>_mosaic_<AOI-name>_<file_type>`\n    for example:\n    `global_monthly_2018_02_mosaic_L15-0369E-1244N_1479_3214_13_UDM`\n\n- `<time>` is a timestamp in `YYYY_MM` format that represents when image collection happened.\n \n-  `<AOI-name>` is a unique identifier of a location. All AOI-names are 28 characters long.\n\n-  All ids (filenames and AOI names) are case sensitive.\n\n- `<file_type>` is either going to be a `Buildings` or `UDM` file type, \n    note that the files in the images, and images_masked directories to not have a `<file_type>`\n\n-  Image data is stored in files named `<filename>.tif` in the `images`, `images_masked` and `UDM_masks` folders. <br>\n    *Note: Not all directories have a `UDM_masks` folder*\n\n-  Vector data (Building Labels and UDM Labels) is stored in files named `<filename>.geojson` in the `labels`, `labels_match` and `labels_match_pix` directories\n\nNote: AOI stands for Area of Interest"},{"metadata":{},"cell_type":"markdown","source":"## Import Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport re\nfrom pathlib import Path\nimport shapely\nimport geopandas as gpd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom glob import glob\ntqdm.pandas();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Directory Paths\n### Input Directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train')\ntest_dir = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_test_public')\nsample_dir = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Output Paths"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"output_path = Path.cwd()\noutput_csv_path = output_path/'output_csvs/'\nPath(output_csv_path).mkdir(parents=True, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extract Paths and Metadata\n\nNow that we have setup the input and output directories we can use the functions below to extract our desired metadata.\n\nThe functions below will be used to extract the following metadata from a list of paths:\n* Complete Path String: <br>`../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train/train/L15-0358E-1220N_1433_3310_13/images/global_monthly_2018_02_mosaic_L15-0358E-1220N_1433_3310_13.tif`\n* Mid Path String: <br>`L15-1210E-1025N_4840_4088_13/labels_match/global_monthly_2018_01_mosaic_L15-1210E-1025N_4840_4088_13_Buildings.geojson`\n* Unique File Name: `L15-0361E-1300N_1446_2989_13`\n* Directory Name: Name of the directory containing the images: `UDM_masks` `images` `images_masked` `labels `labels_match` `labels_match_pix`\n* Year: year in which the image was taken\n* Month: month in which the image was taken\n* Data Type: `Buildings` or `UDM`\n* File Extension: `.geojson` or `.tiff`\n../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train/train/L15-0361E-1300N_1c"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_metadata_from_string(string):\n    # extracted groups\n    # full path - image_dir_name - sub_dir_name - fname - year - month - data_type - extension\n    pattern = r'/(t.+|sample)/(L.+)/(\\w+)/(.+_(\\d+)_(\\d+)_m.+_\\d+_\\d+_\\d+)(?:_(\\w+))?.(\\w+)'\n    match = re.findall(pattern=pattern,string=string)\n    return match[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string1 = '../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample/L15-0506E-1204N_2027_3374_13/UDM_masks/global_monthly_2019_11_mosaic_L15-0506E-1204N_2027_3374_13_UDM.tif'\nstring2 = '../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample/L15-0506E-1204N_2027_3374_13/UDM_masks/global_monthly_2019_11_mosaic_L15-0506E-1204N_2027_3374_13.tif'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_metadata_from_string(string1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_metadata_from_string(string2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_list_of_paths(directory):\n    paths_list = [path for path in Path.glob(directory,pattern = '**/*.*')]\n    return paths_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_metadata_from_list_of_paths(list_of_paths):\n    d_keys = ['parent_dir','image_dir_name','sub_dir_name','fname','year','month','data_type','extension']\n    d = {key:[] for key in d_keys}\n    d['full_path'] = []\n    for path in list_of_paths:\n        metadata = extract_metadata_from_string(str(path))\n        d['full_path'].append(path)\n        \n        for i,data in enumerate(metadata):\n            d[d_keys[i]].append(data)\n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extracting the Metadata\nThe function below will extract a list of the paths of the files inside of the input directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_paths = extract_list_of_paths(directory=train_dir)\ntest_paths = extract_list_of_paths(directory=test_dir)\nsample_paths = extract_list_of_paths(directory=sample_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_metadata_dict = extract_metadata_from_list_of_paths(train_paths)\ntest_metadata_dict = extract_metadata_from_list_of_paths(test_paths)\nsample_metadata_dict = extract_metadata_from_list_of_paths(sample_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.DataFrame(train_metadata_dict)\ndf_test = pd.DataFrame(test_metadata_dict)\ndf_sample = pd.DataFrame(sample_metadata_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['data_type'] == 'Buildings']['extension'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['data_type'] == 'UDM']['extension'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['data_type'] == '']['extension'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you may have noticed some of our data types are giving us a value of `''`. This is because as mentioned earlier the files that are actually images or `.tif` files do not have a datatype at the end. \n\nIn order to remedy this problem we will simple replace the values = `''` with the value of `Images`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[df_train['data_type'] =='','data_type'] = 'Images'\ndf_test.loc[df_test['data_type'] =='','data_type'] = 'Images'\ndf_sample.loc[df_sample['data_type'] =='','data_type'] = 'Images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Finally, let's create a function that automates all the steps above."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_metadata(input_dir):\n    list_of_paths = extract_list_of_paths(input_dir)\n    metadata_dict = extract_metadata_from_list_of_paths(list_of_paths)\n    df = pd.DataFrame(metadata_dict)\n    \n    df.loc[df['data_type'] =='','data_type'] = 'Images'\n    \n\n    # Identify Images that have UDM Masks\n    condition = (df['sub_dir_name'] == 'UDM_masks')\n    # Get the indices of the images that have udm\n    udm_indices = df.loc[condition].index\n    # Get list of unique file names that have UDMs\n    udm_fnames = list(df.loc[udm_indices,'fname'])\n    # Get all rows that match the file names\n    udm_mask = df['fname'].progress_map(lambda x: x in udm_fnames)\n    # Initialize has_udm column \n    df['has_udm'] = False\n    # Apply mask and update udm value\n    df.loc[udm_mask,'has_udm'] = True\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving the Outputs\nFinally we are going to save the output dataframes as csvs. We are going to have 4 csvs in total:\n* CSV for the train dataframe\n* CSV for the test dataframe\n* CSV for the sample dataframe\n* CSV for the concatenated train, test and sample dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_metadata(train_dir)\ndf_test = get_metadata(test_dir)\ndf_sample = get_metadata(sample_dir)\ndf_concat = pd.concat([df_train,df_test,df_sample]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.to_csv(output_csv_path/'df_train.csv',index=False)\ndf_test.to_csv(output_csv_path/'df_test.csv',index=False)\ndf_sample.to_csv(output_csv_path/'df_sample.csv',index=False)\ndf_concat.to_csv(output_csv_path/'df_concat.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make sure that the output is saved"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./output_csvs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so now we have saved the first version of our csv, our csv is formatted in a format known as tidy data. This format, makes it really easy to analyse our metadata. Next we are going to manipulate our dataframe so that we can make it easier to create a dataset class from it in pytorch.\n\nWe are going to do this by adding a column for each of the labels paths."},{"metadata":{"trusted":true},"cell_type":"code","source":"def untidy_df(df):\n    \n    parent_dir = df['parent_dir']\n    im_dir_name = df['image_dir_name']\n    fname = df['fname']\n    year = df['year']\n    month = df['month']\n    has_udm = df['has_udm']\n    \n    images_masked = im_dir_name + '/images_masked/' + fname + '.tif'\n    \n    if parent_dir == 'test_public':\n            images = None\n            labels_buildings = None\n            labels_udm = None\n            labels_match = None\n            labels_match_pix = None\n            udm_masks = None\n    else:\n        if has_udm:\n            udm_masks = im_dir_name + '/UDM_masks/' + fname + '.tif'\n        else:\n            udm_masks = None\n\n\n        images = im_dir_name + '/images/' + fname + '.tif'\n        labels_buildings = im_dir_name + '/labels/' + fname + '_Buildings.geojson'\n        labels_udm = im_dir_name + '/labels/' + fname + '_UDM.geojson'\n        labels_match = im_dir_name + '/labels_match/' + fname + '_Buildings.geojson'\n        labels_match_pix = im_dir_name + '/labels_match_pix/' + fname + '_Buildings.geojson'\n\n    keys = ['parent_dir','image_dir_name','fname','year','month','has_udm','udm_masks','images','images_masked','labels_buildings','labels_udm','labels_match','labels_match_pix']\n    values = [parent_dir,im_dir_name,fname,year,month,has_udm,udm_masks,images,images_masked,labels_buildings,labels_udm,labels_match,labels_match_pix]\n    \n    return {k:v for (k,v) in zip(keys,values)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_untidy_frame(df):\n    # apply function on input dataframe\n    list_of_dicts = df.progress_apply(lambda x: untidy_df(x),axis=1)\n    # drop the duplicated columns\n    untidy_frame = pd.DataFrame.from_records(list_of_dicts).drop_duplicates()\n    # bask in all the glory of your untidy frame ;D\n    return untidy_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_untidy_df = get_untidy_frame(df_test)\ntrain_untidy_df = get_untidy_frame(df_train)\nsample_untidy_df = get_untidy_frame(df_sample)\nconcat_untidy_df = get_untidy_frame(df_concat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_untidy_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above csv format will make it easier for us to create our custom pytorch dataset class, notice how you have access to whichever image or geojson file that you want, and how they are all grouped by the corresponding month and year."},{"metadata":{},"cell_type":"markdown","source":"# Finally we save the untidy dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_untidy_df.to_csv(output_csv_path/'df_train_untidy.csv',index=False)\ntest_untidy_df.to_csv(output_csv_path/'df_test_untidy.csv',index=False)\nsample_untidy_df.to_csv(output_csv_path/'df_sample_untidy.csv',index=False)\nconcat_untidy_df.to_csv(output_csv_path/'df_concat_untidy.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./output_csvs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What Next?\nNext we are going to create a bunch of helper functions,[in our next notebook](https://www.kaggle.com/amerii/spacenet-7-helper-functions), that will make navigating, visualizing and understanding our dataset much much easier!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}