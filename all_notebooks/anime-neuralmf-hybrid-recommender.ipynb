{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Anime NeuralMF Hybrid Recommender\n### In this notebook, we implement a recommender model with the MyAnimeList Anime Recommendations dataset.\n\n> Based on the Neural Collaborative Filtering paper: Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu and Tat-Seng Chua (2017). Neural Collaborative Filtering. In Proceedings of WWW '17, Perth, Australia, April 03-07, 2017.\n\n#### The following is a little motivation for Hybrid recommender systems.\n\n## Why Hybrid?\nWell, there are two main kinds of recommender systems: Content-based and Collaborative filtering-based.\n* Content-based recommenders suggest similar picks to a certain _item_ (an anime movie/series in our case), letting the users know about similar items to the ones they have watched/rated positively. These method typically use _item features_ together with unsupervised methods in an effort to generate a product-space and compute similarities between items. However, this method may end suggesting a limited mix of items, providing a low _surprise factor_ for the user.\n* On the other hand, collaborative filtering recommenders rely on past users' history of watched/rated items, increasing the chances of recommending a serendipitous item to a target user. Classic methods rely solely on a user-item matrix, which maps the interactions that all users have with every item. These matrix methods are heavily memory-intensive and newer neural network-based are more common. Nonetheless, these methods could miss on similar -but typically overseen- items, in comparison to the ones watched/reviewed by the target user.\n\nIn order to get more robust recommendations, a hybrid model can combine both item features and user-item features.\n\n## And... why NeuralMF?\nThe NeuralMF is a mix of General Matrix Factorization (GMF) and Multi Layer Perceptron (MLP) recommenders, resembling a Wide&Deep model, having higih generalization power. Plus, neural nets make easier to handle large volumes of data, and it better leverages the power of GPUs! For more info, refer to the [article](https://arxiv.org/abs/1708.05031)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Some typical imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom numba import jit # Compile some functions when performance is critical\nimport keras\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Embedding, Input, Dense, Concatenate, Multiply, Flatten\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nif tf.test.gpu_device_name():\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\nelse:\n    print(\"No GPU\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Content-based feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check our data structure\nanime = pd.read_csv(\"../input/anime-recommendations-database/anime.csv\")\nanime.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For simplicity, let's use TV anime only. This prevents future complications with the episode number."},{"metadata":{"trusted":true},"cell_type":"code","source":"anime = anime[anime['type'] == 'TV']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The content-based part of our model requires features for each anime. Therefore, we are going to include as many relevant features as possible, to not waste any information. So, we are using the anime genres and number of episodes. We dropped the title here, due to a lack of ways of handling it. Rating and members are not content-related features, since they are dynamic and bounded to users' activity. So, they are going to be leveraged through the collaborative-filtering part.\n\nLet's start by one-hot encoding genres:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the column\nanime['features_genre'] = anime['genre']\n\n# Cast None to an empty string\nanime['features_genre'] = anime['features_genre'].fillna('') \n# Split genres into a list of strings\nanime['features_genre'] = anime['features_genre'].map(lambda x: x.split(', '))\n\n# Create a set of all genres\nall_genres = set()\nfor row in anime['features_genre']:\n    # Union of sets is declared with the | operator\n    all_genres = all_genres | set(row)\nall_genres.remove('') # Drop the empty genre\n\ndef invert_dict(d):\n    return {value: key for key, value in d.items()}\n\nall_genres = sorted(list(all_genres)) # We convert it to a list to enforce alphabetic ordering\nngenres = len(all_genres)\n\nidx2genre = dict(enumerate(all_genres)) # Create a mapping dictionary from index to dict\ngenre2idx = invert_dict(idx2genre) # Inverse dict\n\ngenre2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_genres(genres):\n    out = np.zeros(ngenres)\n    for genre in genres:\n        if genre == '':\n            pass\n        else:\n            out[genre2idx[genre]] = 1\n    return out.tolist()\nanime['features_genre'] = anime['features_genre'].map(encode_genres)\nanime['features_genre'] # See how the encoded features look","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we need to do some more detailed feature engineering for the remaining features.\n\nThe number of episodes contains 'Unknown' labels among numeric values. We impute them by filling missed episodes with 1 episode."},{"metadata":{"trusted":true},"cell_type":"code","source":"anime['features_episodes'] = anime['episodes'].replace({'Unknown' : 1}).astype(np.int32)\nsb.distplot(anime['features_episodes']);\n# This feature is heavily unbalanced! Let's apply a quantile transformation to it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we impose a uniform-like distribution, using Scikit-Learn's QuantileTransformer. This is an easier-to-handle representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"ep_discretizer = QuantileTransformer(n_quantiles = 100)\nfeats_ep = anime['features_episodes'].apply(np.log).to_numpy().reshape(-1, 1)\nfeats_ep = ep_discretizer.fit_transform(feats_ep).flatten().tolist()\nanime['features_episodes'] = feats_ep\nsb.distplot(anime['features_episodes']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collaborative-filtering feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check our data structure\nrating = pd.read_csv(\"../input/anime-recommendations-database/rating.csv\")\nrating.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We lower the number of users for quicker exploration & training, although you can comment the next line.\n### Everything fits in memory, so it won't crash!"},{"metadata":{"trusted":true},"cell_type":"code","source":"rating = rating[rating['user_id'] <= 10000] # Can comment this line\nrating = rating[rating['anime_id'].isin(anime['anime_id'])] # Don't comment this one though!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Our goal is to reach as many users as possible; we need to impute missing reviews\n\nMany users don't review the shows they've watched. We could drop these records or impute them with the median of the users' ratings, for example.\nInspecting the distribution of ratings, we see that most of them are positive (with a median of 8). Therefore, we should consider imputing unrated shows, since most seen shows are positive signals.\n\nAn important thing to stress is that our algorithm is not trying to recommend masterpieces only, but rather a varied mix of shows that the user might enjoy more or less. This helps to reach _hardcore otakus_ as well as casual viewers."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rating['rating'].replace({-1: np.nan}).dropna().describe())\nsb.distplot(rating['rating'], kde = False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We group by user and replace missing ratings with the median of the user.\nMany users don't leave reviews. To not lose this information, we impute them with the median of all users: 8"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_median = rating.groupby('user_id').median()['rating']\nsb.distplot(user_median, kde = False);\noverall_median = user_median.median()\nprint(\"Median of all users' medians: \", overall_median)\nuser_median = dict(user_median.replace({-1 : overall_median}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_medians = rating['user_id'].apply(lambda x: user_median[x])\nrating['rating'] = rating['rating'].replace({-1 : np.nan}).fillna(user_medians)\nrating['rating'] = rating['rating'] / rating['rating'].max() # Divide by the max to normalize!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resulting histogram\nsb.distplot(rating['rating'], kde = False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Construct training and testing sets\nOur current dataset is incomplete, since we need to generate rows including anime that users' havent watched (_negative intances_). The following accounts for that factor. We need to emphasize that we don't want every user to have a row for every anime, to not fill up our entire RAM memory.\n\nAllow us to set that every rating will trigger 4 negative entries (we picked 4 just as a fiducial value from the original repo). To generate these records, we simply sample 4 unwatched animes for each user rating."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_neg = 4\nuser2n_anime = dict(rating.groupby('user_id').count()['anime_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_users = np.sort(rating['user_id'].unique())\nall_anime = np.sort(rating['anime_id'].unique())\nn_anime = len(all_anime)\nn_users = len(all_users)\n\n@jit\ndef choice_w_exclusions(array, exclude, samples):\n    max_samples = len(array)-len(exclude)\n    final_samples = min(samples, max_samples)\n    possible = np.array(list(set(array) - set(exclude)))\n    return np.random.choice(possible, size = final_samples, replace = False)\n@jit\ndef flat(l):\n    return [item for sublist in l for item in sublist]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample negative entries"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#This part takes about 10 minutes with a full dataset. Time for coffee!\nneg_user_id = []\nneg_anime_id = []\nneg_rating = []\n\nfor user in all_users:\n    exclude = list(rating[rating['user_id'] == user]['anime_id'])\n    sampled_anime_id = choice_w_exclusions(all_anime, exclude, len(exclude) * num_neg)\n    \n    neg_user_id.append([user] * len(sampled_anime_id))\n    neg_anime_id.append(sampled_anime_id)\n    neg_rating.append([0.] * len(sampled_anime_id))\n    \nneg_user_id = flat(neg_user_id)\nneg_anime_id = flat(neg_anime_id)\nneg_rating = flat(neg_rating)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negatives = pd.DataFrame({'user_id': neg_user_id,\n                          'anime_id': neg_anime_id,\n                          'rating': neg_rating})\ndata = pd.concat([rating, negatives], ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Join both tables' information and drop unindexed anime"},{"metadata":{"trusted":true},"cell_type":"code","source":"anime['features'] = anime['features_genre'] + anime['features_episodes'].apply(lambda x: [x])\nanime['features'] = anime['features'].apply(np.array)\nn_feats = len(anime['features'].iloc[0])\ndata = data.join(anime['features'], on = 'anime_id').dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embeddings need a compressed index representation of animes: Let's make a quick mapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"anime2item_dict = dict(zip(np.sort(all_anime), list(range(n_anime))))\nitem2anime_dict = {v: k for k, v in anime2item_dict.items()}\n\ndef anime2item(a_id):\n    return anime2item_dict[a_id]\n\ndef item2anime(i_id):\n    return item2anime_dict[i_id]\n                       \ndata['item_id'] = data['anime_id'].apply(anime2item)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split into a 90/10 train/test scheme.\nNote: We can't separate users between train and test sets (like train users versus test users), since we need to feed all users and anime shows to the embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"x0 = data['user_id'].to_numpy()\nx1 =data['item_id'].to_numpy()\nx2 = np.stack(data['features'].to_numpy())\ny = data['rating'].to_numpy()\n\n(x0_train, x0_val,\n x1_train, x1_val,\n x2_train, x2_val,\n y_train, y_val) = train_test_split(x0, x1, x2, y,\n                                    test_size = 0.1,\n                                    random_state = 42)\n\nx_train = [x0_train, x1_train, x2_train]\nx_val = [x0_val, x1_val, x2_val]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model implementation\n> [Heavily based on the [Neural Collaborative Filtering paper repo](https://github.com/hexiangnan/neural_collaborative_filtering)]\n\nHowever, our model improved the reference model by including information of anime features!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(num_users, num_items, num_item_feats, mf_dim, layers = [64, 32, 16, 8]):\n    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n    feats_input = Input(shape=(num_item_feats,), dtype='float32', name = 'feats_input')\n\n    # User&Item Embeddings for Matrix Factorization\n    MF_Embedding_User = Embedding(input_dim = num_users + 1, output_dim = mf_dim,\n                                  name = 'user_embedding',\n                                  embeddings_initializer = RandomNormal(stddev=0.001),\n                                  input_length = 1)\n    MF_Embedding_Item = Embedding(input_dim = num_items + 1, output_dim = mf_dim,\n                                  name = 'item_embedding',\n                                  embeddings_initializer = RandomNormal(stddev=0.001),\n                                  input_length = 1)\n    \n    # User&Item Embeddings for MLP part\n    MLP_Embedding_User = Embedding(input_dim = num_users + 1, output_dim = int(layers[0] / 2),\n                                   name = 'mlp_embedding_user',\n                                   embeddings_initializer = RandomNormal(stddev=0.001),\n                                   input_length = 1)\n    MLP_Embedding_Item = Embedding(input_dim = num_items + 1, output_dim = int(layers[0] / 2),\n                                   name = 'mlp_embedding_item',\n                                   embeddings_initializer = RandomNormal(stddev=0.001),\n                                   input_length = 1) \n    \n    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n\n    # MLP part with item features\n    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n    \n    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent, feats_input])\n    for l in layers:\n        layer = Dense(l, activation='relu')\n        mlp_vector = layer(mlp_vector)\n\n    # Concatenate MF and MLP parts\n    predict_vector = Concatenate()([mf_vector, mlp_vector])\n    \n    # Final prediction layer\n    prediction = Dense(1, activation = 'sigmoid',\n                       kernel_initializer = 'lecun_uniform',\n                       name = 'prediction')(predict_vector)\n    \n    model = Model(input = [user_input, item_input, feats_input], output = prediction)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set hyperparameters, which are very similar to the default values from the NeuralMF model repo, except for the number of epochs and layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.001\nbatch_size = 256\nn_epochs = 3\nmf_dim = 15\nlayers = [128, 64, 32, 16, 8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create model and train!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model(n_users, n_anime, n_feats, mf_dim, layers)\nmodel.compile(optimizer = Adam(lr = learning_rate), loss = 'mean_squared_logarithmic_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val),\n                 batch_size = batch_size, epochs = n_epochs, verbose = True, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Eval'], loc = 'upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alright, let's visualize some recommendations!"},{"metadata":{"trusted":true},"cell_type":"code","source":"indexed_anime = anime.set_index('anime_id')\n\ndef explore(user_id, top = 5):\n    sub = rating[rating['user_id'] == user_id]\n    watched_animes = sub['anime_id']\n    ratings = sub['rating']\n    names = indexed_anime.loc[watched_animes]['name']\n    genres = indexed_anime.loc[watched_animes]['genre']\n    rating_info = pd.DataFrame(zip(watched_animes, names,\n                                   genres, ratings * 10),\n                               columns = ['anime_id', 'name',\n                                          'genre', 'rating']).set_index('anime_id')\n    return rating_info.sort_values(by = 'rating', ascending = False).iloc[:top]\n\ndef recommend(user_id, recommendations = 5):\n    watched_animes = rating[rating['user_id'] == user_id]['anime_id']\n    \n    test_anime = np.array(list(set(all_anime) - set(watched_animes)))\n    test_user = np.array([user_id] * len(test_anime))\n    test_items = np.array([anime2item(a) for a in test_anime])\n    sub_anime = indexed_anime.loc[test_anime]\n    test_features = np.stack(sub_anime['features'].to_numpy())\n    test = [test_user, test_items, test_features]\n    preds = model.predict(test).flatten()\n    results = pd.DataFrame(zip(sub_anime['name'], test_anime,  sub_anime['genre'], preds * 10),\n                           columns = ['name', 'anime_id',\n                                      'genre', 'score']).set_index('anime_id')\n    return results.sort_values(by = 'score', ascending = False).iloc[:recommendations]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore(444) # Action Sports study","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend(444)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore(999) # Action Fantasy case study","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend(999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore(111) # Techno study","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend(111)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apparently, the recommender works like a charm.\nBut... There's still plenty of job to do, like observing extreme cases such as users with few watched anime, niche clusters, and so on."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}