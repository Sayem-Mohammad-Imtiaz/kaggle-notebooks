{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-Libraries\" data-toc-modified-id=\"Project-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Project Libraries</a></span></li><li><span><a href=\"#The-First-Contact-with-Data\" data-toc-modified-id=\"The-First-Contact-with-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The First Contact with Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Overview\" data-toc-modified-id=\"Data-Overview-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Data Overview</a></span></li></ul></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>EDA</a></span></li><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Prep</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Feature Selection</a></span></li><li><span><a href=\"#Train-and-Test\" data-toc-modified-id=\"Train-and-Test-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Train and Test</a></span></li><li><span><a href=\"#Categorical-Pipeline\" data-toc-modified-id=\"Categorical-Pipeline-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Categorical Pipeline</a></span></li><li><span><a href=\"#Numerical-Pipeline\" data-toc-modified-id=\"Numerical-Pipeline-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Numerical Pipeline</a></span></li><li><span><a href=\"#Full-Pipeline\" data-toc-modified-id=\"Full-Pipeline-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Full Pipeline</a></span></li></ul></li><li><span><a href=\"#Predictive-Model\" data-toc-modified-id=\"Predictive-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Predictive Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Decision Trees</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#Voting-Classifier\" data-toc-modified-id=\"Voting-Classifier-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Voting Classifier</a></span></li><li><span><a href=\"#Bootstrap-Agregating\" data-toc-modified-id=\"Bootstrap-Agregating-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Bootstrap Agregating</a></span></li><li><span><a href=\"#Adaptative-Boosting\" data-toc-modified-id=\"Adaptative-Boosting-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Adaptative Boosting</a></span></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Gradient Boosting</a></span></li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>LightGBM</a></span></li></ul></li><li><span><a href=\"#Deep-Neural-Network\" data-toc-modified-id=\"Deep-Neural-Network-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Deep Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Treinando-Rede-Neural-Profunda\" data-toc-modified-id=\"Treinando-Rede-Neural-Profunda-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Treinando Rede Neural Profunda</a></span></li></ul></li></ul></div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hi everyone and welcome to my notebook! I've prepared a helpful (I hope) analysis on the [Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing), a dataset build from a 2014 study based on marketing campaing of a portuguese bank. By the end, it was possible to flag the data if the customer subsribed or not the produt (`yes` or `no`).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Well, the goal of this notebook is to present you a efficient way to gather insights from data. We will also train a Machine Learning model to predict the chance of a customer to subscribe the bank produto, given the features selected from data. _I really hope you enjoy and if you do so, don't forget to **upvote this kernel**_!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Project Libraries","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:05:35.009011Z","end_time":"2020-06-18T01:05:39.556506Z"},"trusted":true},"cell_type":"code","source":"# Standard libraries\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\nfrom math import ceil\nfrom datetime import datetime\n\n# Viz libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Utils (homemade)\nfrom viz_utils import *\nfrom prep_utils import *\nfrom ml_utils import *\n\n# Ml libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, \\\n    GradientBoostingClassifier\nimport lightgbm as lgb\n\n# Deep Learning frameworks\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First Contact with Data","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:05:39.559499Z","end_time":"2020-06-18T01:05:39.635302Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Reading the data\npath = '../input/bank-marketing-dataset/bank.csv'\ndf_ori = pd.read_csv(path, sep=',')\ndf_ori.columns = [col.lower().strip().replace('.', '_') for col in df_ori.columns]\n\nprint(f'Data shape: {df_ori.shape}')\ndf_ori.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we could see on the dataset documentation, we have a group of 16 features and one target (this one represented by the column `deposit`). By looking for the data shape, we can see that the data has a little bit more than 11k rows.\n\nAt this point, it's important to take a look at each feature's meaning individually. With this we can be prepared to take more confident decisions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Overview","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here I take freedom to use some homemade implementations built to make some general Data Science steps easier. The next cell calls the `data_overview()` method from my `DataPrep()` class to consolidate some useful insights from the dataset.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:05:39.638289Z","end_time":"2020-06-18T01:05:39.72406Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating the class object\nprep = DataPrep()\n\n# Transforming the dataset target\ndf = df_ori.copy()\ndf['target'] = (df['deposit'] == 'yes') * 1\ndf.drop('deposit', axis=1, inplace=True)\n\n# Returing an overview from the data\ntarget = 'target'\ndf_overview = prep.data_overview(df, label_name=target)\ndf_overview","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the result above, it's possible to point:\n\n* There is no null data on this dataset;\n* The feature `duration` is the one with the highest correlation with our target (there is an observation of this on the dataset's documentation).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is really an important session on the analysis. Here we will apply some data visualization and data exploration techniques in order to gain insights from data. We will propose some questions to be answeared with coding steps.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Is this a kind of imbalanced dataset?**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:06:00.398253Z","end_time":"2020-06-18T01:06:00.472062Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ploting a donut chart with the target variable\nlabel_names = ['No', 'Yes']\ncolor_list = ['salmon', 'darkslateblue']\n\nfig, ax = plt.subplots(figsize=(8, 8))\ntitle = 'Donut Chart for Target Variable'\ndonut_plot(df, target, label_names, ax=ax, text=f'Total: {len(df)}', colors=color_list, title=title)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, the graph below shows us that this is not a imbalanced dataset. Both classes has similar proportion.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What are the categories of the categorical features?**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:07:13.976367Z","end_time":"2020-06-18T01:07:15.562866Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Visualizing the categorical features\ncat_features = [col for col, dtype in df.dtypes.items() if dtype == 'object']\ncatplot_analysis(df, cat_features, fig_cols=3, hue='target', palette=['salmon', 'darkslateblue'], figsize=(16, 16))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-08T16:03:43.06415Z","start_time":"2020-06-08T16:03:43.027729Z"}},"cell_type":"markdown","source":"**How is the distribution of numerical features?**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:08:07.728303Z","end_time":"2020-06-18T01:08:07.734314Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Parameters\nnum_features = ['age', 'balance', 'duration', 'campaign']\ncolor_list = ['salmon', 'darkslateblue']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:08:08.226934Z","end_time":"2020-06-18T01:08:08.960966Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Analyzing numerical features\ndistplot(df, num_features, fig_cols=3, hue='target', color=color_list, figsize=(16, 12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_Searching for different insights from the stripplot_","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:08:33.05585Z","end_time":"2020-06-18T01:08:37.2791Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Stripplot\nstripplot(df, num_features, fig_cols=3, hue='target', palette=color_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_Searching for different insights from the boxenplot_","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:08:41.261931Z","end_time":"2020-06-18T01:08:44.503421Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Stripplot\nboxenplot(df, num_features, fig_cols=3, hue='target', palette=color_list)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-06-10T16:13:32.424131Z","start_time":"2020-06-10T16:13:32.398056Z"}},"cell_type":"markdown","source":"**What are the features with most positive correlation with the target?**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:09:01.205337Z","end_time":"2020-06-18T01:09:01.632196Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Analisando top variáveis com maior correlação POSITIVA\ntop_pos_corr_cols = target_correlation_matrix(df, label_name='target', corr='positive')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Prep","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After wenting trough the data exploration step, we can start the preparation step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The first task is related with filtering the columns that won't be used on a preditive model. As long as we don't have any key-columns that don't make sense to put on a Machine Learning algorithm, the only column to be dropped is `duration`. Besides this variable is the most correlated one, this drop must be done because the `duration` here represents the call duration with the customer during the product offer, so its values is known only after the contact is finished.\n\nThinking of a preditive perspective, we can build a model that is capable to predict the chance of a customer subscripe a product **BEFORE** the contact is made. That's why we must drop `duration`.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:14:08.781197Z","end_time":"2020-06-18T01:14:08.807126Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Columns to be dropped\nto_drop = ['duration']\ndf_drop = df.drop(to_drop, axis=1)\n\n# Verifyng\nprint(f'Shape before the drop: {df.shape}')\nprint(f'Shape after the drop: {df_drop.shape}')\ndf_drop.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Test","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A fundamental point on the training Pipeline of Machine Learning models is, with no doubts, the dataset split in training and testing sets. This allows us to optimize the model with one set and validate with another unseen set.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:15:54.818283Z","end_time":"2020-06-18T01:15:54.835233Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Splitting the data\nX = df_drop.drop('target', axis=1)\ny = df_drop['target'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this moment, we will put aside the test set and we will work marjoritary with the training set. We go back with test set just to evaluate the model already trained.\n\nLet's make another split: a categorical and a numerical set.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:19:18.524287Z","end_time":"2020-06-18T01:19:18.537255Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Returing features by dtype\nnum_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\ncat_features = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\nprint(f'Total of numerical features: {len(num_features)}')\nprint(f'Total of categorical features: {len(cat_features)}')\n\n# Splitting data by dtype\nX_train_num = X_train[num_features]\nX_train_cat = X_train[cat_features]\nprint(f'\\nShape of numerical training data: {X_train_num.shape}')\nprint(f'Shape of categorical training data: {X_train_cat.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thinking somehow on building a pre-processing pipeline, let's create a class for doing this splitting by dtype automatically.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:20:37.029231Z","end_time":"2020-06-18T01:20:37.037209Z"},"trusted":true},"cell_type":"code","source":"# Class for splitting the data by dtype\nclass SplitDataDtype(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Returing features by dtype\n        self.num_features = [col for col, dtype in X.dtypes.items() if dtype != 'object']\n        self.cat_features = [col for col, dtype in X.dtypes.items() if dtype == 'object']\n        \n        # Indexing data\n        X_num = X[self.num_features]\n        X_cat = X[self.cat_features]\n        \n        return X_num, X_cat","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:21:05.621907Z","end_time":"2020-06-18T01:21:05.632873Z"},"trusted":true},"cell_type":"code","source":"# Creating object and calling the fit_transform method\ndtype_splitter = SplitDataDtype()\nX_train_num, X_train_cat = dtype_splitter.fit_transform(X_train)\n\nprint(f'Shape of numerical training data: {X_train_num.shape}')\nprint(f'Shape of categorical training data: {X_train_cat.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After splitting the data on numerical and categorical sets, let's apply the encoding processing on categorical data. This is important for the Machine Learning model to train the data in a correct way.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:22:49.226382Z","end_time":"2020-06-18T01:22:49.234332Z"},"trusted":true},"cell_type":"code","source":"# Class por encoding the data\nclass DummiesEncoding(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Collecting variables\n        self.cat_features_ori = [col for col, dtype in X.dtypes.items() if dtype == 'object']\n        \n        # Applying encoding with get_dummies()\n        X_cat_dum = pd.get_dummies(X)\n        \n        # Merging the datasets and eliminating old columns\n        X_dum = X.join(X_cat_dum)\n        X_dum = X_dum.drop(self.cat_features_ori, axis=1)\n        self.features_after_encoding = list(X_dum.columns)\n        \n        return X_dum","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:23:01.138261Z","end_time":"2020-06-18T01:23:01.195115Z"},"trusted":true},"cell_type":"code","source":"# Applying encoding on categorical data\nencoder = DummiesEncoding()\nX_train_encoded = encoder.fit_transform(X_train_cat)\n\nprint(f'Shape of X_train_encoded: {X_train_encoded.shape}')\nX_train_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On the numerical features case, it's possible to apply a scaling proccess. For some model like Logistic Regression, this step is really important in order to give to the algorithm a fast chance to reach the optimal cost. But for another ones, like Decision Trees, the scaling is not a request.\n\nFrom a didactic perspective, let's apply the scaling on your data.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:25:49.228749Z","end_time":"2020-06-18T01:25:49.239686Z"},"trusted":true},"cell_type":"code","source":"# Scaling with StandardScaler() class\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_num)\n\n# Looking at the first line\nX_train_scaled[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Full Pipeline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With all the pipeline steps already defined, let's put everything in a complete Pipeline.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:27:30.282165Z","end_time":"2020-06-18T01:27:30.342016Z"},"trusted":true},"cell_type":"code","source":"# Initial block code for splitting the data\ndtype_spliter = SplitDataDtype()\nX_num, X_cat = dtype_spliter.fit_transform(X_train)\nnum_features = dtype_spliter.num_features\ncat_features = dtype_spliter.cat_features\n\n# Numerical pipeline\nnum_pipeline = Pipeline([\n    ('scaler', StandardScaler())\n])\n\n# Categorical pipeline\ncat_pipeline = Pipeline([\n    ('encoder', DummiesEncoding())\n])\n\n# Full pipeline\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_features),\n    ('cat', cat_pipeline, cat_features)\n])\n\n# Applying the complete pipeline on the training set\nX_train_prep = full_pipeline.fit_transform(X_train)\n\n# Returing features\ncat_features_encoded = full_pipeline.named_transformers_['cat']['encoder'].features_after_encoding\nmodel_features = num_features + cat_features_encoded","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:28:04.446183Z","end_time":"2020-06-18T01:28:04.453163Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Result\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_train_prep: {X_train_prep.shape}')\nprint(f'Total features: {len(model_features)}')\nprint(f'\\nFirst line of X_train_prep: \\n\\n{X_train_prep[0]}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:28:21.052743Z","end_time":"2020-06-18T01:28:21.091648Z"},"trusted":true},"cell_type":"code","source":"# Applying the same pipeline for the test set\nX_test_prep = full_pipeline.fit_transform(X_test)\n\nprint(f'Shape of X_test_prep: {X_test_prep.shape}')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:28:44.514575Z","end_time":"2020-06-18T01:28:44.518536Z"},"trusted":true},"cell_type":"code","source":"# Saving everything on a prepared set to feed some homemade classes\nset_prep = {\n    'X_train_prep': X_train_prep,\n    'X_test_prep': X_test_prep,\n    'y_train': y_train,\n    'y_test': y_test\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictive Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After concluding the Data Prep, we can finally start our journey of finding the best Machine Learning model capable to predict the product subscribing chance for a given customer. Let's start with a baseline: Logistic Regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to make life easier, we will use the `BinaryBaselineClassifier()`, a homemade implementation with some useful methods for training and evaluating Machine Learning models.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:31:33.414055Z","end_time":"2020-06-18T01:31:33.418054Z"},"trusted":true},"cell_type":"code","source":"# Creating the model and a class object\nlogreg_clf = LogisticRegression()\nlogreg_tool = BinaryBaselineClassifier(logreg_clf, set_prep, model_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:31:34.827169Z","end_time":"2020-06-18T01:31:41.827634Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Defining hyperparmeters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Training the model and optimizing AUC score\nlogreg_tool.fit(rnd_search=True, param_grid=logreg_param_grid, scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating Metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:31:50.426777Z","end_time":"2020-06-18T01:31:52.517862Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Model performance\nlogreg_train_performance = logreg_tool.evaluate_performance()\nlogreg_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:32:26.890579Z","end_time":"2020-06-18T01:32:27.345576Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting confusion matrix\ntitle = 'Logistic Regression\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(5, 5))\nlogreg_tool.plot_confusion_matrix(classes, title=title)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:32:55.905342Z","end_time":"2020-06-18T01:32:56.505465Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12, 7))\nlogreg_tool.plot_roc_curve()\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:33:07.831651Z","end_time":"2020-06-18T01:33:09.39112Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting the learning curve\nlogreg_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the results gotten from Logistic Regression model, it's possible to conclude that this approach allowed a good performance. At least something expected from a baseline model. Meanwhile, it's not wrong to say that the model maybe suffer from a high bias.\n\nFor this we can:\n\n    - Collect more features;\n    - Collect more data;\n    - Train a more complex model ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on the test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:35:18.294317Z","end_time":"2020-06-18T01:35:18.561598Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Full performance with Logistic Regression\nlogreg_test_performance = logreg_tool.evaluate_performance(test=True)\nlogreg_performance = logreg_train_performance.append(logreg_test_performance)\nlogreg_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Trees","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's go ahead with our trials. On this second approach, we will use a Decision Trees algorithm.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:36:53.747148Z","end_time":"2020-06-18T01:36:53.751136Z"},"trusted":true},"cell_type":"code","source":"# Creating objects\ntree_model = DecisionTreeClassifier()\ntree_tool = BinaryBaselineClassifier(tree_model, set_prep, model_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:36:55.012184Z","end_time":"2020-06-18T01:36:57.957948Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Defining hyperparameters\ntree_param_grid = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth': [3, 5, 10, 20],\n    'max_features': np.arange(1, X_train_prep.shape[1]),\n    'class_weight': ['balanced', None],\n    'random_state': [42]\n}\n\ntree_tool.fit(rnd_search=True, scoring='roc_auc', param_grid=tree_param_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:37:09.699141Z","end_time":"2020-06-18T01:37:10.193791Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Performance\ntree_train_performance = tree_tool.evaluate_performance()\ntree_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:38:04.389378Z","end_time":"2020-06-18T01:38:05.082354Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Variables to plotting\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(10, 5))\n\n# Logistic Regression\nplt.subplot(1, 2, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(1, 2, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:38:49.852768Z","end_time":"2020-06-18T01:38:50.483114Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating a figure and calling the method for each estimator\nplt.figure(figsize=(12, 7))\n\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:38:57.488139Z","end_time":"2020-06-18T01:38:58.483068Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting the learning curve\ntree_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The numbers says that the DecisionTrees model are not so good. By the way it performed worse than our baseline (Logistic Regression). Comparing both metric by metric, the tree model have only a precision higher.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Feature Importance**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:40:24.560783Z","end_time":"2020-06-18T01:40:24.577769Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Evaluating feature importance\nfeat_imp = tree_tool.feature_importance_analysis()\nfeat_imp.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:41:17.998807Z","end_time":"2020-06-18T01:41:18.180279Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete performance\ntree_test_performance = tree_tool.evaluate_performance(test=True)\ntree_performance = tree_train_performance.reset_index().append(tree_test_performance.reset_index())\n\nall_performances = logreg_performance.reset_index().append(tree_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's try something more complex: a `RandomForestClassifier`. The basic idea of this model is to use several DecisionTrees models to have a more robust decision.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:42:50.553742Z","end_time":"2020-06-18T01:42:50.55773Z"},"trusted":true},"cell_type":"code","source":"# Creating objects\nforest_model = RandomForestClassifier()\nforest_tool = BinaryBaselineClassifier(forest_model, set_prep, model_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:43:05.361427Z","end_time":"2020-06-18T01:43:36.348143Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Defining hyperparameters\nforest_param_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [3, 5, 10, 20, 50],\n    'n_estimators': [50, 100, 200, 500],\n    'random_state': [42],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': ['balanced', None]\n}\n\nforest_tool.fit(rnd_search=True, scoring='roc_auc', param_grid=forest_param_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:43:37.758493Z","end_time":"2020-06-18T01:44:20.000278Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Model performance\nforest_train_performance = forest_tool.evaluate_performance()\nforest_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:44:22.60081Z","end_time":"2020-06-18T01:44:24.550428Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Variables for plotting\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(15, 5))\n\n# Logistic Regression\nplt.subplot(1, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(1, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(1, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:46:11.991768Z","end_time":"2020-06-18T01:46:21.608931Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure and calling the method for each estimator\nplt.figure(figsize=(12, 7))\n\n# ROC Curve for the models\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:46:23.19942Z","end_time":"2020-06-18T01:47:12.095927Z"},"trusted":true},"cell_type":"code","source":"# Plotting the learning curve\nforest_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With a better performance than the other models, the Random Forest is certainly a good candidate to be the best model for this task. Meanwhile, looking at the learning curve, it's reasonable to say that this model maybe is suffering for a high variance (common on this type of algorithm). Let's keep moving by now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T01:48:07.131393Z","end_time":"2020-06-18T01:48:16.28946Z"},"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete performance\nforest_test_performance = forest_tool.evaluate_performance(test=True)\nforest_performance = forest_train_performance.reset_index().append(forest_test_performance.reset_index())\n\nall_performances = all_performances.append(forest_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's apply something known as `Voting Classifier`. This approach represents a group of models making predictions and, after all, the final model consider the prediction of the marjority.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T23:14:44.584313Z","end_time":"2020-06-18T23:14:47.717769Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating objects\nvoting_model = VotingClassifier(\n    estimators=[('logreg', logreg_tool.trained_model), ('forest', forest_tool.trained_model)],\n    voting='soft'\n)\n\n# Training the model\nvoting_tool = BinaryBaselineClassifier(voting_model, set_prep, model_features)\nvoting_tool.fit(rnd_search=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating Metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T23:14:47.719055Z","end_time":"2020-06-18T23:15:57.909699Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Model performance\nvoting_train_performance = voting_tool.evaluate_performance()\nvoting_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T23:15:57.911881Z","end_time":"2020-06-18T23:16:13.801948Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting the matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure e calling the model\nplt.figure(figsize=(15, 10))\n\n# Logistic Regression\nplt.subplot(2, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(2, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(2, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(2, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Curva ROC**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T23:16:13.805452Z","end_time":"2020-06-18T23:16:48.861551Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure and calling the method for each estimator\nplt.figure(figsize=(12, 7))\n\n# ROC Curve for the models\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T23:16:48.864983Z","end_time":"2020-06-18T23:18:23.730497Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting the learning curve\nvoting_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results show us that joining the Logistic Regression and the Random Florest classifiers is not very effective in therms of improvement. Of course, for a good performance, this voting approach demands a high number of classifiers and, if they are diverse i.e. makes mistakes at different points, then the group performance could be improved.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-18T23:51:44.917556Z","end_time":"2020-06-18T23:52:01.452462Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete performance\nvoting_test_performance = voting_tool.evaluate_performance(test=True)\nvoting_performance = voting_train_performance.reset_index().append(voting_test_performance.reset_index())\n\nall_performances = all_performances.append(voting_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bootstrap Agregating","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"While the `Voting Classifier` makes predictions based on individual predictions of a set of classifiers, the `Bootstrap Aggregating`(or bagging) uses the **same** classifier trained repeatedly.\n\nOnce trained, the ensemble make predictions based on the aggregation of all estimatores individually.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:03:42.502423Z","end_time":"2020-06-19T00:03:47.771515Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating the bagging model based on the Random Forest Classifier\nbagging_model = BaggingClassifier(\n    RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=10, max_features='sqrt', \n                           n_estimators=50, random_state=42), \n    n_estimators=20,\n    max_samples=100,\n    bootstrap=True,\n    n_jobs=-1,\n    oob_score=True\n)\n\n# Training model\nbagging_tool = BinaryBaselineClassifier(bagging_model, set_prep, model_features)\nbagging_tool.fit(rnd_search=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:18:41.589076Z","end_time":"2020-06-19T00:20:13.972193Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Verificando performance\nbagging_train_performance = bagging_tool.evaluate_performance()\nbagging_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:04:27.357387Z","end_time":"2020-06-19T00:05:01.095866Z"},"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Plotting Confusion Matrix\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the method\nplt.figure(figsize=(15, 10))\n\n# Regressão Logística\nplt.subplot(2, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(2, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(2, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(2, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(2, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:07:24.976493Z","end_time":"2020-06-19T00:08:19.996138Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure and calling the method\nplt.figure(figsize=(12, 7))\n\n# Plotting the ROC Curve\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\n\n# Anotação\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:08:21.864039Z","end_time":"2020-06-19T00:11:25.166934Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting the learning curve\nbagging_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the Bootstrap Aggregating ensemble, we used another ensemble: Random Forest. In fact, it was used 100 estimators for the final model, but the performance was not so good. Besides the lower performance, the time spent on evaluating metrics was much higher.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:21:43.198881Z","end_time":"2020-06-19T00:22:03.130406Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete Performance\nbagging_test_performance = bagging_tool.evaluate_performance(test=True)\nbagging_performance = bagging_train_performance.reset_index().append(bagging_test_performance.reset_index())\n\nall_performances = all_performances.append(bagging_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adaptative Boosting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The concept behind Adaptatve Bosting algorithm (or simply `AdaBoost`) is to train a bunch of classifiers capable of correcting the antecessor's errors.","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:25:27.042971Z","end_time":"2020-06-19T00:25:44.013655Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Training the model\nadaboost_model = AdaBoostClassifier(\n    RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=10, max_features='sqrt', \n                           n_estimators=50, random_state=42), \n    n_estimators=20,\n    learning_rate=0.5,\n    random_state=42\n)\n\nadaboost_tool = BinaryBaselineClassifier(adaboost_model, set_prep, model_features)\nadaboost_tool.fit(rnd_search=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating Performance**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:25:52.77311Z","end_time":"2020-06-19T00:32:29.645233Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Model performance\nadaboost_train_performance = adaboost_tool.evaluate_performance()\nadaboost_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:32:31.560704Z","end_time":"2020-06-19T00:34:28.244822Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# PLotting the matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nadaboost_title = 'Adaptative Boosting\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the methods\nplt.figure(figsize=(15, 10))\n\n# Logistic Regression\nplt.subplot(2, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(2, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(2, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(2, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(2, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\n# Adaboost Classifier\nplt.subplot(2, 3, 6)\nadaboost_tool.plot_confusion_matrix(classes, title=adaboost_title, cmap=plt.cm.Purples)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:34:30.396699Z","end_time":"2020-06-19T00:36:46.133733Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure\nplt.figure(figsize=(12, 7))\n\n# Plotting the ROC Curve for each estimator\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\nadaboost_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:36:48.239044Z","end_time":"2020-06-19T00:45:27.406826Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Learning Curve\nadaboost_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:45:29.489788Z","end_time":"2020-06-19T00:46:47.81651Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete Performance\nadaboost_test_performance = adaboost_tool.evaluate_performance(test=True)\nadaboost_performance = adaboost_train_performance.reset_index().append(adaboost_test_performance.reset_index())\n\nall_performances = all_performances.append(adaboost_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:47:03.308115Z","end_time":"2020-06-19T00:47:03.676053Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating the object\ngboost_model = GradientBoostingClassifier(\n    n_estimators=20,\n    learning_rate=1.0,\n    max_depth=5, \n    max_features=15,\n    random_state=42\n)\n\n# Training the model\ngboost_tool = BinaryBaselineClassifier(gboost_model, set_prep, model_features)\ngboost_tool.fit(rnd_search=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:47:05.44089Z","end_time":"2020-06-19T00:47:14.8124Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Model Performance\ngboost_train_performance = gboost_tool.evaluate_performance()\ngboost_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:47:16.722799Z","end_time":"2020-06-19T00:49:10.032417Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nadaboost_title = 'Adaptative Boosting\\nConfusion Matrix'\ngboost_title = 'Gradient Boosting\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure and calling the methods\nplt.figure(figsize=(15, 15))\n\n# Logistic Regression\nplt.subplot(3, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(3, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(3, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(3, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(3, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\n# Adaboost Classifier\nplt.subplot(3, 3, 6)\nadaboost_tool.plot_confusion_matrix(classes, title=adaboost_title, cmap=plt.cm.Purples)\n\n# Gradient Boosting\nplt.subplot(3, 3, 7)\ngboost_tool.plot_confusion_matrix(classes, title=gboost_title, cmap=plt.cm.cool)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:49:12.191099Z","end_time":"2020-06-19T00:51:24.586179Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure\nplt.figure(figsize=(12, 7))\n\n# Plotando curva para os modelos\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\nadaboost_tool.plot_roc_curve()\ngboost_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:51:26.712992Z","end_time":"2020-06-19T00:51:40.568576Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Learning Curve\ngboost_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:47:30.985Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete performance\ngboost_test_performance = gboost_tool.evaluate_performance(test=True)\ngboost_performance = gboost_train_performance.reset_index().append(gboost_test_performance.reset_index())\n\nall_performances = all_performances.append(gboost_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:47:59.391Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Setting up a LightGBM model\ntrain_data = lgb.Dataset(X_train_prep, label=y_train)\ntest_data = lgb.Dataset(X_test_prep, label=y_test)\n\n# Parameters\nlgbm_params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\nlgbm_model = lgb.LGBMClassifier(**lgbm_params)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:48:06.655Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Training the model\nlgbm_tool = BinaryBaselineClassifier(lgbm_model, set_prep, model_features)\nlgbm_tool.fit(rnd_search=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Metrics**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:48:19.391Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Model Performance\nlgbm_train_performance = lgbm_tool.evaluate_performance()\nlgbm_train_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:49:19.291Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plotting matrices\nlogreg_title = 'LogisticRegression\\nConfusion Matrix'\ntree_title = 'DecisionTree Classifier\\nConfusion Matrix'\nforest_title = 'RandomForest Classifier\\nConfusion Matrix'\nvoting_title = 'Voting Classifier\\nConfusion Matrix'\nbagging_title = 'Bootstrap Aggregating\\nConfusion Matrix'\nadaboost_title = 'Adaptative Boosting\\nConfusion Matrix'\ngboost_title = 'Gradient Boosting\\nConfusion Matrix'\nlgbm_title = 'LightGBM\\nConfusion Matrix'\nclasses = ['No', 'Yes']\n\n# Creating figure\nplt.figure(figsize=(15, 15))\n\n# Logistic Regression\nplt.subplot(3, 3, 1)\nlogreg_tool.plot_confusion_matrix(classes, title=logreg_title)\n\n# Decision Trees\nplt.subplot(3, 3, 2)\ntree_tool.plot_confusion_matrix(classes, title=tree_title, cmap=plt.cm.Greens)\n\n# Random Forest\nplt.subplot(3, 3, 3)\ntree_tool.plot_confusion_matrix(classes, title=forest_title, cmap=plt.cm.Reds)\n\n# Voting Classifier\nplt.subplot(3, 3, 4)\nvoting_tool.plot_confusion_matrix(classes, title=voting_title, cmap=plt.cm.Greys)\n\n# Bagging Classifier\nplt.subplot(3, 3, 5)\nbagging_tool.plot_confusion_matrix(classes, title=bagging_title, cmap=plt.cm.Oranges)\n\n# Adaboost Classifier\nplt.subplot(3, 3, 6)\nadaboost_tool.plot_confusion_matrix(classes, title=adaboost_title, cmap=plt.cm.Purples)\n\n# Gradient Boosting\nplt.subplot(3, 3, 7)\ngboost_tool.plot_confusion_matrix(classes, title=gboost_title, cmap=plt.cm.cool)\n\n# LightGBM\nplt.subplot(3, 3, 8)\nlgbm_tool.plot_confusion_matrix(classes, title=lgbm_title, cmap=plt.cm.winter)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:49:48.082Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Creating figure and calling the method\nplt.figure(figsize=(12, 7))\n\n# Plotting curves\nlogreg_tool.plot_roc_curve()\ntree_tool.plot_roc_curve()\nforest_tool.plot_roc_curve()\nvoting_tool.plot_roc_curve()\nbagging_tool.plot_roc_curve()\nadaboost_tool.plot_roc_curve()\ngboost_tool.plot_roc_curve()\nlgbm_tool.plot_roc_curve()\n\n# Annotation\nplt.annotate('Área under the curve (ROC) with 50%\\n(Random model)', xy=(0.5, 0.5), xytext=(0.6, 0.4),\n             arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Learning Curve**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:49:54.069Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Learning curve\nlgbm_tool.plot_learning_curve()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluating on test set**","execution_count":null},{"metadata":{"ExecuteTime":{"start_time":"2020-06-19T00:50:15.576Z"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Complete performance\nlgbm_test_performance = lgbm_tool.evaluate_performance(test=True)\nlgbm_performance = lgbm_train_performance.reset_index().append(lgbm_test_performance.reset_index())\n\nall_performances = all_performances.append(lgbm_performance).reset_index(drop=True)\ncm = sns.light_palette('cornflowerblue', as_cmap=True)\nall_performances.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}