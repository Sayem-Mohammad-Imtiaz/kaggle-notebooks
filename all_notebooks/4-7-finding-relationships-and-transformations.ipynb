{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/applied-ml-microcourse-telco-churn'\ndata = pd.read_csv('{}/features.csv'.format(path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start by looking at our churn label.  Generally with applied machine learning tasks, we do not expect to get perfect balance between our class.  In fact, in may cases we will get very unbalanced classes as we typically model rare events such as churn, fraud, etc.  In this case, the inbalance is reasonable so we do not need to make any adjustments."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['churn'].value_counts().to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One quick way to explore the relationship between a binary classification target variable and categorical features is to group the data frame by the feature and calculate the mean of the target variable (treating churn as a numeric variable).  Note that this is only meaningful if our binary target has values of 0 or 1.  In our case, the customer has churned if the churn field is 1, so higher mean values of churn imply a larger proportion of churned users for the grouped value.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = ['gender', 'SeniorCitizen', 'Dependents', 'PaperlessBilling', 'PaymentMethod', 'Contract', \n                       'DeviceProtection', 'InternetService', 'MultipleLines', 'OnlineBackup', 'OnlineSecurity',\n                       'PhoneService', 'StreamingMovies', 'StreamingTV', 'TechSupport']\nfor column in categorical_columns:\n    display(data.groupby(column)['churn'].mean().to_frame())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the output above, we can see some relationships emerging.  For instance, customers with 'No internet service' across any of the service columns seem to have a very low proportion of churn, around 5.6%.  40% of customers with Fibre Optic internet services have churned, 40% of customers on month-to-month contracts have churned, and the churn rate is 43% for customers who pay by electronic check.  These signals seem quite strong and we should achieve a good model with this set of features.  "},{"metadata":{},"cell_type":"markdown","source":"For numeric features, a first step is often to calculate the mean value of the feature split by the churn target (treating it as a categorical variable).  "},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = ['MonthlyCharges', 'Tenure', 'MeanMonthlyCharge', 'MeanMonthlyUsage']\nfor column in numeric_columns:\n    display(data.groupby('churn')[column].mean().to_frame())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As in the previous notebook, we can also plot histograms of each numeric feature split by the churn target to see how the distributions vary. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_churn_hist(column):\n    plt.figure()\n    plt.hist(data[data['churn'] == 0][column], bins=20)\n    plt.hist(data[data['churn'] == 1][column], bins=20)\n    plt.title(column)\n\nfor column in numeric_columns:\n    plot_churn_hist(column)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature transformations for modelling\n\n### Categorical variables\n\nThere are several ways to encode categorical variables in python.  One simple way is to use the pandas method get_dummies().  Lets try this on the gender variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data['gender'].head())\ndisplay(pd.get_dummies(data['gender']).head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One consequence of one-hot encoding that can cause problems is that of collinearity - when features are correlated or have a dependence on each other. For instance, if we know that the customer in the first row of data has Female gender, we also know that this customer does not have Male gender.  We can therefore just use the encoding for either Female or Male genders and no information is lost.  In general, when we have n values we only need n-1 encoded features to capture all the information since one of the features is completely determined by the others.\n\nThe get_dummies() method has an argument to do this, drop_first, which removes the encoding for the first value in alphabetical order."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(pd.get_dummies(data['gender'], drop_first=True).head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now apply one-hot encoding with the first value dropped to the whole data frame.  We can do this by simply passing the data frame to the get_dummies() method, and all variables of type 'object' will be encoded.  Numeric variables will be unchanged.  We must first remove the customerID column or we will have a unique column for each customer (-1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_transformed = data.drop(columns='customerID')\ndata_transformed = pd.get_dummies(data_transformed, drop_first=True)\n\nprint('Number of columns before one-hot encoding:', data.shape[1])\nprint('Number of columns after one-hot encoding:', data_transformed.shape[1])\ndata_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a last step, we will add the customerID back into the data (this is fine since we haven't reordered the data set).  We will then write the output to disk to use in the next module."},{"metadata":{},"cell_type":"markdown","source":"### Scaling numeric variables\n\nWe will briefly demonstrate how to rescale variables using two different methods.  However, some machine learning models need this and some don't so we won't apply these methods to our data just yet.  \n\nWe will use the sklearn module 'preprocessing' which has a number of methods for scaling numeric variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# get an array of all the numeric variables (float64)\nnumeric_variables = data_transformed.dtypes[data_transformed.dtypes == 'float64'].index.values\n        \n# display descriptive statistics before transformations\ndisplay(data_transformed[numeric_variables].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Min Max transformation first\n\nmin_max = MinMaxScaler().fit_transform(data_transformed[numeric_variables])\nmin_max = pd.DataFrame(min_max)\nmin_max.columns = numeric_variables\ndisplay(min_max.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that the output features from a min max scaler all have min of 0 and max of 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standard Scaler transformation\n\nstd_scaler = StandardScaler().fit_transform(data_transformed[numeric_variables])\nstd_scaler = pd.DataFrame(std_scaler)\nstd_scaler.columns = numeric_variables\ndisplay(std_scaler.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that the output of the standard scaler transformation all have mean of very close to 0 (around machine precision e-16) and standard deviation of around 1. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}