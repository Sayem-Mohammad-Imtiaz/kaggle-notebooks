{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport shutil\n\nimport random\nimport pickle\n\nfrom ast import literal_eval\nfrom tqdm import tqdm as print_progress\nfrom glob import glob\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_KERAS'] = '1'\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal\nfrom tensorflow.keras.utils import plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install stellargraph","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gradient-centralization-tf","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"datasets_path = '../input/hotel-comment'\nsample_dfs = dict()\nfor dataset in ['training', 'valuating', 'testing']:\n    print(f'\\n\\n\\nProcessing {dataset} ...')\n    sample_dfs[dataset] = dd.read_csv(\n        os.path.join(datasets_path, f'{dataset}_data*.csv')).compute()\n    print(f\"{dataset}-set contains {len(sample_dfs[dataset])} samples\")\n    print(sample_dfs[dataset].sample(n=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = os.path.join(datasets_path, 'label_encoder.pkl')\nlabel_encoder = pickle.load(open(filename, 'rb'))\nlabels = list(label_encoder.classes_)\nlen(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pretrained Sentence-Transformer**","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel_version = '../input/sentence-transformers/distilUSE'\nembedder = SentenceTransformer(model_version)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef tensor_to_nparray(tensor: torch.Tensor) -> np.array:\n    return tensor.cpu().numpy() if torch.cuda.is_available() else tensor.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_vector = embedder.encode(labels, convert_to_numpy=True, output_value='token_embeddings')\nlabels_vector = [np.mean(tensor_to_nparray(l), axis=0) for l in labels_vector]\nlabels_matrix = np.vstack(labels_vector)\nlabels_matrix = np.expand_dims(labels_matrix, axis=0)\nnp.save('./labels_embeddings.npy', labels_matrix)\nlabels_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Word Embeddings**","metadata":{}},{"cell_type":"code","source":"for dataset, sample_df in sample_dfs.items():\n    if dataset != 'testing':\n        continue\n    print(f'\\n\\n\\nProcessing {dataset}-set ...')\n    dir_path = f'/kaggle/working/{dataset}'\n    if not os.path.isdir(dir_path):\n        print(f'Creating {dir_path}')\n        os.makedirs(dir_path)\n    \n    texts = sample_df.Comment.values.tolist()\n    labels = sample_df.label_encoder.values.tolist()\n    batch_size = 32\n    \n    ###########################################\n    # start_idx, end_idx = 0, batch_size*1_000\n    # texts = texts[start_idx:end_idx]\n    # labels = labels[start_idx:end_idx]\n    ###########################################\n    \n    n_samples = len(labels)\n    n_batches = n_samples//batch_size + 1\n    for b_idx in print_progress(range(n_batches)):\n        \n        # Get samples by batch\n        if b_idx != n_batches-1:\n            b_samples = texts[b_idx*batch_size:(b_idx+1)*batch_size]\n            b_labels = labels[b_idx*batch_size:(b_idx+1)*batch_size]\n        else:\n            b_samples = texts[b_idx*batch_size:]\n            b_labels = labels[b_idx*batch_size:]\n        \n        # Apply sentence-BERT for word embeddings\n        embeddings = embedder.encode(b_samples, \n                                     batch_size=batch_size,\n                                     output_value='token_embeddings',\n                                     convert_to_numpy=True,\n                                     show_progress_bar=False)\n        embeddings = [tensor_to_nparray(e) for e in embeddings]\n\n        # Apply LabelEncoder\n        labels_multiclass = []\n        for l in b_labels:\n            l = literal_eval(l) if ',' in l else [int(ch) for ch in l[1:-1].split()]\n            labels_multiclass += [np.sum(to_categorical(l, num_classes=labels_matrix.shape[-2]), axis=0)]\n        \n        # Feed data into DataFrame\n        for w_idx, (w_embs, mt_label) in enumerate(zip(embeddings, labels_multiclass)):\n            np.savez_compressed(f'{dir_path}/sample_{b_idx*batch_size+w_idx:07d}.npz', \n                                emb=w_embs, \n                                mtl=mt_label)\n            del w_embs, mt_label\n\n        del b_samples, b_labels\n        del embeddings, labels_multiclass\n        _ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\nos.chdir(r'/kaggle/working')\n\ndir_path = '/kaggle/working/testing'\n    \nshutil.make_archive(dir_path+\"data\", 'zip', dir_path)\n# FileLink(dir_path+\"data.zip\")\nshutil.rmtree('/kaggle/working/testing')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}