{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/students-performance-in-exams/StudentsPerformance.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Objective: To build a linear regression model to predict math score of a student**"},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data=data.copy()\n\narr=np.array(clean_data['math score'])\n\nq3=np.quantile(arr,0.75)\nq1=np.quantile(arr,0.25)\niqr=q3-q1\nprint(clean_data[clean_data['math score']<(q1-1.5*iqr)])\nprint(clean_data[clean_data['math score']>(q3+1.5*iqr)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no outliers in the data. Next, we will check for missing values in the data set.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there are no missing values in the data set. Now we will scale the data."},{"metadata":{},"cell_type":"markdown","source":"# Data scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"standard=preprocessing.StandardScaler()\nscaled=standard.fit_transform(clean_data[['math score','reading score','writing score']])\nscaled=pd.DataFrame(scaled,columns=['math score','reading score','writing score'])\nclean_data[['math score','reading score','writing score']]=scaled\nclean_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores have been scaled using Standard scaler. Standard scaler was chosen because the scores follow normal distribution and have no outliers. Now, we will do categorical encoding of the data as ML model does not understand string values. We will use one hot encoding technique as the values are nominal categorical and the number of parameters is relatively low."},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data=pd.get_dummies(clean_data)\nclean_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(12,12))\nsns.heatmap(clean_data.corr(),center=0,cmap='inferno',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based in heat map, we decided that math score depends on reading score, writing score, gender, type of lunch and whether test preparation was completed or not. The criteria adopt is correlation being more than 0.1. Since we observed that the model performs better by excluding the factor of test preparation."},{"metadata":{},"cell_type":"markdown","source":"# Test and train split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=clean_data[['reading score','writing score','lunch_free/reduced','lunch_standard','gender_female','gender_male']]\n#X=clean_data[['reading score','writing score','lunch_free/reduced','lunch_standard','test preparation course_completed','test preparation course_none']]\nY=clean_data['math score']\n\nfrom sklearn.model_selection import train_test_split\nX_tr,X_te,Y_tr,Y_te=train_test_split(X,Y,test_size=0.1,random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From a bit of hit and trial, we realized that the fact that whether test preparation has been completed or not does not influence the explainatory power of the model significantly (low adjusted R2 value~0.7) but removing them increases the R2 and adjusted R2 value till 0.85  This is a vast improvement!"},{"metadata":{},"cell_type":"markdown","source":"# Building Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg=LinearRegression()\nreg.fit(X_tr,Y_tr)\ncoef=reg.coef_\ny_pred=X_te['reading score']*coef[0]+X_te['writing score']*coef[1]+X_te['lunch_free/reduced']*coef[2]+X_te['lunch_standard']*coef[3]+X_te['gender_female']*coef[4]+X_te['gender_male']*coef[5]\nplt.scatter(X_te['reading score'],y_pred,color='k',label='predicted')\nplt.scatter(X_te['reading score'],Y_te,color='b',label='actual')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quality check of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nX_te=sm.add_constant(X_tr)\nmodel=sm.OLS(Y_tr,X_tr).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The R2 and adjusted R2 values are nearly equal to 0.85. This indicates absence of non-essential independent variables in the model. The model explains 85% of the actual variance in the math score.  \n\nP-value of F-test is 0 and this indicates that the multi linear regression model predicts better than intercept only model. \n\nDurbin-Watson test yields value of around 2 and this is acceptable. The model has negligible multi-collinearity. \n\nSkewness is negative and this indicates that outliers in the predicted dataset is on the lower side. It's absolute value of around 0 indicates near normal distribution with slight skewness. This is further supported by kurtosis value of around 3. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"R2 score without regularization - test data= \",r2_score(y_pred,Y_te))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge,Lasso, ElasticNet\n\nprint(\"After reqularization - train data\")\nrid=Ridge(alpha=0.05)\nrid.fit(X_tr,Y_tr)\ny_reg=rid.predict(X_tr)\nprint(\"R2 score with Ridge regression=\",r2_score(y_reg,Y_tr))\nrid=Lasso(alpha=0.05)\nrid.fit(X_tr,Y_tr)\ny_reg=rid.predict(X_tr)\nprint(\"R2 score with Lasso regression=\",r2_score(y_reg,Y_tr))\nrid=ElasticNet(alpha=0.05)\nrid.fit(X_tr,Y_tr)\ny_reg=rid.predict(X_tr)\nprint(\"R2 score with Elastic Net regression=\",r2_score(y_reg,Y_tr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lasso regression gives lowest R2 value because it diminishes the coefficients of few independent variables to zero. This leads to feature selection. However, in this case as we observed earlier that R2 and adjusted R2 are nearly equal to 0.85. So, unnecessary features hardly exist that need to be selected and removed.Thus, out of the three techniques Ridge regression would be the best choice, if that is to be applied. \n\nWe note that the model gives R2 value of 0.85 with test data and 0.83 with train data. The reduction is minimal. So,overall regularization would not be much beneficial in this case."},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{},"cell_type":"markdown","source":"**For male students:**\n\n*If free lunch is taken,*\n   Math score = 0.4 x RS + 0.5 x WS - 0.3\n\n\n*If standard lunch is taken,*\n   Math score = 0.4 x RS + 0.5 x WS + 0.5\n\n\n**For female students:**\n\n*If free lunch is taken,*\nMath score = 0.4 x RS + 0.5 x WS - 0.5\n\n\n*If standard lunch is taken,*\nMath score = 0.4 x RS + 0.5 x WS - 0.3\n\nwhere\n**RS:** standardized reading score (Z-score)\n**WS:** standardized writing score (Z-score)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}