{"cells":[{"metadata":{},"cell_type":"markdown","source":"# My first notebook\n\n## Any suggestions will be highly appreciated \n\nThe work is divided into three Sections\n\n1. Section 1 - Data Analysis (Simple Exploratory Data Analysis to gain insights)\n2. Section 2 - Data Handling (Performing imputation, scaling, etc..)\n3. Section 3 - Model fitting and predicting (Used XGBoostRegressor to fit the model)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.pandas.set_option('display.max_columns', None)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SECTION 1 - Data Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/hackerearth-machine-learning-exhibit-art/dataset/train.csv')\ntest = pd.read_csv('/kaggle/input/hackerearth-machine-learning-exhibit-art/dataset/test.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Could have worked with date columns but dropped it anyway\ndataset.drop(['Customer Id', 'Artist Name' ,'Customer Location', 'Scheduled Date', 'Delivery Date'], axis=1, inplace=True)\ntest.drop(['Customer Id', 'Artist Name' ,'Customer Location', 'Scheduled Date', 'Delivery Date'], axis=1, inplace=True)\ndataset['Cost'] = dataset['Cost'].abs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data Exploration "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting all columns with missing data\nmissing_data_col = [col for col in dataset.columns if dataset[col].isnull().sum() > 0]\nmissing_data_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[missing_data_col].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try to see how cost gets affected on columns with missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset[missing_data_col].copy()\ndf.fillna(\"-1\", inplace=True)\nfor col in missing_data_col:\n    df[col] = df[col].apply(lambda x: 'Missing' if x=='-1' else 'Available')\n    \ndf['Cost'] = dataset['Cost']\n\nfor col in missing_data_col:\n    df.groupby(col)['Cost'].median().plot.bar()\n    plt.xlabel(col)\n    plt.ylabel('Cost')\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The median cost of missing data values and the median cost of available data values for the missing data columns do not vary by much"},{"metadata":{},"cell_type":"markdown","source":"# Working with the numerical columns to get some insights about outliers and what role does it play in the cost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install dtale\n# import dtale\n# # Getting all the numerical columns\n# numerical_cols = [feature for feature in dataset.columns if dataset[feature].dtypes != 'O']\n\n# df = dataset[numerical_cols].copy()\n# details = dtale.show(df)\n# details","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note - dtale doesn't works well with kaggle notebook so the below conclusions were drawn from using dtale on Jupyter Notebook \n## Following conclusions are to be drawn\n#### 1. Height is moderately skewed (0.59)\n#### 2. Width is highly skewed (1.55)\n#### 3. Weight is highly skewed (21.56)\n#### 4. Price of Sculpture is highly skewed (22.21)\n#### 5. Base shipping price is moderately skewed (0.92)\n#### 6. Cost is highly skewed (29.87)"},{"metadata":{},"cell_type":"markdown","source":"\n# Looking for outliers in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = [feature for feature in dataset.columns if dataset[feature].dtypes != 'O']\n\ndf = dataset[numerical_cols].copy()\ndf.fillna(0, inplace=True)\n\nfor col in df.columns:\n    plt.boxplot(col, data=df)\n    plt.xlabel(col)\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# High outliers have led to high skewness in these columns"},{"metadata":{},"cell_type":"markdown","source":"# Check for categorical data and finding insights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting all the categorical columns\ncategorical_cols = [feature for feature in dataset.columns if dataset[feature].dtypes == 'O']\ncategorical_cols\n\ndf = dataset[categorical_cols].copy()\ndf[\"Cost\"] = dataset['Cost']\n\nfor col in df.columns[:-1]:\n    df.groupby(col)['Cost'].median().plot.bar()\n    plt.xlabel(col)\n    plt.ylabel('COST')\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### NOTE - Not considering mean as the computing factor as mean is influneced by outliers\n### Some insights are\n##### 1. Marble and stone products are costly followed by brass and bronze\n##### 2. International column has almost no impact on cost\n##### 3. Express shipment leads to a slight increment in the cost\n##### 4. Installation included has almost no to very little impact on cost\n##### 5. Transport has very slight impact on cost (airways being the costliest and waterways being the cheapest\n#### 6. Fragility has a high impact on cost (Not fragile = More cost)\n##### 7. Customer information has slight impact on cost\n##### 8. Remote location has almost no impact on cost"},{"metadata":{},"cell_type":"markdown","source":"# Checking the corelation of numerical columns w.r.t Cost"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.corr()['Cost']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SECTION 2 - Data Handling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dataset.iloc[:, :-1]\ny_train = dataset['Cost']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  The y values are highly skewed so we need to normalize them. \nWe can perform antilog to get back the original results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.apply(lambda x: np.log(x) if x>0.0 else 0.0)\ny_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find all the numberical cols with missing data\nnum_missing_cols = [col for col in dataset.columns if dataset[col].dtypes!='O' and dataset[col].isnull().sum() > 0]\nnum_missing_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset.copy()\nfor col in num_missing_cols:\n    mean = df[col].mean()\n    median = df[col].median()\n    print(f\"{col} has a mean of {round(mean,2)} and a median of {median}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_misisng_cols(col_name, df):\n    for col in col_name:\n        med = df[col].median()\n        df[col].fillna(med, inplace=True)\n        \nfill_misisng_cols(col_name=num_missing_cols, df=X_train)\nfill_misisng_cols(col_name=num_missing_cols, df=test)\n\n\nX_train[num_missing_cols].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get all the categorical columns to fill missing values and also do other transformations if possible"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_missing_cols = [col for col in dataset.columns if dataset[col].dtypes=='O' and dataset[col].isnull().sum()>0]\ncat_missing_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the categorical columns with missing data with the mode of these columns\n\ndef cat_miss(df):\n    for col in cat_missing_cols:\n        mode = dataset[col].mode()[0]\n        df[col].fillna(mode, inplace=True)\n\ncat_miss(X_train)\ncat_miss(test)\nX_train[cat_missing_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding using pd.get_dummies()\nX_train = pd.get_dummies(X_train, drop_first=True)\ntest = pd.get_dummies(test, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SECTION 3 - Model Fitting and Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Height', 'Width', 'Weight', 'Price Of Sculpture' needs to be scaled down as they have high outliers\nnum_cols_all = [col for col in X_train.columns if X_train[col].dtypes!='O' and X_train[col].nunique() > 10]\nnum_cols_all\nscale_cols = num_cols_all[1:-1]\nscale_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\n\nX_train[scale_cols] = scale.fit_transform(X_train[scale_cols])\ntest[scale_cols] = scale.transform(test[scale_cols])\n\nX_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the useful parameters "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### At the moment we will take alpha as 0.001 and select the columns generated.\nBased on results optained we may update alpha"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_selection_model = SelectFromModel(Lasso(alpha=0.001))\nfeature_selection_model.fit(X_train, y_train)\nf_model_arr = list(feature_selection_model.get_support())\n\nall_cols = [i for i in X_train.columns]\ncols_chosen = [all_cols[i] for i in range(len(f_model_arr)) if f_model_arr[i] == True]\n# cols_chosen # Chosen columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[cols_chosen]\ntest = test[cols_chosen]\n\nX_train = X_train.values\ntest = test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using XGBoost to fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxg = XGBRegressor()\n\nxg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Performing the Grid Search to get the best parameters\n# from sklearn.model_selection import GridSearchCV\n\n# param_grid = [\n# {'n_estimators': [250,280,300,330, 360], \n#  'max_depth': [10, 20,30,40,50],\n#  'learning_rate': [0.1,0.3,0.5],\n# }]\n\n# grid_cv = GridSearchCV(xg, param_grid=param_grid, cv=10, n_jobs=-1)\n# grid_cv.fit(X_train, y_train)\n\n# best_params = grid_cv.best_estimator_\n# best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Best Parameters Were found out to be \nn_estimators=250\nmax_depth=10 \nlearning_rate=0.1"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg1 = XGBRegressor(n_estimators=250, n_jobs=-1, max_depth=10, base_score=0.1, learning_rate=0.1)\nxg1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = xg1.predict(test)\npredicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The 'Cost' column in the train dataset was log normalised so we need to antilog the predicted values"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = np.power(np.e, predicted)\npredicted = np.round(predicted, 2)\npredicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/hackerearth-machine-learning-exhibit-art/dataset/test.csv')\n\nid_col = pd.DataFrame(test['Customer Id'], columns=['Customer Id'])\ncost_col = pd.DataFrame(predicted, columns=['Cost'])\nresult = pd.concat([id_col, cost_col], axis=1)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('Submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}