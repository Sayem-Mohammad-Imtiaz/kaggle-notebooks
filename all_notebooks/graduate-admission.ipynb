{"cells":[{"metadata":{"trusted":false,"_uuid":"1d3a1a3ec8aac0d2591520454f78c2655a50f685"},"cell_type":"code","source":"#import libraries\n\nimport warnings; warnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom time import time\n\nimport sklearn\n\n# Regression /Classification\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#Building everything\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n\n# This allows to apply plots in the notebook\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8, 6)\n#plt.rcParams['font.size'] = 10\n#plt.style.use(\"fivethirtyeight\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b1ac952d1f82bd23e4f339dcfc5e83b3a4b2e41"},"cell_type":"code","source":"# Read the data file  and load into dataframe.\ndata = pd.read_csv('../input/Admission_Predict_Ver1.1.csv')\n# print the first 5 rows of the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2813aa301aa031173d26347d630722a2ae6b6daa"},"cell_type":"code","source":"#remove serial no.\ndata.drop(columns = 'Serial No.', inplace = True)\n# print the first 5 rows of the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fedc2170b16aaf6666c8135fdefc84d8d48fc2c2"},"cell_type":"code","source":"# Check the null values - We can see that there are no missing values in the data set\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"829aeb535fc86b4220c40424b2c26a0078140278"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a35d2d8832b916d63677ab98d7cb0503b8ec471e"},"cell_type":"code","source":"# Find correlation between different features\n#Correlation is a statistical technique that can show whether and how strongly pairs of variables are related.\nplt.figure(figsize=(15,7));\nsns.heatmap(data.corr(), annot=True, cmap=\"YlGnBu\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7c46df5463c1fe40b53058639795861b767d2f5c"},"cell_type":"code","source":"#lets pick  GRE,TOEFL and CGPA as 3 factors maily affecting Chances of admit\n#X=data[['GRE Score','TOEFL Score','CGPA']]\n\n#When dataset is small, it is better to use all given features for better prediction\nX=data.iloc[:,:-1]\ny=data.iloc[:,-1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"780f5c49037f1b7608f6c4bf04648635ff00e000"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"abeaeda1001652f2a2ad0d94977416ba8a9e5a72"},"cell_type":"code","source":"#chance of admit is given as percentage/100, values are continuous \n#Linear regression is the best option,still lets check for all available regressors\n#Solving it as a regression problem\n#Check at once for all available regressors\n\nheldout = [0.90, 0.80, 0.70, 0.60]\ntime_arr=[]\naccuracy_arr=[]\nrounds = 50\n# (\"SGD\", SGDClassifier(), \"aqua\"),\nprint(\"Regressor Techniques\")\nclassifiers = [\n           (\"Linear Regression\", LinearRegression(),\"red\"),\n           (\"Random Forest Regression\", RandomForestRegressor(),\"blue\"),\n           (\"Decision Tree Regression\", DecisionTreeRegressor(),\"green\"),\n           (\"KNN Regression\", KNeighborsRegressor(),\"yellow\" )]\n\nxx = 1. - np.array(heldout)\n\nfor name, clf,color in classifiers:\n    print(\"Training %s\" % name)\n    yy = []\n    \n    for i in heldout:\n        t0 = time()\n        yy_ = []\n        for r in range(rounds):\n            X_train, X_test, y_train, y_test = \\\n                train_test_split(X, y, train_size=i, random_state=42)\n            \n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            \n        time_=time()-t0\n        time_arr.append(time_)\n        accuracy_arr.append(clf.score(X_test,y_test))\n        yy.append(np.mean(yy_))\n        print('Testing Accuracy: %f\\tTime: %.2fs' % (clf.score(X_test,y_test),time_)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9b4c10abab978b6a63f56b0b0052c9039e144bd6"},"cell_type":"code","source":"## Accuracy comparison graph\nlr=accuracy_arr[0:4]\nrf=accuracy_arr[4:8]\ndt=accuracy_arr[8:12]\nknnr=accuracy_arr[12:16]\n\n\nacc=[(\"LR\",lr,\"red\"),\n     (\"RF\",rf,\"blue\"),\n     (\"DT\",dt,\"green\"),\n     (\"KNNR\",knnr,\"yellow\")]\n\nfor name, class_,color in acc:\n    l = [class_ * 100 for class_ in class_]\n    plt.plot(xx,l, label=name, color=color)\n    \nmy_xticks = ['10%','20%','30%','40%','50%']\nplt.xticks(xx, my_xticks)\nplt.legend(bbox_to_anchor=(1, 1),\n           bbox_transform=plt.gcf().transFigure)\nplt.xlabel(\"Training data %\")\nplt.ylabel(\"Accuracy %\")\nplt.title(\"Accuracy Comparison of Classifiers without DR\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6a92929b9f4d8a5978a23acf3014d00d15fbaeef"},"cell_type":"code","source":"# Time comparison graph\nlr=time_arr[0:4]\nrf=time_arr[4:8]\ndt=time_arr[8:12]\nknnr=time_arr[12:16]\n\nt=[(\"LR\",lr,\"red\"),\n     (\"RF\",rf,\"blue\"),\n     (\"DT\",dt,\"green\"),\n     (\"KNNR\",knnr,\"yellow\")]\n\nfor name, class_,color in t:\n    plt.plot(xx,class_, label=name, color=color)\n\nmy_xticks = ['10%','20%','30%','40%','50%']\nplt.xticks(xx, my_xticks)\nplt.legend(bbox_to_anchor=(1, 1),\n           bbox_transform=plt.gcf().transFigure)\nplt.xlabel(\"Training data %\")\nplt.ylabel(\"Time in seconds\")\nplt.title(\"Time Comparison of Classifiers without DR\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b840825b76ce0625480c3df23af987d6c270f232"},"cell_type":"code","source":"# Train/Test Split with 70/30\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.70, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a21f03d9e2d2c9c52bf38137a4fc88fc7349c31e"},"cell_type":"code","source":"#LR gives better accuracy and takes less time\n#And also for such data, it is always adviceable to start with something simple:)\n#lets pick Linear Regression at 70/30 Split and tune it to get better accuracy\n## scikit-learn's Four-Step Modeling Pattern\n\n# Make an instance of a LinearRegression object.\nlr = LinearRegression(normalize=False) #we have scalarized\n## Fit the model with data (aka \"model training\").\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\nscore = lr.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bcd6acc60e5b49a54cc41d4bb993a02cc462e211"},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c8f8326b935208d9101ade73aee96add8532e881"},"cell_type":"code","source":"#Lets turn this to a classification model\n# Converting continuous variable into categorical value\ny_ = [1 if each > 0.8 else 0 for each in y]\n\ny_ = np.array(y_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ca8a7dce7cf768edcdc3590c94cb7bef8f35f42d"},"cell_type":"code","source":"time_arr_c=[]\naccuracy_arr_c=[]\nrounds = 50\n\nprint(\"Classification\")\nclassifiers = [\n           (\"Logistic Regression\", LogisticRegression(),\"red\"),\n           (\"Random Forest \", RandomForestClassifier(),\"blue\"),\n           (\"Decision Tree\", DecisionTreeClassifier(),\"green\"),\n           (\"KNN\", KNeighborsClassifier(),\"yellow\" )]\n\nxx = 1. - np.array(heldout)\n\nfor name, clf,color in classifiers:\n    print(\"Training %s\" % name)\n    yy = []\n    \n    for i in heldout:\n        t0 = time()\n        yy_ = []\n        for r in range(rounds):\n            X_train, X_test, y_train, y_test = \\\n                train_test_split(X, y_, train_size=i, random_state=42)\n            \n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            \n        time_=time()-t0\n        time_arr_c.append(time_)\n        accuracy_arr_c.append(clf.score(X_test,y_test))\n        yy.append(np.mean(yy_))\n        print('Testing Accuracy: %f\\tTime: %.2fs' % (clf.score(X_test,y_test),time_)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1b31164afc1afeb57a749be799311a29ee3f8949"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}