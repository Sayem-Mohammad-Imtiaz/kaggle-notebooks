{"cells":[{"metadata":{"id":"66jJliTSo58e"},"cell_type":"markdown","source":"# 1. Importing and Data Loading\nAuthor: Tanay Mehta\n* LinkedIn: https://www.linkedin.com/in/tanaymehta28/\n* Blog: https://tanaymehta.codes\n* Website: http://tanaymehta.codes\n\nNote: I am using Dask rather than pandas because it's faster on bigger datasets","execution_count":null},{"metadata":{"id":"VEkoldl_pIVQ","trusted":true},"cell_type":"code","source":"! pip install --quiet chart-studio","execution_count":null,"outputs":[]},{"metadata":{"id":"1pTQU0v6o58g","outputId":"1bf6352b-35da-40be-b223-cdc39b936a0e","trusted":true},"cell_type":"code","source":"import numpy as np\nimport dask as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport fastai\nfrom fastai import *\nfrom fastai.text import *\nimport os\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\nimport re","execution_count":null,"outputs":[]},{"metadata":{"id":"HIQqPrnKo58k","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"id":"MOw7QeAAo58o","outputId":"3b9e62e3-2c10-41ff-8138-bb3cfd053ca9","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"7RW5C96Ko58s"},"cell_type":"markdown","source":"# 2. EDA and Data Preprocessing\nThe Dataset consists of many features, however every features isn't of importance to sentiment classification. There is also a very significant presence of NuLL values in both important and non-important features.\nFor this, I am just taking 2 features which are the most important ones; `text` and `airline_sentiment` (target)","execution_count":null},{"metadata":{"id":"CfBGaYIto58s","outputId":"8cec890b-4bdd-4262-e6d4-a90377ebd32a","trusted":true},"cell_type":"code","source":"train_data = data[['airline_sentiment', 'text']]\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"dRX5XWl9o58v","outputId":"8c6e313b-2359-4c7e-d81a-b30a1546cabe","trusted":true},"cell_type":"code","source":"train_data.info()\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"FI3NlfQjo58y"},"cell_type":"markdown","source":"### 2.1 Missing Values\nAs seen below, there aren't any missing values in the dataset","execution_count":null},{"metadata":{"id":"J6jZ0mMMo58z","outputId":"78978b7f-4c6f-4a37-a584-2e6187c48445","trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"XVM9xSCzo581"},"cell_type":"markdown","source":"### 2.2 Target Values\nLet's now look at the distribution of target values (`airline_sentiment`)","execution_count":null},{"metadata":{"id":"4SRRuaGWo583","outputId":"6fd9c667-feec-4894-f2b6-71e9734afc39","trusted":true},"cell_type":"code","source":"vals = [len(train_data[train_data['airline_sentiment']=='negative']['airline_sentiment']), len(train_data[train_data['airline_sentiment']=='positive']['airline_sentiment']), len(train_data[train_data['airline_sentiment']=='neutral']['airline_sentiment'])]\nidx = ['negative', 'positive', 'neutral']\nfig = px.pie(\n    train_data,\n    names='airline_sentiment',\n    title='Target Value Distribution Chart',\n    height=400,\n    width=750\n)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"j9TZLpRmo588"},"cell_type":"markdown","source":"### 2.3 Character frequency Count\nAlso, let's count the frequency of characters in the data","execution_count":null},{"metadata":{"id":"bL2xrr8eo588","outputId":"2726a47e-c10e-43ae-9b6d-c1e350d6f87c","trusted":false},"cell_type":"code","source":"neg = train_data[train_data['airline_sentiment']=='negative']['text'].str.len()\npos = train_data[train_data['airline_sentiment']=='positive']['text'].str.len()\nneu = train_data[train_data['airline_sentiment']=='neutral']['text'].str.len()\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\n\nfig.update_layout(height=400, width=800, title_text=\"Character Count\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"QAjW_EtZo58_"},"cell_type":"markdown","source":"### 2.4 Word Count Distribution","execution_count":null},{"metadata":{"id":"xuL6DpPJo58_","outputId":"69a91d7d-24bf-473d-e831-595df7adbbde","trusted":false},"cell_type":"code","source":"neg = train_data[train_data['airline_sentiment']=='negative']['text'].str.split().map(lambda x: len(x))\npos = train_data[train_data['airline_sentiment']=='positive']['text'].str.split().map(lambda x: len(x))\nneu = train_data[train_data['airline_sentiment']=='neutral']['text'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.update_layout(height=500, width=850, title_text=\"Word Count\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"tIWzTjt0o59C"},"cell_type":"markdown","source":"### 2.5 Average Word Length Distribution\nOn an average how many words are present in each set","execution_count":null},{"metadata":{"id":"VzCqHYDNo59D","outputId":"3b92b17b-ff9e-4bd1-815b-b6adb2093bc8","trusted":false},"cell_type":"code","source":"neg = train_data[train_data['airline_sentiment']=='negative']['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\npos = train_data[train_data['airline_sentiment']=='positive']['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\nneu = train_data[train_data['airline_sentiment']=='neutral']['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\n\n\nfig = ff.create_distplot([neg, pos, neu], ['Negative', 'Positive', 'Neutral'])\nfig.update_layout(height=500, width=800, title_text=\"Average Word Length Distribution\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"n3u_Pzkjo59G"},"cell_type":"markdown","source":"### 2.6 Unqiue Word Count Distribution\nHow many unique words are in each set","execution_count":null},{"metadata":{"id":"Ze7DEoODo59G","outputId":"14b1f303-276f-42f3-a019-e2ea6008ea9c","trusted":false},"cell_type":"code","source":"neg = train_data[train_data['airline_sentiment']=='negative']['text'].apply(lambda x: len(set(str(x).split()))).to_list()\npos = train_data[train_data['airline_sentiment']=='positive']['text'].apply(lambda x: len(set(str(x).split()))).to_list()\nneu = train_data[train_data['airline_sentiment']=='neutral']['text'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([neg, pos, neu], ['Negative', 'Positive', 'Neutral'])\nfig.update_layout(height=500, width=800, title_text=\"Unique Word Count Distribution\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"m_BFuopAo59L"},"cell_type":"markdown","source":"### 2.7 URL Count\nHow many URLs are present in each tweet","execution_count":null},{"metadata":{"id":"Thma76s9o59M","outputId":"c0fb7904-8abe-4e94-a588-d761ef4fda6e","trusted":false},"cell_type":"code","source":"neg = train_data[train_data['airline_sentiment']=='negative']['text'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\npos = train_data[train_data['airline_sentiment']=='positive']['text'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\nneu = train_data[train_data['airline_sentiment']=='neutral']['text'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.update_layout(height=500, width=850, title_text=\"URL Count\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"28eh0mbSo59P"},"cell_type":"markdown","source":"### 2.8 Word Cloud\nFinally, we make a word cloud of the most appearing words in negative, positive and neutral categories","execution_count":null},{"metadata":{"id":"ij80reuco59P","outputId":"a1d635b7-39e9-4ae2-d38a-a02caccb4bf1","trusted":false},"cell_type":"code","source":"negative = \" \".join(train_data[train_data['airline_sentiment'] == 'negative']['text'].to_list())\npositive = \" \".join(train_data[train_data['airline_sentiment'] == 'positive']['text'].to_list())\nneutral = \" \".join(train_data[train_data['airline_sentiment'] == 'neutral']['text'].to_list())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,15))\nng_wlc = WordCloud(width=256, height=256, collocations=False).generate(negative)\nps_wlc = WordCloud(width=256, height=256, collocations=False).generate(positive)\nne_wlc = WordCloud(width=256, height=256, collocations=False).generate(neutral)\nwcs = [ng_wlc, ps_wlc, ne_wlc]\ntitls = [\"Negative Tweets\", \"Positive Tweets\", \"Neutral Tweets\"]\n\nfor num, el in enumerate(wcs):\n    ax[num].imshow(el)\n    ax[num].axis('off')\n    ax[num].set_title(titls[num])","execution_count":null,"outputs":[]},{"metadata":{"id":"RB22gV9po59S"},"cell_type":"markdown","source":"### 2.9 Cleaning and Tokenizing\nLet's just clean the data (by removing the hyperlinks and other unwanted elements","execution_count":null},{"metadata":{"id":"nX6Byy_xo59T","trusted":false},"cell_type":"code","source":"# Get the stopwords\nst_wrds = stopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"id":"453HrQndo59V","outputId":"be586339-233e-4a0e-8762-52a8b01d3b71","trusted":false},"cell_type":"code","source":"# Remove everything except basic text characters\ntrain_data['text'] = train_data['text'].str.replace(\"[^a-zA-Z]\", \" \").str.lower()\ntrain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"lkn1yS4Fo59X","trusted":false},"cell_type":"code","source":"# Tokenizing the data\ntokenized_data = train_data['text'].apply(lambda x: x.split())\ntokenized_data = tokenized_data.apply(lambda x: [word for word in x if word not in st_wrds])","execution_count":null,"outputs":[]},{"metadata":{"id":"R4nUbT98o59a","outputId":"1f99bf88-5e21-436e-d93a-11daef2f7923","trusted":false},"cell_type":"code","source":"tokenized_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"_M63zjb2o59d","outputId":"b2783152-51a2-431a-8e5b-0448ab81fe67","trusted":false},"cell_type":"code","source":"# Replace the normal text with tokenized text\ntok = []\nfor i in range(len(train_data)):\n    t = ' '.join(tokenized_data[i])\n    tok.append(t)\ntrain_data['text'] = tok\ntrain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"Zq5g24lA5vk_","trusted":false},"cell_type":"code","source":"# Change the column name and encode the labels\ntrain_data = train_data.rename(columns={'airline_sentiment':'label'})\ntrain_data['label'] = train_data['label'].apply(lambda x: 0 if x=='negative' else (1 if x=='positive' else 2))","execution_count":null,"outputs":[]},{"metadata":{"id":"c-hgj2MPo59h","trusted":false},"cell_type":"code","source":"# Let us now split the dataset into training and validation sets\nsplit_pcent = 0.15  # How much percent of data should go into testing set\nsplit = int(split_pcent * len(train_data))\n\nshuffled_set = train_data.sample(frac=1).reset_index(drop=True)   # Shuffle the data\nvalid_set = shuffled_set[:split]   # Get everything till split number\ntrain_set = shuffled_set[split:]   # Get everything after split number","execution_count":null,"outputs":[]},{"metadata":{"id":"R-gajJTzo59j","outputId":"9740233d-405e-462c-e6bf-0d148f26dbde","trusted":false},"cell_type":"code","source":"# Make a Language Model Data Bunch from our train set\ndata_bunch = TextLMDataBunch.from_df(train_df=train_set, valid_df=valid_set, path=\"\")","execution_count":null,"outputs":[]},{"metadata":{"id":"BKXtPOYJo59l","outputId":"0d972d6b-4577-46e3-d4b1-bb4108575e6f","trusted":false},"cell_type":"code","source":"# Make the data classifier\ndata_clf = TextClasDataBunch.from_df(path=\"\", train_df=train_set, valid_df=valid_set, vocab=data_bunch.train_ds.vocab, bs=16)","execution_count":null,"outputs":[]},{"metadata":{"id":"KIR-E2n4o59n"},"cell_type":"markdown","source":"# 3. Training the Model\n### 3.1 Vanilla Training\nLet's first train the model out of the box (we'll get the learning rate and then fit the model for 2 epochs using that learning rate). Although, it won't perform well.","execution_count":null},{"metadata":{"id":"uE_jePaOo59o","outputId":"4aee7228-1c08-49a1-d9ba-5381dc4906ec","trusted":false},"cell_type":"code","source":"# Define the language learner model and fit for one epoch\nlearner = language_model_learner(data_bunch, arch=AWD_LSTM, drop_mult=0.5)\n\nlearner.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"id":"JaLuJ7htogGk"},"cell_type":"markdown","source":"### 3.2 Unfreezing Layers\nTill now, the internal layers were not training as they were freezed. Let's now train our model by un-freezing layer after layer.","execution_count":null},{"metadata":{"id":"EeNfp2haps7H","outputId":"067b54ce-bd48-4981-d730-57848c85227d","trusted":false},"cell_type":"code","source":"# Try unfreezing last 3 layers first\nlayers_to_unfreeze = [1, 2, 3]\nfor i in layers_to_unfreeze:\n    learner.freeze_to(-i)\n    learner.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZSfasVxT3XfN","outputId":"a3ebc651-ce90-4b1e-e64d-4c304814acad","trusted":false},"cell_type":"code","source":"# Now let's unfreeze all layers and train them\nlearner.unfreeze()\nlearner.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"id":"WUwgnn_O4KWP","trusted":false},"cell_type":"code","source":"learner.save_encoder('learn_encoder')","execution_count":null,"outputs":[]},{"metadata":{"id":"OvXQeFti7nd0"},"cell_type":"markdown","source":"### 3.3 Training the Classifier\nLet's just now train the classifier using the encoder we trained on the data.","execution_count":null},{"metadata":{"id":"QQMCHc8H4IOn","trusted":false},"cell_type":"code","source":"clf = text_classifier_learner(data_clf, arch=AWD_LSTM, drop_mult=0.5)\nclf.load_encoder('learn_encoder')","execution_count":null,"outputs":[]},{"metadata":{"id":"yVEfnmZS4VtI","outputId":"8648530a-c272-467f-9443-6f686b322fea","trusted":false},"cell_type":"code","source":"clf.fit_one_cycle(1, 1e-2)","execution_count":null,"outputs":[]},{"metadata":{"id":"MtCqFV1b5RbS","outputId":"8d6d6cb6-66dc-4a8d-cfed-136cf647bf4c","trusted":false},"cell_type":"code","source":"# Let's unfreeze all it's layers and train it.\nclf.unfreeze()\nclf.fit_one_cycle(1)","execution_count":null,"outputs":[]},{"metadata":{"id":"vrpb8zjV62DE"},"cell_type":"markdown","source":"### 3.3 Discriminative Fine Tuning\nSince the AWD_LSTM architecture consists of 3-Stacked LSTMs, the level of information captured by every layer will be more and more complex the deeper we go.\nFor this, it makes sense to use different learning rates for different layers.\nSo, Let us try discriminative fine tuning by un-freezing and fine tuning with custom learning rates","execution_count":null},{"metadata":{"id":"EorttK-G89oO","outputId":"015ab314-299b-4b53-b65c-393f5c07fbd8","trusted":false},"cell_type":"code","source":"# Unfreeze last layer and give it a learning rate range using `slice()` function\n# This way it'll use the learning rates from 5e-3/2->5e-3 (i.e: 0.0025 -> 0.005)\nclf.freeze_to(-1)\nclf.fit_one_cycle(1, slice(5e-3/2., 5e-3))","execution_count":null,"outputs":[]},{"metadata":{"id":"-ilUfcae8xhQ","outputId":"f1af0156-bd74-43d9-cc70-64cd64189a8a","trusted":false},"cell_type":"code","source":"# No let's unfreeze all the layers and try DFT again\nclf.unfreeze()\nclf.fit_one_cycle(1, slice(2e-3/100, 2e-3))","execution_count":null,"outputs":[]},{"metadata":{"id":"745iV4_n-J7V"},"cell_type":"markdown","source":"# 4. Testing and Conclusion\nAfter all the different learning rates and methods we used, the best accuracy we got was `0.811`.\nLet's try and predict a few sentences using the classifier we trained.\n\nKeep in mind about the labels and we encoded them in Data Preprocessing.\n* 0 = Negative\n* 1 = Positive\n* 2 = Neutral","execution_count":null},{"metadata":{"id":"yovT6mVU9vSc","outputId":"893c3335-0180-4abc-ec4f-1db774155841","trusted":false},"cell_type":"code","source":"# The Classifier classifies it Neutral, which is right\nclf.predict(\"Hello, how are you doing?\")","execution_count":null,"outputs":[]},{"metadata":{"id":"4fGHaG8T-k-a","outputId":"3b197110-6fed-4ec1-c711-3ae505d5aba6","trusted":false},"cell_type":"code","source":"# The Classifier classifier it Negative, which is right\nclf.predict(\"Wow, the flight duration was boring and the passenger treatement was not the best I have seen!\")","execution_count":null,"outputs":[]},{"metadata":{"id":"vbNgerBh_AoD","outputId":"0fcc584b-236e-4817-a94e-02dc4ef94d50","trusted":false},"cell_type":"code","source":"# The Classifier classifier it Positive, which is right\nclf.predict(\"Great service and good staff, I would recommend it!\")","execution_count":null,"outputs":[]},{"metadata":{"id":"gWVEZZbd_lcT"},"cell_type":"markdown","source":"#### Improvements\n* Since the data is a bit imbalanced, Accuracy won't be the best bet in the long run. We could also use Precision, Recall and F1 Score.\n* Further exploration of Discriminative Fine Tuning would be surely helpful to obtain better results.\n* Better Data Preprocessing (like using Porter-Stemming Algorithm), using other vocab dictionary and better data tokenization techniques will further increase the accuracy.\n* Using other models such as BERT, would achieve a better accuracy at classifying tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}