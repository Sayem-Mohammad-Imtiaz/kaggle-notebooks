{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nimport category_encoders as ce\nfrom sklearn.preprocessing import OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the data\ndf_train = pd.read_csv('../input/torontohousingprices/dataset_final.csv')\ndf_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyze 'Sold_price' ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.f' % x)\n\n#Descriptive statistics summary\ndf_train.describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sold Price histogram\nsns.distplot(df_train['Sold_price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'Sold_price' Relationship with Numerical Variables","metadata":{}},{"cell_type":"code","source":"#Scatter plot 'Squarefootage'/'Sold_price'\nvar = 'Squarefootage'\ndata = pd.concat([df_train['Sold_price'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='Sold_price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 'Sold_price' Relationship with Location Variables","metadata":{}},{"cell_type":"code","source":"print('There are', df_train['Community'].nunique(), 'communities in',\n       df_train['Municipality District'].nunique(), 'districts.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get average 'Sold_price' across communities (neighbourhoods)\ncommunities = df_train[['Sold_price','Community']].groupby(['Community'])\n\ncommunity_prices = pd.concat([communities.mean(), communities.count()], axis=1)\ncommunity_prices.columns = ['Avg. Price', 'Count']\ncommunity_prices_sorted = community_prices.sort_values(by=['Avg. Price'], ascending=False)\ncommunity_prices_sorted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"community_prices_sorted.plot.bar(figsize=(24,6))\ncommunity_prices_sorted['Count'].plot(kind='bar', color='Orange', secondary_y=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get average 'Sold_price' across districts\ndistricts = df_train[['Sold_price','Municipality District']].groupby(['Municipality District'])\n\ndistricts_prices = pd.concat([districts.mean(), districts.count()], axis=1)\ndistricts_prices.columns = ['Avg. Price', 'Count']\ndistricts_prices_sorted = districts_prices.sort_values(by=['Avg. Price'], ascending=False)\ndistricts_prices_sorted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"districts_prices_sorted.plot.bar(figsize=(24,6))\ndistricts_prices_sorted['Count'].plot(kind='bar', color='Orange', secondary_y=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Communities with <5 data points\ncommunity_prices_sorted[community_prices_sorted['Count'] <= 15].sort_values(by=['Count'], ascending=True).to_csv(\"./output\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get map\nneighbourhood_map = gpd.read_file('../input/folder/forAnalysis/Neighbourhoods/Neighbourhoods.shp')\nneighbourhood_map['neighbourhood'] = neighbourhood_map['FIELD_7'].str.replace(' \\(.+\\)', '').str.lower()\n\n#merge data\ncommunity_prices['Neighbourhood'] = community_prices.index.str.lower()\ncommunity_prices.reset_index(drop=True, inplace=True)\n\nmerged = neighbourhood_map.set_index('neighbourhood').join(community_prices.set_index('Neighbourhood'))\nmerged = merged.reset_index()\nmerged = merged.fillna(0)\n\n#create heat map\nfig, ax = plt.subplots(1, figsize=(40, 20))\nax.axis('off')\nax.set_title('Heat Map of House Prices by Neighbourhood in Toronto', fontdict={'fontsize': '40', 'fontweight' : '3'})\n\n#create colorbar as a legend\ncolor = 'Oranges'\nvmin, vmax = 0, merged['Avg. Price'].max()\nsm = plt.cm.ScalarMappable(cmap=color, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm._A = []\ncbar = fig.colorbar(sm)\ncbar.ax.tick_params(labelsize=15)\n\n# plot map - annotated neighbourhoods with >5 count\nmerged.plot('Avg. Price', cmap=color, linewidth=0.8, ax=ax, edgecolor='0.8', figsize=(40, 20))\nfor idx, row in merged.iterrows():\n    if(row['Count'] <= 5):\n        plt.annotate(s=int(row['Count']), xy=(row['FIELD_11'], row['FIELD_12']),\n                 horizontalalignment='center', fontsize='large', color='black', wrap=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of 0 count neighbourhoods\nmerged[merged['Count']<16][['neighbourhood','Avg. Price', 'Count']].sort_values(by=['Count'], ascending=True).to_csv('./output.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation matrix","metadata":{}},{"cell_type":"code","source":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Begin Data Engineering","metadata":{}},{"cell_type":"markdown","source":"### Type","metadata":{}},{"cell_type":"code","source":"# Get average 'Sold_price' across communities (neighbourhoods)\ntypes = df_train[['Sold_price','Type']].groupby(['Type'])\n\ntype_prices = pd.concat([types.mean(), types.count()], axis=1)\ntype_prices.columns = ['Avg. Price', 'Count']\ntype_prices.sort_values(by=['Avg. Price'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use one hot encoding for Type data","metadata":{}},{"cell_type":"code","source":"oneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['Type'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\nprint(ce_ohe)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Style","metadata":{}},{"cell_type":"code","source":"# Get average 'Sold_price' across communities (neighbourhoods)\nstyles = df_train[['Sold_price','Style']].groupby(['Style'])\n\nstyle_prices = pd.concat([styles.mean(), styles.count()], axis=1)\nstyle_prices.columns = ['Avg. Price', 'Count']\nstyle_prices.sort_values(by=['Avg. Price'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['Style'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\nprint(ce_ohe)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### District and Community Encoding","metadata":{}},{"cell_type":"code","source":"# Bin districts\ndf_train['district_bin'] = df_train['Municipality District']\ndf_train['district_bin'][df_train['Municipality District'] == 'C12'] = 'district_bin_0'\ndf_train['district_bin'][df_train['Municipality District'] == 'C04'] = 'district_bin_1'\ndf_train['district_bin'][df_train['Municipality District'] == 'C09'] = 'district_bin_2'\ndf_train['district_bin'][df_train['Municipality District'] == 'C03'] = 'district_bin_3'\ndf_train['district_bin'][df_train['Municipality District'] == 'E02'] = 'district_bin_4'\ndf_train['district_bin'][df_train['Municipality District'] == 'C02'] = 'district_bin_4'\ndf_train['district_bin'][df_train['Municipality District'] == 'W07'] = 'district_bin_5'\ndf_train['district_bin'][df_train['Municipality District'] == 'W02'] = 'district_bin_6'\ndf_train['district_bin'][df_train['Municipality District'] == 'E01'] = 'district_bin_6'\ndf_train['district_bin'][df_train['Municipality District'] == 'W01'] = 'district_bin_6'\ndf_train['district_bin'][df_train['Municipality District'] == 'C13'] = 'district_bin_7'\ndf_train['district_bin'][df_train['Municipality District'] == 'E03'] = 'district_bin_7'\ndf_train['district_bin'][df_train['Municipality District'] == 'C06'] = 'district_bin_7'\ndf_train['district_bin'][df_train['Municipality District'] == 'C07'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'E06'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'W03'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'C14'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'W08'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'C11'] = 'district_bin_9'\ndf_train['district_bin'][df_train['Municipality District'] == 'E10'] = 'district_bin_9'\ndf_train['district_bin'][df_train['Municipality District'] == 'C10'] = 'district_bin_9'\ndf_train['district_bin'][df_train['Municipality District'] == 'E08'] = 'district_bin_10'\ndf_train['district_bin'][df_train['Municipality District'] == 'W06'] = 'district_bin_10'\ndf_train['district_bin'][df_train['Municipality District'] == 'C01'] = 'district_bin_10'\ndf_train['district_bin'][df_train['Municipality District'] == 'C08'] = 'district_bin_11'\ndf_train['district_bin'][df_train['Municipality District'] == 'W09'] = 'district_bin_11'\ndf_train['district_bin'][df_train['Municipality District'] == 'W04'] = 'district_bin_11'\ndf_train['district_bin'][df_train['Municipality District'] == 'E07'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'W05'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'E04'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'C15'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'E05'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'E11'] = 'district_bin_13'\ndf_train['district_bin'][df_train['Municipality District'] == 'E09'] = 'district_bin_13'\ndf_train['district_bin'][df_train['Municipality District'] == 'W10'] = 'district_bin_14'\n\n# One-hot encode districts\noneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['district_bin'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bin_range(start_of_range,start_of_next_range):\n    return range(list(community_prices_sorted.index.values).index(start_of_range),list(community_prices_sorted.index.values).index(start_of_next_range))\n\n# Set Community Bins\nbins = [bin_range('Bridle Path-Sunnybrook-York Mills','Lawrence Park South')],\\\n        [bin_range('Lawrence Park South','Lawrence Park North')],\\\n        [bin_range('Lawrence Park North','Princess-Rosethorn')],\\\n        [bin_range('Princess-Rosethorn','Bedford Park-Nortown')],\\\n        [bin_range('Bedford Park-Nortown','St. Andrew-Windfields')],\\\n        [bin_range('St. Andrew-Windfields','Lambton Baby Point')],\\\n        [bin_range('Lambton Baby Point','Danforth')],\\\n        [bin_range('Danforth','Highland Creek')],\\\n        [bin_range('Highland Creek','Danforth Village-East York')],\\\n        [bin_range('Danforth Village-East York','Edenbridge-Humber Valley')],\\\n        [bin_range('Edenbridge-Humber Valley','Woodbine Corridor')],\\\n        [bin_range('Woodbine Corridor','Cabbagetown-South St. James Town')],\\\n        [bin_range('Cabbagetown-South St. James Town','University')],\\\n        [bin_range('University','Dovercourt-Wallace Emerson-Junction')],\\\n        [bin_range('Dovercourt-Wallace Emerson-Junction','Rouge E10')],\\\n        [bin_range('Rouge E10','Waterfront Communities C8')],\\\n        [bin_range('Waterfront Communities C8','Rockcliffe-Smythe')],\\\n        [bin_range('Rockcliffe-Smythe','Briar Hill-Belgravia')],\\\n        [bin_range('Briar Hill-Belgravia','Bayview Village')],\\\n        [bin_range('Bayview Village','Dorset Park')],\\\n        [bin_range('Dorset Park','Eglinton East')],\\\n        [bin_range('Eglinton East','Mount Olive-Silverstone-Jamestown')],\\\n        [bin_range('Mount Olive-Silverstone-Jamestown','Elms-Old Rexdale')],\\\n        [bin_range('Elms-Old Rexdale','Black Creek')]\n\ndf_train['community_bin'] = np.nan\n\nfor idx,item in enumerate(bins):\n    bin_name = 'community_bin_' + str(idx)\n    df_train['community_bin'][df_train['Community'].isin(community_prices_sorted.index.values[item])] = bin_name\n    last_bin_name = bin_name\n\n# Set end of last range \ndf_train['community_bin'][df_train['Community'] == 'Black Creek'] = last_bin_name\n    \n# One-hot encode Community Bins\noneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['community_bin'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combine Bedrooms and Den Columns","metadata":{}},{"cell_type":"code","source":"beds = df_train[['Bedrooms','Dens']]\nmultiplier = 0.48\n\nbeds_prices = pd.concat([beds.Bedrooms, beds.Dens, beds.Bedrooms.astype(float) + multiplier * beds.Dens.astype(float)], axis=1)\npd.options.display.float_format = '{:,.2f}'.format\nbeds_prices.columns = ['Bedrooms', 'Dens', 'Combined']\n\nbeds_prices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compare Combined Correlation","metadata":{}},{"cell_type":"code","source":"#Combined vs. Bedrooms vs. Dens \nbedrooms = pd.concat([df_train['Sold_price'], beds_prices['Bedrooms']], axis=1)\nbedrooms.plot.scatter(x='Bedrooms', y='Sold_price')\n\ndens = pd.concat([df_train['Sold_price'], beds_prices['Dens']], axis=1)\ndens.plot.scatter(x='Dens', y='Sold_price')\n\ncombined = pd.concat([df_train['Sold_price'], beds_prices['Combined']], axis=1)\ncombined.plot.scatter(x='Combined', y='Sold_price')\n\nprint('Correlation:\\nBedrooms - ', df_train['Sold_price'].corr(beds_prices['Bedrooms']),\n                  '\\nDens     - ', df_train['Sold_price'].corr(beds_prices['Dens']),\n                  '\\nCombined - ', df_train['Sold_price'].corr(beds_prices['Combined']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Optimize Dens Multiplier","metadata":{}},{"cell_type":"code","source":"# Correlation score of 'Combined' as a function of the multiplier\ndef combined_correlation(multiplier):\n    return df_train['Sold_price'].corr(beds.Bedrooms + beds.Dens * multiplier)\n\n# Create table with possible multiplier values on each row\noptimize = pd.DataFrame(columns=['multiplier','combined_correlation'])\noptimize['multiplier'] = np.linspace(0,1,1000) # 1000 values b/w 0 and 1\n\n# Calculate Correlation score for each row\nfor idx, row in optimize.iterrows():   \n    optimize['combined_correlation'][idx] = combined_correlation(optimize['multiplier'][idx])\n\n# Get multiplier for maximum Correlation score\nprint('multiplier: ', optimize.loc[optimize['combined_correlation']==optimize['combined_correlation'].max(), 'multiplier'].iloc[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bathrooms, Kitchens, Rooms, Parking Total Correlations","metadata":{}},{"cell_type":"code","source":"bathrooms = pd.concat([df_train['Sold_price'], df_train['Bathrooms']], axis=1)\nbathrooms.plot.scatter(x='Bathrooms', y='Sold_price')\n\nkitchen = pd.concat([df_train['Sold_price'], df_train['Kitchens']], axis=1)\nkitchen.plot.scatter(x='Kitchens', y='Sold_price')\n\nrooms = pd.concat([df_train['Sold_price'], df_train['Rooms']], axis=1)\nrooms.plot.scatter(x='Rooms', y='Sold_price')\n\nparking = pd.concat([df_train['Sold_price'], df_train['Parking Total']], axis=1)\nparking.plot.scatter(x='Parking Total', y='Sold_price')\n\nprint('Correlation:\\nBathrooms - ', df_train['Sold_price'].corr(df_train['Bathrooms']),\n                  '\\nKitchens  - ', df_train['Sold_price'].corr(df_train['Kitchens']),\n                  '\\nRooms     - ', df_train['Sold_price'].corr(df_train['Dens']),\n                  '\\nParking   - ', df_train['Sold_price'].corr(df_train['Parking Total']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transform Skewed Data","metadata":{}},{"cell_type":"code","source":"# histogram and normal probability plot\nsns.distplot(df_train['Squarefootage'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['Squarefootage'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# log squarefootage\ndf_train['log(Squarefootage)'] = np.log(df_train['Squarefootage'])\n\nsns.distplot(df_train['log(Squarefootage)'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['log(Squarefootage)'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop latitude, longitude, kitchens, squarefootage, rooms rows","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(['longitude', 'latitude', 'Rooms', 'Kitchens', 'Squarefootage', 'Community', 'Municipality District'], axis=1)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop/Transform Outliers","metadata":{}},{"cell_type":"code","source":"# Drop 7 dens\ndf_train = df_train.drop(df_train[df_train['Dens']==7].index.values)\n\n# Drop Bathrooms 9+\ndf_train = df_train.drop(df_train[df_train['Bathrooms']>=9].index.values)\ndf_train = df_train.drop(df_train[df_train['Bathrooms']==0].index.values)\n\n# Drop Parking 9+\ndf_train = df_train.drop(df_train[df_train['Parking Total']>=9].index.values)\n\n# Round down .5 Parking Total\ndf_train[\"Parking Total\"].replace({\"1.5\": \"1\", \"3.5\": \"3\", \"5.5\": \"5\"}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalize values to help NN train","metadata":{}},{"cell_type":"code","source":"# Scale Data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_train = scaler.fit_transform(df_train)\n\nscaled_train = pd.DataFrame(data=scaled_train, columns=df_train.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Columns\")\nprint(scaled_train.columns)\n# Get scaled values\n# Print out the adjustment that the scaler applied to the total_earnings column of data\nprint(\"Note: sold_price values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[0], scaler.min_[0]))\nmultiplied_by = scaler.scale_[0]\nadded = scaler.min_[0]\nprint(\"Multiplied by\")\nprint(scaler.scale_)\nprint(\"Added\")\nprint(scaler.min_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Separate Dataset into Train (70%), Validation (20%),Test (10%)","metadata":{}},{"cell_type":"code","source":"# Split Data\ntrain, test = np.split(scaled_train.sample(frac=1, random_state=42),\\\n                        [int(.9*len(scaled_train))])\n\nsns.distplot(train['Sold_price'])\nsns.distplot(test['Sold_price'])\n\nprint(len(train))\nprint(len(test))\nprint(len(test)+len(train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup input, label data","metadata":{}},{"cell_type":"code","source":"y_train_data = train.values[:,0]\ny_test_data = test.values[:,0]\nx_train_data = train.values[:,1:]\nx_test_data = test.values[:,1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get input shape\nlen(x_train_data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finished Data Engineering\n## Begin Neural Network","metadata":{}},{"cell_type":"code","source":"# Imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build model\nmodel = Sequential()\n\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhist = model.fit(\n    x_train_data,\n    y_train_data,\n    epochs=50,\n    shuffle=True,\n    verbose=2,\n    validation_data=(x_test_data, y_test_data)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Results","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = keras.models.load_model('model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference\nprediction = loaded_model.predict(x_test_data[:1])\ny_0 = prediction[0][0]\nprint('Prediction with scaling - {}',format(y_0))\ny_0 -= added\ny_0 /= multiplied_by\nprint(\"Housing Price Prediction  - ${}\".format(y_0))\nactual = y_test_data[:1]\nprint('Actual with scaling - {}',format(actual))\nactual -= added\nactual /= multiplied_by\nprint(\"Housing Price Prediction  - ${}\".format(actual))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}