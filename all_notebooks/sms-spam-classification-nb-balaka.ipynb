{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as ml\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix\n%matplotlib inline\nml.style.use('ggplot')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RENAMING COLUMNS"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = {'v1':'Label','v2':'Text'}\ndata = data.rename(columns=cols)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FIND RATIO OF NULL VALUES IN LAST 3 COLUMNS TO DECIDE WHETHER TO KEEP OR DROP"},{"metadata":{"trusted":true},"cell_type":"code","source":"col3 = data['Unnamed: 2'].isnull().sum()/data.shape[0]\ncol4 = data['Unnamed: 3'].isnull().sum()/data.shape[0]\ncol5 = data['Unnamed: 4'].isnull().sum()/data.shape[0]\n\nprint(\"Portion of NaN values in the 3rd column : \",(col3*100),\"%\")\nprint(\"Portion of NaN values in the 4th column : \",(col4*100),\"%\")\nprint(\"Portion of NaN values in the 5th column : \",(col5*100),\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We see a huge magnitude of the data is NULL in the last 3 columns. So we drop the last 3 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FILTERING OUT THE COMMON WORDS AND FORMING A FREQUENCY DATASET."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting all words in all 5572 SMS's\ntexts = list(data.Text.values)\nwords = []\nfor i in texts:\n    words.extend(i.split(\" \"))\nlen(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing non-alphabetical instances\nfor i in range(len(words)):\n    if not words[i].isalpha():\n        words[i] = \"\"\n\n# Create a Counter dictionary to maintain ('word':count) tuples.\nword_dict = Counter(words)\nword_dict.most_common(1)     # Get the 1st most common word.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### This shouldn't be the case. The most common word can't be a ' '. So we remmove the occurrence of ' '(unwanted character) in the word_dict"},{"metadata":{"trusted":true},"cell_type":"code","source":"del word_dict['']\n# Check\nword_dict.most_common(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the 3000 most common words in text messages (Why 3000 ? Hit and trial)\nw_new = word_dict.most_common(3000)\nw_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Seems alright. Now we form our features(word counts for each message)\n\n## FEATURE ENGINEERING"},{"metadata":{"trusted":true},"cell_type":"code","source":"features,columns = [],[]\n\n# Creating the columns(features)\nfor word in w_new:\n    columns.append(word[0])\n\n# Creating data\nfor s in texts:\n    d = []\n    for wrd in w_new:\n        d.append(s.count(wrd[0]))\n    features.append(d)\n\n# Create dataframe\ndf = pd.DataFrame(features,columns=columns)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DROPPING 'TEXT' AND ASSIGNING LABELS MANUALLY\n\n##### 1. Since the label is of two types only ('spam' or 'ham'), we can manually encode them instead of using a LabelEncoder() object.\n##### 2. We do not need the text messages feature anymore, because we have already got the count of 3000 most popular words in each one of them. So it is not merged"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'] = 0\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = {'spam':1,'ham':0}             # Spam = 1, Not spam = 0\nfor i in range(df.shape[0]):\n    df.iloc[i,3000] = int(labels[data.iloc[i,0]])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TRAINING THE MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = df.iloc[:,:3000].values\nX = np.array(features)\n# Y = df['label'].values\nY = np.array(df.label)\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.7,random_state=10)\nprint(\"Train X size = \",train_x.shape,\", Test X size = \",test_x.shape)\nprint(\"Train Y size = \",train_y.shape,\", Test Y size = \",test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the Naive Bayes classifier\nmnb = MultinomialNB(alpha = 5, fit_prior = True)\n# Training the model\nmnb.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PREDICTIONS AND CONFUSION MATRIX\n##### Confusion matrix to get an idea of the accuracy of the model. X-axis = Predicted label. Y-axis = True label."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = mnb.predict(test_x)\nprint(confusion_matrix(test_y,y_pred,labels=[1,0],normalize=None))\ntn,fp,fn,tp = confusion_matrix(test_y,y_pred).ravel()\nprint(\"\\nTP = \",tp,\", FP = \",fp,\", FN = \",fn,\", TN = \",tn)\nprint(\"Confirmation : TP + FP + FN + TN = \",tp+fp+fn+tn)\nprint(\"Equal to test_y size(3901)\")\n# Accuracy\nprint(\"\\nAccuracy = \",(tp+tn)*100/(tp+tn+fp+fn),\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}