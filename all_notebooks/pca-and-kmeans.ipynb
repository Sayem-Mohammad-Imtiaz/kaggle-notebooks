{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","name":"python","version":"3.6.3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"6d7d49afa4af04f8d5343f147a64e04cbfaa6043","_cell_guid":"9f49ea4f-a3a0-4543-9ab5-f9c694dc2bc0"},"cell_type":"markdown","source":"We're following Anisotropic's notebook on the IMDB data base with the TMDB data base, to see what's happening. ALl credits go to https://www.kaggle.com/arthurtok/principal-component-analysis-with-kmeans-visuals/notebook"},{"metadata":{"_uuid":"d335354af36cdaad433300dfa785cd3a3f7a5dc5","_cell_guid":"1bc05078-dd26-4c33-887f-bd9e6b8e2692"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA # Principal Component Analysis module\nfrom sklearn.cluster import KMeans # KMeans clustering \nimport matplotlib.pyplot as plt # Python defacto plotting library\nimport seaborn as sns # More snazzy plotting library\n%matplotlib inline \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"metadata":{"collapsed":true,"_uuid":"530d6fc5997f3b9ad914e878631f8eb6eff6bd18","_cell_guid":"15f2f0d2-570d-4e50-a3bd-92f25034873c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie = pd.read_csv('../input/tmdb_5000_movies.csv')\ncredit = pd.read_csv('../input/tmdb_5000_credits.csv')"},{"metadata":{"_uuid":"f714fd6441588f90b096a2f9e73b8e3ebafeab18","_cell_guid":"eb676562-90e4-4667-9143-2120979e10b4"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie.head()\n#credit.head()"},{"metadata":{"_uuid":"0aa80d477b5b9929e01f973aeca714cc3e8c6def","_cell_guid":"d604dc1e-2210-4e70-9487-5b15bb3be797"},"cell_type":"markdown","source":"The data frame contains columns with words as well with numbers. We'll do some filtering to extract only the numbered columns:"},{"metadata":{"collapsed":true,"_uuid":"27dd22de676de496082fc99709353b6847221512","_cell_guid":"47df2254-3fb8-4ecc-9d93-45eee40972c8"},"execution_count":null,"cell_type":"code","outputs":[],"source":"str_list = [] # empty list to contain columns with strings\nfor colname, colvalue in movie.iteritems():\n    if type(colvalue[1]) == str:\n        str_list.append(colname)\n#Get to the numeric columns by inversion\nnum_list = movie.columns.difference(str_list)"},{"metadata":{"_uuid":"9afcafe1af0b66457afdeb28f0e92bbd3ba81601","_cell_guid":"a20c4b20-a669-4a58-b55b-7b5175ee5189"},"cell_type":"markdown","source":"We can create a new data frame containing just the numbers:"},{"metadata":{"_uuid":"f76dbb34733ae8e118e81d1fac1d6ede4f7a4262","_cell_guid":"d36e4263-aea2-472c-883c-5a62d8f6d125"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie_num = movie[num_list]\nmovie_num.head()"},{"metadata":{"_uuid":"b6f77e9a48963d7225eeac1148ee199cc18d93f1","_cell_guid":"927cca4c-3154-48bd-ae5c-637d5be89e14"},"cell_type":"markdown","source":"There still exist NaN values, which we have to get rid of:"},{"metadata":{"collapsed":true,"_uuid":"f6459f688a201b1f87a1ad11ddf9bfca0d5403b8","_cell_guid":"675e48c5-124e-4db4-9f1b-aace300c03e8"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie_num = movie_num.fillna(value=0, axis=1)"},{"metadata":{"_uuid":"c5c4cda5a911fd456c28966b00bc7e247aecd16d","_cell_guid":"6c29db65-6e9c-44d1-92ec-4bd39358f748"},"cell_type":"markdown","source":"We standardise the data with sklearn's StandardScaler"},{"metadata":{"_uuid":"374d8f12d0b2e58f9086ecf224dccaf293647399","_cell_guid":"e0e32576-dafc-4d1f-a676-2d650109ce2c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"X = movie_num.values\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)"},{"metadata":{"_uuid":"b29f1d6fa79fc751826953b3b96ed86ede264123","_cell_guid":"75a4a1fd-b239-498c-928e-2251cc3d8f84"},"cell_type":"markdown","source":"Let's look at some visualisations:"},{"metadata":{"_uuid":"3256befa2ab21c5d2641f641fe077c8a7e3a4479","_cell_guid":"93acc3ce-e4a8-4961-9b49-acc4144fb286"},"execution_count":null,"cell_type":"code","outputs":[],"source":"list(movie)"},{"metadata":{"_uuid":"beccba8aa1d4a869297c5ffce744244deb0f5e08","_cell_guid":"c31f9cbc-00f0-4b8c-adc6-581b757bc0e7"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie.plot(y = 'vote_average', x = 'runtime', kind = 'hexbin', gridsize=35, sharex=False, \n           colormap='cubehelix', title='Hexbin of vote_average and runtime',figsize=(12,8))\nmovie.plot(y ='vote_average', x = 'revenue', kind='hexbin', gridsize = 45, sharex = False,\n          colormap = 'cubehelix', title='Hexbin of vote_average and revenue', figsize = (12,8))"},{"metadata":{"_uuid":"0ff6d17dbf29cfc4462a1315bb8db1b7f4221f21","_cell_guid":"209c8c8e-141e-4e01-a9ed-4afb1864c04d"},"cell_type":"markdown","source":"Let's generate a heatmap"},{"metadata":{"_uuid":"9b6dd2c179ea838e8fb96d5d66cdbeb157c2f629","_cell_guid":"d36a7d62-237a-4b28-be8a-ee96e014a4a3"},"execution_count":null,"cell_type":"code","outputs":[],"source":"f, ax = plt.subplots(figsize=(12,10))\nplt.title('Pearson Correlation of Movie Features')\nsns.heatmap(movie_num.astype(float).corr(), linewidths=0.25, vmax=1.0, square=True,\n           cmap=\"YlGnBu\", linecolor='black', annot=True)"},{"metadata":{"_uuid":"b508d3b7a75083b5953602e6c90d30240dd1a091","_cell_guid":"092eb901-1917-4e48-9e5d-c541db067503"},"cell_type":"markdown","source":"The darker reagions have quite a positive correlation amongst eachother. This is a good sign that we may be able to find features on which we can perform PCA projections on. (Principal Component Analysis)"},{"metadata":{"_uuid":"6a84068bda09bd75251b841d7af4f3559a26fd8b","_cell_guid":"b413538f-b039-4491-afba-bfaa11c02494"},"cell_type":"markdown","source":"# Explained Variance Measure"},{"metadata":{"collapsed":true,"_uuid":"17b9c7021e5a117e6ec3d172eb92f2196410e7f5","_cell_guid":"8b57f7a6-e44f-4d26-8216-baff789b9050"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#Calculating Eigenvecors and eigenvalues of Covariance matrix\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)"},{"metadata":{"_uuid":"500bb9d191ddc497d42588a14bc3a276db8ed2d5","_cell_guid":"21a7607e-a30d-46bd-848d-9aa61ccf711b"},"cell_type":"markdown","source":"Let's group them together by creating a list of eigenvalue, eigenvector tuples. Followed by sortin the list in order of highest to lowest eigenvalue to calculate variances for visualisation."},{"metadata":{"collapsed":true,"_uuid":"5774eb00d8bf233ae9548f76ca3760f85b566e5f","_cell_guid":"9ee88564-c3ca-4cfd-88e7-7975c6dab98d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"},{"metadata":{"_uuid":"b88c12523bfe35811d3c43a7c749f9b3d24de025","_cell_guid":"ff7593e7-957b-4077-8973-6d043bda711d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"cum_var_exp"},{"metadata":{"_uuid":"fdce89965f26ef066022c25e90dadcc36ea4a7d0","_cell_guid":"f153828e-378a-44c2-88bf-eb59c60d1de1"},"cell_type":"markdown","source":"Now it's time to plot the explained variance graphs"},{"metadata":{"_uuid":"e2d51b6aca3f2abc462b530b4418a80948a72c6c","_cell_guid":"8dbca7fb-3857-4902-bb7c-f1a3d4385517"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \nplt.figure(figsize=(10, 5))\nplt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\nplt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show()"},{"metadata":{"_uuid":"64b3d5e106d9a3550efec2a515cc4bb83ad53895","_cell_guid":"06e54119-7099-424d-aa2c-abad06636362"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie_num.describe()"},{"metadata":{"_uuid":"b0a683de3f0feba3419996d67167bb4e8a939f23","_cell_guid":"6964a6e8-ccb5-4919-8e42-fb97e83a32f5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie['revenue'].plot.hist()"},{"metadata":{"_uuid":"238960641c386fe09c309845b357bfd0d7bb1058","_cell_guid":"69e1d09c-c413-4646-b0f2-3fe7f71eb8c0"},"execution_count":null,"cell_type":"code","outputs":[],"source":"movie['revenue_classes'] = pd.cut(movie['revenue'],10)\nmovie['vote_classes'] = pd.cut(movie['vote_average'],4, labels=[\"low\", \"medium-low\",\"medium-high\",\"high\"])\n#movie['vote_classes'] = pd.cut(movie['vote_average'],10, labels=[\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])"},{"metadata":{"_uuid":"1048c1cbf79bb048f22190a79903f99239f9066d","_cell_guid":"b2fad456-19f9-485f-bcff-c85bd38c93f5"},"execution_count":null,"cell_type":"code","outputs":[],"source":"list(movie)"},{"metadata":{"_uuid":"25c138de0ca4a124b52f1cf8e7460ae412c990eb","_cell_guid":"b6bed970-7538-44c9-baea-da2d9318f9ff"},"execution_count":null,"cell_type":"code","outputs":[],"source":"X_revenue = movie.ix[:,(0,8,18,19)].values\ny_revenue = movie.ix[:,20].values\n\nX_votes = movie.ix[:,(0,8,12,19)].values\ny_votes = movie.ix[:,21].values"},{"metadata":{"_uuid":"f0bbda6fd095847e6100ae72ea42c4375384a242","_cell_guid":"220336e7-05aa-4b2b-b70b-6cc0dcd5029d"},"execution_count":null,"cell_type":"code","outputs":[],"source":""},{"metadata":{"_uuid":"ced6f2638024aa4373600277c741128b84f7943b","_cell_guid":"055da770-4ecb-4f38-bcd8-aca54f09d3d2"},"cell_type":"markdown","source":"To get a feeling for how the 4 votes classes are distributed along the numerical values, let's visualise it via histograms:"},{"metadata":{"_uuid":"5bff017ea9db3ee836c61e5dfd506e7e6870aca1","_cell_guid":"958a2654-2894-48c6-95f9-144cfa5bcc4b"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from matplotlib import pyplot as plt\nimport numpy as np\nimport math\n\nfeature_dict ={0:'budget',\n              1: 'popularity',\n              2: 'revenue',\n              3: 'vote_count'}\n\n#Use this block for a cut in 4 blocks\n'''\nlabel_dict = {1: 'low',\n              2: 'medium-low',\n              3: 'medium-high',\n              4: 'high'}\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8,6))\n    for cnt in range(4):\n        for lab in('low', 'medium-low','medium-high','high'):\n            plt.hist(X_votes[y_votes==lab, cnt],\n                    label = lab,\n                    bins = 10,\n                    alpha = 0.3,)\n            plt.xlabel(feature_dict[cnt])\n        plt.legend(loc='upper right', fancybox=True, fontsize=8)\n        \n        plt.tight_layout()\n        plt.show()\n'''\n\n#Use this block for a cut in 10 blocks.\nlabel_dict = {0: '0-1',\n               1: '1-2',\n               2: '2-3',\n               3: '3-4',\n               4: '4-5',\n               5: '5-6',\n               6: '6-7',\n               7: '7-8',\n               8: '8-9',\n               9: '9-10'}\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8,6))\n    for cnt in range(4):\n        for lab in(\"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"):\n            plt.hist(X_votes[y_votes==lab, cnt],\n                    label = lab,\n                    bins = 10,\n                    alpha = 0.3,)\n            plt.xlabel(feature_dict[cnt])\n        plt.legend(loc='upper right', fancybox=True, fontsize=8)\n        \n        plt.tight_layout()\n        plt.show()\n\n\n"},{"metadata":{"_uuid":"505ef496b97d67764683d777df3b315b0620ae7f","_cell_guid":"52c9258f-017b-42a4-8cdd-badfbef61aad"},"cell_type":"markdown","source":"Almost everything is medium-high. Let's standardize the data."},{"metadata":{"collapsed":true,"_uuid":"8fa22dc4220c5574a6f647f42743491e266220ce","_cell_guid":"612b26ed-9561-4832-bccd-f62b84c9887f"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X_votes)"},{"metadata":{"_uuid":"3b7c022d81bd0c7f868993f96e3616d9c463d691","_cell_guid":"7401119e-195d-4995-b5ee-cdaafc228731"},"cell_type":"markdown","source":"We'll perform the eigendecomposition on the covariance matrix. This is a $d\\times d$ matrix where each element represents the covariance between two features. The mean vector is a $d$-dimensional vector where each value represents the sample mean of a feature column in the dataframe."},{"metadata":{"scrolled":true,"_uuid":"c1ec9a6583d66783dbb9b2bbd0fc82c1d4cde90a","_cell_guid":"66d6bfb7-6ee7-4589-a182-0b3c22988e68"},"execution_count":null,"cell_type":"code","outputs":[],"source":"import numpy as np\nprint('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))"},{"metadata":{"_uuid":"82ac3c688ef1202be64181c4900c5e158c7187dc","_cell_guid":"b0b1f428-25db-4798-9a55-5522cbc74a55"},"cell_type":"markdown","source":"All the underlying eigendecompositions are equal. We should know why and how to do this:\n\n---------------------------\n\nWe perform an eigendecomposition on the covariance matrix:\n\n"},{"metadata":{"_uuid":"29605fbdef518601a52e105fef11567fa4418a5c","_cell_guid":"d8bba366-5a50-4506-9b50-f16952ef3d72"},"execution_count":null,"cell_type":"code","outputs":[],"source":"cov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)"},{"metadata":{"_uuid":"3fc5a64337e3e3579473fdc2e495ec4ada871524","_cell_guid":"a7dba5a9-4d1f-44be-8cc0-ec5cbfe5341a"},"cell_type":"markdown","source":"Eigendecomposition of the standardized data based on the correlation matrix:"},{"metadata":{"_uuid":"ea28fcbd5249d4af85a58f72e11b3cd3fcad89b9","_cell_guid":"56159152-1591-4e98-bb2d-c2d297813696"},"execution_count":null,"cell_type":"code","outputs":[],"source":"cor_mat1 = np.corrcoef(X_std.T)\n\neig_vals, eig_vecs = np.linalg.eig(cor_mat1)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)"},{"metadata":{"_uuid":"fe3b7cf490d41e14b7bfc8839fce7359603b37f7","_cell_guid":"d04d98e1-caad-4963-9050-081555954240"},"cell_type":"markdown","source":"Eigendecomposition of raw data based on correlation matrix:"},{"metadata":{"_uuid":"bab4bf9eb0dc7322fe773135d49498de0cadf358","_cell_guid":"fb6270a8-04c7-4934-8448-14b418141de0"},"execution_count":null,"cell_type":"code","outputs":[],"source":"cor_mat2 = np.corrcoef(X.T)\n\neig_vals, eig_vecs = np.linalg.eig(cor_mat2)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)"},{"metadata":{"_uuid":"5a3701e34e6952fdb9e31ee5334ee172ce96bc61","_cell_guid":"a8d4630f-4174-40a1-b20e-865d4878ab60"},"execution_count":null,"cell_type":"code","outputs":[],"source":"u,s,v = np.linalg.svd(X_std.T)\nu"},{"metadata":{"_uuid":"4c46327d3f86da20477b9125da59a6c55b35f674","_cell_guid":"52c39e8d-5185-4ce0-af59-b4b8aa6621b7"},"cell_type":"markdown","source":"The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which can confirmed by the following two lines of code:"},{"metadata":{"_uuid":"fbb6c5df5f8b3f4db188d1e410fc5cf598d6dda3","_cell_guid":"795f4ba4-afc7-4923-af53-747b7484c310"},"execution_count":null,"cell_type":"code","outputs":[],"source":"for ev in eig_vecs:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\nprint('Everything ok!')"},{"metadata":{"_uuid":"5f118f15aa4135e078b8f79b4ec11d071f0cc685","_cell_guid":"07839767-4666-4215-ba1a-c728d418d2e8"},"cell_type":"markdown","source":"The idea behind PCA is to find variables which can be dropped, because their influence is negligble. Performing analysis on a data frame with less columns, requires less computing power. Honestly, our data frame isn't that big, but for learning purposes, we'll still try to do it.\n\nThe common approach is to rank the eigenvalues from highest to lowest and choose the top $k$ eigenvectors."},{"metadata":{"_uuid":"d1b6a33737c6a8caa387cb9aa302251d190a36e1","_cell_guid":"deed99af-064e-4a4c-9f73-20e02b16e4b7"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])"},{"metadata":{"_uuid":"dc64ed1017165825bc3d9a27d586506c410380cd","_cell_guid":"e4923943-dbcd-4e7e-be6a-5d1b98641179"},"cell_type":"markdown","source":"After sorting the eigenpairs, the next question is “how many principal components are we going to choose for our new feature subspace?” A useful measure is the so-called “explained variance,” which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components"},{"metadata":{"_uuid":"eb61cb4d01bf5ab056d1517a7551ba31b79f6ce7","_cell_guid":"f25127bd-0a47-48dc-87fe-a1148fec4122"},"execution_count":null,"cell_type":"code","outputs":[],"source":"tot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(var_exp)\nprint(cum_var_exp)"},{"metadata":{"_uuid":"da1cecfc2f254eda65710dac374d11b54b90fd3d","_cell_guid":"2300347c-c23b-4bf5-ae2a-f9204eeb8bdd"},"execution_count":null,"cell_type":"code","outputs":[],"source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(len(var_exp)), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()"},{"metadata":{"_uuid":"47705a8095536870f2f4c5369df366bf9f5f44ce","_cell_guid":"7ab61682-f0ec-4346-9441-444fca0dc005"},"cell_type":"markdown","source":"We see that the first component explains almost $50\\%$ of the variance. The second component explains about $20\\%$. However, we've got to notice that almost none of the components are really negligible. The first $4$ components contain about $94\\%$ of the information."},{"metadata":{"_uuid":"41440c0b95c9f1b69a80a62ca874ca6f4089791a","_cell_guid":"2fafd308-c401-4b3f-b67f-6d61b6e0d6c7"},"cell_type":"markdown","source":"# Projection matrix"},{"metadata":{"_uuid":"0223c55cbe93b9976a5d8db4ba5d8587a7a379e3","_cell_guid":"953694bc-1865-45d2-bc26-4277c9fd9573"},"cell_type":"markdown","source":"A projection matrix can be used to transform data onto the new subspace. It basically is just a matrix of our concatenated top 4 eigenvectors.\n\nWe'll reduce our 7-dimensional space to a 4-dimensional subspace by choosing the top 4 eigenvectors to construct our eigenvector matrix $W$."},{"metadata":{"_uuid":"f8b6e548f847ec0f83bac3741a6de3a23c35da9a","_cell_guid":"4e946a4c-149f-4e92-b4e0-01bd0c011bc1"},"execution_count":null,"cell_type":"code","outputs":[],"source":"matrix_w = np.hstack((eig_pairs[0][1].reshape(7,1),\n                      eig_pairs[1][1].reshape(7,1),\n                      eig_pairs[2][1].reshape(7,1),\n                      eig_pairs[3][1].reshape(7,1)))\n\nprint('Matrix W:\\n', matrix_w)"},{"metadata":{"_uuid":"732b4bfafddbd4004b2325100e9a2613b3d5f1e2","_cell_guid":"92b9c7ef-0964-474f-96aa-067269352951"},"execution_count":null,"cell_type":"code","outputs":[],"source":"eig_pairs[0][1].reshape(7,1)"},{"metadata":{"_uuid":"cdbfa43930cc5eea236705ea60b321020420149f","_cell_guid":"7bf8b8e1-c50f-4592-b6ca-52b78f4d6579"},"cell_type":"markdown","source":"Now we will use our $4\\times 7$ matrix W to transform our samples onto the new subspace via the equation $Y = X\\times W$ where $Y$ is a matrix of our transformed samples."},{"metadata":{"_uuid":"a052535bbd5fabe71aa6afaae80f719b66e0411f","_cell_guid":"22b6cbd6-1f35-47fc-ba32-703d66297575"},"execution_count":null,"cell_type":"code","outputs":[],"source":"Y = X_std.dot(matrix_w.T)"},{"metadata":{"_uuid":"a1eea361406196825b59ba5d9cfbb66306e50df2","_cell_guid":"e184afbc-0106-4e94-9a8f-f5a224e3d61c"},"execution_count":null,"cell_type":"code","outputs":[],"source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('low', 'medium-low','medium-high', 'high'),\n                        ('blue', 'red', 'green','orange')):\n        plt.scatter(Y[y_votes==lab, 0],\n                    Y[y_votes==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()"},{"metadata":{"collapsed":true,"_uuid":"4128c19319c80c7d1937ea072365a77984581ffb","_cell_guid":"6e57e83e-5efc-4a5f-ba25-819d5e0af169"},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components = 4)\nY_sklearn = sklearn_pca.fit_transform(X_std)"},{"metadata":{"_uuid":"d2c1352efa9cf9e33bccbee7c245313090e5ec27","_cell_guid":"cd864962-f4e1-4d76-8bd6-14b5868aaa46"},"execution_count":null,"cell_type":"code","outputs":[],"source":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6,4))\n    for lab, col in zip(('low','medium-low','medium-high','high'),\n                       ('blue','red','green','orange')):\n        plt.scatter(Y_sklearn[y_votes==lab, 0],\n                   Y_sklearn[y_votes==lab, 1],\n                   label = lab,\n                   c = col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='upper left')\n    plt.tight_layout()\n    plt.show()\n    "},{"metadata":{"_uuid":"a9526d8b20da946de5e0339c2ff20733df1b905c","_cell_guid":"deb06a58-064c-4e9c-94a9-572710c151f4"},"cell_type":"markdown","source":"# KMeans clustering"},{"metadata":{"collapsed":true,"_uuid":"1e8c91b7807b55508985d14b8d02a81ff53800ad","_cell_guid":"20b74fbc-ea03-4b80-b260-dfec07650219"},"execution_count":null,"cell_type":"code","outputs":[],"source":"X = movie_num.values\n# Data Normalization\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)"},{"metadata":{"_uuid":"8bb24b97c1829b36b2623efba43ba6ae676e3847","_cell_guid":"555a4a50-ad50-4c84-9722-2c1436f58076"},"execution_count":null,"cell_type":"code","outputs":[],"source":"pca = PCA(n_components=7)\nx_7d = pca.fit_transform(X_std)"},{"metadata":{"collapsed":true,"_uuid":"adffed3b9e42fa14b7b20f44bf3ddfe23aed7e07","_cell_guid":"9b784ab8-2b4d-48a1-9d9c-2e7867bd349e"},"execution_count":null,"cell_type":"code","outputs":[],"source":"pca4 = PCA(n_components=4)\nx_4d = pca.fit_transform(X_std)"},{"metadata":{"_uuid":"b4bfd76cf224503277387a5d45d157ada90b04aa","_cell_guid":"9e787abd-4ebc-4274-af52-92678013e10d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters = 3)\n\n#Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_7d)\n\n#Define our own color map\nLABEL_COLOR_MAP = {0:'r', 1: 'g', 2: 'b'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_7d[:,0],x_7d[:,2], c= label_color, alpha=0.5) \nplt.show()"},{"metadata":{"_uuid":"25f61f82d9bce34ce787de2f4ba592e8477b185b","_cell_guid":"77b94d3b-1eda-4a8c-b249-2ae877c7883a"},"execution_count":null,"cell_type":"code","outputs":[],"source":"#Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters = 3)\n\n#Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_4d)\n\n#Define our own color map\nLABEL_COLOR_MAP = {0:'r', 1: 'g', 2: 'b'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_4d[:,0],x_4d[:,2], c= label_color, alpha=0.5) \nplt.show()"},{"metadata":{"_uuid":"862742e9f0c3fbd7ce3c7855c14aa01f314ff3d8","_cell_guid":"219ee08c-5e89-4078-87eb-77de06f67e7d"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Create a temp dataframe from our PCA projection data \"x_9d\"\ndf = pd.DataFrame(x_4d)\ndf = df[[0,1,2]] # only want to visualise relationships between first 3 projections\ndf['X_cluster'] = X_clustered"},{"metadata":{"_uuid":"d86974bd67eaa51db9acc8cf05556287cbafc7b9","_cell_guid":"26c029f4-7795-4e7d-8d85-41fbe185b0a7"},"execution_count":null,"cell_type":"code","outputs":[],"source":"# Call Seaborn's pairplot to visualize our KMeans clustering on the PCA projected data\nsns.pairplot(df, hue='X_cluster', palette= 'Dark2', diag_kind='kde',size=1.85)"}],"nbformat":4}