{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook is very similar (and somehow an improvement) to the work I have done on this other [kaggle notebook](https://www.kaggle.com/viniciuscleves/an-lise-de-sentimento-com-bert). My motivation behind sentiment analysis on these datasets is to apply the trained model on another dataset I have of tweet-like data in portuguese.\n\nTherefore, on this notebook I will:\n\n1. Load positive and negative tweet samples. I will not use the neutral ones, as I believe that treating news tweets as neutral is a really strong assumption. \n2. Train and evaluate a BERT classifier.\n3. Evaluate the model on the IMDB dataset translated to portuguese and compare the results with the ones obtained on my other notebook mentioned before. \n\n### Reasons to read this notebook:\n\n1. Learn to use state-of-the-art BERT model to sentiment analysis. We will use Hugginface's implementation, which makes it very easy to apply these model in practice.\n2. Learn to use the `Datasets` library from Hugginface. It allows you to cache your dataset on disk, so you can seamlessly deal with huge datasets. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers datasets --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math\nimport os\nimport pickle\nimport re\nfrom dataclasses import dataclass\n\nimport datasets\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport torch.nn.functional as F\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, classification_report,\n                             confusion_matrix, precision_recall_fscore_support)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom tqdm.notebook import tqdm\nfrom transformers import (AdamW, BertForSequenceClassification, BertTokenizer,\n                          DataCollatorWithPadding,\n                          get_linear_schedule_with_warmup)\n\ndatasets.logging.set_verbosity_error()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Tweets Dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    no_theme = pd.read_csv(\n        '/kaggle/input/portuguese-tweets-for-sentiment-analysis/NoThemeTweets.csv', \n        index_col=0)\n    # the `type` column will be important in the future to stratify the splits\n    no_theme['type'] = 'no_theme-'\n\n    with_theme = pd.read_csv(\n        '/kaggle/input/portuguese-tweets-for-sentiment-analysis/TweetsWithTheme.csv', \n        index_col=0)\n    with_theme['type'] = 'with_theme-'\n\n    data = pd.concat([no_theme, with_theme])\n    data['type'] = data['type'] + data['sentiment']\n    # Remove duplicate tweets\n    data = data[~data.index.duplicated(keep='first')]\n    \n    return data\n\ndata = load_data()\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments = data.sentiment.value_counts()\nprint('Class ratio:', sentiments['Positivo']/sentiments['Negativo'])\nsentiments","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_splits(data):\n    test_validation_size = int(0.01*data.shape[0])\n    train_validation, test = train_test_split(data, test_size=test_validation_size, random_state=42, stratify=data['type'])\n    train, validation = train_test_split(train_validation, test_size=test_validation_size, random_state=42, stratify=train_validation['type'])\n    return train, validation, test\ntrain, validation, test = create_splits(data)\nprint('Training samples:  ', train.shape[0])\nprint('Validation samples:', validation.shape[0])\nprint('Test samples:      ', test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_dataset(tokenizer, splits):\n    train, validation, test = splits\n    # I could create the dataset directly from pandas, but I will save and load from disk so Datasets com cache it\n    # on disk. This is specially useful when you have a very large dataset that does not fit in memory, which is not\n    # the case, but I will leave here this way as a demonstration. \n    train.to_csv('train_split.csv')\n    validation.to_csv('validation_split.csv')\n    test.to_csv('test_split.csv')\n    dataset = datasets.load_dataset('csv', data_files={'train': 'train_split.csv',\n                                                       'validation':'validation_split.csv',\n                                                       'test': 'test_split.csv'})\n    dataset = dataset.map(lambda example: {'unbiased_text': re.sub(r':[\\)\\(]+', '', example['tweet_text'])}, batched=False)\n    dataset = dataset.map(lambda examples: tokenizer(examples['unbiased_text']), batched=True)\n    dataset = dataset.map(lambda example: {'labels': 1 if example['sentiment'] == 'Positivo' else 0}, batched=False)\n    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hugginface ships its models with its tokenizers. This makes it really simple to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\ndataset = build_dataset(tokenizer, (train, validation, test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training BERT"},{"metadata":{},"cell_type":"markdown","source":"First lets define some helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_metrics(preds, labels):\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\ndef send_inputs_to_device(inputs, device):\n    return {key:tensor.to(device) for key, tensor in inputs.items()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's create our dataloaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=16, collate_fn=DataCollatorWithPadding(tokenizer))\nvalidation_loader = torch.utils.data.DataLoader(dataset['validation'], batch_size=32, collate_fn=DataCollatorWithPadding(tokenizer))\ntest_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=32, collate_fn=DataCollatorWithPadding(tokenizer))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we load our model, define an optimizer, an scheduler for controlling the learning rate, and set everything up for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 1\nnum_warmup_steps = 5000\n\nmodel = BertForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.train().to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-6)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_epochs*len(train_loader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define an evaluation function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, validation_loader, device):\n    with torch.no_grad():\n        model.eval()\n        preds = []\n        labels = []\n        validation_losses = []\n        for inputs in validation_loader:\n            labels.append(inputs['labels'].numpy())\n            \n            inputs = send_inputs_to_device(inputs, device)\n            loss, scores = model(**inputs)[:2]\n            validation_losses.append(loss.cpu().item())\n\n            _, classifications = torch.max(scores, 1)\n            preds.append(classifications.cpu().numpy())\n        model.train()\n    return np.concatenate(preds), np.concatenate(labels)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally run the training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch_bar = tqdm(range(num_epochs))\nloss_acc = 0\nalpha = 0.95\nfor epoch in epoch_bar:\n    batch_bar = tqdm(enumerate(train_loader), desc=f'Epoch {epoch}', total=len(train_loader))\n    for idx, inputs in batch_bar:\n        inputs = send_inputs_to_device(inputs, device)\n        optimizer.zero_grad()\n        loss, logits = model(**inputs)[:2]\n        \n        loss.backward()\n        optimizer.step()\n        \n        # calculate a simplified ewma to the loss\n        if epoch == 0 and idx == 0:\n            loss_acc = loss.cpu().item()\n        else:\n            loss_acc = loss_acc * alpha + (1-alpha) * loss.cpu().item()\n        \n        batch_bar.set_postfix(loss=loss_acc)\n        \n        if idx%5000 == 0:\n            preds, labels = predict(model, validation_loader, device)\n            metrics = compute_metrics(preds, labels)\n            print(metrics)\n            \n\n        scheduler.step()\n    os.makedirs('/kaggle/working/checkpoints/epoch'+str(epoch))\n    model.save_pretrained('/kaggle/working/checkpoints/epoch'+str(epoch))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we evaluate or model on the holdout test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, labels = predict(model, test_loader, device)\nmetrics = compute_metrics(preds, labels)\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just for the sake of stablishing a baseline, we run some bag-of-word models on the same dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = nltk.stem.snowball.PortugueseStemmer()\nanalyzer = TfidfVectorizer().build_analyzer()\n\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc) if w[0]!='@')\n\nvectorizer = TfidfVectorizer(\n    stop_words=nltk.corpus.stopwords.words('portuguese'), \n    analyzer=stemmed_words,\n    min_df=0.0001, \n    max_features=100000, \n    max_df=0.8)\n\nX_train = vectorizer.fit_transform(train['tweet_text'].apply(lambda s: re.sub(r':[\\)\\(]+', '', s)))\nX_validation = vectorizer.transform(validation['tweet_text'].apply(lambda s: re.sub(r':[\\)\\(]+', '', s)))\nX_test = vectorizer.transform(test['tweet_text'].apply(lambda s: re.sub(r':[\\)\\(]+', '', s)))\n\ny_train = (train['sentiment']=='Positivo').astype(int).values\ny_validation = (validation['sentiment']=='Positivo').astype(int).values\ny_test = (test['sentiment']=='Positivo').astype(int).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on some of those features"},{"metadata":{"trusted":true},"cell_type":"code","source":"' | '.join(vectorizer.get_feature_names()[100:150])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train and evaluate logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=0, class_weight='balanced', max_iter=500, verbose=True)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_validation)\n\nprint(classification_report(y_validation, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train and evaluate naive bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_validation)\nprint(classification_report(y_validation, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, our BERT model does much better than these baselines. Now let's evaluated our model on another related dataset"},{"metadata":{},"cell_type":"markdown","source":"# Evaluate our model on the imdb pt-br dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_dataset = datasets.load_dataset('csv', data_files={'test': '/kaggle/input/imdb-ptbr/imdb-reviews-pt-br.csv'})\nimdb_dataset = imdb_dataset.map(lambda examples: tokenizer(examples['text_pt']), batched=True)\nimdb_dataset = imdb_dataset.filter(lambda example: len(example['input_ids']) <= 512)\nimdb_dataset = imdb_dataset.map(lambda example: {'labels': 1 if example['sentiment'] == 'pos' else 0}, batched=False)\n\nimdb_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\nimdb_loader = torch.utils.data.DataLoader(imdb_dataset['test'], batch_size=16, collate_fn=DataCollatorWithPadding(tokenizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, labels = predict(model, imdb_loader, device)\nmetrics = compute_metrics(preds, labels)\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.heatmap(confusion_matrix(labels, preds), cmap='Greens_r', annot=True, fmt='d')\n_ = ax.set(xlabel='Predicted', ylabel='Truth', title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThe BERT model fits very well on the tweet data. When carrying the model to the imdb dataset the results are modest, though. There are a few explanations for why this happens. First, the length and subject of the messages are essentially different. Second, the tweet data is only distantly supervised, so the labels are not very reliable, which introduces uncertainty to the model. To be very precise, our model is trying to predict whether the autor would put a happy emoji or a sad one. Maybe the model is capable of identifying some sarcasm pattern that degrades the quality of the predictions on the imdb dataset.\n\nTaking the differences between data distribuition on the datasets in consideration, the model is capable of delivering a modest results that might be useful on some applications. The negative predictions are fairly precise and reliable, for example. \n\nIn the future, I would like to evaluate the performance of the model trained on the imdb dataset on the tweet dataset, and whether mixing both datasets together boost performance on both. "},{"metadata":{},"cell_type":"markdown","source":"Thanks for checking up this notebook! \n\n**Cheers!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}