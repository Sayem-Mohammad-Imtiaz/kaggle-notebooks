{"cells":[{"metadata":{"_uuid":"9f65b09a84ebc441ace81e26ddc7cfa588d56a12"},"cell_type":"markdown","source":"Reference:\n\nhttps://www.kaggle.com/bmarcos/image-recognition-gender-detection-inceptionv3/notebook"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Dataset information\n\nContent\n\nOverall 202,599 number of face images of various celebrities 10,177 unique identities, but names of identities are not given 40 binary attribute annotations per image 5 landmark locations\n\nData Files\n\nimg_align_celeba.zip: \nAll the face images, cropped and aligned\n\nlist_eval_partition.csv: \nRecommended partitioning of images into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing\n\nlist_bbox_celeba.csv: \nBounding box information for each image. \"x_1\" and \"y_1\" represent the upper left point coordinate of bounding box. \"width\" and \"height\" represent the width and height of bounding box\n\nlist_landmarks_align_celeba.csv: \nImage landmarks and their respective coordinates. There are 5 landmarks: left eye, right eye, nose, left mouth, right mouth\n\nlist_attr_celeba.csv: \nAttribute labels for each image. There are 40 attributes. \"1\" represents positive while \"-1\" represents negative"},{"metadata":{"trusted":true,"_uuid":"4b3de90caef6955878f2c13e95fc9805246a0ea0"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fea4e9ee3df00c4bd820bb907c1c38e657bea70"},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dropout, Dense, Flatten, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff58f784b889fe4886bbc8d11cd46a8870643c3"},"cell_type":"code","source":"from IPython.core.display import display, HTML\nfrom PIL import Image\nfrom io import BytesIO\nimport base64\nplt.style.use(\"ggplot\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"630fb9549f24095d3a3b2da19fedb83571c8d965"},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c19ce234e6553a5980c43b0a48307520b576991"},"cell_type":"markdown","source":"Step 1: Data Exploration\n\nWe will be using the CelebA Dataset, which includes images of 178 x 218 px. Below is an example of how the pictures looks like."},{"metadata":{"trusted":true,"_uuid":"124dc5774eb33bd10614508cd754d7656fd4b36b"},"cell_type":"code","source":"main_folder = \"../input/celeba-dataset/\"\nimages_folder = main_folder + \"img_align_celeba/img_align_celeba/\"\n\nexample_pic = images_folder + \"000506.jpg\"\n\ntraining_sample = 10000\nvalidation_sample = 2000\ntest_sample = 2000\nimg_width = 178\nimg_height = 218\nbatch_size = 16\nnum_epochs = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c449cf57bcf9737d3c5296f01509d965592dc94"},"cell_type":"markdown","source":"Load the attributes of every picture\n\nFile: list_attr_celeba.csv"},{"metadata":{"trusted":true,"_uuid":"517b52b12ba95b74ab1dd822f7c8160ba9677b08"},"cell_type":"code","source":"df_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\ndf_attr.set_index('image_id', inplace=True)\ndf_attr.replace(to_replace=-1, value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a0feece7d7ce6aec1fd9b52c3a58ad3eec4734c"},"cell_type":"code","source":"df_attr.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d18d0d84dc3672ae43bf31d027c1e7faa0f8e800"},"cell_type":"code","source":"df_attr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72b72f2fff8ec86b8310ac96cb108b0cab6e44b1"},"cell_type":"code","source":"df_attr.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31d18bf3037eadb3310732134ee2da8a8db9f804"},"cell_type":"code","source":"df_attr.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"143aab7e26bccf947eab8d6e51aff6cde6d1dfd4"},"cell_type":"code","source":"df_attr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2176995fc170ad939111407bde504ef16f2a6f2"},"cell_type":"code","source":"for i,j in enumerate(df_attr.columns):\n    print(i+1, j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53b4840981676c5a9c3afd8f0c21fb138c3d29fa"},"cell_type":"code","source":"# load a example image\n\nimg = load_img(example_pic)\nplt.grid(False)\nplt.imshow(img)\ndf_attr.loc[example_pic.split('/')[-1]][['Smiling','Male',\"Young\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e057015ac86714ae2bcfd2af0a539895e00d910"},"cell_type":"markdown","source":"Distribution of the Attribute\n\nAs specified before, this Notebook is an image recognition project of the Gender. There are more Female gender than Male gender in the data set. This give us some insight about the need to balance the data in next steps."},{"metadata":{"trusted":true,"_uuid":"ffe34406387ba8665fe9da4af273ff40686bba1b"},"cell_type":"code","source":"sns.countplot(df_attr[\"Male\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f06a88bf625831fd4ffa39f927732ed7ef626b57"},"cell_type":"markdown","source":"Step 2: Split Dataset into Training, Validation and Test\n\nThe recommended partitioning of images into training, validation, testing of the data set is:\n\n1-162770 are training\n162771-182637 are validation\n182638-202599 are testing\n\nThe partition is in file list_eval_partition.csv\n\nDue time execution, by now we will be using a reduced number of images:\n\nTraining 20000 images\nValidation 5000 images\nTest 5000 Images"},{"metadata":{"trusted":true,"_uuid":"6201c3e7d1e323962eb90c66ac8fe3c1b88d7e27"},"cell_type":"code","source":"df_partition = pd.read_csv(main_folder + \"list_eval_partition.csv\")\ndf_partition.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd15d28286f4f017aa084a2f353e9cb5d17ba93f"},"cell_type":"code","source":"df_partition.sample(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c64ca72f3e9fae8bfa060b9050f7e91fcbfc72c"},"cell_type":"markdown","source":"0 =====> training\n\n1 =====> validation\n\n2 =====> testing"},{"metadata":{"trusted":true,"_uuid":"5057ba2b3e6d043dfe225823f58751aea572f8ff"},"cell_type":"code","source":"df_partition[\"partition\"].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9847ce66ccf9f5e6498eed71ae5ef52905ca7cc2"},"cell_type":"markdown","source":"Join the partition and the attributes in the same data frame"},{"metadata":{"trusted":true,"_uuid":"495285ae15fc4e39cdad792a21e56c68554a38aa"},"cell_type":"code","source":"df_partition.set_index('image_id', inplace=True)\ndf_par_attr = df_partition.join(df_attr[\"Male\"], how=\"inner\")\n\ndf_par_attr.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64d1588a5f86673c3452e73d41212092b7eaca9b"},"cell_type":"code","source":"df_par_attr.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"658fbcc31734cf82d332dcfd200a3ff08d2d9ae4"},"cell_type":"markdown","source":"Generate Partitions (Train, Validation, Test)\n\nNumber of images need to be balanced in order to get a good performance for the model, each model will have its own folder of training, validation and test balanced data.\n\nOn this step we will create functions that will help us to create each partition."},{"metadata":{"trusted":true,"_uuid":"a6697c5112079d290b4c21002f9a62f5a808a075"},"cell_type":"code","source":"def load_reshape_img(fname):\n    img = load_img(fname)\n    x = img_to_array(img)/255.\n    x = x.reshape((1,)+x.shape)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0667103bece0adacbad45aa04883e13343a744f3"},"cell_type":"code","source":"def generate_df(partition, attr, num_samples):\n    \n    df_ = df_par_attr[(df_par_attr['partition'] == partition) \n                           & (df_par_attr[attr] == 0)].sample(int(num_samples/2))\n    df_ = pd.concat([df_,\n                      df_par_attr[(df_par_attr['partition'] == partition) \n                                  & (df_par_attr[attr] == 1)].sample(int(num_samples/2))])\n\n    # for Train and Validation\n    if partition != 2:\n        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n        x_ = x_.reshape(x_.shape[0], 218, 178, 3)\n        y_ = np_utils.to_categorical(df_[attr],2)\n        \n    # for Test\n    else:\n        x_ = []\n        y_ = []\n\n        for index, target in df_.iterrows():\n            im = cv2.imread(images_folder + index)\n            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (img_width, img_height)).astype(np.float32) / 255.0\n            im = np.expand_dims(im, axis =0)\n            x_.append(im)\n            y_.append(target[attr])\n\n    return x_, y_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67e7ed4b750c942b33463f39560c21b8abd61be5"},"cell_type":"markdown","source":"Pre-processing Images: Data Augmentation\n\nGenerates Data Augmentation for images.\n\nData Augmentation allows to generate images with modifications to the original ones. The model will learn from these variations (changing angle, size and position), being able to predict better never seen images that could have the same variations in position, size and position.\n\nLet's start with an example: Data Augmentation\n\nThis is how an image will look like after data augmentation (based in the giving parameters below)."},{"metadata":{"trusted":true,"_uuid":"ed30048868830cbc306e32b25bd41fcb579a2d2c"},"cell_type":"code","source":"# generate image generator for data augmentation\n\ndatagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n\n# load one image and reshape\n\nimg = load_img(example_pic)\nx = img_to_array(img)/255.\nx = x.reshape((1,) + x.shape)\n\n# plot 10 augmented images of the loaded image\n\nplt.figure(figsize=(20,10))\nplt.suptitle(\"Data augmentation\", fontsize=28)\n\ni = 0\n\nfor batch in datagen.flow(x, batch_size=1):\n    plt.subplot(3,5,i+1)\n    plt.grid(False)\n    plt.imshow(batch.reshape(218,178, 3))\n    \n    if i==9:\n        break\n    i = i+1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e912f31353606a0cd4347da913f3bc802ce9131"},"cell_type":"markdown","source":"The result is a new set of images with modifications from the original one, that allows to the model to learn from these variations in order to take this kind of images during the learning process and predict better never seen images."},{"metadata":{"trusted":true,"_uuid":"2e58944b93c0d35afce5161b5e8974fe215f7069"},"cell_type":"code","source":"# build data generators\n\n# train data\n\nx_train, y_train = generate_df(0, \"Male\", training_sample)\n\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n\ntrain_datagen.fit(x_train)\n\ntrain_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cbeb27820b9cba19f4ee45815d3e11fd89ef6c8"},"cell_type":"code","source":"# validation data\n\nx_valid, y_valid = generate_df(1, \"Male\", validation_sample)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"762e0e554d5d0bff9b224e9294273ea59ca9e186"},"cell_type":"markdown","source":"With the data generator created and data for validation, we are ready to start modeling.\n\nBuild the Model - Gender Recognition\n\nSet the Model"},{"metadata":{"trusted":true,"_uuid":"7f9921afe0b43c74f4fbdf522fc03c2f98c77eee"},"cell_type":"code","source":"# import inceptionv3 model\n\ninc_model = InceptionV3(weights=\"../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, input_shape=(img_height,img_width,3))\n\nprint(\"number of layers in the model : \", len(inc_model.layers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d940e878feb67f36864bd400d8dfd690627fc42c"},"cell_type":"markdown","source":"The top layers (including classification) are not included. These layers will be replaced for the following layers:\n\nadding custom layers"},{"metadata":{"trusted":true,"_uuid":"731275bfc1219bf853919b6703465b001dedb901"},"cell_type":"code","source":"x = inc_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\npredictions = Dense(2, activation='softmax')(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"826a389a679e1a8005b5856037b6d326b5889f4d"},"cell_type":"code","source":"# creating the final model\n\nmodel_ = Model(inputs=inc_model.input, outputs=predictions)\n\n# lock initial layers to not to be trained\n\nfor layer in model_.layers[:52]:\n    layer.trainable = False\n    \n# compile the model\n\nmodel_.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50450880d51c3d6738437de4bede6cd4098c74b3"},"cell_type":"code","source":"# train the model\n\ncheckpointer = ModelCheckpoint(filepath='weights.best.inc.male.hdf5', verbose=1, save_best_only=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d803acbe9f51a34fc21afcbf72d829ec104ddeb6"},"cell_type":"code","source":"hist = model_.fit_generator(train_generator, validation_data=(x_valid, y_valid), steps_per_epoch=training_sample/batch_size, epochs=num_epochs, callbacks=[checkpointer], verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec503f168d996d11adcae87e27a6f15c292ec09e"},"cell_type":"code","source":"# plot loss with epochs\n\nplt.figure(figsize=(18,4))\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='validation')\nplt.legend()\nplt.title('loss function')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab6e2062c8c2b3bc50b39092b07394d66cfebfd"},"cell_type":"code","source":"# Plot accuracy through epochs\nplt.figure(figsize=(18, 4))\nplt.plot(hist.history['acc'], label = 'train')\nplt.plot(hist.history['val_acc'], label = 'valid')\nplt.legend()\nplt.title('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5196d43ac99838ef186b6629b15c89f342a8904e"},"cell_type":"markdown","source":"model evaluation"},{"metadata":{"trusted":true,"_uuid":"41d7e2481832f1925012972691aed835008dcd08"},"cell_type":"code","source":"# load the best model\n\nmodel_.load_weights('weights.best.inc.male.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac8d7db43ab25ae364463fa60750932ee81d0667"},"cell_type":"code","source":"# test data\n\nx_test, y_test = generate_df(2, 'Male', test_sample)\n\n# generate predictions\n\nmodel_prediction = [np.argmax(model_.predict(feature)) for feature in x_test]\n\n# report test accuracy\n\ntest_accuracy = 100 * (np.sum(np.array(model_prediction)==y_test)/len(model_prediction))\nprint('model evaluation')\nprint(\"test accuracy : \", test_accuracy)\nprint('f1 score : ', f1_score(y_test, model_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c170c15a0a112799a17291488b36e7311b7aad6f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}