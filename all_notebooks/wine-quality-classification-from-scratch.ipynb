{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing necessary Library","metadata":{}},{"cell_type":"code","source":"import operator as op\nimport random\nrandom.seed(123)\n\nimport numpy as np \nimport pandas as pd \nfrom math import sqrt\nfrom random import randrange\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as skm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize': (12,8)})\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-15T01:19:36.023551Z","iopub.execute_input":"2021-07-15T01:19:36.025391Z","iopub.status.idle":"2021-07-15T01:19:37.331571Z","shell.execute_reply.started":"2021-07-15T01:19:36.025041Z","shell.execute_reply":"2021-07-15T01:19:37.330098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data import and first exploration","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/wine-quality-binary-classification/wine.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:37.33433Z","iopub.execute_input":"2021-07-15T01:19:37.334845Z","iopub.status.idle":"2021-07-15T01:19:37.416685Z","shell.execute_reply.started":"2021-07-15T01:19:37.334794Z","shell.execute_reply":"2021-07-15T01:19:37.414687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:37.418056Z","iopub.execute_input":"2021-07-15T01:19:37.418444Z","iopub.status.idle":"2021-07-15T01:19:37.704765Z","shell.execute_reply.started":"2021-07-15T01:19:37.418414Z","shell.execute_reply":"2021-07-15T01:19:37.703634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding categorical variable\ndf['quality_cat'] = df['quality'].astype('category').cat.codes\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:37.706595Z","iopub.execute_input":"2021-07-15T01:19:37.706933Z","iopub.status.idle":"2021-07-15T01:19:37.7368Z","shell.execute_reply.started":"2021-07-15T01:19:37.706903Z","shell.execute_reply":"2021-07-15T01:19:37.735276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = np.corrcoef(df.corr())\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df.corr(), annot=True, mask=mask)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:37.738509Z","iopub.execute_input":"2021-07-15T01:19:37.738871Z","iopub.status.idle":"2021-07-15T01:19:38.619995Z","shell.execute_reply.started":"2021-07-15T01:19:37.738836Z","shell.execute_reply":"2021-07-15T01:19:38.619117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the highest correlated pairs\nsns.scatterplot(data=df, x='density', y='alcohol', hue='quality')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:38.621219Z","iopub.execute_input":"2021-07-15T01:19:38.6217Z","iopub.status.idle":"2021-07-15T01:19:39.034759Z","shell.execute_reply.started":"2021-07-15T01:19:38.621669Z","shell.execute_reply":"2021-07-15T01:19:39.033407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Nearest Neighbors\n#### A machine learning model hat uses euclidean distance between data points. In other words, clusters are created by proximity from which we make further predictions","metadata":{}},{"cell_type":"code","source":"models = {}","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:39.037406Z","iopub.execute_input":"2021-07-15T01:19:39.037744Z","iopub.status.idle":"2021-07-15T01:19:39.042018Z","shell.execute_reply.started":"2021-07-15T01:19:39.037714Z","shell.execute_reply":"2021-07-15T01:19:39.040711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class kNN():\n    def __init__(self, num_k):\n        # Number of k clusters\n        self.num_k = num_k\n    \n    def fit(self, X_train, Y_train):\n        # Splitting the dataset into training and test dataset\n        self.X_train = X_train\n        self.Y_train = Y_train\n        \n        self.m, self.n = X_train.shape\n\n    def predict(self, X_test):\n        # Feed into the algorithm the test dataset to make some predictions\n        self.X_test = X_test\n        self.m1 = X_test.shape[0]\n        preds = np.zeros(self.m1)\n        for i in range(self.m1):\n            x = self.X_test[i]\n            neighs = np.zeros(self.num_k)\n            neighs = self.get_neighbors(x)\n            preds[i] = mode(neighs)[0][0]\n\n        return preds\n    \n    def accuracy(self, Y_test, Y_pred):\n        return np.sum(Y_test == Y_pred) / len(Y_test)\n\n    def get_neighbors(self, x):\n        # Locate the most simlar neighbors given a number of k clusters\n        distances = np.zeros(self.m)\n        for i in range(self.m):\n            distances[i] = self.euclidean_distance(x, self.X_train[i])\n            \n        # Sorting according to euc. dist.\n        sorted_dist = distances.argsort()\n        Y_train_sorted = self.Y_train[sorted_dist]\n\n        return Y_train_sorted[:self.num_k]\n    \n    def euclidean_distance(self, X1, X2):\n        # Calculating euclidean distance between two vectors (here data points)\n        return np.sqrt(np.sum((X1-X2)**2))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:39.043985Z","iopub.execute_input":"2021-07-15T01:19:39.044436Z","iopub.status.idle":"2021-07-15T01:19:39.058202Z","shell.execute_reply.started":"2021-07-15T01:19:39.044406Z","shell.execute_reply":"2021-07-15T01:19:39.057168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression\n#### Based on a sigmoid function, outputting value (probability) between 0 and 1. Using Gradient Descent, we will attempt to iterate through the epochs at step alpha to minime the errors in order to find the global minimu if possible.","metadata":{}},{"cell_type":"code","source":"class LogReg():\n    def __init__(self, alpha, epochs):\n        # step alpha, epoch as no. of iterations\n        self.alpha = alpha\n        self.epochs = epochs\n        \n    def fit(self, X, y):\n        # Splitting it the dataset into training and test dataset and initialize the parameters\n        self.X = X\n        self.y = y\n        self.m, self.n = X.shape\n        self.theta = np.ones(self.n)\n        self.gradient_descent(X, y, self.theta, self.alpha, self.epochs)\n    \n    def gradient_descent(self, X, y, theta, alpha, epochs):\n        # Base algorithm of gradient descent (GD)\n        J = [self.cost(X, y, theta)] \n        for i in range(0, epochs):\n            h = self.hypothesis(X, theta)\n            for i in range(0, self.n):\n                theta[i] -= (alpha/self.m) * np.sum((h-y)*X[:, i])\n            J.append(self.cost(X, y, theta))\n        return J, theta\n    \n    def cost(self, X, y, theta):\n        # Cost funtion (minmizing the cost)\n        h = self.hypothesis(X, theta)\n        y_0 = y * np.log(h)\n        y_1 = (1 - y) * np.log(1 - h)\n        return -(1/self.m) * sum(y_0 + y_1)\n    \n    def hypothesis(self, X, theta):\n        # Calculating the hypothesis using the wights\n        z = np.dot(theta, X.T)\n        return 1/(1 + np.e**(-z))\n    \n\n    def predict(self, X, threshold):\n        # Predicting using the algo GD\n        J, th = self.gradient_descent(self.X, self.y, self.theta, \n                                      self.alpha, self.epochs) \n        h = self.hypothesis(X, self.theta) \n        return [1 if i >= threshold else 0 for i in h]\n    \n    def accuracy(self, y_test, y_pred):\n        # Actual - predicted to obtain accuracy rate\n        return np.sum(y_test == y_pred) / len(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:39.059952Z","iopub.execute_input":"2021-07-15T01:19:39.060327Z","iopub.status.idle":"2021-07-15T01:19:39.077873Z","shell.execute_reply.started":"2021-07-15T01:19:39.060287Z","shell.execute_reply":"2021-07-15T01:19:39.07668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing for the binary categories\ndef visualize(Y_test, pred_knn, pred_log):\n    # Benchmark for prediction of only 1s (only men)\n    bm_pred = [0 for _ in range(len(Y_test))]\n    bm_auc  = roc_auc_score(Y_test, bm_pred)\n    bm_fpr, bm_tpr, _ = roc_curve(Y_test, bm_pred)\n\n    knn_auc = roc_auc_score(Y_test, pred_knn)\n    log_auc = roc_auc_score(Y_test, pred_log)\n    knn_fpr, knn_tpr, _ = roc_curve(Y_test, pred_knn)\n    log_fpr, log_tpr, _ = roc_curve(Y_test, pred_log)\n\n    \n    plt.plot(bm_fpr, bm_tpr, linestyle='--', label='Benchmark')\n    plt.plot(knn_fpr, knn_tpr, marker='.', label='kNN')\n    plt.plot(log_fpr, log_tpr, marker='.', label='Log Reg')\n    text = 'BM =%.2f, K-NN =%.2f, Logistic Regression =%.2f'\n    plt.title(text % (bm_auc, knn_auc, log_auc))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:39.079394Z","iopub.execute_input":"2021-07-15T01:19:39.079771Z","iopub.status.idle":"2021-07-15T01:19:39.097543Z","shell.execute_reply.started":"2021-07-15T01:19:39.07974Z","shell.execute_reply":"2021-07-15T01:19:39.096228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['quality_cat', 'quality'], axis=1).values\nY = df['quality_cat'].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                                    test_size = 0.2, \n                                                    random_state = 2)\n# Running knn with the elbow method\nclf = kNN(4)\nclf.fit(X_train, Y_train)\npred_knn = clf.predict(X_test)\nmodels['k-NN'] = clf.accuracy(Y_test, pred_knn)\n\n# Running Logistic Regression\ninter_0 = np.ones((X_train.shape[0], 1))\ninter_1 = np.ones((X_test.shape[0], 1))\nX_train = np.hstack((inter_0, X_train))\nX_test = np.hstack((inter_1, X_test))\n\nmodel = LogReg(0.001, 10000)\nmodel.fit(X_train, Y_train)\npred_log = model.predict(X_test, 0.5)\n\nvisualize(Y_test, pred_knn, pred_log)\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T01:19:39.099199Z","iopub.execute_input":"2021-07-15T01:19:39.099523Z","iopub.status.idle":"2021-07-15T01:20:04.508898Z","shell.execute_reply.started":"2021-07-15T01:19:39.099495Z","shell.execute_reply":"2021-07-15T01:20:04.507646Z"},"trusted":true},"execution_count":null,"outputs":[]}]}