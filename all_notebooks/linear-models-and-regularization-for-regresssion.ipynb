{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mglearn==0.1.9","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport sys\nfrom scipy import sparse\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\nimport mglearn\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear models for Regression\n**For regression, the general prediction formula for a linear model looks as follows: ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b**","metadata":{}},{"cell_type":"code","source":"mglearn.plots.plot_linear_regression_wave()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linear models are class of models that make predictions using linear functions of the input features.**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression().fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*The “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute*","metadata":{}},{"cell_type":"code","source":"print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*The test score of around 0.66 is not very impressive, but we can see training and test score are very close to each other. This implies that we are likely underfitting and not overfitting. Although for this one dimensional dataset there is a little danger of overfitting as the model is very simple. Moreover, with higher dimensional datasets which has large number of features, linear models become quite powerful and thus more chance of overfitting.*","metadata":{}},{"cell_type":"markdown","source":"**Let's analyze how Linear Regression behaves on a more complex dataset like - Boston Housing dataset**","metadata":{}},{"cell_type":"code","source":"X, y = mglearn.datasets.load_extended_boston()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlr = LinearRegression().fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*This is a clear overfitting as the performance in training set is higher and there is large difference between performance in training and test set. Therefore, we should try to find a model that allows us to control complexity. One of the most common alternatives to standard linear regression is ridge regression. Let's look at it next.*","metadata":{}},{"cell_type":"markdown","source":"# What is Ridge Regression?","metadata":{}},{"cell_type":"markdown","source":"**It is a linear model for regression. The formula is the same one used for ordinary least squares. In ridge regression, the coefficients (w) are chosen not only so that they predict well on the training data, but they also fit an additional constraint. This constraint is an example of what is called regularization. Regularization means explicitly restricting a model to overfitting. This particular kind used by rudge regression is known as L2 regularization.**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now the training set score is lower than the previous one. In case of Linear regression, the data was overfitting but ridge is a more restricted model so we are less likely to overfit. A less complex model ensures worse performance on training set but leading to better generalization. So, ridge regression is much better than Linear regression model.**","metadata":{}},{"cell_type":"markdown","source":"*The ridge model makes a trade-off between the simplicity of the model(near-zero coefficients) and its performance on the training set. Importance of the model on simplicity versus training set performance can be specified by the alpha parameter. The optimum setting if alpha depends on the particular dataset we are using. Increasing alpha forces coefficients to move more towards zero, which decreases training set performance but might help generalization.*","metadata":{}},{"cell_type":"code","source":"ridge10 = Ridge(alpha=10).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#setting alpha=0.1 will give better score\n\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\nplt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\nplt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\nplt.plot(lr.coef_, 'o', label=\"LinearRegression\")\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.hlines(0, 0, len(lr.coef_))\nplt.ylim(-25, 25)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated\nwith the first feature, x=1 the coefficient associated with the second feature, and so on\nup to x=100. The y-axis shows the numeric values of the corresponding values of the\ncoefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\nbetween around –3 and 3. The coefficients for the Ridge model with alpha=1 are\nsomewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,\nand many of the dots corresponding to linear regression without any regularization\n(which would be alpha=0) are so large they are outside of the chart.**","metadata":{}},{"cell_type":"code","source":"mglearn.plots.plot_ridge_n_samples()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The training score is higher than the test score for all dataset sizes, for both ridge and linear regression. Because ridge is regularized, the training score of ridge is lower than the training score for linear regression across the board.\nHowever, the test score for ridge is better, particularly for small subsets of the data. For less than 400 data points, linear regression is not able to learn anything. As more and more data becomes available to the model, both models improve, and linear regression catches up with ridge in the end.**","metadata":{}},{"cell_type":"markdown","source":"> An alternative to Ridge for regularizing linear regression is lasso. As like ridge regression, lasso also restricts coefficients to be close to zero, but in a different way called L1 regularization. The consequence of L! regularization is some coefficients are wxactly zero, i.e, some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Let's see how lasso work with the extended Boston Housing dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we increase the default setting of \"max_iter\",\n# otherwise the model would warn us that we should increase max_iter.\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A lower alpha allowed us to fit a more complex model, which worked better on the\ntraining and test data. The performance is slightly better than using Ridge, and we are\nusing only 33 of the 105 features.**","metadata":{}},{"cell_type":"code","source":"lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\nplt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\nplt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\nplt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\nplt.legend(ncol=2, loc=(0, 1.05))\nplt.ylim(-25, 25)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For alpha=1, we not only see that most of the coefficients are zero (which we already knew), but that the remaining coefficients are also small in magnitude. Decreasing alpha to 0.01, we obtain the solution shown as the green dots, which causes most features to be exactly zero. Using alpha=0.00001, we get a model that is quite unregularized, with most coefficients nonzero and of large magnitude. For comparison, the best Ridge solution is shown in teal. The Ridge model with alpha=0.1 has similar predictive performance as the lasso model with alpha=0.01, but using Ridge, all coefficients are nonzero. In practice, ridge regression is usually the first choice between these two models. However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a model that is easy to interpret, Lasso will provide a model that is easier to understand, as it will select only a subset of the input features.**","metadata":{}}]}