{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# importing needed libraries\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n# data preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# importing the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/vehicle-dataset-from-cardekho/car data.csv')\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# isolating the dependent variable ( selling price )\n\nthe 'Fuel_type' column is a categorical variable , where the category 'CNG' appears only twice\nfor simplicity , the records with 'Fuel_Type' == 'CNG are not considered  ( they are the 19th and 37th records)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_data=dataset.iloc[:,[2]]\ny=y_data.values\n\n#removing cng cars\ny=np.delete(y,[18,36],0)\n\n#visual\ny_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# isolating the feature matrix\nand the categorical variables are one hot encoded","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = dataset.iloc[:,1:]\n\n#encoding categorical variables\n\nohc_5 = ce.OneHotEncoder(cols=['Transmission','Seller_Type','Fuel_Type'])\nx_data=ohc_5.fit_transform(x_data);\nx_orig=x_data.values\n\n#removing records with 'Fuel_Type' == 'CNG'\nx_orig=np.delete(x_orig,[18,35],0)\n\n#avoiding dummy variable trap\nx_orig=np.delete(x_orig,[1,5,6,8,10],1)\n\n#visual\nx_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" in the above table , columns : 6,7,9,11 are removed to avoid the dummy variable trap and  column : 2 removed as it is the dependent variable\n                                 ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# feature scaling \nhere **standardization** is used to scale the feature matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_x=StandardScaler()\nx=sc_x.fit_transform(x_orig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# adding the intercept term (aka bias term )\nthe intercept term will act as the constant in the hypothesis equation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"[x_row,x_col]=x.shape\nx=np.append(arr=np.ones((x_row,1)).astype(float),values=x,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# splitting the training and test sets\nhere cross validation set is not used , since training set is small and regularization is not done","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>with this the data preprocessing is complete</h2>\n<hr>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# here we are going to use 2 different models\n<h2> <ul> <li> 1. defining the cost function and performing gradient descent manually </li>\n    <li> 2. using a linear regression model from the scikit-learn library </li></ul> </h2>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Manually optimizing the cost function","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n# defining the cost function  (Mean Squared Error)  \n<h3> parameters :</h3><ul><li> 'x' is the feature matrix </li>  <li>'y' is the dependent variable </li> <li> 'theta' is the parameter vector of the hypothesis</li></ul>\n    <h3> return values :</h3><ul><li>'J' is the cost aka 'error' is returned</li></ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost(x,y,theta):\n    [m,n]=x.shape\n    h=np.dot(x,theta);\n    J=(1/(2*m))*np.sum((h-y)**2)\n    return J","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# defining the gradient descent method (Batch gradient descent is used)\n<h2>parameters : </h2> <ul><li> 'x' is the feature matrix </li>  <li>'y' is the dependent variable </li> <li> 'theta' is the parameter vector of the hypothesis</li><li>'iterations' is the number of iterations the gradient descent will run</li>  <li> 'alpha' is the learning rate     </li></ul>\n<h2>return values : </h2><ul><li>'theta_opt' is the optimized theta</li>  <li>'grad_history' is the the array of gradients of each iteration</li>  <li>'J_history is the array of costs at each iteration</li></ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(x,y,theta,iterations,alpha):\n    [m,n]=x.shape\n    theta_opt=theta\n    J_history=np.zeros((iterations,1))\n    grad_history=np.zeros((iterations,n))\n    grad_history=grad_history.reshape(iterations,n)\n    \n    for i in range(iterations):\n        h=np.dot(x,theta_opt);\n        grad = (alpha/m)*(np.sum(((h-y)*x),axis=0))\n        grad=grad.reshape(-1,1)\n        theta_opt= theta_opt - grad\n        J_history[i]=cost(x,y,theta_opt)\n        grad_history[i]=theta_opt.transpose()\n        \n    return theta_opt,grad_history,J_history   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>performing gradient descent</h1>\n<ul><li>initially theta parameters are set randomly</li> <li> we use 1000 iterations </li><li> an alpha rate of 0.01</li></ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"theta=np.random.randn(x_col+1,1)\niterations=1000\n\ntheta_opt,grad_history,J_history = gradient_descent(x_train,y_train,theta,iterations,0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Evaluating the gradient descent algorithm </h1>\n<p><h3> a graph is ploted with the x axis being the no of iterations and y axis being the cost error after that many iterations </h3></p> \n<p><h3> the cost is expected to decrease very rapidly in the beginning <br/> this rate of decrease in the cost is expected to slow down and later flatten out</h3></p>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_plt=np.array([range(iterations)]).transpose()\nplt.plot(iter_plt,J_history)\nplt.xlabel('iterations')\nplt.ylabel('cost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><h2>behaviour of the plot is as expected , hence this confirms that gradient descent was implemented correctly </h2></p>\n<hr>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Cost optimization using scikit learn library","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p><h2>the scikit learn libraries are imported <br/>an object of the linear regression class is made which acts as our multivariate linear regressor</h2></p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><h2>the regressor is fit to the training set (this means the  model will learn from the training set )<br/>the predictions of the trained model on the test set are made</h2></p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(x_train,y_train)\ny_pred_sk=lr.predict(x_test)\ny_pred_sk=y_pred_sk.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n# now we compare the performances of each of the models using their mean squared error as the evaluation metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cost_SK=(1/(2*len(y)))*np.sum((y_pred_sk-y_test)**2)\ncost_GD = cost(x_test,y_test,theta_opt)\nprint(\"the mean squared error of the sckikit learn model is \",cost_SK)\nprint(\"and the mean squared error of the cost of the model that was made manually is \",cost_GD)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p><h2>if we had more data , feature creation can be done to increase the variance of the model <br/>and this new  model can be trained on a larger dataset to balance the higher variance and this could lead to a more robust model.<br/>if variance is too high , regularization can be applied.<br/>that will be all :).</h2></p>   ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}