{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dirname = \"/kaggle/input/stacksample\"\n\nquestions_csv = os.path.join(dirname, \"Questions.csv\")\nanswers_csv = os.path.join(dirname, \"Answers.csv\")\ntags_csv = os.path.join(dirname, \"Tags.csv\")\n\nENCODING = 'ISO-8859-1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The questions\n\ndf_q = pd.read_csv(questions_csv, encoding=ENCODING)\n\ndf_q.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the tags\n\ndf_t = pd.read_csv(tags_csv, encoding=ENCODING)\n\ndf_t.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t['Tag'] = df_t['Tag'].astype(str)\n\n# group all tags given to same question into a single string\ngrouped_tags = df_t.groupby('Id')['Tag'].apply(lambda tags: ' '.join(tags))\n\ngrouped_tags.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reset index for simplicity\ngrouped_tags.reset_index()\n\ndf_tags_final = pd.DataFrame({'Id': grouped_tags.index, 'Tags': grouped_tags.values})\n\ndf_tags_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unnecessary columns from questions\ndf_q.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge questions and tags into a single dataframe\ndf = df_q.merge(df_tags_final, on='Id')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove questions with score lower than 5\ndf = df[df['Score'] > 5]\n\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split tags into list\ndf['Tags'] = df['Tags'].apply(lambda tags: tags.lower().split())\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all tags in the dataset\nall_tags = []\n\nfor tags in df['Tags'].values:\n    for tag in tags:\n        all_tags.append(tag)\n        \nprint(all_tags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\n# create a frequency list of the tags\ntag_freq = nltk.FreqDist(list(all_tags))\n\n# get most common tags\ntag_freq.most_common(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the most common 50 tags without the count\ntag_features = list(map(lambda x: x[0], tag_freq.most_common(50)))\n\nprint(tag_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the tags from the dataset and remove all tags that does not belong to the tag_features\ndef keep_common(tags):\n    \n    filtered_tags = []\n    \n    # filter tags\n    for tag in tags:\n        if tag in tag_features:\n            filtered_tags.append(tag)\n    \n    # return the filtered tag list\n    return filtered_tags\n\n# apply the function to filter in dataset\ndf['Tags'] = df['Tags'].apply(lambda tags: keep_common(tags))\n\n# set the Tags column as None for those that do not have a most common tag\ndf['Tags'] = df['Tags'].apply(lambda tags: tags if len(tags) > 0 else None)\n\n# Now we will drop all the columns that contain None in Tags column\ndf.dropna(subset=['Tags'], inplace=True)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\n\ntokenizer = ToktokTokenizer()\nstemmer = SnowballStemmer('english')\nstop_words = set(stopwords.words('english'))\n\n# Preprocess the text for vectorization\n# - Remove HTML\n# - Remove stopwords\n# - Remove special characters\n# - Convert to lowercase\n# - Stemming\n\ndef remove_html(text):\n    # Remove html and convert to lowercase\n    return re.sub(r\"\\<[^\\>]\\>\", \"\", text).lower()\n\ndef remove_stopwords(text):    \n    # tokenize the text\n    words = tokenizer.tokenize(text)\n    \n    filtered = [w for w in words if not w in stop_words]\n    return ' '.join(map(str, filtered))\n\ndef remove_punc(text):\n    #tokenize\n    tokens = tokenizer.tokenize(text)\n    \n    # remove punctuations from each token\n    tokens = list(map(lambda token: re.sub(r\"[^A-Za-z0-9]+\", \" \", token).strip(), tokens))\n    \n    # remove empty strings from tokens\n    tokens = list(filter(lambda token: token, tokens))\n    \n    return ' '.join(map(str, tokens))\n\ndef stem_text(text):\n    #tokenize\n    tokens = tokenizer.tokenize(text)\n    \n    # stem each token\n    tokens = list(map(lambda token: stemmer.stem(token), tokens))\n    \n    return ' '.join(map(str, tokens))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop Id and Score columns since we don't need them\ndf.drop(columns=['Id', 'Score'], inplace=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply preprocessing to title and body\ndf['Title'] = df['Title'].apply(lambda x: remove_html(x))\ndf['Title'] = df['Title'].apply(lambda x: remove_stopwords(x))\ndf['Title'] = df['Title'].apply(lambda x: remove_punc(x))\ndf['Title'] = df['Title'].apply(lambda x: stem_text(x))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply preprocessing to title and body\ndf['Body'] = df['Body'].apply(lambda x: remove_html(x))\ndf['Body'] = df['Body'].apply(lambda x: remove_stopwords(x))\ndf['Body'] = df['Body'].apply(lambda x: remove_punc(x))\ndf['Body'] = df['Body'].apply(lambda x: stem_text(x))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_title = df['Title']\nX_body = df['Body']\ny = df['Tags']\n\n# binarize our tags \nbinarizer = MultiLabelBinarizer()\ny_bin = binarizer.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorize\nvectorizer_title = TfidfVectorizer(\n    analyzer = 'word', \n    strip_accents = None, \n    encoding = 'utf-8', \n    preprocessor=None, \n    max_features=10000)\n\nvectorizer_body = TfidfVectorizer(\n    analyzer = 'word', \n    strip_accents = None, \n    encoding = 'utf-8', \n    preprocessor=None, \n    max_features=10000)\n\nX_title_vect = vectorizer_title.fit_transform(X_title)\nX_body_vect = vectorizer_body.fit_transform(X_body)\n\nX = hstack([X_title_vect, X_body_vect])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Develop the model\nfrom sklearn.svm import LinearSVC\nfrom skmultilearn.problem_transform import BinaryRelevance # gives better precision\n\nsvc = LinearSVC()\nclf = BinaryRelevance(svc)\n\n# fit training data\nclf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, hamming_loss, f1_score\n\n# make prediction\ny_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate Accuracy\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\n# calculate recall\nprint(\"Recall:\", recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n\n# calculate precision\nprint(\"Precision: \", precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n\n# calculate hamming loss\nprint(\"Hamming Loss (%): \", hamming_loss(y_pred, y_test)*100)\n\n# calculate F1 score\nprint(\"F1 Score: \", f1_score(y_pred, y_test, average='weighted'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual Application\nq_title = \"How to handle or avoid a stack overflow in C++\"\nq_body = \"In C++ a stack overflow usually leads to an unrecoverable crash of the program. For programs that need to be really robust, this is an unacceptable behaviour, particularly because stack size is limited. A few questions about how to handle the problem. Is there a way to prevent stack overflow by a general technique. (A scalable, robust solution, that includes dealing with external libraries eating a lot of stack, etc.) Is there a way to handle stack overflows in case they occur? Preferably, the stack gets unwound until there's a handler to deal with that kinda issue. There are languages out there, that have threads with expandable stacks. Is something like that possible in C++? Any other helpful comments on the solution of the C++ behaviour would be appreciated.\"\n\n# preprocessing title and body\ndef preprocess_text(text):\n    text = remove_html(text)\n    text = remove_stopwords(text)\n    text = remove_punc(text)\n    text = stem_text(text)\n    \n    return text\n\nq_title = preprocess_text(q_title)\nq_body = preprocess_text(q_body)\n\nprint(\"Title:\", q_title)\nprint(\"Body:\", q_body)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_app_title = vectorizer_title.fit_transform([q_title])\n# X_app_body = vectorizer_body.fit_transform([q_body])\n\n# X_app = hstack([X_app_title, X_app_body])\n\n# y_app = clf.predict(X_app)\n\n# print(y_app)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}