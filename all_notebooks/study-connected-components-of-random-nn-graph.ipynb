{"cells":[{"metadata":{},"cell_type":"markdown","source":"Turn on GPU. \n\nNumber of connected components of the NN (nearest neigbour) graph for uniformly sampled points in [0,1]^d (newer version simulate for Gassian  ) is estimated by simulation.\n(It grows linearly with point number, so we are interested in that coefficient of linear dependence).\nTheoretical esimtations are discussed at:\nhttps://cstheory.stackexchange.com/questions/47034/number-of-connected-components-of-a-random-nearest-neighbor-graph\n\nOur simulations supports claim proposed by Neal Young for sampling in sphere or torus, but seems contradict that claim for uniformly or gaussianly distributed points. \n\nWe see some desrepancy (for high d) between theoretical estimates proposed  and numerical simulations here.\nIt might be that we are not simulating large enough number of points.\nFor small d, like d=2 simulation and theory fit each other. (The result for d=2 has been published in 2007, while higher d proposed just in the discussion above, has not yet been published ). \n\nWe use GPU to accelerate the calculations - \"RAPIDS package\"\n\nPS\n\nBy connected component we mean \"weakly\" connected component - in the other words  forgeting orientation\n"},{"metadata":{},"cell_type":"markdown","source":"\nV20 - torus \n\nV19 - sphere - repeat \n\nV18 - correction of V17 - to generate d-sphere we should take points in d+1 space - that affects only small dimensions, but still better to recalculate  \n\nV17 - new simulation for sphere (random points on sphere) \n\nV16 - new simulation for uniform data cloud for several dimensions, sizes of data clouds and each is simulated 100 times\n\nV14,15 - new simulation for Gaussian data cloud  for several dimensions, sizes of data clouds and each is simulated 100 times. (V15 was planned to be for uniform, but due to misprint appeared to be Gaussian again). \n\n\nV11,12,13 - test save \n\nVersion 10  - Gaussian Dimension 10 "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Use rapids - https://www.kaggle.com/cdeotte/rapids - some GPU acceleration for many algorithms\n# \n# Current notebook  is based on Dmitry Simakov notebook: https://www.kaggle.com/simakov/rapids-knn-cugraph-test\n\nif 0:  # That was necessary to do in the past - now we can just import cudf, cuml \n    # That cell may run 3-5 minutes. (And sometimes may hang on - if so - restart notebook and run again )\n    import sys\n    !cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n    !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n    sys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\n    sys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\n    sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n    !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n\n\ntry:\n    import cudf\nexcept:\n    print('Turn ON GPU ! Otherwise will not work')\n#import cugraph\nfrom cuml.neighbors import NearestNeighbors as cuNearestNeighbors\nimport numpy as np\nimport igraph\nimport time\nimport pandas as pd\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_type = 'torus' # 'sphere' # 'uniform'# 'Gaussian'\nn_trials = 100\nlist_sample_sizes =  [1e3,1e4,1e5, 1e6]\nlist_dim = [2,5, 10,15, 20, 50, 100]\nverbose = 10\n\n\n\nt00 = time.time()\nt0 = time.time()\ndf_stat = pd.DataFrame()\nc = 0\n\ntimer_for_print = time.time()\ntime_interval_for_print = 10 # seconds\n\nfor dim in list_dim:\n    if data_type == 'sphere':\n        dim += 1 # n-Sphere is constructed from R^{n+1}  \n        \n    for n_sample in list_sample_sizes: # [1e3,1e4]:# ,1e5,1e6,1e7]:\n        n_sample = int(n_sample)\n\n\n        n_neighbors = 2\n        for i in range(n_trials):\n            t0 = time.time()\n            c+=1\n            df_stat.loc[c,'Dim'] = dim\n            df_stat.loc[c,'sample size'] = n_sample\n\n            np.random.seed(n_sample + i)\n            if data_type == 'uniform':\n                X = np.random.rand(n_sample, dim) # uniform\n            elif data_type == 'sphere':\n                X = np.random.randn(n_sample, dim) # Gaussian\n                nm = np.sqrt( np.sum(X*X,axis = 1)  )\n                X = X / nm[:,np.newaxis]\n            elif data_type == 'torus':\n                X = ( np.random.rand(n_sample,dim) )\n                X1 = np.sin(2*np.pi*X)\n                X2 = np.cos(2*np.pi*X)\n                X = np.concatenate((X1,X2),axis=1)                \n            else:\n                X = np.random.randn(n_sample, dim) # Gaussian\n                \n            device_data = cudf.DataFrame(X) # cudf.DataFrame.from_gpu_matrix(X)\n\n            knn_cuml = cuNearestNeighbors(n_neighbors)\n            knn_cuml.fit(device_data)\n            D_cuml, I_cuml = knn_cuml.kneighbors(device_data, n_neighbors)\n\n\n            g = igraph.Graph(directed = True)\n            g.add_vertices(range(n_sample))\n            g.add_edges(I_cuml.to_pandas().values)\n            r = g.clusters(mode = 'WEAK')\n\n            df_stat.loc[c,'%Coeff'] = len(r) / n_sample * 100\n\n            comp_size = [len(tmp) for tmp in r ]\n            df_stat.loc[c,'Max comp size'] = np.max(comp_size)\n            df_stat.loc[c,'Mean comp size'] = np.mean(comp_size)\n            df_stat.loc[c,'Median comp size'] = np.median(comp_size)\n            df_stat.loc[c,'Seconds passed'] = time.time() - t0\n            df_stat.loc[c,'n_trials'] = n_trials\n            #print(np.max(comp_size))\n            if (verbose >= 1) and ( timer_for_print - time.time() >= time_interval_for_print ) :\n                time_interval_for_print = time.time()\n                print('c',c,'i',i,'dim', dim, 'n_sample', n_sample , np.round(time.time()-t00,1), 'seconds passed' )\n            #print(df_stat.tail(1))\n        if verbose >= 10:\n            print('Finsihed  for n_sample', n_sample, 'c',c,'i',i,'dim', dim, 'n_sample', n_sample , np.round(time.time()-t00,1), 'seconds passed' )\n    if verbose >= 1:\n        print('Finsihed  for dimension', dim,'c',c,'i',i,'dim', dim, 'n_sample', n_sample , np.round(time.time()-t00,1), 'seconds passed' )\n    \n\nprint(np.round(time.time()-t00,1), np.round( (time.time()-t00)/60,1),np.round( (time.time()-t00)/3600,1),'seconds, minutes, hours passed')\ndf_stat.groupby(['Dim', 'sample size']).aggregate(['mean','std']) \n#df_stat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main result\n\nThe formula for the number of connected components divided by number of nodes  for random NN graph\nproposed by  Neal Young: \nhttps://cstheory.stackexchange.com/questions/47034/number-of-connected-components-of-a-random-nearest-neighbor-graph\n(The answer depends on dimension.)\n\nEmpiric simulation for it (we show \"percent\" i.e. we multiplied by 100)  is given by \"%Coef\" in the table below.\n\nNeal Young formula predicts that \"%Coef\" tends to 0.25 for high enough dimension.\n\nOur simulations supports that claim for sampling in sphere or torus, but seems contradict that claim for uniformly or gaussianly distributed points. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_type)\n\ndf_stat.groupby(['Dim', 'sample size'])[['%Coeff','n_trials']].aggregate(['mean','std']) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# More detailed info\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_type)\ndf_aggregated_stat = df_stat.groupby(['Dim', 'sample size']).aggregate(['mean','std']) \ndf_aggregated_stat.to_csv('df_aggregated_stat.csv')\ndf_aggregated_stat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.groupby(['Dim', 'sample size']).mean()# aggregate(['mean','std']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.groupby(['Dim', 'sample size']).std()# aggregate(['mean','std']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.to_csv('df_stat.csv')\ndf_stat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}