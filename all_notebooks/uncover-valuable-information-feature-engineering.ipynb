{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Hey, everyone, I practiced some feature engineering, and I hope that my thoughts and explanations might be helpful for others.\nWhen beginning to study machine learning, I felt like the mathematical models I got tought were pretty mighty and robust, but this changed after making my first steps in real-world data analytics. I began to understand, that even the best and most complex models are performing poor on weak datasets. \n\nNot only do we have to encode the input data for our machine learning models, but also do we have to wrap our minds around the data and uncover further valuable information in the input data. this allows us to create a strong fundament of data for our machine learning models.\n\n**Note:** The following Feature Engineering methods have to be performed on both, train and test data. One **robust and error-resistent way of applying Feature Engineering on both data sets** is using sklearn Pipelines. Feel free to take a look at my [Pipelining Notebook on kaggle](https://www.kaggle.com/milankalkenings/no-pipelines-you-are-probably-doing-it-wrong).\n\n\nTake a look at my [Comprehensive Tutorial: Feature Engineering](https://www.kaggle.com/milankalkenings/comprehensive-tutorial-feature-engineering), if this first glance at the topic arouses your interest in feature engineering."},{"metadata":{},"cell_type":"markdown","source":"imports.."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\n\n! pip install -q country_converter\nimport plotly.express as px\nimport country_converter as co\n\nfrom nltk.stem import WordNetLemmatizer\n\ndf = pd.read_csv('../input/ramen-ratings/ramen-ratings.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n## How can we prepare this data for machine learning algorithms?\nAfter obtaining an overview, we should inspect each column in our dataframe and decide the following:\n* [How do we handle missing values in the column (if there are any) ?](#sec2)\n* [Can we simplify the column without losing too much information ?](#sec2)\n* [How do we handle outliers/errors in the column (if there are any) ?](#sec3)\n* [Can we uncover any further valuable information from this column and save it in other columns ?](#sec4)\n* [How do we encode this column for our machine learning algorithms?](#sec5)\n\nI will give you an example for each of these techniques.\n***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n# 1. Overview\nWe should always get some first impressions of our dataset before treating critical columns individually."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# the desired rate of null values per col\nnulls_per_col = df.isna().sum(axis=0) / len(df.index)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 5))\n    \nnulls_per_col.plot(kind='bar', color='steelblue', x=nulls_per_col.values, y=nulls_per_col.index, ax=ax, \n                       width=1, linewidth=1, align='edge', edgecolor='steelblue', label='Null value rate')\n    \n    \n# centered labels\nlabels=df.columns\nticks = np.arange(0.5, len(labels))\nax.xaxis.set(ticks=ticks, ticklabels=labels)\n\n# workaround to visualize very small amounts of null values per col\nna_ticks = ticks[(nulls_per_col > 0) & (nulls_per_col < 0.05)]\nif (len(na_ticks) > 0):\n    ax.plot(na_ticks, [0,]*len(na_ticks), 's', c='steelblue', markersize=10, \n            label='Very few missing values')\n    \n\nax.set_ylim((0,1))\nax.legend()\nfig.suptitle('Null Value Rate per Column', fontsize=30, y=1.05)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the 'Style' column contains very few missing values and the 'Top Ten' column consists mostly of missing values. Let's get some further insights."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the value '\\n' is the most common value in the column 'Top Ten'. We will have to deal with this later on. Moreover, since 'Chicken' occurs merely 7 times but it is still the most common variety, we can expect a large number of different values in this column."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n# 2. Handling missing values and Simplifying columns\nWe already identified, which columns contain missing values. Let's get some insights and handle the missing values accordingly."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,12))\nstyles_oc = df['Style'].value_counts()\nstyles_oc['others'] = styles_oc[-4:].sum()\nstyles_oc = styles_oc.drop(labels=styles_oc.index[-5:-1])\nstyles_oc.plot(kind='pie', ax=ax, colormap='cividis', rotatelabels = 270)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the most common value is 'Pack',  I guess it will not be too harmful, if we replace the missing values within this column with 'Pack', since this column doesn't contain a high number of missing values, and 'Pack' is by far **the most common value**. This method of replacing missing values is the equivalent to replacing missing numerical data with the column mean or column median."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Style'] = df['Style'].fillna(value='Pack')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's take a look at the column 'Top Ten' to decide how to treat the missing values in that column."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,12))\n# yea.. seems like some errors happened during data collection, that's why we have to drop the '\\n's \ndf['Top Ten'] = df['Top Ten'].replace(to_replace='\\n', value=None)\nstyles_oc = df['Top Ten'].value_counts()\nstyles_oc.plot(kind='pie', cmap='twilight', rotatelabels = 270, labeldistance= 1.01, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, as we can see, after dropping the invalid values, there are **solely the top 10 soups of the covered years** stored inside this column. It wouldn't make any sense to replace the missing values with e.g. '2012 #10'.\n\n**Usually, I would simply remove this column, because the ranking depends on the target and thus will not be available on test. Moreover, its Null Value Rate is icredibly high. I will perform some further steps on that column for educational reasons, and pretend that this feature might be available on test. In that case, this column would be a very strong indicator for high star ratings, and thus I wouldn't necessarily discard it, if further analysis would reveal a significant importance of this feature.**\n\n\nI think we should just replace the Null values with '0', and convert all values within the column to integers.\nWe will lose the year in which they achieved this rank, but according to the dataset description, the column 'Review #' already contains time data, and thus we don't lose much information. \n\nI apply this method, since we lose almost no information and we would have to store the year in another column, which would make the data more complex. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to ranks\ndf['Top Ten'] = df['Top Ten'].str.slice(start=6)\ndf['Top Ten'] = df['Top Ten'].fillna('0')\n\n# convert rank strings to integers\ndf['Top Ten'] = df['Top Ten'].astype(np.int8)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For further informations and explanations on **handling missing data**, I recommend you taking a look at this notebook:\n\n\nhttps://www.kaggle.com/milankalkenings/wine-reviews-data-cleaning"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the other columns of the dataset."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n# 3. Handling errors\nThe column 'country' contains information about the location of the Ramen shops. \n**I guess there might be some differences in taste around the wourld** \nand thus I think this column is pretty important in order to predict the star rating of a soup. Let us get some information about the distribution of reviews across the globe. Therefore, we have to convert the country names to their ISO3 representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"country_raw = df['Country']\n# this might take a while\ncountry_converted = country_raw.apply(lambda x: co.convert(x, to='ISO3'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out whether all strings have been converted automatically."},{"metadata":{"trusted":true},"cell_type":"code","source":"not_found = country_raw[country_converted=='not found'].unique()\nprint(f'{not_found} haven\\'t been converted automatically')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We found the following errors:\n* Dubai is a city in the United Arab Emirates (ARE)\n* Holland is a Region in the Netherlands and is often confused with the Netherlands (NLD)\n* Sarawak is a Malaysian state (MY)\n* UK is the abbreviation for United Kingdom (GBR)\n\nWe have to **fix this manually**, which shouldn't be a big deal, since we solely have to replace 4 values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_man(country):\n    '''\n    country: a string which should be a country name\n    \n    replaces 'country' by its ISO3 representation, if\n    country wasn't already converted automatically.\n    '''\n    if country=='UK':\n        return 'GBR'\n    if country=='Dubai':\n        return 'ARE'\n    if country=='Holland':\n        return 'NLD'\n    if country=='Sarawak':\n        return 'MY'\n    else: \n        return country\n        \n    \n    \nraw_fixed = country_raw.apply(convert_man)\nconverted_fixed = raw_fixed.apply(lambda x: co.convert(x, to='ISO3'))\ndf['Country'] = converted_fixed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rev:=reviewers\n# co:=countries\n# get reviewers per country\nrev_co = df.groupby('Country').count()['Review #']\nrev_co = [pd.Series(rev_co.values, name='reviewers'), pd.Series(rev_co.index, name='country')]\nrev_co_df = pd.concat(rev_co, axis=1)\nrev_co_df = rev_co_df.groupby(by='country', axis=0, as_index=False).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can finally plot the reviewers per country."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.choropleth(rev_co_df, locations=\"country\",\n                    color=\"reviewers\",\n                    color_continuous_scale='oranges',\n                    title=\"Reviewers per Country\",\n                   )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot is inspired by \n\nhttps://www.kaggle.com/heyytanay/beginner-s-eda-notebook\n\nMost reviews were made either in the North America or in East Asia. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n# 4. Generating further valuable information from one column\nI wonder which words are used to describe Ramen and if this information might have some relations with the reviewers country or the rating of the soup. Let's plot the most frequent words in the 'variety' column in a wordcloud."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Variety'] = df['Variety'].str.lower()\ndescriptions = ' '.join(df['Variety'])\n# add some custom stopwords:\ncustom_stop_words = ['noodle', 'soup', 'instant', 'flavor', 'flavour'] \nstop_words = list(STOPWORDS) + custom_stop_words\n\n#plot the wordcloud\nplt.figure(figsize=(15,10))\nwordcloud_ramen = WordCloud(stopwords=stop_words, \n                            max_font_size=80, max_words=160, \n                            width=600, height=400, \n                            colormap='inferno', background_color='white'\n                           ).generate(descriptions)\nplt.imshow(wordcloud_ramen, interpolation='bilinear')\nplt.axis('off')\nplt.savefig('wordcloud.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This wordcloud allows us to gain some feeling of the most used ingredients and styles of Ramen just by looking at it. \n\nBesides giving some insights (and making me hungry), this plot inspires me to create some binary features from the 'Variety' column, each of them should contain the information whether a variety contains one of the most frequent words occurring in the variety descriptions.\n\n\n\n### Let's use some Lexical Processing to obtain meaningful features\nObviously, it wouldn't make any sense to create a new column containing the information whether the variety value contains the words 'the', 'a' or symbols. Likewise, we should remove redundant ('noodles' and 'noodle')or obviously occurring values like 'flavour'. Hence, we should use a **stemmer/lemmatizer** and a customized list of stopwords. \n\n\nMoreover, it wouldn't be a good idea to create too many new columns in order to evade the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). "},{"metadata":{"trusted":true},"cell_type":"code","source":"wnl = WordNetLemmatizer()\ndescriptions_splitted = pd.Series(descriptions.split())\ndescriptions_lemmatized = descriptions_splitted.apply(wnl.lemmatize)\n\n# for simplicity, let's hardcode the initial number of new features as 20\ncommon = descriptions_lemmatized.value_counts()[:20]\n\n\n# we don't want to have any stopwords as features\ncustom_stops = ['flavour', 'flavor', 'cup', 'soup', '&']\nstopwords = custom_stops + list(STOPWORDS)\nstop_labels = common[np.in1d(common.index, stopwords)].index\ncommon = common.drop(labels=stop_labels)\ncommon.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These keywords seem to be helpful. Let's take these keywords as features."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_cols = common.index\ntemp = np.empty([len(df.index), len(new_cols)], dtype=np.int8)\n\ndef fill(row):\n    '''\n    row: a row of the dataframe\n    \n    stores whether the 'Variety' of the row contains the (lemmatized) keywords.\n    '''\n    # we have to lemmatize each row, since we lemmatized the names of the new columns\n    row_lemmatized = pd.Series(row['Variety'].split()).apply(wnl.lemmatize)\n    temp[row.name] = np.in1d(new_cols, row_lemmatized)\n\n    \n\n# rowise:\ndf.apply(fill, axis=1)\n\nnew_cols = pd.DataFrame(temp, columns=common.index)\ndf_augmented = pd.concat([df, new_cols], axis=1)\ndf_augmented.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We generated a number of features from the most common keywords in our 'Variety' column. This technique is often calles **Feature Splitting**. \nNote that I defined myself, how many of those 'most common' keywords I used in this section. We should change this number of keywords and treat it like a **hyperparameter** in real-world applications. \n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec5\"></a>\n# 5. Column encoding\nWe have to convert string data to numerical data. Therefore, we can apply differing methods, these are the most common ones: \n\n* We can either Create **indicator variables**, as we did for the new columns we generated from 'Variety'. This method extremely increase the runtime of the algorithm. One famous way of creating indicator variables is **One-Hot Encoding**.\n\n* We can **factorize** the column and replace each distinct value with an unique integer. The resulting label encoding will indicate a relationship or order between the different values, since they are solely encoded as integers within the same column.\n\n\nI will apply the second method on our dataset in order to keep the data less complex, since the versatility of these columns is pretty high.\n\n\n[>Some more encoding approaches<](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we don't have to factorize all columns\ndf_cat = df_augmented[['Brand', 'Variety', 'Style', 'Country']]\ndf_done = df_augmented.drop(columns=df_cat.columns)\n\n\n# factorize the columns of one df and reunite both dataframes\ndf_factorized = df_cat.copy().apply(lambda x: pd.factorize(x)[0])\ndf_cat.columns = df_cat.columns + '_cat'\ndf_augmented = pd.concat([df_factorized, df_done], axis=1)\n# for better interpretability, we should save the encoding.\n# This allows us to decode the columns after applying our \n# machine learning models.\ndf_encoding = pd.concat([df_cat, df_factorized], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one last thing that might not seem to be important, but it is quite important:\n\nWe should change the names of our columns in order to obtain **uniformity**. The column names within the dataset were capitalised, and our new features aren't.\nLet's change this."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_augmented.columns = df_augmented.columns.str.lower()\n\ndf_augmented","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depending on the model you will apply on the data, some further steps like **Standardization** might be necessary.\n\nThat's it. We handled missing values and errors and obtained some new valuable features from a more or less invaluable one.\nFeel free to comment any recommendations or improvements and help me and other preople to learn more about this topic."},{"metadata":{},"cell_type":"markdown","source":"# Thank you for reading this notebook. Please upvote the notebook if it helped you out in any way. =) "},{"metadata":{},"cell_type":"markdown","source":"Take a look at my [Comprehensive Tutorial: Feature Engineering](https://www.kaggle.com/milankalkenings/comprehensive-tutorial-feature-engineering), if this first glance at the topic arouses your interest in feature engineering."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}