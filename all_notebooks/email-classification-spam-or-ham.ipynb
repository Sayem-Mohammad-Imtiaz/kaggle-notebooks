{"cells":[{"metadata":{"_cell_guid":"4feddf05-b4b4-4a67-b2a5-4948e29a0eac","_uuid":"6777c1f06354bbac7e59e8192b96410493df9a6b"},"cell_type":"markdown","source":"#### SMS Spam Collection Dataset (垃圾邮件分类)\nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset"},{"metadata":{"_cell_guid":"a89760f4-9075-42be-b4c4-c8c87d175c88","_uuid":"4429a082ddabcb23261daec29ee1ae9e68ba2a2e","trusted":true},"cell_type":"code","source":"'''\n    Import training data\n'''\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndata_dir = \"../input/\"\n\ndf = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\nprint ('Training data: ')\nprint (df.head())\n\n# split into train and test\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    df.v2,\n    df.v1, \n    test_size=0.2, \n    random_state=0)  \n\nprint ('Show email contents:')\nprint (data_train[:10]) \nprint ('Labeled the email as Spam or Ham:')\nprint (labels_train[:10])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52f710bc-5b09-4f6e-ab6e-1ee4a759061d","_uuid":"12376529a171b515592b3d9258e2466d68a643b1"},"cell_type":"markdown","source":"统计总共单词个数"},{"metadata":{"_cell_guid":"66f6f422-a93e-4956-92ad-879faf6ff52a","_uuid":"2cc13c73a46a417b3b3f84e38c71b17ca1a2448b","trusted":true},"cell_type":"code","source":"'''\n    Keep words in a dictionary with its unique ID\n'''\ndef GetVocabulary(data): \n    vocab_dict = {}\n    wid = 0\n    for document in data:\n        words = document.split()\n        for word in words:\n            word = word.lower()\n            if word not in vocab_dict:\n                vocab_dict[word] = wid\n                wid += 1\n    return vocab_dict\n\n# 用训练集建立词汇表\nvocab_dict = GetVocabulary(data_train)\nprint ('Number of all the unique words : ' + str(len(vocab_dict)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9e6fcac-a9a5-49cf-91dc-f29ba081fea6","_uuid":"2643c28ffc3bcbc3e4910a725be3db6e300455f7","trusted":false},"cell_type":"code","source":"'''\n    Convert the Document -> Vector\n'''\ndef Document2Vector(vocab_list, data):\n    word_vector = np.zeros(len(vocab_list))\n    words = data.split()\n    for word in words:\n        if word in vocab_list:\n            word_vector[vocab_list[word]] += 1\n    return word_vector\n\nprint (data_train[1:2,])\nans = Document2Vector(vocab_dict,\"we are good good\")\nprint(ans)\n#print (data_train.values[2])\nprint(ans[vocab_dict['we']], ans[vocab_dict['are']], ans[vocab_dict['good']])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a1119d3-6545-4f1b-b6df-a2f62d9d3dda","_uuid":"f8e259bb2ef260294478c4ec7929cdd10b68d84a","trusted":false},"cell_type":"code","source":"train_matrix = []\nfor document in data_train.values:\n    word_vector = Document2Vector(vocab_dict, document)\n    train_matrix.append(word_vector)\n\nprint (len(train_matrix))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e8f487ef-5aa9-43a5-b545-3794676fa376","_uuid":"b453cc74ac8f281bb7785b98a4dd3608b8c0046c"},"cell_type":"markdown","source":"做naive bayes 训练，得到训练集每个词概率"},{"metadata":{"_cell_guid":"8f13984b-7bf2-49f9-abe9-5ae639e87c67","_uuid":"11fd5ac25da3bd3086c8d0f6eafba53544a0e994","trusted":false},"cell_type":"code","source":"'''\n    Possibilities in Trainset：\n        1. Word's possibility in one catagory e.g. P('email'|Spam)\n        2. Spam or Ham's Possibilities e.g. P(Spam)\n        \n    计算实现巧妙利用了numpy的array结构：\n        1. 在每个分类下创建一个与词汇量大小相等的vector(即 numpy array), 即spam_word_counter 和 ham_word_counter\n        2. 在遍历每一个句子的时候，直接与句子对应的vector相加，累积每个单词出现的次数\n        3. 在遍历完所有句子之后，再除以总词汇量，得到每个单词的概率\n'''\ndef NaiveBayes_train(train_matrix, labels_train):\n    num_docs = len(train_matrix)\n    num_words = len(train_matrix[0])\n    \n    spam_vector_count = np.ones(num_words)\n    ham_vector_count = np.ones(num_words)  #计算频数初始化为1\n    \n    spam_total_count = 0\n    ham_total_count = 0\n    \n    spam_count = 0\n    ham_count = 0\n    for i in range(num_docs):\n        if i % 500 == 0:\n            print ('Train on the doc id:' + str(i))\n            \n        if labels_train[i] == 'spam':\n            ham_vector_count += train_matrix[i]\n            ham_total_count += sum(train_matrix[i])\n            ham_count += 1\n        else:\n            spam_vector_count += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n    \n    print (ham_count)\n    print (spam_count)\n    \n    p_spam_vector = np.log(ham_vector_count/ham_total_count + num_words) #注意在分母加上拉普拉斯平滑\n    p_ham_vector = np.log(spam_vector_count/spam_total_count + num_words)#注意在分母加上拉普拉斯平滑\n    \n    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n    #返回各类对应特征的条件概率向量\n    #和各类的先验概率\n    \np_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"665af74a-895f-4dde-beb8-40db552c8739","_uuid":"822c9860c61240ce69289ba71fb94ed29f037a51","trusted":false},"cell_type":"code","source":"data_test.values.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4415b37-0855-49bd-8926-afe63c668268","_uuid":"1d737630d3f2df2b3d28355f57cabbf594c2bf23","trusted":false},"cell_type":"code","source":"'''\n    Test words -> vectors，增加smoothing的部分\n'''\ndef Test2Vector(vocab_dict, data):\n    word_vector = np.zeros(len(vocab_dict.keys()))\n    words = data.split()\n    # 统计out-of-voc的词汇量\n    out_of_voc = 0\n    for word in words:\n        word = word.lower()\n        if word in vocab_dict:\n            word_vector[vocab_dict[word]] += 1\n        else:\n            out_of_voc += \n    return word_vector\n\n'''\n     Predict on testset and check which possibility is greater.\n'''    \ndef Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham):\n    # Note: If it's a new word，test_word_vector对应的维度为0\n    # Thus: test_word_vector * p_spam_vector 不为0的维度正好是句子中每个词的概率\n    spam = sum(test_word_vector * p_spam_vector) + p_spam\n    ham = sum(test_word_vector * p_ham_vector) + p_ham\n    if spam > ham:\n        return 'spam'\n    else:\n        return 'ham'\n\npredictions = []\ni = 0\nfor document in data_test.values:\n    if i % 200 == 0:\n        print ('Test on the doc id:' + str(i))\n    i += 1    \n    test_word_vector = Document2Vector(vocab_list, document)\n    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n    predictions.append(ans)\n\nprint (len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fff37f61-bacb-4709-8d0a-7a131dc57b36","_uuid":"fd7f42c4490b9b6fa961f765af46053dccdb2627","trusted":false},"cell_type":"code","source":"'''Accuracy, report in Testset '''\n\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\nprint (accuracy_score(labels_test, predictions))\nprint (classification_report(labels_test, predictions))\nprint (confusion_matrix(labels_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"58f76c67baf2197571c3cb68878f0f29c3312efb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}