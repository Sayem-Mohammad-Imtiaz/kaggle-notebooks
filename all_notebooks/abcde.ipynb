{"cells":[{"execution_count":null,"cell_type":"markdown","outputs":[],"source":"","metadata":{"_uuid":"34f5db2c60c9f57065c062e395ca2ecea2463356","_cell_guid":"968d485f-2f52-d04c-c3f0-3cab04d64357"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"df8a0824f1cafdd7708d6283dab4c386b24cec42","_cell_guid":"f1bde854-22aa-08e7-a53c-1b67e6c13d25"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"layout_data = pd.read_csv('../input/public_layout.csv', sep=',')\ndata = pd.read_csv('../input/recs2009_public.csv', sep=',')","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"55cbaedc586b4a58d16bcb8326dfd042ddcf4584","_cell_guid":"33bd1def-8269-f752-d4a4-cd17127f2af2"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import matplotlib.pyplot as plt\nnewdata=pd.DataFrame()\nnewdata['DOEID']=data['DOEID']\nnewdata['REGIONC']=data['REGIONC']\nnewdata['Climate_Region_Pub']=data['Climate_Region_Pub']\nnewdata['AIA_Zone']=data['AIA_Zone']\nselect=['METROMICRO','UR','YEARMADE','NUMFLRS','NUMAPTS','STUDIO','NAPTFLRS','BEDROOMS','TOTROOMS',\n       'CELLAR','BASEFIN','BASEHEAT','BASECOOL','ATTIC','PRKGPLC1','SIZEOFGARAGE','STOVEN','STOVE',\n       'OVENUSE','MICRO','OUTGRILL','TOPGRILL','NUMFRIG','KWH','SEPFREEZ','DISHWASH','CWASHER','DRYER'\n       ,'TVCOLOR','COMPUTER','INTERNET','HEATHOME','MOISTURE','HEATROOM','NUMTHERM','TEMPHOME','NUMH2ONOTNK',\n       'AIRCOND','ACROOMS','TEMPHOMEAC','NUMBERAC','HIGHCEIL','SWIMPOOL','AUDIT','USENG','USESOLAR','USEKERO',\n       'ONSITE','ONSITEGRID','KERODEL','WOODLOGS','HHSEX','EMPLOYHH','EDUCATION','NHSLDMEM','HHAGE','ATHOME',\n       'WORKPAY','MONEYPY','FOODASST','TOTSQFT','TYPEHUQ4','HDD30YR','CDD30YR','KOWNRENT','DIVISION',\n       'TYPEHUQ','REPORTABLE_DOMAIN','USELP','USEWOOD','USEFO','Householder_Race',\n       'SPOUSE','HBUSNESS','POVERTY100','PERIODEL','SCALEEL','DOLLAREL']\nnewdata[select]=data[select]","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"7616ba6c1c663341d63c413e91ca15bff3468e94","_cell_guid":"b143b896-a30f-1d9c-da72-1ec09ba6301f"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import scale\nfrom scipy import stats\nfrom sklearn import grid_search\nfrom sklearn.preprocessing import scale\nnp.random.seed(1)\nX=newdata.drop(['KWH'],axis=1)\nY=newdata['KWH']\nCost=newdata['DOLLAREL']\n\n\nX['ACROOMS_TEMPHOMEAC']=X['ACROOMS']*X['TEMPHOMEAC']\nX['IS_ACROOMS']=(X['ACROOMS']>0)+1-1\nX['ACROOMS_CDD30YR']=X['ACROOMS']*X['CDD30YR']\n\n#X=X.drop(['ACROOMS'],axis=1)\n#X=X.drop(['CDD30YR'],axis=1)\n#X=X.drop(['USECENAC'],axis=1)\n#X=X.drop(['TEMPHOMEAC'],axis=1)\n#X=X.drop(['HDD30YR'],axis=1)\nX['TOTSQFT']=scale(X['TOTSQFT'])\nX['NHSLDMEM']=scale(X['NHSLDMEM'])\n#out=np.array([9951,9648,8060,1309,5292,2153,11539,1168])\n#X=X.drop(out,axis=0)\n#Y=Y.drop(out,axis=0)\n#X=X.drop(out,axis=0)\n#Y=Y.drop(out,axis=0)\nc=10000\nn=2\n\nX=X.drop(Y[Y>30000].index,axis=0)\nY=Y.drop(Y[Y>30000].index,axis=0)\nCost=Cost.drop(Y[Y>30000].index,axis=0)\nCost=Cost.astype(float)\n#Check -2\nunit=Cost/Y\nunit[unit<0.01]=0.01\nunit[unit>0.7]=0.7\n#X['COSTunit']=np.log(unit)\n\n#X=X.drop(['NUMAPTS'],axis=1)\n#X=X.drop(['USEKERO','HEATHOME','KERODEL','ONSITEGRID','ONSITE','MICRO','USESOLAR','NUMFLRS','TOPGRILL','SCALEEL'],axis=1)\n#X=X.drop(['MOISTURE','PERIODEL','POVERTY100','FOODASST','NUMH2ONOTNK','USEFO','COMPUTER','CELLAR','USEWOOD','HHSEX'],axis=1)\n#X=X.drop(['ATHOME','WOODLOGS','AUDIT','BASECOOL','BASEFIN','WORKPAY','INTERNET','DISHWASH','HBUSNESS','STOVE'],axis=1)\n#X=X.drop(['HIGHCEIL','USELP','ATTIC','BASEHEAT','KOWNRENT','AIRCOND','NUMAPTS','EMPLOYHH','SIZEOFGARAGE','Householder_Race','SPOUSE'],axis=1)\n#X=X.drop(['NUMBERAC','STOVEN','CWASHER','UR','METROMICRO','STUDIO','SEPFREEZ','Climate_Region_Pub'],axis=1)\n#X=X.drop(['NUMFRIG','NAPTFLRS','DRYER','OVENUSE','AIA_Zone','TYPEHUQ','PRKGPLC1','TEMPHOME','NUMTHERM'],axis=1)\n#X=X.drop(['REGIONC','HHAGE','EDUCATION','MONEYPY','YEARMADE','SWIMPOOL','HDD30YR','TVCOLOR','HEATROOM','DIVISION'],axis=1)\n\n\nlogY=np.log(Y)\nboxY,ld=stats.boxcox(Y)\n\n\n#One-hot encoding for catagoric features\n'''\nCat_features=['REGIONC','DIVISION','Householder_Race','Climate_Region_Pub'\n,'AIA_Zone','METROMICRO','SPOUSE','USEFO','USEWOOD','USELP','REPORTABLE_DOMAIN'\n,'KOWNRENT','FOODASST','WORKPAY','ATHOME','EDUCATION','EMPLOYHH',\n'HHSEX','WOODLOGS','KERODEL','ONSITEGRID','ONSITE','USEKERO','USESOLAR','USENG','AUDIT',\n'SWIMPOOL','HIGHCEIL','AIRCOND','MOISTURE','HEATHOME','INTERNET','COMPUTER','DRYER','CWASHER','DISHWASH'\n,'SEPFREEZ','STUDIO','CELLAR','BASEFIN','BASEHEAT','BASECOOL','ATTIC','PRKGPLC1','SIZEOFGARAGE','UR']\n'''\n'''\nCat_features=['REGIONC','DIVISION','Householder_Race','Climate_Region_Pub'\n,'AIA_Zone','METROMICRO','SPOUSE','USEFO','USEWOOD','USELP','REPORTABLE_DOMAIN'\n,'KOWNRENT','FOODASST','WORKPAY','ATHOME','EDUCATION','EMPLOYHH',\n'HHSEX','WOODLOGS','KERODEL','USEKERO','USENG',\n'SWIMPOOL','HIGHCEIL','AIRCOND','MOISTURE','INTERNET','COMPUTER','DRYER','CWASHER','DISHWASH'\n,'PRKGPLC1','SIZEOFGARAGE','UR','SEPFREEZ','STUDIO','CELLAR','BASEFIN','BASEHEAT','BASECOOL','NUMFLRS']\n'''\n#Cat_features=['METROMICRO','UR','REPORTABLE_DOMAIN']\nCat_features=['REPORTABLE_DOMAIN']\n\nX[Cat_features]=X[Cat_features].astype(np.str)\nX=pd.get_dummies(X)\nfor i in X.columns:\n    X[i]=scale(X[i])\n\n#X=scale(X)\n\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n\ndef get_level(pred):\n    pred=pred.astype(int)\n    pred=np.floor(pred/c)\n    pred[pred>n]=n\n    pred[pred<0]=0\n    return pred\n\ndef plot_confusion(cm):\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure()\n    plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n\ndef correct_rate(cm):\n    sum=0\n    for i in range(0,n+1):\n        sum=sum+float(cm[i,i])\n    c4 = sum/np.sum(cm)\n    return 1-c4\n    \ndef invboxcox(y,ld):\n   if ld == 0:\n      return(np.exp(y))\n   else:\n      return(np.exp(np.log(ld*y+1)/ld))\n      \n\n    \n\n#Data split\n'''\nX_train=X.iloc[0:11000]\nX_test=X.iloc[11000:]\nY_train=Y.iloc[0:11000]\nY_test=Y.iloc[11000:]\nlevel_train=level.iloc[0:11000]\nlevel_test=level.iloc[11000:]\n'''\n\n#good seed: 2 \nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.1, random_state=1)\n\nX_train, X_val, Y_train, Y_val = train_test_split(\nX_train, Y_train, test_size=0.3, random_state=2)\n\nX_train, X_test, logY_train, logY_test = train_test_split(\nX, logY, test_size=0.1, random_state=1)\n\nX_train, X_val, logY_train, logY_val = train_test_split(\nX_train, logY_train, test_size=0.3, random_state=2)\n\nX_train, X_test, boxY_train, boxY_test = train_test_split(\nX, boxY, test_size=0.1, random_state=1)\n\nX_train, X_val, boxY_train, boxY_val = train_test_split(\nX_train, boxY_train, test_size=0.3, random_state=2)\n\n'''\nlogY_train=np.log(Y_train)[0]\nlogY_val=np.log(Y_val)[0]\nlogY_test=np.log(Y_test)[0]\n'''\nscaleY_train=np.asarray(Y_train/30000)\nscaleY_val=np.asarray(Y_val/30000)\nscaleY_test=np.asarray(Y_test/30000)\n\nlevel_train=np.floor(Y_train/c)\nlevel_train[level_train>n]=n\nlevel_val=np.floor(Y_val/c)\nlevel_val[level_val>n]=n\nlevel_test=np.floor(Y_test/c)\nlevel_test[level_test>n]=n\n\n\n\n\nextend_X=X_train[level_train==2]\nextend_Y=Y_train[level_train==2]\next_X_train=pd.concat([X_train,extend_X])\next_Y_train=pd.concat([Y_train,extend_Y])\n\ndimdata = X.shape[1]","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"55920047acbf805740f283dda1733d24120634f1","_cell_guid":"50a69669-0b41-9aab-aa03-7c8cf63267e8"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.metrics import confusion_matrix\nallpred_val=pd.DataFrame()\nallpred_test=pd.DataFrame()\n","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"fad972653f5ddc14288418e4d128fd62c23457f8","_cell_guid":"43c92cf5-7673-df54-8fe7-7b4d0b0edda4"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"257d4a786ccbdbc70be2cf667490dc21708da532","_cell_guid":"435e38d9-ee57-ead3-e176-b911dcefc1f9"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#模型对比---------------------------------\nfrom sklearn.metrics import confusion_matrix\nallpred_val=pd.DataFrame()\nallpred_test=pd.DataFrame()\n\n'''\nfrom sklearn.ensemble.forest import RandomForestRegressor\n\nrf =  RandomForestRegressor(n_estimators=100)\nrf.fit(X_train,Y_train)\nval_rf=rf.predict(X_val)\npred_rf=rf.predict(X_test)\npred_rf_level=get_level(pred_rf)\ncm_rf = confusion_matrix(level_test, pred_rf_level)\n\nprint(\"cm_rf\",correct_rate(cm_rf))\nprint(\"MAPE\",np.sqrt(np.mean((pred_rf-Y_test)*(pred_rf-Y_test))))\n\n\nallpred_val['rf']=val_rf\nallpred_test['rf']=pred_rf\n\nrf2 = RandomForestRegressor(n_estimators=100)\nrf2.fit(X_train,logY_train)\nval_rf2=np.exp(rf2.predict(X_val))\npred_rf2=np.exp(rf2.predict(X_test))\npred_rf2_level=get_level(pred_rf2)\ncm_rf2 = confusion_matrix(level_test, pred_rf2_level)\n\nprint(\"cm_rf2\",correct_rate(cm_rf2))\nprint(\"MAPE\",np.sqrt(np.mean((pred_rf2-Y_test)*(pred_rf2-Y_test))))\n\nallpred_val['rf2']=val_rf2\nallpred_test['rf2']=pred_rf2\n\nrf3 = RandomForestRegressor(n_estimators=100)\nrf3.fit(ext_X_train,ext_Y_train)\nval_rf3=rf3.predict(X_val)\npred_rf3=(rf3.predict(X_test))\npred_rf3_level=get_level(pred_rf3)\ncm_rf3 = confusion_matrix(level_test, pred_rf3_level)\n\nprint(\"cm_rf3\",correct_rate(cm_rf3))\nprint(\"MAPE\",np.sqrt(np.mean((pred_rf3-Y_test)*(pred_rf3-Y_test))))\nallpred_val['rf3']=val_rf3\nallpred_test['rf3']=pred_rf3\n\n#Base model 2(GBDT)-----------------------------------------------------\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor(n_estimators=400,learning_rate=0.1)\ngb.fit(X_train,Y_train)\nval_gb=gb.predict(X_val)\npred_gb=gb.predict(X_test)\npred_gb_level=get_level(pred_gb)\ncm_gb = confusion_matrix(level_test, pred_gb_level)\n\nallpred_val['gb']=val_gb\nallpred_test['gb']=pred_gb\n\nprint(\"cm_gb\",correct_rate(cm_gb))\nprint(\"MAPE\",np.sqrt(np.mean((pred_gb-Y_test)*(pred_gb-Y_test))))\n\n\n#log-gbdt model\n\ngb2 = GradientBoostingRegressor(n_estimators=400,learning_rate=0.1)\ngb2.fit(X_train,logY_train)\nval_gb2=np.exp(gb2.predict(X_val))\npred_gb2=np.exp(gb2.predict(X_test))\npred_gb2_level=get_level(pred_gb2)\ncm_gb2 = confusion_matrix(level_test, pred_gb2_level)\nallpred_val['gb2']=val_gb2\nallpred_test['gb2']=pred_gb2\n\nprint(\"cm_gb2\",correct_rate(cm_gb2))\nprint(\"MAPE\",np.sqrt(np.mean((pred_gb2-Y_test)*(pred_gb2-Y_test))))\n#extend model\n\ngb3 = GradientBoostingRegressor(n_estimators=400,learning_rate=0.1)\ngb3.fit(ext_X_train,ext_Y_train)\nval_gb3=gb3.predict(X_val)\npred_gb3=(gb3.predict(X_test))\npred_gb3_level=get_level(pred_gb3)\ncm_gb3 = confusion_matrix(level_test, pred_gb3_level)\n\nallpred_val['gb3']=val_gb3\nallpred_test['gb3']=pred_gb3\n\n\nprint(\"cm_gb3\",correct_rate(cm_gb3))\nprint(\"MAPE\",np.sqrt(np.mean((pred_gb3-Y_test)*(pred_gb3-Y_test))))\n\n#Base model 3(LASSO)\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=10)\nlasso.fit(X_train,Y_train)\nval_lasso=lasso.predict(X_val)\npred_lasso= lasso.predict(X_test)\npred_lasso_level=get_level(pred_lasso)\ncm_lasso = confusion_matrix(level_test, pred_lasso_level)\n\nallpred_val['lasso']=val_lasso\nallpred_test['lasso']=pred_lasso\n\n#Base model (log-lasso)\nlasso2 = Lasso(alpha=10)\nlasso2.fit(X_train,logY_train)\nval_lasso2=np.exp(lasso2.predict(X_val))\npred_lasso2= np.exp(lasso2.predict(X_test))\npred_lasso2_level=get_level(pred_lasso2)\ncm_lasso2 = confusion_matrix(level_test, pred_lasso2_level)\n\nallpred_val['lasso2']=val_lasso2\nallpred_test['lasso2']=pred_lasso2\n\nprint(\"cm_lasso\",correct_rate(cm_lasso))\nprint(\"MAPE\",np.sqrt(np.mean((pred_lasso-Y_test)*(pred_lasso-Y_test))))\nprint(\"cm_lasso2\",correct_rate(cm_lasso2))\nprint(\"MAPE\",np.sqrt(np.mean((pred_lasso2-Y_test)*(pred_lasso2-Y_test))))\n\n#EX-tree-------------------------\nfrom sklearn.ensemble import ExtraTreesRegressor\nex=ExtraTreesRegressor(n_estimators=200, max_features=32)\nex.fit(X_train,Y_train)\nval_ex=ex.predict(X_val)\npred_ex=ex.predict(X_test)\npred_ex_level=get_level(pred_ex)\ncm_ex = confusion_matrix(level_test, pred_ex_level)\n\nallpred_val['ex']=val_ex\nallpred_test['ex']=pred_ex\n\nprint(\"cm_ex\",correct_rate(cm_gb))\nprint(\"MAPE\",np.sqrt(np.mean((pred_ex-Y_test)*(pred_ex-Y_test))))\n\n#EX2-tree-------------------------\nex2=ExtraTreesRegressor(n_estimators=200, max_features=32)\nex2.fit(X_train,logY_train)\nval_ex2=np.exp(ex2.predict(X_val))\npred_ex2=np.exp(ex2.predict(X_test))\npred_ex2_level=get_level(pred_ex2)\ncm_ex2 = confusion_matrix(level_test, pred_ex2_level)\nallpred_val['ex2']=val_ex2\nallpred_test['ex2']=pred_ex2\n\nprint(\"cm_ex2\",correct_rate(cm_ex2))\nprint(\"MAPE\",np.sqrt(np.mean((pred_ex2-Y_test)*(pred_ex2-Y_test))))\n#Base model (KNN-50)\n\nfrom sklearn.neighbors import KNeighborsRegressor\nneigh = KNeighborsRegressor(n_neighbors=50)\npred_KNN=neigh.fit(X_train, Y_train).predict(X_test)\nval_KNN=neigh.predict(X_val)\npred_KNN_level=get_level(pred_KNN)\ncm_KNN = confusion_matrix(level_test, pred_KNN_level)\n\nallpred_val['KNN']=val_KNN\nallpred_test['KNN']=pred_KNN\n\nprint(\"KNN\",correct_rate(cm_KNN))\nprint(\"MAPE\",np.sqrt(np.mean((pred_KNN-Y_test)*(pred_KNN-Y_test))))\n'''","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"710f7a3efa1782bccdba756652386dab7c0f8abe","_cell_guid":"1755d412-9d5c-b1e9-a604-1a28d788c2de"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#建立ENSEMBLE 模型------------------------------\nallpred_val=pd.DataFrame()\nallpred_test=pd.DataFrame()\n#------------------------------\nfrom sklearn.metrics import confusion_matrix\nimport xgboost as xgb\ndtrain = xgb.DMatrix(X_train.values,label = Y_train.values)\ndval = xgb.DMatrix(X_val.values,label = Y_val.values)\ndtest = xgb.DMatrix(X_test.values,label = Y_test.values)\nparams = {}\nparams['booster']  = 'gbtree'\nparams['objective'] = 'reg:linear'\nparams['max_depth'] = 5\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.8\nparams['silent'] = 1\nparams['eval_metric'] = 'rmse'\nnum_round = 60\neval_list  = [(dtrain,'train')]\n\nprint('training xgb model...')\nbst = xgb.train(params, dtrain, num_round, eval_list)\npred_xg=(bst.predict(dtest))\nval_xg=(bst.predict(dval))\npred_xg_level=get_level(pred_xg)\ncm_xg = confusion_matrix(level_test, pred_xg_level)\n\nallpred_val['xg']=val_xg\nallpred_test['xg']=pred_xg\n\nprint(\"MAPE\",np.sqrt(np.mean((pred_xg-Y_test)*(pred_xg-Y_test))))\nprint(\"cm_xg\",correct_rate(cm_xg))\nplot_confusion(cm_xg)\n#------------------------------\nparams2 = {}\nparams2['booster']  = 'gbtree'\nparams2['objective'] = 'reg:linear'\nparams2['max_depth'] = 4\nparams2['subsample'] = 0.8\nparams2['colsample_bytree'] = 0.8\nparams2['silent'] = 1\nparams2['eval_metric'] = 'rmse'\nnum_round2 = 50\neval_list  = [(dtrain,'train')]\nbst2 = xgb.train(params2, dtrain, num_round2, eval_list)\npred_xg2=(bst2.predict(dtest))\nval_xg2=(bst2.predict(dval))\npred_xg2_level=get_level(pred_xg2)\ncm_xg2 = confusion_matrix(level_test, pred_xg2_level)\nprint(\"MAPE_2\",np.sqrt(np.mean((pred_xg2-Y_test)*(pred_xg2-Y_test))))\nprint(\"cm_xg2\",correct_rate(cm_xg2))\n\nallpred_val['xg2']=val_xg2\nallpred_test['xg2']=pred_xg2\n\n#------------------------------\ndtrain2 = xgb.DMatrix(X_train.values,label = logY_train.values)\ndval2 = xgb.DMatrix(X_val.values,label = logY_val.values)\ndtest2 = xgb.DMatrix(X_test.values,label = logY_test.values)\nparams3 = {}\nparams3['booster']  = 'gbtree'\nparams3['objective'] = 'reg:linear'\nparams3['max_depth'] = 4\nparams3['subsample'] = 0.8\nparams3['colsample_bytree'] = 0.8\nparams3['silent'] = 1\nparams3['eval_metric'] = 'rmse'\nnum_round3 = 50\neval_list2  = [(dtrain2,'train')]\nbst3 = xgb.train(params3, dtrain2, num_round3, eval_list2)\npred_xg3=np.exp(bst3.predict(dtest2))\nval_xg3=np.exp(bst3.predict(dval2))\npred_xg3_level=get_level(pred_xg3)\ncm_xg3 = confusion_matrix(level_test, pred_xg3_level)\nprint(\"MAPE_3\",np.sqrt(np.mean((pred_xg3-Y_test)*(pred_xg3-Y_test))))\nprint(\"cm_xg3\",correct_rate(cm_xg3))\n\nallpred_val['xg3']=val_xg3\nallpred_test['xg3']=pred_xg3\n\n#------------------------------\nparams4 = {}\nparams4['booster']  = 'gbtree'\nparams4['objective'] = 'reg:linear'\nparams4['max_depth'] = 5\nparams4['subsample'] = 0.8\nparams4['colsample_bytree'] = 0.8\nparams4['silent'] = 1\nparams4['eval_metric'] = 'rmse'\nnum_round4 = 60\neval_list2  = [(dtrain2,'train')]\nbst4 = xgb.train(params4, dtrain2, num_round4, eval_list2)\npred_xg4=np.exp(bst4.predict(dtest2))\nval_xg4=np.exp(bst4.predict(dval2))\npred_xg4_level=get_level(pred_xg4)\ncm_xg4 = confusion_matrix(level_test, pred_xg4_level)\nprint(\"MAPE_4\",np.sqrt(np.mean((pred_xg4-Y_test)*(pred_xg4-Y_test))))\nprint(\"cm_xg4\",correct_rate(cm_xg4))\n\nallpred_val['xg4']=val_xg4\nallpred_test['xg4']=pred_xg4\n","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"df3b34c4eda5dfbfea2da0eaaae63a617db068f4","_cell_guid":"f133e4cf-eef8-831b-7830-e4d89303accf"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import lightgbm as lgbm\n\nt4_params = {\n    'boosting_type': 'gbdt','silent': False,\n    'learning_rate': 0.01, 'max_depth': 4,\n    'max_bin': 255, 'metric':'rmse',\n    'subsample': 0.9, 'subsample_freq': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.05, 'reg_lambda': 0.05,\n    'min_split_gain': 0.6, 'min_child_weight': 1, 'min_child_samples': 5, 'scale_pos_weight': 1}\n\nlg = lgbm.sklearn.LGBMRegressor(n_estimators=500, seed=0)\nlg.fit(X_train,Y_train)\nval_lg=lg.predict(X_val)\npred_lg=lg.predict(X_test)\npred_lg_level=get_level(pred_lg)\ncm_lg = confusion_matrix(level_test, pred_lg_level)\n\nallpred_val['lg']=val_lg\nallpred_test['lg']=pred_lg\n\nprint(\"cm_lg\",correct_rate(cm_lg))\nprint(\"MAPE\",np.sqrt(np.mean((pred_lg-Y_test)*(pred_lg-Y_test))))\n","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"3bc31f4aa255fc843a9551a91208de67e1973d22","_cell_guid":"fd8e74fd-8f31-cdbb-0466-a68df5c6eff4"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn import linear_model\nblending = linear_model.Ridge(alpha=1)\nblending.fit(allpred_val,Y_val)\nfinal=blending.predict(allpred_test)\npred_level=get_level(final)\ncm = confusion_matrix(level_test, pred_level)\ncorrect_rate(cm)\nprint(\"cm_final\",correct_rate(cm))\nprint(\"MAPE\",np.sqrt(np.mean((final-Y_test)*(final-Y_test))))","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"2255fd2d74f3b1ec0b9b461ba575f604133bf3a3","_cell_guid":"16dd9b98-b735-5d23-3785-f898898d3873"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#开始做图\nallpred_val=pd.DataFrame()\nallpred_test=pd.DataFrame()\nimport xgboost as xgb\ndtrain = xgb.DMatrix(X_train.values,label = Y_train.values)\ndval = xgb.DMatrix(X_val.values,label = Y_val.values)\ndtest = xgb.DMatrix(X_test.values,label = Y_test.values)\nrmsewatch_4=[]\nfor i in range(1,5):\n    params = {}\n    params['booster']  = 'gbtree'\n    params['objective'] = 'reg:linear'\n    params['max_depth'] = 4\n    params['subsample'] = 0.8\n    params['colsample_bytree'] = 0.8\n    params['silent'] = 1\n    params['eval_metric'] = 'rmse'\n    num_round = i*40+1\n    eval_list  = [(dtrain,'train')]\n\n    print('training xgb model...')\n    bst = xgb.train(params, dtrain, num_round, eval_list)\n    pred_xg=(bst.predict(dtest))\n    val_xg=(bst.predict(dval))\n    #pred_xg_level=get_level(pred_xg)\n\n    #allpred_val['xg']=val_xg\n    #allpred_test['xg']=pred_xg\n\n    #print(\"MAPE\",np.sqrt(np.mean((pred_xg-Y_val)*(pred_xg-Y_val))))\n    #print(\"cm_xg\",correct_rate(cm_xg))\n    rmsewatch_4.append(np.sqrt(np.mean((val_xg-Y_val)*(val_xg-Y_val))))\n \nrmsewatch_5=[]\nfor i in range(1,5):\n    params = {}\n    params['booster']  = 'gbtree'\n    params['objective'] = 'reg:linear'\n    params['max_depth'] = 5\n    params['subsample'] = 0.8\n    params['colsample_bytree'] = 0.8\n    params['silent'] = 1\n    params['eval_metric'] = 'rmse'\n    num_round = i*40+1\n    eval_list  = [(dtrain,'train')]\n\n    print('training xgb model...')\n    bst = xgb.train(params, dtrain, num_round, eval_list)\n    pred_xg=(bst.predict(dtest))\n    val_xg=(bst.predict(dval))\n    #pred_xg_level=get_level(pred_xg)\n\n    #allpred_val['xg']=val_xg\n    #allpred_test['xg']=pred_xg\n\n    #print(\"MAPE\",np.sqrt(np.mean((pred_xg-Y_val)*(pred_xg-Y_val))))\n    #print(\"cm_xg\",correct_rate(cm_xg))\n    rmsewatch_5.append(np.sqrt(np.mean((val_xg-Y_val)*(val_xg-Y_val))))\n\nrmsewatch_6=[]\nfor i in range(1,5):\n    params = {}\n    params['booster']  = 'gbtree'\n    params['objective'] = 'reg:linear'\n    params['max_depth'] = 6\n    params['subsample'] = 0.8\n    params['colsample_bytree'] = 0.8\n    params['silent'] = 1\n    params['eval_metric'] = 'rmse'\n    num_round = i*40+1\n    eval_list  = [(dtrain,'train')]\n\n    print('training xgb model...')\n    bst = xgb.train(params, dtrain, num_round, eval_list)\n    pred_xg=(bst.predict(dtest))\n    val_xg=(bst.predict(dval))\n    #pred_xg_level=get_level(pred_xg)\n\n    #allpred_val['xg']=val_xg\n    #allpred_test['xg']=pred_xg\n\n    #print(\"MAPE\",np.sqrt(np.mean((pred_xg-Y_val)*(pred_xg-Y_val))))\n    #print(\"cm_xg\",correct_rate(cm_xg))\n    rmsewatch_6.append(np.sqrt(np.mean((val_xg-Y_val)*(val_xg-Y_val))))\n\nrmsewatch_3=[]\nfor i in range(1,5):\n    params = {}\n    params['booster']  = 'gbtree'\n    params['objective'] = 'reg:linear'\n    params['max_depth'] = 3\n    params['subsample'] = 0.8\n    params['colsample_bytree'] = 0.8\n    params['silent'] = 1\n    params['eval_metric'] = 'rmse'\n    num_round = i*40+1\n    eval_list  = [(dtrain,'train')]\n\n    print('training xgb model...')\n    bst = xgb.train(params, dtrain, num_round, eval_list)\n    pred_xg=(bst.predict(dtest))\n    val_xg=(bst.predict(dval))\n    #pred_xg_level=get_level(pred_xg)\n\n    #allpred_val['xg']=val_xg\n    #allpred_test['xg']=pred_xg\n\n    #print(\"MAPE\",np.sqrt(np.mean((pred_xg-Y_val)*(pred_xg-Y_val))))\n    #print(\"cm_xg\",correct_rate(cm_xg))\n    rmsewatch_3.append(np.sqrt(np.mean((val_xg-Y_val)*(val_xg-Y_val))))\n","metadata":{"trusted":false,"_execution_state":"idle","_uuid":"90bc902a12508786a28742771caac90c07d16579","_cell_guid":"5f95f5af-6fd5-da1a-45a6-eca3dd02b762"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"plt.plot(rmsewatch_3,'p--')\nplt.plot(rmsewatch_4,'r')\nplt.plot(rmsewatch_5,'g')\nplt.plot(rmsewatch_6,'b')\nprint(rmsewatch_3)\nprint(rmsewatch_4)\nprint(rmsewatch_5)\nprint(rmsewatch_6)\n","metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"9c8f54c41649516ecbac6a3278ed4fdde900c082","_cell_guid":"90b5bdc1-2668-40fb-b992-67ac037c5c27"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"ANNvalrmse_layer1=[]\nANNvalrmse_layer2=[]\nANNvalrmse_layer3=[]\nimport theano\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras import regularizers\n#from keras.regularizers import l1, activity_l1\nfrom keras import optimizers\n\nfor i in range(0,8):\n    model = Sequential()\n    model.add(Dense(dimdata,input_dim=dimdata,init='uniform', activation='tanh'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(20*i+1, activation='sigmoid'))\n    #model.add(Dropout(0.1))\n    #model.add(Dense(20, activation='sigmoid'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(1, activation='linear'))\n\n    op=optimizers.Adam(lr=0.0002)\n    model.compile(loss='mse',optimizer=op)\n    history = model.fit(np.asarray(X_train),np.asarray(scaleY_train),batch_size=16,nb_epoch=20,verbose=0,validation_split=0.2)\n    yPred = model.predict(np.asarray(X_test))\n\n    pred_ANN=yPred*30000\n    val_ANN=model.predict(np.asarray(X_val))*30000\n    pred_ANN_level=get_level(pred_ANN)\n    cm_ANN = confusion_matrix(level_test, pred_ANN_level)\n\n    #print(\"ANN\",correct_rate(cm_ANN))\n    #print(\"MAPE\",np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n    ANNvalrmse_layer1.append(np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n\nfor i in range(0,8):\n    model = Sequential()\n    model.add(Dense(dimdata,input_dim=dimdata,init='uniform', activation='tanh'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(40, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(20*i+1, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='linear'))\n\n    op=optimizers.Adam(lr=0.0002)\n    model.compile(loss='mse',optimizer=op)\n    history = model.fit(np.asarray(X_train),np.asarray(scaleY_train),batch_size=16,nb_epoch=20,verbose=0,validation_split=0.2)\n    yPred = model.predict(np.asarray(X_test))\n\n    pred_ANN=yPred*30000\n    val_ANN=model.predict(np.asarray(X_val))*30000\n    pred_ANN_level=get_level(pred_ANN)\n    cm_ANN = confusion_matrix(level_test, pred_ANN_level)\n\n    #print(\"ANN\",correct_rate(cm_ANN))\n    #print(\"MAPE\",np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n    ANNvalrmse_layer2.append(np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n\nfor i in range(0,8):\n    model = Sequential()\n    model.add(Dense(dimdata,input_dim=dimdata,init='uniform', activation='tanh'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(80, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(20*i+1, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='linear'))\n\n    op=optimizers.Adam(lr=0.0002)\n    model.compile(loss='mse',optimizer=op)\n    history = model.fit(np.asarray(X_train),np.asarray(scaleY_train),batch_size=16,nb_epoch=20,verbose=0,validation_split=0.2)\n    yPred = model.predict(np.asarray(X_test))\n\n    pred_ANN=yPred*30000\n    val_ANN=model.predict(np.asarray(X_val))*30000\n    pred_ANN_level=get_level(pred_ANN)\n    cm_ANN = confusion_matrix(level_test, pred_ANN_level)\n\n    #print(\"ANN\",correct_rate(cm_ANN))\n    #print(\"MAPE\",np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n    ANNvalrmse_layer3.append(np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n\nfor i in range(0,8):\n    model = Sequential()\n    model.add(Dense(dimdata,input_dim=dimdata,init='uniform', activation='tanh'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(40, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(20*i+1, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='linear'))\n\n    op=optimizers.Adam(lr=0.0002)\n    model.compile(loss='mse',optimizer=op)\n    history = model.fit(np.asarray(X_train),np.asarray(scaleY_train),batch_size=16,nb_epoch=20,verbose=0,validation_split=0.2)\n    yPred = model.predict(np.asarray(X_test))\n\n    pred_ANN=yPred*30000\n    val_ANN=model.predict(np.asarray(X_val))*30000\n    pred_ANN_level=get_level(pred_ANN)\n    cm_ANN = confusion_matrix(level_test, pred_ANN_level)\n\n    #print(\"ANN\",correct_rate(cm_ANN))\n    #print(\"MAPE\",np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n    ANNvalrmse_layer2.append(np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n\nplt.plot(ANNvalrmse_layer1)\nplt.plot(ANNvalrmse_layer2)\nplt.plot(ANNvalrmse_layer3)","metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"2b8ac1743093f76be2e44446f11b45144d66033f","_cell_guid":"e39dc864-a176-468c-a723-a557c1716376"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import theano\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras import regularizers\n#from keras.regularizers import l1, activity_l1\nfrom keras import optimizers\nepochemse=[]\nfor i in range(0,8):\n    model = Sequential()\n    model.add(Dense(dimdata,input_dim=dimdata,init='uniform', activation='tanh'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(40, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(40+1, activation='sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='linear'))\n\n    op=optimizers.Adam(lr=0.0002)\n    model.compile(loss='mse',optimizer=op)\n    history = model.fit(np.asarray(X_train),np.asarray(scaleY_train),batch_size=16,nb_epoch=4*i+1,verbose=0,validation_split=0.2)\n    yPred = model.predict(np.asarray(X_test))\n\n    pred_ANN=yPred*30000\n    val_ANN=model.predict(np.asarray(X_val))*30000\n    pred_ANN_level=get_level(pred_ANN)\n    cm_ANN = confusion_matrix(level_test, pred_ANN_level)\n\n    #print(\"ANN\",correct_rate(cm_ANN))\n    #print(\"MAPE\",np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n    epochemse.append(np.sqrt(np.mean((val_ANN[:,0]-Y_val)*(val_ANN[:,0]-Y_val))))\n\nplt.plot(epochemse)","metadata":{"_cell_guid":"1c74adbf-8e4e-444c-bb71-bcad8e61824a","trusted":false,"_execution_state":"idle","_uuid":"221591f8517720df6cb88f1b8a55efe29c4e5837","collapsed":false}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"","metadata":{"_cell_guid":"be000f60-f050-43a5-a617-c677ffb13986","trusted":false,"_execution_state":"idle","_uuid":"aeeabb222fe082ffdf8f305f2c00d09e0b0ee661","collapsed":false}}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"}},"_is_fork":false,"_change_revision":0,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":0}