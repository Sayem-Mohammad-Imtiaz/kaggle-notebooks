{"cells":[{"metadata":{},"cell_type":"markdown","source":"<p>&nbsp;</p>\n<img src=\"https://1000logos.net/wp-content/uploads/2017/05/Reddit-logo.png\" width=400>\n<p>&nbsp;</p>"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis is a brief exploratory data analysis focused only on word clouds methodology using Pandas for a given public sample of random Reddit posts.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Getting all the packages we need: \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport seaborn as sns #statist graph package\nimport matplotlib.pyplot as plt #plot package\n\nimport wordcloud #will use for the word cloud plot\n#from wordcloud import WordCloud, STOPWORDS # optional to filter out the stopwords\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#Optional helpful plot stypes:\nplt.style.use('bmh') #setting up 'bmh' as \"Bayesian Methods for Hackers\" style sheet\n#plt.style.use('ggplot') #R ggplot stype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?WordCloud","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a name=\"read\"></a>Reading the dataset\nAccessing Reddit dataset:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dataisbeautiful/r_dataisbeautiful_posts.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Empty values:\n\ndf.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df.title[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a name=\"corr\"></a>The most common words in reddits:\n\nLet's see the word map of the most commonly used words from reddit titles:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To build a wordcloud, we have to remove NULL values first:\ndf[\"title\"] = df[\"title\"].fillna(value=\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's add a string value instead to make our Series clean:\nword_string=\" \".join(df['title'].str.lower())\n\n#word_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And - plotting:\n\nplt.figure(figsize=(15,15))\nwc = WordCloud(background_color=\"purple\", stopwords = STOPWORDS, max_words=2000, max_font_size= 300,  width=1600, height=800)\nwc.generate(word_string)\n\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), interpolation=\"bilinear\")\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change color\n#set max words to 50\n\nplt.figure(figsize=(15,15))\nwc = WordCloud(background_color=\"yellow\", stopwords = STOPWORDS, max_words=50, max_font_size= 300,  width=1400, height=800)\nwc.generate(word_string)\n\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), interpolation=\"bilinear\")\nplt.axis('off')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}