{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Water Quality Prediction**\nWe all know water is one of the most essential resource for our living. But as the development is increasing, we are exploiting water by wasting it and treating it with harmful materials which makes water impure and unfit for use. This is the reason it is very important to know the quality of water. This kernel is based on water quality prediction. In this kernel, water quality index (WQI) and quality status of water is predicted through some parameters that affects water quality. \nIn this notebook I have performed Data Cleaning steps and did Exploratory Data Analysis. Then I have did some calculations as the data does not contain the column which can be used for prediction.\nThen I have created 3 models for prediction. The first model is Non-Deep Learning based Linear Regression model. The second model is Deep Learning Based Linear Regression and the last one is Logistic Regression model. I have only used sparkml to create all the models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Table of Contents**\n* [Setting up the environment](#1)\n* [Importing Libraries](#2)\n* [Uploading the data](#3)\n* [Data Cleaning](#4)\n* [EDA](#5)\n* [Feature Engineering](#6)\n* [Model Creation](#7)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=1></a>\n# **Setting up the environment**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Before starting we first have to change the java version because if we will use version 11 then we will get some errors and we will not be able to use pyspark properly. So we will delete java version 11 and install java version 8.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"! apt remove -y openjdk-11-jre-headless","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!apt install -y openjdk-8-jdk openjdk-8-jre","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we will first install pyspark.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=2></a>\n# **Importing libraries**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pylab import *\nfrom pyspark.sql.functions import udf, concat, col, lit\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()\nsqlContext = SQLContext(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3></a>\n# **Uploading the data**\n#### Then, we upload the data in the spark frame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.format(\"csv\").option(\"header\", \"true\").load('../input/water-quality-data/waterquality.csv')\ngdf = gpd.read_file('../input/india-states/Igismap/Indian_States.shp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4></a>\n# **Data Cleaning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.types import FloatType","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we observed obove that all the columns have string data types, but for the calculation of water quality index we need to convert them in float data type. So we will convert the required columns in the float data type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.withColumn(\"TEMP\",df[\"TEMP\"].cast(FloatType()))\ndf = df.withColumn(\"pH\",df[\"pH\"].cast(FloatType()))\ndf = df.withColumn(\"DO\",df[\"DO\"].cast(FloatType()))\ndf = df.withColumn(\"CONDUCTIVITY\",df[\"CONDUCTIVITY\"].cast(FloatType()))\ndf = df.withColumn(\"BOD\",df[\"BOD\"].cast(FloatType()))\ndf = df.withColumn(\"NITRATE_N_NITRITE_N\",df[\"NITRATE_N_NITRITE_N\"].cast(FloatType()))\ndf = df.withColumn(\"FECAL_COLIFORM\",df[\"FECAL_COLIFORM\"].cast(FloatType()))\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now as column TOTAL_COLIFORM is not required so we will drop this column. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop('TOTAL_COLIFORM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we want to remove all the rows which contain any null value in it. So for applying a SQL query we first have to register it has a virtual temporary table and then we will issue SQL query. We are doing this because it is important to perform data cleansing steps as it will make our model to work better.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.createOrReplaceTempView(\"df_sql\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = spark.sql('''Select * from df_sql where TEMP is not null and DO is not null \n                        and pH is not null and BOD is not null and CONDUCTIVITY is not null\n                        and NITRATE_N_NITRITE_N is not null and FECAL_COLIFORM is not null''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=5></a>\n# **EDA**\n### Let's visualize our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.createOrReplaceTempView(\"df_sql\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do = spark.sql(\"Select DO from df_sql\")\ndo = do.rdd.map(lambda row : row.DO).collect()\nph = spark.sql(\"Select pH from df_sql\")\nph = ph.rdd.map(lambda row : row.pH).collect()\nbod = spark.sql(\"Select BOD from df_sql\")\nbod = bod.rdd.map(lambda row : row.BOD).collect()\nnn = spark.sql(\"Select NITRATE_N_NITRITE_N from df_sql\")\nnn = nn.rdd.map(lambda row : row.NITRATE_N_NITRITE_N).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nsize=len(do)\nax.plot(range(0,size), do, color='blue', animated=True, linewidth=1, label='Dissolved Oxygen')\nax.plot(range(0,size), ph, color='red', animated=True, linewidth=1, label='pH')\nfig,ax2 = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nax2.plot(range(0,size), bod, color='orange', animated=True, linewidth=1, label='BOD')\nax2.plot(range(0,size), nn, color='green', animated=True, linewidth=1, label='NN')\nlegend=ax.legend()\nlegend=ax2.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con = spark.sql(\"Select CONDUCTIVITY from df_sql\")\ncon = con.rdd.map(lambda row : row.CONDUCTIVITY).collect()\nfec = spark.sql(\"Select FECAL_COLIFORM from df_sql\")\nfec = fec.rdd.map(lambda row : row.FECAL_COLIFORM).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nax.plot(range(0,size), con, color='blue', animated=True, linewidth=1)\nfig,ax2 = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nax2.plot(range(0,size), fec, color='red', animated=True, linewidth=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=6></a>\n# **Feature Engineering**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let us convert our data to pandas frame. We are doing this because to train a model we need what we have to predict which is not in data. So we have to calculate water quality index which requires many steps but can be easily done using pandas and in less number of steps. Also we will able to visualize our data in tabular form more effectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_clean.toPandas()\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initialization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"start=0\nend=448\nstation=df.iloc [start:end ,0]\nlocation=df.iloc [start:end ,1]\nstate=df.iloc [start:end ,2]\ndo= df.iloc [start:end ,4].astype(np.float64)\nvalue=0\nph = df.iloc[ start:end,5]  \nco = df.iloc [start:end ,6].astype(np.float64)\nbod = df.iloc [start:end ,7].astype(np.float64)\nna= df.iloc [start:end ,8].astype(np.float64)\nfc=df.iloc [2:end ,9].astype(np.float64)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([station,location,state,do,ph,co,bod,na,fc],axis=1)\ndf. columns = ['station','location','state','do','ph','co','bod','na','fc']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Water Quality Index is calculated by aggregating the quality rating with the weight linearly, \n#### WQI = âˆ‘ (qn x Wn)\n#### where qn =Quality rating for the nth Water quality parameter, Wn= unit weight for the nth parameters.       \n#### Although for calculation qn we have standard formula but it was not possible in this case, so we applied a standard method for calculating quality rating for each parameter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['npH']=df.ph.apply(lambda x: (100 if (8.5>=x>=7)  \n                                 else(80 if  (8.6>=x>=8.5) or (6.9>=x>=6.8) \n                                      else(60 if (8.8>=x>=8.6) or (6.8>=x>=6.7) \n                                          else(40 if (9>=x>=8.8) or (6.7>=x>=6.5)\n                                              else 0)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ndo']=df.do.apply(lambda x:(100 if (x>=6)  \n                                 else(80 if  (6>=x>=5.1) \n                                      else(60 if (5>=x>=4.1)\n                                          else(40 if (4>=x>=3) \n                                              else 0)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nco']=df.fc.apply(lambda x:(100 if (5>=x>=0)  \n                                 else(80 if  (50>=x>=5) \n                                      else(60 if (500>=x>=50)\n                                          else(40 if (10000>=x>=500) \n                                              else 0)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nbdo']=df.bod.apply(lambda x:(100 if (3>=x>=0)  \n                                 else(80 if  (6>=x>=3) \n                                      else(60 if (80>=x>=6)\n                                          else(40 if (125>=x>=80) \n                                              else 0)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nec']=df.co.apply(lambda x:(100 if (75>=x>=0)  \n                                 else(80 if  (150>=x>=75) \n                                      else(60 if (225>=x>=150)\n                                          else(40 if (300>=x>=225) \n                                              else 0)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nna']=df.na.apply(lambda x:(100 if (20>=x>=0)  \n                                 else(80 if  (50>=x>=20) \n                                      else(60 if (100>=x>=50)\n                                          else(40 if (200>=x>=100) \n                                              else 0)))))\n\ndf.head()\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we apply the formula of wqi by first multiplying all the quality rating with its weight and then summed all the values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['wph']=df.npH * 0.165\ndf['wdo']=df.ndo * 0.281\ndf['wbdo']=df.nbdo * 0.234\ndf['wec']=df.nec* 0.009\ndf['wna']=df.nna * 0.028\ndf['wco']=df.nco * 0.281\ndf['wqi']=df.wph+df.wdo+df.wbdo+df.wec+df.wna+df.wco \ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Then we classify the water on the basis of their water quality index.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality']=df.wqi.apply(lambda x:('Excellent' if (25>=x>=0)  \n                                 else('Good' if  (50>=x>=26) \n                                      else('Poor' if (75>=x>=51)\n                                          else('Very Poor' if (100>=x>=76) \n                                              else 'Unsuitable')))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's visualize the water quality index in each state of India.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#renaming state names\ngdf['st_nm'].replace({\"Andaman & Nicobar Island\": \"Andaman and Nicobar Islands\",\n                      \"Arunanchal Pradesh\": \"Arunachal Pradesh\",\n                      'Dadara & Nagar Havelli':'Dadra and Nagar Haveli and Daman and Diu',\n                      'Jammu & Kashmir':'Jammu and Kashmir',\n                      'NCT of Delhi':'Delhi'}, inplace=True)\ndf['state'].replace({\"TAMILNADU\": \"TAMIL NADU\"}, inplace=True)\n\n#Capitalizing only the first letter of each word\ndf['state'] = df['state'].str.title()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gdf = gdf.rename(columns={\"st_nm\": \"state\"})\nmerged = pd.merge(gdf, df , how='outer', on='state')\nmerged['coords'] = merged['geometry'].apply(lambda x: x.representative_point().coords[:])\nmerged['coords'] = [coords[0] for coords in merged['coords']]\nmerged = merged.drop_duplicates(subset =\"state\") \n\nsns.set_context(\"talk\")\nsns.set_style(\"dark\")\ncmap = 'Blues'\nfigsize = (20, 15)\nax = merged.plot(column= 'wqi', cmap=cmap, \n                          figsize=figsize, scheme='User_Defined',\n                          classification_kwds=dict(bins=[0,25,50,75,100]),\n                          edgecolor='black', legend = True)\nfor idx, row in merged.iterrows():\n    ax.text(row.coords[0], row.coords[1], s=row['wqi'], horizontalalignment='center', bbox={'facecolor': 'yellow', 'alpha':0.8, 'pad': 1, 'edgecolor':'blue'})\n\nax.get_legend().set_title('Water Quality Index')\nax.set_title(\"Water Quality Index in each state \", size = 25)\n\nax.set_axis_off()\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let us again convert the whole data in spark frame for further processes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df = sqlContext.createDataFrame(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df.createOrReplaceTempView(\"df_sql\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"State = spark.sql(\"Select state from df_sql\")\nState = State.rdd.map(lambda row : row.state).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Wqi = spark.sql(\"Select wqi from df_sql\")\nWqi = Wqi.rdd.map(lambda row : row.wqi).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.barh(State,Wqi)\n\nplt.xlabel(\"WQI\")\nplt.ylabel(\"STATES\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=7></a>\n# **Model Creation**\n#### Now we apply machine learning and deep learning algorithms to predict the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Non Deep Learning Based Linear Regresion Model\n\n#### In this, first data is converted which are required to predict WQI into vector form by using VectorAssembler. Then we normalize our data by using Normalizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler = VectorAssembler(inputCols=[\"npH\",\"ndo\",\"nbdo\",\"nec\",\"nna\",\"nco\"], outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\",outputCol=\"features_norm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Then import LinearRegression from pyspark.ml.regression and applied it to our normalized data. Afterthat, import Pipeline from pyspark.ml and include all those steps in the pipeline that have been done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.regression import LinearRegression\nlr = LinearRegression(featuresCol=\"features_norm\",labelCol=\"wqi\",maxIter=10,regParam=0.3,elasticNetParam=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(stages=[vectorAssembler,normalizer,lr])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Before training, our data is randomly split in two parts so as to avoid overfitting and then training is done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data,test_data=spark_df.randomSplit([0.8,0.2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pipeline.fit(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.transform(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.select(\"wqi\",\"prediction\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we check the performance of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.stages[2].summary.r2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Learning Based Linear Regression Model\n#### In this first we collect our data in an array form and to reduce number of steps we converted our data in pandas frame. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark_df.toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.iloc[:,9:15].values\npred = df.iloc[:,21:22].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \ndata_train,data_test,pred_train,pred_test = train_test_split(data,pred,test_size=0.20,random_state=1)\npred_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Then we initialize model and add layers to it. Afterwards, the model is compiled with optimizer Adam and loss function mean squared error and then training is done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Dense(350,input_dim=6, activation='relu'))\nmodel2.add(Dense(350,activation='relu'))\nmodel2.add(Dense(350,activation='relu'))\nmodel2.add(Dense(1,activation='linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False )\nmodel2.compile(loss='mean_squared_error',optimizer='Adam', metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perform = model2.fit(data_train,pred_train,epochs=50,batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model2.predict(data_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we check performance of our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(perform.history['loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pred_train,'bo',prediction,'g+')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Water Quality Prediction\n#### After predicting water quality index, now we classify water on the basis of its WQI and predict its quality.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spark_df = sqlContext.createDataFrame(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Model\n#### Here we are creating a logistic regression model because we don't have to predict a continuous value. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As our quality column contains values in string format so first we indexed them using StringIndexer. Then data is converted which are required to predict water quality into vector form by using VectorAssembler. Then we normalize our data by using Normalizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"indexer = StringIndexer(inputCol=\"quality\",outputCol=\"label\")\nvectorAssembler2 = VectorAssembler(inputCols=[\"npH\",\"ndo\",\"nbdo\",\"nec\",\"nna\",\"nco\",\"wqi\"], outputCol=\"features2\")\nnormalizer2 = Normalizer(inputCol=\"features2\",outputCol=\"features_norm2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Then import LogisticRegression from pyspark.ml.classification and applied it to our normalized data. Afterthat, import Pipeline from pyspark.ml and include all those steps in the pipeline that have been done.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lor = LogisticRegression(featuresCol=\"features_norm2\",labelCol=\"label\",maxIter=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline2 = Pipeline(stages=[indexer,vectorAssembler2,normalizer2,lor])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data,test_data=spark_df.randomSplit([0.8,0.2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = pipeline2.fit(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2 = model3.transform(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now let us check our predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2.select(\"label\",\"prediction\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we check performance of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')\neval.evaluate(predictions2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As the quality column is in string format so we convert our predicted data which are in numbers to their real string values and compared with the actual data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"Very Poor\",\"Poor\",\"Good\",\"Unsuitable\",\"Excellent\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2.createOrReplaceTempView(\"predictions2_sql\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = spark.sql(\"Select prediction from predictions2_sql\")\npred = pred.rdd.map(lambda row : int(row.prediction)).collect()\nqua = spark.sql(\"Select quality from predictions2_sql\")\nqua = qua.rdd.map(lambda row : row.quality).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in range(100):\n    print(\"Predicted:\", names[pred[x]], \"Actual:\", qua[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Please if you want to share any suggestion or any doubts regarding any step in the notebook comment below and I will definitely try to solve your doubt.<br> Also, if you want to know more about Spark ML or if you don't know much about Spark ML you can view my another notebook: - https://www.kaggle.com/utcarshagrawal/titanic-spark-ml-magic-eda-feature-engineering/notebook.<br> This notebook will work as a perfect tutorial for beginners.**\n\n## <font color='red'> Please do an upvote if you find this kernel useful or if you liked the kernel! </font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}