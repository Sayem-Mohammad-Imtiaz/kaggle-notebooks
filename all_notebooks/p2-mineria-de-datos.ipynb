{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Práctica 2: Técnicas de análisis\n## Minería de Datos, Máster Universitario en Ciencia de Datos\n## Jorge García Carrasco","metadata":{}},{"cell_type":"code","source":"!pip install openpyxl\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# para que pandas muestre todas las columnas del DataFrame\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 67)\n\nimport datetime\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introducción\n\nEl objetivo de esta práctica consiste en familiarizarse con las diferentes técnicas de análisis propias de la minería de datos. \nPartiremos del dataset **EM-DAT** (https://www.emdat.be/), que contiene información sobre los diferentes desastres que han ocurrido a lo largo del planeta, desde 1900 hasta la actualidad.\n\nPreprocesaremos y echaremos un primer vistazo al dataset, para familiarizarnos con las variables presentes, observar si faltan valores...\n\nA continuación, comenzaremos con la **Parte 1** de la práctica, en la que se utilizarán diversos algoritmos para un problema de **regresión** (es decir, intentar buscar la relación entre las variables explicativas y una variable respuesta numérica), y un problema de **clasificación** (en este caso, la variable respuesta es **categórica**). Una vez se hayan probado varios algoritmos se evaluarán y se analizarán los resultados que se hayan podido extraer. \n\nConcretamente, el problema de regresión consistirá en predecir los costes ocasionados por el desastre, mientras que en el problema de clasificación se tratará de clasificar los desastres en diferentes grupos según su naturaleza.\n\nFinalmente, para la **Parte 2**, realizaremos una tarea de **ensembles** y otra de **clustering**. Para la tarea de ensembles se utilizará el mismo dataset que en la parte 1, pero para la parte de clustering se utilizarán datos sobre las estadísticas de los campeones de League of Legends.\n\n# Primer vistazo al dataset","metadata":{}},{"cell_type":"markdown","source":"Los siguientes enlaces contienen una gran cantidad de información sobre el dataset:\n- https://emdat.be/guidelines\n-http://drm.cenn.org/Trainings/Multi%20Hazard%20Risk%20Assessment/Lectures_ENG/Session%2001%20Introduction%20to%20risk%20management/Tasks/disaster%20database/EM%20dat%20data%20base%20guidelines.doc\n\nVamos a echar un primer vistazo:","metadata":{}},{"cell_type":"code","source":"emdat = pd.read_excel('../input/emdat/emdat.xlsx')\nemdat.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tal y como está especificado en la página, en el dataset de EM-DAT se consideran como desastres aquellos que cumplan al menos uno de estos criterios:\n\n- $10 \\geq$ muertos.\n- $100 \\geq$ personas afectadas.\n- Se ha declarado estado de alarma debido a este evento.\n- Se ha pedido ayuda internacional\n\nEl criterio escogido para incluir el desastre en el dataset se indica en la columna `Entry Criteria`:","metadata":{}},{"cell_type":"code","source":"emdat['Entry Criteria'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En en enlace (http://drm.cenn.org/Trainings/Multi%20Hazard%20Risk%20Assessment/Lectures_ENG/Session%2001%20Introduction%20to%20risk%20management/Tasks/disaster%20database/EM%20dat%20data%20base%20guidelines.doc) explican algunos de los códigos utilizados:\n\n- **Kill**: 10 or more people killed\n- **Affected**: 100 or more people affected/injured/homeless\n- **SigDis**: Significant disaster (e.g. «second worst »)\n- **SigDam**: Significant damage\n- **Decla/int**: Declaration of a state of emergency or/and appeal for an international assistance\n- **Regional**: Disaster entered at the country level without data, because it has affected several countries/regions.\n- **Unknown**: Reason not known (old entries)\n\nSiguiendo estas pautas, juntaremos varias categorías (por ejemplo, `Affected` y `Affect`, `Declar` y `Declar/Int`), y todas las categorías que no aparezcan en el listado de arriba las clasificaremos como `Unknown`:","metadata":{}},{"cell_type":"code","source":"emdat['Entry Criteria'] = emdat['Entry Criteria'].replace(\"Affect\", \"Affected\")\nemdat['Entry Criteria'] = emdat['Entry Criteria'].replace(\"Declar\", \"Declar/Int\")\nemdat['Entry Criteria'] = emdat['Entry Criteria'].replace([\"Waiting\", \"OFDA\", \"Govern\", \"--\"], \"Unknown\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emdat['Entry Criteria'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = len(emdat)\nprint('El dataset contiene {} desastres'.format(n))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emdat['Disaster Group'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El dataset esta compuesto por un total de 24923 desastres, clasificados en dos grandes grupos: desastres de carácter **natural**, y desastres de carácter **tecnológico**. También hay un tercer grupo muy reducido denominado **Complex disasters**, o desastres complejos, compuesto por sólo 14 desastres.\n\nA su vez, estos grupos se dividen en subgrupos, y una gran variedad de tipos y subtipos:","metadata":{}},{"cell_type":"code","source":"df = emdat[['Disaster Group', 'Disaster Subgroup', 'Disaster Type', 'Disaster Subtype']].groupby(['Disaster Group', 'Disaster Subgroup', 'Disaster Type'])['Disaster Subtype'].unique().to_frame()\ndf.explode('Disaster Subtype').dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos observar, la cantidad de tipos de desastre es abrumadora. Estos tipos y subtipos están descritos en la página de EM-DAT: https://www.emdat.be/classification.\n\nTambién se puede observar que el grupo `Complex Disasters` sólo contiene desastres del subtipo `Famine`, es decir, hambruna.","metadata":{}},{"cell_type":"markdown","source":"Ahora, vamos a tratar los datos de fecha de inicio y final del desastre. Primero, debemos decidir qué hacer con los valores ausentes:","metadata":{}},{"cell_type":"code","source":"emdat[['Start Year', 'Start Month', 'Start Day', 'End Year', 'End Month', 'End Day']].isna().sum(axis=0)/n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver que los meses de inicio y final del desastre no están específicados para alrededor de un 2% y 3% respectivamente. Podríamos tratar de aplicar técnicas de imputación, pero para un porcentaje tan bajo no vale la pena. Además, los diferentes desastres son independientes entre sí, y no tendría sentido sustituir los valores ausentes por la moda.\n\nEs probable que diferentes tipos de desastres ocurran en una temporada determinada (por ejemplo, las sequías suelen pasar en meses de verano, y las heladas en meses de invierno). Tal vez, esta podría ser una buena estrategia a estudiar, pero por ahora simplemente **eliminaremos** los desastres en los que se desconozca el mes de inicio y final.","metadata":{}},{"cell_type":"code","source":"emdat = emdat[emdat['Start Month'].notna() & emdat['End Month'].notna()]\n\nprint('Se han eliminado {} columnas ({:.2f}% de los datos)'.format(n - len(emdat), (n - len(emdat))/n*100))\n# actualizamos el numero de muestras del dataset\nn = len(emdat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El día de inicio y final del desastre no está especificado para aproximadamente un 15% de los datos. Sin embargo, el día de inicio y final no parece que sea un valor relevante a la hora de analizar los datos, por tanto, reemplazaremos los valores ausentes simplemente por el **primer** día del mes:","metadata":{}},{"cell_type":"code","source":"emdat[['Start Day', 'End Day']] = emdat[['Start Day', 'End Day']].fillna(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez hemos tratado con los valores ausentes, podemos convertir los datos a formato `datetime`:","metadata":{}},{"cell_type":"code","source":"# Tomamos las fechas de inicio\ndf = emdat[['Start Year', 'Start Month', 'Start Day']].astype('int32')\n# cambiamos el nombre de las columnas\ndf = df.rename(columns={'Start Year': 'year',\n                   'Start Month': 'month',\n                   'Start Day': 'day'})\ndf = pd.to_datetime(df, errors='coerce')\nemdat[df.isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que hay una fecha que da error a la hora de convertir a formato `datetime`. Analizando con mayor detalle, vemos que el error salta porque esa fecha **no existe**: el inicio del desastre se ha registrado como el día 31 de septiembre de 1992, pero el mes de septiembre de 1992 sólo tenia 30 días. \n\nComo sólo se trata de un error, lo eliminaremos del dataset.","metadata":{}},{"cell_type":"code","source":"emdat = emdat[df.notna()]\ndf = df[df.notna()]\n# Creamos la columna con la fecha de inicio del desastre en un formato correcto\nemdat['Start Date'] = df\n# Eliminamos las columnas de año, mes y día\n#emdat = emdat.drop(['Start Year', 'Start Month', 'Start Day'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realizamos el mismo proceso para las fechas de fin del desastre:","metadata":{}},{"cell_type":"code","source":"# Tomamos las fechas de inicio\ndf = emdat[['End Year', 'End Month', 'End Day']].astype('int32')\n# cambiamos el nombre de las columnas\ndf = df.rename(columns={'End Year': 'year',\n                   'End Month': 'month',\n                   'End Day': 'day'})\ndf = pd.to_datetime(df, errors='coerce')\nemdat[df.isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emdat = emdat[df.notna()]\ndf = df[df.notna()]\n# Creamos la columna con la fecha de inicio del desastre en un formato correcto\nemdat['End Date'] = df\n# Eliminamos las columnas de año, mes y día\n#emdat = emdat.drop(['End Year', 'End Month', 'End Day'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez tenemos las fechas en el formato adecuado, las podemos utilizar para crear la variable `Duration`, compuesta por la **duración** del desastre en días. Esta variable puede resultar útil a la hora de buscar patrones y aplicar algoritmos:","metadata":{}},{"cell_type":"code","source":"emdat['Duration'] = emdat['End Date'] - emdat['Start Date'] + datetime.timedelta(days=1)\nemdat['Duration'] = emdat['Duration'].dt.days\nemdat.sort_values(by='Duration')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(emdat[(emdat['Duration'] < 0)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el código anterior podemos ver que hay desastres con **duración negativa**. Evidentemente esto se trata de un error; puede que al introducir el desastre en la base de datos se hayan confundido entre fecha de inicio y fecha final. \n\nPodríamos tratar de revertir las fechas, pero como sólo se tratan de 14 muestras, las eliminaremos.","metadata":{}},{"cell_type":"code","source":"emdat = emdat[emdat['Duration']>0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 1: Regresión y Clasificación\n## Regresión\n\nAntes de comenzar, es importante recalcar que el objetivo de los próximos algoritmos **no es el de predecir**, ya que el momento en el que ocurre un desastre y sus consecuencias tiene un fuerte carácter aleatorio. Por tanto, el objetivo de estos algoritmos consistirá en **analizar** la relación y patrones entre las diferentes variables, en otras palabras, se utilizarán como herramienta para la **extracción de conocimiento** a partir de los datos.","metadata":{}},{"cell_type":"markdown","source":"La variable que analizaremos será la de los daños estimados causados por el desastre, `Total Damages`. En la página de EM-DAT, se indica que esta variable representa el daño estimado a la propiedad, cultivos y ganado, en miles de dólares (\\$). Es importante recalcar que el **valor es exactamente el mismo que se estimó en el propio año del desastre** y no tiene en cuenta otras variables como la inflación.\n\nComo podemos apreciar, este valor no está indicado para **aproximadamente un 78\\% de los desastres**. Desgraciadamente, no podremos utilizar esas muestras para el análisis, y deberemos descartarlos. Sin embargo, el dataset es bastante grande y nos quedan un total de 5302 muestras, que serán suficientes para nuestro análisis de regresión.","metadata":{}},{"cell_type":"code","source":"n = len(emdat)\n(emdat.isna().sum(axis=0)/n).sort_values().to_frame('% de valores ausentes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('El valor `Total Damages` está indicado para {} muestras'.format(len(emdat['Total Damages (\\'000 US$)'].dropna())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reg = emdat[emdat['Total Damages (\\'000 US$)'].notna()]\n# añadimos la columna de Start Month\ndf_reg['Start Month'] = df_reg['Start Date'].dt.month\nX, y = df_reg.drop(['Total Damages (\\'000 US$)'], axis=1), df_reg['Total Damages (\\'000 US$)']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = len(df_reg)\n(df_reg.isna().sum(axis=0)/n).sort_values().to_frame('% de valores ausentes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para la tarea de regresión, utilizaremos las siguientes variables numéricas:\n\n- `Total Deaths`: Es esperable que a mayor número de muertes, mayor sean los costes del desastre.\n\n- `Total Affected`: De nuevo, se espera que a mayor número de afectados, mayor será el desastre y su coste asociado.\n\n- `Duration`: También se espera que la duración tenga alguna relación con el coste.\n\n- `Year`:  Probablemente el año tenga relación con el coste debido a la inflación.\n\nRespecto a las variables categóricas, utilizaremos:\n\n- `Continent`: La incluimos para estudiar si hay cierta dependencia del coste con la localización del desastre.\n\n- `Entry Criteria`: Ciertos criterios de entrada, como la declaración del estado de alarma, dan información sobre la magnitud del desastre, y por tanto pueden estar relacionados con el coste.\n\n- `Start Month`: Se incluye para ver si hay cierta dependencia estacional con el coste o calibre del desastre.\n\n- `Disaster Type`, `Disaster Group` y `Disaster Subgroup`: Se espera que ciertos tipos de desastres supongan más coste que otros (por ejemplo, un tsunami o terremoto puede ser mucho más catastrófico que un incendio)\n\n\nCrearemos un pipeline de preprocesado de datos que conectaremos a los diferentes algoritmos. Inicialmente, se tendrán dos pipelines diferentes, una para las variables numéricas y otro para las categóricas:\n\n- El pipeline de las variables numéricas estará formado por un `SimpleImputer`, que simplemente rellenará valores ausentes con la mediana (en el caso de que hubieran casos ausentes), y un `StandardScaler`, que escala las variables para que formen una distribución de media cero y desviación estándar 1. Estandarizar es muy importante para que funcionen correctamente ciertos algoritmos como kNN o SVR.\n\n- El pipeline de las variables categóricas estarán formado simplemente por un `OneHotEncoder`\n\nEstos dos pipelines se unirán y se le conectará el algoritmo deseado.\n\nProbaremos primero con una simple **regresión lineal** para tener un modelo base de referencia. **Fijaremos una semilla** para poder reproducir los resultados, aunque es importante recalcar que en una **situación real** esto no se debe hacer, ya que algunos resultados pueden variar de una ejecución a otra.","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lista de variables numericas que vamos a utilizar\nnumeric_features = ['Total Deaths', \n                    'Total Affected', \n                    'Duration', \n                    'Year']\n# Lista de variables categóricas\ncategorical_features = ['Continent', \n                        'Entry Criteria', \n                        'Start Month',\n                        'Disaster Type',\n                        'Disaster Group',\n                        'Disaster Subgroup']\n\n# Pipeline de procesado de variables numericas\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# Pipeline de procesado de variables categoricas\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\nreg = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('linear_reg', LinearRegression())])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=0)\n\nreg.fit(X_train, y_train)\nprint(\"R^2: %.3f\" % reg.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n\ndef summary(model, X, y, cv=5, scoring=('r2', 'neg_root_mean_squared_error')):\n    \"\"\" Función auxiliar que evalúa un modelo utilizando cross-validation y\n        imprime un pequeño resumen.\n    \"\"\"\n    cv_result = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=True)\n    scores = -cv_result['test_neg_root_mean_squared_error']\n    rmse = scores.mean()\n    rmse_train = -cv_result['train_neg_root_mean_squared_error'].mean()\n    print(\"RESUMEN CV=5, modelo {}\".format(model.steps[-1][0]))\n    print(\"RMSE_Train: {:.0f}\".format(rmse_train))\n    print(\"RMSE_Test: {:.0f}\".format(rmse))\n    print(\"Valor medio de la variable Total damages: {:.0f}\".format(y.mean()))\n    print(\"RMSE_Test/(Valor medio): {:.03f}%\".format(rmse/y.mean()*100))\n    print(\"R2: {}\".format(cv_result['test_r2'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(reg, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluando el modelo con cross-validation (con 5 folds), obtenemos un RMSE **8 veces mayor** que la media de la variable que intentamos predecir. Se considera que el ajuste es bueno cuando el RMSE es aproximadamente un 15\\% del valor medio. Además, se obtiene un $R^2$ negativo, indicando que el modelo no es capaz de explicar la relación entre las variables explicativas y la variable respuesta.\n\nLos resultados desde luego no son prometedores, pero hemos de tener en cuenta que un modelo de regresión lineal sólo puede captar dependencias lineales, y es probable que las variables que hemos escogido tengan no-linealidades. Por tanto, vamos a probar modelos que puedan captar patrones no lineales, como pueden ser los algoritmos de **KNN**, **Decision Trees** y los **SVM** con kernel no lineal.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('knn', KNeighborsRegressor())])\n\nsummary(knn, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndtree = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('dtree', DecisionTreeRegressor(max_depth=1, max_features=5))])\n\nsummary(dtree, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvr = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('svr', SVR())])\n\nsummary(svr, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Al usar modelos no lineales el RMSE se reduce considerablemente, y el $R^2$ ya no tiene un valor tan negativo. El mejor modelo parece ser el SVR, con un $R^2$ cercano a 0. Evidentemente, un $R^2$ próximo a 0 sigue siendo un mal resultado, pero es una mejoría. \n\nAhora, intentaremos hacer un ajuste de hiperparámetros para ver si podemos mejorar el modelo ligeramente: cambiando el kernel, o sus parámetros `C` y `epsilon`.\n\nPara realizar la búsqueda utilizaremos `RandomizedSearchCV`, de sklearn. Esta implementación nos permite automatizar relativamente la búsqueda de hiperparámetros: indicamos en un diccionario las distribuciones de las que queremos extraer los hiperparámetros, y `RandomizedSearchCV` va extrayendo hiperparámetros y evaluandolos por CV. \n\nLos parámetros se extraerán de las siguientes distribuciones:\n\n- `C` y `epsilon` se extraerán de una distribución uniforme [0, 4].\n- El kernel se escogerá aleatoriamente entre ['rbf', 'sigmoid' y 'poly']","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\n# distribuciones de las que se extraerán los hiperparámetros\ndistributions = {'svr__C':uniform(loc=0, scale=4),\n                 'svr__epsilon':uniform(loc=0, scale=4),\n                 'svr__kernel':['rbf', 'sigmoid', 'poly']}\nsvr_hyper = RandomizedSearchCV(svr, distributions)\n# iniciamos la busqueda de hiperparametros\nsearch = svr_hyper.fit(X, y)\n# Mejor conjunto de hiperparametros\nsearch.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Después de completar la búsqueda, podemos visualizar los mejores hiperpámetros y acceder al modelo final con dichos hiperparámetros:","metadata":{}},{"cell_type":"code","source":"best_svr = search.best_estimator_\n\nsummary(best_svr, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como vemos, no hay prácticamente mejora. \n\nA partir de estos resultados podemos afirmar casi con seguridad que el problema que intentamos resolver no tiene solución, ya que es demasiado aleatorio. \n\nInicialmente, se pensaba que según el tipo de desastre, cuando ocurrió, su duración... se conseguiría obtener algún tipo de relación con los daños totales. Sin embargo, parece que es demasiado aleatorio o no hay información suficiente como para extraer conclusiones.","metadata":{}},{"cell_type":"markdown","source":"Como último intento, probaremos a corregir el coste de los daños respecto a la inflación.\n\nEn la página de EM-DAT mencionan que los precios que aparecen en `Total Damages` son los que se reportaron en la época, sin tener en cuenta la **inflación**. Sin embargo, en el dataset hay una columna `CPI`, que contiene el denominado **Consumer Price Index**. El CPI representa un ratio estimado entre el dinero que cuesta una cesta de la compra en un determinado momento comparado con un año anterior de referencia (https://www.investopedia.com/terms/c/consumerpriceindex.asp). Por ejemplo, si el CPI en el año 2000 es de 10, significa que pagamos 10 veces más por el mismo producto que en un año de referencia anterior (no es necesario saber el año de referencia en nuestro caso)","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=emdat, x='Year', y='Total Damages (\\'000 US$)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver que, a medida que avanzan los años, los costes representados en dólares aumenta debido a la inflación. Esto impide que nosotros seamos capaces de comparar los daños **reales** de desastres que hayan ocurrido en epocas distintas.\n\nPor ejemplo, puede ocurrir que hayan ocurrido dos desastres similares en los años 1920, y en 2000, pero debido a la inflación, el coste representado en dólares sea mucho mayor en el desastre del 2000.\n\nEn resumen, al corregir la inflación, estudiaremos una medida del **coste real** del desastre.\n\nProbamos a corregir la inflación:","metadata":{}},{"cell_type":"code","source":"emdat['Total Damages Corregida'] = emdat['Total Damages (\\'000 US$)']/emdat['CPI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=emdat, x='Year', y='Total Damages Corregida')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parece que los costes una vez corregidos tienen más sentido, ya que podemos comparar desastres de diferentes épocas.\n\nProbaremos ahora varios modelos y analizaremos los resultados:","metadata":{}},{"cell_type":"code","source":"df_reg = emdat[emdat['Total Damages Corregida'].notna()]\n# añadimos la columna de Start Month\ndf_reg['Start Month'] = df_reg['Start Date'].dt.month\nX, y = df_reg.drop(['Total Damages Corregida'], axis=1), df_reg['Total Damages Corregida']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(reg, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(knn, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(dtree, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(svr, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notamos una mejoría en el modelo lineal. Esto tiene sentido, ya que de alguna manera, hemos modificado la variable respuesta para que se pueda ajustar mejor mediante una recta.\n\nSin embargo, los resultados de los demás modelos siguen siendo poco prometedores.","metadata":{}},{"cell_type":"markdown","source":"## Clasificación","metadata":{}},{"cell_type":"markdown","source":"Ahora, estudiaremos si existe alguna relación entre el subgrupo de desastre, `Disaster Subgroup`, y el resto de variables. Escogemos el subgrupo y no el grupo, porque de los 3 grupos presentes (Natural, Technological y Complex Disasters) hay muy pocas observaciones de este último. ","metadata":{}},{"cell_type":"code","source":"emdat['Disaster Group'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Intentaremos predecir la variable `Disaster Subgroup`. Hay un total de 8 clases:","metadata":{}},{"cell_type":"code","source":"emdat['Disaster Subgroup'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se puede ver que hay un desbalance de clases, es decir, hay más muestras de unas clases que de otras. Esto puede provocar ciertos problemas, sobretodo a la hora de evaluar el modelo. En estos casos, la accuracy no es una buena métrica, así que nos decantaremos por el **F1 score**. En el caso multiclase, hay varias formas de calcular el F1, en concreto, **micro** o **macro** average. \n\nLa elección de la métrica depende es una decisión que hemos de tomar. Generalmente, el micro average se suele escoger cuando se quiere maximizar el número de aciertos, sin tener en cuenta las clases. Sin embargo, el macro average tiene en cuenta la precisión de aciertos en cada clase. \n\nPor poner un ejemplo, si se quiere predecir una enfermedad que posee el 1% de la población, un clasificador que siempre predice que el individuo está sano, tendra un **F1 score calculado con micro average de 0.99**, cuando evidentemente, no es un buen clasificador para la tarea. En conclusión, como en el problema actual se desea que el clasificador funcione correctamente con todas las clases posibles, utilizaremos el F1 score calculado con **macro average**. (Para más información sobre el F1 score, acudir a la discusión https://datascience.stackexchange.com/questions/36862/macro-or-micro-average-for-imbalanced-class-problems).\n\nEvidentemente, no podemos escoger las variables 'Disaster Group', 'Disaster Type', 'Disaster Subtype' y 'Disaster Subsubtype' como variables respuesta porque tienen relación directa con la variable respuesta, así que las descartamos","metadata":{}},{"cell_type":"code","source":"# Lista de variables numericas que vamos a utilizar\nnumeric_features = ['Total Deaths', \n                    'Total Affected', \n                    'Duration',\n                    'Total Damages Corregida',\n                    'Year']\n# Lista de variables categóricas\ncategorical_features = ['Continent', \n                        'Entry Criteria', \n                        'Start Month']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = emdat[numeric_features+categorical_features], emdat['Disaster Subgroup']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.isna().sum(axis=0)/len(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que la variable `Total Damages Corregida` está ausente para un 80% de los valores. La cantidad de valores ausentes es demasiado alta como para utilizar técnicas de imputación, así que eliminaremos aquellos desastres que no contengan dicho valor.","metadata":{}},{"cell_type":"code","source":"mask = X['Total Damages Corregida'].notna()\nX, y = X[mask], y[mask]\nprint(\"El conjunto de entrenamiento contiene {} muestras\".format(len(mask)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.isna().sum(axis=0)/len(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver que después de la limpieza de datos, tan solo hay 6 desastres de tipo `Biological`, y tan sólo uno de tipo `Extra-terrestrial`. Se tratan de desastres muy poco comunes, así que los eliminaremos, ya que trabajar con dos clases tan desbalanceadas respecto al resto es prácticamente imposible, se necesitarían más muestras.","metadata":{}},{"cell_type":"code","source":"mask = (y != 'Extra-terrestrial') & (y != 'Biological')\n# descartamos las clases extra-terrestrial y biological\nX, y = X[mask], y[mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalmente, nuestro problema a resolver será una clasificación con **5 clases**: `Meteorological`, `Hydrological`, `Geophysical`, `Technological` y `Climatological`\n\nUtilizaremos como **baseline** el modelo `DummyClassifier`. La estrategia de este modelo para predecir consiste en elegir una clase **aleatoriamente** para cada muestra. Este no es un modelo real, pero sirve para tener un F1 score de referencia:","metadata":{}},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\n# Pipeline de procesado de variables numericas\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# Pipeline de procesado de variables categoricas\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n\nbaseline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('dummy', DummyClassifier(strategy='uniform'))])\ncv_result = cross_validate(baseline, X, y, cv=5, scoring=('f1_macro'), return_train_score=True)\ncv_result['test_score'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tal y como se esperaba, obtenemos un F1 score de aproximadamente $1/5=0.2$.\n\nAhora entrenaremos un modelo sencillo, como el modelo de regresión logística, y evaluaremos su F1 respecto al baseline:","metadata":{}},{"cell_type":"code","source":"def summary_clf(model, X, y):\n    \"\"\" Función auxiliar para evaluar distintos clasificadores \"\"\"\n    cv_result = cross_validate(model, X, y, cv=5, scoring=('f1_macro'), return_train_score=True)\n    print(\"=== MODELO {} ===\".format(model.steps[-1][0]))\n    print(\"F1 Train: {:.02f}\".format(cv_result['train_score'].mean()))\n    print(\"F1 Test: {:.02f}\".format(cv_result['test_score'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('log_clf', LogisticRegression(max_iter=1000))])\nsummary_clf(log, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede ver, el modelo de regresión logística supone una mejoría considerable respecto al modelo baseline aleatorio. En un primer intento, apareció la advertencia `ConvergenceWarning: lbfgs failed to converge`, que se solucionó aumentando el número de iteraciones del algoritmo con `max_iter=1000`","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('knn_clf', KNeighborsClassifier())])\nsummary_clf(knn_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtree_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('dtree_clf', DecisionTreeClassifier())])\nsummary_clf(dtree_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('svc_clf', SVC())])\nsummary_clf(svc_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En dos de los tres modelos probados (`KNeighborsClassifier` y `DecisionTreeClassifier`), podemos ver claramente que se están **sobreajustando** a los datos (overfit): el F1 en el conjunto de train es muy alto (el árbol de decisión tiene un F1=1), y el F1 en el conjunto de test es incluso peor que la regresión logística. En estos casos es necesario modificar los hiperparámetros por defecto para aumentar la regularización. Probaremos a modificar el `DecisionTreeClassifier` porque es el que menos tarda, y da resultados parecidos.\n\nRespecto al `SVC`, el resultado es parecido al de la regresión logística, a diferencia de que tarda mucho más tiempo en calcular. \n\nEn conclusión, por ahora vamos a intentar ajustar los hiperparámetros del `DecisionTreeClassifier`.\n\nEn la guía de usuario de sklearn, encontramos el siguiente fragmento:\n\n*\"\"Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem. \"\"*\n\nEn resumen, hemos de modificar `max_depth`, `min_samples_split` y `min_samples_leaf` para intentar reducir el error de generalización.","metadata":{}},{"cell_type":"code","source":"# distribuciones de las que se extraerán los hiperparámetros\ndistributions = {'dtree_clf__max_depth':[2, 3, 4, 5, 6 ,7],\n                 'dtree_clf__min_samples_split':[2, 3, 4, 5, 6, 7, 8],\n                 'dtree_clf__min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8]\n                }\ndtree_hyper = RandomizedSearchCV(dtree_clf, distributions, random_state=42)\n# iniciamos la busqueda de hiperparametros\nsearch = dtree_hyper.fit(X, y)\n# Mejor conjunto de hiperparametros\nsearch.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_clf(search.best_estimator_, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez hemos ajustado los hiperparámetros del árbol de decisión, obtenemos un F1 por validación cruzada ligeramente superior al obtenido con regresión logística. \n\nEstudiaremos este modelo en más profundidad, calculando la matriz de confusión:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\n# hacemos un split en conjunto de entrenamiento y test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Ajustamos el modelo\nbest_dtree = search.best_estimator_\nbest_dtree.fit(X_train, y_train)\n\n# predecimos las clases\ny_pred = best_dtree.predict(X_test)\n\n# calculamos la matriz de confusión\nplot_confusion_matrix(best_dtree, X_test, y_test, xticks_rotation=90, normalize='true')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En la matriz de confusión anterior podemos observar varios aspectos interesantes:\n\n- Lo que más llama la atención, es que no hay ninguna muestra que haya sido predecida con la clase `Technological`. Todas los desastres tecnológicos han sido clasificados como meteorológicos. Una posible explicación a esto es que, mientras que el tipo de desastres naturales puede tener relación con la época del año, la duración... Un desastre tecnológico es más aleatorio respecto a estas variables, y por tanto el modelo es incapaz de predecirlo correctamente.\n\n- La clase `Geophysical` se confunde bastante con la clase `Meteorological`. Puede que su relación con las variables explicativas sea similar, y como la clase meteorológica tiene un mayor número de muestras, esta ''absorbe'' a la clase más pequeña.\n\n- Un tercio de las muestras pertenecientes a la clase `Climatological`, mientras que otro tercio se clasifica erróneamente como `Hydrological`, y otro tercio como `Meteorological`. De nuevo, puede que estas clases tengan una estrecha relación.\n\nFinalmente, sklearn nos permite obtener la importancia de las diferentes variables a la hora de ajustar el modelo:","metadata":{}},{"cell_type":"code","source":"best_dtree.steps[-1][1].feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtenemos una lista de 30 elementos: los 5 primeros corresponden a las variables numéricas, los 6 siguientes corresponden a los contintentes, los 7 siguientes corresponden al `Entry criteria`, y los 12 últimos a los meses del año. Las variables más importantes son:\n\n1. `Duration`: 0.82\n2. `Total Deaths:` 0.07\n3. `Entry Criteria` = : 0.06\n4. `Total Damages Corregida`: 0.05\n\nSe puede apreciar claramente que **la duración del desastre está fuertemente relacionada con el tipo de desastre**. El número de muertos, el criterio de entrada, y los daños totales también parecen estar relacionadas, pero en mucha menor medida.","metadata":{}},{"cell_type":"markdown","source":"### Datos desbalanceados\n\nFinalmente, trataremos de aplicar una técnica para tratar con los datos desbalanceados e intentar mejorar el modelo.\n\nHay varias estrategias para tratar con datos desbalanceados, como el submuestreo de datos para que cada clase tenga el mismo número de muestras, sobremuestrear aplicando *bootstrap*, generar muestras sintéticas... (https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n\nEn este caso, `sklearn` implementa una funcionalidad que puede mejorar considerablemente el modelo. Cuando se crea un modelo, el argumento `class_weight='balanced'` asigna automáticamente un **peso a cada muestra en función del número de muestras de cada clase**. En otras palabras, a una muestra que pertenezca a una clase con muchas otras muestras se le asignará un peso menor en la función de coste, mientras que a las muestras de una clase más pequeña se le asignará un peso mayor. De esta manera, se incita al modelo a **no ignorar** las clases menos representadas.\n\nProbaremos esta estrategia con el modelo de regresión logística y el árbol de decisión:","metadata":{}},{"cell_type":"code","source":"log = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('log_clf', LogisticRegression(class_weight='balanced', max_iter=5000))])\nsummary_clf(log, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtree_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('dtree_clf', DecisionTreeClassifier(class_weight='balanced'))])\nsummary_clf(dtree_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribuciones de las que se extraerán los hiperparámetros\ndistributions = {'dtree_clf__max_depth':[2, 3, 4, 5, 6 ,7],\n                 'dtree_clf__min_samples_split':[2, 3, 4, 5, 6, 7, 8],\n                 'dtree_clf__min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8]\n                }\ndtree_hyper = RandomizedSearchCV(dtree_clf, distributions, random_state=42)\n# iniciamos la busqueda de hiperparametros\nsearch = dtree_hyper.fit(X, y)\n# Mejor conjunto de hiperparametros\nsearch.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_clf(search.best_estimator_, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el modelo de regresión logística no parece haber una mejoría notable, mientras que en el árbol de decisión, el F1 score ha aumentado de 0.31 a 0.34. \n\nEstudiemos más en profundidad el modelo calculando la matriz de confusión:","metadata":{}},{"cell_type":"code","source":"# hacemos un split en conjunto de entrenamiento y test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Ajustamos el modelo\nbest_dtree = search.best_estimator_\nbest_dtree.fit(X_train, y_train)\n\n# predecimos las clases\ny_pred = best_dtree.predict(X_test)\n\n# calculamos la matriz de confusión\nplot_confusion_matrix(best_dtree, X_test, y_test, xticks_rotation=90, normalize='true')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A primera vista, se pueden extraer las siguientes conclusiones:\n\n- La clase `Technological`, una de las menos representadas, ha pasado de que no se prediga bien **ninguna** muestra, a que se prediga correctamente un **70%** de las muestras. Esto es un claro síntoma de que la estrategia aplicada funciona para tratar con clases desbalanceadas.\n- Por otro lado, la clase más abundante, `Meteorological`, ha pasado de predecirse correctamente un 73% de las veces a un 36%.\n- A pesar del inconveniente anterior, en este nuevo modelo se aprecia claramente la diagonal principal de la matriz de confusión.\n\nEn función de las prioridades y del problema a tratar, se podría decir que el segundo modelo es mejor que el primero, o al revés. De nuevo, esto es una decisión que se ha de tomar en función de los objetivos. Por ejemplo, si nuestro objetivo fuera el de predecir correctamente la clase `Meteorological`, el segundo modelo sería peor que el primero.\n\nSin embargo, como deseamos que el modelo sea capaz de explicar todas las clases, se elegirá el segundo modelo.\n\nEstudiamos ahora la importancia de las variables utilizadas:","metadata":{}},{"cell_type":"code","source":"best_dtree.steps[-1][1].feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparando este resultado con el anterior, hacemos las siguientes reflexiones:\n\n- Las variables importantes en el anterior modelo, también lo son para este. Sin embargo, este modelo tiene en cuenta **más** variables que el modelo anterior.\n- Probablemente, las variables del análisis anterior eran las más importantes para predecir la clase `Meteorological`, que era la más grande. Al tener en cuenta las clases más pequeñas, entran en juego más variables. Por ejemplo, en el modelo anterior, la variable `Duration` estaba fuertemente correlacionada con la clase, probablemente porque todos los desastres meteorológicos tienen una duración común. Sin embargo, al tener en cuenta las demás clases, la importancia de esta variable se reduce (aunque siga siendo la más importante), y se tienen en cuenta variables como `Total Affected`, `Year`, `Total Damages Corregida` o `Total Deaths` (en orden de mayor a menor importancia).","metadata":{}},{"cell_type":"markdown","source":"# Parte 2: Ensembles, clustering y reglas de asociación","metadata":{}},{"cell_type":"markdown","source":"## Ensembles\n\nAhora, intentaremos mejorar el resultado anterior mediante la utilización de ensembles. Concretamente, probaremos con un `RandomForestClassifier`:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('rf_clf', RandomForestClassifier(class_weight='balanced'))])\nsummary_clf(rf_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A primera vista, podemos comprobar que el modelo `RandomForestClassifier` con los hiperparámetros por defecto supera al `DecisionTreeClassifier` con los parámetros ajustados, pasando de un F1 score de 0.34 a 0.41. Por tanto, es de esperar que, con un ajuste adecuado de hiperparámetros, el modelo de random forest suponga una mejoría todavía más notable respecto al árbol de decisión.\n\nAdemás, es evidente que el modelo se sobreajusta (overfits) a los datos, ya que predice correctamente **todos** los datos de entrenamiento. Este es otro motivo por el cual se han de ajustar los hiperpámetros, para intentar regularizar el modelo y disminuir su error de generalización.\n\nEn la página de `sklearn` (https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) recomiendan una serie de pautas a la hora de ajustar los hiperparámetros:\n\n- Los principales hiperparámetros a ajustar son `n_estimators` (número de estimadores) y `max_features` (tamaño máximo del subconjunto de variables a tener en cuenta)\n- Para la regresión, se recomienda utilizar todas las variables (`max_features=None`), mientras que para clasificación se recomienda utilizar la raíz cuadrada del número total (`max_features='sqrt'`)\n\nEn el siguiente enlace (https://stackoverflow.com/questions/20463281/how-do-i-solve-overfitting-in-random-forest-of-python-sklearn) también se discute cómo reducir el overfitting. Concretamente, se recomienda ajustar los siguientes parámetros:\n\n- `n_estimators`: En general, cuando mayor sea el número de árboles utilizado, menor será la probabilidad de overfitting. Es importante recalcar que el tiempo de entrenamiento aumenta considerablemente en función del número de estimadores.\n\n- `max_features`: Reducir el número de características utilizado por cada árbol suele disminuir el overfitting. Aunque en la página de sklearn recomiendan tomar la raíz cuadrada del número total de características, se probará con otros valores.\n\n- `max_depth`: Cuanto menor sea su valor, más simples serán los modelos, y por tanto será menos probable que haya overfitting.\n\n- `min_samples_leaf`: Este parámetro indica el mínimo número de muestras que ha de tener un nodo para dividirse. Se probará a aumentar el valor de este parámetro.","metadata":{}},{"cell_type":"code","source":"rf_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('rf_clf', RandomForestClassifier(class_weight='balanced',\n                                                            n_estimators= 200,\n                                                            max_features= 'sqrt',\n                                                            max_depth= 40,\n                                                            min_samples_leaf= 4))])\nsummary_clf(rf_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ajustando los parámetros se consigue aumentar el F1 de 0.43 a 0.45, aproximadamente. Respecto al modelo de `DecisionTreeClassifier` utilizado anteriormente, el uso del modelo de ensemble `RandomForestClassifier` supone una mejora de aproximadamente una décima en el F1 score.\n\nEl modelo de Random Forest pertenece a los llamados **Bagging** ensembles, en el que se entrenan una serie de modelos débiles a partir de un subconjunto de características en un dataset obtenido mediante bootstrap.\n\nAhora utilizaremos un modelo que pertenece a otro grupo de ensembles llamados **Boosting**. La idea detrás de los modelos de ensemble boosting es la de **actualizar los pesos de cada muestra en función de las predicciones de cada modelo débil**. En otras palabras, se darán mas peso a las muestras clasificadas incorrectamente por un modelo débil, de manera que el siguiente modelo intente predecir correctamente dichas clases.\n\nConcretamente, utilizaremos un modelo **Adaboost**:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nadaboost_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('ada_clf', AdaBoostClassifier())])\nsummary_clf(adaboost_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con los parámetros por defecto se obtiene un resultado incluso peor que con un modelo de regresión logística. Utilizaremos como estimador base un `DecisionTreeClassifier`, basándonos en los hiperparámetros que han funcionado correctamente para pruebas anteriores:","metadata":{}},{"cell_type":"code","source":"adaboost_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('ada_clf', AdaBoostClassifier(\n                               n_estimators=200,\n                               base_estimator=DecisionTreeClassifier(class_weight='balanced',\n                                                                    min_samples_split=7,\n                                                                    min_samples_leaf=4,\n                                                                    max_depth=10)))])\nsummary_clf(adaboost_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se consigue mejorar el resultado, pero se obtienen resultados similares a los obtenidos con un modelo más simple, como el de regresión logística.\n\nFinalmente, probaremos un último modelo perteneciente al grupo de los **stacking** ensembles. La idea básica consiste en utilizar las predicciones de varios modelos como **entrada a un modelo final**, que tomará la decisión en función de estas predicciones.\n\nProbaremos a utilizar como estimatores base un modelo de regresión logística y el árbol de decisión obtenidos anteriormente. Como estimador final, utilizaremos otro modelo de regresión logística:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nestimators = [log.steps[-1], best_dtree.steps[-1]]\n\nstacking_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('stacking_clf', StackingClassifier(estimators,\n                                                    final_estimator=LogisticRegression(max_iter=4000)))])\nsummary_clf(stacking_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El ensemble formado por ambos modelos no parece mejorar el F1 score. Por último, probaremos a utilizar el modelo de regresión logística y el de random forest:","metadata":{}},{"cell_type":"code","source":"estimators = [log.steps[-1], rf_clf.steps[-1]]\n\nstacking_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('stacking_clf', StackingClassifier(estimators,\n                                                    final_estimator=LogisticRegression(max_iter=4000)))])\nsummary_clf(stacking_clf, X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"De nuevo, el ensemble funciona incluso peor que el modelo de random forest individual. \n\nPor tanto, podemos concluir que, de todos los modelos probados, el de random forest es el que mejor resultados proporciona. Resulta conveniente representar la matriz de confusión para analizar los resultados con más detalle:","metadata":{}},{"cell_type":"code","source":"# hacemos un split en conjunto de entrenamiento y test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Ajustamos el modelo\nrf_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('rf_clf', RandomForestClassifier(class_weight='balanced',\n                                                            n_estimators= 200,\n                                                            max_features= 'sqrt',\n                                                            max_depth= 40,\n                                                            min_samples_leaf= 4))])\nrf_clf.fit(X_train, y_train)\n\n# predecimos las clases\ny_pred = rf_clf.predict(X_test)\n\n# calculamos la matriz de confusión\nplot_confusion_matrix(rf_clf, X_test, y_test, xticks_rotation=90, normalize='true')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparando con los resultados obtenidos con el `DecisionTreeClassifier`, se puede observar una mejoría considerable en las predicciones; sobretodo en la clasificación de la clase `Meteorological`: se ha pasado de predecir correctamente un 36% de las muestras a  un **58%**.\n\n## Clustering","metadata":{}},{"cell_type":"markdown","source":"Para la parte de clustering, se intentará predecir a qué clase pertenece cada campeón de League of Legends en función de sus estadísticas. League of Legends es un juego multijugador competitivo, en el que típicamente se enfrentan dos grupos de 5 jugadores cada uno. Cada jugador elegirá uno de los 146 campeones (número de campeones en el parche 9.22). Cada campeón tiene un conjunto de habilidades y estadísticas que lo caracterizan.","metadata":{}},{"cell_type":"markdown","source":"Los campeones se pueden separar en 7 clases en función del rol al que mejor se adaptan (https://leagueoflegends.fandom.com/wiki/Champion_classes):\n\n- **Controller** (Controlador): Los controladores asisten a sus aliados gracias a sus habilidades de utilidad. \n\n- **Fighter/Bruiser** (Luchador): Los luchadores son campeones caracterizados por tener un balance entre resistir daño e infligirlo.\n\n- **Mage** (Mago): Los magos se caracterizan por infligir una gran cantidad de daño mágico, además de tener ciertas habilidades de control. Suelen tener vida reducida.\n\n- **Marksman** (Francotirador): Caracterizados por infligir daño físico a distancia, suelen tener vida reducida.\n\n- **Slayer** (Asesino): Suelen infligir una gran cantidad de daño a corto alcance y tener mucha movilidad, tienen vida reducida.\n\n- **Tank** (Tanque): Caracterizados por su gran capacidad de resistir daño y sus efectos de control.\n\n- **Especialista**: Esta clase engloba a todos los campeones que no encajan en las clases anteriores.\n\nEn (https://leagueoflegends.fandom.com/wiki/List_of_championshttps://leagueoflegends.fandom.com/wiki/List_of_champions) se puede consultar a qué clase pertenece cada campeón","metadata":{}},{"cell_type":"code","source":"df_lol = pd.read_csv('../input/league-of-legends-champion-stats-922/champions-stats-09-22.csv')\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n    display(df_lol)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El dataset contiene las estadísticas de cada uno de los 150 campeones que existen en el parche 10.02. A continuación se describen brevemente estas estadísticas:\n\n- `name`: Nombre del campeón.\n- `hp`: Puntos de vida iniciales.\n- `hp+`: Estadística de crecimiento de la vida en función del nivel. Algunas estadísticas aumentan en función del nivel, siguiendo la fórmula $Statistic = b + g(n-1)(0.7025+0.0175(n-1))$, donde b sería el valor inicial (en el caso de la vida, `hp`) y g sería la estadística de crecimiento (en este caso, `hp+`).\n- `hp5`/`hp5+`: Regeneración de vida cada 5 segundos\n- `mp`/`mp+`: Puntos de maná.\n- `mp5`/`mp5+`: Regeneración de maná cada 5 segundos.\n- `AD`/`AD+`: Daño de ataque (físico)\n- `AS`/`AS+`: Velocidad de ataque\n- `AR`/`AR+`: Armadura (resistencia física)\n- `MR`/`MR+`: Resistencia mágica\n- `MS`: Velocidad de movimiento.\n- `rng`: Rango de ataque.\n\nA partir de estos stats, intentaremos agrupar los campeones en las 7 clases definidas. Para ello, aplicaremos el algoritmo de Mini Batch K-means, recomendado por https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html:","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\n# Tomamos las estadísticas de cada campeón\nX = df_lol.drop(columns={'name'})\n# Estandarizamos las características para que tengan media cero y std 1\nX = StandardScaler().fit_transform(X)\n# Creamos el modelo K-means y ajustamos los datos, indicando que queremos \n# dividir los datos en 7 clusters (o clases)\nkmeans = MiniBatchKMeans(n_clusters=7, random_state=42).fit(X)\n# Obtenemos las etiquetas obtenidas\ndf_lol['Label'] = kmeans.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtenemos la clasificación de los campeones:","metadata":{}},{"cell_type":"code","source":"for label in np.unique(kmeans.labels_):\n    champions = df_lol[df_lol['Label'] == label]['name']\n    print(\"CLASE {}\".format(label))\n    print(\"Campeones: {}\\n\\n\".format(champions.values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este primer intento de aplicar clustering, se pueden observar ciertas agrupaciones de campeones interesantes:\n\n- La clase 1 parece representar a los **marksman**, por presencia de campeones como Ashe, Caitlyn, Draven, Jhin, Jinx, KaiSa, Kalista...\n- De la misma manera, la clase 2 parece estar asociado a los **magos**.\n- La clase 3 solamente contiene asesinos.\n\nSin embargo, la clase 5 es demasiado generalista, y engloba tanto a tanques, como magos, como luchadores...\n\nProbablemente se podrían mejorar los resultados mediante la **eliminación de un cluster** asociado a la clase especialista, ya que esta clase está formada por campeones con stats muy diversos.","metadata":{}},{"cell_type":"code","source":"# Tomamos las estadísticas de cada campeón\nX = df_lol.drop(columns={'name'})\n# Estandarizamos las características para que tengan media cero y std 1\nX = StandardScaler().fit_transform(X)\n# Creamos el modelo K-means y ajustamos los datos, indicando que queremos \n# dividir los datos en 7 clusters (o clases)\nkmeans = MiniBatchKMeans(n_clusters=6, random_state=42).fit(X)\n# Obtenemos las etiquetas obtenidas\ndf_lol['Label'] = kmeans.labels_\nfor label in np.unique(kmeans.labels_):\n    champions = df_lol[df_lol['Label'] == label]['name']\n    print(\"CLASE {}\".format(label))\n    print(\"Campeones: {}\\n\\n\".format(champions.values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tras analizar los distintos clusters, podemos asociar las siguientes clases a cada cluster:\n\n- Clase 0: Mages\n- Clase 1: Fighters\n- Clase 2: Controllers\n- Clase 3: Marksmen\n- Clase 4: Slayers\n- Clase 5: Tanks\n\nSin embargo, el clustering no es perfecto, y hay campeones que se han agrupado incorrectamente. A continuación presentamos algunas observaciones:\n\n- La clase de los **slayers** (asesinos) está formada completamente por asesinos (excepto Shen); el resto de asesinos ha sido agrupado incorrectamente en otras clases, como Katarina en la clase de los tanks, o fizz en la de los controllers.\n- Parece que las diferencias entre la clase tank y fighter son muy leves, y hay cierta confusión a la hora de agrupar estas dos clases.\n- Los clusters que mejor han agrupado son aquellos asociados a la clase **mage** y **marksman**. Este resultado era de esperar, ya que los campeones de estas clases tienen stats muy diferenciados respecto al resto: los magos se caracterizan por tener mucho maná y rango, y los marksman por tener mucho daño de ataque.","metadata":{}},{"cell_type":"markdown","source":"# Conclusiones\n\nA lo largo de esta práctica, nos hemos familiarizado con el uso de modelos para los problemas de regresión y clasificación (aprendizaje supervisado) y clustering (aprendizaje no supervisado.\n\nAdemás, ha servido como una primera toma de contacto con la aplicación de técnicas de aprendizaje automático en un dataset real. Es decir, mientras que en muchas ocasiones se utilizan datasets ya preparados con fines didácticos, aquí se ha utilizado un dataset que se ha tenido que preprocesar para poder aplicar los modelos.\n\nRespecto a la parte de **regresión**, se han intentado predecir los daños causados por cada desastre. Sin embargo, los modelos aplicados proporcionaban un $R^2$ negativo o muy cercano a cero. Esto podría deberse por dos razones:\n\n1. La variable que se intentaba predecir depende de otras variables que no se han tenido en cuenta.\n2. El fenómeno es prácticamente aleatorio, y por tanto es un problema incompatible con técnicas de machine learning.\n\nEn este caso, es más probable que sea por el segundo motivo, ya que tanto los desastres como sus efectos suelen ser algo impredecible, y por tanto, el coste ocasionado también.\n\nRespecto a la parte de **clasificación**, se intentó aplicar una serie de modelos para intentar encontrar la relación entre una serie de variables (duración del desastre, número de afectados...) con el tipo de desastre (Meteorológico, Tecnológico...). En este caso, si que se consiguió obtener un modelo (concretamente, un árbol de decisión) que conseguía captar dicha relación, obteniendo un F1 score de 0.34 en el conjunto de test. Se discutió también la elección de la métrica. \n\nTambién es importante recalcar que, para obtener dicho F1 score se tuvo que aplicar una técnica para tratar con datos balanceados, ya que habían clases con un número elevado de muestras respecto a las demás. Concretamente, se modificó el peso de cada muestra durante el entrenamiento en función del número de muestras por clase, mediante el argumento `class_weight='balanced'`. \n\nA continuación, se intentó mejorar el resultado de la parte de clasificación mediante modelos de **ensemble**. En concreto, el clasificador de **Random Forest** proporcionaba un F1 score de 0.45 en el conjunto de test.\n\nFinalmente, para la parte de clustering se utilizó un dataset diferente, compuesto por las estadísticas de cada campeón del juego League of Legends, con el objetivo de agrupar los campeones en clases diferentes según su rol o características. Como el número de muestras era reducido, se aplicó el algoritmo de **mini batch k-means**. En un primer intento, se agruparon los campeones en 7 clases. Sin embargo, los resultados no fueron muy buenos, probablemente porque una de las clases (especialist) es difícil de agrupar en función de las estadísticas de cada campeón. Tras reducir el número de clusters a 6, se obtuvieron resultados considerablemente mejores, y se consiguieron diferenciar las diferentes clases con bastante claridad.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}