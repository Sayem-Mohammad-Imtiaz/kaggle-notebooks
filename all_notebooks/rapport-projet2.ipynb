{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install sparkmagic\n!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import *\nfrom pyspark.sql import SparkSession\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler, OneHotEncoder\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.sql.functions import countDistinct, col\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring dataset"},{"metadata":{},"cell_type":"markdown","source":"World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.csv('../input/cardiovascular-study-dataset-predict-heart-disea/train.csv', inferSchema=True, header=True)\ndf.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Input variables : id, age, education, sex, is_smoking, cigsPerDay, BPMeds, prevalentStroke, prevalentHyp, diabetes, totChol, sysBP, diaBP, BMI, heartRate, glucose.\n\nOoutput variable : TenYearCHD"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.DataFrame(df.take(5), columns=df.columns).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary statistics for numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = [t[0] for t in df.dtypes if (t[1] == 'int' or t[1] == 'double')]\ndf.select(numeric_features).describe().toPandas().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlations between independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nnumeric_data = df.select(numeric_features).toPandas()\naxs = scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It’s obvious that there aren’t highly correlated numeric variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.select('age', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'sysBP', 'diaBP', 'sex', 'is_smoking', 'TenYearCHD')\ncols = df.columns\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data for Machine learning"},{"metadata":{},"cell_type":"markdown","source":"The process includes Category Indexing, One-Hot Encoding and VectorAssembler — a feature transformer that merges multiple columns into a vector column."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategoricalColumns = ['sex', 'is_smoking']\nstages = []\nfor categoricalCol in categoricalColumns:\n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    stages += [stringIndexer, encoder]\nlabel_stringIdx = StringIndexer(inputCol = 'TenYearCHD', outputCol = 'label')\nstages += [label_stringIdx]\nnumericCols = ['age', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'sysBP', 'diaBP']\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{},"cell_type":"markdown","source":"We use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipeline’s stages are specified as an ordered array."},{"metadata":{"trusted":true},"cell_type":"code","source":"\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features'] + cols\ndf = df.select(selectedCols)\ndf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df.take(5), columns=df.columns).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train valid split"},{"metadata":{},"cell_type":"markdown","source":"Randomly split data into train and test sets, and set seed for reproducibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid = df.randomSplit([0.8, 0.2], seed = 7)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Validation Dataset Count: \" + str(valid.count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingSummary = lrModel.summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = lrModel.transform(valid)\npredictions.select('age', 'sex', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Validation Area Under ROC', evaluator.evaluate(predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\ndtModel = dt.fit(train)\npredictions = dtModel.transform(valid)\npredictions.select('age', 'sex', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator()\nprint(\"Validation Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nrfModel = rf.fit(train)\npredictions = rfModel.transform(valid)\npredictions.select('age', 'sex', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator()\nprint(\"Validation Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient-Boosted Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\npredictions = gbtModel.transform(valid)\npredictions.select('age', 'sex', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator()\nprint(\"Validation Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient-Boosted Tree achieved the best results, we will try tuning this model with the ParamGridBuilder and the CrossValidator. Before that we can use explainParams() to print a list of all params and their definitions to understand what params available for tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gbt.explainParams())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 60])\n             .addGrid(gbt.maxIter, [10, 20])\n             .build())\ncv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\ncvModel = cv.fit(train)\npredictions = cvModel.transform(valid)\nevaluator.evaluate(predictions)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}