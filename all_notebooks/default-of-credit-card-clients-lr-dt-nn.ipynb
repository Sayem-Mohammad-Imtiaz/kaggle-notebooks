{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThis notebook was created for analysis and prediction making of the *Default of credit card clients Data Set* from UCI Machine Learning Library. The data set can be accessed separately from the UCI Machine Learning Repository page, [here](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)."},{"metadata":{},"cell_type":"markdown","source":"## Relevant Papers\n\nIn their paper \"The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. (Yeh I. C. & Lien C. H.,2009)\", which can be found [here](https://bradzzz.gitbooks.io/ga-dsi-seattle/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf), Yeh I. C. & Lien C. H. review six data mining techniques (discriminant analysis, logistic regression, Bayesclassifier, nearest neighbor, artificial neural networks, and classification trees) and their applications on credit scoring. Then, using the real cardholders’ credit risk data in Tai-wan, they compare the classification accuracy among them."},{"metadata":{},"cell_type":"markdown","source":"## Attribute Information\n\nBelow there are the description of the attributes that will be used in our model for better understanding of the data:\n\n- `LIMIT_BAL`: Amount of the given credit (NT dollar). It includes both the individual consumer credit and his/her family (supplementary) credit.\n- `SEX`: Gender (1 = male; 2 = female).\n- `EDUCATION`: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n- `MARRIAGE`: Marital status (1 = married; 2 = single; 3 = others).\n- `AGE`: Age (year).\n- `PAY_1`: the repayment status in September, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_2`: the repayment status in August, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_3`: the repayment status in July, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_4`: the repayment status in June, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_5`: the repayment status in May, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `PAY_6`: the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n- `BILL_AMT1`: Amount of bill statement (NT dollar). Amount of bill statement in September, 2005.\n- `BILL_AMT2`: Amount of bill statement (NT dollar). Amount of bill statement in August, 2005.\n- `BILL_AMT3`: Amount of bill statement (NT dollar). Amount of bill statement in July, 2005.\n- `BILL_AMT4`: Amount of bill statement (NT dollar). Amount of bill statement in June, 2005.\n- `BILL_AMT5`: Amount of bill statement (NT dollar). Amount of bill statement in May, 2005.\n- `BILL_AMT6`: Amount of bill statement (NT dollar). Amount of bill statement in April, 2005.\n- `PAY_AMT1`: Amount of previous payment (NT dollar). Amount paid in September, 2005.\n- `PAY_AMT2`: Amount of previous payment (NT dollar). Amount paid in August, 2005.\n- `PAY_AMT3`: Amount of previous payment (NT dollar). Amount paid in July, 2005.\n- `PAY_AMT4`: Amount of previous payment (NT dollar). Amount paid in June, 2005.\n- `PAY_AMT5`: Amount of previous payment (NT dollar). Amount paid in May, 2005.\n- `PAY_AMT6`: Amount of previous payment (NT dollar). Amount paid in June, 2005.\n- `dpnm`: Default payment next month.(Yes = 1, No = 0)"},{"metadata":{},"cell_type":"markdown","source":"## Models\n\nWe will create 3 models in order to make predictions and compare them with the original paper. These models are:\n- Logistic Regression\n- Decision tree\n- Neural Network\n\nAfter the initial predictions, each model will be \"optimized\" by `GridSearchCV` estimator, which will search for the best set of hyperparameters for every model.  "},{"metadata":{},"cell_type":"markdown","source":"## Metrics\n\nIn order to be consistent with the original paper and have the same base for our results, we will use the same metrics. These metrics are: Accuracy and F1 score. Accuracy is $\\frac{Number of correct predictions}{Number of samples}$. When the dataset is imbalanced, accuracy may not be sufficient, because simply predicting all samples to be the major class can still get high accuracy. In such situation, a good metrics to use is f1 score. F1-score is calculated by $\\frac{2*precision*recall}{precision+recall}$, where precision is $\\frac{True Positives}{True Positives+False Positives}$ and recall is $\\frac{True Positives}{True Positives+False Negatives}$.\nPrecision measures a model’s ability to correctly identify positive samples and recall measures the proportion of positive samples that are identified. F1-score ranges from 0 (cannot make true positive predictio) to 1 (being correct in all predictions). \n\nIn addition to the above, we will use the cross-validation accuracy that `GridSeachCV` provides, while we compute confusion matrix for each model as well the Area Under the Curve (AUC) and plot the ROC curves. A typical ROC curve has False Positive Rate (FPR) on the X-axis and True Positive Rate (TPR) on the Y-axis. The area covered by the curve is the area between the line (ROC) and the axis. This area covered is AUC. The bigger the area covered, the better the machine learning models is at distinguishing the given classes. Ideal value for AUC is 1."},{"metadata":{},"cell_type":"markdown","source":"## Goal \n\nUsing the models we created, we will try to predict the class value of `dpnm` column with better scores (accuracy and f1) than the scores presented in the original paper."},{"metadata":{},"cell_type":"markdown","source":"## Import libraries/packages "},{"metadata":{"trusted":true},"cell_type":"code","source":"### General libraries ###\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz \nfrom graphviz import Source\nfrom IPython.display import SVG\n\n##################################\n\n### ML Models ###\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\n##################################\n\n### Metrics ###\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Load and clean the data\n\nIn this section we will load the data from the csv file and check for any \"impurities\", such as null values or duplicate rows. If any of these will appear, we will remove them from the data set. We will also plot the correlations of the class column with all the other columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data.\ndata=pd.read_csv('../input/default of credit card clients.csv')\n\n# Information\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the `ID` column is for indexing purposes only, we remove it from the data set."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Drop \"ID\" column.\ndata=data.drop(['ID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we check for duplicate rows. If any, we remove them from the data set, since they provide only reduntant information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for duplicate rows.\nprint(f\"There are {data.duplicated().sum()} duplicate rows in the data set.\")\n\n# Remove duplicate rows.\ndata=data.drop_duplicates()\nprint(\"The duplicate rows were removed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also check for null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values.\nprint(f\"There are {data.isna().any().sum()} cells with null values in the data set.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the plot of the correlation matrix for the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(data.corr(),annot=True, cmap='rainbow',linewidth=0.5, fmt='.2f');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Pre-processing\n\nIn this part we prepare our data for our models. This means that we choose the columns that will be our independed variables and which column the class that we want to predict. Once we are done with that, we split our data into train and test sets and perfom a standardization upon them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distinguish attribute columns and class column.\nX=data[data.columns[:-1]]\ny=data['dpnm']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split to train and test sets. \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3: Modeling\n\nIn this section we build and try 3 models:\n - Logistic Regression\n - Decision tree\n - Neural network\n\nEach model will be trained and make a prediction for the test set. Accuracy, f1 score, confusion matrix and ROC will be calculated for each model. Then we will use the `GridSearchCV` module to tune our models and search for the best hyperparameters in order to increase the accuracy of each model."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a Logistic Regression estimator.\nlogreg=LogisticRegression(multi_class='auto', random_state=25, n_jobs=-1)\n\n# Train the estimator.\nlogreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\nlog_pred=logreg.predict(X_test)\n\n#CV score\nlogreg_cv=cross_val_score(logreg, X_train, y_train, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % logreg.score(X_test, y_test))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.3f' % logreg_cv)\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, log_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, log_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, log_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nlogreg_probs = logreg.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\nlogreg_probs = logreg_probs[:, 1]\n\n# Compute the AUC Score.\nauc_logreg = roc_auc_score(y_test, logreg_probs)\nprint('AUC: %.2f' % auc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Logistic Regression.\nlogreg_matrix = confusion_matrix(y_test,log_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(logreg_matrix, annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Logistic Regression');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search for Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters to be checked.\nparameters = {'C':[0.0001, 0.001, 0.01, 1, 0.1, 10, 100, 1000], 'penalty':['none','l2'] ,\n              'solver':['lbfgs','sag','saga','newton-cg']}\n\n# Logistic Regression estimator.\ndefault_logreg=LogisticRegression(multi_class='auto', random_state=25, n_jobs=-1)\n\n# GridSearchCV estimator.\ngs_logreg = GridSearchCV(default_logreg, parameters, cv=10, n_jobs=-1, verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_logreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_log_pred=gs_logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Grid search Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best Logistic Regression Parameters: {}\".format(gs_logreg.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_logreg.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_logreg.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_log_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_log_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_log_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print confusion matrix for Logistic regression.\ngs_logreg_matrix = confusion_matrix(y_test,gs_log_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(gs_logreg_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Logistic Regression');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_logreg_probs = gs_logreg.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_logreg_probs = gs_logreg_probs[:, 1]\n\n# Compute the AUC Score.\ngs_logreg_auc = roc_auc_score(y_test, gs_logreg_probs)\nprint('AUC: %.2f' % gs_logreg_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC curves.\nlogreg_fpr, logreg_tpr, logreg_thresholds = roc_curve(y_test, logreg_probs)\ngs_logreg_fpr, gs_logreg_tpr, gs_logreg_thresholds = roc_curve(y_test, gs_logreg_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(logreg_fpr, logreg_tpr, color='black', label='LogReg ROC (AUC= %0.2f)'% auc_logreg)\nplt.plot(gs_logreg_fpr, gs_logreg_tpr, color='red', linestyle='--',label='GridSearch+LogReg ROC (AUC= %0.2f)'% gs_logreg_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a decision tree estimator.\ntr = tree.DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=25)\n\n# Train the estimator.\ntr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the tree.\ndot_data = tree.export_graphviz(tr, out_file=None, feature_names=X.columns, filled=True, rounded=True, special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the tree.\nfig=plt.figure(figsize=(23,15))\ntree.plot_tree(tr.fit(X_train, y_train),feature_names=X.columns,filled=True,rounded=True,fontsize=16);\nplt.title('Decision Tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the tree in a simplified version.\nr = export_text(tr, feature_names=X.columns.tolist())\nprint(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\ntr_pred=tr.predict(X_test)\n\n# CV score\ntr_cv=cross_val_score(tr, X_train, y_train, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % tr.score(X_test, y_test))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.3f' % tr_cv)\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, tr_pred))\n\n# Recall\nprint('Precision: %.3f' % recall_score(y_test, tr_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, tr_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Decision tree.\ntr_matrix = confusion_matrix(y_test,tr_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Decision tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict propabilities for the test data.\ntr_probs = tr.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ntr_probs = tr_probs[:, 1]\n\n# Compute the AUC Score.\nauc_tr = roc_auc_score(y_test, tr_probs)\nprint('AUC: %.2f' % auc_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search for Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters to be checked.\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n             }\n\n# MLP estimator.\ndefault_tr = tree.DecisionTreeClassifier(random_state=25)\n\n# GridSearchCV estimator.\ngs_tree = GridSearchCV(default_tr, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_tree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_tree_pred=gs_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Grid Search Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best Decision tree Parameters: {}\".format(gs_tree.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_tree.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_tree.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_tree_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_tree_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_tree_pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Decision tree.\ngs_tr_matrix = confusion_matrix(y_test,gs_tree_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(gs_tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Decision tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_tree_probs = gs_tree.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_tree_probs = gs_tree_probs[:, 1]\n\n# Compute the AUC Score.\ngs_tree_auc = roc_auc_score(y_test, gs_tree_probs)\nprint('AUC: %.2f' % gs_tree_auc)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Get the ROC Curves.\ngs_tr_fpr, gs_tr_tpr, gs_tr_thresholds = roc_curve(y_test, gs_tree_probs)\ntr_fpr, tr_tpr, tr_thresholds = roc_curve(y_test, tr_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(tr_fpr, tr_tpr, color='red', label='Decision tree ROC (AUC= %0.2f)'% auc_tr)\nplt.plot(gs_tr_fpr, gs_tr_tpr, color='green', label='GridSearch+Decision tree ROC (AUC= %0.2f)'% gs_tree_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a Multi-layer Perceptron classifier.\nmlp = MLPClassifier(hidden_layer_sizes=(12,5),max_iter=1000, random_state=25,shuffle=True, verbose=False)\n\n# Train the classifier.\nmlp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\nmlp_pred = mlp.predict(X_test)\n\n# CV score\nmlp_cv=cross_val_score(mlp, X_train, y_train, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Neural network (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % mlp.score(X_test, y_test))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.3f' % mlp_cv)\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, mlp_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, mlp_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Multi-layer Perceptron.\nmatrix = confusion_matrix(y_test,mlp_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for MLP');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nmlp_probs = mlp.predict_proba(X_test)\n\n# Keep probabilities of the positive class only.\nmlp_probs = mlp_probs[:, 1]\n\n# Compute the AUC Score.\nauc_mlp = roc_auc_score(y_test, mlp_probs)\nprint('AUC: %.2f' % auc_mlp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search for Neural network"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Hyperparameters to be checked.\nparameters = {'activation':['logistic','relu'],\n              'solver': ['lbfgs','adam','sgd'],\n              'alpha':10.0 ** -np.arange(1,3),\n              'hidden_layer_sizes':[(23),(12,5),(12,5,2),(3,1),(5)]}\n\n# Decision tree estimator.\ndefault_mlp = MLPClassifier(random_state=42)\n\n# GridSearchCV estimator.\ngs_mlp = GridSearchCV(default_mlp, parameters, cv=10, n_jobs=-1,verbose=10)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_mlp.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_mlp_pred=gs_mlp.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Grid Search MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best MLP Parameters: {}\".format(gs_mlp.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_mlp.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_mlp.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_mlp_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_mlp_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for GridSearchCV Multi-layer Perceptron.\nmatrix = confusion_matrix(y_test,gs_mlp_pred)\nplt.figure(figsize=(8,8))\nsns.heatmap(matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV MLP');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_mlp_probs = gs_mlp.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_mlp_probs = gs_mlp_probs[:, 1]\n\n# Compute the AUC Score.\ngs_mlp_auc = roc_auc_score(y_test, gs_mlp_probs)\nprint('AUC: %.2f' % gs_mlp_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC curves.\ngs_mlp_fpr, gs_mlp_tpr,gs_mlp_thresholds = roc_curve(y_test, gs_mlp_probs)\nmlp_fpr, mlp_tpr, mlp_thresholds = roc_curve(y_test, mlp_probs)\n\n# Plot the ROC curve.\nplt.figure(figsize=(8,8))\nplt.plot(mlp_fpr, mlp_tpr, color='red', label='MLP ROC (AUC= %0.2f)'% auc_mlp)\nplt.plot(gs_mlp_fpr, gs_mlp_tpr, color='green', label='GridSearch+MLP ROC (AUC= %0.2f)'% gs_mlp_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=['Accuracy', 'CV accuracy', 'Precision','Recall','F1','ROC AUC']\n\n# Plot metrics.\nfig = go.Figure(data=[\n    go.Bar(name='Logistic Regression', x=metrics,\n           y=[logreg.score(X_test, y_test),logreg_cv,precision_score(y_test, log_pred),recall_score(y_test, log_pred),f1_score(y_test, log_pred),auc_logreg]),\n    go.Bar(name='Decision tree', x=metrics,\n           y=[tr.score(X_test, y_test),tr_cv,precision_score(y_test, tr_pred),recall_score(y_test, tr_pred),f1_score(y_test, tr_pred),auc_tr]),\n    go.Bar(name='Neural Network', x=metrics,\n           y=[mlp.score(X_test, y_test),mlp_cv,precision_score(y_test, mlp_pred),recall_score(y_test, mlp_pred),f1_score(y_test, mlp_pred),auc_mlp]),\n    go.Bar(name='GridSearchCV+Logistic Regression',\n           x=metrics, y=[gs_logreg.score(X_test,y_test),gs_logreg.best_score_,precision_score(y_test, gs_log_pred),recall_score(y_test, gs_log_pred),f1_score(y_test, gs_log_pred),gs_logreg_auc]),\n    go.Bar(name='GridSearchCV+Decision tree',\n           x=metrics, y=[gs_tree.score(X_test,y_test),gs_tree.best_score_,precision_score(y_test, gs_tree_pred),recall_score(y_test, gs_tree_pred), f1_score(y_test, gs_tree_pred),gs_tree_auc]),\n    go.Bar(name='GridSearchCV+Neural Network',\n           x=metrics, y=[gs_mlp.score(X_test,y_test),gs_mlp.best_score_,precision_score(y_test, gs_mlp_pred),recall_score(y_test, gs_mlp_pred), f1_score(y_test, gs_mlp_pred),gs_mlp_auc]),\n    \n])\n\nfig.update_layout(title_text='Metrics for each model',\n                  barmode='group',xaxis_tickangle=-45,bargroupgap=0.05)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the ROC curve.\nplt.figure(figsize=(8,8))\nplt.plot(gs_mlp_fpr, gs_mlp_tpr, color='green', label='GridSearch+MLP ROC (AUC= %0.2f)'% gs_mlp_auc)\nplt.plot(gs_tr_fpr, gs_tr_tpr, color='black', label='GridSearch+Decision tree ROC (AUC= %0.2f)'% gs_tree_auc)\nplt.plot(gs_logreg_fpr, gs_logreg_tpr, color='red',label='GridSearch+LogReg ROC (AUC= %0.2f)'% gs_logreg_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves for GridSearch')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={\n'': ['Logistic Regression','GridSearchCV + Logistic Regression','Decision Tree','GridSearchCV + Decision Tree','Neural Network (MLP)','GridSearchCV + Neural Network (MLP)'],\n'Accuracy': [logreg.score(X_test, y_test), gs_logreg.score(X_test,y_test),tr.score(X_test, y_test),gs_tree.score(X_test,y_test),mlp.score(X_test, y_test),gs_mlp.score(X_test, y_test)],\n'CV Accuracy': [logreg_cv, gs_logreg.best_score_, tr_cv,gs_tree.best_score_,mlp_cv,gs_mlp.best_score_],\n'Precision': [precision_score(y_test, log_pred), precision_score(y_test, gs_log_pred),precision_score(y_test, tr_pred),precision_score(y_test, gs_tree_pred),precision_score(y_test, mlp_pred),precision_score(y_test, gs_mlp_pred)],\n'Recall': [recall_score(y_test, log_pred), recall_score(y_test, gs_log_pred),recall_score(y_test, tr_pred),recall_score(y_test, gs_tree_pred),recall_score(y_test, mlp_pred),recall_score(y_test, gs_mlp_pred)],\n'F1': [f1_score(y_test, log_pred), f1_score(y_test, gs_log_pred),f1_score(y_test, tr_pred),f1_score(y_test, gs_tree_pred),f1_score(y_test, mlp_pred),f1_score(y_test, gs_mlp_pred)],\n'ROC AUC': [auc_logreg, gs_logreg_auc, auc_tr, gs_tree_auc, auc_mlp, gs_mlp_auc]\n}\n\nresults=pd.DataFrame(data=d).round(3).set_index('')\nresults","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}