{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nHello community, welcome to this kernel. In this kernel we're going to discover how to classify texts using TD-IDF vectorization and fully connected Pytorch models.\n\n**We'll use Torch based solutions and play the game by its rules :)**\n\nIn this kernel I did not wanna use RNNs and Word Embeddings because they're way harder so we'll see them in the next kernel.\n\nSo let's start!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Preparing Dataset\nIn this step we're going to read our dataset, create our dataset class and prepare our dataloaders. ","metadata":{}},{"cell_type":"code","source":"data = pd.concat([pd.read_csv('../input/tweets-with-sarcasm-and-irony/train.csv'),\n                  pd.read_csv(\"../input/tweets-with-sarcasm-and-irony/test.csv\")],axis=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* You know, if test set has labels it's pretty useless to split them before making it ready to use. So I concatenated them.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(list(data[\"class\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It seems like we have nan values, let's remove them.","metadata":{}},{"cell_type":"code","source":"data.dropna(inplace=True)\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now it's okay. Let's write a class which will clean and vectorize our texts.","metadata":{}},{"cell_type":"code","source":"class Vectorizer():\n    def __init__(self,clean_pattern=None,max_features=None,stop_words=None):\n        self.clean_pattern = clean_pattern\n        self.max_features = max_features\n        self.stopwords = stop_words\n        self.tfidf = TfidfVectorizer(stop_words=self.stopwords,max_features=self.max_features)\n        self.builded = False\n        \n    \n    def _clean_texts(self,texts):\n        \n        cleaned = []\n        for text in texts:\n            if self.clean_pattern is not None:\n                text = re.sub(self.clean_pattern,\" \",text)\n            \n            text = text.lower().strip()\n            cleaned.append(text)\n        \n        return cleaned\n    \n    \n    def _set_tfidf(self,cleaned_texts):\n        self.tfidf.fit(cleaned_texts)\n    \n    def build_vectorizer(self,texts):\n        cleaned_texts = self._clean_texts(texts)\n        self._set_tfidf(cleaned_texts)\n        self.builded = True\n        \n    def vectorizeTexts(self,texts):\n        if self.builded:\n            cleaned_texts = self._clean_texts(texts)\n            return self.tfidf.transform(cleaned_texts)\n        \n        else:\n            raise Exception(\"Vectorizer is not builded.\")\n            \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And let's create an object from this class and make our dataset cleaned and vectorized","metadata":{}},{"cell_type":"code","source":"x = list(data[\"tweets\"])\ny = list(data[\"class\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = Vectorizer(\"[^a-zA-Z0-9]\",max_features=7000,stop_words=\"english\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer.build_vectorizer(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorized_x = vectorizer.vectorizeTexts(x).toarray()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorized_x.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And now everything is okay with texts, let's encode the classes.","metadata":{}},{"cell_type":"code","source":"label_map = {\n    \"figurative\":0,\n    \"sarcasm\":1,\n    \"irony\":2,\n    \"regular\":3\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_encoded = []\nfor y_sample in y:\n    y_encoded.append(label_map[y_sample])\n    \ny_encoded = np.asarray(y_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_encoded.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And we're ready to create our custom Dataset object by inheriting it.","metadata":{}},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    \n    def __init__(self,x_vectorized,y_encoded):\n        self.x_vectorized = x_vectorized\n        self.y_encoded = y_encoded\n        \n    \n    def __len__(self):\n        return len(self.x_vectorized)\n    \n    \n    def __getitem__(self,index):\n        return self.x_vectorized[index],self.y_encoded[index]\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It's really easy to implement a custom dataset, let's create an object and test it.","metadata":{}},{"cell_type":"code","source":"dataset = TweetDataset(vectorized_x,y_encoded)\nprint(\"Length of our dataset is\",len(dataset))\n\nprint(dataset[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* You know, to get random samples we need a random subset sampler, now we'll prepare it.","metadata":{}},{"cell_type":"code","source":"# We've splitted our indices as train and test to use them in subset samplers.\ntrain_indices,test_indices = train_test_split(list(range(0,len(dataset))),test_size=0.25,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_indices))\nprint(len(test_indices))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sampler = SubsetRandomSampler(train_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Our dataset and samplers are ready, we can create our data loader objects and start to model our artifical neural network.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n                                                sampler=test_sampler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Building Neural Network Architecture\n\nIn this section we're gonna create a custom network class which will be inherited from nn.Module and after creating our simple neural network we'll create loss function and optimizer.","metadata":{}},{"cell_type":"code","source":"class DenseNetwork(nn.Module):\n    \n    def __init__(self):\n        super(DenseNetwork,self).__init__()\n        self.fc1 = nn.Linear(7000,1024)\n        self.drop1 = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(1024,256)\n        self.drop2 = nn.Dropout(0.4)\n        self.prediction = nn.Linear(256,4)\n        \n    def forward(self,x):\n        \n        x = F.relu(self.fc1(x.to(torch.float)))\n        x = self.drop1(x)\n        x = F.relu(self.fc2(x))\n        x = self.drop2(x)\n        x = F.log_softmax(self.prediction(x),dim=1)\n        \n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And our small and lovely neural network is ready, before creating a model object let's define our device (gpu)","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And now it's time for creating the model.","metadata":{}},{"cell_type":"code","source":"model = DenseNetwork().to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* And now we'll declare our criterion (loss) and optimizer.","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(model.parameters(),lr=1e-3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Training The Neural Network\nOur model and dataset is ready, so in this section we're gonna train our model.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 6\nTRAIN_LOSSES = []\nTRAIN_ACCURACIES = []\n\nfor epoch in range(1,EPOCHS+1):\n    epoch_loss = 0.0\n    epoch_true = 0\n    epoch_total = 0\n    for data_,target_ in train_loader:\n        data_ = data_.to(device)\n        target_ = target_.to(device)\n        \n        # Cleaning optimizer cache.\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(data_)\n        \n        # Computing loss & backward propagation\n        loss = criterion(outputs,target_)\n        loss.backward()\n        \n        # Applying gradients\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        _,pred = torch.max(outputs,dim=1)\n        epoch_true = epoch_true + torch.sum(pred == target_).item()\n        \n        epoch_total += target_.size(0)\n        \n    TRAIN_LOSSES.append(epoch_loss)\n    TRAIN_ACCURACIES.append(100 * epoch_true / epoch_total)\n    \n    print(f\"Epoch {epoch}/{EPOCHS} finished: train_loss = {epoch_loss}, train_accuracy = {TRAIN_ACCURACIES[epoch-1]}\")\n    \n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Testing Model\nWe've trained our model and it's time to test our model using our test set.","metadata":{}},{"cell_type":"code","source":"test_true = 0\ntest_total = len(test_sampler)\ntest_loss = 0.0\nwith torch.no_grad():\n    for data_,target_ in validation_loader:\n        data_,target_ = data_.to(device),target_.to(device)\n        \n        outputs = model(data_)\n        \n        loss = criterion(outputs,target_).item()\n        \n        _,pred = torch.max(outputs,dim=1)\n        \n        test_true += torch.sum(pred==target_).item()\n        test_loss += loss\n        \n\nprint(f\"Validation finished: Accuracy = {round(100 * test_true / test_total,2)}%, Loss = {test_loss}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nHey! We've finished this kernel and discovered how to use Pytorch and TF-IDF together. Validation accuracy might seems bad, but it's because of our data processing. If we would process it better it'd be better.\n\nIf you have a question about this, please ask me in the comment section of this kernel and also mention me because I generally can't see them if you don't mention me.\n\nHave a good day/night and if you liked this kernel, please upvote to support and motivate me :)","metadata":{}}]}