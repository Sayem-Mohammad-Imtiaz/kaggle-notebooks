{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### GRU and LSTM implementation with TensorFlow to build a language model\n#### It is based on my previous notebook. Now I'm trying to improve my\n#### previous character-level language model by using GRU and LSTM units\n#### and making the model deeper","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-24T16:22:38.2882Z","iopub.execute_input":"2021-06-24T16:22:38.288611Z","iopub.status.idle":"2021-06-24T16:22:39.871091Z","shell.execute_reply.started":"2021-06-24T16:22:38.288521Z","shell.execute_reply":"2021-06-24T16:22:39.870257Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\nfrom typing import Union\nfrom math import ceil, sqrt\nfrom os import mkdir, listdir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EOS = chr(10) # End of sentence\n\ndef build_vocabulary() -> list:\n    # builds a vocabulary using ASCII characters\n    vocabulary = [chr(i) for i in range(10, 128)]\n    return vocabulary\n\ndef word2index(vocabulary: list, word: str) -> int:\n    # returns the index of 'word' in the vocabulary\n    return vocabulary.index(word)\n\ndef words2onehot(vocabulary: list, words: list) -> np.ndarray:\n    # transforms the list of words given as argument into\n    # a one-hot matrix representation using the index in the vocabulary\n    n_words = len(words)\n    n_voc = len(vocabulary)\n    indices = np.array([word2index(vocabulary, word) for word in words])\n    a = np.zeros((n_words, n_voc))\n    a[np.arange(n_words), indices] = 1\n    return a\n\ndef sample_word(vocabulary: list, prob: np.ndarray, threshold: float) -> str:\n    # sample a word from the vocabulary according to 'prob'\n    # probability distribution (the softmax output of our model)\n    \n    prob = prob.tolist()\n    vocab_prob = [[vocabulary[i], prob[i]] for i in range(len(prob))]\n    vocab_prob.sort(reverse=True, key=lambda e: e[1])\n    \n    s = 0\n    for i in range(len(vocab_prob)):\n        if s > threshold:\n            vocab_prob[i][1] = 0\n        s += vocab_prob[i][1]\n        \n    vocab = [w for w, p in vocab_prob]\n    prob = np.array([p/s for w, p in vocab_prob])\n    \n    return np.random.choice(vocab, p=prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model:\n    def __init__(self,\n                 vocabulary: list,\n                 inter_time_step_size: int,\n                 unit_type: str,\n                 depth: int):\n        \n        # unit_type can be one of: 'gru' or 'lstm'\n        # depth >= 1\n        \n        self._init(vocabulary, inter_time_step_size, unit_type, depth)\n        \n        self.weights = (self._init_gru() if unit_type == 'gru'\n                                         else self._init_lstm())\n        \n        # weights and bias used to compute y (the softmax predictions)\n        self.wy = tf.Variable(tf.random.normal(\n            stddev=sqrt(2.0/(self.inter_time_step_size+self.vocab_size)),\n            shape=(self.inter_time_step_size, self.vocab_size),\n            dtype=tf.double))\n        self.by = tf.Variable(tf.random.normal(\n            stddev=sqrt(2.0/(1+self.vocab_size)),\n            shape=(1, self.vocab_size),\n            dtype=tf.double))\n        \n        self.weights.extend([self.wy, self.by])\n    \n    def _init(self,\n              vocabulary: list,\n              inter_time_step_size: int,\n              unit_type: str,\n              depth: int):\n        \n        self.vocab = vocabulary\n        self.vocab_size = len(vocabulary)\n        self.inter_time_step_size = inter_time_step_size\n        self.combined_size = self.vocab_size + self.inter_time_step_size\n        \n        self.unit_type = unit_type\n        self.depth = depth\n        \n        self.weights_shape_0 = (self.combined_size, self.inter_time_step_size)\n        self.weights_std_dev_0 = sqrt(2.0/(self.combined_size+self.inter_time_step_size))\n        \n        self.weights_shape_1 = (2*self.inter_time_step_size, self.inter_time_step_size)\n        self.weights_std_dev_1 = sqrt(2.0/(3*self.inter_time_step_size))\n        \n        self.biases_shape = (1, self.inter_time_step_size)\n        self.biases_std_dev = sqrt(2.0/(1+self.inter_time_step_size))\n        \n        self.w_shapes = [(self.weights_shape_0, self.weights_std_dev_0)]\n        self.w_shapes.extend([(self.weights_shape_1, self.weights_std_dev_1)\n                              for i in range(self.depth-1)])\n        self.b_shapes = [(self.biases_shape, self.biases_std_dev)\n                         for i in range(self.depth)]\n        \n        self.optimizer = tf.keras.optimizers.Adam()\n    \n    def _init_gru(self):\n        \n        for s in ['wr', 'wu', 'wa']:\n            setattr(self, s, [tf.Variable(tf.random.normal(\n                                    stddev=std_dev,\n                                    shape=shape,\n                                    dtype=tf.double))\n                               for shape, std_dev in self.w_shapes])\n        \n        for s in ['br', 'bu', 'ba']:\n            setattr(self, s, [tf.Variable(tf.random.normal(\n                                    stddev=std_dev,\n                                    shape=shape,\n                                    dtype=tf.double))\n                               for shape, std_dev in self.b_shapes])\n        \n        all_weights = []\n        for w in [self.wr, self.br, self.wu, self.bu, self.wa, self.ba]:\n            all_weights.extend(w)\n        \n        return all_weights\n    \n    def _init_lstm(self):\n        \n        for s in ['wu', 'wf', 'wo', 'wc']:\n            setattr(self, s, [tf.Variable(tf.random.normal(\n                                    stddev=std_dev,\n                                    shape=shape,\n                                    dtype=tf.double))\n                               for shape, std_dev in self.w_shapes])\n        \n        for s in ['bu', 'bf', 'bo', 'bc']:\n            setattr(self, s, [tf.Variable(tf.random.normal(\n                                    stddev=std_dev,\n                                    shape=shape,\n                                    dtype=tf.double))\n                               for shape, std_dev in self.b_shapes])\n        \n        all_weights = []\n        for w in [self.wu, self.bu, self.wf, self.bf,\n                  self.wo, self.bo, self.wc, self.bc]:\n            all_weights.extend(w)\n        \n        return all_weights\n    \n    def reset_state(self, num_samples: int) -> None:\n        def get_init_values():\n            return [tf.zeros((num_samples, self.inter_time_step_size),\n                             dtype=tf.double) for i in range(self.depth)]\n        \n        self.a = get_init_values()\n        if self.unit_type == 'lstm':\n            self.c = get_init_values()\n    \n    def __call__(self,\n                 x: Union[np.ndarray, tf.Tensor],\n                 y: Union[np.ndarray, tf.Tensor, None] = None) -> tf.Tensor:\n        \n        for i in range(self.depth):\n            x = self._call_level(i, x)\n        \n        y_logits = tf.linalg.matmul(x, self.wy)+self.by\n        if y is None:\n            # during prediction return softmax probabilities\n            return tf.nn.softmax(y_logits)\n        else:\n            # during training return loss\n            return tf.math.reduce_mean(\n                        tf.nn.softmax_cross_entropy_with_logits(y, y_logits))\n    \n    def _call_level(self,\n                    level: int,\n                    x: Union[np.ndarray, tf.Tensor]) -> tf.Tensor:\n        \n        return (self._call_gru(level, x) if self.unit_type == 'gru'\n                                         else self._call_lstm(level, x))\n    \n    def _call_gru(self,\n                  level: int,\n                  x: Union[np.ndarray, tf.Tensor]) -> tf.Tensor:\n        \n        n = x.shape[0]\n        \n        self.a[level] = self.a[level][0:n]\n        \n        concat_matrix = tf.concat([self.a[level], x], axis=1)\n        \n        relevance_gate = tf.math.sigmoid(\n                                tf.linalg.matmul(concat_matrix, self.wr[level])\n                                + self.br[level])\n        update_gate = tf.math.sigmoid(\n                                tf.linalg.matmul(concat_matrix, self.wu[level])\n                                + self.bu[level])\n        \n        a_candidate = tf.math.tanh(\n                          tf.linalg.matmul(\n                              tf.concat([tf.math.multiply(relevance_gate, self.a[level]), x], axis=1),\n                              self.wa[level])\n                          + self.ba[level])\n        \n        self.a[level] = (tf.math.multiply(update_gate, a_candidate) +\n                         tf.math.multiply((1-update_gate), self.a[level]))\n        \n        return self.a[level]\n    \n    def _call_lstm(self,\n                   level: int,\n                   x: Union[np.ndarray, tf.Tensor]) -> tf.Tensor:\n        \n        n = x.shape[0]\n        \n        self.a[level] = self.a[level][0:n]\n        self.c[level] = self.c[level][0:n]\n        \n        concat_matrix = tf.concat([self.a[level], x], axis=1)\n        \n        update_gate = tf.math.sigmoid(\n                            tf.linalg.matmul(concat_matrix, self.wu[level])\n                            + self.bu[level])\n        forget_gate = tf.math.sigmoid(\n                            tf.linalg.matmul(concat_matrix, self.wf[level])\n                            + self.bf[level])\n        output_gate = tf.math.sigmoid(\n                            tf.linalg.matmul(concat_matrix, self.wo[level])\n                            + self.bo[level])\n        \n        c_candidate = tf.math.tanh(\n                            tf.linalg.matmul(concat_matrix, self.wc[level])\n                            + self.bc[level])\n        \n        self.c[level] = (tf.math.multiply(update_gate, c_candidate) +\n                         tf.math.multiply(forget_gate, self.c[level]))\n        \n        self.a[level] = tf.math.multiply(output_gate, tf.math.tanh(self.c[level]))\n        \n        return self.a[level]\n    \n    def fit(self,\n            sentences: list,\n            batch_size: int = 128,\n            epochs: int = 10) -> None:\n        \n        n_sent = len(sentences)\n        num_batches = ceil(n_sent / batch_size)\n        \n        for epoch in range(epochs):\n            \n            random.shuffle(sentences)\n            start = 0\n            batch_idx = 0\n            \n            while start < n_sent:\n                \n                print('Training model: %05.2f%%' %\n                      (100*(epoch*num_batches+batch_idx+1)/(epochs*num_batches),),\n                      end='\\r')\n                \n                batch_idx += 1\n                end = min(start+batch_size, n_sent)\n                batch_sent = sentences[start:end]\n                start = end\n                batch_sent.sort(reverse=True, key=lambda s: len(s))\n                \n                init_num_words = len(batch_sent)\n                self.reset_state(init_num_words)\n                x = np.zeros((init_num_words, self.vocab_size))\n                \n                time_steps = len(batch_sent[0])+1\n                \n                with tf.GradientTape() as tape:\n                \n                    losses = []\n                    for t in range(time_steps):\n                        words = []\n                        for i in range(init_num_words):\n                            if t > len(batch_sent[i]):\n                                break\n                            if t == len(batch_sent[i]):\n                                words.append(EOS)\n                                break\n                            words.append(batch_sent[i][t])\n\n                        y = words2onehot(self.vocab, words)\n                        n = y.shape[0]\n                        loss = self(x[0:n], y)\n                        losses.append(loss)\n                        x = y\n                    \n                    loss_value = tf.math.reduce_mean(losses)\n                \n                grads = tape.gradient(loss_value, self.weights)\n                self.optimizer.apply_gradients(zip(grads, self.weights))\n\n    def sample(self, threshold: float = 0.9) -> str:\n        # sample a new sentence from the learned model\n        sentence = ''\n        self.reset_state(1)\n        x = np.zeros((1, self.vocab_size))\n        while True:\n            y_hat = self(x)\n            word = sample_word(self.vocab,\n                               tf.reshape(y_hat, (-1,)).numpy(),\n                               threshold)\n            if word == EOS:\n                break\n            sentence += word\n            x = words2onehot(self.vocab, [word])\n        return sentence\n    \n    def predict_next(self, sentence: str,\n                     threshold: float = 0.9) -> str:\n        \n        # predict the next part of the sentence given as parameter\n        \n        self.reset_state(1)\n        for word in sentence.strip():\n            x = words2onehot(self.vocab, [word])\n            y_hat = self(x)\n        s = ''\n        while True:\n            word = sample_word(self.vocab,\n                               tf.reshape(y_hat, (-1,)).numpy(),\n                               threshold)\n            if word == EOS:\n                break\n            s += word\n            x = words2onehot(self.vocab, [word])\n            y_hat = self(x)\n        return s\n    \n    def save(self, name: str) -> None:\n        mkdir(f'./{name}')\n        mkdir(f'./{name}/weights')\n        with open(f'./{name}/vocabulary.txt', 'w') as f:\n            f.write('[separator]'.join(self.vocab))\n        with open(f'./{name}/inter_time_step_size.txt', 'w') as f:\n            f.write(str(self.inter_time_step_size))\n        with open(f'./{name}/unit_type.txt', 'w') as f:\n            f.write(self.unit_type)\n        with open(f'./{name}/depth.txt', 'w') as f:\n            f.write(str(self.depth))\n        \n        if self.unit_type == 'gru':\n            for s in ['wr', 'br', 'wu', 'bu', 'wa', 'ba']:\n                for i in range(self.depth):\n                    np.save(f'./{name}/weights/{s}_{i}.npy',\n                            getattr(self, s)[i].numpy())\n        else:\n            for s in ['wu', 'bu', 'wf', 'bf', 'wo', 'bo', 'wc', 'bc']:\n                for i in range(self.depth):\n                    np.save(f'./{name}/weights/{s}_{i}.npy',\n                            getattr(self, s)[i].numpy())\n        \n        np.save(f'./{name}/weights/wy.npy', self.wy.numpy())\n        np.save(f'./{name}/weights/by.npy', self.by.numpy())\n    \n    def load(self, name: str) -> None:\n        with open(f'./{name}/vocabulary.txt', 'r') as f:\n            vocabulary = f.read().split('[separator]')\n        with open(f'./{name}/inter_time_step_size.txt', 'r') as f:\n            inter_time_step_size = int(f.read())\n        with open(f'./{name}/unit_type.txt', 'r') as f:\n            unit_type = f.read()\n        with open(f'./{name}/depth.txt', 'r') as f:\n            depth = int(f.read())\n        \n        self._init(vocabulary, inter_time_step_size,\n                   unit_type, depth)\n        \n        weights_names = []\n        filenames = listdir(f'./{name}/weights')\n        filenames.sort()\n        for filename in filenames:\n            if filename in ['wy.npy', 'by.npy']:\n                continue\n            \n            attr_name, index = filename.replace('.npy', '').split('_')\n            index = int(index)\n            \n            if index == 0:\n                setattr(self, attr_name, [])\n                weights_names.append(attr_name)\n            getattr(self, attr_name).append(\n                tf.Variable(np.load(f'./{name}/weights/{filename}')))\n        \n        self.wy = tf.Variable(np.load(f'./{name}/weights/wy.npy'))\n        self.by = tf.Variable(np.load(f'./{name}/weights/by.npy'))\n        \n        self.weights = [getattr(self, weight_name)\n                        for weight_name in weights_names]\n        self.weights.extend([self.wy, self.by])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/million-headlines/abcnews-date-text.csv')\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = build_vocabulary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = df['headline_text'].values.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(vocabulary,\n              inter_time_step_size=512,\n              unit_type='lstm',\n              depth=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(sentences, batch_size=1024, epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('news_headlines_model')\n# model.load('news_headlines_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    print(model.sample())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = 'scientists just discovered'\ns += model.predict_next(s)\ns","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}