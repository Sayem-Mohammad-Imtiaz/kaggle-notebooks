{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Domain: Cement manufacturing\n\nContext: Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n\nObjective: Modeling of strength of high performance concrete using Machine Learning - EDA, Building ML model for regression and Hyper parameter tuning ","metadata":{}},{"cell_type":"code","source":"#1. Data pre-processing: Perform all the necessary preprocessing on the data ready to be fed for \n# Featurization, Model Selection & Tuning","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.393625Z","iopub.execute_input":"2021-06-20T00:39:43.393983Z","iopub.status.idle":"2021-06-20T00:39:43.398258Z","shell.execute_reply.started":"2021-06-20T00:39:43.393952Z","shell.execute_reply":"2021-06-20T00:39:43.397088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.422731Z","iopub.execute_input":"2021-06-20T00:39:43.423121Z","iopub.status.idle":"2021-06-20T00:39:43.428853Z","shell.execute_reply.started":"2021-06-20T00:39:43.423083Z","shell.execute_reply":"2021-06-20T00:39:43.427733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data and name it as cData (i.e. Concrete Data)\ncData = pd.read_csv(\"../input/concrete/concrete.csv\")\ncData.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.442292Z","iopub.execute_input":"2021-06-20T00:39:43.442643Z","iopub.status.idle":"2021-06-20T00:39:43.455296Z","shell.execute_reply.started":"2021-06-20T00:39:43.442609Z","shell.execute_reply":"2021-06-20T00:39:43.454285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Given data set have total of 9 properties or columns and 1030 rows or concrete properties data points","metadata":{}},{"cell_type":"code","source":"# Display the data set info\ncData.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.462034Z","iopub.execute_input":"2021-06-20T00:39:43.462368Z","iopub.status.idle":"2021-06-20T00:39:43.48993Z","shell.execute_reply.started":"2021-06-20T00:39:43.462338Z","shell.execute_reply":"2021-06-20T00:39:43.489204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 9 properties are provided to capture the measurement of various ingredients used in concrete preparation and has contribution in concrete strength\n- 1 property i.e. '# of days' termed as 'age' is of integer type data\n- remaining 8 properties are having decimal point values","metadata":{}},{"cell_type":"code","source":"# Make a copy of the original data set cData for further processing through various Machine Learning technique \n# in later part of the project\ncDataOrig = cData.copy() #cDataOrig = Original copy of master data set for Concrete Strength measurement\n\n# Let's get the sample look of the data\ncDataOrig.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.491358Z","iopub.execute_input":"2021-06-20T00:39:43.491761Z","iopub.status.idle":"2021-06-20T00:39:43.514838Z","shell.execute_reply.started":"2021-06-20T00:39:43.491723Z","shell.execute_reply":"2021-06-20T00:39:43.513855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All attributes are numerical in the data set\n- None of the properties are categorical in the data set, hence no need of encoding or dummy propertie creation required","metadata":{}},{"cell_type":"code","source":"# Check if there is any missing values in the data set\ncData.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.516043Z","iopub.execute_input":"2021-06-20T00:39:43.516354Z","iopub.status.idle":"2021-06-20T00:39:43.524264Z","shell.execute_reply.started":"2021-06-20T00:39:43.51632Z","shell.execute_reply":"2021-06-20T00:39:43.523252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above indicates there is no blank values in the dataset provided","metadata":{}},{"cell_type":"code","source":"# Check how many ZERO values present in each column\n(cData == 0).sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.525571Z","iopub.execute_input":"2021-06-20T00:39:43.525875Z","iopub.status.idle":"2021-06-20T00:39:43.549443Z","shell.execute_reply.started":"2021-06-20T00:39:43.525845Z","shell.execute_reply":"2021-06-20T00:39:43.548245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above data point we see following % of values in respective columns are missing\n    - 47% in Slag\n    - 56% in ash\n    - 37% in superplastic\n    \n- Let's review the other data distribution below and later shall take a technique to address the missing values","metadata":{}},{"cell_type":"code","source":"# Let's have a quick look into the data description to get an idea about the 5 point summary\ncData.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.552239Z","iopub.execute_input":"2021-06-20T00:39:43.552539Z","iopub.status.idle":"2021-06-20T00:39:43.597452Z","shell.execute_reply.started":"2021-06-20T00:39:43.552511Z","shell.execute_reply":"2021-06-20T00:39:43.596516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All properties are numerical, hence none of the columns are dropped in the 5 point summary\n- slag: 25% concrete samples have no slag in it. Value in Q3 i.e. in 75% bucket is 142.95 where as max value in 359.5. It indicates a possible presence of outliers in this column\n- ash: 50% concrete samples have no ash in it. Value in Q3 i.e. in 75% bucket is 118.3 where as max value in 200.1. It indicates a possible presence of outliers in this column\n- superplastic: 25% concrete samples have no superplastic in it. Value in Q3 i.e. in 75% bucket is 10.2 where as max value in 32.2. It indicates a possible presence of outliers in this column as well\n- age: The concrete strength is measured from 1 day to 365 days range, however 75% of the data points are measured for 56 days\n- Strength is the target variable in the data set and remaining 8 ingredients to contribute in measuring the concrete strength are independent attributes","metadata":{}},{"cell_type":"code","source":"# Display the range of values of each column\ncData.max() - cData.min()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.59919Z","iopub.execute_input":"2021-06-20T00:39:43.599698Z","iopub.status.idle":"2021-06-20T00:39:43.60829Z","shell.execute_reply.started":"2021-06-20T00:39:43.599665Z","shell.execute_reply":"2021-06-20T00:39:43.607318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's review the skewness of the properties\ncData.skew()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.609794Z","iopub.execute_input":"2021-06-20T00:39:43.610501Z","iopub.status.idle":"2021-06-20T00:39:43.620192Z","shell.execute_reply.started":"2021-06-20T00:39:43.610453Z","shell.execute_reply":"2021-06-20T00:39:43.619204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Following properties are positively skewed (with value >.5). It indicates that these atributes are not normally distributed. The tail of the distribution is longer on the right side. The mean is greater than the median for these parameters.\n\n    - slag, superplastic, age\n\n- Following properties are negatively skewed. It indicates that these atributes are also not normally distributed. The tail of the distribution is longer on the left side. The mean is lesser than the median for these parameters.\n\n    - coarseagg, fine aggregator","metadata":{}},{"cell_type":"code","source":"# Understanding the attributes - Find relationship between different attributes (Independent variables) and \n# choose which all attributes have to be a part of the analysis and why","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.621521Z","iopub.execute_input":"2021-06-20T00:39:43.622107Z","iopub.status.idle":"2021-06-20T00:39:43.627393Z","shell.execute_reply.started":"2021-06-20T00:39:43.622062Z","shell.execute_reply":"2021-06-20T00:39:43.626668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe the correlation of the variables through graphical Heat map\n# +ve and -ve numbers indicate how the variabes are correalted to each other\n\ncolormap = plt.cm.viridis # Color range to be used in heatmap\nplt.figure(figsize=(15,15))\nplt.title('Concrete properties Correlation of attributes', y=1.05, size=19)\nsns.heatmap(cData.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:43.628619Z","iopub.execute_input":"2021-06-20T00:39:43.629218Z","iopub.status.idle":"2021-06-20T00:39:44.366821Z","shell.execute_reply.started":"2021-06-20T00:39:43.629174Z","shell.execute_reply":"2021-06-20T00:39:44.365769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Following features have strong relationship comparitively on the target column strength:\n    - cement, superplastic, duration i.e. age and water respectively\n- Fineagg, coarseagg, water are not strongly related to the strength of the concrete\n- superplastic and ash are closely related and superlastic has positive influence on concrete strength","metadata":{}},{"cell_type":"code","source":"# Display the pair plot\nsns.pairplot(cData)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:44.368037Z","iopub.execute_input":"2021-06-20T00:39:44.368355Z","iopub.status.idle":"2021-06-20T00:39:59.279499Z","shell.execute_reply.started":"2021-06-20T00:39:44.368324Z","shell.execute_reply":"2021-06-20T00:39:59.27775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above pair plot between given features, we observe following -\n    - slag, ash, age features don't have normalized data distribution and are right skewed\n    - strength properties have multiple peaks indicating possible clusters in the dataset\n    - all features don't have any linear relationsship with strength\n    - slag, ash, superplastic feature have multiple zero values in the dataset\n    - age columns can be grouped into 4 separate cluster","metadata":{}},{"cell_type":"code","source":"# Measure of possible outliers in the dataset -\n# It can be done by checking the box plot of each properties or by evaluating the z scores of each columns\n\n#As there are 9 properties given in the dataset, let's get the Z score of the entire data set \n#to check outliers statistically\n\nfrom scipy.stats import zscore\n\n# Get the z score\nz_cData = cData.apply(zscore)\n\n# Set the limt to 3 sigma to check outliers in the voice sample provided\nlimit = 3\nt1 = np.where(z_cData > limit) #store the outliers in the touple variable\n\n# print the index value of original data set which contains outliers >3 sigma\nprint(t1[0]) # depicts the row# containing outliers\nprint(t1[1]) # depicts the column# containing outliers","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:59.281469Z","iopub.execute_input":"2021-06-20T00:39:59.281867Z","iopub.status.idle":"2021-06-20T00:39:59.295181Z","shell.execute_reply.started":"2021-06-20T00:39:59.281827Z","shell.execute_reply":"2021-06-20T00:39:59.294024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list1 = t1[0] #load the row# of the data set containing outliers\nlist2 = t1[1] #load the column# of the data set containing outliers\n\nj = 0 #initiate the iterator\n\n# print the outlier column and it's value from the data set provided\nfor i in list2:   #loop through the columns\n    print(\"Outliers exist in properties: \", cData.columns[i], \" and the value is: \",cData.loc[list1[j]][i])\n    j +=1 #move to the next value of the corresponding row#  ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:59.296613Z","iopub.execute_input":"2021-06-20T00:39:59.296899Z","iopub.status.idle":"2021-06-20T00:39:59.323988Z","shell.execute_reply.started":"2021-06-20T00:39:59.296867Z","shell.execute_reply":"2021-06-20T00:39:59.323005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Following are few of the properties which have multiple outliers in the dataset, programatically above depicts the same.\n    - slag, age, superplastic, water","metadata":{}},{"cell_type":"code","source":"# Let's review the outliers visually through below graph i.e. to find presence of leverage points\n# plot strength vs age relation\nsns.boxplot(x = \"age\", y = \"strength\", data = cData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:59.325329Z","iopub.execute_input":"2021-06-20T00:39:59.325589Z","iopub.status.idle":"2021-06-20T00:39:59.813266Z","shell.execute_reply.started":"2021-06-20T00:39:59.325563Z","shell.execute_reply":"2021-06-20T00:39:59.812228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are outliers for the age group of 14, 28 and 180\n- 14 unique age groups can be obtained from the dataset","metadata":{}},{"cell_type":"code","source":"# Let's view the presence of outliers in overall age dataset\nsns.boxplot(y = \"age\", data = cData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:59.814553Z","iopub.execute_input":"2021-06-20T00:39:59.814859Z","iopub.status.idle":"2021-06-20T00:39:59.93748Z","shell.execute_reply.started":"2021-06-20T00:39:59.814828Z","shell.execute_reply":"2021-06-20T00:39:59.936393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the presence of outliers in slag dataset\nsns.boxplot(y = \"slag\", data = cData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:39:59.938661Z","iopub.execute_input":"2021-06-20T00:39:59.938976Z","iopub.status.idle":"2021-06-20T00:40:00.063859Z","shell.execute_reply.started":"2021-06-20T00:39:59.938938Z","shell.execute_reply":"2021-06-20T00:40:00.062854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the presence of outliers in overall water dataset\nsns.boxplot(y=\"water\", data = cData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:00.068965Z","iopub.execute_input":"2021-06-20T00:40:00.069291Z","iopub.status.idle":"2021-06-20T00:40:00.188428Z","shell.execute_reply.started":"2021-06-20T00:40:00.06926Z","shell.execute_reply":"2021-06-20T00:40:00.187251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the presence of outliers in overall superplastic dataset\nsns.boxplot(y=\"superplastic\", data = cData); \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:00.193608Z","iopub.execute_input":"2021-06-20T00:40:00.193906Z","iopub.status.idle":"2021-06-20T00:40:00.311655Z","shell.execute_reply.started":"2021-06-20T00:40:00.193878Z","shell.execute_reply":"2021-06-20T00:40:00.31071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution plot of Slag\nsns.distplot(cData['slag'])\nplt.show()\n\n#Distribution plot of ash\nsns.distplot(cData['ash'])\nplt.show()\n\n#Distribution plot of age\nsns.distplot(cData['age'])\nplt.show()\n\n#Distribution plot of superplastic\nsns.distplot(cData['superplastic'])\nplt.show()\n\n#Distribution plot of water\nsns.distplot(cData['water'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:00.312964Z","iopub.execute_input":"2021-06-20T00:40:00.313261Z","iopub.status.idle":"2021-06-20T00:40:01.551739Z","shell.execute_reply.started":"2021-06-20T00:40:00.313233Z","shell.execute_reply":"2021-06-20T00:40:01.550713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform necessary imputation - to address the presence outliers and missing values","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:01.553129Z","iopub.execute_input":"2021-06-20T00:40:01.553413Z","iopub.status.idle":"2021-06-20T00:40:01.557545Z","shell.execute_reply.started":"2021-06-20T00:40:01.553386Z","shell.execute_reply":"2021-06-20T00:40:01.556482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From above histograms of slag, ash, superplastic columns we see a significant % of data points doesn't have any value\n# Hence let's impute these variables and replace it with it's median value\n\n# Replace all the zero values with corresponding median in slag, ash and superplastic columns\nfrom sklearn.impute import SimpleImputer #Use the SimpleImputer library to replace all blanks in a generic way\n\n# Get the Imputer initialized for the data set\nimputer_for_cData = SimpleImputer(missing_values = 0 , strategy = 'median')\nimputer_for_cData.fit(cData)\n\ncol_names_cData = cData.columns.values # Get the column names for the data set\n\n# Get the new data set after replacing the blanks with corresponding column median\ncData = pd.DataFrame(imputer_for_cData.transform(cData), columns=col_names_cData)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:01.558728Z","iopub.execute_input":"2021-06-20T00:40:01.559018Z","iopub.status.idle":"2021-06-20T00:40:01.936282Z","shell.execute_reply.started":"2021-06-20T00:40:01.55899Z","shell.execute_reply":"2021-06-20T00:40:01.935236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's obtain before and after comparison with respect to 5 point summary of the data set\ncDataOrig.describe().T # from the dataset before the imputation","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:01.940053Z","iopub.execute_input":"2021-06-20T00:40:01.940379Z","iopub.status.idle":"2021-06-20T00:40:01.989613Z","shell.execute_reply.started":"2021-06-20T00:40:01.940347Z","shell.execute_reply":"2021-06-20T00:40:01.98864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cData.describe().T # from the dataset after the imputation","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:01.99113Z","iopub.execute_input":"2021-06-20T00:40:01.991562Z","iopub.status.idle":"2021-06-20T00:40:02.036037Z","shell.execute_reply.started":"2021-06-20T00:40:01.991519Z","shell.execute_reply":"2021-06-20T00:40:02.035027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above comparison, we see the 5 point summary (i.e. mean, standard deviation, min, IQR, max) data is updated for slag, ash and superplastic properties","metadata":{}},{"cell_type":"code","source":"# Let's compare the median value of columns before/after of the imputation\ncDataOrig.median() #column median from the original data set before imputation","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:02.037296Z","iopub.execute_input":"2021-06-20T00:40:02.037572Z","iopub.status.idle":"2021-06-20T00:40:02.04594Z","shell.execute_reply.started":"2021-06-20T00:40:02.037546Z","shell.execute_reply":"2021-06-20T00:40:02.045065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cData.median() #column median from the data set after imputing zero values","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:02.047143Z","iopub.execute_input":"2021-06-20T00:40:02.047556Z","iopub.status.idle":"2021-06-20T00:40:02.060628Z","shell.execute_reply.started":"2021-06-20T00:40:02.047527Z","shell.execute_reply":"2021-06-20T00:40:02.05955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above indicates that the median value changed only for slag, ash and superplastic properties","metadata":{}},{"cell_type":"code","source":"# Let's review the data distribution after imputation of zeros i.e. missing values in the given concrete data set\n#Distribution plot of Slag - after imputation of zeros\nsns.distplot(cData['slag'])\nplt.show()\n\n#Distribution plot of superplastic - after imputation of zeros\nsns.distplot(cData['superplastic'])\nplt.show()\n\n#Distribution plot of ash\n#more than 50% data points had zero in the ash column, kde bandwidth is zero as well. \n#Hence marked kde as False in histogram plot\nsns.distplot(cData['ash'], kde=False) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:02.061881Z","iopub.execute_input":"2021-06-20T00:40:02.06242Z","iopub.status.idle":"2021-06-20T00:40:02.975073Z","shell.execute_reply.started":"2021-06-20T00:40:02.062377Z","shell.execute_reply":"2021-06-20T00:40:02.97382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above 3 distribution graph of slag, ash and super plastic we see the data is better distributed and following normal distribution with one major peak","metadata":{}},{"cell_type":"code","source":"# Let's review the dataset to get rid of the outliers\n# Previously we measused the 3 sigma z-score on the entire dataset and found outliers statistically.\n\n# Let's remove the outliers from the data set\ncData_zScore = cData.apply(zscore) # Get the data set with z score\n\n#Make new dataframe after removing the out liers\ncData_wo_outliers_by_Zscore = cData[(cData_zScore <3).all(axis=1)]\n\nprint(\"The shape of the dataset (before outliers removal by Z-score): \", cData.shape)\nprint(\"The shape of the dataset (after outliers removal by Z-score): \", cData_wo_outliers_by_Zscore.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:02.976673Z","iopub.execute_input":"2021-06-20T00:40:02.976987Z","iopub.status.idle":"2021-06-20T00:40:02.990565Z","shell.execute_reply.started":"2021-06-20T00:40:02.976954Z","shell.execute_reply":"2021-06-20T00:40:02.989215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above computation, 77 rows out of total 1030 (i.e. 7%) records are getting dropped","metadata":{}},{"cell_type":"code","source":"# In an another approach we can replace the outliers by referring the upper or lower whisker\n# In this case no records need to be dropped from the data set\n\n# Get the quartile and inter quartile ranges\nQ1 = cData.quantile(0.25)\nQ3 = cData.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:02.992764Z","iopub.execute_input":"2021-06-20T00:40:02.993066Z","iopub.status.idle":"2021-06-20T00:40:03.007917Z","shell.execute_reply.started":"2021-06-20T00:40:02.993037Z","shell.execute_reply":"2021-06-20T00:40:03.004598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cData_wo_outliers_by_whisker = cData.copy()\n\n# Replace every outlier on the lower side by the lower whisker\nfor i, j in zip(np.where(cData_wo_outliers_by_whisker < Q1 - 1.5 * IQR)[0], np.where(cData_wo_outliers_by_whisker < Q1 - 1.5 * IQR)[1]): \n    \n    whisker  = Q1 - 1.5 * IQR\n    cData_wo_outliers_by_whisker.iloc[i,j] = whisker[j]\n    \n    \n#Replace every outlier on the upper side by the upper whisker    \nfor i, j in zip(np.where(cData_wo_outliers_by_whisker > Q3 + 1.5 * IQR)[0], np.where(cData_wo_outliers_by_whisker > Q3 + 1.5 * IQR)[1]):\n    \n    whisker  = Q3 + 1.5 * IQR\n    cData_wo_outliers_by_whisker.iloc[i,j] = whisker[j]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:03.009527Z","iopub.execute_input":"2021-06-20T00:40:03.010051Z","iopub.status.idle":"2021-06-20T00:40:03.506984Z","shell.execute_reply.started":"2021-06-20T00:40:03.010004Z","shell.execute_reply":"2021-06-20T00:40:03.50588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The shape of the dataset (before outliers replacement by whisker): \", cData.shape)\nprint(\"The shape of the dataset (after outliers replacement by whisker): \", cData_wo_outliers_by_whisker.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:03.508383Z","iopub.execute_input":"2021-06-20T00:40:03.508691Z","iopub.status.idle":"2021-06-20T00:40:03.513334Z","shell.execute_reply.started":"2021-06-20T00:40:03.508662Z","shell.execute_reply":"2021-06-20T00:40:03.512342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above the outliers are replaced but not removed, hence the shape of the data set didn't change","metadata":{}},{"cell_type":"code","source":"# Let's review if still there is outliers in the dataset - after removal by whisker\n\n# Check outliers in superplastic\nsns.boxplot(y=\"superplastic\", data = cData_wo_outliers_by_whisker); \nplt.show()\n\n# Check outliers in superplastic\nsns.boxplot(y=\"age\", data = cData_wo_outliers_by_whisker); \nplt.show()\n\n# Check outliers in superplastic\nsns.boxplot(y=\"water\", data = cData_wo_outliers_by_whisker); \nplt.show()\n\n# Check outliers in superplastic\nsns.boxplot(y=\"slag\", data = cData_wo_outliers_by_whisker); \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:03.514275Z","iopub.execute_input":"2021-06-20T00:40:03.51456Z","iopub.status.idle":"2021-06-20T00:40:03.96634Z","shell.execute_reply.started":"2021-06-20T00:40:03.514534Z","shell.execute_reply":"2021-06-20T00:40:03.965237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above graphical representation of slag, water, age and superplastic, we see no more outliers in the dataset","metadata":{}},{"cell_type":"code","source":"# Identify opportunities to create a composite feature, drop a feature etc","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:03.967615Z","iopub.execute_input":"2021-06-20T00:40:03.967886Z","iopub.status.idle":"2021-06-20T00:40:03.972403Z","shell.execute_reply.started":"2021-06-20T00:40:03.96786Z","shell.execute_reply":"2021-06-20T00:40:03.97108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a copy of data set to check if any of the feature can be divided into subgroups\ncDataFE = cData.copy() # cDataFE - stands for Concrete data set for Feature Engineering\ncDataFE.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:03.973913Z","iopub.execute_input":"2021-06-20T00:40:03.974406Z","iopub.status.idle":"2021-06-20T00:40:04.000389Z","shell.execute_reply.started":"2021-06-20T00:40:03.974365Z","shell.execute_reply":"2021-06-20T00:40:03.999362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's review if there are unique values in each of the columns\nprint(\"Print the unique values in cement column: \", len(cDataFE.cement.unique()))\nprint(\"Print the unique values in slag column: \", len(cDataFE.slag.unique()))\nprint(\"Print the unique values in ash column: \", len(cDataFE.ash.unique()))\nprint(\"Print the unique values in water column: \", len(cDataFE.water.unique()))\nprint(\"Print the unique values in superplastic column: \", len(cDataFE.superplastic.unique()))\nprint(\"Print the unique values in coarseagg column: \", len(cDataFE.coarseagg.unique()))\nprint(\"Print the unique values in fineagg column: \", len(cDataFE.fineagg.unique()))\nprint(\"Print the unique values in Age column: \", len(cDataFE.age.unique()))\nprint(\"Print the unique values in strength column: \", len(cDataFE.strength.unique()))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.002146Z","iopub.execute_input":"2021-06-20T00:40:04.002601Z","iopub.status.idle":"2021-06-20T00:40:04.015541Z","shell.execute_reply.started":"2021-06-20T00:40:04.002558Z","shell.execute_reply":"2021-06-20T00:40:04.014398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As the Age i.e. count of unique # of days are only 14 out of total  1030 records, we can split the above feature as categoricl as follows -\n\n    - Less than 1 month (for # of days between 1 - <30)\n    - Less than 3 month (for # of days between 30 - <90)\n    - Less than 6 month (for # of days between 90 - <180)\n    - Less than a year  (for # of days between 180 - 365)","metadata":{}},{"cell_type":"code","source":"# Let's categorize the Age column as explained above.\n# As a first steps converting the range of days into 4 categories\n\nj = 0 # Instantiate the iterator\n\nfor i in cDataFE['age']: # Loop through the Age column to find the match defined in below conditions\n    if i > 0 and i <=30:\n        cDataFE.loc[j]['age'] = 1 # Denote the range Less than 1 month (for # of days between 1 - <30) with 1\n        j +=1\n    elif i > 30 and i <= 90:\n        cDataFE.loc[j]['age'] = 2 # Denote the range Less than 3 month (for # of days between 30 - <90) with 2\n        j +=1\n    elif i > 90 and i <= 180:\n        cDataFE.loc[j]['age'] = 3 # Denote the range Less than 6 month (for # of days between 90 - <180) with 3\n        j +=1\n    elif i > 180 and i <= 365:\n        cDataFE.loc[j]['age'] = 4 # Denote the range Less than a year (for # of days between 180 - 365) with 4\n        j +=1","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.017368Z","iopub.execute_input":"2021-06-20T00:40:04.017763Z","iopub.status.idle":"2021-06-20T00:40:04.134924Z","shell.execute_reply.started":"2021-06-20T00:40:04.017723Z","shell.execute_reply":"2021-06-20T00:40:04.133945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the Age column's unique value now\ncDataFE.age.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.136525Z","iopub.execute_input":"2021-06-20T00:40:04.13691Z","iopub.status.idle":"2021-06-20T00:40:04.144427Z","shell.execute_reply.started":"2021-06-20T00:40:04.136871Z","shell.execute_reply":"2021-06-20T00:40:04.143281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now replace the categorical value with the target category of the date range\ncDataFE['age'] = cDataFE['age'].replace({1: 'Less than 1 month', \n                                         2: 'Less than 2 month', \n                                         3: 'Less than 6 month',\n                                         4: 'Less than a year'})\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.145719Z","iopub.execute_input":"2021-06-20T00:40:04.14613Z","iopub.status.idle":"2021-06-20T00:40:04.158007Z","shell.execute_reply.started":"2021-06-20T00:40:04.146091Z","shell.execute_reply":"2021-06-20T00:40:04.15684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the Age column's unique value - after replacement with exact category\ncDataFE.age.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.159242Z","iopub.execute_input":"2021-06-20T00:40:04.159547Z","iopub.status.idle":"2021-06-20T00:40:04.177492Z","shell.execute_reply.started":"2021-06-20T00:40:04.159517Z","shell.execute_reply":"2021-06-20T00:40:04.176229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dummy properties for age column\ncDataFE = pd.get_dummies(cDataFE, columns=['age'])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.178978Z","iopub.execute_input":"2021-06-20T00:40:04.179307Z","iopub.status.idle":"2021-06-20T00:40:04.1889Z","shell.execute_reply.started":"2021-06-20T00:40:04.179277Z","shell.execute_reply":"2021-06-20T00:40:04.188133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cDataFE.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.190445Z","iopub.execute_input":"2021-06-20T00:40:04.191051Z","iopub.status.idle":"2021-06-20T00:40:04.220649Z","shell.execute_reply.started":"2021-06-20T00:40:04.191009Z","shell.execute_reply":"2021-06-20T00:40:04.219516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Above shows that the # of days converted into boolean categorical properties denoting 1 = yes and 0 = no in each category of days range\n- In following sections we will validate the importance of these features on target variable 'strength'","metadata":{}},{"cell_type":"code","source":"# Let's review if any composite feature can be created from the given properties in the data set\n# In the concrete preparation water and cement ratio is very critical combination. Hence we can calculate the w/c ratio\n# and drop the cement-water individual features\n\n# Take copy of the data set prepared for Feature Engineerting above (this has the additional categorization of days column)\ncDataFE_unscaled_wc_ratio = cDataFE.copy()\n\n# Insert the new composite feature to the unscaled dataset\ncDataFE_unscaled_wc_ratio.insert(cDataFE_unscaled_wc_ratio.shape[-1]-1,\n                                 'water-cement-ratio', \n                                 cDataFE_unscaled_wc_ratio['water']/cDataFE_unscaled_wc_ratio['cement'])\n\n# Drop the original individual feature as the water/cement ratio is introduced as composite feature\ncDataFE_unscaled_wc_ratio.drop(['water', 'cement'], axis=1, inplace=True)\n\n# Let's scale the dataset\nfrom sklearn.preprocessing import MaxAbsScaler\n\nscaler = MaxAbsScaler() #Instanciate MaxAbsScaler\ncDataFE_unscaled_wc_ratio_copy = cDataFE_unscaled_wc_ratio.copy() \ncDataFE_scaled_wc_ratio = scaler.fit_transform(cDataFE_unscaled_wc_ratio) # Scale through fit-transform\ncDataFE_unscaled_wc_ratio_copy.loc[:,:] = cDataFE_scaled_wc_ratio # Prepare the scaled dataset\ncDataFE_scaled_wc_ratio = cDataFE_unscaled_wc_ratio_copy.copy() # Name the scaled dataset\ncDataFE_scaled_wc_ratio.head() # View the scaled data set with a composite feature water/cement ratio","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.221966Z","iopub.execute_input":"2021-06-20T00:40:04.222245Z","iopub.status.idle":"2021-06-20T00:40:04.25515Z","shell.execute_reply.started":"2021-06-20T00:40:04.222215Z","shell.execute_reply":"2021-06-20T00:40:04.254408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the distribution of the new composite feature introduced\ncDataFE_scaled_wc_ratio['water-cement-ratio'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.256098Z","iopub.execute_input":"2021-06-20T00:40:04.256489Z","iopub.status.idle":"2021-06-20T00:40:04.264226Z","shell.execute_reply.started":"2021-06-20T00:40:04.25646Z","shell.execute_reply":"2021-06-20T00:40:04.263528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe the correlation of the variables through graphical Heat map\n# +ve and -ve numbers indicate how the variabes are correalted to each other\n\ncolormap = plt.cm.viridis # Color range to be used in heatmap\nplt.figure(figsize=(15,15))\nplt.title('Concrete properties Correlation of attributes - with water-cement ratio', y=1.05, size=19)\nsns.heatmap(cDataFE_scaled_wc_ratio.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:04.265367Z","iopub.execute_input":"2021-06-20T00:40:04.265629Z","iopub.status.idle":"2021-06-20T00:40:05.684866Z","shell.execute_reply.started":"2021-06-20T00:40:04.265604Z","shell.execute_reply":"2021-06-20T00:40:05.683847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- water-cement ratio composite feature doesn't have influence on concrete strength, however age less than 2 months and 1 months have stronger influence on strengh","metadata":{}},{"cell_type":"code","source":"# As explained above, water to binder ratio also can be computed to obtain another composite feature in the dataset\n\n# Take copy of the data set prepared for Feature Engineerting above (this has the additional categorization of days column)\ncDataFE_unscaled_wb_ratio = cDataFE.copy()\n\n# Insert the new composite feature to the unscaled dataset\ncDataFE_unscaled_wb_ratio.insert(cDataFE_unscaled_wb_ratio.shape[-1]-1,\n                                 'water-binder-ratio', \n                                 cDataFE_unscaled_wb_ratio['water']/(cDataFE_unscaled_wb_ratio['cement']+\n                                                                     cDataFE_unscaled_wb_ratio['ash']+\n                                                                     cDataFE_unscaled_wb_ratio['slag']))\n\n# Drop the original individual feature as the water/binder (consists of cement, ash & slag) ratio is introduced \n# as composite feature\ncDataFE_unscaled_wb_ratio.drop(['water','cement','ash','slag'], axis=1, inplace=True)\n\n# Let's scale the dataset\nfrom sklearn.preprocessing import MaxAbsScaler\n\nscaler1 = MaxAbsScaler() #Instanciate MaxAbsScaler\ncDataFE_unscaled_wb_ratio_copy = cDataFE_unscaled_wb_ratio.copy() \ncDataFE_scaled_wb_ratio = scaler1.fit_transform(cDataFE_unscaled_wb_ratio) # Scale through fit-transform\ncDataFE_unscaled_wb_ratio_copy.loc[:,:] = cDataFE_scaled_wb_ratio # Prepare the scaled dataset\ncDataFE_scaled_wb_ratio = cDataFE_unscaled_wb_ratio_copy.copy() # Name the scaled dataset\ncDataFE_scaled_wb_ratio.head() # View the scaled data set with a composite feature water/binder ratio","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:05.686262Z","iopub.execute_input":"2021-06-20T00:40:05.686547Z","iopub.status.idle":"2021-06-20T00:40:05.72034Z","shell.execute_reply.started":"2021-06-20T00:40:05.686519Z","shell.execute_reply":"2021-06-20T00:40:05.71928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the distribution of the new composite feature introduced\ncDataFE_scaled_wb_ratio['water-binder-ratio'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:05.721836Z","iopub.execute_input":"2021-06-20T00:40:05.72221Z","iopub.status.idle":"2021-06-20T00:40:05.733321Z","shell.execute_reply.started":"2021-06-20T00:40:05.722178Z","shell.execute_reply":"2021-06-20T00:40:05.732411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe the correlation of the variables through graphical Heat map\n# +ve and -ve numbers indicate how the variabes are correalted to each other\n\ncolormap = plt.cm.viridis # Color range to be used in heatmap\nplt.figure(figsize=(15,15))\nplt.title('Concrete properties Correlation of attributes - with water-binder ratio', y=1.05, size=19)\nsns.heatmap(cDataFE_scaled_wb_ratio.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:05.734939Z","iopub.execute_input":"2021-06-20T00:40:05.735328Z","iopub.status.idle":"2021-06-20T00:40:06.522832Z","shell.execute_reply.started":"2021-06-20T00:40:05.735248Z","shell.execute_reply":"2021-06-20T00:40:06.521679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above correlation matrix as well we see that water-binder ratio as a composite feature doesn't have strong influence on concrete strength","metadata":{}},{"cell_type":"code","source":"# Decide on complexity of the model, should it be simple linear model in terms of parameters or \n# would a quadratic or higher degree help","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.524134Z","iopub.execute_input":"2021-06-20T00:40:06.524459Z","iopub.status.idle":"2021-06-20T00:40:06.52814Z","shell.execute_reply.started":"2021-06-20T00:40:06.524427Z","shell.execute_reply":"2021-06-20T00:40:06.52707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's implement the linear model and check the model accuracy\n\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\n\n# Scale the original dataset cData - without dummy features for 'age' column is considered\ncData_scaled = cData.apply(zscore)\n\n# Scale the dataset cDataFE - with dummy features for 'age' column is considered\ncDataFE_scaled = cDataFE.apply(zscore)\n\n# Prepare dependent-independent data set - For original dataset\n# Copy all the predictor variables into X dataframe. Since 'strength' is dependent variable, let's drop it\nX_Linear_orig = cData_scaled.drop('strength', axis=1) # cDataFE dataset is considered with dummy columns for 'age' feature\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_Linear_orig = cData_scaled[['strength']]\n\n# Prepare dependent-independent data set - For dataset with age column dummy\n# Copy all the predictor variables into X dataframe. Since 'strength' is dependent variable, let's drop it\nX_Linear_wd = cDataFE_scaled.drop('strength', axis=1) # cDataFE dataset is considered with dummy columns for 'age' feature\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_Linear_wd = cDataFE_scaled[['strength']]\n\n# Prepare dependent-independent data set - For dataset with water-cement ratio\n# Copy all the predictor variables into X dataframe. Since 'strength' is dependent variable, let's drop it\nX_Linear_wc = cDataFE_scaled_wc_ratio.drop('strength', axis=1) # cDataFE dataset is considered with dummy columns for 'age' feature\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_Linear_wc = cDataFE_scaled_wc_ratio[['strength']]\n\n# Prepare dependent-independent data set - For dataset with water-binder ratio\n# Copy all the predictor variables into X dataframe. Since 'strength' is dependent variable, let's drop it\nX_Linear_wb = cDataFE_scaled_wb_ratio.drop('strength', axis=1) # cDataFE dataset is considered with dummy columns for 'age' feature\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_Linear_wb = cDataFE_scaled_wb_ratio[['strength']]\n\nfrom sklearn.model_selection import train_test_split # Import tain test split to split the dataset\n\n# Split X and y into training and test set in 70:30 ratio - For original dataset\nX_train_Linear_orig, X_test_Linear_orig, y_train_Linear_orig, y_test_Linear_orig = train_test_split(X_Linear_orig, y_Linear_orig, test_size=0.30 , \n                                                                            random_state=1)\n\n# Split X and y into training and test set in 70:30 ratio - For dataset with age column dummy\nX_train_Linear_wd, X_test_Linear_wd, y_train_Linear_wd, y_test_Linear_wd = train_test_split(X_Linear_wd, y_Linear_wd, test_size=0.30 , \n                                                                            random_state=1)\n\n# Split X and y into training and test set in 70:30 ratio - For dataset with water-cement ratio\nX_train_Linear_wc, X_test_Linear_wc, y_train_Linear_wc, y_test_Linear_wc = train_test_split(X_Linear_wc, y_Linear_wc, test_size=0.30 , \n                                                                            random_state=1)\n\n# Split X and y into training and test set in 70:30 ratio - For dataset with water-binder ratio\nX_train_Linear_wb, X_test_Linear_wb, y_train_Linear_wb, y_test_Linear_wb = train_test_split(X_Linear_wb, y_Linear_wb, test_size=0.30 , \n                                                                            random_state=1)\n\n# invoke the LinearRegression function and find the bestfit model on training data - For original dataset\nLinear_regression_model_orig = LinearRegression()\nLinear_regression_model_orig.fit(X_train_Linear_orig, y_train_Linear_orig)\n\n# invoke the LinearRegression function and find the bestfit model on training data - For dataset with age column dummy\nLinear_regression_model_wd = LinearRegression()\nLinear_regression_model_wd.fit(X_train_Linear_wd, y_train_Linear_wd)\n\n# invoke the LinearRegression function and find the bestfit model on training data - For dataset with water-cement ratio\nLinear_regression_model_wc = LinearRegression()\nLinear_regression_model_wc.fit(X_train_Linear_wc, y_train_Linear_wc)\n\n# invoke the LinearRegression function and find the bestfit model on training data - For dataset with water-binder ratio\nLinear_regression_model_wb = LinearRegression()\nLinear_regression_model_wb.fit(X_train_Linear_wb, y_train_Linear_wb)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.529751Z","iopub.execute_input":"2021-06-20T00:40:06.53005Z","iopub.status.idle":"2021-06-20T00:40:06.593839Z","shell.execute_reply.started":"2021-06-20T00:40:06.530021Z","shell.execute_reply":"2021-06-20T00:40:06.592717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print coefficients for each of the independent attributes\n\nprint(\"The coefficient for - original dataset\")\nfor i, col_name in enumerate(X_train_Linear_orig.columns):\n    print(\"{} = {}\".format(col_name, Linear_regression_model_orig.coef_[0][i]))\n\n# Print the intercept for the model\nintercept1 = Linear_regression_model_orig.intercept_[0]\nprint(\"The intercept of the model is {}\".format(intercept1))\nprint(\"===============================================================\")\n\nprint(\"The coefficient for - dataset with age column dummy\")\nfor i, col_name in enumerate(X_train_Linear_wd.columns):\n    print(\"{} = {}\".format(col_name, Linear_regression_model_wd.coef_[0][i]))\n\n# Print the intercept for the model\nintercept2 = Linear_regression_model_wd.intercept_[0]\nprint(\"The intercept of the model is {}\".format(intercept2))\nprint(\"===============================================================\")\n\nprint(\"The coefficient for - dataset with water-cement ratio\")\nfor i, col_name in enumerate(X_train_Linear_wc.columns):\n    print(\"{} = {}\".format(col_name, Linear_regression_model_wc.coef_[0][i]))\n\n# Print the intercept for the model\nintercept3 = Linear_regression_model_wc.intercept_[0]\nprint(\"The intercept of the model is {}\".format(intercept3))\nprint(\"===============================================================\")\n\nprint(\"The coefficient for - dataset with water-binder ratio\")\nfor i, col_name in enumerate(X_train_Linear_wb.columns):\n    print(\"{} = {}\".format(col_name, Linear_regression_model_wb.coef_[0][i]))\n\n# Print the intercept for the model\nintercept4 = Linear_regression_model_wb.intercept_[0]\nprint(\"The intercept of the model is {}\".format(intercept4))\nprint(\"===============================================================\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.595475Z","iopub.execute_input":"2021-06-20T00:40:06.595888Z","iopub.status.idle":"2021-06-20T00:40:06.61156Z","shell.execute_reply.started":"2021-06-20T00:40:06.595844Z","shell.execute_reply":"2021-06-20T00:40:06.610642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the Linear Regression model score - in Train data\nprint(\"Train data model score - for original dataset: \", Linear_regression_model_orig.score(X_train_Linear_orig, y_train_Linear_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with age column dummy: \", Linear_regression_model_wd.score(X_train_Linear_wd, y_train_Linear_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with water-cement ratio: \", Linear_regression_model_wc.score(X_train_Linear_wc, y_train_Linear_wc))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with water-binder ratio: \", Linear_regression_model_wb.score(X_train_Linear_wb, y_train_Linear_wb))\nprint(\"-----------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.613177Z","iopub.execute_input":"2021-06-20T00:40:06.613471Z","iopub.status.idle":"2021-06-20T00:40:06.645962Z","shell.execute_reply.started":"2021-06-20T00:40:06.613441Z","shell.execute_reply":"2021-06-20T00:40:06.645081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the Linear Regression model score - in Test data\nprint(\"Test data model score - for original dataset: \", Linear_regression_model_orig.score(X_test_Linear_orig, y_test_Linear_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with age column dummy: \", Linear_regression_model_wd.score(X_test_Linear_wd, y_test_Linear_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with water-cement ratio: \", Linear_regression_model_wc.score(X_test_Linear_wc, y_test_Linear_wc))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with water-binder ratio: \", Linear_regression_model_wb.score(X_test_Linear_wb, y_test_Linear_wb))\nprint(\"-----------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.647775Z","iopub.execute_input":"2021-06-20T00:40:06.648052Z","iopub.status.idle":"2021-06-20T00:40:06.67299Z","shell.execute_reply.started":"2021-06-20T00:40:06.648025Z","shell.execute_reply":"2021-06-20T00:40:06.671313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Based on the above analogy we see that when we introduced the dummy features for the different range of age column, we see a better score of the model accuracy in simple liner model\n- The other data set i.e. the original, dataset with water-cement ratio or water-binder ratio don't yeild better score\n- However the overall accuracy in the simple liner model is around 68%, but this is not significantly high. Hence shall continue to try other models to measure accuracy","metadata":{}},{"cell_type":"code","source":"# Compute the sum of squared errors by predicting value of y for test cases and \n# subtracting from the actual y for the test cases\nmse_orig = np.mean((Linear_regression_model_orig.predict(X_test_Linear_orig)-y_test_Linear_orig)**2)\nmse_wd = np.mean((Linear_regression_model_wd.predict(X_test_Linear_wd)-y_test_Linear_wd)**2)\nmse_wc = np.mean((Linear_regression_model_wc.predict(X_test_Linear_wc)-y_test_Linear_wc)**2)\nmse_wb = np.mean((Linear_regression_model_wb.predict(X_test_Linear_wb)-y_test_Linear_wb)**2)\n\nprint(\"MSE - for original dataset: \", mse_orig)\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"MSE - for dataset with age column dummy: \", mse_wd)\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"MSE - for dataset with water-cement ratio: \", mse_wc)\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"MSE - for dataset with water-binder ratio: \", mse_wb)\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# underroot of mean_sq_error is standard deviation i.e. avg variance between predicted and actual\nimport math\nprint(\"SRMSE - for original dataset: \", math.sqrt(mse_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"SRMSE - for dataset with age column dummy: \", math.sqrt(mse_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"SRMSE - for dataset with water-cement ratio: \", math.sqrt(mse_wc))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"SRMSE - for dataset with water-binder ratio: \", math.sqrt(mse_wb))\nprint(\"-----------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.674488Z","iopub.execute_input":"2021-06-20T00:40:06.674896Z","iopub.status.idle":"2021-06-20T00:40:06.700738Z","shell.execute_reply.started":"2021-06-20T00:40:06.674849Z","shell.execute_reply":"2021-06-20T00:40:06.699429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As the dataaset with additional dummy columns yeilds best scrore, \n# hence predict strength - for dataset with age column dummy\ny_pred_wd = Linear_regression_model_wd.predict(X_test_Linear_wd)\n\n# Since this is regression, plot the predicted y value vs actual y values for the test data\n# A good model's prediction will be close to actual leading to high R and R2 values\nplt.scatter(y_test_Linear_wd['strength'], y_pred_wd)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.704957Z","iopub.execute_input":"2021-06-20T00:40:06.705315Z","iopub.status.idle":"2021-06-20T00:40:06.9209Z","shell.execute_reply.started":"2021-06-20T00:40:06.705283Z","shell.execute_reply":"2021-06-20T00:40:06.920011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a Ridge model for each of the different data set and let's review corresponding coefficients\n\nfrom sklearn.linear_model import Ridge # Load Ridge library\n\n# Instantiate the model\nridge_orig = Ridge(alpha=.3) # for original data set\nridge_wd = Ridge(alpha=.3) # for dataset with age dummy\nridge_wc = Ridge(alpha=.3) # for dataset with water-cement ratio\nridge_wb = Ridge(alpha=.3) # for dataset with water-binder ratio\n\n# Fit the model\nridge_orig.fit(X_train_Linear_orig,y_train_Linear_orig)\nridge_wd.fit(X_train_Linear_wd,y_train_Linear_wd)\nridge_wc.fit(X_train_Linear_wc,y_train_Linear_wc)\nridge_wb.fit(X_train_Linear_wb,y_train_Linear_wb)\n\n# Display the co-efficiants\nprint (\"Ridge model - for original data set: \", (ridge_orig.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Ridge model - for dataset with age dummy: \", (ridge_wd.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Ridge model - for dataset with water-cement ratio: \", (ridge_wc.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Ridge model - for dataset with water-binder ratio: \", (ridge_wb.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.922486Z","iopub.execute_input":"2021-06-20T00:40:06.922792Z","iopub.status.idle":"2021-06-20T00:40:06.956707Z","shell.execute_reply.started":"2021-06-20T00:40:06.922762Z","shell.execute_reply":"2021-06-20T00:40:06.955904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a Lasso model for each of the different data set and let's review corresponding coefficients\n\nfrom sklearn.linear_model import Lasso # Load Ridge library\n\n# Instantiate the model\nlasso_orig = Lasso(alpha=.1) # for original data set\nlasso_wd = Lasso(alpha=.1) # for dataset with age dummy\nlasso_wc = Lasso(alpha=.1) # for dataset with water-cement ratio\nlasso_wb = Lasso(alpha=.1) # for dataset with water-binder ratio\n\n# Fit the model\nlasso_orig.fit(X_train_Linear_orig,y_train_Linear_orig)\nlasso_wd.fit(X_train_Linear_wd,y_train_Linear_wd)\nlasso_wc.fit(X_train_Linear_wc,y_train_Linear_wc)\nlasso_wb.fit(X_train_Linear_wb,y_train_Linear_wb)\n\n# Display the co-efficiants\nprint (\"Lasso model - for original data set: \", (lasso_orig.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Lasso model - for dataset with age dummy: \", (lasso_wd.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Lasso model - for dataset with water-cement ratio: \", (lasso_wc.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Lasso model - for dataset with water-binder ratio: \", (lasso_wb.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.95796Z","iopub.execute_input":"2021-06-20T00:40:06.958335Z","iopub.status.idle":"2021-06-20T00:40:06.992664Z","shell.execute_reply.started":"2021-06-20T00:40:06.958305Z","shell.execute_reply":"2021-06-20T00:40:06.991499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In Lasso model many of the coefficient for the associated properties becomes zero. It indicates the corresponding features from the dataset can be dropped as it doesn't have enough influence on the target feature 'strength' in this case\n- Ex: \n    - In the original data set - cement, water, coarseagg, fineagg and age have correlation with strength of the cement mixture \n    - In the data set with dummy - cement, water, coarseagg, fineagg and age_less than 30 days have correlation with strength of the cement mixture. Other age category can be dropped from the data set\n    - In the data set with water-cement ratio or water-binder ratio - none of the feature yeilds result to build a predictive model. The composite features doesn't benefit in the model building. Hence we shall not use these data set for further model building technique.","metadata":{}},{"cell_type":"code","source":"# Let's get the Ridge and Lasso model score for comparison\n\n# Get the Ridge model score - in Train data\nprint(\"Ridge Model score\")\nprint(\"=========================================================================================\")\nprint(\"Train data model score - for original dataset: \", ridge_orig.score(X_train_Linear_orig, y_train_Linear_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with age column dummy: \", ridge_wd.score(X_train_Linear_wd, y_train_Linear_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# Get the Ridge model score - in Test data\nprint(\"Test data model score - for original dataset: \", ridge_orig.score(X_test_Linear_orig, y_test_Linear_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with age column dummy: \", ridge_wd.score(X_test_Linear_wd, y_test_Linear_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# Get the Lasso model score - in Train data\nprint(\"Lasso Model score\")\nprint(\"=========================================================================================\")\nprint(\"Train data model score - for original dataset: \", lasso_orig.score(X_train_Linear_orig, y_train_Linear_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with age column dummy: \", lasso_wd.score(X_train_Linear_wd, y_train_Linear_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# Get the Ridge model score - in Test data\nprint(\"Test data model score - for original dataset: \", lasso_orig.score(X_test_Linear_orig, y_test_Linear_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with age column dummy: \", lasso_wd.score(X_test_Linear_wd, y_test_Linear_wd))\nprint(\"-----------------------------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:06.996323Z","iopub.execute_input":"2021-06-20T00:40:06.996643Z","iopub.status.idle":"2021-06-20T00:40:07.04872Z","shell.execute_reply.started":"2021-06-20T00:40:06.99661Z","shell.execute_reply":"2021-06-20T00:40:07.047743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The Ridge model accuracy for dataset with age dummy is still around 69% on test data, however lasso yeilds on 53% ","metadata":{}},{"cell_type":"code","source":"# It was evident earlier from the joint plot that independent variables have non-linear relation with the \n# dependent or target variable. Hence below building a ploynomial model with higher degree of diamesion \n# to evaluate the model score and compare\n\nfrom sklearn.preprocessing import PolynomialFeatures # Get the polynomial feature library\n\n# Instanstiate the ploynomial\npoly_orig = PolynomialFeatures(degree = 2, interaction_only=True) # Instantiate - for original data set\npoly_wd = PolynomialFeatures(degree = 2, interaction_only=True) # Instantiate - for data set with age dummy feature\n\n# Fit Transform the dataset\nX_poly_orig = poly_orig.fit_transform(X_Linear_orig) # for original data set\nX_poly_wd = poly_wd.fit_transform(X_Linear_wd) # for data set with age dummy feature\n\n# Split the data set\nX_train_poly_orig, X_test_poly_orig, y_train_poly_orig, y_test_poly_orig = train_test_split(X_poly_orig, y_Linear_orig, test_size=0.30, random_state=1)\nX_train_poly_wd, X_test_poly_wd, y_train_poly_wd, y_test_poly_wd = train_test_split(X_poly_wd, y_Linear_wd, test_size=0.30, random_state=1)\n\n# View the shape of the model after introducing the higher degree diamensions\nprint(\"Shape of the original data set: \", X_train_poly_orig.shape)\nprint(\"Shape of the data set with age dummy feature: \", X_train_poly_wd.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:07.05043Z","iopub.execute_input":"2021-06-20T00:40:07.050872Z","iopub.status.idle":"2021-06-20T00:40:07.07304Z","shell.execute_reply.started":"2021-06-20T00:40:07.050828Z","shell.execute_reply":"2021-06-20T00:40:07.071939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the linear model score with polynomial features introduced\n\n# Apply for Ridge model:\n# Instantiate the model\nridge_poly_orig = Ridge(alpha=.3) # for original data set\nridge_poly_wd = Ridge(alpha=.3) # for dataset with age dummy\n\n# Fit the model\nridge_poly_orig.fit(X_train_poly_orig,y_train_poly_orig)\nridge_poly_wd.fit(X_train_poly_wd,y_train_poly_wd)\n\n# Display the co-efficiants\nprint (\"Ridge model (polynomial) - for original data set: \", (ridge_poly_orig.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Ridge model (polynomial) - for dataset with age dummy: \", (ridge_poly_wd.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\n\n# Apply for the Lasso model:\n# Instantiate the model\nlasso_poly_orig = Lasso(alpha=.1) # for original data set\nlasso_poly_wd = Lasso(alpha=.1) # for dataset with age dummy\n\n# Fit the model\nlasso_poly_orig.fit(X_train_poly_orig,y_train_poly_orig)\nlasso_poly_wd.fit(X_train_poly_wd,y_train_poly_wd)\n\n# Display the co-efficiants\nprint (\"Lasso model (polynomial co-efficiants) - for original data set: \", (lasso_poly_orig.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\nprint (\"Lasso model (polynomial co-efficiants) - for dataset with age dummy: \", (lasso_poly_wd.coef_))\nprint(\"---------------------------------------------------------------------------------------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:07.074601Z","iopub.execute_input":"2021-06-20T00:40:07.074933Z","iopub.status.idle":"2021-06-20T00:40:07.142923Z","shell.execute_reply.started":"2021-06-20T00:40:07.074903Z","shell.execute_reply":"2021-06-20T00:40:07.141722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In Lasso most of the coefficiants become zero, indicating to drop those polynomial features","metadata":{}},{"cell_type":"code","source":"# Let's get the Ridge and Lasso model score for comparison\n\n# Get the Ridge model score - in Train data\nprint(\"Ridge Model (polynomial) score\")\nprint(\"=========================================================================================\")\nprint(\"Train data model score - for original dataset: \", ridge_poly_orig.score(X_train_poly_orig,y_train_poly_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with age column dummy: \", ridge_poly_wd.score(X_train_poly_wd,y_train_poly_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# Get the Ridge model score - in Test data\nprint(\"Test data model score - for original dataset: \", ridge_poly_orig.score(X_test_poly_orig,y_test_poly_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with age column dummy: \", ridge_poly_wd.score(X_test_poly_wd,y_test_poly_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# Get the Lasso model score - in Train data\nprint(\"Lasso Model (polynomial) score\")\nprint(\"=========================================================================================\")\nprint(\"Train data model score - for original dataset: \", lasso_poly_orig.score(X_train_poly_orig,y_train_poly_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Train data model score - for dataset with age column dummy: \", lasso_poly_wd.score(X_train_poly_wd,y_train_poly_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n\n# Get the Ridge model score - in Test data\nprint(\"Test data model score - for original dataset: \", lasso_poly_orig.score(X_train_poly_orig,y_train_poly_orig))\nprint(\"-----------------------------------------------------------------------------------------\")\nprint(\"Test data model score - for dataset with age column dummy: \", lasso_poly_wd.score(X_train_poly_wd,y_train_poly_wd))\nprint(\"-----------------------------------------------------------------------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:07.144805Z","iopub.execute_input":"2021-06-20T00:40:07.146771Z","iopub.status.idle":"2021-06-20T00:40:07.227348Z","shell.execute_reply.started":"2021-06-20T00:40:07.146718Z","shell.execute_reply":"2021-06-20T00:40:07.226093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- After introducing the higher degree dimension through polynomial feature and applying Ridge and Lasso we observed following -\n    - The Ridge model score jumped to 72% on Train and 70% on Test for dataset with age dummy columns\n    - The Lasso score improved as well to 55% on test data\n    \n- Few general observation:\n    - In all cases model score is better on the data set where Age column (i.e. # of days to achieve a concrete strength level) is further classified into 4 groups. \n    - Original dataset didn't performed that well on these models\n    - Applying higher degree polynomial feature improves the model accuracy as score compared against Ridge & Lasso model","metadata":{}},{"cell_type":"code","source":"# Explore for gaussians. If data is likely to be a mix of gaussians,explore individual clusters and \n# present your findings in terms of the independent attributes and their suitability to predict strength","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:07.229046Z","iopub.execute_input":"2021-06-20T00:40:07.22947Z","iopub.status.idle":"2021-06-20T00:40:07.235783Z","shell.execute_reply.started":"2021-06-20T00:40:07.229428Z","shell.execute_reply":"2021-06-20T00:40:07.2344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display each of the feature distribution in the original dataset to view the gaussians\n\nsns.distplot(cData['cement'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['slag'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['ash'], kde=False)\nplt.show()\nsns.distplot(cData['water'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['superplastic'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['coarseagg'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['fineagg'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['age'], hist=False, kde=True)\nplt.show()\nsns.distplot(cData['strength'], hist=False, kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:07.238885Z","iopub.execute_input":"2021-06-20T00:40:07.239635Z","iopub.status.idle":"2021-06-20T00:40:09.160797Z","shell.execute_reply.started":"2021-06-20T00:40:07.239587Z","shell.execute_reply":"2021-06-20T00:40:09.159665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above plot we see the following gaussians -\n    - Cement, fineaggregator are almost normally distributed\n    - slag, ash and superplastic have a high peak indicating the data needs up/down sampling in the model building technique\n    - Age, water, coarseagg have multiple gaussians. Hence Let's review the number of unique values in the dataset","metadata":{}},{"cell_type":"code","source":"print(\"Print the unique values in water column: \", len(cData.water.unique()))\nprint(\"Print the unique values in coarseagg column: \", len(cData.coarseagg.unique()))\nprint(\"Print the unique values in Age column: \", len(cData.age.unique()))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.162509Z","iopub.execute_input":"2021-06-20T00:40:09.162923Z","iopub.status.idle":"2021-06-20T00:40:09.170539Z","shell.execute_reply.started":"2021-06-20T00:40:09.162879Z","shell.execute_reply":"2021-06-20T00:40:09.169571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Though water and coarse aggregator have multiple gaussians, these features have too many unique values to make further subgroups\n- Applied the subgroup technique on the Age column and observed earlier how did it benefit in improving the model accuracy","metadata":{}},{"cell_type":"markdown","source":"- The target column strength is a continuous variable and the highest model score we observed for simple linear regression was 70% with higher degree of dimension introduced.\n- In order to apply further models to predict the strength, let's review the distribution of this feature and if it can be grouped further.\n- From the 5 point summary of Strength feature, we see following:\n    - mean: 35.817961\n    - standard deviation: 16.705742\n    - min: 2.33\n    - Q1: 23.710\n    - Q2: 34.445\n    - Q3: 46.135\n    - Max: 82.6\n    - Number of unique values:845\n- Based on the above oservation, breaking the strength feature into following 3 groups:\n    - strength level 1 (value between 2.33 and 23)\n    - strength level 2 (value between 23 and 46)\n    - strength level 3 (value between 46 and above)\n- Also get the corresponding count in each level of strength    ","metadata":{}},{"cell_type":"code","source":"# Let's categorize the Strength column as explained above.\n# As a first steps take a copy of the dataset with dummies\ncDataFES = cDataFE.copy() # cDataFES denotes Concrete dataset for Feature Engineering and for Strength dummies\n\nj = 0 # Instantiate the iterator\n\nfor i in cDataFES['strength']: # Loop through the strength column to find the match defined in below conditions\n    if i > 0 and i <=23.0:        \n        cDataFES.loc[j,'strength'] = 1 # Denote the strength level 1 (value between 2.33 and 23) with 1\n        j +=1\n    elif i > 23.0 and i <= 46.0:\n        cDataFES.loc[j,'strength'] = 2 # Denote the strength level 2 (value between 23 and 46) with 2\n        j +=1    \n    elif i > 46.0:\n        cDataFES.loc[j,'strength'] = 3 # Denote the strength level 3 (value between 46 and above) with 4\n        j +=1","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.172179Z","iopub.execute_input":"2021-06-20T00:40:09.1726Z","iopub.status.idle":"2021-06-20T00:40:09.41664Z","shell.execute_reply.started":"2021-06-20T00:40:09.17255Z","shell.execute_reply":"2021-06-20T00:40:09.415535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cDataFES.head() # View the dataset after dividing the strength into 3 categories","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.41834Z","iopub.execute_input":"2021-06-20T00:40:09.418789Z","iopub.status.idle":"2021-06-20T00:40:09.437828Z","shell.execute_reply.started":"2021-06-20T00:40:09.41874Z","shell.execute_reply":"2021-06-20T00:40:09.436824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now replace the categorical value with the target category of the date range\ncDataFES['strength'] = cDataFES['strength'].replace({1: 'level 1', \n                                         2: 'level 2', \n                                         3: 'level 3'})","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.439001Z","iopub.execute_input":"2021-06-20T00:40:09.439325Z","iopub.status.idle":"2021-06-20T00:40:09.449473Z","shell.execute_reply.started":"2021-06-20T00:40:09.439296Z","shell.execute_reply":"2021-06-20T00:40:09.448596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dummy properties for age column\ncDataFES = pd.get_dummies(cDataFES, columns=['strength'])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.450498Z","iopub.execute_input":"2021-06-20T00:40:09.450792Z","iopub.status.idle":"2021-06-20T00:40:09.468498Z","shell.execute_reply.started":"2021-06-20T00:40:09.450762Z","shell.execute_reply":"2021-06-20T00:40:09.467624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look into the dataset now\ncDataFES.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.46945Z","iopub.execute_input":"2021-06-20T00:40:09.469749Z","iopub.status.idle":"2021-06-20T00:40:09.495259Z","shell.execute_reply.started":"2021-06-20T00:40:09.46972Z","shell.execute_reply":"2021-06-20T00:40:09.494259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the distribution of strength level in the dataset\nprint(\"Strength Level 1: \", len(cDataFES.loc[cDataFES['strength_level 1'] == 1]))\nprint(\"Strength Level 2: \", len(cDataFES.loc[cDataFES['strength_level 2'] == 1]))\nprint(\"Strength Level 3: \", len(cDataFES.loc[cDataFES['strength_level 3'] == 1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.496961Z","iopub.execute_input":"2021-06-20T00:40:09.497269Z","iopub.status.idle":"2021-06-20T00:40:09.510734Z","shell.execute_reply.started":"2021-06-20T00:40:09.49724Z","shell.execute_reply":"2021-06-20T00:40:09.509919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on the cluster obtained above for different strength level, in the upcoming model building technique we shall \n# see how individual features have strong/less influence on concrete strength\n\n# Let's obtain feature importance for the individual features and present your findings\n\n# Build the Decision Tree classifier, assess model score and evaluate feature importance\n\n# Let's build the Decision Tree Model\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Make a copy of the data set and scale it\ncDataFES_scaled = cDataFES.apply(zscore)\n\n# Prepare dependent-independent data set\n# Copy all the predictor variables into X dataframe. Since 'strength' is dependent variable, let's drop it\nX_sl1 = cDataFES_scaled.drop(['strength_level 1','strength_level 2','strength_level 3'], axis=1) # for strength_level 1\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_sl1 = cDataFES[['strength_level 1']]\n\nX_sl2 = cDataFES_scaled.drop(['strength_level 1','strength_level 2','strength_level 3'], axis=1) # for strength_level 2\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_sl2 = cDataFES[['strength_level 2']]\n\nX_sl3 = cDataFES_scaled.drop(['strength_level 1','strength_level 2','strength_level 3'], axis=1) # for strength_level 3\n# Copy the target variable 'strength' column alone into the y dataframe. This is the dependent variable\ny_sl3 = cDataFES[['strength_level 3']]\n\n\n# Split X and y into training and test set in 70:30 ratio - for strength_level 1\nX_train_sl1, X_test_sl1, y_train_sl1, y_test_sl1 = train_test_split(X_sl1, y_sl1, test_size=0.30 , \n                                                                            random_state=1)\n\n# Split X and y into training and test set in 70:30 ratio - for strength_level 2\nX_train_sl2, X_test_sl2, y_train_sl2, y_test_sl2 = train_test_split(X_sl2, y_sl2, test_size=0.30 , \n                                                                            random_state=1)\n\n# Split X and y into training and test set in 70:30 ratio - for strength_level 3\nX_train_sl3, X_test_sl3, y_train_sl3, y_test_sl3 = train_test_split(X_sl3, y_sl3, test_size=0.30 , \n                                                                            random_state=1)\n\n#considered the Gini criteria to take decision. Other option was to use Entropy\ncDataDecisionTree_sl1 = DecisionTreeClassifier(criterion = 'gini', random_state=1) # for strength_level 1\ncDataDecisionTree_sl2 = DecisionTreeClassifier(criterion = 'gini', random_state=1) # for strength_level 2\ncDataDecisionTree_sl3 = DecisionTreeClassifier(criterion = 'gini', random_state=1) # for strength_level 3\n\n# Fit the model\ncDataDecisionTree_sl1.fit(X_train_sl1, y_train_sl1) # for strength_level 1\ncDataDecisionTree_sl2.fit(X_train_sl2, y_train_sl2) # for strength_level 2\ncDataDecisionTree_sl3.fit(X_train_sl3, y_train_sl3) # for strength_level 3\n\n# Get the decision tree score on Train and Test data\nprint(\"Decision Tree Score (on Train data - For strength_level 1): \", cDataDecisionTree_sl1.score(X_train_sl1, y_train_sl1))\nprint(\"Decision Tree Score (on Test data - For strength_level 1): \", cDataDecisionTree_sl1.score(X_test_sl1, y_test_sl1))\n\nprint(\"Decision Tree Score (on Train data - For strength_level 2): \", cDataDecisionTree_sl2.score(X_train_sl2, y_train_sl2))\nprint(\"Decision Tree Score (on Test data - For strength_level 2): \", cDataDecisionTree_sl2.score(X_test_sl2, y_test_sl2))\n\nprint(\"Decision Tree Score (on Train data - For strength_level 3): \", cDataDecisionTree_sl3.score(X_train_sl3, y_train_sl3))\nprint(\"Decision Tree Score (on Test data - For strength_level 3): \", cDataDecisionTree_sl3.score(X_test_sl3, y_test_sl3))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.511989Z","iopub.execute_input":"2021-06-20T00:40:09.512662Z","iopub.status.idle":"2021-06-20T00:40:09.608311Z","shell.execute_reply.started":"2021-06-20T00:40:09.51262Z","shell.execute_reply":"2021-06-20T00:40:09.607208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importance of features in the tree building based in Gini value\nprint(\"Feature Importance - considering model for Strenth Level 1\")\nprint(\"===========================================================\")\nprint (pd.DataFrame(cDataDecisionTree_sl1.feature_importances_, columns = [\"Imp\"], index = X_train_sl1.columns))\nprint(\" \")\nprint(\"Feature Importance - considering model for Strenth Level 2\")\nprint(\"===========================================================\")\nprint (pd.DataFrame(cDataDecisionTree_sl2.feature_importances_, columns = [\"Imp\"], index = X_train_sl2.columns))\nprint(\" \")\nprint(\"Feature Importance - considering model for Strenth Level 3\")\nprint(\"===========================================================\")\nprint (pd.DataFrame(cDataDecisionTree_sl3.feature_importances_, columns = [\"Imp\"], index = X_train_sl3.columns))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.609716Z","iopub.execute_input":"2021-06-20T00:40:09.610024Z","iopub.status.idle":"2021-06-20T00:40:09.626345Z","shell.execute_reply.started":"2021-06-20T00:40:09.609994Z","shell.execute_reply":"2021-06-20T00:40:09.625233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above feature importance values, following observations can be made:\n    - cement, coarseagg, fineagg, water, slag and within a month will have importance in order to reach the strength level 1 (i.e. upto 23 unit)\n    - cement, coarseagg, fineagg, water, ash, slag and within a month or 3 month will have importance in order to reach the strength level 2 (i.e. between 23 and 46 unit). At this level of strength to achieve, ash will have influence on the concrete strength\n    - cement, fineagg, water, slag, ash, coarseagg,and within a month will have importance in order to reach the strength level 3 (i.e. more than 46 unit). Coarseagg has less importance to achieve this highest level of strength","metadata":{}},{"cell_type":"code","source":"# Let's review below models and apply technique to confirm which will be suitable for this project\n# Refer AUC-ROC implementation to identify which model to selct","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.627811Z","iopub.execute_input":"2021-06-20T00:40:09.628292Z","iopub.status.idle":"2021-06-20T00:40:09.639278Z","shell.execute_reply.started":"2021-06-20T00:40:09.628249Z","shell.execute_reply":"2021-06-20T00:40:09.638116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's implement Logistic and SVM model for the datasets with specific strength level and measure the ROC/AUC to compare\n# the models and pick the best one\n\nfrom sklearn.linear_model import LogisticRegression # Load the logistic regression library\nfrom sklearn import svm # Load the support vector machine library\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_curve, auc\nrandom_state = np.random.RandomState(0)\n\n# Set up the dependent and independent variables\narray_x_data_scaled = cDataFES_scaled.values\nX = array_x_data_scaled[:,0:11]\n\narray_y_data = cDataFES.values\nY_sl1 = array_y_data[:,11]\nY_sl2 = array_y_data[:,12]\nY_sl3 = array_y_data[:,13]\n\n# Split the dataset\n# suffix 'mt' = model tuning\nX_train_mt_sl1, X_test_mt_sl1, y_train_mt_sl1, y_test_mt_sl1 = train_test_split(X, Y_sl1, test_size=0.30, random_state=1) # for strength level 1\nX_train_mt_sl2, X_test_mt_sl2, y_train_mt_sl2, y_test_mt_sl2 = train_test_split(X, Y_sl2, test_size=0.30, random_state=1) # for strength level 2\nX_train_mt_sl3, X_test_mt_sl3, y_train_mt_sl3, y_test_mt_sl3 = train_test_split(X, Y_sl3, test_size=0.30, random_state=1) # for strength level 3\n\n# Instantiate the models:\n# For logistic regression\nLR_sl1 = LogisticRegression() # for strength level 1\nLR_sl2 = LogisticRegression() # for strength level 2\nLR_sl3 = LogisticRegression() # for strength level 3\n# For SVM\nSVM_sl1 = svm.SVC(kernel='linear', probability=True) # for strength level 1\nSVM_sl2 = svm.SVC(kernel='linear', probability=True) # for strength level 2\nSVM_sl3 = svm.SVC(kernel='linear', probability=True) # for strength level 3\n\n# Fit the models & get the model score\n# For LR model\nprob_score_LR_sl1 = LR_sl1.fit(X_train_mt_sl1, y_train_mt_sl1).predict_proba(X_test_mt_sl1)\nprob_score_LR_sl2 = LR_sl2.fit(X_train_mt_sl2, y_train_mt_sl2).predict_proba(X_test_mt_sl2)\nprob_score_LR_sl3 = LR_sl3.fit(X_train_mt_sl3, y_train_mt_sl3).predict_proba(X_test_mt_sl3)\n# For SVM model\nprob_score_SVM_sl1 = SVM_sl1.fit(X_train_mt_sl1, y_train_mt_sl1).predict_proba(X_test_mt_sl1)\nprob_score_SVM_sl2 = SVM_sl2.fit(X_train_mt_sl2, y_train_mt_sl2).predict_proba(X_test_mt_sl2)\nprob_score_SVM_sl3 = SVM_sl3.fit(X_train_mt_sl3, y_train_mt_sl3).predict_proba(X_test_mt_sl3)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.64087Z","iopub.execute_input":"2021-06-20T00:40:09.641243Z","iopub.status.idle":"2021-06-20T00:40:09.903931Z","shell.execute_reply.started":"2021-06-20T00:40:09.641209Z","shell.execute_reply":"2021-06-20T00:40:09.902936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the model score: LR model:\nprint(\"Logistic Regression model score (in Train data) for strength level 1: \", LR_sl1.score(X_train_mt_sl1, y_train_mt_sl1))\nprint(\"Logistic Regression model score (in Test data) for strength level 1: \", LR_sl1.score(X_test_mt_sl1, y_test_mt_sl1))\nprint(\"======================================================================================================\")\nprint(\"Logistic Regression model score (in Train data) for strength level 2: \", LR_sl2.score(X_train_mt_sl2, y_train_mt_sl2))\nprint(\"Logistic Regression model score (in Test data) for strength level 2: \", LR_sl2.score(X_test_mt_sl2, y_test_mt_sl2))\nprint(\"======================================================================================================\")\nprint(\"Logistic Regression model score (in Train data) for strength level 3: \", LR_sl3.score(X_train_mt_sl3, y_train_mt_sl3))\nprint(\"Logistic Regression model score (in Test data) for strength level 3: \", LR_sl3.score(X_test_mt_sl3, y_test_mt_sl3))\nprint(\"======================================================================================================\")\n# Let's view the model score: SVM model:\nprint(\"SVM model score (in Train data) for strength level 1: \", SVM_sl1.score(X_train_mt_sl1, y_train_mt_sl1))\nprint(\"SVM model score (in Test data) for strength level 1: \", SVM_sl1.score(X_test_mt_sl1, y_test_mt_sl1))\nprint(\"======================================================================================================\")\nprint(\"SVM model score (in Train data) for strength level 2: \", SVM_sl2.score(X_train_mt_sl2, y_train_mt_sl2))\nprint(\"SVM model score (in Test data) for strength level 2: \", SVM_sl2.score(X_test_mt_sl2, y_test_mt_sl2))\nprint(\"======================================================================================================\")\nprint(\"SVM model score (in Train data) for strength level 3: \", SVM_sl3.score(X_train_mt_sl3, y_train_mt_sl3))\nprint(\"SVM model score (in Test data) for strength level 3: \", SVM_sl3.score(X_test_mt_sl3, y_test_mt_sl3))\nprint(\"======================================================================================================\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.905182Z","iopub.execute_input":"2021-06-20T00:40:09.90544Z","iopub.status.idle":"2021-06-20T00:40:09.957677Z","shell.execute_reply.started":"2021-06-20T00:40:09.905415Z","shell.execute_reply":"2021-06-20T00:40:09.956732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute ROC curve and area the curve for logistic model\nfpr1_LR, tpr1_LR, thresholds1_LR = roc_curve(y_test_mt_sl1, prob_score_LR_sl1[:, 1])\nroc_auc1_LR = auc(fpr1_LR, tpr1_LR)\nprint(\"Area under the ROC curve - for strength level 1: %f\" % roc_auc1_LR)\n\nfpr2_LR, tpr2_LR, thresholds2_LR = roc_curve(y_test_mt_sl2, prob_score_LR_sl2[:, 1])\nroc_auc2_LR = auc(fpr2_LR, tpr2_LR)\nprint(\"Area under the ROC curve - for strength level 2: %f\" % roc_auc2_LR)\n\nfpr3_LR, tpr3_LR, thresholds3_LR = roc_curve(y_test_mt_sl3, prob_score_LR_sl3[:, 1])\nroc_auc3_LR = auc(fpr3_LR, tpr3_LR)\nprint(\"Area under the ROC curve - for strength level 3: %f\" % roc_auc3_LR)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.959353Z","iopub.execute_input":"2021-06-20T00:40:09.959783Z","iopub.status.idle":"2021-06-20T00:40:09.969707Z","shell.execute_reply.started":"2021-06-20T00:40:09.959739Z","shell.execute_reply":"2021-06-20T00:40:09.96848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute ROC curve and area the curve for SVM model\nfpr1_SVM, tpr1_SVM, thresholds1_SVM = roc_curve(y_test_mt_sl1, prob_score_SVM_sl1[:, 1])\nroc_auc1_SVM = auc(fpr1_SVM, tpr1_SVM)\nprint(\"Area under the ROC curve - for strength level 1: %f\" % roc_auc1_SVM)\n\nfpr2_SVM, tpr2_SVM, thresholds2_SVM = roc_curve(y_test_mt_sl2, prob_score_SVM_sl2[:, 1])\nroc_auc2_SVM = auc(fpr2_SVM, tpr2_SVM)\nprint(\"Area under the ROC curve - for strength level 2: %f\" % roc_auc2_SVM)\n\nfpr3_SVM, tpr3_SVM, thresholds3_SVM = roc_curve(y_test_mt_sl3, prob_score_SVM_sl3[:, 1])\nroc_auc3_SVM = auc(fpr3_SVM, tpr3_SVM)\nprint(\"Area under the ROC curve - for strength level 3: %f\" % roc_auc3_SVM)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.971427Z","iopub.execute_input":"2021-06-20T00:40:09.971909Z","iopub.status.idle":"2021-06-20T00:40:09.98511Z","shell.execute_reply.started":"2021-06-20T00:40:09.971866Z","shell.execute_reply":"2021-06-20T00:40:09.984039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Not a significant difference found for the AUC between Logistic and SVM model","metadata":{}},{"cell_type":"code","source":"# Depict the ROC curve and determine better model at the same level of strength\n\nimport pylab as pl\n\n# Plot the curve - for LR vs SVM for strength level 1\npl.clf()\npl.plot(fpr1_LR, tpr1_LR, label='ROC curve for logistic at strength level 1 (area = %0.2f)' % roc_auc1_LR)\npl.plot(fpr1_SVM, tpr1_SVM, label='ROC curve for SVC at strength level 1 (area = %0.2f)' % roc_auc1_SVM)\npl.plot([0, 1], [0, 1], 'k--')\npl.xlim([0.0, 1.0])\npl.ylim([0.0, 1.0])\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('Receiverrating characteristic example')\npl.legend(loc=\"lower right\")\npl.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:09.98637Z","iopub.execute_input":"2021-06-20T00:40:09.98673Z","iopub.status.idle":"2021-06-20T00:40:10.341406Z","shell.execute_reply.started":"2021-06-20T00:40:09.9867Z","shell.execute_reply":"2021-06-20T00:40:10.34043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From above plot it shows either Logistic regression or SVM model yeilds same result to predict concrete strength level 1","metadata":{}},{"cell_type":"code","source":"# Plot the curve - for LR vs SVM for strength level 2\npl.clf()\npl.plot(fpr2_LR, tpr2_LR, label='ROC curve for logistic at strength level 2 (area = %0.2f)' % roc_auc2_LR)\npl.plot(fpr2_SVM, tpr2_SVM, label='ROC curve for SVC at strength level 2 (area = %0.2f)' % roc_auc2_SVM)\npl.plot([0, 1], [0, 1], 'k--')\npl.xlim([0.0, 1.0])\npl.ylim([0.0, 1.0])\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('Receiverrating characteristic example')\npl.legend(loc=\"lower right\")\npl.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:10.342915Z","iopub.execute_input":"2021-06-20T00:40:10.343325Z","iopub.status.idle":"2021-06-20T00:40:10.561272Z","shell.execute_reply.started":"2021-06-20T00:40:10.343281Z","shell.execute_reply":"2021-06-20T00:40:10.560075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The overall predictibility score is low at strength level 2, however from above we can derive that LR model is better than SVM","metadata":{}},{"cell_type":"code","source":"# Plot the curve - for LR vs SVM for strength level 3\npl.clf()\npl.plot(fpr3_LR, tpr3_LR, label='ROC curve for logistic at strength level 3 (area = %0.2f)' % roc_auc3_LR)\npl.plot(fpr3_SVM, tpr3_SVM, label='ROC curve for SVC at strength level 3 (area = %0.2f)' % roc_auc3_SVM)\npl.plot([0, 1], [0, 1], 'k--')\npl.xlim([0.0, 1.0])\npl.ylim([0.0, 1.0])\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('Receiverrating characteristic example')\npl.legend(loc=\"lower right\")\npl.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:10.562479Z","iopub.execute_input":"2021-06-20T00:40:10.562763Z","iopub.status.idle":"2021-06-20T00:40:10.77755Z","shell.execute_reply.started":"2021-06-20T00:40:10.562734Z","shell.execute_reply":"2021-06-20T00:40:10.776182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The overall predictibility score is very good at strength level 3, however from above we can derive that LR model is better than SVM","metadata":{}},{"cell_type":"code","source":"# Let's implement k-neighbor classifier to see the best hyper paramer set, controlling of which would provie the best \n# result in model performance\n\nfrom sklearn.neighbors import KNeighborsClassifier # Import the library\n\n# Instantiate knn model for each of the data set\nknn_sl1 = KNeighborsClassifier() # for strength level 1\nknn_sl2 = KNeighborsClassifier() # for strength level 2\nknn_sl3 = KNeighborsClassifier() # for strength level 3\n\n# Fit the models & get the model score\nknn_sl1.fit(X_train_mt_sl1, y_train_mt_sl1)\nknn_sl2.fit(X_train_mt_sl2, y_train_mt_sl2)\nknn_sl3.fit(X_train_mt_sl3, y_train_mt_sl3)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:10.779204Z","iopub.execute_input":"2021-06-20T00:40:10.779666Z","iopub.status.idle":"2021-06-20T00:40:10.792943Z","shell.execute_reply.started":"2021-06-20T00:40:10.77962Z","shell.execute_reply":"2021-06-20T00:40:10.792187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the model score: KNN model:\nprint(\"KNN model score (in Train data) for strength level 1: \", knn_sl1.score(X_train_mt_sl1, y_train_mt_sl1))\nprint(\"KNN model score (in Test data) for strength level 1: \", knn_sl1.score(X_test_mt_sl1, y_test_mt_sl1))\nprint(\"======================================================================================================\")\nprint(\"KNN model score (in Train data) for strength level 2: \", knn_sl2.score(X_train_mt_sl2, y_train_mt_sl2))\nprint(\"KNN model score (in Test data) for strength level 2: \", knn_sl2.score(X_test_mt_sl2, y_test_mt_sl2))\nprint(\"======================================================================================================\")\nprint(\"KNN model score (in Train data) for strength level 3: \", knn_sl3.score(X_train_mt_sl3, y_train_mt_sl3))\nprint(\"KNN model score (in Test data) for strength level 3: \", knn_sl3.score(X_test_mt_sl3, y_test_mt_sl3))\nprint(\"======================================================================================================\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:10.794294Z","iopub.execute_input":"2021-06-20T00:40:10.794571Z","iopub.status.idle":"2021-06-20T00:40:10.95913Z","shell.execute_reply.started":"2021-06-20T00:40:10.794545Z","shell.execute_reply":"2021-06-20T00:40:10.9582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Take the list of parameter which will be used for tuning\nparam_grid = {'n_neighbors': list(range(1,9)),\n             'algorithm': ('auto', 'ball_tree', 'kd_tree' , 'brute') }\n\n# Instantiate the grid search algorithm to identify best fit parameter set\nfrom sklearn.model_selection import GridSearchCV\n\ngs_sl1 = GridSearchCV(knn_sl1,param_grid,cv=10) # for strength level 1\ngs_sl2 = GridSearchCV(knn_sl2,param_grid,cv=10) # for strength level 2\ngs_sl3 = GridSearchCV(knn_sl3,param_grid,cv=10) # for strength level 3\n\n# Fit the Grid Search model\ngs_sl1.fit(X_train_mt_sl1, y_train_mt_sl1)\ngs_sl2.fit(X_train_mt_sl2, y_train_mt_sl2)\ngs_sl3.fit(X_train_mt_sl3, y_train_mt_sl3)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:10.960872Z","iopub.execute_input":"2021-06-20T00:40:10.961292Z","iopub.status.idle":"2021-06-20T00:40:17.512287Z","shell.execute_reply.started":"2021-06-20T00:40:10.961249Z","shell.execute_reply":"2021-06-20T00:40:17.511231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the model score: GridSearch model:\nprint(\"Grid Search model score (in Train data) for strength level 1: \", gs_sl1.score(X_train_mt_sl1, y_train_mt_sl1))\nprint(\"Grid Search model score (in Test data) for strength level 1: \", gs_sl1.score(X_test_mt_sl1, y_test_mt_sl1))\nprint(\"======================================================================================================\")\nprint(\"Grid Search model score (in Train data) for strength level 2: \", gs_sl2.score(X_train_mt_sl2, y_train_mt_sl2))\nprint(\"Grid Search model score (in Test data) for strength level 2: \", gs_sl2.score(X_test_mt_sl2, y_test_mt_sl2))\nprint(\"======================================================================================================\")\nprint(\"Grid Search model score (in Train data) for strength level 3: \", gs_sl3.score(X_train_mt_sl3, y_train_mt_sl3))\nprint(\"Grid Search model score (in Test data) for strength level 3: \", gs_sl3.score(X_test_mt_sl3, y_test_mt_sl3))\nprint(\"======================================================================================================\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.513981Z","iopub.execute_input":"2021-06-20T00:40:17.514691Z","iopub.status.idle":"2021-06-20T00:40:17.712008Z","shell.execute_reply.started":"2021-06-20T00:40:17.514643Z","shell.execute_reply":"2021-06-20T00:40:17.710935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the best parameter at each strength level which yeilds the best model score\nprint(\"Best parameter for strength level 1 dataset: \", gs_sl1.best_params_)\nprint(\"Best parameter for strength level 2 dataset: \", gs_sl2.best_params_)\nprint(\"Best parameter for strength level 3 dataset: \", gs_sl3.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.713422Z","iopub.execute_input":"2021-06-20T00:40:17.713684Z","iopub.status.idle":"2021-06-20T00:40:17.721148Z","shell.execute_reply.started":"2021-06-20T00:40:17.713658Z","shell.execute_reply":"2021-06-20T00:40:17.720082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the mean test score for each of the dataset\nprint(\"Mean test score for strength level 1\")\ngs_sl1.cv_results_['mean_test_score']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.722696Z","iopub.execute_input":"2021-06-20T00:40:17.723169Z","iopub.status.idle":"2021-06-20T00:40:17.735009Z","shell.execute_reply.started":"2021-06-20T00:40:17.72311Z","shell.execute_reply":"2021-06-20T00:40:17.734031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean test score for strength level 2\")\ngs_sl2.cv_results_['mean_test_score']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.736495Z","iopub.execute_input":"2021-06-20T00:40:17.736897Z","iopub.status.idle":"2021-06-20T00:40:17.746246Z","shell.execute_reply.started":"2021-06-20T00:40:17.736856Z","shell.execute_reply":"2021-06-20T00:40:17.745196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean test score for strength level 3\")\ngs_sl3.cv_results_['mean_test_score']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.75405Z","iopub.execute_input":"2021-06-20T00:40:17.754536Z","iopub.status.idle":"2021-06-20T00:40:17.762077Z","shell.execute_reply.started":"2021-06-20T00:40:17.754502Z","shell.execute_reply":"2021-06-20T00:40:17.761353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Measure the model performance range at 95% confidence level","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.763152Z","iopub.execute_input":"2021-06-20T00:40:17.763596Z","iopub.status.idle":"2021-06-20T00:40:17.769614Z","shell.execute_reply.started":"2021-06-20T00:40:17.763566Z","shell.execute_reply":"2021-06-20T00:40:17.768497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's build a model based on bootstrap sampling and measure the model score confidence range at 95%\n\n# Implement the boot starp\nfrom sklearn.utils import resample\n\nn_iterations = 1000              # Number of bootstrap samples to create\nn_size = int(len(cDataFES) * 0.50)   # picking only 50 % of the given data in every bootstrap sample\nvalues = cDataFES.values\n\n# run bootstrap\nstats_sl1 = list() # for Strength level 1\nstats_sl2 = list() # for Strength level 2\nstats_sl3 = list() # for Strength level 3\n\nfor i in range(n_iterations):\n    # prepare train and test sets at each of the strength level\n    train = resample(values, n_samples=n_size)  # Sampling with replacement\n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    # fit model\n    model_sl1 = DecisionTreeClassifier() # for Strength level 1\n    model_sl2 = DecisionTreeClassifier() # for Strength level 2 \n    model_sl3 = DecisionTreeClassifier() # for Strength level 3\n    model_sl1.fit(train[:,:-3], train[:,-3]) # considering strength level 1 as predictor\n    model_sl2.fit(train[:,:-3], train[:,-2]) # considering strength level 2 as predictor\n    model_sl3.fit(train[:,:-3], train[:,-1]) # considering strength level 3 as predictor\n    # evaluate model\n    predictions_sl1 = model_sl1.predict(test[:,:-3])\n    predictions_sl2 = model_sl2.predict(test[:,:-3])\n    predictions_sl3 = model_sl3.predict(test[:,:-3])\n    # Get the score\n    score_sl1 = accuracy_score(test[:,-3], predictions_sl1)\n    score_sl2 = accuracy_score(test[:,-2], predictions_sl2)\n    score_sl3 = accuracy_score(test[:,-1], predictions_sl3)\n    # Store the score into a list\n    stats_sl1.append(score_sl1)\n    stats_sl2.append(score_sl2)\n    stats_sl3.append(score_sl3)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:40:17.770899Z","iopub.execute_input":"2021-06-20T00:40:17.771206Z","iopub.status.idle":"2021-06-20T00:44:47.832216Z","shell.execute_reply.started":"2021-06-20T00:40:17.77115Z","shell.execute_reply":"2021-06-20T00:44:47.831083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot scores - for strength level 1\nplt.hist(stats_sl1)\nplt.show()\n# plot scores - for strength level 2\nplt.hist(stats_sl2)\nplt.show()\n# plot scores - for strength level 3\nplt.hist(stats_sl3)\nplt.show()\n\n# confidence intervals - for strength level 1\nalpha = 0.95                             # for 95% confidence \np1 = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower1 = max(0.0, np.percentile(stats_sl1, p1))  \np1 = (alpha+((1.0-alpha)/2.0)) * 100\nupper1 = min(1.0, np.percentile(stats_sl1, p1))\nprint('%.1f confidence interval for strength lelve 1 %.1f%% and %.1f%%' % (alpha*100, lower1*100, upper1*100))\n\n# confidence intervals - for strength level 2\np2 = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower2 = max(0.0, np.percentile(stats_sl2, p2))  \np2 = (alpha+((1.0-alpha)/2.0)) * 100\nupper2 = min(1.0, np.percentile(stats_sl2, p2))\nprint('%.1f confidence interval for strength lelve 2 %.1f%% and %.1f%%' % (alpha*100, lower2*100, upper2*100))\n\n# confidence intervals - for strength level 3\np3 = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower3 = max(0.0, np.percentile(stats_sl3, p3))  \np3 = (alpha+((1.0-alpha)/2.0)) * 100\nupper3 = min(1.0, np.percentile(stats_sl3, p3))\nprint('%.1f confidence interval for strength lelve 3 %.1f%% and %.1f%%' % (alpha*100, lower3*100, upper3*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T00:44:47.833601Z","iopub.execute_input":"2021-06-20T00:44:47.833919Z","iopub.status.idle":"2021-06-20T00:44:48.478395Z","shell.execute_reply.started":"2021-06-20T00:44:47.833887Z","shell.execute_reply":"2021-06-20T00:44:48.477308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From all previous analysis we can draw following inferences:\n    - Clustering on Age feature and additional dummy columns improved the predictibility on concrete strength through Linerar model\n    - Composite features i.e. water-cement or water-binder combination didn't improve model score to predict strength\n    - Ridge model highlighted the features which are important to predict strength\n    - Lasso model indicated which features to drop from the model building\n    - Polynomial parameters i.e. higher degree diamensions improved the model predictibility and both Ridge and Lasso indicated importance of the features\n    - Ridge model on ploynomial features improved the model score on dataset with age dummy columns\n    - Once strength column is clusered into 3 subsets,\n        - Decision Tree classifier model yeilds the best model accuracy to predict strength level 1 and 2\n        - Logistic regression, SVM and KNN model performed with higher score with respect to Linear model\n        - Grid search technique highlighted the hyper parameters to focus to get best model accuracy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}