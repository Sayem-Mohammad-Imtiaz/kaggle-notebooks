{"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","file_extension":".py","name":"python","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"aa5e259a-b4ed-4a1e-919a-a219d3a845ef","_uuid":"f54bb33fbd9c5b0f3e428a5bc29175fb7a90151f"},"cell_type":"markdown","source":"# Pearson's r with Health Searches\n\nThe [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a statistic which measures the degree of linear correlation between two variables. In this notebook, we'll use the Pearson correlation coefficient (as abbreviated Pearson's r, as in the notebook title) to guage the degree of linearity in Google health search volume.\n\nFirst though we'll have to understand what Pearson's correlation coefficient is. The following chart (from Wikipedia) demonstrates the correlation coefficients of various distributions:\n\n![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)\n\nThe more tightly linear two variables X and Y are, the closer Pearson's correlation coefficient (henceforth PCC) will be to either -1, if the relationship is negative, or +1, if the relationship is positive. Perfectly linearly uncorrelated numbers will recieve a PCC of 0.\n\nMathematically, Pearson's correlation coefficient can be stated as:\n\n$$\\rho_{X, Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_x \\sigma_y}$$\n\nPearson's correlation coefficient is *normalized covariance*. But what is covariance?\n\nTo understand this formula, start by looking at variance. The variance ($\\text{Var}(X)$ or $\\sigma^2_x$) of a distribution measures the average distance between a point created by the distribution and the distribution mean. A high variance means that the distribution is very spread out, but it can be difficult to interpret variance because it is a squared term.\n\nThe standard deviation of a distribution ($\\sigma(X)$ or $\\sigma_x$) is the square root of the variance. Variance is a squared property, while the standard deviation is a linear property. The utility of the standard deviation is demonstrated very nicely by the normal distribution, whose values can be measured in their unusualness very effectively using that number:\n\n![](https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg)\n\nSo for example a point as rare or rarer than one three standard deviations from the mean has only a 0.2% chance of occuring.\n\nCovariance measures the joint variance of two distributions. Like unary variance, covariance measures the square of the average distance between the points in a distribution and the mean of the distribution; but since there are now two variables, that measurement is now a distance in two dimensions!\n\nPearson's correlation coefficient normalizes covariance by dividing it by the standard deviation of either of the variables being measured. Simple!\n\nSo far we've been talking about a distribution, but when we work with data we don't know the underlying distribution: we just have a bunch of values. So we need to come up with a Pearson's correlation coefficient statistic, using statistics which measure the covariance and standard deviation of the variables in question. Hence we write:\n\n$$\\rho_{X, Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_x \\sigma_y} \\approx \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})^2}}$$"},{"metadata":{"_cell_guid":"6cd08ec6-4bab-4a8c-a728-dade4c63b40e","_uuid":"fe6fe284863985b3255d3367688a0bfa26d34d00"},"cell_type":"markdown","source":"Here's a hand implementation of this measurement:"},{"metadata":{"_cell_guid":"b9d7e15a-6b5f-4903-9224-b0b7c0df5eca","collapsed":true,"_uuid":"795db7d35cdd6a7ef8603ecc5481b948fd6931aa"},"execution_count":null,"source":"import numpy as np\n\ndef pearson_r(x, y):\n    x_bar, y_bar = np.mean(x), np.mean(y)\n    cov_est = np.sum((x - x_bar) * (y - y_bar))\n    std_x_est = np.sqrt(np.sum((x - x_bar)**2))\n    std_y_est = np.sqrt(np.sum((y - y_bar)**2))\n    return cov_est / (std_x_est * std_y_est)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2f7a011c-11b8-4c92-8cb3-d18452ec10dc","_uuid":"83f7b9cf077bb53752b8c39811b205ebef508e6c"},"cell_type":"markdown","source":"Correlation is a very fundamental property of your variables. It's a general purpose measurement that can be taken before you do any modeling. The higher the correlation between your target variable and your predictor variables, as a rule, the better your model will perform.\n\n## Application\n\nWe're now going to look at an application of the Pearson correlation coefficient to real data. We'll use the Google health search dataset, helpfully published on Kaggle by their News Labs, to do so.\n\nThe health search dataset includes an index of volumes of searches for various common medical topics throughout an assortment of areas in the United States. The data covers the period 2004 through 2017, with a different index value for every place and every year. This dataset is interesting because we naturally expect there to be a relationship between the search volume for a specific term in 2004 and 2017, but we expect this relationship to be *much stronger* the closer we get to 2017, maxing out in 2016.\n\nWe can verify that this is indeed the case by plotting the joint distributions of these variables using `seaborn`:"},{"metadata":{"_cell_guid":"389fc4e7-f4e5-4ce3-b0de-8718ed8569a3","collapsed":true,"_uuid":"b0edcaf699302544d352f2bebcaebcaafe9f106b"},"execution_count":null,"source":"import pandas as pd\nsearches = pd.read_csv(\"../input/RegionalInterestByConditionOverTime.csv\")\nsearches.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d9f1e3b5-8992-4690-b4aa-65f704cda19e","collapsed":true,"_uuid":"844d69e1f6ad6a322f040b7fa82adc2049fce1e1"},"execution_count":null,"source":"import seaborn as sns\nsns.jointplot('2004+cancer', '2017+cancer', data=searches)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"075e1445-573d-499d-b152-0e559e61daf9","collapsed":true,"_uuid":"20c66f5934fb183677e4ca7fc8fc4df2cbb62d2f"},"execution_count":null,"source":"import seaborn as sns\nsns.jointplot('2016+cancer', '2017+cancer', data=searches)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"247ba6af-dd4e-4520-9b8d-1611bab05fea","_uuid":"756524cdf7c322340494d53c311dcabe3e58b0cf"},"cell_type":"markdown","source":"As you can see, the second of these distributions is significantly closer to linear than the first one is!\n\nLet's see if our hypothesis holds for the `cancer` search topic for the rest of the years in the dataset."},{"metadata":{"_cell_guid":"4240ff8e-b1b6-4b83-b0e6-e837232d2906","collapsed":true,"_uuid":"d2e3b5904bad9b6b56b7ed4ce0b89292c061c6ca"},"execution_count":null,"source":"p_corrs = [pearson_r(searches[\"{0}+cancer\".format(year)], searches[\"2017+cancer\"]) for year \n           in range(2004, 2018)]","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"4ee6c01c-f530-4229-af66-6ec130dfe429","collapsed":true,"_uuid":"72aba34d89da04da1b4d97ed58c7a8ca87083857"},"execution_count":null,"source":"p_corrs = [pearson_r(searches[\"{0}+cancer\".format(year)], searches[\"2017+cancer\"]) for year \n           in range(2004, 2018)]\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\npd.Series(p_corrs, index=range(2004, 2018)).plot.line()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3b2c7619-74d6-477f-95ec-41537b74c8c0","_uuid":"09bc51cad04db5e9aea131dc16a3068fd9154484"},"cell_type":"markdown","source":"We are correct: the Pearson correlation coefficient (within a certain amount of randomness) does go up over time!\n\nThe health searches dataset gives us access to nine time series like this one. It was very taxing to plot all nine scatter plots over time. By plotting the correlation coefficents over time, we have a way to understand how similar medical searches today are to ones in the past that can be fit into a single line chart."},{"metadata":{"_cell_guid":"73b101a7-7dd8-4d45-ae4c-bf55e7379879","collapsed":true,"_uuid":"b2cb1ca1b73fe2da3c793fbc6be7c344b378f38b"},"execution_count":null,"source":"df = pd.DataFrame()\nfor topic in ['cancer', 'cardiovascular', 'stroke', 'depression', 'rehab', 'vaccine', \n              'diarrhea', 'obesity', 'diabetes']:\n    p_corrs = [pearson_r(searches[\"{0}+{1}\".format(year, topic)], \n                         searches[\"2017+{0}\".format(topic)]) for year in range(2004, 2018)]\n    df[topic] = p_corrs\n    \ndf.index = range(2004, 2018)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"21e8774c-3275-4cce-b475-cbe60d22dd7a","collapsed":true,"_uuid":"deb62c00b6caf422677116848c3ea9c0585be8b2"},"execution_count":null,"source":"df.plot.line(figsize=(12, 6), cmap='viridis', \n             title='Google Medical Searches by Similarity to 2017')","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"93f9a7ee-9aa2-40e5-97e4-7e6ba5f06c59","_uuid":"fd2ce8381ede75911897e647a3c039573d5c78a5"},"cell_type":"markdown","source":"All of the medical topics includes in this dataset are relatively samey with respect to the present search pattern within the last few years, but there is increasing amounts of variance going back to 2014.To interpret this chart, it's helpful to look at the distributions image from earlier:\n\n![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)\n\nA comparison between search today and search three to five years ago is much like looking at the blob in the second, 0.8 Pearson-scoring distribution in the top row of this picture. Pretty tight! Going back ten years, we get a coefficient of around 0.4, like that of the third scoring distribution in this chart; not nearly as good, but still with an identifiable trend.\n\nDiarrhea seems to be the medical topic whose searches are least similar today to what they were in the past, while (more weakly) cancer seems to be the most consistent.\n\nAnd note that the correlation coefficient of a variable to itself is always exactly 1."},{"metadata":{"_cell_guid":"afb7cbb7-b3e5-458f-9342-6a2e5201fe2a","_uuid":"a73521bf4b4a909de3cea7b7a7adb464b499745d"},"cell_type":"markdown","source":"### Closing remarks\n\nThere are a few more things that are worth stating about the Pearson correlation coefficient.\n\nFirst of all, it's important to note that the Pearson correlation coefficient is both scale and location invariant. Linear changes in the data, like increasing the slope of the data or moving every data point up by some number of units,  will not change the Pearson correlation coefficient. This is a helpful property because it makes the coefficient easier to interpret and compare over datasets in practice, but because of this the coefficient is not a good model fit metric.\n\nSecond of all, the Pearson correlation coefficient can only measure linear relationships between data. Relationships between data points that are more complex, such as polynomial relationships, will elude this measurement and have a coefficient near 0. This is demonstrated quite nicely in the third row of the diagram above, which shows various shapes that have near-0 correlation. In practice, always scatter plot your data to confirm what you think you are seeing!\n\nFinally, the Pearson is just one \"kind\" of correlation coefficient. A correlation coefficient is any measurement of data variable interdependence which has the property that it ranges between -1 and 1 for total dependence and 0 for total independence. Because of how popular the Pearson coefficient is, it is sometimes \"just\" called variable correlation, but it's not the only way to measure this. The second most popular correlation coefficient, the Spearmann correlation coefficient, is the subject of [another notebook](https://www.kaggle.com/residentmario/spearman-correlation-with-montreal-bikes/)!"}]}