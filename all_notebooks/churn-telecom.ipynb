{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nnp.random.seed(42)\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n%matplotlib inline\nimport seaborn as sns\nimport glob\npd.set_option('display.max_columns', None)\nimport missingno as msno\nfrom sklearn.impute import MissingIndicator,SimpleImputer\nfrom sklearn.preprocessing import RobustScaler,OneHotEncoder,LabelEncoder\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline,Pipeline,FeatureUnion,make_union\nimport imblearn\nfrom imblearn.over_sampling import ADASYN, RandomOverSampler, SMOTENC\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN,SMOTETomek\nimport os\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.base import TransformerMixin\nfrom sklearn.base import BaseEstimator\nimport tensorflow as tf\nfrom scipy.stats import chi2_contingency,ttest_ind\nimport category_encoders as CE\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8bb1162c-cccd-41cb-a528-60e4dd62f776","_cell_guid":"7c61106f-4bd7-4bea-abce-29f660e271c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:43:39.215461Z","iopub.execute_input":"2021-07-19T12:43:39.215833Z","iopub.status.idle":"2021-07-19T12:43:39.238946Z","shell.execute_reply.started":"2021-07-19T12:43:39.215802Z","shell.execute_reply":"2021-07-19T12:43:39.237564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets take a look at the data and their datatypes\ndf = pd.read_csv(\"/kaggle/input/datasets-for-churn-telecom/cell2celltrain.csv\",index_col=\"CustomerID\")\nprint(df.shape)\n\n# data clean up\ndef cleanup(df):\n    df = df.replace('Unknown',np.nan)\n    df['CreditRating'] = df['CreditRating'].apply(lambda v : int(v.split('-')[0]) if v else np.nan)\n    df['HandsetPrice'] = df['HandsetPrice'].astype('float')\n    return df\n\ndf = cleanup(df)\ndf.head().append(df.dtypes.rename('dtypes'))","metadata":{"_uuid":"1972c26e-512f-4504-a69b-81a5a1eed1aa","_cell_guid":"78b396fb-8803-4f10-8226-5dc473facdf3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:03:37.783322Z","iopub.execute_input":"2021-07-19T12:03:37.783707Z","iopub.status.idle":"2021-07-19T12:03:38.794274Z","shell.execute_reply.started":"2021-07-19T12:03:37.783675Z","shell.execute_reply":"2021-07-19T12:03:38.792596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes.value_counts()","metadata":{"_uuid":"5232dc73-bd7c-48d0-861d-b90abfaa509d","_cell_guid":"016f0c9d-5045-436f-93fc-5ec0829f51e2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:03:38.796223Z","iopub.execute_input":"2021-07-19T12:03:38.79656Z","iopub.status.idle":"2021-07-19T12:03:38.806832Z","shell.execute_reply.started":"2021-07-19T12:03:38.79653Z","shell.execute_reply":"2021-07-19T12:03:38.805805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target variable distribution : shows the dataset is imbalanced. \nsns.countplot(x=df.Churn)","metadata":{"_uuid":"bb3c77e5-491f-42e3-a9bc-acfe5fea096b","_cell_guid":"c319dfb3-8cfc-4bbe-bc87-ca4c4df21ebc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T09:40:55.363135Z","iopub.execute_input":"2021-07-19T09:40:55.363508Z","iopub.status.idle":"2021-07-19T09:40:55.547861Z","shell.execute_reply.started":"2021-07-19T09:40:55.363479Z","shell.execute_reply":"2021-07-19T09:40:55.546885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets visualize distribution of numeric variables with repect to Churn.\n## I don't see any striking patterns though\nplots_per_row = 5\n\nnumber_of_plots = df.select_dtypes(exclude=['object']).shape[-1]\n\nfig, axes = plt.subplots((number_of_plots//plots_per_row)+(number_of_plots%5!=0),plots_per_row) \n\naxes = axes.flatten()\ni = 0\nfor index,col in df.select_dtypes(exclude=['object']).columns.to_series().items():    \n    a = sns.histplot(data=df, x=col, hue=\"Churn\", ax=axes[i],kde=False,bins=100)\n    i+=1\n    \nfor i in range(1,len(axes)-number_of_plots+1):\n    fig.delaxes(axes[-i]) # remove empty subplot\n# plt.tight_layout()\n\nfig.set_figwidth(20)\nfig.set_figheight(25)\n\nplt.show()","metadata":{"_uuid":"2eb9e48a-b9d0-4b7f-878e-4501989e236f","_cell_guid":"c958135c-5bfb-403a-be2e-34b1ba6b3300","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T09:40:58.126949Z","iopub.execute_input":"2021-07-19T09:40:58.12746Z","iopub.status.idle":"2021-07-19T09:41:26.309119Z","shell.execute_reply.started":"2021-07-19T09:40:58.127414Z","shell.execute_reply":"2021-07-19T09:41:26.307867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now lets look at categorical variables\ndf.select_dtypes(include=['object']).describe()","metadata":{"_uuid":"4b2d99dd-9fd8-44b3-be49-6257f89dbd15","_cell_guid":"d494e3cc-9496-48fe-be4a-092d9a6e0aaf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T09:41:32.231699Z","iopub.execute_input":"2021-07-19T09:41:32.232097Z","iopub.status.idle":"2021-07-19T09:41:32.654552Z","shell.execute_reply.started":"2021-07-19T09:41:32.232053Z","shell.execute_reply":"2021-07-19T09:41:32.653438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ServiceArea has high cardinality (747 unique values). We will test later to check if ServiceArea has any statistically significant relationship with Churn","metadata":{}},{"cell_type":"code","source":"## visualize distribution of categorical variables with respect to Churn\n## I don't see any striking patterns here too\nplots_per_row = 5\n\nnumber_of_plots = df.select_dtypes(include=['object']).shape[-1]\n\nfig, axes = plt.subplots((number_of_plots//plots_per_row)+(number_of_plots%5!=0),plots_per_row) \n\naxes = axes.flatten()\ni = 0\nfor index,col in df.select_dtypes(include=['object']).columns.to_series().items():\n    \n    a = sns.countplot(x=df[col],hue=df.Churn,ax=axes[i])\n    a.tick_params(axis='x', labelrotation= 90)\n    i+=1\n    \nfor i in range(1,len(axes)-number_of_plots+1):\n    fig.delaxes(axes[-i]) # remove empty subplot\n# plt.tight_layout()\n\nfig.set_figwidth(20)\nfig.set_figheight(25)\n\nplt.show()","metadata":{"_uuid":"daeecf00-b912-4d7b-b553-2db2fc8f29ee","_cell_guid":"1fee2f37-3fe8-481e-a844-3094904883b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T09:41:36.711663Z","iopub.execute_input":"2021-07-19T09:41:36.712004Z","iopub.status.idle":"2021-07-19T09:41:56.493626Z","shell.execute_reply.started":"2021-07-19T09:41:36.711977Z","shell.execute_reply":"2021-07-19T09:41:56.492519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets look at count of null values in the dataset. Fortunately there are no nulls in the target variable\n\nnull_rows_selector = df.isnull().any(axis=1)\nnull_row_count = df[null_rows_selector].shape[0]\n\ndf_null = df.isnull().groupby(df.Churn).sum().transpose()\ndf_null['total'] = df.isnull().sum()\ndf_null['percent'] = (df_null['total']/len(df))*100\ndf_null = df_null[df_null.total!=0]\n\nprint(\"rows with null values:\",null_row_count,\", {:.2f}%\".format((null_row_count/len(df))*100))\nprint('columns with null values:',df_null.shape[0])\n\ndf_null","metadata":{"_uuid":"865ee2d4-b9f6-4b64-b188-c5802789ac07","_cell_guid":"f2da040d-bb9c-41d9-91ea-f156b6c73ffb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:28.523989Z","iopub.execute_input":"2021-07-19T12:04:28.524442Z","iopub.status.idle":"2021-07-19T12:04:28.919324Z","shell.execute_reply.started":"2021-07-19T12:04:28.524407Z","shell.execute_reply":"2021-07-19T12:04:28.917001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets focus on rows and columns with nulls, we see that values are not missing at random\nviz_null = df[null_rows_selector][df_null.index]\nmsno.matrix(viz_null)","metadata":{"_uuid":"932ae9ff-cbdb-4a6d-960f-3642fec2bb31","_cell_guid":"f0c26cee-89c9-4de2-910e-37ec348fc698","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:31.905068Z","iopub.execute_input":"2021-07-19T12:04:31.905435Z","iopub.status.idle":"2021-07-19T12:04:32.777469Z","shell.execute_reply.started":"2021-07-19T12:04:31.905404Z","shell.execute_reply":"2021-07-19T12:04:32.775875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check if churn distribution is different within rows with null values. And looks like it may be different.\npd.concat([df.Churn.value_counts(normalize=True).rename(\"Overall\"), df[null_rows_selector].Churn.value_counts(normalize=True).rename(\"within_null_rows\")],axis=1)","metadata":{"_uuid":"c2581dd3-7345-4123-9611-020698fa336b","_cell_guid":"05a7b28d-7c38-44b7-9bb0-9fc0745f8d42","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:34.784708Z","iopub.execute_input":"2021-07-19T12:04:34.785093Z","iopub.status.idle":"2021-07-19T12:04:34.839325Z","shell.execute_reply.started":"2021-07-19T12:04:34.785062Z","shell.execute_reply":"2021-07-19T12:04:34.837824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets do a chi square independence test to see if the difference in distributions is statistically significant\n\ncontingency_table = pd.concat([df.Churn.value_counts().rename(\"Overall\"), df[null_rows_selector].Churn.value_counts().rename(\"within_null_rows\")],axis=1).transpose()\ncontingency_table","metadata":{"_uuid":"857a50f1-fdb8-4ebe-b890-b676a431040e","_cell_guid":"891bd808-7232-4dfc-a754-d18468699bd4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:41.680303Z","iopub.execute_input":"2021-07-19T12:04:41.680689Z","iopub.status.idle":"2021-07-19T12:04:41.730943Z","shell.execute_reply.started":"2021-07-19T12:04:41.680658Z","shell.execute_reply":"2021-07-19T12:04:41.729842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# chi square independence test\n# Null Hypothesis HO: Distribution of Churn is independent of presence of null values\n\n\nstat, p, dof, expected = chi2_contingency(contingency_table.values)\n  \n# interpret p-value\nalpha = 0.05 # significance value for test\nprint(\"p value is \" + str(p))\n\nprint('Dependent (reject H0)') if p <= alpha else print('Independent (H0 holds true)')","metadata":{"_uuid":"4b26d980-bfc0-4dc2-a248-2896e2314e0b","_cell_guid":"845d5c5f-f0a2-4e4f-9513-3f475948ffe8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:43.926077Z","iopub.execute_input":"2021-07-19T12:04:43.926448Z","iopub.status.idle":"2021-07-19T12:04:43.936333Z","shell.execute_reply.started":"2021-07-19T12:04:43.92642Z","shell.execute_reply":"2021-07-19T12:04:43.934682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Presence of null values has a statistically significant effect on Churn. So I will try to include missing value indicator as additional feature while training","metadata":{}},{"cell_type":"code","source":"# Chi squared Independence test between categorical values and Churn. This will help to identify important variables on which Churn depends.\n# Based on the test, we can recommend to drop/include them in while training\ndef chi2test(X,y,alpha=0.05):\n    '''\n        X : dataframe \n        y : series\n    '''\n    target = y.name\n    print('ch2test with alpha',alpha)\n    test_df = []\n    for index,col in X.select_dtypes(include=['object']).columns.to_series().items():\n        df = pd.concat([y,X[col]],axis=1)\n        contingency_table = df.value_counts().rename(\"counts\").reset_index().pivot(index=target,columns=col,values='counts').fillna(0)\n        stat, p, dof, expected = chi2_contingency(contingency_table.values)\n        test_df.append([target,col,stat,p,'Dependent (reject H0)' if p <= alpha else 'Independent (H0 holds true)','include' if p <= alpha else 'drop'])\n        \n    test_df = pd.DataFrame(test_df,columns=[\"variable1\",\"variable2\",\"chi2-stat\",\"p-value\",\"result\",\"recommendation\"])\n    return test_df\n\nchi2test(df.drop('Churn',axis=1),df['Churn'])","metadata":{"_uuid":"56a34f02-76ac-46d9-9631-490902264893","_cell_guid":"978aa807-c307-43aa-93d8-a9c659dec64f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:47.089007Z","iopub.execute_input":"2021-07-19T12:04:47.089345Z","iopub.status.idle":"2021-07-19T12:04:47.538098Z","shell.execute_reply.started":"2021-07-19T12:04:47.089317Z","shell.execute_reply":"2021-07-19T12:04:47.536864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This test shows ServiceArea and Churn are dependent. So I will keep it for training but I will use the LeaveOneOut version of Target Encoder to deal with high cardinality of ServiceArea.","metadata":{"execution":{"iopub.status.busy":"2021-07-19T07:26:17.630618Z","iopub.execute_input":"2021-07-19T07:26:17.631038Z","iopub.status.idle":"2021-07-19T07:26:17.637735Z","shell.execute_reply.started":"2021-07-19T07:26:17.630998Z","shell.execute_reply":"2021-07-19T07:26:17.636458Z"}}},{"cell_type":"code","source":"# t test to check if means of a numerical variable differ significantly if Churn is different. \n\ndef t_test(X,y,alpha=0.05):   \n    target = y.name\n    print('t_test with alpha',alpha)\n    test_df = []\n    for index,col in X.select_dtypes(exclude=['object']).columns.to_series().items():\n        df = pd.concat([y,X[col]],axis=1)\n        ttest_df = df.set_index(target,drop=True).fillna(0)\n        stat, p = ttest_ind(ttest_df.loc[\"Yes\"],ttest_df.loc[\"No\"],equal_var=False)\n        test_df.append([target,col,stat,p,'Dependent (reject H0)' if p <= alpha else 'Independent (H0 holds true)','include' if p <= alpha else 'drop'])\n        \n    test_df = pd.DataFrame(test_df,columns=[\"variable1\",\"variable2\",\"t-stat\",\"p-value\",\"result\",\"recommendation\"])\n    return test_df\n\nt_test(df.drop('Churn',axis=1),df['Churn'])","metadata":{"_uuid":"4341b9f9-7c0b-43d7-91bc-5fec0c60f8c5","_cell_guid":"0b8a5c23-0deb-45a8-8a0d-9fd15544af12","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:50.863363Z","iopub.execute_input":"2021-07-19T12:04:50.863726Z","iopub.status.idle":"2021-07-19T12:04:51.543558Z","shell.execute_reply.started":"2021-07-19T12:04:50.863697Z","shell.execute_reply":"2021-07-19T12:04:51.542432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets set up the experiment code. We can try different models using this as common template\n \nclass Experiment:\n    \n    def __init__(self,model,data_transformer,pbounds=None):\n        self.model = model\n        self.data_transformer = data_transformer\n        self.results = None\n        self.pbounds = pbounds\n\n    def evaluate(self,X,y):\n        results = dict()\n        predictions = self.model.predict(X)\n        probas = self.model.predict_proba(X)\n        results[\"report\"] = metrics.classification_report(y, predictions,output_dict=True)\n        results[\"roc\"] = metrics.roc_auc_score(y,probas[:,1])\n        return results\n    \n    def hyper_parameter_tuning(self,X_train,y_train,X_val,y_val):\n            \n        def opt_function(**kwargs):\n            current_params = dict()\n            for key in self.pbounds:\n                current_params[key] = int(kwargs[key])\n            self.model.set_params(**current_params)\n            self.model.fit(X_train,y_train)\n            result = self.evaluate(X_val,y_val)\n            return result['roc']\n        \n        optimizer = BayesianOptimization(\n            f=opt_function,\n            pbounds=self.pbounds,\n            random_state=42,\n            verbose=2\n        )\n        \n        optimizer.maximize(\n            init_points=10,\n            n_iter=10,\n        )\n        \n        params = optimizer.max['params']\n        for key in params:\n            params[key] = int(params[key])\n        print(\"optimal target\",optimizer.max['target'])\n        print(params)\n        \n        return params  \n    \n    def prep_data(self,df,target=\"Churn\"):\n        X_train,X_test,y_train,y_test = train_test_split(df.drop(target,axis=1),df[target],test_size=.2, random_state=42,stratify=df[target])\n        X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=.25,random_state=42,stratify=y_train)\n\n        print(X_train.shape,y_train.shape,X_val.shape,y_val.shape,X_test.shape,y_test.shape)\n\n        X_train,y_train = data_transformer.fit_transform(X_train,y_train)\n        X_val,y_val = data_transformer.transform(X_val,y_val)\n        X_test,y_test = data_transformer.transform(X_test,y_test)\n        \n        print(X_train.shape,y_train.shape,X_val.shape,y_val.shape,X_test.shape,y_test.shape)\n        return X_train,y_train,X_val,y_val,X_test,y_test\n    \n    def __format_results__(self,results):\n        r = pd.DataFrame(results) \n        g = pd.concat([pd.DataFrame(x) for x in r.loc['report']])\n        g.index = pd.MultiIndex.from_product([[\"train_data\",\"val_data\",\"test_data\"],['precision', 'recall', 'f1-score', 'support']],names=[\"dataset\",\"metric\"])\n        return g.join(r.loc[\"roc\"],on=[\"dataset\"])\n        \n        \n    def run(self,df,hptuning=False,**kwargs):\n        \n        X_train,y_train,X_val,y_val,X_test,y_test = self.prep_data(df) \n        \n        if hptuning:\n            params = self.hyper_parameter_tuning(X_train,y_train,X_val,y_val)\n            self.model = model.set_params(**params)\n            \n        %time self.model.fit(X_train,y_train,**kwargs)\n        \n        results = dict()\n        results[\"train_data\"] = self.evaluate(X_train,y_train)\n        results[\"val_data\"] = self.evaluate(X_val,y_val)\n        results[\"test_data\"] = self.evaluate(X_test,y_test)\n        \n        self.results = self.__format_results__(results)\n        \n        return self.results\n    ","metadata":{"_uuid":"e0189698-de82-422b-a1b9-d13cad2fb4d5","_cell_guid":"204c4d53-bd45-4e7e-8016-fca503d35f46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:45:04.262046Z","iopub.execute_input":"2021-07-19T12:45:04.262405Z","iopub.status.idle":"2021-07-19T12:45:04.30126Z","shell.execute_reply.started":"2021-07-19T12:45:04.262376Z","shell.execute_reply":"2021-07-19T12:45:04.299899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets define the pre processing pipeline for data. I included missing value indicator,\n# and feature selection using statistical tests.\n\nclass DataTransformer:\n\n    def selectColumns(self, X, y):\n        '''\n            selects columns based on chi2test and t_test\n        '''\n        cols = []\n        for func in [chi2test,t_test]:\n            test_df = func(X,y)\n            included_columns = test_df[test_df['recommendation']=='include']['variable2'].tolist()\n            dropped_columns = test_df[test_df['recommendation']=='drop']['variable2'].tolist()\n\n            print(\"dropped columns:\",dropped_columns,len(dropped_columns))\n            cols.extend(included_columns)\n        return cols\n\n    def __init__(self,missingIndicator=False,featureSelection=False):\n        \n        self.featureSelection = featureSelection\n        self.selectedColumns = None # its value is initialized at time of fitting the pipeline\n        \n        cont_imputer = SimpleImputer(strategy='mean')\n        cont_normalizer = RobustScaler() \n        cont_pipeline =  make_pipeline(cont_imputer,cont_normalizer)\n        \n        if missingIndicator:\n            cont_missing_indicator = MissingIndicator(features='all')\n            cont_pipeline = make_union(cont_pipeline,cont_missing_indicator)\n        \n        cat_imputer = SimpleImputer(strategy='constant',fill_value='Unknown')\n        cat_encoder = CE.leave_one_out.LeaveOneOutEncoder(sigma=0.05)\n        cat_pipeline = make_pipeline(cat_imputer,cat_encoder)\n        \n        if missingIndicator:\n            cat_missing_indicator = MissingIndicator(features='all')\n            cat_pipeline = make_union(cat_pipeline,cat_missing_indicator)\n\n        cont_selector = make_column_selector(dtype_exclude='object')\n        cat_selector = make_column_selector(dtype_include='object')\n        cont_cat_split_transform = make_column_transformer((cont_pipeline, cont_selector), (cat_pipeline,cat_selector), remainder='drop')\n        xpipe =  make_pipeline(cont_cat_split_transform)\n            \n        ypipe = LabelEncoder() \n        \n        self.X_pipeline = xpipe\n        self.y_pipeline = ypipe\n        \n    def fit_transform(self,X,y):\n        if self.featureSelection:\n            self.selectedColumns = self.selectColumns(X,y)\n            X = X[self.selectedColumns]\n        y = self.y_pipeline.fit_transform(y)\n        X = self.X_pipeline.fit_transform(X,y)\n        \n        return (X,y)\n    \n    def transform(self,X,y):\n        y = self.y_pipeline.transform(y)\n        if self.featureSelection:\n            X = X[self.selectedColumns]\n        X = self.X_pipeline.transform(X)\n        return (X,y)\n","metadata":{"_uuid":"692d6aca-a164-4cfb-a8dd-35ccd7462b43","_cell_guid":"2341277a-57f0-4c75-8771-21b445b93940","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:04:01.370191Z","iopub.execute_input":"2021-07-19T12:04:01.370675Z","iopub.status.idle":"2021-07-19T12:04:01.389541Z","shell.execute_reply.started":"2021-07-19T12:04:01.370624Z","shell.execute_reply":"2021-07-19T12:04:01.388048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Lets start with a gradient boosting model\n\ndata_transformer = DataTransformer(missingIndicator=False,featureSelection=False)\n\nmodel = GradientBoostingClassifier(random_state=42)\nexp = Experiment(model,data_transformer)\nexp.run(df,hptuning=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T09:47:16.79843Z","iopub.execute_input":"2021-07-19T09:47:16.798942Z","iopub.status.idle":"2021-07-19T09:47:56.361485Z","shell.execute_reply.started":"2021-07-19T09:47:16.798907Z","shell.execute_reply":"2021-07-19T09:47:56.360413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n## lets see if we can get same/better results by dropping features that we tagged as insignificant using statistical tests\n## 16 features were dropped. Results dropped slightly but training time reduced.\ndata_transformer = DataTransformer(missingIndicator=False,featureSelection=True)\n\nmodel = GradientBoostingClassifier(random_state=42)\nexp = Experiment(model,data_transformer)\nexp.run(df,hptuning=False)\n\n","metadata":{"_uuid":"0d3aed38-d447-436e-b444-fa4dca071c43","_cell_guid":"ed569491-72bc-471e-a4a1-114ae5b60b0b","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T12:05:00.356934Z","iopub.execute_input":"2021-07-19T12:05:00.357344Z","iopub.status.idle":"2021-07-19T12:05:25.526306Z","shell.execute_reply.started":"2021-07-19T12:05:00.35731Z","shell.execute_reply":"2021-07-19T12:05:25.525241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n## Let's add missing value indicator and try. \n## we see that results don't change much.\n\ndata_transformer = DataTransformer(missingIndicator=True,featureSelection=True)\n\nmodel = GradientBoostingClassifier(random_state=42)\nexp = Experiment(model,data_transformer)\nexp.run(df,hptuning=False)\n","metadata":{"_uuid":"c9bc2e72-4fd9-4969-ae27-4bd0ed2754d2","_cell_guid":"aed6f911-062e-481d-a377-4de67af49f80","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-19T09:48:46.736793Z","iopub.execute_input":"2021-07-19T09:48:46.737184Z","iopub.status.idle":"2021-07-19T09:49:11.681466Z","shell.execute_reply.started":"2021-07-19T09:48:46.737151Z","shell.execute_reply":"2021-07-19T09:49:11.680391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# the evaluation results of class 1 are much worse than of class 0. This is due to imbalanced dataset\n# lets try undersampling to deal with this.\n\n# we see that the results of class 1 increased but that of class 0 dropped which caused a drop in overall results\n\ndata_transformer = DataTransformer(missingIndicator=True,featureSelection=True)\n\nmodel = imblearn.pipeline.make_pipeline(RandomUnderSampler(random_state=42),GradientBoostingClassifier(random_state=42))\nexp = Experiment(model,data_transformer)\nexp.run(df,hptuning=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T12:05:25.528603Z","iopub.execute_input":"2021-07-19T12:05:25.529034Z","iopub.status.idle":"2021-07-19T12:05:43.23773Z","shell.execute_reply.started":"2021-07-19T12:05:25.528998Z","shell.execute_reply":"2021-07-19T12:05:43.236599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# lets tune the hyperparameters\n# Now we have better results on class 1 and also slightly better overall results.\n# these resuls\ndata_transformer = DataTransformer(missingIndicator=True,featureSelection=True)\n\nmodel = imblearn.pipeline.make_pipeline(RandomUnderSampler(random_state=42),GradientBoostingClassifier(random_state=42))\nexp = Experiment(model,data_transformer,{\"gradientboostingclassifier__max_depth\":(3,20),\"gradientboostingclassifier__max_features\":(3,40),\"gradientboostingclassifier__max_leaf_nodes\":(32,128)})\n\nexp.run(df,hptuning=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T12:30:34.469482Z","iopub.execute_input":"2021-07-19T12:30:34.469842Z","iopub.status.idle":"2021-07-19T12:33:27.292222Z","shell.execute_reply.started":"2021-07-19T12:30:34.469813Z","shell.execute_reply":"2021-07-19T12:33:27.291151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# lets tune the hyperparameters\n# Now we have better results on class 1 and also slightly better overall results.\n# Please note these results may not be exactly reproducible due to an issue with BayesianOptimization package use for hyperparameter tuning. Setting the random state is not working correctly.\ndata_transformer = DataTransformer(missingIndicator=True,featureSelection=True)\n\nmodel = imblearn.pipeline.make_pipeline(RandomUnderSampler(random_state=42),GradientBoostingClassifier(random_state=42))\nexp = Experiment(model,data_transformer,{\"gradientboostingclassifier__max_depth\":(3,20),\"gradientboostingclassifier__max_features\":(3,40),\"gradientboostingclassifier__max_leaf_nodes\":(32,128)})\n\nexp.run(df,hptuning=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T12:45:16.543729Z","iopub.execute_input":"2021-07-19T12:45:16.544111Z","iopub.status.idle":"2021-07-19T12:48:20.017228Z","shell.execute_reply.started":"2021-07-19T12:45:16.544081Z","shell.execute_reply":"2021-07-19T12:48:20.016236Z"},"trusted":true},"execution_count":null,"outputs":[]}]}