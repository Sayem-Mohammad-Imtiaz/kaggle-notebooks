{"nbformat":4,"metadata":{"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"4c2eda3c-858a-43ab-a8b1-040a335c79b2","_uuid":"11988a642b1fa17063aef0f5116a13642bcc33f6"},"source":"I'll try some classification models on the dataset in the following code. Generally, we need two steps.\n## 1. Data Preparation and analysis\n## 2. Machine learning models training\nLet's begin!\n## 1. Data Preparation and analysis\nFirstly, we need import necessary models.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ed914b58-0ba0-4b66-b7c0-01480639e1b1","collapsed":true,"_uuid":"20bcd13d5da171bb6c1aa29ce0c429cc4546ea31"},"execution_count":null,"source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegressionCV\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\n\nfrom pprint import pprint\nimport json\nimport warnings\nfrom time import time\nfrom subprocess import check_output\n\nwarnings.filterwarnings('ignore', category=RuntimeWarning)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"058d2a69-4a3d-4c13-bdde-923436219754","collapsed":true,"_uuid":"3efa637d0f3cc942afaba6cba711ca5d1ba0f025"},"execution_count":null,"source":"# set display options\nnp.set_printoptions(suppress=True, linewidth=300)\npd.options.display.float_format = lambda x: ('%0.6f' % x)\n%matplotlib inline\npyo.init_notebook_mode(connected=True)\nprint(check_output([\"ls\", \"../input\"]).decode('utf-8'))","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"912a8ab7-b52a-4ae3-bf7f-0772fc1f0bc9","collapsed":true,"_uuid":"bc23e5f78edca531bf7e915efda894e65125fefd"},"execution_count":null,"source":"data_df = pd.read_csv('../input/mushrooms.csv')\ndata_df.info()","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"59d6f7e6-cc53-4470-825f-f27aec0e971a","collapsed":true,"_uuid":"61906f7780ece129b2e0904d83e6fc2dde0ae4da"},"execution_count":null,"source":"data_df.head()","cell_type":"code"},{"metadata":{"_cell_guid":"edde7d22-49ab-4252-8cba-e961f562aed8","_uuid":"d83023e6ebe0511fd885d9bd984c8d2eb785c1c8"},"source":"The following code is to visualize the positive percentage and quantity percentage by different single columns one by one. The columns that has only one unique value will be ignored.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"7fc44c40-d607-438f-9dbd-7fb3bfd1d98a","collapsed":true,"_uuid":"a4637977ad1304eab9b1bf748d56493918ef39cc"},"execution_count":null,"source":"data_df['y'] = data_df['class'].map({'p':1, 'e':0})\nfeature_columns = [c for c in data_df.columns if not c in ('class', 'y')]\nstats_df = []\nsingle_val_c = {}\nfor i, c in enumerate(feature_columns):\n    if data_df[c].nunique()==1:\n        single_val_c[c] = data_df[c].unique()[0]\n        continue\n    gb = data_df.groupby(c)\n    m = gb['y'].mean()\n    s = gb.size()\n    df = pd.DataFrame(index=range(len(m)))\n    df['col'] = c\n    df['val'] = m.index.values\n    df['positive_percentage'] = m.values\n    df['quantity_percentage'] = s.values / s.sum()\n    stats_df.append(df)\n    trace_prate = go.Bar(x=df['val'], y=df['positive_percentage']*100, name='positive percentage')\n    trace_cnt = go.Bar(x=df['val'], y=df['quantity_percentage']*100, name='quantity percentage')\n    layout = go.Layout(xaxis=dict(title=c), yaxis=dict(title=\"positive and quantity percentage\"))\n    fig = go.Figure(data=[trace_prate, trace_cnt], layout=layout)\n    pyo.iplot(fig)\nstats_df = pd.concat(stats_df, axis=0)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"260d0fa9-13c3-4a4a-b55a-50eb25d9f236","collapsed":true,"_uuid":"1c93b57110fdc7b5fcccc1f65e4b51bb3b3f5394"},"execution_count":null,"source":"stats_df.describe()","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"34305c11-0108-48f2-a125-9aac1b478063","collapsed":true,"_uuid":"570d4ee970db799110e9284c3c6fa135873caab6"},"execution_count":null,"source":"for c in single_val_c.keys():\n    print(\"The column %s only has one unique value with %r.\" % (c, single_val_c[c]))\n    print(\"It does work for the classification, which will be removed.\")\n    feature_columns.remove(c)\ndata_df = data_df[feature_columns+['y']].copy()","cell_type":"code"},{"metadata":{"_cell_guid":"9e6c85b9-7ff9-4304-8db3-f8f22015177e","_uuid":"e29f3dff7c0be7ae248e37bbbd15e28de9ea0b50"},"source":"## 2. Machine Learning models building","cell_type":"markdown"},{"metadata":{"_cell_guid":"edc701bf-d0fe-4c8e-9ec7-6d68e4e2dee3","_uuid":"bbf50d609809ea4cbe3e2a50b0dfbda0fc5699d9"},"source":"Looking over these above diagrams in the last part, it's easy to find that some unique values take a very low quantity percentage in some indivvidual columns, which could be taken as nosie. To build a machine learning model, one way is to use the original dataset directly and let the model hanld it automatically. Another way is to convert them into other similiar features set then train a model by using a the new dataset. I'll try the two way one by one.","cell_type":"markdown"},{"metadata":{"_cell_guid":"3b5f24a4-1741-4c93-b6d5-0d3b42e7da30","_uuid":"4574d262a21e4f0aa352e26691d3d39abf71e6fc"},"source":"### 1. Build model direcly","cell_type":"markdown"},{"metadata":{"_cell_guid":"6e6e0734-644d-4826-a1b6-b85edcede7e5","_uuid":"d2cc92732cc932752645a1dbb192ee1c6d63d6d4"},"source":"Because the trining of a lot of machine models contain stochastic process more or less, the original data will be copied to two or more times to decrease unsteainess of the final model.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"bf4f5f27-c95a-4b5c-baab-aae22f81bdeb","collapsed":true,"_uuid":"dcd5dc4c3c9f145991d64d38a81e3d431fa72d6e"},"execution_count":null,"source":"data_df = pd.concat((data_df, data_df), axis=0, ignore_index=True)\ndata_df.info()","cell_type":"code"},{"metadata":{"_cell_guid":"dc80b7bd-7265-4173-81a4-83b8bc2073fc","_uuid":"ad9f83a8eafdfc2331649d6c5b8247bfc383cdb8"},"source":"Firstly, we need convert features columns to one-hot code.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ff9dbd7a-61c1-4ffa-b747-e70a7380aec6","collapsed":true,"_uuid":"c2cbaffdcdb0e2e1eb9b4b1f08b35234a27825e4"},"execution_count":null,"source":"data_all = pd.get_dummies(data=data_df, columns=feature_columns, prefix=feature_columns)\ndata_all.info()","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"c4e2a278-a565-47d2-8302-0052fd95f73c","collapsed":true,"_uuid":"3adcefb018f6499e52407cb029d6f281822b9106"},"execution_count":null,"source":"data_all.head()","cell_type":"code"},{"metadata":{"_cell_guid":"6d9d9d1e-94d2-4413-b342-d2728a379d75","_uuid":"fab2d69901a7750463ffdea581369354cc924ea3"},"source":"Let' train the model.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"f73fe5bd-1e09-41bb-84ed-48620acead52","collapsed":true,"_uuid":"2ad7608e9bd2056254ed6f634aff118d1ba330b8"},"execution_count":null,"source":"def grid_search(base_model, param_grid, X_train, y_train):\n    gs_c = GridSearchCV(base_model, param_grid=param_grid, n_jobs=-1, cv=3)\n    gs_c.fit(X_train, y_train)\n    for param_name in sorted(gs_c.best_params_):\n        print(\"The best value of param  %s is %r\" % (param_name, gs_c.best_params_[param_name]))\n    return gs_c\n\ndef ridge_model(X_train, y_train):\n    r_c = Pipeline([\n        ('poly', PolynomialFeatures(interaction_only=True)),\n        ('clf', RidgeClassifier(random_state=1))\n    ])\n    params_pool = dict(poly__degree=[2], clf__alpha=[0.01, 0.03, 0.1, 0.3, 1])\n    return grid_search(r_c, params_pool, X_train, y_train)\n\ndef randomForest_model(X_train, y_train):\n    rf_c = RandomForestClassifier(random_state=1)\n    params_pool = dict(max_depth=[5, 7, 9], max_features=[0.3, 0.5], n_estimators=[12, 20, 36, 50])\n    return grid_search(rf_c, params_pool, X_train, y_train)\n\ndef gaussianNB_model(X_train, y_train):\n    gnb = Pipeline([\n        ('poly', PolynomialFeatures(interaction_only=True)),\n        ('clf', GaussianNB())\n    ])\n    gnb.fit(X_train, y_train)\n    return gnb\n\ndef multinomialNB_model(X_train, y_train):\n    mnb = Pipeline([\n        ('poly', PolynomialFeatures(interaction_only=True)),\n        ('clf', MultinomialNB(alpha=0.00001))\n    ])\n    mnb.fit(X_train, y_train)\n    return mnb\n\ndef do_model_train(model_name, X_train, y_train, X_test, y_test):\n    bg = time()\n    print(\"The model %s begin to train ...\" % model_name)\n    if 'Ridge' == model_name:\n        model = ridge_model(X_train, y_train)\n    elif 'RandomForest' == model_name:\n        model = randomForest_model(X_train, y_train)\n    elif 'GaussianNB' == model_name:\n        model = gaussianNB_model(X_train, y_train)\n    elif 'multinomialNB' == model_name:\n        model = multinomialNB_model(X_train, y_train)\n    print(\"Seconds spent on %s training: %0.3f\" % (model_name, time() - bg))\n\n    y_hat = model.predict(X_train)\n    print(\"%s accuracy of train dataset: %0.3f%%\" % (model_name, accuracy_score(y_train, y_hat) * 100))\n    print(\"%s precision of train dataset: %0.3f%%\" % (model_name, precision_score(y_train, y_hat) * 100))\n    print(\"%s recall rate of train dataset: %0.3f%%\" % (model_name, recall_score(y_train, y_hat) * 100))\n    print(\"%s f1 score of train dataset: %0.3f%%\" % (model_name, f1_score(y_train, y_hat) * 100))\n\n    y_hat = model.predict(X_test)\n    print(\"%s accuracy of test dataset: %0.3f%%\" % (model_name, accuracy_score(y_test, y_hat) * 100))\n    print(\"%s precision of test dataset: %0.3f%%\" % (model_name, precision_score(y_test, y_hat) * 100))\n    print(\"%s recall rate of test dataset: %0.3f%%\" % (model_name, recall_score(y_test, y_hat) * 100))\n    print(\"%s f1 score of test dataset: %0.3f%%\" % (model_name, f1_score(y_test, y_hat) * 100))\n","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"ea51252a-f208-45bd-bc42-ca2b1c7a5015","collapsed":true,"_uuid":"548c4b7b07ff6e5eb17560f47705797d85947920","scrolled":true},"execution_count":null,"source":"X_all, y_all = data_all[[c for c in data_all.columns if c!='y']], data_all['y']\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.33, random_state=1)\ndo_model_train('RandomForest', X_train, y_train, X_test, y_test)","cell_type":"code"},{"metadata":{"_cell_guid":"9aa8608a-e478-44c0-bbe2-4fd2f2ecb916","_uuid":"116243f41b289d101f870545b4a57652bc5437a9"},"source":"Great, we got 100%! Then I'll try to decrease the of number of training features.","cell_type":"markdown"},{"metadata":{"_cell_guid":"35fa5d5a-b51d-4d49-89ca-9025ed1e90b0","_uuid":"ab81ba48aa2c5d4452da655b3400b2ce51e59a17"},"source":"### 2. Clean nosie then build model","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"50334ea6-0c35-42f8-bed1-4aa865098165","collapsed":true,"_uuid":"83e71d0f9eb5046c110fe5edb2009f8df5e63490"},"execution_count":null,"source":"def clean_data_df(df, threshold=0.02):\n    feature_convert = dict()\n    for col, sub in stats_df.groupby('col'):\n        ns = sub[(sub.quantity_percentage<threshold)]\n        n_ns = sub[(sub.quantity_percentage>=threshold)]\n        for idx in ns.index:\n            if ns.loc[idx, 'positive_percentage'] > 0.5:\n                p_n_ns = n_ns[n_ns.positive_percentage > 0.5]\n                if not p_n_ns.empty:\n                    feature_convert.setdefault(col, []).append((ns.loc[idx, 'val'], p_n_ns['val'].values[0]))\n                    df.loc[df[col]==ns.loc[idx, 'val'], col] = p_n_ns['val'].values[0]\n            else:\n                n_n_ns = n_ns[n_ns.positive_percentage <= 0.5]\n                if not n_n_ns.empty:\n                    feature_convert.setdefault(col, []).append((ns.loc[idx, 'val'], n_n_ns['val'].values[0]))\n                    df.loc[df[col]==ns.loc[idx, 'val'], col] = n_n_ns['val'].values[0]\n    return pd.get_dummies(data=df, columns=feature_columns, prefix=feature_columns),\\\n            feature_convert\ncleaned_df, feature_convert = clean_data_df(data_df.copy())\ncleaned_df.info()","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"ab14e088-1495-4380-92a5-eabe90e1ba23","collapsed":true,"_uuid":"8a4d468970400dbd059f39bf549917347fcb6aca"},"execution_count":null,"source":"pprint(feature_convert)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"ace42443-a079-40e6-85a8-9edae2ba663c","collapsed":true,"_uuid":"11ef9f051c36f990f0800134d4ae52d0ca2e89d0"},"execution_count":null,"source":"X_all, y_all = cleaned_df[[c for c in cleaned_df.columns if c!='y']], cleaned_df['y']\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.33, random_state=1)\ndo_model_train('RandomForest', X_train, y_train, X_test, y_test)","cell_type":"code"},{"metadata":{"_cell_guid":"94e9e0bf-8522-43c9-b8be-4c5d56e6796a","_uuid":"dbd2690048ef1defd03be96971ab1e44baba8b7e"},"source":"Great, we also got 100%. We also found that the param max_featrues grid seached is only 0.3 and we got only 12 trees. So probaly the necessary quantity of features could be much less. The following code will try to find really importance features.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ce8b2bbb-7bbb-46b0-ad5f-aada72af225d","collapsed":true,"_uuid":"bf16c097caf9ea69fdbd4753608610f6156a495b"},"execution_count":null,"source":"def get_features_importance(x, y):\n    rf = RandomForestClassifier(n_estimators=500, class_weight={0: 1, 1: 1 / np.sqrt(np.mean(y))}, \\\n                                max_features=0.75, n_jobs=-1, random_state=1)\n    rf.fit(x, y)\n    feature_importance = pd.DataFrame(data={\"columns\": x.columns, \"importance\": rf.feature_importances_})\n    feature_importance.sort_values(by=\"importance\", axis=0, ascending=False, inplace=True)\n    feature_importance.loc[:, \"cum_importance\"] = feature_importance.importance.cumsum()\n    return feature_importance\n\ndef get_features_corr(df, ycol):\n    corr_y = df.corr()[ycol].map(np.abs)\n    corr_y = corr_y[[c for c in corr_y.index if c!=ycol]]\n    corr_y = corr_y / corr_y.sum()\n    feature_importance = pd.DataFrame(data={\"columns\": corr_y.index.values, \"importance\": corr_y.values})\n    feature_importance.sort_values(by=\"importance\", axis=0, ascending=False, inplace=True)\n    feature_importance.loc[:, \"cum_importance\"] = feature_importance.importance.cumsum()\n    return feature_importance\n\ndata_all = pd.get_dummies(data=data_df, columns=feature_columns, prefix=feature_columns)\nX_all, y_all = data_all[[c for c in data_all.columns if c!='y']], data_all['y']\nfi = get_features_importance(X_all, y_all)\n# fi = get_features_corr(data_all, 'y')\nbg = time()\naccuracyScores, precisionScores, recallScores, f1Scores = [], [], [], []\nfor i in range(len(fi)):\n    cols = fi.iloc[:i+1]['columns'].values   \n    model = Pipeline([\n        ('poly', PolynomialFeatures(interaction_only=True, degree=2)),\n        ('clf', GaussianNB())\n    ])\n    model.fit(X_all[cols], y_all)\n    y_p = model.predict(X_all[cols])\n    accuracyScores.append(accuracy_score(y_true=y_all, y_pred=y_p))\n    precisionScores.append(precision_score(y_true=y_all, y_pred=y_p))\n    recallScores.append(recall_score(y_true=y_all, y_pred=y_p))\n    f1Scores.append(f1_score(y_true=y_all, y_pred=y_p))\n    if accuracyScores[-1] == 1:\n        break\nprint()\nprint('It took %.3f seconds.' % (time() - bg))\ntraces = [go.Bar(x=np.arange(len(fi))+1, y=fi['importance'][:i+1], name='importance', opacity=0.5, \\\n                    text=fi['columns']),\n          go.Bar(x=np.arange(len(fi))+1, y=fi['cum_importance'][:i+1], name='left sum of importance', opacity=0.8, \\\n                    text=fi['columns']),\n          go.Scatter(x=np.arange(len(fi))+1, y=accuracyScores, mode='markers+lines', name='accuracy Score'),\n          go.Scatter(x=np.arange(len(fi))+1, y=precisionScores, mode='markers+lines', name='precision Score'),\n          go.Scatter(x=np.arange(len(fi))+1, y=recallScores, mode='markers+lines', name='recall Score'),\n          go.Scatter(x=np.arange(len(fi))+1, y=f1Scores, mode='markers+lines', name='F1 Score')]\nlayout=go.Layout(title='Feature importance/accuracy/precision/recall/F1 Score on different number of features',\n                xaxis=dict(title='number of features'), yaxis=dict(title='importance/accuracy/precision/recall/F1 Score'))\npyo.iplot(go.Figure(data=traces, layout=layout))","cell_type":"code"},{"metadata":{"_cell_guid":"930d48ba-4186-438b-99e4-5ab752951f35","_uuid":"a5347fa12eb182c38b1e3f801590571eac8e3fae"},"source":"Looking over the score curves changed by feature number, we could find some features obviously change the score, which are really import and could be used to build a much simpler model.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ac4544e4-7921-4d73-8af5-c10bdecec22d","collapsed":true,"_uuid":"f99921bc12b0204281d1e2d5fda56caadd5c3fd5"},"execution_count":null,"source":"col_idx = [0]\nfor i in range(len(f1Scores)):\n    if i != 0:\n        rst = False\n        for l in (f1Scores, precisionScores, recallScores, accuracyScores):\n            rst |= (l[i]!=l[i-1])\n        if rst:\n            col_idx.append(i)\nx_cols = fi.iloc[col_idx]['columns'].values\nprint(\"%d features were found:\" % len(x_cols))\npprint(x_cols)\nX_all, y_all = data_all[x_cols], data_all['y']\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.33, random_state=1)\ndo_model_train('RandomForest', X_train, y_train, X_test, y_test)","cell_type":"code"},{"metadata":{"_cell_guid":"38735f7b-9f7b-49a1-9279-b03254e1172f","_uuid":"e95c3bb8e5fb969a39d801801c35c6d2dc2b098e"},"source":"Great! We got a prefect but much simpler model with only 16 features used.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"19ecef18-70d8-4c3d-8bb6-560bedabad0e","collapsed":true,"_uuid":"9720cc43331be1726e30d95ee5b8d7a2104533a7"},"execution_count":null,"source":"col_vals = {}\nno_pt = [col_vals.setdefault(it[0], []).append(it[1]) for it in[v.split('_') for v in x_cols]]\nprint(\"The avaliable columns and value are following:\")\npprint(col_vals)","cell_type":"code"},{"metadata":{"_cell_guid":"8e8f3fdb-a02b-49ca-a4f2-c0b045f2d317","_uuid":"9360425e2034d60d4c90f9ae8c7330e9789cd5a9"},"source":"The following code is to review the distribution and pairwise correlation of these selected features.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"176e1ee0-7c8e-4dbe-a37a-4711bb9d7e62","collapsed":true,"_uuid":"73823da07522e78c08707ed86624460e8edc4b7e"},"execution_count":null,"source":"for c in x_cols:\n    gb = data_all.groupby(c)\n    m = gb['y'].mean()\n    s = gb.size()\n    df = pd.DataFrame(index=range(len(m)))\n    df['col'] = c\n    df['val'] = m.index.values\n    df['val'] = df['val'].map({0: 'N', 1: 'Y'})\n    df['positive_percentage'] = m.values\n    df['quantity_percentage'] = s.values / s.sum()\n    trace_prate = go.Bar(x=df['val'], y=df['positive_percentage']*100, name='positive percentage')\n    trace_cnt = go.Bar(x=df['val'], y=df['quantity_percentage']*100, name='quantity percentage')\n    layout = go.Layout(xaxis=dict(title='='.join(c.split('_'))+'?'), yaxis=dict(title=\"positive and quantity percentage\"))\n    fig = go.Figure(data=[trace_prate, trace_cnt], layout=layout)\n    pyo.iplot(fig)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"0f434b01-cc93-49c6-9714-44dbc1ad36c1","collapsed":true,"_uuid":"632d47c5209af28ec17a474d74577ca29a54ec54"},"execution_count":null,"source":"plt.figure(figsize=(18, 10))\ncorr = data_all[list(x_cols)+['y']].corr()\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.01)","cell_type":"code"},{"metadata":{"_cell_guid":"c9f7a82f-18a8-4fb8-b33c-373b7e6a4ea5","_uuid":"7656cd9022712c19631a636df2ab6d9f9faf9b34"},"source":"The following code is a sklearn transform class to make feature transformation, which will be used to train a pipeline model as the first step.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"7ba56b1c-7fe6-4c12-a30e-a6ca713aa2da","collapsed":true,"_uuid":"5cd566e58c0ec7ba2c0e7ed949bfc96569722cea"},"execution_count":null,"source":"class Feature_filter(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, necessary_col_vals: dict):\n        self.necessary_col_vals = necessary_col_vals\n    \n    def fit(self, X, y=None):\n        if not isinstance(X, pd.DataFrame):\n            raise TypeError(\"The input X must be a pandas.DataFrame instance.\")\n        diff = set(self.necessary_col_vals.keys()).difference(X.columns)\n        if diff:\n            raise IndexError(\"The input X lacks of necessary columns: %r\" % list(diff))\n        self.X = X\n        return self\n    \n    def transform(self, X):\n        cols = list(self.necessary_col_vals.keys())\n        XP = X[cols]\n        XP = pd.get_dummies(data=XP, columns=cols, prefix=cols)\n        n_cols = np.sum([[t[0]+'_'+ v for v in t[1]] for t in self.necessary_col_vals.items()])\n        return XP[n_cols].copy()","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"1301c3dc-c972-4cf3-af4a-78947361cd3e","collapsed":true,"_uuid":"815ac014e44381d77bdc46e9a9d1a0d2f47290ac"},"execution_count":null,"source":"fea_filter = Feature_filter(col_vals)\nfea_filter.fit_transform(data_df).info()","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"9978840f-c09f-44b5-b0b3-b634a1966540","collapsed":true,"_uuid":"c6a423dee2de0ea6bec43f547400f19ff8835ecd"},"execution_count":null,"source":"def mushroom_lr_model(X_train, y_train, enough_col_vals):\n    model = Pipeline([\n        ('fillter', Feature_filter(enough_col_vals)),\n        ('poly', PolynomialFeatures(interaction_only=True)),\n        ('clf', LogisticRegressionCV(n_jobs=-1, random_state=1))\n    ])\n    print(\"Begin to fit...\")\n    bg = time()\n    model.fit(X_train, y_train)\n    print(\"Done fit and %.2f seconds was token.\" % (time() - bg))\n    y_hat = model.predict(X_train)\n    print(\"accuracy of train dataset: %0.3f%%\" % (accuracy_score(y_train, y_hat) * 100))\n    print(\"precision of train dataset: %0.3f%%\" % (precision_score(y_train, y_hat) * 100))\n    print(\"recall rate of train dataset: %0.3f%%\" % (recall_score(y_train, y_hat) * 100))\n    print(\"f1 score of train dataset: %0.3f%%\" % (f1_score(y_train, y_hat) * 100))\n\n    y_hat = model.predict(X_test)\n    print(\"accuracy of test dataset: %0.3f%%\" % (accuracy_score(y_test, y_hat) * 100))\n    print(\"precision of test dataset: %0.3f%%\" % (precision_score(y_test, y_hat) * 100))\n    print(\"recall rate of test dataset: %0.3f%%\" % (recall_score(y_test, y_hat) * 100))\n    print(\"f1 score of test dataset: %0.3f%%\" % (f1_score(y_test, y_hat) * 100))\nX_all, y_all = data_df, data_df['y']\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.33, random_state=1)\nmushroom_lr_model(X_train, y_train, col_vals)","cell_type":"code"},{"metadata":{"_cell_guid":"be4e081a-f8a7-4386-b23c-681aa3d3a9a8","_uuid":"0d2442de8501a86faab5291b3de5220c99baa003"},"source":"Now, we get a very simple model to classify if a mushroom is poisonous. ","cell_type":"markdown"}]}