{"nbformat_minor":1,"nbformat":4,"metadata":{"language_info":{"mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.1","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"execution_count":null,"outputs":[],"source":"from json import loads\nimport pandas as pd\nfrom itertools import chain\nfrom dask import bag\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"collapsed":true,"_uuid":"48276b78006168cb8054f6a12ef28c2c5f3d06bb","_cell_guid":"b9d47a5d-1ad7-4cb0-b1e1-472973addf2a"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"drink_df = pd.read_csv('../input/all_drinks.csv')\ndrink_df.sample(3)","metadata":{"_uuid":"8cc80387c85a26431fb9981e3309e9b048b6c26c","_cell_guid":"8110599f-e7e8-44d4-a762-1cf3db5cc10d"},"cell_type":"code"},{"source":"# Parse the drink names \nThey are the input for our model (this is a simple approach by just counting the letters that show up","metadata":{"collapsed":true,"_uuid":"20cac86a84bf1507109592c42dc04871320718dd","_cell_guid":"409148fa-6f23-47eb-9778-c648cc195b42"},"cell_type":"markdown"},{"source":"## Tokenize the names\nHere we translate the names into a tokenized vector so we can feed it to a sequence to sequence model","metadata":{"_uuid":"8940610977d156d0656e1942b82417065936734f","_cell_guid":"6a7f2a98-a9a2-4d09-bbcb-f81ca05d7b20"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical","metadata":{"_uuid":"4598c4e7b9f7080effb411aa46bd4dc5a4d2a49a","_cell_guid":"e0b8ad99-6a59-4d60-a6cb-b3124cae3489"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"str_vec = drink_df['strDrink'].str.lower()\nMAX_NB_WORDS, MAX_SEQUENCE_LENGTH = 100, 40\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=True)\ntokenizer.fit_on_texts(str_vec)\ntrain_sequences = tokenizer.texts_to_sequences(str_vec)\ntrain_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nchar_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(char_index))\ntrain_x = np.stack([to_categorical(x, num_classes=len(char_index)+1) for x in train_data],0)\nprint(train_x.shape)","metadata":{"_uuid":"9967d2f6b07d1bc9cc41540b1c78c1e22d0ce1da","_cell_guid":"edd8e4fc-1080-40e9-bc6d-11663bebccfa"},"cell_type":"code"},{"source":"# Process Ingredients\nThis is what we want to predict so we need to transform it to a reasonable vector","metadata":{"_uuid":"3810f3cd230968aad8699ff5a24bd9197efee75c","_cell_guid":"5943b049-70cd-4afa-97b2-8ceff7472e2f"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"def isempty(x):\n    try:\n        if x is None: \n            return True\n        elif len(x)<1:\n            return True\n        else:\n            return False\n    except:\n        # floating point nans\n        return True\nall_ingred = drink_df[[x for x in drink_df.columns \n                       if 'Ingredient' in x]].apply(lambda c_row: [v.lower() for k,v in c_row.items() if not isempty(v)],1)\nall_ingred[0:3]","metadata":{"_uuid":"99cb954ee5aa3a430b37f2dca12b86415e1e3a10","_cell_guid":"07440455-ff7a-4dcb-8a3a-915057efd38f"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\n\ningred_label = LabelEncoder()\ningred_label.fit(list(chain(*all_ingred.values)))\nprint('Found', len(ingred_label.classes_), 'unique ingredients, ', ingred_label.classes_[0:3])","metadata":{"_uuid":"a2998ea109da1cb32186347549f87930778020c5","_cell_guid":"9a03efd7-ed82-4132-91ec-016d39a22485"},"cell_type":"code"},{"source":"Convert each ingredient to a one hot vector and sum them all together","metadata":{"_uuid":"8edb1f1b641085f5e97166b37eddea5366d56df7","_cell_guid":"9fe8cd23-f8f0-465d-b877-2ed9de00fc28"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"y_vec = np.stack(all_ingred.map(lambda x: np.sum(to_categorical(ingred_label.transform(x), \n                                        num_classes=len(ingred_label.classes_)),0)),0).clip(0,1)","metadata":{"collapsed":true,"_uuid":"603e425c087917a865ab808c3124b80fd7499f30","_cell_guid":"e256797e-1a15-454f-9507-9bbc50c0d73b"},"cell_type":"code"},{"source":"# Prepare Training","metadata":{"_uuid":"1e60ab2f423e7a0f175b45155b193ff9ea919f61","_cell_guid":"2c0ba4a6-7418-45e7-a87a-e0f81125b827"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\ntrain_idx, test_idx = train_test_split(range(y_vec.shape[0]), \n                                                    random_state = 12345,\n                                                   train_size = 0.7)","metadata":{"_uuid":"299496883153b71f9aec483ce1f6c8741f6bc575","_cell_guid":"7a85ec42-82af-49ae-a948-9b28c4ab4b61"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Masking\nfrom keras.optimizers import Adam\n# model based loosely on https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py\nsimple_sequence_model = Sequential()\nsimple_sequence_model.add(Masking(0, input_shape = (None,)))\nsimple_sequence_model.add(Embedding(len(char_index)+1, 32))\nsimple_sequence_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nsimple_sequence_model.add(Dense(y_vec.shape[1], \n                                activation = 'sigmoid'))\nsimple_sequence_model.compile(loss = 'binary_crossentropy', # categorical and mae don't work well here\n                              optimizer = Adam(lr = 5e-4, decay = 1e-6), \n                             metrics = ['mae'])\nsimple_sequence_model.summary()","metadata":{"_uuid":"188b305c1ec518575ae6be84d4b3298724078c81","_cell_guid":"89a46d03-1a5b-4d85-956c-841c3e287439"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"simple_sequence_model.fit(train_data[train_idx], y_vec[train_idx], epochs=10,\n                          batch_size = 32,\n                         validation_data = (train_data[test_idx], y_vec[test_idx]), \n                          verbose = 1)\npred_vec = simple_sequence_model.predict(train_data[test_idx])\n\nprint('Mean Error %2.2f%%' % (100*mean_absolute_error(y_vec[test_idx], pred_vec)))","metadata":{"_uuid":"189a32ac0251170f5698ea1c8b943d5d34de7596","_cell_guid":"6126970f-0a43-4ed3-890d-0664d6c9efb8"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"print('Input Name:', drink_df['strDrink'].values[test_idx[0]])\nprint('Real Ingredients', all_ingred.values[test_idx[0]])\n\nproc_pred = lambda out_pred: sorted([(ingred_label.inverse_transform(idx), out_pred[idx])\n                              for idx in np.where(out_pred>0)[0]], key = lambda x: -x[1])\n\nprint('Predicted Ingredients')\nfor _, (i,j) in zip(range(5), proc_pred(pred_vec[0])):\n    print('%25s\\t\\t%2.2f%%' % (i,100*j))","metadata":{"_uuid":"26116a9f2fb2a34cdf393fb13d8b3515fa262d9f","_cell_guid":"7da2ea4a-bdd4-4d78-9f6f-86ad310e59db"},"cell_type":"code"},{"source":"# Make more interesting drinks","metadata":{"_uuid":"497967814f79bc190971399d371be8331047f7ba","_cell_guid":"cf0b6431-9739-4a97-8af6-b372a0b6498e"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"rchar = lambda : np.random.choice(list(char_index.keys()))\nSENTENCE_SWAP = 0.25\nADD_LETTERS = 0.95\nDEL_LETTERS = 0.9\ndef tweak_sequence_gen(verbose = False):\n    while True:\n        c_train_idx = np.random.permutation(train_idx)\n        s_str = str_vec.values[c_train_idx]\n        if verbose: \n            print(s_str[0])\n        # randomly reorder strings\n        s_str = [(' '.join(np.random.permutation(x.split(' '))) if np.random.uniform(0,1)>SENTENCE_SWAP else x)\n                      for x in s_str]\n        if verbose: \n            print(s_str[0])\n        # randomly add letters\n        s_str = [''.join([(rchar() if np.random.uniform(0,1)>ADD_LETTERS else '') + y for y in x])\n                      for x in s_str]\n        if verbose: \n            print(s_str[0])\n        # randomly delete letters\n        s_str = [''.join([y for y in x if np.random.uniform(0,1)>(1-DEL_LETTERS)])\n                      for x in s_str]\n        if verbose: \n            print(s_str[0])\n        t_seq = tokenizer.texts_to_sequences(s_str)\n        t_data = pad_sequences(t_seq, maxlen=MAX_SEQUENCE_LENGTH)\n        yield t_data, y_vec[c_train_idx]\nfor _, (x,y) in zip(range(1), tweak_sequence_gen(True)):\n    print(x[0])","metadata":{"_uuid":"26cdc5b5ec006c522cf9aade704604bccbd315ca","_cell_guid":"5041aa53-880a-4c3c-b5ec-9a9f3743a423"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"epochs = 40\n# use a for loop so we can control batch size\nfor _, (c_bx, c_by) in zip(range(epochs), tweak_sequence_gen()):\n    simple_sequence_model.fit(c_bx, c_by,\n                                        batch_size = 16,\n                                        epochs=3,\n                             validation_data = (train_data[test_idx], y_vec[test_idx]), \n                              verbose = 0)\n\npred_vec = simple_sequence_model.predict(train_data[test_idx])\nprint('Mean Error %2.2f%%' % (100*mean_absolute_error(y_vec[test_idx], pred_vec)))","metadata":{"_uuid":"54329255e0be58a04f5ecf5d03ea0826086c994e","_cell_guid":"6629c80d-bd51-478c-b4a9-843822392b03"},"cell_type":"code"},{"source":"# Test Case","metadata":{"_uuid":"0f65e7e1ddf6787664a4801e58665a7b4287e44a","_cell_guid":"e3b77723-16d9-41b0-aa0b-4b4d20e425bb"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"for rand_idx in np.random.choice(range(len(test_idx)), size = 3):\n    print('Input Name:', drink_df['strDrink'].values[test_idx[rand_idx]])\n    print('Real Ingredients', all_ingred.values[test_idx[rand_idx]])\n\n    proc_pred = lambda out_pred: sorted([(ingred_label.inverse_transform(idx), out_pred[idx])\n                                  for idx in np.where(out_pred>0)[0]], key = lambda x: -x[1])\n\n    print('Predicted Ingredients')\n    for _, (i,j) in zip(range(5), proc_pred(pred_vec[rand_idx])):\n        print('%25s\\t\\t%2.2f%%' % (i,100*j))\n    print('')","metadata":{"_uuid":"e32e2a39e42c9d4a0f734d73183c0f22390b8b6c","_cell_guid":"85de7516-f0d9-4b80-b419-8c8ad94bb924"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"def predict_from_name(in_drink_name):\n    seq_arr = np.array(tokenizer.texts_to_sequences([in_drink_name.lower()]))\n    c_pred = simple_sequence_model.predict(seq_arr)\n    for _, (i,j) in zip(range(5), proc_pred(c_pred[0])):\n        print('%25s\\t\\t%2.2f%%' % (i,100*j))","metadata":{"collapsed":true,"_uuid":"75422bece0ef1db5bce40e65b95273355b3c0247","_cell_guid":"c9cb83fa-3fee-4d05-a419-0dc75fa52602"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"predict_from_name('super fancy drink')","metadata":{"_uuid":"bbc56c964578883d8254a0ef306fb32832f52c90","_cell_guid":"dd8403a7-d3a0-47c8-ac60-d6beb3628001"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"predict_from_name('hopping hippo')","metadata":{"_uuid":"a041af1c7e58c70146ddd5899291b3af41056320","_cell_guid":"66c8fd57-e6a2-45d5-a454-732774150eb3"},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"","metadata":{"collapsed":true,"_uuid":"0f9004e316e17956c5b0e76d9844bd82ca7fdc7d","_cell_guid":"fd324854-44b0-4010-8ce0-a6f8995c33c9"},"cell_type":"code"}]}