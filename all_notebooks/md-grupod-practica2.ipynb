{"cells":[{"metadata":{"papermill":{"duration":0.166559,"end_time":"2020-12-14T10:14:14.31552","exception":false,"start_time":"2020-12-14T10:14:14.148961","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Práctica 2: Aprendizaje y selección de modelos de clasificación\n\n### Minería de Datos: Curso académico 2020-2021\n\n* Ángel Torres del Álamo\n* José Ángel Royo López"},{"metadata":{"papermill":{"duration":0.162276,"end_time":"2020-12-14T10:14:14.645412","exception":false,"start_time":"2020-12-14T10:14:14.483136","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Introducción"},{"metadata":{"papermill":{"duration":0.165228,"end_time":"2020-12-14T10:14:14.975869","exception":false,"start_time":"2020-12-14T10:14:14.810641","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Como ocurría con la práctica 1 de análisis exploratorio, vamos a crear un *notebook* replicando la libreta y utilizando el script proporcionado por `Júan Carlos Alfaro Jiménez`, al que al igual que en la práctica anterior le hemos agregado la función que nos permite eliminar outlier (`outlier_rejection`).\n\nEn este caso los dataset de `Diabetes` y `Winconsin` crearemos modelos simples y *ensembles* de estos modelos, conociendo y estudiando sus hiperparámetros para luego optimizarlos y sacar el mejor resultado de `tasa de acierto` posible para cada modelo.\n\nComo tercer punto de esta práctica hemos replicado una libreta de kaggle, corrigiendo algunos de sus errores, que para adelantar podemos decir que comete fuga de datos."},{"metadata":{"papermill":{"duration":0.164162,"end_time":"2020-12-14T10:14:15.303073","exception":false,"start_time":"2020-12-14T10:14:15.138911","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Primero como es habitual, cargaremos las librerias que hemos utilizado."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:15.643842Z","iopub.status.busy":"2020-12-14T10:14:15.642863Z","iopub.status.idle":"2020-12-14T10:14:17.708544Z","shell.execute_reply":"2020-12-14T10:14:17.707795Z"},"papermill":{"duration":2.241415,"end_time":"2020-12-14T10:14:17.708699","exception":false,"start_time":"2020-12-14T10:14:15.467284","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns  \n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Third party\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import KNNImputer\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn import FunctionSampler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import IsolationForest\n\n# Local application\nimport miner_a_de_datos_aprendizaje_modelos_utilidad as utils","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.16693,"end_time":"2020-12-14T10:14:18.044927","exception":false,"start_time":"2020-12-14T10:14:17.877997","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Dataset Diabetes"},{"metadata":{"papermill":{"duration":0.161571,"end_time":"2020-12-14T10:14:18.370183","exception":false,"start_time":"2020-12-14T10:14:18.208612","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Establecemos una semilla para que los resultados sean reproducibles."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:18.70052Z","iopub.status.busy":"2020-12-14T10:14:18.699686Z","iopub.status.idle":"2020-12-14T10:14:18.70226Z","shell.execute_reply":"2020-12-14T10:14:18.702795Z"},"papermill":{"duration":0.170189,"end_time":"2020-12-14T10:14:18.702958","exception":false,"start_time":"2020-12-14T10:14:18.532769","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"random_state = 27912","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.180636,"end_time":"2020-12-14T10:14:19.046285","exception":false,"start_time":"2020-12-14T10:14:18.865649","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Carga de datos."},{"metadata":{"papermill":{"duration":0.163932,"end_time":"2020-12-14T10:14:19.444743","exception":false,"start_time":"2020-12-14T10:14:19.280811","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Como es habitual, cargamos los datos del dataset de `Diabetes`."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:19.791573Z","iopub.status.busy":"2020-12-14T10:14:19.790782Z","iopub.status.idle":"2020-12-14T10:14:19.814057Z","shell.execute_reply":"2020-12-14T10:14:19.813324Z"},"papermill":{"duration":0.205131,"end_time":"2020-12-14T10:14:19.814205","exception":false,"start_time":"2020-12-14T10:14:19.609074","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"filepath = \"../input/pima-indians-diabetes-database/diabetes.csv\"\n\nindex_col = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index_col, target)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:20.168076Z","iopub.status.busy":"2020-12-14T10:14:20.16724Z","iopub.status.idle":"2020-12-14T10:14:20.180527Z","shell.execute_reply":"2020-12-14T10:14:20.179665Z"},"papermill":{"duration":0.197341,"end_time":"2020-12-14T10:14:20.180675","exception":false,"start_time":"2020-12-14T10:14:19.983334","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data.sample(5,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.16645,"end_time":"2020-12-14T10:14:20.512805","exception":false,"start_time":"2020-12-14T10:14:20.346355","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Dividimos la base de datos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:20.850184Z","iopub.status.busy":"2020-12-14T10:14:20.849163Z","iopub.status.idle":"2020-12-14T10:14:20.852469Z","shell.execute_reply":"2020-12-14T10:14:20.851759Z"},"papermill":{"duration":0.176843,"end_time":"2020-12-14T10:14:20.852586","exception":false,"start_time":"2020-12-14T10:14:20.675743","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"(X, y) = utils.divide_dataset(data, target)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.163452,"end_time":"2020-12-14T10:14:21.182262","exception":false,"start_time":"2020-12-14T10:14:21.01881","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Comprobamos que se ha dividido correctamente."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:21.527252Z","iopub.status.busy":"2020-12-14T10:14:21.516694Z","iopub.status.idle":"2020-12-14T10:14:21.532304Z","shell.execute_reply":"2020-12-14T10:14:21.531629Z"},"papermill":{"duration":0.185544,"end_time":"2020-12-14T10:14:21.532431","exception":false,"start_time":"2020-12-14T10:14:21.346887","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X.sample(5,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:21.872492Z","iopub.status.busy":"2020-12-14T10:14:21.871308Z","iopub.status.idle":"2020-12-14T10:14:21.876338Z","shell.execute_reply":"2020-12-14T10:14:21.8757Z"},"papermill":{"duration":0.17897,"end_time":"2020-12-14T10:14:21.876465","exception":false,"start_time":"2020-12-14T10:14:21.697495","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"y.sample(5,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.165686,"end_time":"2020-12-14T10:14:22.210315","exception":false,"start_time":"2020-12-14T10:14:22.044629","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Dividimos el conjunto de entrenamiento y de test con un *houdout* estratificado, de igual manera que lo hicimos en la práctica 1."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:22.54924Z","iopub.status.busy":"2020-12-14T10:14:22.54841Z","iopub.status.idle":"2020-12-14T10:14:22.557847Z","shell.execute_reply":"2020-12-14T10:14:22.55697Z"},"papermill":{"duration":0.181701,"end_time":"2020-12-14T10:14:22.557985","exception":false,"start_time":"2020-12-14T10:14:22.376284","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"stratify = y\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.186109,"end_time":"2020-12-14T10:14:22.931966","exception":false,"start_time":"2020-12-14T10:14:22.745857","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Comprobamos las particiones."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:23.29009Z","iopub.status.busy":"2020-12-14T10:14:23.288884Z","iopub.status.idle":"2020-12-14T10:14:23.29328Z","shell.execute_reply":"2020-12-14T10:14:23.29384Z"},"papermill":{"duration":0.188375,"end_time":"2020-12-14T10:14:23.294015","exception":false,"start_time":"2020-12-14T10:14:23.10564","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:23.649964Z","iopub.status.busy":"2020-12-14T10:14:23.648975Z","iopub.status.idle":"2020-12-14T10:14:23.653341Z","shell.execute_reply":"2020-12-14T10:14:23.653965Z"},"papermill":{"duration":0.189611,"end_time":"2020-12-14T10:14:23.654145","exception":false,"start_time":"2020-12-14T10:14:23.464534","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X_test.sample(5,random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.167229,"end_time":"2020-12-14T10:14:23.990809","exception":false,"start_time":"2020-12-14T10:14:23.82358","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Creación del pipeline."},{"metadata":{"papermill":{"duration":0.167206,"end_time":"2020-12-14T10:14:24.32577","exception":false,"start_time":"2020-12-14T10:14:24.158564","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Utilizaremos el pipeline discretizado que creamos y analizamos en el estudio de la práctica anterior. Con las funciones y decisiones ya vistas en la anterior práctica."},{"metadata":{"papermill":{"duration":0.166745,"end_time":"2020-12-14T10:14:24.663492","exception":false,"start_time":"2020-12-14T10:14:24.496747","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Estas son las columnas que eliminaremos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:25.010517Z","iopub.status.busy":"2020-12-14T10:14:25.009721Z","iopub.status.idle":"2020-12-14T10:14:25.013373Z","shell.execute_reply":"2020-12-14T10:14:25.012706Z"},"papermill":{"duration":0.178297,"end_time":"2020-12-14T10:14:25.013505","exception":false,"start_time":"2020-12-14T10:14:24.835208","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"delete_colum = ['SkinThickness','Insulin']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.167676,"end_time":"2020-12-14T10:14:25.349997","exception":false,"start_time":"2020-12-14T10:14:25.182321","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Estas son las columnas que utilizará el imputador para establecer un valor válido a los valores perdidos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:25.691243Z","iopub.status.busy":"2020-12-14T10:14:25.690284Z","iopub.status.idle":"2020-12-14T10:14:25.693663Z","shell.execute_reply":"2020-12-14T10:14:25.692913Z"},"papermill":{"duration":0.176103,"end_time":"2020-12-14T10:14:25.693787","exception":false,"start_time":"2020-12-14T10:14:25.517684","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"imputer_col = ['Glucose','BloodPressure','BMI','DiabetesPedigreeFunction','Age']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.168296,"end_time":"2020-12-14T10:14:26.031578","exception":false,"start_time":"2020-12-14T10:14:25.863282","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El imputador funcionará mediante los k-vecinos como ya vimos en el primer estudio. Con la diferencia de que en este estudio se búscara el número de vecinos y el peso de los vecinos óptimo para el modelo que estamos estudiando."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:26.376413Z","iopub.status.busy":"2020-12-14T10:14:26.37525Z","iopub.status.idle":"2020-12-14T10:14:26.380312Z","shell.execute_reply":"2020-12-14T10:14:26.37949Z"},"papermill":{"duration":0.178833,"end_time":"2020-12-14T10:14:26.380451","exception":false,"start_time":"2020-12-14T10:14:26.201618","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"knnimputer = KNNImputer(n_neighbors=5, weights=\"uniform\",missing_values=0)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.166938,"end_time":"2020-12-14T10:14:26.714461","exception":false,"start_time":"2020-12-14T10:14:26.547523","status":"completed"},"tags":[]},"cell_type":"markdown","source":"La eliminación de las columnas y el imputador estarán dentro de un `Column Transformer`, ya que solo queremos que estas acciones las hagan las columnas que hemos visto anteriormente."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:27.059246Z","iopub.status.busy":"2020-12-14T10:14:27.058192Z","iopub.status.idle":"2020-12-14T10:14:27.06158Z","shell.execute_reply":"2020-12-14T10:14:27.060883Z"},"papermill":{"duration":0.17808,"end_time":"2020-12-14T10:14:27.061709","exception":false,"start_time":"2020-12-14T10:14:26.883629","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"column_transformer = make_column_transformer((\"drop\", delete_colum),(knnimputer, imputer_col), remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.172128,"end_time":"2020-12-14T10:14:27.404203","exception":false,"start_time":"2020-12-14T10:14:27.232075","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Y por último definimos el discretizador, donde el hiperparámetro de `n_bins` tambien sera optimizado."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:27.768377Z","iopub.status.busy":"2020-12-14T10:14:27.767576Z","iopub.status.idle":"2020-12-14T10:14:27.770485Z","shell.execute_reply":"2020-12-14T10:14:27.771016Z"},"papermill":{"duration":0.177564,"end_time":"2020-12-14T10:14:27.771221","exception":false,"start_time":"2020-12-14T10:14:27.593657","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\", encode=\"onehot-dense\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.170603,"end_time":"2020-12-14T10:14:28.117436","exception":false,"start_time":"2020-12-14T10:14:27.946833","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Validación cruzada."},{"metadata":{"papermill":{"duration":0.168509,"end_time":"2020-12-14T10:14:28.455751","exception":false,"start_time":"2020-12-14T10:14:28.287242","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para empezar vamos a definir la validación cruzada que vamos a utilizar para entrenar los modelos. Utilizamos una validación cruzada estratificada de $5x10-cv$, que como vimos en clase de prácticas es la más idonea para este tipo de validación cruzada."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:28.801Z","iopub.status.busy":"2020-12-14T10:14:28.799912Z","iopub.status.idle":"2020-12-14T10:14:28.803805Z","shell.execute_reply":"2020-12-14T10:14:28.803062Z"},"papermill":{"duration":0.180645,"end_time":"2020-12-14T10:14:28.803934","exception":false,"start_time":"2020-12-14T10:14:28.623289","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.167951,"end_time":"2020-12-14T10:14:29.143155","exception":false,"start_time":"2020-12-14T10:14:28.975204","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Modelos de clasificación supervisada"},{"metadata":{"papermill":{"duration":0.168313,"end_time":"2020-12-14T10:14:29.483871","exception":false,"start_time":"2020-12-14T10:14:29.315558","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para la simplificación a la hora de escribir y como el dataset completo lo tenemos ya guardado. Vamos a cambiar el nombre de `X_train` e `y_train` a `X` e `Y` respectivamente."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:29.829206Z","iopub.status.busy":"2020-12-14T10:14:29.82806Z","iopub.status.idle":"2020-12-14T10:14:29.831511Z","shell.execute_reply":"2020-12-14T10:14:29.830727Z"},"papermill":{"duration":0.179976,"end_time":"2020-12-14T10:14:29.831653","exception":false,"start_time":"2020-12-14T10:14:29.651677","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X = X_train\ny = y_train","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.167944,"end_time":"2020-12-14T10:14:30.169941","exception":false,"start_time":"2020-12-14T10:14:30.001997","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Y tambíen, antes de empezar vamos a establecer el conjunto de los valores de los hiperparámetros de nuestro pipeline que utilizaremos para encontrar el más óptimo en todos los diferentes modelos (excepto los hiperparámetros de los estimadores)."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:30.511273Z","iopub.status.busy":"2020-12-14T10:14:30.510452Z","iopub.status.idle":"2020-12-14T10:14:30.514165Z","shell.execute_reply":"2020-12-14T10:14:30.513452Z"},"papermill":{"duration":0.176766,"end_time":"2020-12-14T10:14:30.514295","exception":false,"start_time":"2020-12-14T10:14:30.337529","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# ****Los comentarios en codigo no tendran tildes o caracteres especiales para ahorrarnos errores. *****\n\nn_bins_discretizer = [1, 2, 3]           # Numero de particiones\nn_imputer = [5, 10]                      # Numero de vecinos\nknn_weights = [\"uniform\", \"distance\"]    # Peso de los vecinos","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.171034,"end_time":"2020-12-14T10:14:30.85638","exception":false,"start_time":"2020-12-14T10:14:30.685346","status":"completed"},"tags":[]},"cell_type":"markdown","source":"* `n-bins`: Hemos escogido este conjunto para el discretizador porque en el primer estudio, de forma gráfica, vimos que el número de particiones no iba a ser muy elevado, como mucho veiamos 2 particiones claras.\n* `n_neighbors`: En el conjunto para los vecinos del imputador hemos cogido número impares porque reduce los casos de empate y no hemos puesto un número elevado de opciones para que el algortimo no tarde mucho en ejecutarse, ya que tiene un alto coste computacional.\n* `weights`: Por último, en el peso de los vecinos, son todas las opciones que existen para ese hiperparámetro."},{"metadata":{"papermill":{"duration":0.169885,"end_time":"2020-12-14T10:14:31.239615","exception":false,"start_time":"2020-12-14T10:14:31.06973","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### K-Vecinos"},{"metadata":{"papermill":{"duration":0.167341,"end_time":"2020-12-14T10:14:31.575666","exception":false,"start_time":"2020-12-14T10:14:31.408325","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El primer modelo a estudiar es de los `K-Vecinos`, mediante la clase `KNeighborsClassifier` de sklearn. Los hiperparámetros más importantes de este modelo y los que analizaremos son:\n* `n_neighbors` (Número de vecinos)\n* `weights` (Pesos de los vecinos)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:31.922945Z","iopub.status.busy":"2020-12-14T10:14:31.921859Z","iopub.status.idle":"2020-12-14T10:14:31.925482Z","shell.execute_reply":"2020-12-14T10:14:31.924855Z"},"papermill":{"duration":0.179644,"end_time":"2020-12-14T10:14:31.925656","exception":false,"start_time":"2020-12-14T10:14:31.746012","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"n_neighbors = 10 # Por defecto, el optimizador encontrara el numero de vecinos que de más score.\n\nk_neighbors_model = KNeighborsClassifier(n_neighbors)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.169313,"end_time":"2020-12-14T10:14:32.266316","exception":false,"start_time":"2020-12-14T10:14:32.097003","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Creamos el pipeline con lo visto anteriormente y el estimador. Lo creamos de una forma diferente al primer estudio, con `Pipeline` en lugar de `make_pipeline` para poder ponerle los nombres en lugar de que te los genere automaticamente. Obtenemos el nombre de los parámetros para saber cuales son y como escribirlos a la hora de optimizarlos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:32.613897Z","iopub.status.busy":"2020-12-14T10:14:32.612611Z","iopub.status.idle":"2020-12-14T10:14:32.616525Z","shell.execute_reply":"2020-12-14T10:14:32.617122Z"},"papermill":{"duration":0.181285,"end_time":"2020-12-14T10:14:32.617279","exception":false,"start_time":"2020-12-14T10:14:32.435994","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = k_neighbors_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.171932,"end_time":"2020-12-14T10:14:32.958286","exception":false,"start_time":"2020-12-14T10:14:32.786354","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Utilizaremos para los hiperparámetros:\n* `weights`: Todos los posibles valores de weights (uniform, distance)\n* `n_neighbors`: Los valores pequeños los eliminaremos ya que el modelo se sobreajusta, valores altos llevarán al modelo a parecerse a un 0-R y los valores pares de menor valor los eliminaremos porque producen empate. "},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:14:33.309374Z","iopub.status.busy":"2020-12-14T10:14:33.308507Z","iopub.status.idle":"2020-12-14T10:16:46.893998Z","shell.execute_reply":"2020-12-14T10:16:46.891264Z"},"papermill":{"duration":133.764588,"end_time":"2020-12-14T10:16:46.894217","exception":false,"start_time":"2020-12-14T10:14:33.129629","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"weights = [\"uniform\", \"distance\"]\nn_neighbors = [5, 6, 7, 8, 9]\n\nk_neighbors_clf = utils.optimize_params(pip,\n                                        X, y, cv,\n                                        estimator__weights=weights,\n                                        estimator__n_neighbors=n_neighbors,\n                                        ct__knnimputer__n_neighbors = n_imputer,\n                                        ct__knnimputer__weights = knn_weights,\n                                        discretizer__n_bins = n_bins_discretizer\n                                        )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.180839,"end_time":"2020-12-14T10:16:47.261395","exception":false,"start_time":"2020-12-14T10:16:47.080556","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para estudiar el resultado que nos ha dado el algortimo de optimización vamos a ir por partes. Primero analizaremos los hiperparámetros del imputador, luego los del discretizador y por último los del estimador.\n\nPara KNN el imputador ha sacado un resultado de `10 vecinos` y un peso `uniforme`. Estos resultados son los mejores que el algortimo al imputar los valores perdidos dan más información al clasificador para mejorar el score.\n\nPara el discretizador ha resultado en `3 particiones`. Lo cual es diferente a como lo vimos en el análisis exploratorio, según las gráficas el resultado más claro que teniamos es de 2 particiones. Pero, al ojo humano hay cosas que se escapan y más con una base de datos grande, que una máquina \"ve\" de mejor forma.\n\nPara el estimador KNN ha sacado un resultado de `5 vecinos` y un peso `uniforme`. Es un número de vecinos intermedio que permite que las fronteras de decisión queden más definidas. Al tener todos los vecinos el mismo peso, la información que debe guardar es mejor. Son unos buenos hiperparámetros que sin este algortimo de decisión podiamos haber utilizado por criterio.\n\nTasa de acierto resultante: $0.755 \\pm 0.05$"},{"metadata":{"papermill":{"duration":0.176592,"end_time":"2020-12-14T10:16:47.616584","exception":false,"start_time":"2020-12-14T10:16:47.439992","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Árboles de decisión."},{"metadata":{"papermill":{"duration":0.181795,"end_time":"2020-12-14T10:16:47.976871","exception":false,"start_time":"2020-12-14T10:16:47.795076","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El siguiente modelo a analizar es el de `árboles de decisión`, mediante la clase `DecisionTreeClassifier` de sklearn. Los hiperparámetros más importantes son:\n* `criterion` (criterio de corte)\n* `max_depth` (profundidad máxima del árbol)\n* `cpp_alpha` (coste de complejidad para encontrar el subárbol óptimo, este parámetro decidirá si hacer la poda)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:16:48.351539Z","iopub.status.busy":"2020-12-14T10:16:48.350285Z","iopub.status.idle":"2020-12-14T10:16:48.354098Z","shell.execute_reply":"2020-12-14T10:16:48.353064Z"},"papermill":{"duration":0.19056,"end_time":"2020-12-14T10:16:48.354305","exception":false,"start_time":"2020-12-14T10:16:48.163745","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"decision_tree_model = DecisionTreeClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.180279,"end_time":"2020-12-14T10:16:48.725353","exception":false,"start_time":"2020-12-14T10:16:48.545074","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Como haremos en todos los diferentes modelos, creamos el pipeline y generamos el diccionario para conocer los diferentes hiperparámetros de el pipeline creado. Por tanto, no volveremos a comentar este paso."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:16:49.092469Z","iopub.status.busy":"2020-12-14T10:16:49.091436Z","iopub.status.idle":"2020-12-14T10:16:49.097322Z","shell.execute_reply":"2020-12-14T10:16:49.096562Z"},"papermill":{"duration":0.19516,"end_time":"2020-12-14T10:16:49.097451","exception":false,"start_time":"2020-12-14T10:16:48.902291","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = clone(decision_tree_model)\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.17807,"end_time":"2020-12-14T10:16:49.455264","exception":false,"start_time":"2020-12-14T10:16:49.277194","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Utilizaremos para los hiperparámetros:\n* `criterion`: Todos los posibles valores de criterion (gini, entropy)\n* `max_depth`: Hasta un valor de profundidad 4, como hemos dicho en el primer estudio no hemos encontrado muchos cortes para separar la variable clase, por tanto, el árbol no se ramificará tanto y ramificarse tanto supone que el modelo se sobreajuste.\n* `ccp_alpha`: Los valores de alpha son muy pequeños y un buen árbol intentará tener el más bajo."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:16:49.820619Z","iopub.status.busy":"2020-12-14T10:16:49.81947Z","iopub.status.idle":"2020-12-14T10:21:06.276997Z","shell.execute_reply":"2020-12-14T10:21:06.274748Z"},"papermill":{"duration":256.646266,"end_time":"2020-12-14T10:21:06.277176","exception":false,"start_time":"2020-12-14T10:16:49.63091","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"criterion = [\"gini\", \"entropy\"]\nmax_depth = [0, 1, 2, 3, 4]\nccp_alpha = [0.0, 0.1, 0.2]\n\ndecision_tree_clf = utils.optimize_params(pip,\n                                          X, y, cv,\n                                          estimator__criterion=criterion,\n                                          estimator__max_depth=max_depth,\n                                          estimator__ccp_alpha=ccp_alpha,\n                                          ct__knnimputer__n_neighbors = n_imputer,\n                                          ct__knnimputer__weights = knn_weights,\n                                          discretizer__n_bins = n_bins_discretizer\n                                         )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.187071,"end_time":"2020-12-14T10:21:06.647221","exception":false,"start_time":"2020-12-14T10:21:06.46015","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Utilizaremos el mismo orden de evaluación que con el modelo anterior.\n\nEn la primera posición hay muchos empates, el resultado de la tabla nos muestra las 5 primeras filas y todas consiguen la misma puntación. El algortimo nos ha imprimido por pantalla los mejores hiperparámetros resultados, pero esta no es la única combinación posible, si no que es la primera que el algortimo ha entrenado.\n\nComo vemos en el resultado y en las tablas hay muchas combinaciones de mejores resultados: \n\n* Teniendo para el imputador todo el conjunto de hiperparámetros. \n* Para el discretizador por 2 y 3 particiones.\n* Para el estimador, todas las profundidades de árboles del conjunto que hemos indicado, los diferentes criterios de corte y un alpha de 0.0 y 0.1. Un resultado de alpha pequeño permite tener resultados de subárboles óptimos.\n\nEstos resultados son los mejores que podemos utilizar para este modelo. Aunque los que ha devuelto el algoritmo son los que se utilizarán en la evaluación de todos los modelos.\n\nTasa de acierto resultante: $0.747 \\pm 0.062$\t"},{"metadata":{"papermill":{"duration":0.185132,"end_time":"2020-12-14T10:21:07.014911","exception":false,"start_time":"2020-12-14T10:21:06.829779","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### AdaBoost."},{"metadata":{"papermill":{"duration":0.186173,"end_time":"2020-12-14T10:21:07.388872","exception":false,"start_time":"2020-12-14T10:21:07.202699","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El siguiente modelo a analizar es el de `adaboost` o `boosting`, mediante la clase `AdaBoostClassifier` de sklearn. Los hiperparámetros más importantes son:\n* `base_estimator` (el estimador base del modelo que será un árbol de decision)\n* `learning_rate` (tasa de aprendizaje, reduce la importancia de cada clasificador)\n\nHemos visto que como hiperparámetro, adaboost tiene un árbol de decisión por lo que los hiperparámetros de este modelo también estarán incluidos. Muchos de los *ensembles* que estabmos viendo funcionaran con árboles de decisión y por lo tanto con sus hiperparámetros.\n* `criterion` (criterio)\n* `max_depth` (profundidad máxima)\n* `cpp_alpha` (coste de complejidad)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:21:07.773374Z","iopub.status.busy":"2020-12-14T10:21:07.772545Z","iopub.status.idle":"2020-12-14T10:21:07.776214Z","shell.execute_reply":"2020-12-14T10:21:07.775577Z"},"papermill":{"duration":0.19755,"end_time":"2020-12-14T10:21:07.77634","exception":false,"start_time":"2020-12-14T10:21:07.57879","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"adaboost_model = AdaBoostClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:21:08.191707Z","iopub.status.busy":"2020-12-14T10:21:08.190558Z","iopub.status.idle":"2020-12-14T10:21:08.194715Z","shell.execute_reply":"2020-12-14T10:21:08.195611Z"},"papermill":{"duration":0.211434,"end_time":"2020-12-14T10:21:08.195848","exception":false,"start_time":"2020-12-14T10:21:07.984414","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = adaboost_model\n\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.185211,"end_time":"2020-12-14T10:21:08.572177","exception":false,"start_time":"2020-12-14T10:21:08.386966","status":"completed"},"tags":[]},"cell_type":"markdown","source":"En este caso los hiperparámeetros del árbol de decisión serán menores que anteriormente para que el coste computacional baje y viendo los resultados de la optimización anterior nos parece lo correcto.\n* `ccp_alpha`: Solo dos conjuntos de alpha, los que hemos visto anteriormente que eran mejor para el árbol de decisión.\n* `criterion`: Todos los criterios posibles ya que no son muchos (gini y entropy).\n* `max_depth`: Como nos indica en la libreta de \"Aprendizaje de Modelos\", el algortimo de adaboost funciona mejor con árboles de decisión poco profundos (*shallow decision trees*), por su poder de generalización. Por esto, el conjunto de profundidad va a ser de números bajos (1 y 2).\n\nPara el resto de hiperparámetros:\n* `base_estimator`: Con los hiperparámetro anteriores se encontrará el mejor estimador base, siendo este un árbol de decisión.\n* `learning_rate`: Por defecto este hiperparámetro es de 1. Entonces necesitaremos números cercanos a este. Probaremos con 1 y 0.95."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:21:08.966256Z","iopub.status.busy":"2020-12-14T10:21:08.965116Z","iopub.status.idle":"2020-12-14T10:31:14.973731Z","shell.execute_reply":"2020-12-14T10:31:14.971911Z"},"papermill":{"duration":606.21201,"end_time":"2020-12-14T10:31:14.973858","exception":false,"start_time":"2020-12-14T10:21:08.761848","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(pip,\n                                     X, y, cv,\n                                     estimator__base_estimator=base_estimator,\n                                     estimator__learning_rate=learning_rate,\n                                     estimator__base_estimator__criterion=criterion,\n                                     estimator__base_estimator__max_depth=max_depth,\n                                     estimator__base_estimator__ccp_alpha=ccp_alpha,\n                                     ct__knnimputer__n_neighbors = n_imputer,\n                                     ct__knnimputer__weights = knn_weights,\n                                     discretizer__n_bins = n_bins_discretizer\n                                     )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.190412,"end_time":"2020-12-14T10:31:15.355643","exception":false,"start_time":"2020-12-14T10:31:15.165231","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Siguiendo el orden que hemos establecido:\n\nPara el imputador KNN el número de vecinos es de `5` y un peso `uniform`. Los valores por defecto que hemos elegido y hemos visto que funcionan bien.\n\nPara el discretizador el número de particiones es de `3 particiones`. Como hemos comentado en k-vecinos, este resultado no lo esperabamos pero funciona mejor que discretizar en 2 particiones.\n\nPara el estimador el número de profundidad máxima es de `1`. Vemos que con la mínima profunidad que hemos elegido obtiene el mejor resultado. El alpha de `0.0`, como ocurría en árboles de decisión. Para los dos últimos hiperparámetros, alpha y learning rate, vemos que cualquier combinación con el resto de hiperparámetros anteriores se obtiene el máximo *score*. Hay 4 empates en la primera posición, pero como ocurría anteriormente, se elegirá el que devuelve el algortimo que es el primero en crear.\n\nTasa de acierto resultante: $0.753 \\pm 0.045$\t"},{"metadata":{"papermill":{"duration":0.190398,"end_time":"2020-12-14T10:31:15.737333","exception":false,"start_time":"2020-12-14T10:31:15.546935","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Bagging."},{"metadata":{"papermill":{"duration":0.193348,"end_time":"2020-12-14T10:31:16.119953","exception":false,"start_time":"2020-12-14T10:31:15.926605","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El siguiente modelo a analizar es el de `bagging`, mediante la clase `BaggingClssifier` de sklearn. Los hiperparámetros más importantes son:\n* `base_estimator` (el estimador base del modelo que será un árbol de decision)\n* `criterion` (criterio de corte del árbol)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:31:16.513511Z","iopub.status.busy":"2020-12-14T10:31:16.512421Z","iopub.status.idle":"2020-12-14T10:31:16.515837Z","shell.execute_reply":"2020-12-14T10:31:16.515121Z"},"papermill":{"duration":0.200293,"end_time":"2020-12-14T10:31:16.51596","exception":false,"start_time":"2020-12-14T10:31:16.315667","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"bagging_model = BaggingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:31:16.905281Z","iopub.status.busy":"2020-12-14T10:31:16.904185Z","iopub.status.idle":"2020-12-14T10:31:16.907805Z","shell.execute_reply":"2020-12-14T10:31:16.908398Z"},"papermill":{"duration":0.203575,"end_time":"2020-12-14T10:31:16.908552","exception":false,"start_time":"2020-12-14T10:31:16.704977","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = bagging_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])\npip.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.192971,"end_time":"2020-12-14T10:31:17.29374","exception":false,"start_time":"2020-12-14T10:31:17.100769","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para los hiperparámetros de bagging escogeremos:\n* `base_estimator`: El árbol de decisión creado anteriormente.\n* `criterion`: Todas las combinaciones de criterio para el árbol (gini, entropy)\n\nTambién vamos a incluir la profundidad máxima del árbol, ya que bagging funciona mejor con árboles de decisión profundos.\n* `max_depth`: Vamos a probar con la profundidad por defecto que es 1, 4 y 10 (muy profundo)."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:31:17.684601Z","iopub.status.busy":"2020-12-14T10:31:17.683485Z","iopub.status.idle":"2020-12-14T10:32:41.058021Z","shell.execute_reply":"2020-12-14T10:32:41.05866Z"},"papermill":{"duration":83.575459,"end_time":"2020-12-14T10:32:41.058826","exception":false,"start_time":"2020-12-14T10:31:17.483367","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1,4,10]\n\nbagging_clf = utils.optimize_params(pip,\n                                    X, y, cv,\n                                    estimator__base_estimator=base_estimator,\n                                    estimator__base_estimator__criterion=criterion,\n                                    estimator__base_estimator__max_depth=max_depth,\n                                    ct__knnimputer__n_neighbors = n_imputer,\n                                    ct__knnimputer__weights = knn_weights,\n                                    discretizer__n_bins = n_bins_discretizer\n                                    )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.194614,"end_time":"2020-12-14T10:32:41.449257","exception":false,"start_time":"2020-12-14T10:32:41.254643","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Como ocurre en el resto, hay muchos empates.Entonces, para el imputador KNN el número de vecinos y el peso produce un mismo resultado final.\n\nPara el discretizador el número de particiones es de `2 particiones`. Para este clasificador, vemos en los resultados que este valor de partición es el más idoneo.\n\nPor los empates que encontramos, para el estimador el criterio del árbol de decision da igual si es por entropía o gini. Mientra que la profundidad máxima del árbol encontrada es de `1 de profundidad`. Con el valor por defecto encuentra el mejor *score*\n\nTasa de acierto resultante: $0.747 \\pm 0.062$"},{"metadata":{"papermill":{"duration":0.194542,"end_time":"2020-12-14T10:32:41.837835","exception":false,"start_time":"2020-12-14T10:32:41.643293","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Random Forest."},{"metadata":{"papermill":{"duration":0.196817,"end_time":"2020-12-14T10:32:42.230536","exception":false,"start_time":"2020-12-14T10:32:42.033719","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El siguiente modelo a analizar es el de `random forest`, mediante la clase `RandomForestClassifier` de sklearn. Los hiperparámetros más importantes son:\n* `criterion` (criterio de partición)\n* `max_features` (número de atributos para buscar la mejor partición)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:32:42.628363Z","iopub.status.busy":"2020-12-14T10:32:42.627366Z","iopub.status.idle":"2020-12-14T10:32:42.630619Z","shell.execute_reply":"2020-12-14T10:32:42.629976Z"},"papermill":{"duration":0.203265,"end_time":"2020-12-14T10:32:42.630757","exception":false,"start_time":"2020-12-14T10:32:42.427492","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"random_forest_model = RandomForestClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:32:43.0351Z","iopub.status.busy":"2020-12-14T10:32:43.033988Z","iopub.status.idle":"2020-12-14T10:32:43.03745Z","shell.execute_reply":"2020-12-14T10:32:43.036831Z"},"papermill":{"duration":0.206981,"end_time":"2020-12-14T10:32:43.037596","exception":false,"start_time":"2020-12-14T10:32:42.830615","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = random_forest_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.194424,"end_time":"2020-12-14T10:32:43.430643","exception":false,"start_time":"2020-12-14T10:32:43.236219","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para los hiperparámetros de random forest:\n* `criterion`: Los dos diferentes valores que hemos visto en el resto de ejemplos.\n* `max_features`: Para este hiperparámetro utilizaremos sqrt y log2 aunque tambíen existe la posibilidad de auto (visto en la documentación de sklearn), funciona de la misma forma que sqrt. Como su nombre indica, sqrt hace la raiz cuadrada del número de atributos y log2 hace el logaritmo en base 2 del número de atributos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:32:43.830044Z","iopub.status.busy":"2020-12-14T10:32:43.829242Z","iopub.status.idle":"2020-12-14T10:36:06.749122Z","shell.execute_reply":"2020-12-14T10:36:06.74849Z"},"papermill":{"duration":203.124023,"end_time":"2020-12-14T10:36:06.74925","exception":false,"start_time":"2020-12-14T10:32:43.625227","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"criterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf = utils.optimize_params(pip,\n                                          X, y, cv,\n                                          estimator__criterion=criterion,\n                                          estimator__max_features=max_features,\n                                          ct__knnimputer__n_neighbors = n_imputer,\n                                          ct__knnimputer__weights = knn_weights,\n                                          discretizer__n_bins = n_bins_discretizer\n                                          )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.205767,"end_time":"2020-12-14T10:36:07.158432","exception":false,"start_time":"2020-12-14T10:36:06.952665","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Tenemos dos empates en la primera posición, vamos a indicar que valores son los mejores y cuales dan igual.\n\nPara el imputador KNN el número de vecinos es de `10` y un peso `uniform`. Al igual que ocurria con el modelo de clasificación de K-Vecinos.\n\nPara el discretizador el número de particiones es de `3 particiones`.\n\nPara el estimador el criterio elegido es por `entropía` y con el máximo de características da igual si son por raiz cuadrada o logartimo en base 2.\n\nTasa de acierto resultante: $0.746 \\pm 0.052$"},{"metadata":{"papermill":{"duration":0.201405,"end_time":"2020-12-14T10:36:07.563283","exception":false,"start_time":"2020-12-14T10:36:07.361878","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Gradient Boosting."},{"metadata":{"papermill":{"duration":0.202843,"end_time":"2020-12-14T10:36:07.968852","exception":false,"start_time":"2020-12-14T10:36:07.766009","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El siguiente modelo a analizar es el de `gradient boosting`, mediante la clase `GradientBoostingClassifier` de sklearn. Los hiperparámetros más importantes son:\n* `learning rate` (tasa de aprendizaje)\n* `criterion` (criterio de partición)\n* `max_depth` (máxima profundidad)\n* `cpp_alpha` (coste de complejidad)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:36:08.386409Z","iopub.status.busy":"2020-12-14T10:36:08.385368Z","iopub.status.idle":"2020-12-14T10:36:08.388121Z","shell.execute_reply":"2020-12-14T10:36:08.388689Z"},"papermill":{"duration":0.210883,"end_time":"2020-12-14T10:36:08.388871","exception":false,"start_time":"2020-12-14T10:36:08.177988","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"gradient_boosting_model = GradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:36:08.800391Z","iopub.status.busy":"2020-12-14T10:36:08.799568Z","iopub.status.idle":"2020-12-14T10:36:08.802868Z","shell.execute_reply":"2020-12-14T10:36:08.80211Z"},"papermill":{"duration":0.211683,"end_time":"2020-12-14T10:36:08.803011","exception":false,"start_time":"2020-12-14T10:36:08.591328","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = gradient_boosting_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.203899,"end_time":"2020-12-14T10:36:09.213642","exception":false,"start_time":"2020-12-14T10:36:09.009743","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para los hiperparámetros de gradient boosting:\n\n* `learning rate`: Por defecto la tasa de aprendizaje es de 0.01. Entonces, utilizaremos valores cercanos a este.\n* `criterion`: Para este modelo el clasificador es diferente a los árboles de decisión ya que se usan los residuos de las modelos aprendidos. Tenemos entonces mse o error cuadrático medio (ECM) y Friedman mse o el ECM de Friedman.\n* `max_depth`: Máxima profundidad del árbol que aprende, cogeremos valores pequeños que hemos visto que funcionan bien.\n* `cpp_alpha`: Y el coste de complejidad como siempre."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:36:09.630133Z","iopub.status.busy":"2020-12-14T10:36:09.62899Z","iopub.status.idle":"2020-12-14T10:44:01.929246Z","shell.execute_reply":"2020-12-14T10:44:01.928495Z"},"papermill":{"duration":472.51227,"end_time":"2020-12-14T10:44:01.929379","exception":false,"start_time":"2020-12-14T10:36:09.417109","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"learning_rate = [0.01, 0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf = utils.optimize_params(pip,\n                                              X, y, cv,\n                                              estimator__learning_rate=learning_rate,\n                                              estimator__criterion=criterion,\n                                              estimator__max_depth=max_depth,\n                                              estimator__ccp_alpha=ccp_alpha,\n                                              ct__knnimputer__n_neighbors = n_imputer,\n                                              ct__knnimputer__weights = knn_weights,\n                                              discretizer__n_bins = n_bins_discretizer\n                                              )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.212381,"end_time":"2020-12-14T10:44:02.362988","exception":false,"start_time":"2020-12-14T10:44:02.150607","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Como es habitual tenemos empates en el primer puesto. Diremos que valores de loshiperparámetros son los mejores y cuales dan igual entre unos y otros.\n\nPara el imputador KNN el número de vecinos es de `5` y un peso `distance`\n\nPara el discretizador el número de particiones es de `3 particiones`\n\nPara el estimador el alpha elegido es `0.0` (para el árbol de decisión hemos visto que es el mejor alpha en el problema), la tasa de aprendizaje es la intermedia entre los diferentes valores elegidos `0.05`, la máxima profundidad es de `1`. Por último, para el criterio da igual entre el ECM y el ECM de friedman, el empate ocurre en esta dos combinaciones de los valores ya vistos y los posibles del criterio.\n\nTasa de acierto resultante: $0.752 \\pm 0.046$\t"},{"metadata":{"papermill":{"duration":0.219986,"end_time":"2020-12-14T10:44:02.807199","exception":false,"start_time":"2020-12-14T10:44:02.587213","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Histogram Gradient Boosting."},{"metadata":{"papermill":{"duration":0.216002,"end_time":"2020-12-14T10:44:03.23589","exception":false,"start_time":"2020-12-14T10:44:03.019888","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El siguiente modelo a analizar es el de `histogram gradient boosting`, mediante la clase `HistgradientBoostingClassifier` de sklearn. Los hiperparámetros más importantes son:\n* `learning rate` (tasa de aprendizaje)\n* `max_leaf_nodes` (máximo número de nodos hoja)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:44:03.664069Z","iopub.status.busy":"2020-12-14T10:44:03.663275Z","iopub.status.idle":"2020-12-14T10:44:03.666792Z","shell.execute_reply":"2020-12-14T10:44:03.666076Z"},"papermill":{"duration":0.219351,"end_time":"2020-12-14T10:44:03.666921","exception":false,"start_time":"2020-12-14T10:44:03.44757","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:44:04.107774Z","iopub.status.busy":"2020-12-14T10:44:04.106834Z","iopub.status.idle":"2020-12-14T10:44:04.110552Z","shell.execute_reply":"2020-12-14T10:44:04.109802Z"},"papermill":{"duration":0.231919,"end_time":"2020-12-14T10:44:04.110678","exception":false,"start_time":"2020-12-14T10:44:03.878759","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimator = hist_gradient_boosting_model\npip = Pipeline([('ct',column_transformer),('discretizer',discretizer),('estimator',estimator)])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.222576,"end_time":"2020-12-14T10:44:04.545822","exception":false,"start_time":"2020-12-14T10:44:04.323246","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Por último, los hiperparámetros para historical gradient boosting son:\n* `learning rate`: Igual que con gradient boosting, el valor por defecto es 0.1. Tendremos valores cercanos a este.\n* `max_leaf_nodes`: Y por último, el número máximo nodos hoja. Tendremos valores diferentes entre ellos para ver cual mejora el modelo, pondremos valores bajos porque hemos visto que nuestro árbol no crea mucha profundidad y no especifica tanto."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:44:04.999733Z","iopub.status.busy":"2020-12-14T10:44:04.996774Z","iopub.status.idle":"2020-12-14T10:50:51.334254Z","shell.execute_reply":"2020-12-14T10:50:51.335003Z"},"papermill":{"duration":406.576121,"end_time":"2020-12-14T10:50:51.335225","exception":false,"start_time":"2020-12-14T10:44:04.759104","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"learning_rate = [0.01, 0.05, 0.1]\nmax_leaf_nodes = [5,10,15, 31]\n\nhist_gradient_boosting_clf = utils.optimize_params(pip,\n                                                   X, y, cv,\n                                                   estimator__learning_rate=learning_rate,\n                                                   estimator__max_leaf_nodes=max_leaf_nodes,\n                                                   ct__knnimputer__n_neighbors = n_imputer,\n                                                   ct__knnimputer__weights = knn_weights,\n                                                   discretizer__n_bins = n_bins_discretizer\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.218657,"end_time":"2020-12-14T10:50:51.79464","exception":false,"start_time":"2020-12-14T10:50:51.575983","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para el imputador KNN el número de vecinos es de `10` y un peso `uniform`.\n\nPara el discretizador el número de particiones es de `3 particiones`.\n\nPara el estimador la tasa de aprendizaje es `0.1` la mayor del conjunto y por defecto y el máximo número de nodos hoja es 15 o 31, hay un empate.\n\nTasa de acierto resultante: $0.751 \\pm 0.046$"},{"metadata":{"papermill":{"duration":0.225674,"end_time":"2020-12-14T10:50:52.241345","exception":false,"start_time":"2020-12-14T10:50:52.015671","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Construcción y validación del modelo final"},{"metadata":{"papermill":{"duration":0.217939,"end_time":"2020-12-14T10:50:52.679364","exception":false,"start_time":"2020-12-14T10:50:52.461425","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Por último, vamos a ver de todos los modelos que hemos visto y creado cual es el que obtiene una tasa de acierto mayor para el conjunto de entrenamiento que hemos dividido con el *houldout*."},{"metadata":{"papermill":{"duration":0.223616,"end_time":"2020-12-14T10:50:53.121967","exception":false,"start_time":"2020-12-14T10:50:52.898351","status":"completed"},"tags":[]},"cell_type":"markdown","source":"En el proceso de optimización con validación cruzada el mejor resultado de tasa de acierto lo obteniamos con K-Vecinos con `0.755` seguido muy cerca de AdaBoost (`0.753`) y Grandient Boosting (`0.752`). Todo esto dentro del conjunto de entrenamiento."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:50:53.576754Z","iopub.status.busy":"2020-12-14T10:50:53.57595Z","iopub.status.idle":"2020-12-14T10:50:53.579817Z","shell.execute_reply":"2020-12-14T10:50:53.579066Z"},"papermill":{"duration":0.234479,"end_time":"2020-12-14T10:50:53.579977","exception":false,"start_time":"2020-12-14T10:50:53.345498","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.220714,"end_time":"2020-12-14T10:50:54.019626","exception":false,"start_time":"2020-12-14T10:50:53.798912","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Mediante la lista de estimadores que hemos creado y una función en el script vamos a evaular todos los modelos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:50:54.478629Z","iopub.status.busy":"2020-12-14T10:50:54.47776Z","iopub.status.idle":"2020-12-14T10:50:54.622978Z","shell.execute_reply":"2020-12-14T10:50:54.623701Z"},"papermill":{"duration":0.383778,"end_time":"2020-12-14T10:50:54.623884","exception":false,"start_time":"2020-12-14T10:50:54.240106","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X = X_test\ny = y_test\n\nutils.evaluate_estimators(estimators, X, y)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.220523,"end_time":"2020-12-14T10:50:55.09389","exception":false,"start_time":"2020-12-14T10:50:54.873367","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Gradient Boosting es el modelo que obtiene la mejor tasa de acierto con un `0.757576` de todos los estudiados. La mayoría de ensembles nos dan una tasa de acierto mayor que modelos simples como árboles de decisión. Cabe remarcar que KNN da una tasa de acierto considerable para el tipo de modelo que es. Por ello, vamos a hacer una evaluación en terminos de coste para elegir el mejor clasificador para este dataset.\n\nLos modelos *ensemble* son modelos computacionalmente más costos, por tanto el tiempo de inferencia es mucho mayor que modelos como Árboles de Decisión o KNN. A la hora de elegir que modelo es mejor no solo nos podemos fijar en terminos de tasa de aciertos (*score*), si un modelo *ensemble* y un modelo sencillo consiguen resultados muy parecidos o iguales es mejor elegir el modelo sencillo.\n\nEn los resultados observanos que KNN obtiene la misma tasa de acierto que Random Forest (`0.727273`). En este caso eligiriamos hacer nuestro clasificador con KNN por la sencillez de este. También, Decision Tree y Bagging obtienen un mismo *score* (`0.718615`), ocurriría lo mismo que anteriormente.\n\nEntonces, vemos que modelos como KNN consiguen más puntuación que muchos de los *ensembles* e incluso estos modelos que quedan por encima de KNN tienen una diferencia poco significativa que haría que nos decatasemos por el modelo de los vecinos cercanos."},{"metadata":{"papermill":{"duration":0.225813,"end_time":"2020-12-14T10:50:55.542728","exception":false,"start_time":"2020-12-14T10:50:55.316915","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Dataset Wisconsin"},{"metadata":{},"cell_type":"markdown","source":"## Carga de datos"},{"metadata":{},"cell_type":"markdown","source":"Vamos a empezar cargando las datos al igual que hicimos en la práctica anterior."},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"../input/wisconsin/wisconsin.csv\"\n\nindex_col = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index_col, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y comprobando que se han cargado correctamente:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A su vez, lo dividimos en variables predictoras (X) y variable clase (y):"},{"metadata":{"trusted":true},"cell_type":"code","source":"(X, y) = utils.divide_dataset(data, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a comprobar que se ha separado correctamente. Comenzamos con las variables predictoras:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y continuamos con la variable clase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por último, dividimos el conjunto de datos en entrenamiento y prueba mediante un holdout estratificado:"},{"metadata":{"trusted":true},"cell_type":"code","source":"stratify = y\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=stratify,\n                                                      train_size=train_size,\n                                                      random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y nos aseguramos que se ha realizado adecuadamente. Comenzamos con el conjunto de datos de entrenamiento:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y finalizamos con el conjunto de datos de prueba:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.sample(5, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pipeline"},{"metadata":{},"cell_type":"markdown","source":"Dado que en la práctica anterior se realizó un análisis exploratorio de los datos para sacar unas conclusiones que nos ayudaran a hacer un buen preprocesamiento de datos, vamos a aprovechar que ya disponemos de esta información y a aplicar el pipeline que calculamos en la práctica anterior.\n\nEn la práctica anterior habíamos definido una función para eliminar estas columnas y las eliminábamos mediante un FuctionTransformer, pero ahora las vamos a eliminar con la función make_column_transformer ya que así no necesitamos crear nosotros nuestra propia función.\n\nAhora vamos a crear un column transformer que nos permita eliminar las variables que habíamos detectado que eran inútiles y la variable corrupta `Unnamed: 32`."},{"metadata":{"trusted":true},"cell_type":"code","source":"column_rm_mean = [\"smoothness_mean\", \"symmetry_mean\", \"fractal_dimension_mean\"]\ncolumn_rm_se = [\"texture_se\", \"smoothness_se\", \"symmetry_se\", \"fractal_dimension_se\"]\ncolumn_rm_worst = [\"symmetry_worst\", \"fractal_dimension_worst\"]\ncolumn_error = [\"Unnamed: 32\"]\ncolumn_rm = column_rm_mean + column_rm_se + column_rm_worst + column_error\n\ndrop_col = make_column_transformer((\"drop\", column_rm), remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora definimos un FuctionSampler que nos permite eliminar los outlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_rm = FunctionSampler(func=utils.outlier_rejection, kw_args={'seed':random_state})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como último elemento del Pipeline creamos el discretizador que le vamos a pasar. En la práctica anterior mediante el análisis multivariado observamos que lo mejor era discretizar las variables en dos intervalos, pero como ahora vamos a realizar una validación cruzada para elegir los mejores hiperparámetros de cada algoritmo para este dataset, también vamos a incluir que compruebe a discretizar en 3 intervalos, para comprobar que si las conclusiones del análisis multivariado eran correctas."},{"metadata":{"trusted":true},"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\", encode=\"onehot-dense\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indicar que todavía no podemos crear el pipeline, pues hay que crear un pipeline para cada modelo, ya que hay que incluir dicho modelo en el pipeline."},{"metadata":{},"cell_type":"markdown","source":"## Selección de modelos de clasificación supervisada"},{"metadata":{},"cell_type":"markdown","source":"Antes de empezar con los modelos tenemos que definir la validación cruzada que les vamos a aplicar. En concreto vamos a definir una 5x10 validación cruzada estratificada al igual que para `Diabetes`."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para simplificar vamos a renombrar a las variables de entrenamiento."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_train\ny = y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos a definir que valores para el discretizador vamos a optimizar mediante la validación cruzada. En este caso como se dijo anteriormente vamos a probar si efectivamente es mejor el discretizador con dos intervalos que con tres (se validará para todos los modelos)."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_bins = [2, 3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vecinos más cercanos"},{"metadata":{},"cell_type":"markdown","source":"Vamos a empezar la selección de modelos con el algoritmo de los vecinos más cercanos.\n\nPara ello igual que con el dataset Diabetes vamos a utilizar la clase KNeighborsClassifier. Así que vamos a crearlo y lo vamos a añadir en el pipeline junto con los otros elementos que definimos en el apartado del pipeline.\n\nDespués vamos a usar el método get_params().key() que nos devuelve una lista con el nombre de todos los hiperparámetros que hay en el pipeline. Esto es importante porque para indicar los hiperparámetros a optimizar necesitamos conocer su nombre, y al incluir el modelo k vecinos en el pipeline los nombres de sus hiperparámetros cambian."},{"metadata":{"trusted":true},"cell_type":"code","source":"k_neighbors_model = KNeighborsClassifier()\n\npipe_k_neighbors = make_pipeline(drop_col, outlier_rm, discretizer, k_neighbors_model)\npipe_k_neighbors.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por ejemplo el hiperparámetro n_neighbors ahora se llama kneighborsclassifier__n_neighbors. Y el hiperparámetro n_bins del discretizador se llama kbinsdiscretizer__n_bins.\n\nPodemos llegar a la conclusión que la forma de llamar a los hiperparámetros del pipeline es el nombre de la clase en minúsculas (kneighborsclassifier, kbinsdiscretizer), seguido de dos barras bajas (__) y seguido del nombre del hiperparámetro original."},{"metadata":{},"cell_type":"markdown","source":"Los hiperparámetros que vamos a optimizar en este caso son:\n\n* n_neighbors. El número de vecinos. Vamos a probar entre 1 y 6 vecinos.\n* weights. La función de peso que utilizará el algoritmo de los vecinos más cercanos. Probaremos con las funciones de peso uniform y distance.\n* Además, como se indicó anteriormente que siempre vamos a optimizar el número de intervalos del discretizador (2 o 3)."},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = [\"uniform\", \"distance\"]\nn_neighbors = [1, 2, 3, 4, 5, 6]\n\nk_neighbors_clf = utils.optimize_params(pipe_k_neighbors,\n                                        X, y, cv,\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors,\n                                        kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido que la configuración de hiperparámetros óptima para este caso en concreto es utlizar 4 vecinos, la función de peso distancia y discretizar las variables predictoras en tres intervalos.\n\nComo nos podíamos imaginar hemos obtenido un número de vecinos intermedio, en este caso 4. Esto es debido a que si generamos un modelo utilizando solo al vecino más cercano, estaríamos creando un modelo muy sobreajustado a los datos de entrenamiento que no sería capaz de generalizar, ya que solo se fija en el dato más cercano. Si por el contrario generamos un modelo que utiliza una gran cantidad de vecinos para clasificar, se generaría un modelo similar al 0R ya que clasificaría como la clase mayoritaría.\n\nPodemos ver que los valores elegidos para n_neighbors en las mejores configuraciones de parámetros tienen un valor de entre 3 y 6. Pudiendo ver que utilizar solo 1 o 2 vecinos han obtenido peores resultados porque como hemos indicado se ajustan a los datos y no permiten que el modelo sea capaz de generalizar.\n\nComo vemos hemos obtenido como función de peso distance en la configuración óptima, incluso parece que funciona bien con este dataset pues si miramos las cinco mejores configuraciones obtenidas se ha elegido distance en cuatro de ellas. Esto es porque esta función le asigna un peso en función de la distancia que hay entre el punto a clasificar y los puntos más cercanos, mientras que uniform le aigna el mismo peso, independientemente de la distancia que haya. Aún así, dependiendo del dataset que estemos tratando y de otros factores (como el preprocesamiento que se haya realizado y como afecten el resto de hiperparámetros) puede ser que funcione mejor otra función de pesos.\n\nPor último, tenemos que indicar que por el contrario a lo que nos esperábamos, en este caso ha funcionado mejor un discretizado con tres intervalos que con dos, tal y como vimos que debería de funcionar mejor en el análisis exploratorio de la práctica anterior.\n\nComprobaremos con el resto de modelos si se trata de un caso aislado y es que para este modelo en concreto funciona mejor con 3 intervalos, o es que nuestra conclusión del análisis exploratorio estaba equivocada."},{"metadata":{},"cell_type":"markdown","source":"### Árboles de decisión"},{"metadata":{},"cell_type":"markdown","source":"Para los árboles de decisión vamos a usar la clase DecisionTreeClassifier y la vamos a agregar al pipeline de la misma forma hicimos con los k vecinos. También observaremos el nombre de los hiperparámetros, aunque por las conclusiones que sacamos en el k vecinos nos podemos imaginar como serán."},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_model = DecisionTreeClassifier(random_state=random_state)\n\npipe_tree = make_pipeline(drop_col, outlier_rm, discretizer, decision_tree_model)\npipe_tree.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los árboles de decisión tienen más hiperparámetros que el modelo de los k vecinos, aunque no podemos optimizarlos todos ya que implicaría un gran coste computacional, necesitando mucho tiempo.\n\nPor eso seleccionaremos los que creamos más importantes. Optimizaremos:\n\n* criterion. El criterio utilizado para seleccionar la mejor partición, que puede ser gini o entropía.\n* max_depth. La máxima profundidad del árbol, que probaremos con 3, 4 y 5. No probando con 1 y 2 porque el árbol saldría muy pequeño y obtendríamos un modelo con underfitting. Por el contrario si obtenemos un árbol muy grande (con profundidad mayor que 6) tendríamos un modelo con overfitting, es decir sería demasiado complejo y se ajustaría demasiado a los datos de entrenamiento.\n* ccp_alpha. Parámetro de complejidad que usa el algoritmo de post-poda, es decir, este hiperparámetro nos permitirá decidir si podar alguna rama del árbol. \n\nOtro hiperparámetro que podría resultar interesante sería el min_samples_split o el min_samples_leaf que representan al mínimo número de registros que tiene que haber en un nodo para poder ramificarlo, y el mínimo número de ejemplos que tiene que haber en un nodo para que pueda ser hoja. Esto nos ayudaría a reducir el sobreajuste, aunque como esto ya lo hacemos con el max_depth no vamos a optimizar estos hiperparámetros pues sería muy costoso."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Should not modify the original model\nestimator = clone(decision_tree_model)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [3, 4, 5]\nccp_alpha = [0.0, 0.1]\n\ndecision_tree_clf = utils.optimize_params(pipe_tree,\n                                          X, y, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                          kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido como configuración óptima una profundidad máxima de 3, con un criterio gini, un 0.0 en el parámetro de post poda y ha discretizado en 2 intervalos.\n\nPrimero podemos ver que en este caso ha funcionado mucho mejor el discretizado en 2 intervalos como vimos en la práctica anterior, obteniendo mejores resultados en los que se ha usado 2 intervalos en lugar de 3, al contrario que en el k vecinos, en el que las 5 configuraciones óptimas utilizaban 3 intervalos para discretizar. Seguiremos observando en el resto de modelos para ver si obtienen 2 intervalos como en este caso.\n\nPodemos ver también que la configuración óptima ha utilizado gini aunque si continuamos mirando en la tabla podemos ver que también se han obtenido buenos resultados con el criterio entropía. Así podemos concluir que el valor óptimo de este hiperparámetro varía en función de otros factores, como otros hiperparámetros o los datos que tengamos.\n\nComo profundidad máxima hemos obtenido una profundidad de 3, generandose así un árbol que no es muy complejo. Esto es importante, ya que un árbol muy complejo implica que está sobreajustado a los datos de entrenamiento, además de generar reglas más complejas. Por eso podemos ver en la tabla que se han obtenido mejores resultados con árboles de profundidad 3 y 4 que los árboles de profundidad 5.\n\nTambién vemos que se han obtenido mejores resultados con un valor de ccp_alpha de 0. Esto implica que no se poda en los árboles obtenidos. Este parámetro nos permite reducir la varianza en los árboles complejos, pero en este caso como no hemos generado árboles demasiado complejos pues parece que no ha obtenido buenos resultados podándolos. Ya que podemos ver que los mejores resultados se han obtenido con un valor de 0 para este hiperparámetro, mejor que con un valor de 0.1. Aún así es un hiperparámetro a tener en cuenta si se van a obtener árboles complejos en los que será necesario aplicar una postpoda."},{"metadata":{},"cell_type":"markdown","source":"### Adaptative Boosting (AdaBoost)"},{"metadata":{},"cell_type":"markdown","source":"Vamos a empezar a validar los ensembles, para ello vamos a empezar con Boosting, en concreto con el algoritmo AdaBoost, utilizando la clase AdaBoostClassifier. Al igual que con los k vecinos y los árboles de decisión, lo vamos a incluir en el pipeline y ver el nombre de sus hiperparámetros."},{"metadata":{"trusted":true},"cell_type":"code","source":"adaboost_model = AdaBoostClassifier(random_state=random_state)\n\npipe_adaboost = make_pipeline(drop_col, outlier_rm, discretizer, adaboost_model)\npipe_adaboost.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los hiperparámetros más importantes de AdaBoost son:\n\n* n_estimator. Es el número de estimadores, que por defecto es 50. En este caso no lo vamos a validar ya que aumentaría bastante la complejidad computacional y tardaría mucho. Lo vamos a dejar en su valor por defecto. Como se nos indicó en clase, sabemos que en la teoría los ensembles funcionan mejor cuantos más estimadores utilicen, aunque en la práctica sabemos que esto no es siempre cierto, ya que depende de cada problema. Así que en este caso nos vamos a quedar con 50.\n* base_estimator. Representa al estimador a utilizar por el AdaBoost. Utilizaremos un árbol de decisión y validaremos varios de sus hiperparámetros. Probaremos con una profundidad máxima de 1 y 2, con el criterio gini y entropía, y con un ccp_alpha de 0 y 0.1. Como se puede ver todos los árboles que vamos a probar son muy simples debido a que el AdaBoost funciona muy bien con modelos muy simples, reduciendo así su error debido al sesgo combinando estos modelos simples.\n* learning_rate. Parámetro del ratio de aprendizaje que nos permite controlar cuanto contribuye cada estimador. Probaremos con 0.95 y 1.0."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = adaboost_model\n\n# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\nlearning_rate = [0.95, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(pipe_adaboost,\n                                     X, y, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha,\n                                     kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido como hiperparámetros óptimos un learning rate de 1, utilizando 2 intervalos para discretizar en el preprocesamiento, y se ha utilizado como estimador un árbol de profundidad 1, con un criterio gini y un ccp_alpha de 0.\n\nPodemos ver que ha funcionado mucho mejor el discretizado de dos intervalos en lugar de tres. Con lo que seguimos confirmando lo que vimos en el análisis exploratorio de la práctica anterior, que es que funciona mejor un discretizado en dos intervalos para este dataset.\n\nTambién hemos obtenido un learning rate de 1, aunque también funciona bien el learning rate de 0.95, ya que en las 5 primeras combinaciones de hiperparámetros óptimos obtenidos podemos ver que en las dos primeras se ha obtenido un learning rate de 1, pero en las tres siguientes de 0.95.\n\nComo árbol podemos ver que cuanto más simple era el árbol mejores resultados hemos obtenido, ya que los cuatro primeros árboles son de profundidad 1. Esto sigue lo que se indicó anteriormente y es que el algoritmo AdaBoost funciona mejor con estimadores muy simples (en este caso árboles de clasificación de 1 nivel) y al combinarlos permite reducir el error debido al sesgo. También vemos que se ha utilizado un ccp_alpha de 0, ya que teniendo árboles de un solo nivel no los puedes podar. Y como criterio ha dado mejores resultados gini, aunque al igual que vimos en los árboles de decisión, entropía también da buenos resultados, dependiendo más de su combinación con el valor de otros hiperparámetros."},{"metadata":{},"cell_type":"markdown","source":"### Bootstrap Aggregating (Bagging)"},{"metadata":{},"cell_type":"markdown","source":"Después de validar AdaBoost, vamos a validar Bagging."},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging_model = BaggingClassifier(random_state=random_state)\n\npipe_bagging = make_pipeline(drop_col, outlier_rm, discretizer, bagging_model)\npipe_bagging.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bagging tiene más hiperparámetros que AdaBoost. No vamos a probarlos todos ya que tardaría mucho tiempo y hay que tener en cuenta que si sumamos el tiempo de cada modelo que estamos optimizando, tardaría demasiado, así que para reducir este tiempo solo vamos a optimizar:\n\n* base_estimator. Este ensembler a diferencia de AdaBoost se beneficia de utilizar estimadores complejos, por lo que en lugar de optimizar el hiperparámetro base_estimator como hicimos en AdaBoost le vamos a pasar como valor para este hiperparámetro un árbol de decisión (de la clase DecissionTreeClassifier) con todos los hiperparámetros por defecto, para que así entrene los árboles más complejos posibles, dejándolos ramificar hasta el final. El único hiperparámetro que vamos a validar de los árboles es si utiliza como criterio gini o entropìa.\n* n_estimators. Número de estimadores a aprender, que en AdaBoost no optimizamos ya que teníamos que optimizar algunas hiperparámetros del estimador base, pero en este caso como Bagging se aprovecha de estimadores complejos para reducir el error provocado por su varianza, en este caso vamos a probar con 10, 20 y 30 estimadores."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = bagging_model\n\n# Should not modify the base original model\nbase_estimator = clone(decision_tree_model)\n\nbase_estimator = [base_estimator]\ncriterion = [\"gini\", \"entropy\"]\nn_estimators = [10, 20, 30]\n\nbagging_clf = utils.optimize_params(pipe_bagging,\n                                    X, y, cv,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido como hiperparámetros óptimos 20 número de estimadores, hacer el discretizado en dos intervalos y como criterío de los árboles de decisión que usábamos como estimadores hemos obtenido entropía.\n\nPodemos ver que seguimos obteniendo que funcionan mejor nuestros modelos para este dataset discretizando en dos intervalos en el preprocesamiento.\n\nTambién podemos observar que tienden a funcionar mejor los casos en los que hemos aprendido más estimadores. Se ha obtenido que para la mejor configuración de hiperparámetros se han aprendido 20 estimadores, pero si miramos la tabla vemos una tendencia en la que se han obtenido mejores resultados utilizando 20 o 30 estimadores que con 10."},{"metadata":{},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{},"cell_type":"markdown","source":"Vamos a seguir con RandomForests, que al igual que Bagging busca generar árboles complejos para reducir su varianza al agregarlos. A diferencia de Bagging aplica otra técnica para reducir aún más la varianza, y es que aplica aleatorización al aprender los árboles.\n\nVamos a utilizar la clase RandomForestClassifier y a incluirla en el pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = RandomForestClassifier(random_state=random_state)\n\npipe_rd_forest = make_pipeline(drop_col, outlier_rm, discretizer, random_forest_model)\npipe_rd_forest.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los hiperparámetros de RandomForest son los que mismos que los de los árboles, además del número de estimadores. Como nos interesa que entrene árboles complejos, vamos a dejar sus hiperparámetros por defecto.\n\nSolo vamos a utilizar el criterio utilizado, como viene siendo habitual. El número de estimadores (50 o 100), y el número de características a considerar en cada nodo, que es la técnica que se utilizará para reducir la varianza."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = random_forest_model\n\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\nn_estimators = [50, 100]\n\nrandom_forest_clf = utils.optimize_params(pipe_rd_forest,\n                                          X, y, cv,\n                                          randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features,\n                                          kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido como configuración óptima de hiperparámetros gini como criterio para seleccionar las particiones de los árboles, un discretizado de dos intervalos a aplicar en el preprocesamiento, utilizar 100 estimadores, y como técnica para conseguir el número de características a aplicar en cada nodo sqrt.\n\nHemos vuelto a obtener que es mejor discretizar en dos intervalos que en tres, ya que ha obtenido mejores resultados con dos intervalos que con tres. En las dos mejores configuraciones ha obtenido gini aunque también ha conseguido buenos resultados con entropìa.\n\nVemos una tendencia en la que ha obtenido mejores resultados con 100 estimadores que con 50, con lo que puede que obtenga mejores resultados al utilizar más estimadores. Esto se cumplirá aunque llegará un momento que no resulte óptimo utilizar más estimadores pues se obtendría una ventaja demasiado pequeña con referente al coste de utilizar más estimadores.\n\nTambién vemos que ha obtenido como hiperparámetro óptimo sqrt, que significa que ha aplicado una raiz cuadrada para obtener el número de características a utilizar en cada nodo, aunque si miramos la tabla vemos que también se han obtenido buenos resultados si utiliza el logaritmo (log2), viendo que en este caso este hiperparámetro no ha jugado un papel diferenciador."},{"metadata":{},"cell_type":"markdown","source":"### Gradient Tree Boosting (Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"Ahora vamos a ver el algoritmo Gradient Tree Boosting, que es otro ensembler, que es un tipo de algoritmo de Boosting que es capaz de optimizar cualquier tipo de función de pérdida.\n\nUtilizaremos la clase GradientBoostingClassifier y lo agregaremos al pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boosting_model = GradientBoostingClassifier(random_state=random_state)\n\npipe_grad_boosting = make_pipeline(drop_col, outlier_rm, discretizer, gradient_boosting_model)\npipe_grad_boosting.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los hiperparámetros de este modelo son:\n\n* Learning rate. Mide la aportación de cada árbol, probaremos con 0.05 y 0.1.\n* loss. Es la función de perdida a optimizar. La dejaremos por defecto, que vale deviance.\n* n_estimators. Dejaremos el valor por defecto que es 100.\n* Los hiperparámetros propios de los árboles. La diferencia con los anteriores es que el criterio que utiliza para medir la calidad de una partición no es gini y entropía, sino que es el mse, friedman_mse y mae. Nosotros probaremos con mse y friedman_mse. También probaremos con árboles de profundidad máxima de 1 y de 2, ya que al ser un algoritmo de Boosting funciona mejor con árboles simples para reducir su error debido al sesgo al agregarlos."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = gradient_boosting_model\n\nlearning_rate = [0.05, 0.1]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2]\n\ngradient_boosting_clf = utils.optimize_params(pipe_grad_boosting,\n                                              X, y, cv,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hemos obtenido como configuración de hiperparámetros óptima un 0.1 para el learning rate, friedman_mse como criterio, como estimadores se utilizan árboles de profundidad 1.\n\nSeguimos obteniendo mejores resultados al discretizar en el preprocesamiento las variables en dos intervalos que con tres, cumpliendo con las conclusiones que obtuvimos en el análisis exploratorio de la práctica anterior.\n\nHemos obtenido que funciona mejor el learning rate de 0.1 que de 0.05, esto puede ser debido a que 0.05 es un valor demasiado pequeño y así cada árbol aprendido aportaría muy poco al modelo final formado por la agregación de todos los árboles. Como sabemos de teoría tampoco es bueno tener un learning rate muy alto ya que los modelos no podrían converger bien. Pero en este caso 0.1 es un buen valor ya que no es alto, siendo un valor pequeño, pero no demasiado pequeño como 0.05.\n\nPara los árboles entrenados tenemos que ha utilizado como criterio friedman_mse, aunque también ha obtenido buenos resultados con mse. Lo importante de los árboles es que hay una tendencia por la que ha funcionado mejor con árboles con una profundidad máxima de 1 que de 2. Esto era de esperar, ya que este algoritmo funciona mejor con estimadores muy simples con mucho error debido al sesgo, que se encargará de solucionar mediante la agregación de todos los estimadores aprendidos al modelo final, por eso ha funcionado mejor con árboles de profundidad 1, que son más simples."},{"metadata":{},"cell_type":"markdown","source":"### Histogram-Based Gradient Boosting (Histogram Gradient Boosting"},{"metadata":{},"cell_type":"markdown","source":"Para finalizar con la selección de modelos, vamos a estudiar el algoritmo Histogram-Based Gradient Boosting. Este algoritmo es una optimización del anterior que discretiza los datos de entrada para reducir el número de puntos de corte a probar, reduciendo el tiempo que tarda en el entrenamiento y en la inferencia.\n\nUtilizaremos la clase HistGradientBoostingClassifier y lo añadiremos al pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_gradient_boosting_model = HistGradientBoostingClassifier(random_state=random_state)\n\npipe_hist_grad_boosting = make_pipeline(drop_col, outlier_rm, discretizer, hist_gradient_boosting_model)\npipe_hist_grad_boosting.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como hiperparámetros de este algoritmo tenemos:\n\n* learning_rate. Ratio de aprendizaje igual que en el anterior. Probaremos con 0.03 y 0.1.\n* max_iter. Es el número máximo de iteraciones del algoritmo. Lo dejaremos por defecto que vale 100 para no aumentar más el coste.\n* Tiene algunos hiperparámetros propios de los árboles como max_depth o max_leaf_nodes. Estos hiperparámetros sirven para ajustar la complejidad de los árboles. Como en los modelos anteriores hemos optimizado max_depth, en este caso vamos a optimizar max_leaf_nodes, que imaginamos que funcionará mejor cuanto menor valor tenga pues generará árboles más simples."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = hist_gradient_boosting_model\n\nlearning_rate = [0.03, 0.1]\nmax_leaf_nodes = [15, 31, 65]\n\nhist_gradient_boosting_clf = utils.optimize_params(pipe_hist_grad_boosting,\n                                                   X, y, cv,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_leaf_nodes=max_leaf_nodes,\n                                                   kbinsdiscretizer__n_bins=n_bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La configuración óptima de hiperparámetros obtenida es un learning rate de 0.03, con árboles con 15 nodos hoja como máximo y se ha aplicado un discretizado con dos intervalos en el preprocesamiento.\n\nAhora si que podemos concluir que como vimos en el análisis exploratorio de la práctica anterior el discretizado en el preprocesamiento de dos intervalos a funcionado mejor que el de tres, ya que en todos los modelos (excepto en el k vecinos) han funcionado mejor los modelos con un discretizado de dos intervalos. El cambio de tendencia en el k vecinos se puede deber a que como para clasificar se fija en los vecinos más cercanos, si se discretiza en más intervalos tiene más posibles valores para generar los vecinos.\n\nEl learning rate ha funcionado mejor con 0.03 que con 0.1. Aunque si nos fijamos en los modelos en los que se ha discretizado en tres intervalos ha funcionado al contrario, mejor con 0.1 que con 0.03.\n\nPara el número máximo de hojas (max_leaf_nodes) ha funcionado mejor con menos hojas. Podemos ver que los modelos con un valor de 15 han funcionado mejor que los de 31, y estos a su vez han funcionado mejor de los de 65. Esto puede ser porque si recordamos que este algoritmo es un tipo de Boosting, y Boosting funciona mejor con estimadores simples, que tiene error debido al sesgo y lo que hace es agregarlos para reducir este error. Pues cuantas menos hojas tengan los árboles significa que son árboles más pequeños y más simples."},{"metadata":{},"cell_type":"markdown","source":"## Construcción y validación del modelo final"},{"metadata":{},"cell_type":"markdown","source":"Una vez que hemos seleccionado para cada modelo su mejor configuración de hiperparámetros mediante la validación cruzada, vamos a evaluarlos con el conjunto de test y veremos que resultados han obtenido."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_test\ny = y_test\n\nutils.evaluate_estimators(estimators, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver que los modelos que han obtenido mejores resultados son los ensemblers, que han obtenido todos mejores resultados que los k vecinos y los árboles de decisión. Esto era de esperar ya que como se nos dijo en clase o podemos ver en las competiciones (donde se tiende a utilizar ensemblers en vez de otros algoritmos) suelen funcionar mejor los ensembler.\n\nVemos que el algoritmo que peores resultados ha obtenido es el k vecinos, aunque para ser un modelo que no aprende ningún clasificador, sino que 'memoriza' todos los datos de entrenamiento y luego clasifica según los casos que más se le parezcan, ha obtenido muy buenos resultados (un score de 0.918).\n\nDentro de los ensembler vemos que el Gradient Boosting y el Histogram Gradient Boosting son los que mejores resultados han obtenido.\n\nAún así hay que tener en cuenta que los ensembler han obtenido mejores resultados si tenemos en cuenta solo la tasa de aciertos. Si tenemos otros factores como el tiempo de aprendizaje y el tiempo de inferencia, tenemos que modelos más sencillos como el árbol de decisión tardan mucho menos tiempo y han obtenido unos resultados (en tasa de aciertos) muy similares a los ensembler."},{"metadata":{"papermill":{"duration":0.219719,"end_time":"2020-12-14T10:50:55.982497","exception":false,"start_time":"2020-12-14T10:50:55.762778","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Libreta reproducida sobre Titanic"},{"metadata":{"papermill":{"duration":0.23389,"end_time":"2020-12-14T10:50:56.437866","exception":false,"start_time":"2020-12-14T10:50:56.203976","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para finalizar la práctica vamos a replicar una libreta que realiza un análisis exploratorio, preprocesamiento y al final entrena varios modelos del dataset `Titanic` e iremos comentando lo que se hace.\n\nEl enlace de la libreta original: https://www.kaggle.com/hasanburakavci/titanic-eda-and-classification-top-5"},{"metadata":{"papermill":{"duration":0.221459,"end_time":"2020-12-14T10:50:56.899809","exception":false,"start_time":"2020-12-14T10:50:56.67835","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 1. Carga de datos"},{"metadata":{"papermill":{"duration":0.219698,"end_time":"2020-12-14T10:50:57.342317","exception":false,"start_time":"2020-12-14T10:50:57.122619","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Lo primero que se va a hacer en la libreta es cargar los datos. En este caso el dataset `titanic` viene dividido en dos conjuntos, uno de entrenamiento y otro de test, así que se van a cargar estos dos conjuntos en las variables `train_data` y `test_data`."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:50:57.797294Z","iopub.status.busy":"2020-12-14T10:50:57.796484Z","iopub.status.idle":"2020-12-14T10:50:57.820094Z","shell.execute_reply":"2020-12-14T10:50:57.819319Z"},"papermill":{"duration":0.252029,"end_time":"2020-12-14T10:50:57.820225","exception":false,"start_time":"2020-12-14T10:50:57.568196","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/titanic/train.csv\")\ntest_data = pd.read_csv(\"../input/titanic/test.csv\")\n\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.223042,"end_time":"2020-12-14T10:50:58.266501","exception":false,"start_time":"2020-12-14T10:50:58.043459","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Ahora se van a mostrar algunas instancias para comporbar que se han cargado los datos correctamente. Vamos a utilizar la función `head()` que nos muestra las primeras instancias. Nosotros prefeririamos usar la función `sample()` que mostraría instancias al azar para así evitar obtener una muestra sesgada que no represente al conjunto de datos. Pero hemos usado `head()` porque es la que se ha usado en la libreta que estamos reproduciendo."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:50:58.732176Z","iopub.status.busy":"2020-12-14T10:50:58.730916Z","iopub.status.idle":"2020-12-14T10:50:58.73647Z","shell.execute_reply":"2020-12-14T10:50:58.735847Z"},"papermill":{"duration":0.246245,"end_time":"2020-12-14T10:50:58.736607","exception":false,"start_time":"2020-12-14T10:50:58.490362","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:50:59.253655Z","iopub.status.busy":"2020-12-14T10:50:59.252447Z","iopub.status.idle":"2020-12-14T10:50:59.256361Z","shell.execute_reply":"2020-12-14T10:50:59.257109Z"},"papermill":{"duration":0.300377,"end_time":"2020-12-14T10:50:59.257268","exception":false,"start_time":"2020-12-14T10:50:58.956891","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.220879,"end_time":"2020-12-14T10:50:59.700505","exception":false,"start_time":"2020-12-14T10:50:59.479626","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Aquí ha utilizado la función `info()` que nos proporciona información de las variables. Por ejemplo, podemos ver que hay variables como `Cabin`, que tienen valores nulos (debido a que en total nos dicen que hay 891 casos, de las cuales `Cabin` solo tiene 204 con valores no nulos. También nos indica información del tipo de cada variable.\n\nAunque no se han sacado conclusiones en la libreta de esta parte ya que todavía no ha iniciado el análisis exploratorio, y vamos a intentar seguir la misma estructura que en la libreta origial. Se analizará cada variable más adelante, y la detección de variables con valores nulos se realizará más adelante en el análisis exploratorio."},{"metadata":{"papermill":{"duration":0.276873,"end_time":"2020-12-14T10:51:00.197184","exception":false,"start_time":"2020-12-14T10:50:59.920311","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 1.1. Detección de outlier\n\n\n![oie_384549KoGQkTap.png](attachment:oie_384549KoGQkTap.png)\n\n* Q1 = 1.Quartile 25%\n* Q2 = 2.Quartile 50% (median)\n* Q3 = 3.Quartile 75%\n* IQR = Q3 - Q1\n* Outlier data = (Q1 - 1.5 IQR ) U (Q3 + 1.5 IQR)\n\n","attachments":{"oie_384549KoGQkTap.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZAAAACtCAYAAACeLCkLAAAYOWlDQ1BpY2MAAFiFlVkJOFXd19/n3Jlrnud5nud5nkPmmXJd8xTXmGjAq0KTJBkiZCpEZQpFg5RIVJSKpIiKlGT8Dqr3/b//5/m+59vPs8/53bXXXnvttdYe1j0AcCmTIiJCYXoAwsKjKfZmhvyubu78uHcABmyAGVADDIkcFWFga2sFkPL7/Z9l8RmANt+DMpuy/rv9fy0Mvn5RZAAgWwT7+EaRwxB8DQC0KjmCEg0AZhahC8VFRyAYi2gJmCmIgggW3sQB21h9E/tsY6stHkd7IwR7A4AnkkiUAABoN/XijyUHIHJoM5E2xnDfoHCEtRjBuuRAki8AnGMIj3RY2B4EcxERLO7zDzkB/yHT549MEingD96ey1bBGwdFRYSS9v4/zfF/l7DQmN9jCCGVGEgxt9+c86bdQvZYbmJEd+h+uM9OGwQzIngoyHeLfxO/D4wxd/rFv0SOMkJsBlgBgIm+JGNLBHMjWDA8dKfVL7quf5CpBYIR28OOQdEWjtt9YV/KHvtf8uF4vygTh9+YRNkaa5MnPSbEyeCXzPOBfha/ZbYmBDq6bOsJP44Nct6JYFoEj0aFOFj+4vmYEGi08zcPJcZ+U2fE5yjgTzG13+ZBCYdF/Z4XSjMwyGLnL2wVHehovt0XtYtM2tKNHcHBflGuVr/19PUzNtmeFyrZL9zpl/6orIhoQ/tf/GURoba/+FFtfqFmm3RBBPdFxTr87jsXjQTb9nzRICLa1nFbNzRzMGmH7bYOaElgBYyAMeAHMUj1AXtAMAjqm22aRX5tt5gCEqCAAOAHZH5Rfvdw2WoJR54OIAF8QpAfiPrTz3Cr1Q/EIvS1P9Ttpwzw32qN3eoRAt4jOAxYglDkd8xWr/A/ozmDdwgl6L9GJyO6hiJ1s+2/aPx0v2lYE6wx1hxripVAc6J10VpoK+Spj1RFtDpa47def/Nj3mMGMG8xTzFjmJHdQcmUf2nOD6zBGKKj6a/Z+fxzdmhRRKoK2hCtg8hHZKNZ0ZxABq2MjGSA1kPGVkGo/9Q15s+M/7blL1kEeQJMYCPoE8T/rQGtJK3KHymblvqnLbb18vljLaM/Lf+eh9E/7OeLvC3/zYk6grqK6kZ1onpQbagmwI+6iWpG9aLaN/Gf2Hi3FRu/R7Pf0icEkRP0X+ORfo25abUo+Uvy0/Krv9pAtF989OZiMdoTsZcSFBAYzW+A7NZ+/BbhZFlpfkV5RXkANvf+7a1lwX5rT4dY+/+m+SgAoL6GbG29f9PCVgGoTQWA9+7fNNEqZPnsB+BSATmGErtNQ28+MIAK0CErhQPwInuXODIjRaAKtIA+MAE7gA1wBG5gF2LnQCROKSAOJIJDIA1kgJPgDMgDRaAUVIIa0ACaQBvoBPfAQ/AYPAUvkViZBDNgDiyCFQiCcBANxARxQHyQCCQFKULqkC5kAllB9pAb5A0FQOFQDJQIpUAZUBaUB12AqqB6qAXqhHqgAWgEGoemoa/QMoyCiTAzzAOLwnKwOmwAW8KOsBccAEfCCXAqfBzOhUvgy3Aj3Ak/hJ/CY/AM/B0FUNQoVpQASgaljjJC2aDcUf4oCmo/Kh2VgypB1aJaEU8PosZQs6ifaCyaCc2PlkHi1RzthCajI9H70ZnoPHQluhF9Bz2IHkfPodcxNBhujBRGE2OBccUEYOIwaZgcTDnmOuYusnYmMYtYLJYVK4ZVQ9aeGzYYuw+biS3E1mFvYQewE9jvOByOAyeF08HZ4Ei4aFwa7hzuMu4m7gluEreEp8bz4RXxpnh3fDg+GZ+Dr8Z34J/gP+BXCPQEEYImwYbgS9hLOEEoI7QS+gmThBUqBioxKh0qR6pgqkNUuVS1VHepRqkWqKmpBak1qO2og6gPUudSX6G+Tz1O/ZPISJQkGhE9iTHE48QK4i3iCHGBhoZGlEafxp0mmuY4TRXNbZrXNEu0TLSytBa0vrQHaPNpG2mf0H6mI9CJ0BnQ7aJLoMuhu0rXTzdLT6AXpTeiJ9Hvp8+nb6F/Tv+dgYlBgcGGIYwhk6GaoYdhihHHKMpowujLmMpYynibcYIJxSTEZMREZkphKmO6yzTJjGUWY7ZgDmbOYK5h7mOeY2FkUWZxZolnyWdpZxljRbGKslqwhrKeYG1gfca6zMbDZsDmx3aUrZbtCdsPdi52fXY/9nT2Ovan7Msc/BwmHCEcpziaOF5xojklOe044zjPc97lnOVi5tLiInOlczVwveCGuSW57bn3cZdy93J/5+HlMeOJ4DnHc5tnlpeVV583mDebt4N3mo+JT5cviC+b7ybfR34WfgP+UP5c/jv8cwLcAuYCMQIXBPoEVgTFBJ0EkwXrBF8JUQmpC/kLZQt1Cc0J8wlbCycKXxJ+IUIQURcJFDkr0i3yQ1RM1EX0sGiT6JQYu5iFWILYJbFRcRpxPfFI8RLxIQmshLpEiEShxGNJWFJFMlAyX7JfCpZSlQqSKpQakMZIa0iHS5dIP5chyhjIxMpckhmXZZW1kk2WbZL9LCcs5y53Sq5bbl1eRT5Uvkz+pQKjwg6FZIVWha+KkopkxXzFISUaJVOlA0rNSvPKUsp+yueVh1WYVKxVDqt0qaypqqlSVGtVp9WE1bzVCtSeqzOr26pnqt/XwGgYahzQaNP4qamqGa3ZoPlFS0YrRKtaa0pbTNtPu0x7QkdQh6RzQWdMl1/XW7dYd0xPQI+kV6L3Vl9I31e/XP+DgYRBsMFlg8+G8oYUw+uGP4w0jZKMbhmjjM2M0437TBhNnEzyTF6bCpoGmF4ynTNTMdtndsscY25pfsr8uQWPBdmiymJuh9qOpB13LImWDpZ5lm+tJK0oVq3WsPUO69PWoztFdobvbLIBNhY2p21e2YrZRtresMPa2drl2723V7BPtO92YHLY7VDtsOho6HjC8aWTuFOMU5cznbOnc5XzDxdjlyyXMVc51yTXh26cbkFuze44d2f3cvfvHiYeZzwmPVU80zyfeYl5xXv17OLcFbqrfTfdbtLuq94Ybxfvau9Vkg2phPTdx8KnwGeObEQ+S57x1ffN9p320/HL8vvgr+Of5T8VoBNwOmA6UC8wJ3A2yCgoL2g+2Dy4KPhHiE1IRchGqEtoXRg+zDusJZwxPCT8zh7ePfF7BiKkItIixiI1I89EzlEsKeVRUJRXVHM0M3LJ7o0Rj/krZjxWNzY/dinOOe5qPEN8eHzvXsm9R/d+SDBNuLgPvY+8rytRIPFQ4niSQdKF/dB+n/1dB4QOpB6YPGh2sPIQ1aGQQ4+S5ZOzkr+luKS0pvKkHkyd+Mvsr0tptGmUtOeHtQ4XHUEfCTrSd1Tp6Lmj6+m+6Q8y5DNyMlYzyZkPjikcyz22cdz/eN8J1RPnT2JPhp98dkrvVGUWQ1ZC1sRp69ON2fzZ6dnfzuw+05OjnFN0lupszNmxXKvc5nPC506eW80LzHuab5hfV8BdcLTgR6Fv4ZPz+udri3iKMoqWi4OKhy+YXWgsES3JKcWWxpa+L3Mu676ofrGqnLM8o3ytIrxirNK+8k6VWlVVNXf1iUvwpZhL05c9Lz+uMa5prpWpvVDHWpdxBVyJufKx3rv+WYNlQ9dV9au110SuFVxnup7eCDXubZxrCmwaa3ZrHmjZ0dLVqtV6/YbsjYo2gbb8dpb2Ex1UHakdGzcTbn6/FXFrtjOgc6Jrd9fL2663h+7Y3em7a3n3/j3Te7e7Dbpv3te539aj2dPyQP1B00PVh429Kr3XH6k8ut6n2tfYr9bf/FjjceuA9kDHE70nnYPGg/eGLIYePt35dOCZ07Ph557Px4Z9h6dGQkfmX8S+WHl5cBQzmv6K/lXOa+7XJW8k3tSNqY61jxuP9751ePtygjwx8y7q3epk6nua9zkf+D5UTSlOtU2bTj/+6PFxciZiZmU27RPDp4LP4p+vfdH/0jvnOjc5T5nf+Jq5wLFQ8U35W9d32++vF8MWV36kL3EsVf5U/9m97LL8YSVuFbeauyax1rpuuT66EbaxEUGikLauAiikwv7+AHytAIDGDQCmxwBQeWznZr8KCrl8wMjbGZKFZuBUlCJqCl2MISFn3RyuGZ9JCKDSo2Ym0tMw0tLTMdNzMwgwSjFpMluz+LDGs51mr+Xo5Zzi+s69xovnY+cXEZAX1BWyEfYWiRI9IlYo3iDxSAojTZLpkOOX36cwpCSpHKfSpYZVN9KI1izV6tX+okulx6DPYsBuyGHEacxmwmxKZ4Y32zD/ZjGz443loNU966adlTZ5tsfsUuwTHKIdI52inONc9rsecct2L/Go8az1qt5VvrvU+wKpyKeYfMG3zK/Kvy6gJfB2UH/w65D5MGw45x6pCM1Ic4pT1K5ov5jQ2Mi42Pi9e5MSDu4rTGxNGtz/8cD6IdpkrhTRVNm/VNI0D+seMTxqlm6XQcksONZzfOmkxCnPrGOnO7Jnc1jPauV6novPO5lfWtBY+OD8q6L5C+gS1lLxMo2LFuWuFQGVMVXJ1ccv5V++WFNfe7Ou/8pY/cJV7DWu6/KNJk1uzWEtB1uzblxoO9Ye3eF2U+cWXyfofNt1+3b5nYy7lHte3Tvua/bw9sw8KHvo0ovtrXvk+GipL69fr3/y8ekBk4HVJzcGk4dsnvI9/fys/Xn6sNMI38jUi5qXkaNKo99eNbyOeKP4Zm2sb7zobcSE/juGd28n698f+mA7xTf1afrGx6MzjrMcsyOfzn52/yL05efch/n5BYlvJxalluiW7VZnNza2/C8EXYGt4DlUDtoU/R1TgfXBCeOm8LWEaCoDahEiA3GZZob2Jd1j+rsMHYxNTM3MbSztrJ1sPewDHN2c7VyN3PU8l3gr+cr5zwucFjwpVCrcLfJZjFVcXyJUMk/qvvSirKicg/x+hWrFAaVFFXZVdTUbdZJGsCZZy03bWsdAV1lPRJ/ZADb4YvjS6J7xFZM801SzUHN7C9UdXJbA8oNVn/X1nUU2R21j7cj29g6GjopOws6sLniXFddPbm/cBzzueDYh0VC0O9s7nXTQJ5Yc7hvuF+4fFhAaGBIUHBwY4hdKCtsV7r7HJcIh0oZiHeUW7Y9cmZPi0uKP781OOLevILEwqWh/8YHig8WHSpMbU/pSp9IwhwWReCCnp2SUZnYee338x0naU0JZaqetsn3OxOecOFuW23quP28i/0ch4TxnkVSx5gWzEufSwLL9F7PLayruV76rBpd4LmvVuNRS6jKvXKxva3h89dW16evfm6BmmhaOVpEb8m0a7YYdO27a3nLqdO/yuO14x+Ku9j2Zbq77uPtzPc8etDzM70185Nmn1c/Zv/T42cC1J6cHo4dcn+o/k3nON8w+wvaC96XkqPYrh9c+b9zHzMaV3wpM0E6sv/s8Of7++Yf+qYfTPR97Zh7NDn76+AWaY5uX/mq84P0t5Xv94syS9s/iFfbV6nWrLf9LgTuQJTSMRMA9lAXqKdob/Q2ThhXANuEccAv4CoIXFRPVY+rjRHsafpo52gd0l+mzGQ4xRjOFMHux6LCysy4gkVDMEctpySXA9YP7EU8ZbxKfC7+SAL3AB8EWoSPCjiI8ImOiJWJkcRHx9xJVknuk1KXR0gMyebIkOXG5Wfl6hVhFTcV1pU7lVBUzVbzqA7UMdWsNgsY9zRQtfa1V7WadaF153Y96F/W9DTgNhgyPG1kYrRtfNQk25TMdNDtsrmX+yaJoh50lZNlg5WPNYH13Z5KNps0P22t2FOT+8MWh1jHcScbpk3ONS5irtOuUW4m7mwezx4DnSS+HXRy73u6u9d5HsvTh9vlM7vTN8vP0F/KfCbgamBhkEkwfPBpSHZoQZh0uGP5zz5OIisgIihJlMaolOinGKJYYOxxXEZ+41yVBbR9XIirxc9LI/tsHag6ePXQoOSTFOdXgL9k0nsM0h9ePzB99n/48oyPzwrGU434nzE/KnGI6tZb18fRo9pMzj3Ienu3PHTo3nDea/6bgXeHU+S9FixdACXUpR5nERe1yu4rAyoNVudVXLnVfHqv5Wcd0RabeoiHgatq18uv3G2ebGVr0WtNuDLbzdvjfrLm10KV1+687A/eEuxPvv3xg+LD+kWRf0WP0AOlJ55Di09bnlBHyy4uvPcerJgdnSAtCm/7f/o9us2BVATjDi2So9AA4nAHgGHJAiH0BgI0KAFsaABw1ALxDB8DMCQAy4f1zfkBI4kkFGAE3km2qIpm0CwgGB0E2qAK3wDCYh4iQOJIbkqFkqBS6A32AqWF52AVOhuvgVygiSgcVgSpDjaDpkRztELoNvYRRxURhLmHeYFmwlthk7A0kx1LAheNqcV+QXCoef4tAJLgTqgnrSJZUR81EvZd6nGhFbKGRpClEMp1MJLc5gmQzxxlYGIoYZRjbmKyZ3jBTWLAseayyrF1sLmyz7MkcHBz1nPacy1xl3Lbcazw1vF58dHxd/LECUgJjgjlCO4Wxwu0i0aIyotNiVeKhEooSK5I9UnnS4TImsnyyq3Iv5dsVMhW9lBSVqZWnVHpU69Ty1I9qJGpStMK0g3SCdIP1/PWtDZQNOY2A0XvkhtxoesHsuPl+i6gdoZYhVnusE3am21ywbbEbtP/iSOMk42znEuda7NbrvuIp4+WzK2/3UxKTjwM51/eVv2BAQGBN0EKIZmhKWO8e9gj/yKYoqmhSzI04nvjWBK9EfFLLgYhDUsnTqRVp5CPcRwczDh9TP6FwKvz0wxyL3In8o+cliutLBS7urWisenuZudb6yvGGketyTVmtUFviTXRnzh34ntf9tof8jw73f3viP/T6ucfIq1G/11/Hz77b9UFmemo26nPH3IOvZ78pfK9c3FiS/emwbLditSq7hl4bXj+5Yba1f0AAA4iABQgAOaAH7IAfSARZiPe7wChYglggZcgRioVyoTboLUyAFWAPOAO+AX9C8aOcUZmou2gIrY/eh76GnsWIYkiYQswLLDvWBZuLfYHjwfngqnDzeC38EfwzghghgTBAJUV1lGqO2oO6h6hBvEwjSFNAy06bS8dJV0wvTn+VwYBhiDGYCWYqZNZgHmaJZ+VkbWfzYcezX+Fw58RwXuUic7NwP+BJ5tXi/cF3nT9GQFVgWfCm0GFhWxEWkVHRcjGKuIEEo8R7yXaps9JRMvaySnJscmvyEwodigVKScokFQtVZTUhdRYNak2MFqQNdGBdnB6NPkZ/2WDOcNLopfETkx7TLrN281aL1h3tlnetBqzHd363JdoJ2Ws5OCG3mJPOV1yGXJfdhTxsPA94Neya8hYk7fYpID/3Y/C3Cjgc2BW0hng7Pqw5fDlCL/IIZShaOCYutj9eeW/BPmwiJWn8gP3Be8k6Kc1/qaW1HNE7+iDDNfPj8cMnVU99PF1+JuisyjmqvKmC/vPtxQ0ll8sulddUNlZ3Xn5U++zKSMPTa/cbrzQfa/Vuk2qfu1nbGXjb7K5Hd2RP+sOLj272jwzMD2GesQwLvpAeVX6tMabxlucdZnL+w9h030zrp6Iv8fMmC/C3qkXdH/d+6i9XrVKvkdevbvkfBnjADISQtW8F/EEKKAI3wRiERlb9TigayoduQ7MwK2wAR8FV8DiKB+WBKkSNo8XQEegbGDzGBVOJWce6YVtwvLijuJ/4UPxbZH0PUllT9VBbUw8RvYlfaJJpOWiv0TnQLdIXMpgzrDDWM4UySzF/ZmlkTWFzYJfggDnecHZzNXCX8pzjPcN3FrmHXEa8Oiw8I/JTjFqcX0JN0kEqUvq0TIvshDyjgqXiKaUhFVZVJ7VT6g810Vo62nE6V3Xn9Y0M8g1/GDubNJlxmO+zGLXUtsqz/mnjYvvc3t9h2emUi6hrs7uJxwMvs10d3nKkAjKVb5Lft4CwwI/BQSEfw0LDP0dERn6Liotejk2K59h7e19kkuD+pwcPJ6ulTPyVfljqSHf67oyvx5JPsJ+sz7I6PX3myFmx3O48/wJMYUmRXvGLkugy2osVFQaVr6uTLvPW3KrzrSc0XLnmfH2tqazFsvVr27kO/ZsfOk/c5rmTfY+u+1gP3YMzvbyPKvsVH3c8MR988tT52fCw80j/S+PRa68F3vw1NvvWfWJ4cvf7qanw6akZ+9m6T3Nf+Oe0542+qi7wL3z61vqdssi52PWD9OP9UujS/M/ony+W9ZfLV2hWolburdKtuqwWr06tKaztXetam1/nWbdYj1kvXu9dX9oQ3rDd2LdRsTG46f8ofyXFreMDIhoCgHm9sbEgCgAuC4C1UxsbKyUbG2ulSLIxCsCt0O3vPltnDXLGFCxsogcCmer//v7yP9ih3DHBgR6zAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAAGYktHRAD/AP8A/6C9p5MAAAAHdElNRQfkDAMHLgJ2As9eAAAcg3pUWHRSYXcgcHJvZmlsZSB0eXBlIGljYwAAaIGtm2mO5Lpyhf9rFV4C52E5nALw/jfg71BZ1V19+9p4gLOgzkylRHGIOAPJfv57ree/eJVe0uN4+RlT8dVVV5YLXqdc2eXUVEMOqaYQXG655xGcqyfxMx+4wXl9nhz5Kb7EGqtLPrvs0nKf15/f/7eX8VTVyH+d2DHs75r9h6/nP7vc+5JKrrHE92v+nC/hKUmny35/mOm+p1hosKuhvt/z+dQwxFrpOfc5X74a49NDd95u/BScvn4o9ffzsX2f/3H9Vx++BSVG5q1qKe/55haD4Gso9X4/X30WXKWupX5qdNr61IymHVrdy3nLHZ8mx8PljcLuDX6cTxPCYoB2sc/56b4e0J7ybzWq/17T8peaPveH+s8ffozOr1dL1F9xVyiy9l8//IfD/++v//+C6MJV859NCV/fV2k1pJbD+9W/o0bclVkszRy+Cnr7xq/GKIZ0kn1ueG/0WyObfj3g6/wpPCDzgPz8fII1pXCKn4Bz/h2uQI3Ah5TS/Jx/HxxC4MGLB6/n5w3ETyk9lfQZj08KECKMVUo9f0f+e54EC8UIE/sq6K1qqE2DnnKO/zhPnP/WtE9BbdCvM+fkvwr6VLUf4jmn/p1Dn5rOUUdphGj90adhK2V6Lul8dfb5/LAIwpnjd198Cjq3oBTT+UdBoeYcvjv7k/VR2cAwl+9hfguKnNr1t9D4NDlGiqdP+3eNvn5IqyY1IbUfT45ZSex/67v391iG+jS3X6P2q1OpDsP8gb7PqMUWSi4n/zkIsQeBXArfAfn1hJGUjr9A/+uGuShopfrV5PA5vw4JXOjT74D8PNncpaD0Ryrwr54Mr3zF0dt3CcQcNHlk9zNFUiCiVRvhUEqXvlzN73vhn0LqFK4pfZ2zZhWW67fbR183pfKiuQJP+FTyHd8fBf/5EMXVew8FCWb1RR1ZedrZn3MUpCF/n/hbYeG3B+oaVUDDb/svF//+5P1i7dfN98bPQ8oUrH7VSNXnhGD2XpB/PU01+7N5mRvF//ea/vX78+/tF8jdzl3v+9eNf7/u+f1CTts/LiyMeuFzIdqJVgavM+yDSw7H5LKlz89bmXsycDSO/mef/fUBZI3L2/OO5tkEZKYb8u7ZSm9B1f9fO16/7Rcs1Je/Qub5dGp4O/YvI5hC+dQ238qluD8teGueiHKa+txeB2jeZiXPu71t143xXzv3rdGvkX7e0fgVob8C9I+add46JztKrFNmD+33a56/9sMh507Mf6nNV2z9Izw+NfqRZ+EvOfaXmPpZ4+cvo/IvgSiC1PebBf8o9GdBsTVALAJYlcNzdIEaR70PidM4uGbp3P4+/08Y+c6h+pcc+y3/yldu9r8V9GfH+p9N/YaNGN2g2odxtPXHqPGzhFWr30IBYWvAa4eSO0KMX4jFjwYXWyXkYonhhBVgq+ie+FEu3xbht5fFi93fGtiXZX+7roT4UnZs9ursttZfL9x2MX2c9orQs/Kf192CQhlXffhWLyWP2RfyJkPH5UeJvxSSeE/Sx16m8f1R+a8yuyG/xc+iUI31e6fIUrpZ4kt0lLu7kHEju70Ie7O/MtSVqjf6olFAm0oqDgroFNApYHDT4KbBTZMHKX6IL0esocE4JgWpezY37fnizOHzmbfWGCiOSXfrr3JsujxydA6DiDMHv0dq5IEPD74gETgWjidw9EumKHAOfgMyPLX2jUKose9cQ2394AH0r5+Mmp98oIqedPBAp9+8H94P78YNNojLKB2Ju0wcG+ovHEbXNhSR56BpMD0HP/L0UDkJSQaGObSDguNCxjgMCpnczAPDmoxJRnxJgPHZsnTVwz/TRdoRMbM4SMQTSUlaREZJ6hbr5iLpEQmN2Pje+U7hEacT53gTfJMidA8H2WzQjBP+xsvK6UKw6AYIzsAuw52ocaLAhL1K9FFiJBM1TQs8SpsL6BOFSbJDqG2XEaeZ0chxEDrdEccQbAWROQiRTK3y4JgoHeTUh474wDDfPy8CJC9lf0SIjF5BxpQ6ib0FamxUjb0IwsCUEwGGLPH0uEpMVJ5eoZlKtSrwIYeICiT5+Y2OrgBa3dhHHlqt4dsm8WuukQUNHGsZOsJ9uNY4QXXbGAQ237mpneoawdi9GCPDIgP2MCdP0nFqvfF58JkO7wvw7zylc8Pw3g2aNajZICQGTRqkzSAEBiM06IuxlxumceYPqTDhvZm9m4WkBYLd5OI5lT680/GTEVwI0QUtLUJh5eUwYW4RW2tUt5RmBO6i0K2/UB63k1JtuE2/bC7chPxeXLD5TH8cwuEQfYfRw0i7Uw2UnaRjQdwFd84hLamR0X4jzA1oMKLa6CubgmNzdpTe9FIo4nKGifZXqtwn1eb8rrrRk8gPB+/JwNHtfVveD3prde9Po3HVk1c+pOyxVJ7U8WFEHxbHidSVA5wlEx6PkwYAOAYxvTJAkElNxDOJh0OTkvapTcT0BhwOzwQcfPCodJ9z8VAGYDEfn+f2tErZJkPvMX2+lOVLM19moKKcs+Er0FCT9wh9XxtSYhxPbPlq1TdPQSS6x6P51oZvw3zbfObGCzk0qZfpOy3tk9DdW6LPjzD8oGtGrcDR8YTG4wcdSsX9jNNPmjkpcFKTuYcXAhECfmXe6/SL/llreViJ+h08M/WsgrD+eIbabwo7vngYG+M8wLTkDwF1TvfGaFg8HmnojfM2zdsZ4UIpfU/iIZraQ9QBGoAdQwUANhrN4we9uU7gLow0cheTGBofh4WwN5wzArgVwKoQewpxhSeg9EBLCyntkOoKacyQ9lB2hczFuXDA3HlxGAxOwbB70IxCmRwHDeupUU24t0qWUUDdC3bboUULjRBqg4rshLJGXVMoHR963wE0IXmoIJyOGQ2D4QeODRAJYcYCLPcwuZCICIK2hUVcBaFAgWtXMHOFTb/sWsLmwfsQlwSNJhACNQPDayBnAg0Ey0ew7oPRFDOQK2KbCsk3yN49yQNCOTF8dJGfvFuI1A01UmIMfYL7CeGzYwScY7WIMouRIUtB053EcT8x7RaJ/ZjB59xwxSvEbJvkqE/EnmOWTywIJ5w6FhXC6CtW/HnzqO8sRbhiWw30yXeQOg/qk4YbcEcxo6Yn0lSYxSItjrMQlmPEeRojWiOSKi6atE6MO4RItkTiNIIwEQsSsSPxwEggycP4+GhcYISGnQAzxnTt0LjgQ9cShYWMHosoPdQHpqLqYWY6tNIfPKnuh8ddPYvXx8C3gWs6Ag2oDLMNKOd9UgkhlYI6HDMhJxO9k2qdhB0UB2E1KDuRyKkBj53M6YU+HT2hZBiXmkbdiYqgrFe6MyYUPKk1+ZcY1LTmwCsRv2k8aXcGY690EAGnnEQovFyZ4Ax+sy2iB7CrzwA7UA9UQIXAQSYxM0GQQ10PUVczsJYZR4h1ZDosa5IiNWOUJ1nWcq40dJEt4GqBWpGKGdGb6YxcySRY/MmNEGx157Zm7qBMLx3Ag/LwToPbxuCwnCe9NjvHoczIAYitXfKmtru2J+/VM9CEs1z5zJ2BfBjcZwigXEudiSxKgw3AX34ZwCgVC/RoQLQik4owuxBdJcIt1/9qeu10oodHdZL8aI53F1KwkOUFWi+1E8NcQ2IX2Lm0s0pXQb2P0k2R0mi6oRFqmTx5Dt7NqGctaxjatQHfaIeJ5vea7ZvlLKKBHjeaVgxMR45B9OiWw9gQ8L7nikZggMnmQUVQkJGeIP+I0PROjS9GCbBBuVf65lG7agHZi7VaEUF1RggGoiin4gQYlFF7QzYcUCuVysAiclIF3+skj8DVuhoFLbEKdoUcqtKvBwg8XAA6oGBStYPKShUl0SQpGhzV/NqAiAPpAGBLQEJ9GlkKf+5GrLd0SHZuymOijtS7gY7BNKlN3bCFqbXSAYPTOqDdO3W100jLp401GlTf6MM2bTVcCC0abQNdm3u2WTt3cE6zCG3206/jA3fchloQn360p0MfgPbocLmSpYMlQA6aqpF05EnO1vPqNw0Z3o6C6nRmpx96S6u3ScVCeqid690WQdD62AQ1WD+HdWiwL65ZVvsmwmChDu/1Mw7Rsmno7GZIvIK83e1BIVSotQDvHL1gXzgqB0yZyCrSnARsIw+ehVShAwYxQ5SdQa1Gg0ga8ngAsFzSETNrDEgfRhmTNJunQ+lrLJTgTmgRuPzAHQfaV/xSI83DomIzqms8Exky/YawEoy/QPpI349DC8nTTib7OHMb6DPqXfsE2Ai6NqkNqU16b9wMeIRZDJDKnGNmSjp8UJz5ufqU4JkbaUsLgHEwzWB+mNQMUYmUdAdFUSrCIT6gjMF6UAJAEjdmANkP3K6cJi1rmlpf0NoiNxAkxEfcq8214A+e1REobdEXzwIt1pxlUdKiIQvGWuAZxFEWoIBwrctoAG5mu4lsi4vGIEOjbdzSBu03dPPQj516AETp7LzprZx32aA45dEPuxVo8RCbBUHMuDNce9ik7xG/uJDVBvLLPZsroPC1kTzbkAc2cDbQpZucTtidhb7J0NieVDIesOtAIyfZIguBebdP6eU54MUhAKBEZMBcaCxifzOggOo4gCbgMulvrXSge8/WNC2ukHE9BvPZQkfn9qC+s3n0gjdDy9AlvhuQZAkCgXdAa7B3HytlW9GgITQb1zSCDqlDClQDWR4bSPlZqzGeJEqjB7ttcuSkaWcvMJK0NcN3ab3N/zad8DV1DwyXu5iJ+B/v6sLO74xEfGc4MDfzx8Lcz1f8zJqEz5QGRIFcQoXOa4oROaiCdicf0OOvEdSsD8MEEGGz5KAIb2S9DAaaDL9CQYY0ll9HfmIXJuqHPE7EdEGsA7qwBvr7+E4QD2zFRBovgP7I+qG2sOjIEPcgVg76A62GG88V3YULBDhCQ9D2teVskX9eo4+ioz2AupcoQ0GiJK7Yos5PbJzrqBNkUSTy0UKoalUeq4OWxjS0FPAA0aNUMBE4FZ5pEIkAHcA2QvqgRsa2NAn0RVZtBMQBwq2P7K6XQLYKjrgfkZ+1BkLPY7aJxUbu4zEGSprMeUCckzccc+B4Q8fhSQthUtC/aDo8DhwEqGG2DbMNsK9YgM0yCGhMAMQJCfsBQYocRYoJBX6fee58Wy6rlupr5V8ohL9dR6OH+q7oorp3rAeQRPlCU+fBMyBf6eJEhqBtGtWAfk7jROtQEy4IFCPmQXAERzvWukO2exgBNQ9ZkFVnPR031uGt3kQVGDEsLeAfwcbS9yIJSWszTfCkQUCMiB9IkU5E2xWgkwrz5Axm4wBGO2Ny+wLi9yRcJhgACqEFJtYI2kNXYL6Q7sgWwl0QKzmBKYfIQFX/4ND3REKin/o00M95uxNeXlOMREMcZWFFESWEE/CDsgQlQUd/FsMOY4BmJC0WHnTk+SDbwp4vkYdF6sNguh4xssCdKTIJkoY/QFVmz3EOlLEh9LG1Krqxcxs0IgI2YSQfSLUMfSrPUrH+mAszJDowRFgcUg8v4Q6RinINJ08EEmrkoFcZP8bw7ANxEI9UGNxdvh/AFSBjlDPegFwyQzgDOyQZ5XeFNQE3rTkDj6gnpCHM+cxVCg2i1uBR0Lhac3cWS4mRDF0M5GAcpcJcBSVIaEeKPbgn7yZ+kfx3y6Bx4OTsqQUJTmCl8bbELDSJxZnr2nvcPOqqI1dQIAunPffDoBa/poceaCw6ygaP6/jZjg0j9LA/QTKfzsVyljuPB6PgGkdgyAPZEk4oD6CMTauH5OYOWAuxGpWtyCRy1WLD8EhbDGwcSjwuaAyFB50kDFEGIbQIEh4cjyXABnNTZDqukSFhUlX4ZNR0JG/91Ew/wSYNemQIUNGWyBasBZaiRCwEf1GWIcqUI2Q8GeuQqoZ2gDNhOsKdroHMFzF9kaIj6rvmxFA6TWhcEOx3e8XR8ilkgCCH6hsamuaUcT9lbVsoRGAhDIrdqb8IWuDdE5IZ8YYHe95ZayACPVxbOxWwgi87cLEYmF1J2YpLhLwcyhagRjVgx7GY6BNAAo8B/oTnhQt6djSEkaTFYlQ38WnaO3Hw23h2XC4YSmMCUp7L4UVEN/pxSBt2LPFDpgL1yEhQrgNyONRx5SCmCrwEMqhPRkkBiQNlDlAkhCOCiYBZaOudKxwbHvh3TTEh0orscjOSZQzJxL4BnV6iY3ZcnqAZPJG3nhtHS3kwE2ILx+XzkNCSyEJgofPw1PgmoMJr/o/nm6Ob8DR0GHiFoVigBAEA34B15BW6CRFEdDxQfQR9saUZ2kicDkADiNCk16+zIGHCABUWphSnzgUExNHsr4/kf0D4gdkHjw4ckOAHOEBaoc8OrT0d2BoXujYoEMEGEMBpfp7sJzECwBRFcBg3OuaxAq4ylnS9s+mbAXkokd/QAK7FZJMJ02FZXkRYBYPfXdz5LjkhgJ47rQzNg8qYMEC5GsYEqOgA5IDU78KDZkRzcBvhsSn/LJ46QOQuIkYzACM+lnnRlObDjLgopNaFiOE8LhWiZkARGIQJKhCRCe6cCvpmTbtHfnH0UkVETOIN4QAph+w3cKGVm0C4QzwkItYowFEYSaKMoF4uBcoLGxg5iAuTZYPXiAxPFbhLwGMnxsVYoCyyNm854AQlXTEDDfiA8qJWg8iCOJPW7RhL+HVrtkZTTVBvNM09a8qfxiBbSJyDjESF0PVpxutv8OmpIDi0foZjTKBDwlol4PrRWMiFJCIReeXTRvPTB4ztSEjH7CZKRjvsNHVxNFtxctJGi7BzBmaKkqRszUQscAeJQg6QVFldLxOyutYEkBRYblAWpYKkxhLzWAaZx55T6AacXEDjr0cziegSjA9eolVNBvAQooFYxvtDs3WUnXY52tljHaumafhatfoXYkRYF0mHBw+PW7gzYsiYjWXnIgqs4EWlmyuDX5fDkBwaLJJbuG/IFQ4iixCRuG7k1wM6e6AZR03YQqwNf94aKlP7dQaEDGdif0g88AocRelJ1pCGcIJIOhy840bWJNIW/wLDzE6e9aaB2PK3uKiNsD+4Htz5oV3wKq12IzjNG8ISRJhm4YD/B30DWAGiWPiBDx2TTlmoUcrAckatONH/fQb1IhAOxGHCmiZGoEzcY7Q5WnmkMRgkLE9BEmq9wiKCF4xCeREPiopVwSHUz6IKeDusVOBZXSugSXkl34ZfgxYlS2ul4SViQ7Aw+D7NeOetCSEJhviFBr9v6fu1bBpfL9Kzvz/k9pqLHPvd7dP26zZCPbcQRnx8lkSvWcl3YwHAMuJWQUFYr+vSW+APs+LeBd/0Lqdq94+2YWkFVRNeAqO76UBbGbRjSFsKtFardV1CBIMiKcJ97V2bLFp7DA5OoxxNw1ceM99VWQoCAB4XhjEweF9uRKwRencfEuin/SHuXaijzUgakLDRbk27RXQSLgWUK177SubjCk8qc77rZNGjCY6DqZz65S57VcTU0tItFisP12E1FOC7UDVkt0CiqkUozBZG3xGPQClCS4ZW60Xcbdxumn/Ssk873nuEF1Y7oH8QRwAXOmhj1CBIVcyT+ponR4lp7YB8REECmr7rDxE2tJ5giBugatFl8msbiXIkeOlQ0+IBbQkUD+46FBpYkDSFTBhOYLemAI2R9u8UU+NJV8gZTLc0rT/uVD6k+FAPA6+w01uErHmshVLaEVxHBQONGHnAKeKOkBKojAbcVnQ8IDKLJiHI2dIfTROh9koi2hF0QKwfd0MDP7xT0pgGrUNVoLdVMrjGJDMwSbtVEOTIktPGk2g6cEpw73ihFNa+MKo5YjwWtsVl/GweKLjFeTgdxTYy2r8AJXxDbvb2YBs3KBm0AVUAVDpEPGVAIJCNpgCaJKzRFRhYoFH7ptElVatGTZsJM/7NtaciWqohGunwFk5vaaDdEF7Nr9ZxH5NA4q1pisJrhYE4Aagxbuibhq4CkwimBzJjdOlFcJ18ZzB8GtKtxA16CZ0UjHiTAkPr4B3NJdwFneRet6S5pBncswBXFAM4WYTxBwCfxOTeugoDAonQw/AIKHMwP2dhL0EvlNOw0DWHAnuPhhoBaGiT9RyvL2EsCIYDammWvtkhuE3G/EudXDQQzKx3X7W2mAft9onaNOE/u1SUVOGDBJIefCdRSQ1HaGgaAFDC9GAdaSgoELQjyd19I0g/YICDxA1nv5s0cEjoWxe1EZEa3B0zmjOG8DFgLody501Q8yAAvt43sh88KTyhcIIhVcaQ/R2dRJ6C/Q1maLs4bX1EE7u+tVQNS8BKJL+bVGQush+VSr8tMh8h1ReZ31DXWDkvMUV1AtmvRSzkhdeaWJH8CrAaLhVqS6hNtPcDCZD58Ihyl2e/mzDyoJiMD4t3gRP9i8gaUKiUJ3YM0UWgU3XN1HCjDQktTKY2Fme6C8FBaPBQfAq+QAvFSH/tkg2YO7Idq11FOyGgpMKdU7MNeMWHRx7sKZa4bv23AQQWmY4Xk0Kg0lqI14aGqG0mjCFgjLkFS+0upzOMBkiZexI+OcFmdy0zy26swABtrD5mg4OaaI4UL093HJ+Opu/kUQNJjhWMWq2q/skZHVTA8SrJTkgOLpi44FUto9SJ6abdJlpyuQsq6DyAwQM8S7MvpdETkO7zZnrVBEVD/6CLoxan6bSt/wSRNFmEhbPaUHxYIW2zQPSESu14VNC29hYgSEKlNXwc8qA3RHzrWgPIWh/ACDWsVizdS7oMbTLGyOH0iCCsFZeiXs4kACfuiHxXR1wbhcYYpP9ATw9NWBBYU8ogSX0Qdw0lMQrxp/kNmMi0EK6pqVSflXFHoNRC1SwExNpoXvQT9ED4kQB0DuETNjCPsaG/Yz8uys/vk7TrxODd456jqdc5qQCsR09AUhkTEdC8HePOKUCya5D5vqjELgjmGX9DA2mE9rx7rETiVxSsd8dqi+9OqGl3c5NX6l4doLUcPneiakXt+iJZihbDyX6QOJCkQJiLJC2Df+fAo/bbkKipyvATH6iNTIIDYi5D2Xe7DOxcIURs7kOWdwfxgazh5fat1WacEUJ78k48vtuwAEU6i6Tu7gAnSHcQDyzpkvkkLdkH7otYQGs+YAI9XMZgYFGylqyhbRrQt8xnvbsTtBNhd6ICFsNvay7lAfrXm7g8LoH32YQ8M2gWVovSyFZN1gTIFlmKGiZb3V0Yn+SoRwlhgHx5IvofKvYAopIzRWx2JB5Iv8iYeW0mTV4LQaY10DtVR2p1WHeim2HciJbpWqdlcDFx2cEfMMmlgNy99hjkptVRrYYuaJBskfYlMopXzmmNuu6SMUwyNaUdzYH0MnnHmRd8Mi7Dg1wYNv0PGht3yqPBS1p0p5kVNK0YoyYVFkgvvNrTIHcylOhfmgW1RndhF8J1ETBoR2b3eHon/ejW2rmuj1IAZaB/oHCipsEGuVYTivGAH9iZ5EfDlw/s/iJT73YjBKJ25YD3ODdUP/oHATI7wQVFgvt9kivPNJy+XDoDuvRfDrSyWzUry/vApVAgKazlCU14HOAPKbDl+spdiGm8a0fUxcZF5CkLrWGNjxZUxiEcsIl2CjQNUl26Bu9x6nbIfNgciRkX4xqJnfLwINzH0ez4kAPXGrMBt4ZTt3qKEfVGhxF3yegUEFoJXAyMQV3gaEjqpXnIlatpPkBrlQg/2xXglBLUzhcqiUaj9VRtaF/TAiKaWcDuZjKkTjNNae/4fMFB1f+i0lLGr5e/u2Wu6UCX6BXDu2Oz9nveND+r1xzhFmR7vKsumjy+MqP8+I9Kv1zQa1K6Nnl9XmsEydf2f/4vn+d/AHvlrro2XzU1AABObklEQVR42u3dd3xUVdrA8d+dPumFVNJIIQESSiA0QcDQFay4Ava2rh3La0NEXSuKuCyriA11UVkVRBFWBAEFNNIJhJDeG+lTMvW+f2RzJYICQRmQ8/182DUzc2fO3Nyc5572HEmWZRlBEARBOEkqTxdAEARBODuJACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIgiB0iQgggiAIQpeIACIIxyHLMm63++ef3bKniyQIZwQRQISTZrfb2b17N42Njcd9rdPppLGxEZfL5elinxCXy0VjYyNOp1N5bPfu3bz77ru4XC5aWlpYtGgRZWVlJ/3eBQUFbNiwgQ0bNrB582Zyc3M7BSZBONuIACKctJaWFh599FF279593NfW1NQwd+7cEwo2Z4Lm5maee+45ampqlMfa2tqU8lssFlZ/tZr6+vqTfu+33nqLRx55hGXLlrFkyRJuueUW/v3vfyPLokUjnJ00ni6AcPaRZRmbzabcPVdXV6PRaLBYLBQWFhIeHk5SUhJqtZri4mJ27drF7t27CQ8PJyYmBj8/PywWCwcPHqSxsZG4uDh69OiBSqXCYrFQV1dHQEAAubm5aLVa+vXrh9VqZf/+/bhcLpKTkzGZTAQFBeHl5UVZWRn+/v4EBAQAUH+4HovVQlRUFLIsU1lZSXFxMQ6Hg/j4eGJiYpAkCbvdTmlpKUFBQVRUVGC1WlGpVOzbt489e/bQ2NhIVFQUKSkphIaGolK132+p1epO56O2tpbc3FwkSaJXr14EBwcf87zZ7XYmTZrEY489hsvlYtGiRbz++uuMGzeO8PBw7HY7RUVFVFZWotPpSElJUd6rpaWFuro6goKCOHToEG63mz59+uDn56e8f1lZGQUFBYSFhREUFITVaiUmJgaVSoXL5SI/P5/y8nKCg4Pp1asXer3e05eScJYTAUQ4Ze+//z55eXkYDAZqa2upqKjgvvvu4+KLL2bjxo0UFhaycOFCAgICeOCBB4jqHsXcJ+dSVFSEj48PZrOZmTNmcuVfrqSgoICnn36a+Ph48vPzSU1NJTIykmeffZa8vDz8/PyIioqitLSU++67j/T0dJ5//nkmTJjAJZdcAsCXq79ky5YtvPbaa5QUl/DSyy/R2NiI2+3GarVyxx13MGHCBOrq6rjjjjuIjIzEZrMRFhZG9+7dyc3N5bXXXsPPz4+7776b5uZmVq9ezcsvv4yE1Om7b968mVdffRWVSoUkSRiNRmbPnk1SUtJR50mSJDQaDVqtFq1WS0pKCs3NzZjNZgC+//573nnnHVwuFzabDS8vL56Y8wSJSYn88MMPPPHEEwwaNIiWlhaqq6vp3bs3Tz/9ND4+PmzevJkXXngBg8FAQEAAra2t+Pr68s9//hOdTsfixYtZu3YtPj4+mEwm+vfvz0MPPYS3t7enLx/hLCYCiHDKmpqaqKys5LXXXiMkJIR3332XVatWceGFF3L55ZezZcsW5s2bR/fu3TEajbz55pu0trby9ttvExAQwHfffceLL77I8POGI8sy2dnZTJw4kdmzZ6PT6Vi1ahXl5eX861//IiwsjJUrV/LFF18o4xQtLS20tbUp5enocnK73YSFhzF37ly8vLxwu92sXLmSzz77jNGjRyPLMlVVVaSlpfHoo49iNBqpq6vjp59+4vHHH6dHjx4YjUa+/PLL9kr+iJ4mSZKwWq0sXryYCRMmcN111+F2u5k/fz7vvfcec+fOPaqlAlBeXs727dtpaWnho48+YtKkSURFRQEwYMAABgwYgEajwW638+KLL7L6q9Xcc8892Gw2SkpKmDVrFlOnTqWkpIS7776bvLw8evXqxVtvvcUFF1zAbbfdhtVq5cEHH6Surg6VSsW2bdv4+OOPefXVV0lNTaWiooK7776bzZs3M2nSJE9fPsJZTAQQ4ZTp9DrGjBlDdHQ0AIMHDyYrKwun04ler0ej0eDt7Y23tzcWi4VvvvkGtVrNv//9b9xuN21tbRw+fJjKykoMBgPh4eGMGTMGHx8foH3wediwYcTFxQFwwQUXsGTJEqULTZIkJOnnlsGRP2s0GrZs2cKuXbswm80cPnyYuro6TCYTAHq9nokTJxIUFKT8rFar8fLy+s27c0mSKCsrIycnh5iYGF577TWgfcynsLAQk8mEv7//Ucfs3r2b9957j9LSUiorK/m///s/pSvJ7XLz6WefUlxcjM1m4+DBg9hsNqC92zAoKIgRI0ZgMBiIjIwkMDCQuro6vLy8KC4u5tFHH1XO85AhQ/jqq6+QJInc3Fzsdjvr16/n22+/RZIkGhsb2bRpkwggwikRg+jCKVNJKry8vJSfJUlSxgt+yeFw0NTUhMFgULpyfH19uf3224mJicHtdhMUFITRaFSOsdvtne7mtVrtb/bfu1wuZFlGpVKxbt063nzzTTIyMpg+fToTJ05Ep9Mps8J+q6xHkf7373/HOR3tAdLb2xutVotGrSE5OZlbbrkFg8Fw1OGyLDNx4kTmzZvHkiVLyMjI4PXXX1eCxL9e+xfff/89EydOZMaMGQwbNgy3260MsktInQbc1Wo1sizjdDpxuVydvseRAdXpdGIwGNDpdO3l1GiYPn06F1100Wm9ToQ/H9ECEX4XvzWTyGazYbfbAfD392fgwIGYzWZuv/125TUOhwO1Wk1VVdVRx4eHh7Nr1y5sNht6vZ4DBw5QXFyMJElotVq8vb1pbm5WXp+fn4/D4UCSJPbs2cOgQYOYMmUKANnZ2bS0tCgV7LHKbbPZlEr917jdbrp3705YWBiDBw9m/Pjxnb6LVqs95nFqtRq9Xk9ISAh33nknf/3rX9mxYweDBw+moKCAK664gvPPPx9Zllm6dCl2u/3nstK5rB3rU2JjYwkLC2Pbtm0kJSVht9vZu3ev8pqQkBAkSeIvf/kLoaGhyvFHTlUWhK4QAUTokiPvcH8teHR0uwQGBvLiiy8yatQozj//fK699loeeeQR5s6dS2pqKlarlT179nDPPfd0et8OY8aMYfny5Tz33HMkJyezefNmjEaj0lU1dOhQli5disFgwGKxsG/fPgICApBlmb59+/L8888r4wxr1qxBr9f/fFf/i8/z9fXFz8+P+fPnM3r0aEaMGIEk/e/OX+78XYOCgrj00kt54YUXKCoqIjg4mKqqKkJCQrjqqquOeT6OlJyczPnnn8/SpUtJT0+nR48eLF68GKvFSklpCTk5OaSkpPxqWTve08fHhyuvvJJ//etf1NbWYjKZ2L9/P4GBgbjdbsaOHcvmzZt57LHHyMzMRKVSsX//fiZNmsTQoUM9fSkJZzH13Llz53q6EMLZRZIkgoODSUtLw9/fH71eT3x8POHh4UD7XXZwcDAJCQl4e3uTkZGBWq2mra2NuLg4EhISGDx4MLW1tZSWluJ2u8nIyKB3r94YDAbCwsJITk5W7uKDgoIYOHAgpaWlmEwmxo0bx6FDhxg8eDBxcXEkJyfj6+tLUWERUdFRXHLJJaSmphIfH098fDyhoaEUFRXh5eXFjBkz6N+/PwkJCej1ekJDQ+nXr58yXtExbViv12O1WpW7+8jISBISEpTv1rt3b7y8vEhNTSUuLo7i4mJqamoICAhgyJAhdOvW7ajzptfrSUpKUsaKJEmiZ8+eaDQaEhISSE9PB6C0rJT4+Hj+8pe/kJKSQkxMDHq9noSEBNLS0tDpdEiShL+/P7169SIgIIDevXrTp08f2qxtpKalYjQasVgsTJ06FS8vL4YOHYosyxQWFtLS0kJycjLp6eliFpZwSiRZrGISzjKtra1ce+213HPPPYwePdrTxTkjFBcXU1lZSVRUFGVlZbz44otcffXVTJs2zdNFE/7ERBeWcNaRJImEhAR8fX09XZQzRltbG8uXL6eiogKtVstFF10kBsmFP5xogQhnHVmWaW1tVWYWCe2sVismkwmdTnfUFGJB+COIACKcMxoaGigvLyc1NVUJQj4+Pmg0PzfETSYTVVVVSJJEREREpzECh8NBa2ursv7EYDAoa1WgvQLPycmhd+/ex5zGKwh/NqILSzgnyG6ZN998E71eT9++fSkpKeHvf/87Dz30EImJiciyzKpVq1ixYgU2mw1ZljEYDFx11VWMnzAeldQ+c+nhhx8mLCwMnU6H3W4nMzOT6dOnKwP+//73v5k6dSqjRo3y9FcWhD+cWEgonBPKysvIyspi4sSJAMpK744UKN988w1PPfUUo0ePZv78+cyfP58RI0bw5JNPsmP7DqA9ZUphYSE33ngjc+fOZdKkSSxevFhZc2E0GhkzZowShAThz04EEOGcsP2n7fj4+CjpUODnlCd2u50lS5ZwwQUXcN111xEREUFkZCQ33HADAwcOZNmyZcrrNRoNkZGRdO/encsuu4zY2FiKi4uV90xPT6eiooLCwkJPf2VB+MOJACL86blcLtZvWE9SUtJRKVAkSaL+cD3FxcWMGTOm02I9tVrNqFGj2Lp1K9XV1UiShNvtpqWlBbPZTFZWFpWVlXTv3l05Jjg4GC8vrxPaK0UQznZiDET407Pb7VRWVjJ8+PBjPu9wOnA4HJ3yeXUICQnBZDJhNpuRJAmTycRzzz2Hj48PWVlZXHrppcoCQEAZC8nOzvb01xaEP5wIIMI549cmHBr0BoxGIw0NDUc9V1paSnBwMAEBAVRVVeHl5cUtt9xCbGwsy5Yto6SkBLPZ3Gk6sSzLYpdB4ZwgurCEPz2dTqdsQvVLsiwT3C2YpKQk1qxZoyR9hPaB9vXr1zNq1CiCg4Nxu91oNBri4+NJSUnh/vvvp7W1lZUrVyrHdCRxTE1N9fTXFoQ/nAggwp+eWq0mMzOTQ4cOdZod1ZH2Xa1Wc9ddd1FaWsrf//53tm/fTlZWFk899RS1tbVMnz690zEdqeD9/f259NJL+eCDDygrKwOgvr6etrY2Bg4c6OmvLQh/OBFAhHNCeno6ZrOZkpISALy8vBgyZIiyUHDQoEG8+OKLNDY2smDBAh555BE2bNjAs88+S+/evQEIDAxk5MiRnRYXTp06lf79+3PgwAEAdu7cSffu3YmNjfX0VxaEP5xYiS6cE9xuNy+99BIGg4G7774bt9ut7Ntx5EZMTqcTu91Obm4us2fPJjMzkzvvvBOdTofb7cZut6PT6Tod09Ht5XK5eOKJJ5g8ebJI8iicE0QAETzK4XCg0WiOudfF762hoYHCwkL69+/fKX3JrykuLqa0tJSMjIxOOyT+mo59OPr37/+bOyZ2Vcfug7+2WZUgnG4igAgeY7FY+PDDD7n44ouPuX+G0FltbS0bNmxg2rRpnbb4FQRP8dgYiCzLyiAmtDf/O5LUCecGl8tFXl6ekk5E+G02m43i4mIxRfgcdqbVmye8DsRms/H9999TXl4OQExMDOedd94JpdN2u90cPnwYHx8fZbFWSUkJn3zyCX/9618xGAy8/vrrZGZmKgOWwrlBpVKdlu6rP4sjx16EM5/T6SQrK4u8vDwAIiIiGDFixDEXrf6SLMvU19ej1+uVvW+qqqpYunQp119/PWGhYbzxxhuMGDGCtLQ0j3y/E7oa6+vrmT17Nq+99hplZWUUFxfz0ksvMXfuXEwm03GPt1qtzJ49m6ysLOWxxsZGNm7ciM1mQ6VSERAQIPZ2EAThT8NsNvPCCy/w1FNPUVRURHl5OYsWLeLhhx/m8OHDxz3e7Xbzyiuv8PnnnyuPmUwmNm/ejMlkQlJJBAUF/SHjbSfqhFog77zzDllZWbz11lskJiYCcODAAW699VZ69erFNddcQ3NzMzabjdDQUKC9aVVTU0NISAi1tbUcPHiQAwcOEB4eTnh4OCqVSunH7ZinHxAQoHxmbW0tBw4cwGQykZKSQnx8PCqVCpPJRHNzM2q1mn379hETE0N8fDy5ubkUFBRgNBpJTEwkLi5O3K0JguAxX331FV9++SX/+Mc/yMjIAKCoqIi/3fY33njjDR555BHMZjNNTU1ERkaiUqlwuVxUV1cTEhJCfX09Bw8eBODgwYMEBwcjSZJSb0qSxKjzR+Hr9/POnA0NDezfv5/GxkYSExPp2bMnGo0Gs9lMTU0NXl5eHDhwgODgYFJTUykoKCA3NxeNRkNSUpJSz56o476ypaWFdevWMW3aNCV4APTu3ZvJkyezfPlyrFYrGzdu5O233+503EsvvURNTQ35+fnU1tayfft2vvjiCwoLCzt1W9jtdl588UX27dsHwP79+7nvvvv47LPP+Omnn3jggQfYsGED0D7P/rbbbuOFF15g48aNlJWV8cEHHzBnzhz27dvH1q1b+eijj0S/+llCkiQR6E+QWq0W3X1nCafDyddff83UqVOV4AHQo0cPrpp+FevXr6e1tZUff/yRp59+GqvVCkBraysvvPAC5eXl5OXlUVZWxoEDB/jiiy84ePBgp9+/2+3mHwv/wZ49e4D2YYFZs2bxwQcfsGfPHh599FGWL18OwJ49e/jLX/7CY489xjfffENhYSGrV6/mwQcfZMeOHWzfvp2PP/4Yi8VyUt/zuC2Quro6mpub6du371HPDRgwgA0bNmC327HZbJhMJmRZ7pS11OVyMXToUOLj47n00kuZMmUKwFHZSk0mEw6HA4D33nuPtLQ0Zs2ahU6n44svvmDp0qUMGTIEp9NJbm4u9913H6NHj8ZqtXLLLbdwxRVXMGPGDKC96Sh2hDs7dNyghISEiMHh3yBJErW1te1dFyKInPFMZhPFxcUMHjz4qOcGDhzI8uXLsdls2O12WltaledkWaalpQWn08nw4cPp168faWlp3H333QDKWErHa00mk7IO6YMPPiAkJIQnn3wSb29vtm7dyiuvvEJmZiZOp5OamhqmTJnCxRdfjNPp5N5772XIkCE8+uijQPusyJOtN3/XZIod+ysc+fORj6sk1XGPbWpqYvfu3QQGBjJnzhxkWcZsNlNWVkZzczOyLJOUlES/fv2QJAmj0Ui/fv14/733ycvLY9T5oxg4aKC4qz1LdDTJ1Wq1CCC/oeM8ieDx5yJJEkjHeIyf68Tj/c4lScLlclFUVERTUxNPP/00brebtrY2ysvLaW1tD1C+vr6kpqYiSRJarZaMjAzef/99LBYL5513HkOGDDmhwf0jHTeAhISE4O/vz969ezn//PM7Pbd7926MRqMy+H1kBSDL8tHTy07g2nc6ndhsNpKTk0lNTVXe48ILL6Rbt27k5OTg5+enDBxJksTtt99Oeno6OTk5LHlzCf9e9m+ef/55goODf5dfsvDH8fX1JfOCTMIjwj1dlDNeZWUlVVVVItCeBby9vYmNjT3mYPmOHTuw2WwYDIajpuX+Mlic6O/a7XbjdrlJTExk0KBBSr62cePGERERQXl5+VH18YwZM0hISGD//v0sW7aMZcuW8fzzz3fa3+Z4jhtA/Pz8GDduHJ999hkXXXSRsqPbgQMHWLt2LTfddBNGoxG1Wk1JSQlWqxWj0cj27dvJz89XBst1Ot1xZ2zJskxwcDBpaWkEBQUxbdo05bmOtBO/TJXtdrvRarSMHTuWsWPHMn78eG644QaKiopEADkLyLKMy+3ydDHOCkdWNMKZTavVMm7cOObPn8/48eOV5JolJSV8+OGHXHDBBfj6+uLn54fJZMJqteLj40NRURFVVVXK2KBer8fpdB7zM44MNlqtlojICCRJ4vLLL1ee60i9A0ff4LtcLkaOHMnIkSO5+OKLue222ygqKvp9AwjADTfcQGVlJQ888AD9+/dX5jafN/w8Lr30UgD69u3L0qVLeeqppwgPD6ekpASj0YgsyxiNRs4//3zee+89srKyuOKKK/Dx8em0mvbIZtv06dOZN28e5eXlJCcnU1ZWRrdu3bj99ts7zd6C9vUpCxYswGq1Eh0dzZ49e4iPjycmJsaDl48gCOe6yZMnc+jQIf7vwf9j5Pkj0ev1/Pjjj/Ts2ZNbbrkFgD59+hAYGMjTTz9NUlISBQUFWK1WZSx58ODByr4zkyZNIiEhQakrf3kzMX36dJ544glmzZpFnz59qK6uxu12c88996BSqTAajcqxDruDRYsWcfjwYRISEsjLyztqy+cToZ47d+7c473Iy8uLUaNGERYahkajISgoiMmTJ3P11VcrfWbBwcH069sPWZYJDAzkoosuYtiwYcTExKDT6UhNTaV7ZHeCg4NJSEggMjKSxMREevTogVajJSw8jMTERLy9vYmOjlZmLmg0GsLDwxk6dChhYWF4eXnRs2dPYmJilGBiMBjQaDRotVoSEhK4/vrriYiI8PT1IxyH3W7n+++/Z/Dgwfj5+Xm6OGe8lpYWsrOzGTZsmBjjOwvo9XqGDx9OfEI8Go0Gf39/xowZw4033qgsWTAYDPRN64ukkvD19WXy5MmMHDmSHj16YDAYlLouKCiIhPgEoqKi6NGjB4mJiRgMBrp160ZiYiI+Pj6EhoYyfPhwJbdcSEgIw4cPJzY2Fm8vb9L6ppGcnIxer0elVuHt7Y1Wo0Wj1RAdHc0111xz0jfeIheW4DGtra0888wz3HnnnURFRXm6OGe8srIyli1bxv33339CySAF4Y8mbmMEj1Gr1SQkJIgp1yeoY2dFMRNLOFOIFojgOTLY7DZ0Op2oFE9Axx4mnkxdIQhH8lgLpLm5mZycnF+dYSCcA6T2fmIRPE5Mx6wc4dzlcDjIzc3FbDZ7uiiABwPI559/zt133019fb2nz4EgCMJZoaSkhDvvvJO9e/d6uijA77wS/WTU19fT1NQk9gA5h3Wk+e9IxXCmkAH/gDq0Gjv19ZEgn1ktJBnw9fHplHxUODfYbDaqqqrOmFx/HgsgkiSh0+nEbJJzmN3h4If1G2jZl43mDOrGkmWJfhd9jV+3Zra8dwmyfAZdozLYNSqSxo1l6MiRJ5LcQfiTOZH0JqeLR/8ytFqtCCDnONliJsXcRmJwN86YtqgsYVTbUKmdjDd4I7vPoGvU7WZ7fR2u/yUeFc49IoAAyO2LBMXezuc2GQlvg4EAP3/aO2fOAG4JWadF1ugI9PWDM6kF4naja2nydCkEDxMBBLGdqdBBbv93ps0ol8+8colZ98KZxGOzsGTkM6opJgiCcLY4U+pNj65EFwFEaCeuAUE4UWdSvenxACIIgiCcnTzXhfW/dMWCIAjCyTlT6k6PBhAxICgoxKVw0s6MKkTwBOkM+e17LICIFeiCcGpEzD03HWsfdU8RAUQ4M5whfxCCcDY457uwRBZeQRCEs5vHAojdbhdjIML/iOtAEM5GHgkgsiwrGVjPlKaYIAiCcHI8FkBsNtsptUBkWcbpdIpWzFlO3D4I5zKXy4XL5fJ0MbrMoy2QUxlILyws5NFHHqWoqMgTX0H4nYjwL5zLli5dyvvvv3/Crz/Tbpg92gI5lWSKDQ0NfLn6SxobGz3xFQRBEE7Z9u3b2blz50kfd06vA+logQQGBnY5nbskSajVajGGIgjCWUutVqNSdaEaPkOqPY8GkICAgK6dPEEQBMHjTmvtLcsyVqsVt9tNW1sbPj4+SJKEzWbz9HkQBEE4YzkcDpxOZ6ceF5fL5fH1dKc1gFRUVHDXXXfx/fffY7PZMBqNbNy4kSeeeAKTyeTREyF4xhnSEheEM9qaNWt48sknMZlMqFQqzGYzTz75JJ9//rlHy3VaA4ifnx/V1dV88MEHWK1W2traWLx4MY2Njej1eo+eCMEzzqw5JYJwZvL19eU///kP27ZtQ61Ss3PnLj755BN8fX09Wq7TuqWtn58f1157Lffffz9WqxWLxYLdbmfx4sVotVqPnghBEIQz1dChQxk0cBCvv/46jY2NfPTRh/Tv35/zzjvPo+U67SPYY8eOpXfv3tTX1/PTTz8xdOhQBg4c6NGTIAiCcCYzGo1cd/111NfXU1tbS3l5OTfddBPe3t4eLddpDyBBQUFMnz4dLy8v4uLiuOGGG7o8lVc4+0kd/yv6sk6aGD86t4wcOZJx48YBMHr0aI+3PuA0d2F1mDp1KqtXryY1NVW0Ps5xIm4IwokxGAzcfvvtlJSUcMstt2AwGDxdJM8EkKCgIF599VV8vL1F60NoJyGiyUmSEa2Qc82QIUP44IMPiI6O9nRRAA8EEFmWaWlpQavX4ZLduF0uVCcQRMxmM2arGZWkIsA/QKxAFwThT8vtctPc2ozd6UCv0eHv54+kktBqtfTo0QOLxUJDUwMqSYWfrx86nc4j5TytAcRht7N55xZ2VO/FYXCjskGfwJ6MHTQGLy+vYx4jyzIH8nJYf3AzrRoLuCFKG06gw8cjJ0wQBOGP1GZtY/32jexrOIhLL6Nuk0gPS2VU+kj0Oh37Du3n29zvadVawSUTpQljQnom4aHhp72spzWA7Diwi28P/4Ax1ge1RovbLZNVtRfVDhWTz5twzLQmpeWlrMxegytKhc5oQJah2FTN1g2FtLW1IYlMKGctlQraG5Ki7+pkSLSfN9EG//OR3TLf79nKNtMuvGJ9UatVuFxuNlf8hG6vjviIHnx+YC2u7mr0Rj2yDPlNlbRlfcWMUdNO+7qQ0xZA2qxt/FS6G313LzSa9o9VqyS8InzJLs5lcP1AQkNCf3E2YU9hNm3Bbny9vJFlGUkCrwBv3N012NxuKoolgnzAJbZYP6tIgM0hUVslEWcTVeHJMJslzGUShYc8XRLhVEhAcyMEdfv5sVZTK7uqs9tvsv/Xta9RqzFE+LCrLJuKmkpcISoMXkb4X33oE+RDWWsNBWWF9O/d77R+h9MWQOxOOyaXBbW684JBtVqNVW3BYrUedYzL5aLFbkLrrz0qD77R14jTrWLfdgnLYXCLm9izjtMpUZCjItUoAsgJk6GpXiJ/lwqXSCF3VpOA6nKJ4JCfH2uztdGmsqNRd+7S12jVmN0WKpraUIdr4Ij6UELCrZVpsjSf9u9w2gKIXqvHX+NDvdOCWvPzCXA5XXi7Dfh4Hb0gRq1WE2jww2EuweBl6BRELM0WdBo3wy6QGdBftEDORna7jGR341sqov8JkyAk3E3g+W6GjPR0YYRT9c2P8pGxAC+jF16yAavThUr7v+pZAofNib/ah4iwEPZbitEZdEp9KCOjtkl08w0+7eU/fQHEoGdYQgYr8/8LkaDRaXG7XFgrTFwQPpzg4GN8eQkGJPUnZ1s+Jr0ZvU/7GIilwYyxRo1eUuMfKOMfdNrPm/A7sNllfHxl1BpZDIOcBK0OvALkTl0fwtnJYOz8s4+3D0Oi0/lv5SaI9Eat0eCyO7FXmBkaN4zY8BiKtpRj1pgxdNSHda0kqqNJiI4/7eU/rYPofXum4XQ6+bFkJ61uM16SnvMizmN4v6G/Oi03MjyCKwZN5dv931FdV4cGNb0DkvHK6Mumj77hDNvhUTgJsnxES1ysAzkpsizWgfwpSTAkNQNk2FG+B7Pchp/KSEbMYNJ7D0Cj0XB5+hQ2HdhCTd1hNKhI9+vNmGEjMXoZT/3zT9JpDSBqjZqMvoPok9Abm92GVqPBx8f3uH8FcdGxzAgNx2KxoJJU+Pv7s2PnjjNuf2BBEIRTpdVpGZE+nP7JfXHY7eh0erx9fu7iT4iNJzo8CrPFjEqlwsfHx2MLsj2yEt3L2wsvb6+TOkav13dK+S6Cx5+JuI8WhE4k8PH59bVuOr0Ond4ziwePJFZRCGcAcTMgCGcjEUAEz5MRMUQQzkIigAieJyF6sQThLCQCiCAIgtAlIoAIHiUd8b+CIJxdRAARPEo+6j8EQThbiAAiCIIgdIkIIMKZQfRiCcJZRwQQ4Qwg+q8E4WzkkZXovwdZlnE6nbjdIg2vIAhnJ5fL5ekinJKzNoCEh4dz0003ERoaeupvJgiCcJrJssz48eN/NZHs2eCsDSDR0dE88MADni6GcIrENF7hXCVJEpdccomni3FKxBiI4FHyEf8rCMLZRQQQQRAEoUtEABHOAKILSxDORiKACGcG0YslCGed3y2AyLLMgQMHqKuro7i4mLKyMgDq6+vZsGEDRUVFnV7f2NjIt99+y8GDBwGorq5m8eLFmEym435Wbm4ub7/9Ng6Hw6Mnr7a2lpycHJqamsjOzsbpdHq0PGc10QgR/mB2u50dO3bQ0NBAfn4+lZWVAJSUlLB+/XoqKio6vf7w4cNs2LCBgoKCU/rc1tZWFi1aRF5enke/v8PhYM+ePTQ3N7N//36amppO+T1/twDidrt577332L17N6tWreLr/34NwM6dO7n22mt57rlnsVgsyutXrVrFNddcw5IlS5Q1HY2NjSe0rsNut9PU1OTxXQm3b9/OkiVLyMvLY9GiRVitbR4tz9lLND+EP15TUxOPP/44OTk5rFixki1btgCwcuVKrrrqKp5//vlON4FvvfUW1113HStWrDilz5VlmZaWFo/f8FosFl555RXy8/N56623yM/PP+X3/N2m8UqSpCzsczgcOJztJ8tut5OWlkZ9fQM5OTkMHDiQtrY2fvzxRwYMGIDdbkeWZYKCgpg6dSpGoxG3201JSQkBAQGUlJRQUVFBamoqsbGxAERFRTF58mQ0Gg1Op5OSkhICAwPJycnBYrEwaNAg/P392bt3L1VVVfTr14/IyEgAKisrUalUhIeHKye1oqKC2NhYVCoVRUVFhIaGkpubS2NjIwMHDqRbt25kZ2dTUlLSqRxOp5O2tjZcLhd2ux1REZ4CceqEP5gsy5jNZlwuF06nQwkWVquV0NBQqqurKSsro0ePHtTX17N//34GDBigLPZzuVwUFxdz6NAhZFkmNTWVmJgYABoaGqivrychIQGVSoXL5SI/P5+goCACAwO55JJL6N69OwBFRUXodDpaW1opLCqkV69e9OjRg7KyMvbu3UtkZCT9+vVDpVJhMpmorq4mLi4OjUaD2+2mqKiI4OBgAgICKCsrQ6vV0trayqFDh0hMTCQ5OZnKykr27NlDeHi48l6yLNPW1tZeR9sdv8si7N91HUhAQABGo5HAwECMRqPyS+vevTuxsbF89dVXDBw4kP379+NyuUhPT6e+vh5JkqioqOCll17i5Zdfxmg08tJLL2E0GtHpdFRXV/Puu+/y9NNPk5KSwoEDB/jggw9YsGABFouFhx9+mOjoaLRaLQcPHiQmJoZBgwaRk5NDaWkp7733Hi+//DKRkZG8/fbb+Pr6cs899wCQl5fHM888wyuvvILBYOChhx4iMTERSZIoKCjgww8/ZMyYMeTk5FBdXc3SpUt55plnSEpKwtvbm5CQEAwGA926dUOlEkNKXSKCh3AaqNVqoqKiMBqNBAQEKHuOu91uoqKiiI6OZvPmzfTo0YM9e/ZgNBoJCgpSWg7l5eUsXrwYi8WKzdbGW2+9xV133cXo0aNpamrikUce4dZbb2X8+PF88803vPfeezz99NNoNBqeeeYZ7rzzToYMGcLzzz9PdXU1ffr0oaamhubmZq688koOHDhAdXU1ubm5zJo1i6lTp3Lo0CFeffVVFixYQGBgIA6Hg5dffpnLLruMsWPHsnjxYgoLC4mNjaWuro7q6mr+8pe/UFxcTHV1NYWFhdx4441MmzYNtUpNaGgoOp2O4G7B6PX6Uz6nv1sAUUkqbr/9doxGI/379++0urJjxeWCBQuor69n/fr19O/fH6fTyeHDhwGULqyObqna2loiIiJ46qmn0Gg03HHHHSxbtoynnnoKh8OhdGG53W4OHz7M2LFjufnmm6moqOC6664jKCiIxx9/HJfLxV//+lfWrFnDTTfdRFNTU6eyOZ1OmpqacLvduN1uysvLGThwIA8++CBNTU3MnDmTn376ieeeew6tVsutt97Kp59+ysMPP8ywYcMYMGAA3t7ePPjgg0rQFE5Sx46EIpAIf6DAwEDmz5+Pv78/SUlJaLVa5TmNRsO4ceNYu3Yt06ZNY+3atWRmZrJv3z6lvggNDeWxxx7DaDRit9t57bXX+Oijjxg5ciTx8fFcfPHFLFiwAFmWefnll7npppuIj4/n8OHDNDQ0YLPZAKipqUGv1/Pggw/i7e3NHXfcwZtvvsnChQtJTk7mnXfeYd26dUyZMuWoehGgublZea+Ghgaam5u599576datG48//jivv/46ixcvJjU1laXvLmX16tVMnToVbx9vHn/8cfz8/LjrrruUAHoqfr8WiARBQUEAR1WksizTq1cvAgIC+Oabb9izZw8PPvgg33/3fee3OKJi9/X1ZdKkSXh5eQEQERFBVVVVp9d2vN7X15dhw4ahVqsJDw8nJSWFzMxMvL29AQgODlb6+4487lg/h4WFceGFF6LT6QgKCiI+Pp7MzEx8fX2V92poaADAy8tLKd/vEc3PRWIlunC6qNVqwsLCADAYDJ2ec7vd9OvXj5UrV7LmqzWUlpZy2223sW/fvk6v+/bbb8nKysJms1FcXIxOp8Nut2M0Grnqqqs4ePAgd955J1dddRWXXXaZctyR9YwsyyQmJhIYGAi0p2XS6/UkJycDkJyczM6dO3E5XcqxR/rlz4MGDVK+V1pqGq2trfTq1QuApJ5JfLXmK6xWK3q9npCQEOD3q69OS5+LLMt4e3szbtw4XnnlFby9venduzdu+df74FQqFRpN5/h2VM4YufNndPz/kcfKsqz863Bk35/b7e6U0EytVqNWq5VjNRqNEiQ6ynA2564504hGh3AmkGWZkJAQ+vbty0MPP6R0ux9ZV6xevZolS5aQmZnJ7bffzqRJk3C5XMprOuqG1tZWgoODO7Vwfotare7U/e12u9vrLH6u035Z1l8er/y3Ro2Xl1enOsrlcv1hE45OW6e9LMsMHTqUIUOGcNlll6HT6U7LLCqp4+72f/8XFhZGRUWF8tmHDh2iubn5hIOCp2d+/RkprRDpDPoHni/DMf5JkiTaa3+g0aNHM27cOK666qpOFTPAjz/+SFJSEpmZmSQkJByVDfyzzz7jp59+Yv78+axZs4b169ef0Gf+Vp1iMBgwm800NzcD7V375eXlJzXe+kfe8J6WZIodLYDIyEhefvll5cv/smVw5M8dUfjI5478Zf1WhD7WsbK7/eeJEyfywAMPMGfOHAICAjhw4AAGg0F5/S+j9fHKIZy6JrOZqvrDyu/T49wSvm021HYrzYfrkWUNZ0JbSQJkt4zFbsfX04X5E+mod9xuN3369GHhwoVKD8aRf/8jRozgn//8J88++yxqtZrvvvsOo9GIJEns27ePd955h/vvv5/x48ejUql49dVXSUhIwM/P75h13S/LcOR/d4zJxsfHExsby9y5c+nXrx+NjY20tLQcVfZfHnvkz39kyvg/PID06dMHvV6vRPMju6VGjRpFv379kCSJsLAwbrrpJry9vdFoNEyfPp2ePXsqr508eTJmsxmAxMRErrv2OrQaLV7eXtx6663KNF2NRsMVV1xBjx492g+UYNq0aeh0OgB69+7NY489xqZNm/D39+e+++6jsLCQwIBA1Go1N998s/JearWaadOmER8fr5TjsssuE11YvxMJ0AUGUhgZSrGnC6OUSEaWJfppAvBTwRaj9n8BBDwfRNrL5/KLJNhobC+OuBRP2fjx40lNTVXqpiPrqIsuukjpipo4cSIAu3btIjY2lieeeIKGhgZ0Oh0Oh4M777yTzMxMAC699FIMBgNWq5WIiAhuu+02EhMTAbjl5lsICg5SPmPy5MlYrVbl5/j4eK644go0Gg16vZ4nnniCFStW0NbWxpQpUxg+fLgyxnHllVd26mLv378/kZGRSh0VHx/PDTfc8IdN8JFk0ScjeIjb7aa1tRWHy3VG1YMy4O3VgFrtpNUU6vm4cYzyGfV6ZZKIIHiKCCCCIAhCl4iVb4IgCEKXnNIYiMPhoKqqisjISKXf0Ol0UlFRgdvtpnv37srYQwe73Y7ValXGOrrC5XJhNpvR6/W/Op/ZZrN1yr2l0+k6NfnNZjM6nU7p3+zogzyyr9BsNlNdXY23t7eS+qTj800mEy6XC7VajY+3D2qNWnmuvLxcmdsteIbZbFamj3f0BzudTsxms5Lh4ETV1tbicrmIiIigpqYGtVpNt27dPP0VheNwuVxUVVUpq6+hfVC5qqoKq9VKZGRkp793l8vF4cOHaW1tJaRbCP4B/r9bWdxuNxaLBZVK1WnM4kgOh0O5bqF9DNbHx0eZdGRqNSGpJKUes9lsOByOTte4zWajoqICnU5HZGSkcqzb7cZkMuF0OpGk9vc48pxUVlYSEBBw0t2ipxRAduzYwaZNm7j33nvRaDQUFhTy1ttvKbli4uLiuPHGG9vXfLjdrF27ljVr1tDY2Mhjjz2mDASdjF27dvHRRx+Rn5/PzJkzOy3WOdKGDRtYsmQJvr6+uFwuzj//fG6++WZsNhtqtZq9e/cSFxeHr68vRqORlStXIssyM2bMANoXDC1btoyWlhYkSWLQoEHccMMNBAcHU1VVxZw5c5QA4ePjw3XXXUdqaiqyLPOf//yHPn36MGnSpFM5vUIXORwOFi5cSG5uLs8884wyKWLdunW89tpr3HXXXYwbN+6E32/ZsmWYzWYee+wxVqxYgZ+fn3KdCGeu3Nxcli1bxkMPPYROp6O2tpZ33nmHvXv3YrfbCQsL49prr2Xw4MG4XC7effdd1q5di8Viwd/fnxtuuIGxY8f+5qQZl8uFJEm/Oa22traWN954g+zsbMaPH8+NN954zNft3r2befPmKbNCu3fvzv/93//h4+OD2+1m67atGI1GhgwZgkqlYuPGjWzatIk5c+ZgMBjYvXs37777LuXl5Wg0GpKTk7n55puJjo7GZDIxe/ZsrFYrWq0WlUrFzJkzGTZsGJIksX79eux2OzfffPNJneMud2E5nU5WrVrFoEGD0Ov1HD58mMfnPI5er2f+/Pn84x//ICIigscff5zy8nKg/e5+woQJWCwWWltbu/S5Go2GMWPG0K1bNyVl/LFUVFTQrVs3Zs2axaxZsxg3bhwqlUrJjfXTTz/x9ddfs2rVKqwWK9XV1cpK96ysLObNm8fEiRNZuHAhjz/+OHl5ecyfPx+3243ZbKayspKbbrqJe+65B71ez1tvvYXL5UKj0XDeeefxxRdfdJpuJ5w+sixTVlZGYWEh3333HdDewvzvf/9LRUVFp4wGLS0t5OXlUV1d3Wk6pM1mo6CggNraWlpaWpTfZWZmJsOHDQd+bm0fOnRIScnT8bjFYsFkMpGfn9/pOeH0kGWZTz/9lJiYGHx9fbFarTz33HMcPnyYuXPn8s9//pNhw4Yxe/Zs9uzZg1qtZsiQIcyZM4c33niDCRMmsGjRouOmPP/444/ZunXrb75GkiSGDBmiJDn8NQ0NDcpsrnvvvZdrrrkGPz8/mpub+eSTT9i6dSvbtm3j448/prGxkdraWvbs2YMsy+Tn5zN79mx69+7Nwn8s5LnnnkOWZZ5++un2iSoOBwUFBUybNo3777+fxMREnnvuOWpqagAYOXIkWVlZR6W0P54ut0BKSkooLy+nT58+AGzduhWz2czdd99NQEAAAH/729/48ccf2bhxI1dffTVjxoyhra3tlNIjp6WlkZaWxq5du45a6PNLOp0OjUaDr68v0dHRAMTFxXEw5yCvvf4a8fHx3HHHHfj4tueEUalUuN1u3n33Xfr378/ll18OtOfAuffee7n99tvZt2+fksIkISGBwMBALrjgAj766CMcDgdqtZrevXsrFdCAAQO6/F2FrpFlGb1eT2ZmJrt37+byyy+nsLAQh8NBWlqaEii2b9/O22+/jdPpxOVyKQvIWlpamDdvHqWlpYSFhZGdnU3//v0B2LJlC35+fsT1iGPbtm18+umnWCwWXC4XV155JRMmTKC4uJglS5bg7+9PUVERdrude++9V1wLp1F9fT0FBQVcccUVAOTk5JCTk8OSJUuUumD69Ols3ryZNWvW0K9fP6UHwe12k5SUdEJTXw8cOHDcRX0hISGMGzeOw4cPU1xc/Kuvk2UZtVqNVqvF29ub6OhoNBoNwcHBxMfH89FHHyG7ZR56+CFCQkKQaG/5SJLEhg0bCA0N5cYbb1SGBu655x6uvfZadu7cSd++fdHpdMTFxZGQkMCECRP45JNPaGxsJCwsjOjoaPz8/Dh06JCSNfhEdDmAZGdn09zcrCTkys/PV/JddfDy8iI5OZndu3czc+ZMJEn63ZbVH+89wsLCMJlMfPjhh+zevZuxY8dy9913k5eXh8ls4tprr8XX15esrCyioqKQJAlZlrHb7ZSWljJ06NBO75eQkEBsTCzl5eWkpKRQX1/P8uXL8fX15YcffqBv375Kl5a/vz+xsbEcOnRIVBoe4nK5SElJISsri+zsbLKyshg6dCg//PADkiRhMpl48803mTx5MpmZmeTl5fHkk0+Snp7Ozp07KS8v58UXX8TtdnPPPfco11tpaamS8y0tLY2ePXvidrvZvHkz/1n+H0aNGoXZbGbLli08++yz3HPPPSxdupQVK1aIa+E0Kisro7GxkdDQUACKi4uJjY0lLDRMeY1KpWLw4MFs2rQJh8OBVqtl06ZNfPbZZxw4cIBbb721U33WobGxUcle0draSn19vdIbEh4e/qspTI63ADkwMBCDwcDKlSvZtWsXSUlJPPHEE7S2tpKfn8/YsWPx8vKisLCwPWO46uexvb1795KcnNxpXDkgIIDk5GSKioro168fbW1tfP7558TExLB9+3aGDx+upKPXaDRERkaSnZ3NmDFjTvg8dzmANDU14ePj0ynn1LH6Cjsq5hNVUlLCd999h8vlIjk5+aiK/ESNHz+eMWPGoNfr2bdvH3PmzGH8+PGkpKTQq1cv9u3bR1RUFL6+vkelVZEk6ajWjSRJqNQ/32lYLBYOHDiALMvs27ePGTNmHJUM8sjFQcLPOlbZqtXqX+1f7vh9dHXRpizL+Pn5kZaWxpdffklNTQ133nmnEkBqa2s5ePAgcXFxFBcXKwOo+fn5HDx4kNGjR/+coC4tTdkp88j+7uLiYlauXInb7aaxsZGCwgJl4kZiYiL9+/fH29ubpKQkysrKcLvdXUr537GaWKVSHfP4jrvm4/XFn0tqa2s7TaIBjnm9/fLvvHfv3nh5ebFlyxY2btzI2LFjlRuGDl999RUff/wxKpWKvLw8tm3bxrp16+jWrRtPPvnkSd3BH2nAgAG89tprGAwGKsormHXfLNavX8/FF1/M9OnT2b59Owa9gbS+aajV6t/MiQWd67GO/Zry8vKora0lKyuLBx54oNOAfnBw8G92sR1LlwOIn5+fMqoP7X8wP/zwAy0tLfj5+QHt/c75+flMnjxZ+cV1NLl+7ULX6XQEBgbicrnw9vn1GQHHS2p45AytlJQUjEYjVVVVSpdbamrqMf8gdTodUVFR5Obmdnq8uLiY8vJyoqOjlf0DZs+eTUhICG+//TbLli0jPT1dmdlgNpmJiIjo6un9UzKbzXzyySfs379fuQ7GjRvH6NGjlU14srKy2Lx5M1VVVUyZMkVZ2dsVsiwzZMgQ3nzzTQYNGnRUcjydTkd8fDz+/v7Isszjjz/OgAED+OGHHzqlf/jlH6okSVgsFhYuXMioUaOYOHEiBQUFPPvss8prdTpdp+yrXQmEbreb7zZ/x9r/rgXaW1W9evVi2rRpSsv/2w3fsva/a5Ugk5yczMyZM8/5RYbdunXr1AUVGxtLaWkptXW1SgXvdrv56aef6NWrl9JqCA0NJTQ0lJ49e3L99ddz8OBBhg8f3um9p0yZwujRo5EkiXnz5tGzZ08uvfRSZFn+zdl5Hdf8r9HpdEr9EdcjjuTkZGpra5V6KiMjA6DTTXvHz2lpaezevbvTTUpTUxN5eXlceOGFyG4ZHx8fZs2aRa9evVjz1RqWvreUwYMHKzdK9fX1R2UpPp4u366kpaUREBCg3JkNHz4cg8HAkiVLqKyspKamhg8++ACn08no0aOB9qZffn4+zc3NlJSUUFlZeVSzLiIiggsvvJCpU6eSlpp21OdaLBYKCwupra2lpqaG0tJS7HY7TqeTpUuXsnXrVqVVUFhYSEVFBV988QVtbW0kJSUp76PRaDoFj467OJVKxfXXX8/OnTtZs2YNtbW1FBYWMn/+K2RkDCY1NVW5g+4o+8UXX0x9fT3ffPMN0J6vv7ikWEnPLLTfTDz11FMsX76cESNGcOGFF5KQkMCcOXNYvnw50D57atOmTVgsFnJzc9mzZ0+XP68jy3LPnj159tlnuf3229HpdMrj4eHh9OjRg+bmZgYOHEh6eroSTPr27cu3335LUVER+fn5bNu2TXnfjtxDdpud5uZmQkJCUKlU/PTTT8rmaB2ff+QxXcmftmLFCh6b/RhxcXFMnjyZ888/nxUrVvDkk08qd9c6vY5hQ4cxceJEhg8fznvvvcfSpUs9/ev2uNjYWIKDg6mrqwPaUyrFxsbyyiuvUFJSQm1tLatWraK4uJgpU6bgcrnYv38/xcXFVFRUsGHDBpxO5zFbE35+fnTv3p3IyEi8vb0JDAwkPDyciIiIY3ZfdXSLV1ZWUldXR2FhoVJvrly5ko0bNwLtiV0PHTpERUUF69ev56effupUh2g0mk5dVB3XlSzLyhjLsmXLqK6uprKykkWLFhESEsKgQYNwuV2dMgdnjm3f7uKTTz4B2rvBKisr6du370md5y63QGJjY4mIiCAnJ4fw8HBCQkKYM2cOS5YsYc6cOVRXV2M2m1m4cCFRUVFA+wBkx2Dzxx9/zP79+096I6b8/HxeeeUVpQ9y3rx5PPTQQ4SHh5Ofn6/0WW7ZsoXNmzcrQeKvf/2r0t93LB1TPQEGDx7MrFmz+Pjjj5X+yL59+3LffbNQqVR4e3srg1LQ3vSbNm2asufIwYMH0Wg0Su4bAb777jvWrVvHkiVLGDhwINA+8wNgyZIlXHDBBcpkBb1ezwMPPNDlsTKVSkVCQgJBQUGoVCrlczoGR8PDw/Hy8uLuu+/mzTffZPv27ciyTHx8PH/729+YPHkyBQUFPPnkkwQHBysDj9C+nbKfnx8BgQFceeWVfPjhh3z55ZdERkaSkZGBWq1Wuq06ug8CAgKIiYk5qVZIQ0MDb7/9Ntdddx233HKL8nhUVBTXXXcdmZmZTJw4kfPOO6/TcdnZ2ezevVtZo3SuCg4OJiEhgW3btpGSkoLBYODhhx/mzTff5O9//zuNjY2UlJSwYMECUlNTsdvtrF+/nh9//BFJktDpdNx1112/WWcAxMTEKHts/Jra2lrmzZtHVVUVbrebv//979x+++0MGjSIsrIyZXOo7OxsPv30U1QqFU6nk6uvvvqo1s+RQkND6du3L5IkER8fz8MPP8w777zDpk2bKCwsxN/fn/nz5+Pj44PL5SItLU3pstLpdFx99dVs3LiRtrY2Dh8+jMlkIiUl5eROtHwKtm3bJj///PNyW1ub8pjdbpdramrkzZs3y1deeaW8Zs0a5Tmr1So3NDTITU1NcmNjo9zS0iK73e6T+ky73S43NDTIjY2Nyj+n0ynLsiybTCbZZrPJsizLNptNrqmpkauqquSW5pbjvq/VapWtVmunx0ytJrmqqkp+44035Ouvv14uKiqSZVmWXS6XbDabZbfr57I7nU7ZZDLJdrtdnjdvnvzZZ5+dyqn903nyySfla665Rvn9dCgsLJQzMjLk1atXK4+5XC551qxZ8ksvvdTlz7NYLLLdbj/m40eWoa2tTa6qqpKrq6s7/f4dDodcXV0tNzY2tl8blvbnrFarcr273W65rq5OrqmpkW02m2wymWS32y07nc726+N/17bdbpfNZvNJlX/t2rXyoEGD5IKCgk6P22w2ecaMGfLs2bOVx2pra+V169bJK1eulGfOnCl/9NFHf8wv8SyTnZ0tP/DAA3JTU5PymNPplOvq6uSdO3fKM2fOlJctW6b8ntra2n6uM1qOX2fI8q9fZ0dyOp2d6quGhgblGrRYLMr15HA45Lq6OrmqskpubGw87mcfec0dWZ6amhr5s88+k2fOnCnv2bNHluX2a9VsNit1pSzLstvlVq7Td999V160aNFJ18entJAwPT39qL5WrVar9CMGBARgs9mUriGDwXDSfWy/pNVqlZ28funIsuh0OmUGxok4Vrm8fbzx9vHmhhtu6LTo8VirSTvuPB0OB+PHj++USfhcJ8sy5eXlREVFHbUC3N/fH51O97uvlfi1Vu0vH9fr9Z2yDHTQaDRK3/CRjrxOJEnq1Ofd8d3UanWn60Or1Z7w5kIdamtr0ev1R13rOp1OWRHf8XeVn5/PM888Q11dHd27d2fYsGG/67k8W6WkpHD99dd36vbpyCLQrVs3nnnmGSorK5X1W3q9/qTqDOCEek/UavUxZ3P98niNRnNSGQ6OHDM58v2MRiOXXnopsbGxyvOSJB1VZ0mq9sdkWSY9PZ3u3buf9FjdKQUQnU5HWlrarz7/W8+dTTQaDSNGjDih12q12pPuR/yzkyQJX19fJU3DkRepw+HA6XSedAX7Z6fX67Hb7Er3RgdZlrFYLOj1euU8DhgwgA8//BCLxcKCBQtYsGABL7zwwjl/TtVqtTJp5lhiY2OJjY31dDH/MOnp6Sf0OkmSulxXizl/wmmRnp7Ojh07qK6u7vR4dnY2kiR1WiMh9ltpDwpanZbs7OxOj9fU1LBnzx4yMjKU82QwGAgPDyc+Pp7Jkyezb98+ZZBWEP5IIoAIp0VmZiaBgYEsWrRISQ+Sk5PDq6++yoQJE5QZchaLhcbGRmw2G1arlZaWlnNyPU3HauFXX32Vffv20dLSwuHDh1m0aBFeXl5kZmZit9vZs2cPdXV1NDU1UVFRwfLly4mKijrnp/EKp4fYD0Q4bfbv38/ChQupqalBpVJRUlLChAkTuP/++5XFWm+//TarV6+mqqoKvV5PSEgIM2bM4JJLLvF08U+7+vp6/vWvf7Ft2zZ8fX2prq5WFqulpqZisVh4/vnnlXU1NpuN8PBw7r33Xnr37u3p4gvnABFAhNPKZDJRUVFBU1MTL774IomJiTzxxBPKAF9NTQ0NDQ3KFFS3201ISAjBwcGeLrpHuFwuSktLsVgsLF++nF27drFgwQJlm+Xm5maqqqqUgWDR+hBOJxFABI/Jz8/n66+/ZsqUKUqCO+HXmc1mli9fTs+ePY9a/yEIniACiCAIgtAlYhBdEARB6BIRQARBEIQuEQFEUJyLvZnn4ncWhN/LKa1EF/4cZFlm27Zt9OjRA0mS2LJlC06nk759+9KrVy/q6+vZuHEjDocDHx8fMjMzcblc2G12goKDKCoqorS0lFGjRh3z/ffv38++fftQq9VkZGQQFxf3h32XmpoaZUuA7OxsHA4HISEhFBcXHzObQGNjI/v372f48OEnlXywqKiI2tpaUlJSWL9+vbKivkePHqSnp/Ptt9/S1NSEWq1mxIgRhIaGUl1dTWRkJHa7nS1btnDeeecdM4VOVVWVsidOfHw8gwYN+sMSI7a2tmIymZT0KDt37mTEiBFs2rSJMWPGHDWjy+VysXHjRgYMGHDUPhnCuUe0QATq6upYt24dRqORyspKVCoVfn5+LF68mJKSEsrKytiwYQPBQcEEBASgUqnYunUrn634jMbGRkpLS5Vd/I61Ic13331HaWkpDoeDhQsXUltbiyzLynsDyl7zhw8fpqGhAWjf1z4/P5+2tjagfcpqXl6eksrcZDIp+46bTCZkWeb9999n5cqVtLS04OXlha+vL7W1tezduxdoT1tdWFiorIj38vJi69atv7nV6C+5XC7Wrl2LJEloNBr8/f0JCgpi586d7Ny5E7PZzIoVK9BqtQQHB6PT6aiqqmLhwoUUFBTQ0tJCVlYWVVVVFBYWKnvqdMjNzWXr1q34+vry9ttv8/XXXwMoG17Z7Xbl+7e0tCh7vLe0tHDw4EHl/DkcDvLz86mtrVV+bm1tpaysTPn+33//Pa+//jo1NTXtub2Cg7HZbHz//ffKeT+ynGq1GrPZzM6dOz192QpnANECESgoKCAmJoaAgAAGDBhAeno6DoeD//73v9TW1qLT6XA4HJjMJuIT4lGr1eTl5ZGXl0dsbCySJJGTk8MXX3xBRUUF11xzzVF7oaSnp3PBBRewd+9eampq+PHHHzl48CBut5uhQ4eSnp7OokWL0Ov1DBo0CIfDwbZt2zAajQwePJju3bvz+eef43Q60el0zJw5k3feeUdJKGiz2bj11lspKiqioqKC1NRUWltacTgdhIWFoVKpsFqtvP/++zQ3NyPLMhdccAGDBg0iLi6O3NxcJWX78dTV1VFRUaFs3JSZmYnVamXt2rVK6niHw4HJZCI2NpagoCD27NlDfn4+WVlZZGRk0NjYyFdffUVdXR19+vTh8ssv77Q/TVJSEhdddBEVFRXs2bOH6OhoPv/8cyUozZgxg+XLl9PQ0EB8fDzJycmsWLECnU5H9+7dmTJlCh9//DF1dXWoVCouvvhiAN5//30iIyOprq7m2muvpbS0lAMHDrB9+3YiIiL4fssWYuPilLJs3bqV77//HkmSiI6O5sorryQtLY1///vfjBgx4pSTowpnN9ECESgqKlI2zunIr/T1118TEhJCWloaoaGhTJ48GYfDwfz58ykuLqZ///4MHTqUcePGYTAY8PHx4YYbbmDcuHH88MMPR33GunXrmD9/PgaDAZfLxYYNGxgzZgwZGRls2LABk8lEa2srV155JbGxsaxatYprrrmGu+++m4EDB7JixQp8fHyYMmUKzc3N7Nq1i8rKSqKjo7n33nuRZZn6+nqSk5MZNmwYGRkZNDU3KZs8qVQqDh48SHZ2NhMnTiQ5OZmvvvoKl8tFYmKi0hI6ER173RyZCXXv3r0YjUZ69+6NXq9n8uTJhIaG8vnnn/Ppp58SGRlJr169mDx5MhEREciyzMSJE7n55ps5sP9Ap3QtkiSxa9culixZwr59+xgzZgyrV68mKSmJyZMnK3t+NDc3M3jwYC655BLWrFnDwIEDeeCBB5g2bRpbt25l//79TJ06lfj4eL766iuam5vRarXceuutDBgwgG3bttEzqSf9+vXjwskXotVqKS0tVbbHNZlMfPrpp6SlpZGZmcmhQ4c4fPgwvr6+NDQ0YDabPX3pCh4mWiDCUf3rmzZtIisrizvvvBODwUBERISSSqSgoICcnBy6devWab/l2NhYDAYDwcHB5OTkHPUZoaGhjBw5kt69e3Pw4EHq6uqUrpfzzz8flUpFUFAQISEh5ObmYrfbCQ0NVXaOrKqqQpZlCgsL6dOnD/Hx8ciyTFhYGBqNBoPBgMPhOGof6CMTM5rNZtxutxIszjvvPCRJUvYbP1HHem12djZDhgxRUrdPmzYNgLCwML744gtGjBiBLMtotVpl69OQkBCQwcfXp9NgvizLBAQEkJKSwsSJE4mIiGDdunW0tbVRWlrKiBEjCAsLw8vLi+7du6NSqXC73cTGxqJSqdDpdNTV1eFyuSgrK0Ov1ysp3oOCgtDpdHh5eVFTU4OM3P59pKPPV1tbGzabjZaWFtxuNxkZGRiNxnMyN5lwbCKACCQlJbF9+3agvU/85ZdfZubMmRQVFaHVamlsbKSurg673U5ZWRlTp06lra2N7Oxs+vTug91uVyrAY23dKssyaWlpDB48WPm8lJQU9Ho9YWFhOBwONBqNst1sQkICoaGhrFq1iuTkZPz9/cnIyCA3N5fo6GhaW1uVrpNffm5oaKgyqN+x3af8v60/Y2Ji8Pf3x9fXF29vb/R6PSqVikOHDp1UWu/IyEj8/Pxoa2vDYDBQUVFBTk6O0k3U0tLC3r178fX1Ze3atURFReHl5YXVamXbtm306tVLKduxzpksy8TExCjdYdC+E6HJZGLgwIGYzWZCQ0M7bVHao0cP1qxZo6TGHzhwIAcPHiQgIAC9Xo+XlxfNzc3K6zvOS2BgILW1tezevRun09mpTAEBASQnJ+N0OomLi8NutyuBJzQ0FF9fX09fuoKHqefOnTvX04UQPEuv1/Ptt9+SmpqKyWTC5XLhdDqprq4mLi4Oh8PBxo0bqaioYNKkSaSnpxMYGIjVaqW0rJTExES6detGdHQ0sizj4+PTaYtgWW6vdDtm7RgMBmJjY9m+fTv5+fmEhoYSExODRqMhJiYGg8FAz5492bt3L0VFRcTExDB06FDMZjM7duygra2NlJQUfHx8SExMJDAwENktExsTS8+ePamtrcVkMpEQn0BISAihoaF4eXnRp08fIiMj+fHHHykpKSE6Oho/Pz/WrVvH6NGjf3XTn18yGo3KoHJsbCxms5mgoCB69eqFJEnYbDa2bNnCgQMHSEhI4NJLL8XX15eIiAhyc3OJiIggKCiI2NhYpYXV8f3bz5eMn59fp/QuCQkJVFRUsHv3biRJIjk5GZVKRVT39uCUkJBAc3MzO3bswGg0kpGRQUhICD/88APl5eXExcURGhqKj4+P8nsKCgqiX79+6PV6CgoKSIhv3wY4KSkJrVZLUlISvXr1Ij8vn7379mIwGEhISGDDhg0kJyef8JiR8OclUpkIAOzevZvQ0NBOFf+5oL6+nqKiItLT00+qG6uiooLKykoyMjI8/RVOK5fLxZYtW+jXrx/+/v6eLo7gYSKACIIgCF0iZmEJgiAIXSICiCAIgtAlIoAIgiAIXSICiCAIgtAlIoAIgiAIXSICiCAIgtAlIoAIgiAIXSICiCAIgtAlIoAIgiAIXSICiCAIgtAlIoAIgiAIXSICiCAIgtAlIoAIgiAIXfL/LazwZVK3C4sAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMTItMDNUMDc6NDU6NDkrMDA6MDAO1hoAAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTEyLTAzVDA3OjQ1OjQ5KzAwOjAwf4uivAAAACh0RVh0aWNjOmNvcHlyaWdodABDb3B5cmlnaHQgQXBwbGUgSW5jLiwgMjAyMAq63rAAAAAXdEVYdGljYzpkZXNjcmlwdGlvbgBEaXNwbGF5FxuVuAAAAABJRU5ErkJggg=="}}},{"metadata":{"papermill":{"duration":0.225163,"end_time":"2020-12-14T10:51:00.642395","exception":false,"start_time":"2020-12-14T10:51:00.417232","status":"completed"},"tags":[]},"cell_type":"markdown","source":"En este apartado se van a detectar los outlier. Para ello, primero se va a obtener información de las variables numéricas."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:01.105052Z","iopub.status.busy":"2020-12-14T10:51:01.103966Z","iopub.status.idle":"2020-12-14T10:51:01.14023Z","shell.execute_reply":"2020-12-14T10:51:01.139438Z"},"papermill":{"duration":0.275468,"end_time":"2020-12-14T10:51:01.14037","exception":false,"start_time":"2020-12-14T10:51:00.864902","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.22222,"end_time":"2020-12-14T10:51:01.584984","exception":false,"start_time":"2020-12-14T10:51:01.362764","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Con el estudio de los estadísticos vemos que las variables `Age`, `SibSp`, `Parch` y `Fare` presentan outlier:\n* Para `Age` vemos que la media de sus valores es 29, y el tercer cuartil es 38, sin embargo el máximo valor es 80. Por lo que hemos detectado que esta variable tiene al menos un outlier con valor 80.\n* Para `SibSp`vemos la media con un valor de 0.52, el tercer cuartil con 1, y sin embargo el máximo valor es 8, por lo que hemos detectado un outlier.\n* Para `Parch` vemos su madia de 0.38, su tercer cuartil vale 0, y el máximo vale 6.\n* Para `Fare` su media vale 32 y su tercer cuartil 31, sin embargo tiene un valor máximo de 512. Este outlier es el más alejado de todos los que hemos detectado.\n\nAhora se define una función para eliminar estos outlier."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:02.042654Z","iopub.status.busy":"2020-12-14T10:51:02.040859Z","iopub.status.idle":"2020-12-14T10:51:02.047578Z","shell.execute_reply":"2020-12-14T10:51:02.046676Z"},"papermill":{"duration":0.239559,"end_time":"2020-12-14T10:51:02.047728","exception":false,"start_time":"2020-12-14T10:51:01.808169","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def outlier_detect(feature, data):\n    outlier_index = []\n\n    for each in feature:\n        Q1 = np.percentile(data[each], 25)\n        Q3 = np.percentile(data[each], 75)\n        IQR = Q3 - Q1\n        min_quartile = Q1 - 1.5*IQR\n        max_quartile = Q3 + 1.5*IQR\n        outlier_list = data[(data[each] < min_quartile) | (data[each] > max_quartile)].index\n        outlier_index.extend(outlier_list)\n        \n    outlier_index = Counter(outlier_index)\n    #If there are three or more outlier data features we must delete them. (n)\n    outlier_data = list(i for i, n in outlier_index.items() if n > 3)\n    return outlier_data","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.223741,"end_time":"2020-12-14T10:51:02.508296","exception":false,"start_time":"2020-12-14T10:51:02.284555","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Se van a eliminar los outlier de las variables que hemos detectado que tienen algún outlier, que eran: `Age`, `SibSp`, `Parch` y `Fare`. Se van a eliminar los outlier de las variables solo si hay más de tres outlier en esa variable.\n\nNosotros habríamos eliminado los outlier en el preprocesamiento después de haber terminado el análisis exploratorio, y lo habríamos incluido en el pipeline como hicimos en la práctica 1. Además, aquí se define una función para eliminar los outlier, y este es un trabajo innecesario, ya que `Scikit` tiene funciones implementadas que realizan esta función, y no sería necesario escribir nuestra propio función."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:02.959447Z","iopub.status.busy":"2020-12-14T10:51:02.958164Z","iopub.status.idle":"2020-12-14T10:51:03.014956Z","shell.execute_reply":"2020-12-14T10:51:03.014181Z"},"papermill":{"duration":0.284588,"end_time":"2020-12-14T10:51:03.015104","exception":false,"start_time":"2020-12-14T10:51:02.730516","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"outlier_data = outlier_detect([\"Age\",\"SibSp\",\"Parch\",\"Fare\"], train_data)\ntrain_data.loc[outlier_data]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.223258,"end_time":"2020-12-14T10:51:03.467886","exception":false,"start_time":"2020-12-14T10:51:03.244628","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Obtenemos que no se ha detectado ningún outlier porque hay menos de tres outlier en cada variable por lo que no merece la pena eliminar ninuna instancia."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:03.943827Z","iopub.status.busy":"2020-12-14T10:51:03.942808Z","iopub.status.idle":"2020-12-14T10:51:03.946705Z","shell.execute_reply":"2020-12-14T10:51:03.947278Z"},"papermill":{"duration":0.235926,"end_time":"2020-12-14T10:51:03.947472","exception":false,"start_time":"2020-12-14T10:51:03.711546","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(outlier_data, axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.222672,"end_time":"2020-12-14T10:51:04.398344","exception":false,"start_time":"2020-12-14T10:51:04.175672","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 1.2. Combina los datos"},{"metadata":{"papermill":{"duration":0.222389,"end_time":"2020-12-14T10:51:04.843532","exception":false,"start_time":"2020-12-14T10:51:04.621143","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Ahora se van a combinar los datos de test y de entrenamiento para obtener todos los datos juntos. Esta variable no la usaremos para nada, ya que en la libreta ha utilizado todos los datos para hacer el análisis exploratorio, pero esto es incorrecto ya que no podemos utilizar la información de los datos de test en el análisis exploratorio."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:05.300558Z","iopub.status.busy":"2020-12-14T10:51:05.299735Z","iopub.status.idle":"2020-12-14T10:51:05.303085Z","shell.execute_reply":"2020-12-14T10:51:05.302325Z"},"papermill":{"duration":0.236494,"end_time":"2020-12-14T10:51:05.303214","exception":false,"start_time":"2020-12-14T10:51:05.06672","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.22365,"end_time":"2020-12-14T10:51:05.74926","exception":false,"start_time":"2020-12-14T10:51:05.52561","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 2. Análisis de datos"},{"metadata":{"papermill":{"duration":0.229141,"end_time":"2020-12-14T10:51:06.201239","exception":false,"start_time":"2020-12-14T10:51:05.972098","status":"completed"},"tags":[]},"cell_type":"markdown","source":"En este apartado se va a realizar el análisis exploratorio. Cuando se realizo el `data_train.info()` obtuvimos el tipo de cada variable:"},{"metadata":{"papermill":{"duration":0.222631,"end_time":"2020-12-14T10:51:06.646738","exception":false,"start_time":"2020-12-14T10:51:06.424107","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Variables tipo Object:\n1. Name        : nombre de los pasajeros\n1. Sex         : hombre y mujer (male and female)\n1. Ticket      : número del ticket\n1. Cabin       : categoría de la cabina\n1. Embarked    : puerto (puede ser C, Q, S)\n\nVariables tipo Int64\n1. PassengerId : id único de los pasajeros\n1. Survived    : 0 -> muerto ,1-> sobrevivió. Esta es la variable clase.\n1. Pclasss     : Clase de ticket: 1, 2 y 3 \n1. SibSp       : Cantidad de hermanos / esposo/a a bordo\n1. Parch       : Cantidad de padres / hijos a bordo\n\nVariables tipo Float64:\n1. Age         : edad del pasajero\n1. Fare        : precio del ticket"},{"metadata":{"papermill":{"duration":0.226232,"end_time":"2020-12-14T10:51:07.096607","exception":false,"start_time":"2020-12-14T10:51:06.870375","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Primero se realiza un análisis univariado de la variable clase, que es la variable `Survived`, y es una variable categórica, que está formada por dos estados distintos, 0 y 1."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:07.551364Z","iopub.status.busy":"2020-12-14T10:51:07.54983Z","iopub.status.idle":"2020-12-14T10:51:07.714698Z","shell.execute_reply":"2020-12-14T10:51:07.714015Z"},"papermill":{"duration":0.392545,"end_time":"2020-12-14T10:51:07.714828","exception":false,"start_time":"2020-12-14T10:51:07.322283","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.countplot('Survived',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.259402,"end_time":"2020-12-14T10:51:08.199673","exception":false,"start_time":"2020-12-14T10:51:07.940271","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que la muestra está desbalanceada, ya que hay casi el doble de casos en los que la variable clase vale 0, que los que vale 1. Por lo que tenemos más casos en los que no se sobrevivió, que los que si se sobrevivió."},{"metadata":{"papermill":{"duration":0.222791,"end_time":"2020-12-14T10:51:08.658105","exception":false,"start_time":"2020-12-14T10:51:08.435314","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Ahora vamos a ver la relación que tiene cada variable predictora con la variable clase de forma individual.\n* Sex - Survived\n* Pclass - Survived\n* Embarked - Survived\n* SibSp - Survived\n* Parch - Survived\n* Age - Survived\n* Fare - Survived"},{"metadata":{"papermill":{"duration":0.241074,"end_time":"2020-12-14T10:51:09.122348","exception":false,"start_time":"2020-12-14T10:51:08.881274","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Sex - Survived\n\nAquí se ha realizado una fuga de datos (aquí y en todo el análisis exploratorio), pues se ha realizado el análisis exploratorio con la variable `data`, que incluye a los datos de entrenamiento y a los de test. Se ha dejado comentada la línea de código original que utilizaba todos los datos para que se vea el error en este caso, pero nosotros vamos a utilizar solo los datos de entrenamiento, y para el resto de casos corregiremos esta línea."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:09.65893Z","iopub.status.busy":"2020-12-14T10:51:09.652211Z","iopub.status.idle":"2020-12-14T10:51:09.664435Z","shell.execute_reply":"2020-12-14T10:51:09.663519Z"},"papermill":{"duration":0.262102,"end_time":"2020-12-14T10:51:09.664598","exception":false,"start_time":"2020-12-14T10:51:09.402496","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean()\ntrain_data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:10.143013Z","iopub.status.busy":"2020-12-14T10:51:10.142088Z","iopub.status.idle":"2020-12-14T10:51:10.429353Z","shell.execute_reply":"2020-12-14T10:51:10.428652Z"},"papermill":{"duration":0.525531,"end_time":"2020-12-14T10:51:10.429484","exception":false,"start_time":"2020-12-14T10:51:09.903953","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"Sex\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.226279,"end_time":"2020-12-14T10:51:10.882485","exception":false,"start_time":"2020-12-14T10:51:10.656206","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que un 74% de las mujeres sobreviven, mientras que de los hombres solo sobreviven un 18%. Por lo que las mujeres sobreviven más que los hombres."},{"metadata":{"papermill":{"duration":0.225761,"end_time":"2020-12-14T10:51:11.339575","exception":false,"start_time":"2020-12-14T10:51:11.113814","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Pclass - Survived"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:11.80845Z","iopub.status.busy":"2020-12-14T10:51:11.799875Z","iopub.status.idle":"2020-12-14T10:51:12.135592Z","shell.execute_reply":"2020-12-14T10:51:12.134774Z"},"papermill":{"duration":0.570045,"end_time":"2020-12-14T10:51:12.135758","exception":false,"start_time":"2020-12-14T10:51:11.565713","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"Pclass\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.225797,"end_time":"2020-12-14T10:51:12.588086","exception":false,"start_time":"2020-12-14T10:51:12.362289","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que los pasajeros que tenían un ticket de 1 sobrevivieron más que los que tenían un ticket de 2, que a su vez, sobrevivieron más que los que tenían un ticket de 3."},{"metadata":{"papermill":{"duration":0.228879,"end_time":"2020-12-14T10:51:13.055487","exception":false,"start_time":"2020-12-14T10:51:12.826608","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Embarked - Survived"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:13.529761Z","iopub.status.busy":"2020-12-14T10:51:13.528484Z","iopub.status.idle":"2020-12-14T10:51:13.843189Z","shell.execute_reply":"2020-12-14T10:51:13.843759Z"},"papermill":{"duration":0.560517,"end_time":"2020-12-14T10:51:13.843941","exception":false,"start_time":"2020-12-14T10:51:13.283424","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"Embarked\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.23099,"end_time":"2020-12-14T10:51:14.305096","exception":false,"start_time":"2020-12-14T10:51:14.074106","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que los que pertenecían al puerto de embarcación de C, sobrevivieron más que los del puerto S y Q. Mientras que la cantidad de pasajeros que sobrevivieron de los puertos S y Q es muy similar."},{"metadata":{"papermill":{"duration":0.225576,"end_time":"2020-12-14T10:51:14.76602","exception":false,"start_time":"2020-12-14T10:51:14.540444","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### SibSp - Survived"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:15.23428Z","iopub.status.busy":"2020-12-14T10:51:15.233231Z","iopub.status.idle":"2020-12-14T10:51:15.748891Z","shell.execute_reply":"2020-12-14T10:51:15.74807Z"},"papermill":{"duration":0.755961,"end_time":"2020-12-14T10:51:15.749064","exception":false,"start_time":"2020-12-14T10:51:14.993103","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"SibSp\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.227708,"end_time":"2020-12-14T10:51:16.203308","exception":false,"start_time":"2020-12-14T10:51:15.9756","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que los pasajeros con 1 `SibSp` tienen un mayor porcentaje de haber sobrevivido que el resto."},{"metadata":{"papermill":{"duration":0.227568,"end_time":"2020-12-14T10:51:16.656187","exception":false,"start_time":"2020-12-14T10:51:16.428619","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Parch - Survived"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:17.12558Z","iopub.status.busy":"2020-12-14T10:51:17.124271Z","iopub.status.idle":"2020-12-14T10:51:17.573207Z","shell.execute_reply":"2020-12-14T10:51:17.572419Z"},"papermill":{"duration":0.688736,"end_time":"2020-12-14T10:51:17.573333","exception":false,"start_time":"2020-12-14T10:51:16.884597","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"Parch\", y =\"Survived\", data=train_data, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.229997,"end_time":"2020-12-14T10:51:18.031244","exception":false,"start_time":"2020-12-14T10:51:17.801247","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Aquí etá más repartido ya que los valores de 1, 2 y 3 para `Parch` tienen la misma frecuencia para la clase."},{"metadata":{"papermill":{"duration":0.249785,"end_time":"2020-12-14T10:51:18.509534","exception":false,"start_time":"2020-12-14T10:51:18.259749","status":"completed"},"tags":[]},"cell_type":"markdown","source":" ### Age - Survived"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:18.971653Z","iopub.status.busy":"2020-12-14T10:51:18.970789Z","iopub.status.idle":"2020-12-14T10:51:19.807342Z","shell.execute_reply":"2020-12-14T10:51:19.806674Z"},"papermill":{"duration":1.069119,"end_time":"2020-12-14T10:51:19.807491","exception":false,"start_time":"2020-12-14T10:51:18.738372","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data, row=\"Survived\")\ng.map(sns.distplot, \"Age\", bins=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.228974,"end_time":"2020-12-14T10:51:20.266954","exception":false,"start_time":"2020-12-14T10:51:20.03798","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que las dos gráficas siguen una distribución normal, por lo que no podemos intuir a priori si esta variable es inútil para la clasificación. Aunque si nos fijamos en las dos gráficas vemos que hay la misma cantidad de valores para los mismos datos, es decir, que independientemente de si `Survived `vale 1 o 0, los dos gráficos son muy similares, por lo que parece que esta variable no aporta mucha información."},{"metadata":{"papermill":{"duration":0.233085,"end_time":"2020-12-14T10:51:20.728748","exception":false,"start_time":"2020-12-14T10:51:20.495663","status":"completed"},"tags":[]},"cell_type":"markdown","source":" ### Fare - Survived"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:21.201222Z","iopub.status.busy":"2020-12-14T10:51:21.194471Z","iopub.status.idle":"2020-12-14T10:51:21.906102Z","shell.execute_reply":"2020-12-14T10:51:21.905436Z"},"papermill":{"duration":0.949889,"end_time":"2020-12-14T10:51:21.906229","exception":false,"start_time":"2020-12-14T10:51:20.95634","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data, row=\"Survived\")\ng.map(sns.distplot, \"Fare\", bins=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.230749,"end_time":"2020-12-14T10:51:22.373568","exception":false,"start_time":"2020-12-14T10:51:22.142819","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que las dos gráficas siguen una distrbucióna asimétrica positiva, por lo que al no seguir una distribución uniforme pueden aportar algo de información. Aunque si nos fijamos en que las dos gráficas son muy similares, concluimos que por si sola esta variable no aporta mucha información.\n\nVeremos como se relaciona con el resto en el análisis multivariado, al igual que `Age`, que por si sola tampoco parece que aporte información."},{"metadata":{"papermill":{"duration":0.231654,"end_time":"2020-12-14T10:51:22.833467","exception":false,"start_time":"2020-12-14T10:51:22.601813","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='7'></a><br>\n### 2.2. Matriz de correlación"},{"metadata":{"papermill":{"duration":0.231098,"end_time":"2020-12-14T10:51:23.293824","exception":false,"start_time":"2020-12-14T10:51:23.062726","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para realizar un análisis multivariado, en el que vemos como se relacionan las variables, se va utilizar una matriz de correlación. Se ha vuelto a realizar una fuga de datos, ya que ha utilizado todos los datos para calcular la matriz de correlación. Nosotros lo corregiremos y utilizaremos solo los datos de entrenamiento."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:23.771075Z","iopub.status.busy":"2020-12-14T10:51:23.767825Z","iopub.status.idle":"2020-12-14T10:51:24.305434Z","shell.execute_reply":"2020-12-14T10:51:24.304794Z"},"papermill":{"duration":0.778798,"end_time":"2020-12-14T10:51:24.305564","exception":false,"start_time":"2020-12-14T10:51:23.526766","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data['Sex'].replace(['male','female'],[0,1],inplace=True) # Se sustituyen las categoias de la variable por 0 y 1\ntrain_data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n# male: 0, famela: 1\nsns.heatmap(train_data[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\",\"Fare\",\"Embarked\", \"Survived\"]].corr(), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.231962,"end_time":"2020-12-14T10:51:24.773554","exception":false,"start_time":"2020-12-14T10:51:24.541592","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Podemos ver que las variables `Sex`, `Pclass`, `Fare`y `Embarked` están relacionadas con la variable clase `Survived`.\n\nPara saber si están relacionandas nos fijamos en que el valor que pueden tomar está en el intervalo [-1, +1]. Y sabemos que cuanto más cerca del 0 esté, significa que no hay relación entre las dos variables, mientras que cuanto más cerca esté del -1 o +1 significa que hay relación entre esas dos variables.\n* Vemos que la relación entre `Sex` y `Survived` es de 0.54, que está más próximo de 1 que de 0.\n* La vairable `Pclass` tiene una relación de -0.34 con `Survived`, que aunque esté más cerca del 0 que del -1, vemos que tiene algo de relación.\n* Con `Fare` y `Embarked` pasa lo mismo, tienen una relación con `Survived`del 0.26 y 0.11, y aunque no haya tanta relacion como con `Sex`, si quetienen relación con la variable clase.\n* Por el contrario las variables`Age`, `SibSp` y `Parch` no tienen casi relación con la variable clase pues su valor de la matriz de correlación es menor de 0.1."},{"metadata":{"papermill":{"duration":0.23369,"end_time":"2020-12-14T10:51:25.241661","exception":false,"start_time":"2020-12-14T10:51:25.007971","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 3. Valores perdidos\n"},{"metadata":{"papermill":{"duration":0.233958,"end_time":"2020-12-14T10:51:25.707957","exception":false,"start_time":"2020-12-14T10:51:25.473999","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vimos al principio con el `data_train.info()` que había valores perdidos en los datos. En este apartado se van a buscar y a tratar estos valores. En la libreta original encuentra muchos más valores perdidos que nosotros ya que utiliza el dataset completo (train y test) para el análisis exploratorio. Nosotros corregiremos esto."},{"metadata":{"papermill":{"duration":0.232603,"end_time":"2020-12-14T10:51:26.173498","exception":false,"start_time":"2020-12-14T10:51:25.940895","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 3.1. Encontrar valores perdidos"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:26.644062Z","iopub.status.busy":"2020-12-14T10:51:26.642987Z","iopub.status.idle":"2020-12-14T10:51:26.648409Z","shell.execute_reply":"2020-12-14T10:51:26.647622Z"},"papermill":{"duration":0.244353,"end_time":"2020-12-14T10:51:26.648574","exception":false,"start_time":"2020-12-14T10:51:26.404221","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data.columns[train_data.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.232974,"end_time":"2020-12-14T10:51:27.11426","exception":false,"start_time":"2020-12-14T10:51:26.881286","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vemos que las variables `Age`, `Cabin` y `Embarked` presentan algún valor perdido. Vamos a comprobar cuantos valores perdidos tienen cada variable.\n\nCorrelation Matrix\n* Pclass is associated with Fare.\n* Embarked is not associated with any feature.\n* Pclass and SibSp are associated with Age."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:27.600201Z","iopub.status.busy":"2020-12-14T10:51:27.599012Z","iopub.status.idle":"2020-12-14T10:51:27.604168Z","shell.execute_reply":"2020-12-14T10:51:27.603558Z"},"papermill":{"duration":0.250597,"end_time":"2020-12-14T10:51:27.6043","exception":false,"start_time":"2020-12-14T10:51:27.353703","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.239261,"end_time":"2020-12-14T10:51:28.07589","exception":false,"start_time":"2020-12-14T10:51:27.836629","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 3.2. Rellenar los valores perdidos\n\n* Age tiene 177 valores perdidos\n* Cabin tiene 687 valores perdidos\n* Embarked tiene 2 valores perdidos"},{"metadata":{"papermill":{"duration":0.233073,"end_time":"2020-12-14T10:51:28.540503","exception":false,"start_time":"2020-12-14T10:51:28.30743","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Age"},{"metadata":{"papermill":{"duration":0.234398,"end_time":"2020-12-14T10:51:29.006377","exception":false,"start_time":"2020-12-14T10:51:28.771979","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Como sabemos de la matriz de correlación `Age` tiene relación con las variables `Pclass`(0.37) y `SibSp` (0.31), por lo que para asignarle un valor a los casos perdidos de `Age`, se usará la información de la variable que más este relacionada con ella, que en este caso es `Pclass`.\n\nVisualizamos las instancias en las que `Age` está a null."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:29.528206Z","iopub.status.busy":"2020-12-14T10:51:29.526938Z","iopub.status.idle":"2020-12-14T10:51:29.534272Z","shell.execute_reply":"2020-12-14T10:51:29.533451Z"},"papermill":{"duration":0.295389,"end_time":"2020-12-14T10:51:29.534407","exception":false,"start_time":"2020-12-14T10:51:29.239018","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data[train_data[\"Age\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.231673,"end_time":"2020-12-14T10:51:30.022243","exception":false,"start_time":"2020-12-14T10:51:29.79057","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Se utilizará la mediana de los valores, que tengan el mismo valor en la variable `Pclass`. Es decir, para cada registro con `Age` on valor nulo, se escogerán todos los casos en los que la variable `Pclass` valga lo mismo que el de ese registro, y se calculará la mediana del valor de `Age` en todos esos casos y se le asignará ese valor al registro con `Age` nulo."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:30.538283Z","iopub.status.busy":"2020-12-14T10:51:30.536962Z","iopub.status.idle":"2020-12-14T10:51:30.688705Z","shell.execute_reply":"2020-12-14T10:51:30.687992Z"},"papermill":{"duration":0.432623,"end_time":"2020-12-14T10:51:30.688833","exception":false,"start_time":"2020-12-14T10:51:30.25621","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data_age_nan_index = train_data[train_data[\"Age\"].isnull()].index\nfor i in data_age_nan_index:\n    mean_age = train_data[\"Age\"][(train_data[\"Pclass\"]==train_data.iloc[i][\"Pclass\"])].median()\n    train_data[\"Age\"].iloc[i] = mean_age","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.234509,"end_time":"2020-12-14T10:51:31.157524","exception":false,"start_time":"2020-12-14T10:51:30.923015","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Comprobamos que en los datos de entrenamiento ya no hay instancias con `Age` a null."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:31.63322Z","iopub.status.busy":"2020-12-14T10:51:31.632312Z","iopub.status.idle":"2020-12-14T10:51:31.636443Z","shell.execute_reply":"2020-12-14T10:51:31.636977Z"},"papermill":{"duration":0.248005,"end_time":"2020-12-14T10:51:31.63718","exception":false,"start_time":"2020-12-14T10:51:31.389175","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data[train_data[\"Age\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.233027,"end_time":"2020-12-14T10:51:32.105936","exception":false,"start_time":"2020-12-14T10:51:31.872909","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Cabin\n\nLa mayoría de los valores de esta variable son nulos, por lo que se eliminará esta variable por completo, en lugar de tratar sus valores."},{"metadata":{"papermill":{"duration":0.231503,"end_time":"2020-12-14T10:51:32.5729","exception":false,"start_time":"2020-12-14T10:51:32.341397","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Embarked\n\nDe la matriz de correlación anterior vemos que `Embarked` no está relacionado con ninguna variable (excepto la variable clase). Y también tiene un pocoo de relación con `Sex`, aunque tiene poca (0.12), y ya que solo hay dos valores perdidos, para rellenarlos se les asignará el valor mayoritario, que como en el gráfico del análisis univariado era 1 (C).\n\nVisualizamos las instancias en las que `Embarked` está a null."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:33.061883Z","iopub.status.busy":"2020-12-14T10:51:33.060917Z","iopub.status.idle":"2020-12-14T10:51:33.065812Z","shell.execute_reply":"2020-12-14T10:51:33.065144Z"},"papermill":{"duration":0.258193,"end_time":"2020-12-14T10:51:33.065941","exception":false,"start_time":"2020-12-14T10:51:32.807748","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data[train_data[\"Embarked\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.23359,"end_time":"2020-12-14T10:51:33.540795","exception":false,"start_time":"2020-12-14T10:51:33.307205","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Le asignamos un 1 a las instancias correspondientes."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:34.017713Z","iopub.status.busy":"2020-12-14T10:51:34.016834Z","iopub.status.idle":"2020-12-14T10:51:34.020335Z","shell.execute_reply":"2020-12-14T10:51:34.019551Z"},"papermill":{"duration":0.245543,"end_time":"2020-12-14T10:51:34.020484","exception":false,"start_time":"2020-12-14T10:51:33.774941","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(1)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.242313,"end_time":"2020-12-14T10:51:34.499463","exception":false,"start_time":"2020-12-14T10:51:34.25715","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Comprobamos que en los datos de entrenamiento ya no hay instancias con `Embarked` a null."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:34.981671Z","iopub.status.busy":"2020-12-14T10:51:34.98054Z","iopub.status.idle":"2020-12-14T10:51:34.986067Z","shell.execute_reply":"2020-12-14T10:51:34.985457Z"},"papermill":{"duration":0.253266,"end_time":"2020-12-14T10:51:34.986199","exception":false,"start_time":"2020-12-14T10:51:34.732933","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data[train_data[\"Embarked\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.233854,"end_time":"2020-12-14T10:51:35.455429","exception":false,"start_time":"2020-12-14T10:51:35.221575","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='8'></a><br>\n# 4. Ingeniería de datos.\n"},{"metadata":{"papermill":{"duration":0.233646,"end_time":"2020-12-14T10:51:35.923143","exception":false,"start_time":"2020-12-14T10:51:35.689497","status":"completed"},"tags":[]},"cell_type":"markdown","source":"En este apartado el autor realiza cambios en los datos del dataset, como la creación de nuevos atributos con la combinación de otros ya existentes para incrementar la información de clasificación. La eliminación de valores dentro de un atributo para generalizar. Y la eliminación de atributos que no aportan más información al clasificador (por ejemplo, `Cabin` que casi todos sus valores valen null)."},{"metadata":{"papermill":{"duration":0.235201,"end_time":"2020-12-14T10:51:36.394171","exception":false,"start_time":"2020-12-14T10:51:36.15897","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='12'></a><br>\n## 4.1. Nuevos atributos."},{"metadata":{"papermill":{"duration":0.233837,"end_time":"2020-12-14T10:51:36.861069","exception":false,"start_time":"2020-12-14T10:51:36.627232","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Crearemos nuevas columnas para la base de datos con la informacion de otras. Aquí se ha realizado fuga de datos, nosotros en su lugar utilizaremos un pipeline."},{"metadata":{"papermill":{"duration":0.234117,"end_time":"2020-12-14T10:51:37.331233","exception":false,"start_time":"2020-12-14T10:51:37.097116","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Vamos a crear a su vez una dataframe por separado para hacer las mismas transformaciones fuera del pipeline, para ver las gráficas de como quedaria nuestra base de datos y al final sacar una matriz de correlaciones, para ver si se puede eliminar columnas que aportan la misma información que otras al haber creado nuevas columnas. Y así seguimos la misma estructura que la libreta original."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:37.805363Z","iopub.status.busy":"2020-12-14T10:51:37.804268Z","iopub.status.idle":"2020-12-14T10:51:37.807821Z","shell.execute_reply":"2020-12-14T10:51:37.807024Z"},"papermill":{"duration":0.242732,"end_time":"2020-12-14T10:51:37.807955","exception":false,"start_time":"2020-12-14T10:51:37.565223","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_clone = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.233882,"end_time":"2020-12-14T10:51:38.27751","exception":false,"start_time":"2020-12-14T10:51:38.043628","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Alone and Family \n* SibSp + Parch = family"},{"metadata":{"papermill":{"duration":0.233388,"end_time":"2020-12-14T10:51:38.74415","exception":false,"start_time":"2020-12-14T10:51:38.510762","status":"completed"},"tags":[]},"cell_type":"markdown","source":"A partir del número de hermanos y número de padres, se puede crear una variable del número de miembros de la familia. "},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:39.252919Z","iopub.status.busy":"2020-12-14T10:51:39.251863Z","iopub.status.idle":"2020-12-14T10:51:39.257408Z","shell.execute_reply":"2020-12-14T10:51:39.256784Z"},"papermill":{"duration":0.275485,"end_time":"2020-12-14T10:51:39.257538","exception":false,"start_time":"2020-12-14T10:51:38.982053","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creamos una función que realizará esto para incorporarla en el pipeline."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:39.731705Z","iopub.status.busy":"2020-12-14T10:51:39.730597Z","iopub.status.idle":"2020-12-14T10:51:39.733606Z","shell.execute_reply":"2020-12-14T10:51:39.734149Z"},"papermill":{"duration":0.241885,"end_time":"2020-12-14T10:51:39.734309","exception":false,"start_time":"2020-12-14T10:51:39.492424","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def suma_columnas(data, column1, column2, result):\n    data[result] = data[column1] + data[column2]\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"También lo haremos sobre los datos copiados para seguir la estructura, aunque no lo haremos sobre los datos de test para no comenter la fuga de datos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:40.217981Z","iopub.status.busy":"2020-12-14T10:51:40.214599Z","iopub.status.idle":"2020-12-14T10:51:40.845466Z","shell.execute_reply":"2020-12-14T10:51:40.844653Z"},"papermill":{"duration":0.873434,"end_time":"2020-12-14T10:51:40.845597","exception":false,"start_time":"2020-12-14T10:51:39.972163","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_clone[\"Family\"] = train_clone[\"SibSp\"] + train_clone[\"Parch\"]\n#test_data[\"Family\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]\nsns.factorplot(x=\"Family\", y =\"Survived\", data=train_clone, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.24294,"end_time":"2020-12-14T10:51:41.384387","exception":false,"start_time":"2020-12-14T10:51:41.141447","status":"completed"},"tags":[]},"cell_type":"markdown","source":"También podemos crear una variable de si esa persona viajaba sola. Si el valor de familia es 0 viajaba solo y se le pondrá un valor de 1. Si estaba acompañado un valor de 0."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:41.877722Z","iopub.status.busy":"2020-12-14T10:51:41.876456Z","iopub.status.idle":"2020-12-14T10:51:41.880326Z","shell.execute_reply":"2020-12-14T10:51:41.879698Z"},"papermill":{"duration":0.253114,"end_time":"2020-12-14T10:51:41.880464","exception":false,"start_time":"2020-12-14T10:51:41.62735","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def alone_column(data, nombre, family):\n    data[nombre] = [1 if i == 0 else 0 for i in data[family]]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:42.454637Z","iopub.status.busy":"2020-12-14T10:51:42.453475Z","iopub.status.idle":"2020-12-14T10:51:42.457943Z","shell.execute_reply":"2020-12-14T10:51:42.458848Z"},"papermill":{"duration":0.290464,"end_time":"2020-12-14T10:51:42.459096","exception":false,"start_time":"2020-12-14T10:51:42.168632","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_clone[\"Alone\"] = [1 if i == 0 else 0 for i in train_clone[\"Family\"]]\n#test_data[\"Alone\"] = [1 if i == 0 else 0 for i in test_data[\"Family\"]]\n#train_data[\"Family\"].replace([0,1,2,3,4,5,6,7,10], [0,1,1,1,0,2,0,2,2], inplace=True)\ntrain_clone.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.243649,"end_time":"2020-12-14T10:51:42.944828","exception":false,"start_time":"2020-12-14T10:51:42.701179","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Name - Title"},{"metadata":{"papermill":{"duration":0.241967,"end_time":"2020-12-14T10:51:43.426509","exception":false,"start_time":"2020-12-14T10:51:43.184542","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Hay diferentes categorías de los titulos con los que se llama a una persona. Podemos reducir los titulos que tenga menos instancias en la base de datos los podemos denominar como \"Other\"."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:43.909846Z","iopub.status.busy":"2020-12-14T10:51:43.908567Z","iopub.status.idle":"2020-12-14T10:51:43.913589Z","shell.execute_reply":"2020-12-14T10:51:43.912932Z"},"papermill":{"duration":0.2522,"end_time":"2020-12-14T10:51:43.913718","exception":false,"start_time":"2020-12-14T10:51:43.661518","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def name_title(data, nombre):\n    data[nombre] = data.Name.str.extract('([A-Za-z]+)\\.')\n    data[nombre].replace(['Mme','Ms','Mlle','Lady','Countess','Dona','Dr','Major','Sir','Capt','Don','Rev','Col', 'Jonkheer'],['Miss','Miss','Miss','Mrs','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Other','Other','Other'], inplace=True)\n    # Se convierten los titulos en identificadores numericos\n    data[nombre].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\"], [1,2,2,3,1], inplace=True)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:44.403806Z","iopub.status.busy":"2020-12-14T10:51:44.402857Z","iopub.status.idle":"2020-12-14T10:51:44.412496Z","shell.execute_reply":"2020-12-14T10:51:44.411732Z"},"papermill":{"duration":0.261726,"end_time":"2020-12-14T10:51:44.412633","exception":false,"start_time":"2020-12-14T10:51:44.150907","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_clone['Title']=train_clone.Name.str.extract('([A-Za-z]+)\\.')\n#test_data['Title']=test_data.Name.str.extract('([A-Za-z]+)\\.')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:44.895528Z","iopub.status.busy":"2020-12-14T10:51:44.894439Z","iopub.status.idle":"2020-12-14T10:51:44.897403Z","shell.execute_reply":"2020-12-14T10:51:44.897939Z"},"papermill":{"duration":0.246127,"end_time":"2020-12-14T10:51:44.898118","exception":false,"start_time":"2020-12-14T10:51:44.651991","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#sns.countplot(train_data[\"Title\"])\n#plt.xticks(rotation = 90)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:45.383939Z","iopub.status.busy":"2020-12-14T10:51:45.382837Z","iopub.status.idle":"2020-12-14T10:51:45.390518Z","shell.execute_reply":"2020-12-14T10:51:45.389767Z"},"papermill":{"duration":0.254398,"end_time":"2020-12-14T10:51:45.390679","exception":false,"start_time":"2020-12-14T10:51:45.136281","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_clone['Title'].replace(['Mme','Ms','Mlle','Lady','Countess','Dona','Dr','Major','Sir','Capt','Don','Rev','Col', 'Jonkheer'],['Miss','Miss','Miss','Mrs','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Other','Other','Other'], inplace=True)\n#test_data['Title'].replace(['Mme','Ms','Mlle','Lady','Countess','Dona','Dr','Major','Sir','Capt','Don','Rev','Col', 'Jonkheer'],['Miss','Miss','Miss','Mrs','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Other','Other','Other'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:45.871669Z","iopub.status.busy":"2020-12-14T10:51:45.870805Z","iopub.status.idle":"2020-12-14T10:51:46.027167Z","shell.execute_reply":"2020-12-14T10:51:46.026352Z"},"papermill":{"duration":0.399685,"end_time":"2020-12-14T10:51:46.027302","exception":false,"start_time":"2020-12-14T10:51:45.627617","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.countplot(train_clone[\"Title\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:46.531958Z","iopub.status.busy":"2020-12-14T10:51:46.523504Z","iopub.status.idle":"2020-12-14T10:51:46.933915Z","shell.execute_reply":"2020-12-14T10:51:46.933231Z"},"papermill":{"duration":0.654694,"end_time":"2020-12-14T10:51:46.934087","exception":false,"start_time":"2020-12-14T10:51:46.279393","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"Title\", y =\"Survived\", data=train_clone, kind=\"bar\", size=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.236896,"end_time":"2020-12-14T10:51:47.414386","exception":false,"start_time":"2020-12-14T10:51:47.17749","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Convertimos los titulos en identificadores numéricos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:47.896084Z","iopub.status.busy":"2020-12-14T10:51:47.895056Z","iopub.status.idle":"2020-12-14T10:51:47.900348Z","shell.execute_reply":"2020-12-14T10:51:47.899604Z"},"papermill":{"duration":0.25011,"end_time":"2020-12-14T10:51:47.900474","exception":false,"start_time":"2020-12-14T10:51:47.650364","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_clone[\"Title\"].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\"], [1,2,2,3,1], inplace=True)\n#test_data[\"Title\"].replace([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\"], [1,2,2,3,1], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.240263,"end_time":"2020-12-14T10:51:48.37962","exception":false,"start_time":"2020-12-14T10:51:48.139357","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Age Limit"},{"metadata":{"papermill":{"duration":0.236665,"end_time":"2020-12-14T10:51:48.852841","exception":false,"start_time":"2020-12-14T10:51:48.616176","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para estos dos nuevos atributos se utiliza la variable clase para agrupar los de un mismo rango de edad. Como la variable clase no la obtenemos en el dataset del test, todos los pasos siguientes se comentarán. Esta variable además es redundante con el atributo edad. Así que no lo pasaremos al pipeline."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:49.336864Z","iopub.status.busy":"2020-12-14T10:51:49.335965Z","iopub.status.idle":"2020-12-14T10:51:49.340631Z","shell.execute_reply":"2020-12-14T10:51:49.340015Z"},"papermill":{"duration":0.248107,"end_time":"2020-12-14T10:51:49.340773","exception":false,"start_time":"2020-12-14T10:51:49.092666","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\"\"\"\ndef age_limit(data):\n    data['Age_Limit'] = pd.cut(data['Age'], 5)\n    data.groupby(['Age_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')\n    return data\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:49.823855Z","iopub.status.busy":"2020-12-14T10:51:49.82299Z","iopub.status.idle":"2020-12-14T10:51:49.826141Z","shell.execute_reply":"2020-12-14T10:51:49.825471Z"},"papermill":{"duration":0.246345,"end_time":"2020-12-14T10:51:49.826306","exception":false,"start_time":"2020-12-14T10:51:49.579961","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train_data['Age_Limit']=pd.cut(train_data['Age'], 5)\n#test_data['Age_Limit']=pd.cut(test_data['Age'], 5)\n\n#train_data.groupby(['Age_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')\n#test_data.groupby(['Age_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:50.312894Z","iopub.status.busy":"2020-12-14T10:51:50.312086Z","iopub.status.idle":"2020-12-14T10:51:50.315517Z","shell.execute_reply":"2020-12-14T10:51:50.314749Z"},"papermill":{"duration":0.248034,"end_time":"2020-12-14T10:51:50.315649","exception":false,"start_time":"2020-12-14T10:51:50.067615","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train_data['Age_Limit'] = LabelEncoder().fit_transform(train_data['Age_Limit'])\n#test_data['Age_Limit'] = LabelEncoder().fit_transform(test_data['Age_Limit'])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.239793,"end_time":"2020-12-14T10:51:50.794052","exception":false,"start_time":"2020-12-14T10:51:50.554259","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Fare Limit"},{"metadata":{},"cell_type":"markdown","source":"Al igual que on `Age limit` para generar esta variable utiliza los datos de test así que comentaremos estos pasos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:51.808996Z","iopub.status.busy":"2020-12-14T10:51:51.807984Z","iopub.status.idle":"2020-12-14T10:51:52.075453Z","shell.execute_reply":"2020-12-14T10:51:52.074783Z"},"papermill":{"duration":0.514004,"end_time":"2020-12-14T10:51:52.075596","exception":false,"start_time":"2020-12-14T10:51:51.561592","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train_clone['Fare_Limit']=pd.qcut(train_clone['Fare'],4)\n#train_clone.groupby(['Fare_Limit'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:52.570383Z","iopub.status.busy":"2020-12-14T10:51:52.569438Z","iopub.status.idle":"2020-12-14T10:51:52.57271Z","shell.execute_reply":"2020-12-14T10:51:52.571969Z"},"papermill":{"duration":0.256519,"end_time":"2020-12-14T10:51:52.572833","exception":false,"start_time":"2020-12-14T10:51:52.316314","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train_clone['Fare_Limit'] = LabelEncoder().fit_transform(train_clone['Fare_Limit'])\n#test_data['Age_Limit'] = LabelEncoder().fit_transform(test_data['Age_Limit'])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:53.072844Z","iopub.status.busy":"2020-12-14T10:51:53.060327Z","iopub.status.idle":"2020-12-14T10:51:53.429638Z","shell.execute_reply":"2020-12-14T10:51:53.428864Z"},"papermill":{"duration":0.618072,"end_time":"2020-12-14T10:51:53.429773","exception":false,"start_time":"2020-12-14T10:51:52.811701","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#sns.factorplot(x=\"Fare_Limit\", y =\"Survived\", data=train_clone, kind=\"bar\", size=3)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.240273,"end_time":"2020-12-14T10:51:53.91127","exception":false,"start_time":"2020-12-14T10:51:53.670997","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"papermill":{"duration":0.242229,"end_time":"2020-12-14T10:51:54.395304","exception":false,"start_time":"2020-12-14T10:51:54.153075","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Con las nuevas variables, la matriz de correlaciones quedan de esta manera:"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:54.892419Z","iopub.status.busy":"2020-12-14T10:51:54.886756Z","iopub.status.idle":"2020-12-14T10:51:55.343331Z","shell.execute_reply":"2020-12-14T10:51:55.343958Z"},"papermill":{"duration":0.709238,"end_time":"2020-12-14T10:51:55.34415","exception":false,"start_time":"2020-12-14T10:51:54.634912","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.heatmap(train_clone[[\"Ticket\",\"Cabin\",\"Pclass\",\"Embarked\",\"Sex\",\"Age\", \"Title\",\"Family\", \"Survived\"]].corr(), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.23986,"end_time":"2020-12-14T10:51:55.824154","exception":false,"start_time":"2020-12-14T10:51:55.584294","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='13'></a><br>\n## 4.2. Eliminación de variables\n* Ticket, Cabin, Name y PassengerId son eliminadas por la poca correlación que hemos visto con otras variables.\n* Además eliminaremos Cabin porque casi todos sus valores son desconocidos (valen null).\n* El resto de variables que hemos utilizado para crear nuevas variables tambien las eliminaremos.\n\nLas eliminaremos en el pipeline."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:56.323364Z","iopub.status.busy":"2020-12-14T10:51:56.32255Z","iopub.status.idle":"2020-12-14T10:51:56.325139Z","shell.execute_reply":"2020-12-14T10:51:56.325658Z"},"papermill":{"duration":0.252888,"end_time":"2020-12-14T10:51:56.325816","exception":false,"start_time":"2020-12-14T10:51:56.072928","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data['Age']=train_data['Age'].astype(int)\n\ncolumns_drop = [\"SibSp\",\"Parch\",\"Cabin\",\"Fare\", \"Ticket\", \"Name\", \"PassengerId\"]\n\n#train_data.drop(labels=columns_drop, axis=1, inplace = True)\n#test_data.drop(labels=columns_drop, axis=1, inplace = True)\n\n#train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.240705,"end_time":"2020-12-14T10:51:56.810068","exception":false,"start_time":"2020-12-14T10:51:56.569363","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Nuestra base de datos por tanto queda muy reducida y con gran valor informativo para la clasificación."},{"metadata":{"papermill":{"duration":0.243073,"end_time":"2020-12-14T10:51:57.29699","exception":false,"start_time":"2020-12-14T10:51:57.053917","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='14'></a><br>\n## 4.3. One Hot Encoding"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:57.787164Z","iopub.status.busy":"2020-12-14T10:51:57.786361Z","iopub.status.idle":"2020-12-14T10:51:57.789602Z","shell.execute_reply":"2020-12-14T10:51:57.788891Z"},"papermill":{"duration":0.25254,"end_time":"2020-12-14T10:51:57.789725","exception":false,"start_time":"2020-12-14T10:51:57.537185","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train_data = pd.get_dummies(data,columns=[\"Pclass\"])\n#train_data = pd.get_dummies(data,columns=[\"Embarked\"])\n#train_data = pd.get_dummies(data,columns=[\"Family\"])\n#train_data = pd.get_dummies(data,columns=[\"Title\"])\n\n#train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.244368,"end_time":"2020-12-14T10:51:58.278248","exception":false,"start_time":"2020-12-14T10:51:58.03388","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='16'></a><br>\n# 5. Modelos"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:51:58.766987Z","iopub.status.busy":"2020-12-14T10:51:58.765984Z","iopub.status.idle":"2020-12-14T10:51:58.769419Z","shell.execute_reply":"2020-12-14T10:51:58.76857Z"},"papermill":{"duration":0.250349,"end_time":"2020-12-14T10:51:58.769549","exception":false,"start_time":"2020-12-14T10:51:58.5192","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.301681,"end_time":"2020-12-14T10:51:59.321821","exception":false,"start_time":"2020-12-14T10:51:59.02014","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='17'></a><br>\n## 5.1 Train Test Split"},{"metadata":{"papermill":{"duration":0.240557,"end_time":"2020-12-14T10:51:59.806005","exception":false,"start_time":"2020-12-14T10:51:59.565448","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Esta parte la vamos a comentar entera. Como el test y el train están en diferentes dataset cada uno cargados en una odentificador independiente no es necesario realizar un *houldout*. Al estar a divididos no tiene sentido unirlos y luego separarlos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:00.316159Z","iopub.status.busy":"2020-12-14T10:52:00.315159Z","iopub.status.idle":"2020-12-14T10:52:00.318713Z","shell.execute_reply":"2020-12-14T10:52:00.31789Z"},"papermill":{"duration":0.254252,"end_time":"2020-12-14T10:52:00.318879","exception":false,"start_time":"2020-12-14T10:52:00.064627","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#if len(data) == (len(train_data) + len(test_data)):\n#    print(\"success\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:00.811019Z","iopub.status.busy":"2020-12-14T10:52:00.809794Z","iopub.status.idle":"2020-12-14T10:52:00.813692Z","shell.execute_reply":"2020-12-14T10:52:00.812936Z"},"papermill":{"duration":0.250603,"end_time":"2020-12-14T10:52:00.813826","exception":false,"start_time":"2020-12-14T10:52:00.563223","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#test = data[len(train_data):]\n#test.drop(labels=\"Survived\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:01.361361Z","iopub.status.busy":"2020-12-14T10:52:01.360197Z","iopub.status.idle":"2020-12-14T10:52:01.363528Z","shell.execute_reply":"2020-12-14T10:52:01.3629Z"},"papermill":{"duration":0.252743,"end_time":"2020-12-14T10:52:01.36365","exception":false,"start_time":"2020-12-14T10:52:01.110907","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#train = data[:len(train_data)]\n#X_train = train.drop(labels = \"Survived\", axis=1)\n#y_train = train[\"Survived\"]\n#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.242342,"end_time":"2020-12-14T10:52:01.847476","exception":false,"start_time":"2020-12-14T10:52:01.605134","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='18'></a><br>\n## 5.2. Metodos de clasificación.\n\n\n* Logistic Regression\n* Random Forest Regression\n* Support Vector Machine (SVM)\n* K-Nearest Neighbors (KNN)"},{"metadata":{"papermill":{"duration":0.243168,"end_time":"2020-12-14T10:52:02.341889","exception":false,"start_time":"2020-12-14T10:52:02.098721","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Estos serán los modelos que se estudian en el notebook original. Primero dividimos los datos test y train en sus diferentes variables. Para tener la variable clase dividida del resto. "},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:02.836365Z","iopub.status.busy":"2020-12-14T10:52:02.835538Z","iopub.status.idle":"2020-12-14T10:52:02.838925Z","shell.execute_reply":"2020-12-14T10:52:02.838194Z"},"papermill":{"duration":0.256083,"end_time":"2020-12-14T10:52:02.839069","exception":false,"start_time":"2020-12-14T10:52:02.582986","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"target = \"Survived\"\n(X_train,y_train) = (train_data.drop(target, axis=1), train_data[target])\n\n#(X_test,y_test) = (test_data.drop(target, axis=1), test_data[target])\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:03.864436Z","iopub.status.busy":"2020-12-14T10:52:03.863317Z","iopub.status.idle":"2020-12-14T10:52:03.866928Z","shell.execute_reply":"2020-12-14T10:52:03.866341Z"},"papermill":{"duration":0.253266,"end_time":"2020-12-14T10:52:03.867077","exception":false,"start_time":"2020-12-14T10:52:03.613811","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import make_column_transformer\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\n\nsum_col = FunctionTransformer(suma_columnas, kw_args={'column1':\"SibSp\", 'column2':'Parch', 'result':'Family'})\nalone_col = FunctionTransformer(alone_column, kw_args={'nombre':\"Alone\", 'family':'Family'})\nname = FunctionTransformer(name_title, kw_args={'nombre':\"Title\"})\ndrop_col = make_column_transformer((\"drop\", columns_drop), remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.244224,"end_time":"2020-12-14T10:52:04.370598","exception":false,"start_time":"2020-12-14T10:52:04.126374","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 5.2.1. Regresión logística"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:04.888171Z","iopub.status.busy":"2020-12-14T10:52:04.887413Z","iopub.status.idle":"2020-12-14T10:52:04.94807Z","shell.execute_reply":"2020-12-14T10:52:04.946846Z"},"papermill":{"duration":0.328449,"end_time":"2020-12-14T10:52:04.948231","exception":false,"start_time":"2020-12-14T10:52:04.619782","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(random_state=42)\n\np = make_pipeline(sum_col, alone_col, name, drop_col, log_reg)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.243693,"end_time":"2020-12-14T10:52:05.441541","exception":false,"start_time":"2020-12-14T10:52:05.197848","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para regresión logística obtenemos la mayor tasa de acierto, al ser un modelo simple lo consideraremos por encima del resto."},{"metadata":{"papermill":{"duration":0.241137,"end_time":"2020-12-14T10:52:05.92321","exception":false,"start_time":"2020-12-14T10:52:05.682073","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 5.2.2. Random Forest"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:06.437102Z","iopub.status.busy":"2020-12-14T10:52:06.432119Z","iopub.status.idle":"2020-12-14T10:52:06.709702Z","shell.execute_reply":"2020-12-14T10:52:06.70889Z"},"papermill":{"duration":0.532669,"end_time":"2020-12-14T10:52:06.709835","exception":false,"start_time":"2020-12-14T10:52:06.177166","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"rf_reg = RandomForestClassifier(random_state=42)\n#rf_reg.fit(X_train, y_train)\n#print(\"Accuracy: \", rf_reg.score(X_test,y_test))\n\np = make_pipeline(sum_col, alone_col, name, drop_col, rf_reg)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.246466,"end_time":"2020-12-14T10:52:07.200363","exception":false,"start_time":"2020-12-14T10:52:06.953897","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Random forest, consigue menor *score* que el anterior aún siendo un modelo más complejo. Por tanto, lo descartaremos por modelos simples."},{"metadata":{"papermill":{"duration":0.240476,"end_time":"2020-12-14T10:52:07.682436","exception":false,"start_time":"2020-12-14T10:52:07.44196","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 5.2.3. Support Vector Machine (SVM)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:08.202277Z","iopub.status.busy":"2020-12-14T10:52:08.201141Z","iopub.status.idle":"2020-12-14T10:52:08.243166Z","shell.execute_reply":"2020-12-14T10:52:08.241948Z"},"papermill":{"duration":0.318611,"end_time":"2020-12-14T10:52:08.243331","exception":false,"start_time":"2020-12-14T10:52:07.92472","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"svm_clsf = SVC()\n#svm_clsf.fit(X_train, y_train)\n#print(\"Accuracy: \", svm_clsf.score(X_test,y_test))\n\np = make_pipeline(sum_col, alone_col, name, drop_col, svm_clsf)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.243946,"end_time":"2020-12-14T10:52:08.728495","exception":false,"start_time":"2020-12-14T10:52:08.484549","status":"completed"},"tags":[]},"cell_type":"markdown","source":"La tasa de acierto en este caso es muy baja, respecto a los anteriores. Este modelo no servirá."},{"metadata":{"papermill":{"duration":0.247803,"end_time":"2020-12-14T10:52:09.220898","exception":false,"start_time":"2020-12-14T10:52:08.973095","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 5.2.4. KNN\n"},{"metadata":{"papermill":{"duration":0.242138,"end_time":"2020-12-14T10:52:09.712592","exception":false,"start_time":"2020-12-14T10:52:09.470454","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Para knn comprobamos del 1 al 12 en número de vecinos."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:10.231111Z","iopub.status.busy":"2020-12-14T10:52:10.224924Z","iopub.status.idle":"2020-12-14T10:52:10.658367Z","shell.execute_reply":"2020-12-14T10:52:10.65756Z"},"papermill":{"duration":0.699585,"end_time":"2020-12-14T10:52:10.658502","exception":false,"start_time":"2020-12-14T10:52:09.958917","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"best_knn = []\nfor n in range(1,12):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    #knn.fit(X_train, y_train)\n    #best_knn.insert(n, knn.score(X_test,y_test))\n    p = make_pipeline(sum_col, alone_col, name, drop_col, knn)\n    p.fit(X_train, y_train)\n    best_knn.insert(n, p.score(X_test,y_test))\nbest_knn\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:11.170696Z","iopub.status.busy":"2020-12-14T10:52:11.169702Z","iopub.status.idle":"2020-12-14T10:52:11.203665Z","shell.execute_reply":"2020-12-14T10:52:11.202868Z"},"papermill":{"duration":0.299065,"end_time":"2020-12-14T10:52:11.203798","exception":false,"start_time":"2020-12-14T10:52:10.904733","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"knn_clsf = KNeighborsClassifier(n_neighbors=5)\np = make_pipeline(sum_col, alone_col, name, drop_col, knn_clsf)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.250204,"end_time":"2020-12-14T10:52:11.70564","exception":false,"start_time":"2020-12-14T10:52:11.455436","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Con el número de vecinos de 5 obtenemos la mayor tasa de acierto. Esta no es nada despreciable y queda muy cerca de regresión logística."},{"metadata":{"papermill":{"duration":0.247075,"end_time":"2020-12-14T10:52:12.201498","exception":false,"start_time":"2020-12-14T10:52:11.954423","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='19'></a><br>\n## 5.3. Modelos \"Ensemble\""},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:12.710349Z","iopub.status.busy":"2020-12-14T10:52:12.708802Z","iopub.status.idle":"2020-12-14T10:52:13.120082Z","shell.execute_reply":"2020-12-14T10:52:13.119322Z"},"papermill":{"duration":0.671614,"end_time":"2020-12-14T10:52:13.120214","exception":false,"start_time":"2020-12-14T10:52:12.4486","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"voting_classfication = VotingClassifier(estimators = [('knn', knn_clsf),('lg', log_reg), ('rfg', rf_reg), ('svc', svm_clsf)], voting=\"hard\", n_jobs=-1)\np = make_pipeline(sum_col, alone_col, name, drop_col, voting_classfication)\np.fit(X_train, y_train)\nprint(\"Accuracy: \", p.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.277842,"end_time":"2020-12-14T10:52:13.643603","exception":false,"start_time":"2020-12-14T10:52:13.365761","status":"completed"},"tags":[]},"cell_type":"markdown","source":"El ensemble de los modelos anteriores por un metodo de voto da peores resultados qque un modelo más simple. Por tanto, para evaluar el modelo vamos a utilizar modelos simples."},{"metadata":{"papermill":{"duration":0.251954,"end_time":"2020-12-14T10:52:14.141202","exception":false,"start_time":"2020-12-14T10:52:13.889248","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id='20'></a><br>\n## 5.4. Resultados\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-14T10:52:14.64527Z","iopub.status.busy":"2020-12-14T10:52:14.644417Z","iopub.status.idle":"2020-12-14T10:52:14.64728Z","shell.execute_reply":"2020-12-14T10:52:14.648024Z"},"papermill":{"duration":0.258009,"end_time":"2020-12-14T10:52:14.648198","exception":false,"start_time":"2020-12-14T10:52:14.390189","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#test_result = pd.Series(voting_classfication.predict(test_data), name = \"Survived\").astype(int)\n#results = pd.concat([test_data[\"PassengerId\"], test_result],axis = 1)\n#results.to_csv(\"titanic_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.275794,"end_time":"2020-12-14T10:52:15.169386","exception":false,"start_time":"2020-12-14T10:52:14.893592","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Viendo los resultados que nos ofrecen los modelos al entrenarlo, no necesitamos más código. Ya hemos visto que el mejor es regresión logística y por debajo queda KNN que también nos aporta buenos resultados.\n\nAdemás destacar que regresión logística ha obtenido muy buenos resultados para ser un modelo tan simple."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}