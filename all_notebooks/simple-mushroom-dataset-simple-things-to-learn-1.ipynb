{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"# Notebook for beginners. Easy ML task as Iris dataset.\n# Used in this notebook: Basic dataframe methods to select data; basic scikit-learn models:\n# logistic regression, gradient boosting, decision tree and Knn;\n# some visualization: matplotlib and graphviz.\n# There are no  NaN (missing) values. \n# Binary Classification.","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nfrom IPython.display import display # display() tool from IPython for dataframe visualization\n#plt.rc('font', family='Verdana') # you can comment this\nfrom sklearn import preprocessing\n# basic import Cell. ","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"data = pd.read_csv(\"../input/mushrooms.csv\")","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"display(data.head()) #Show us 5 first rows\n# or just data.head()\n#As expected (read description), features are indicated by letters.","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"data_encoded = preprocessing.LabelEncoder() #So, lets code them into numeric categories. Used scikit \nfor column in data.columns[1:]: #OneHotEncoder. Another way - pandas.get_dummies. Code \n    data[column] = data_encoded.fit(data[column]).transform(data[column])\n# data is our pd.Dataframe object and  .columns[1:] are our features\n# 1st is column number 0 (class) and it contains our labels\n# I \"separate\" labels and features only for example in this case\n","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"data['class'] = data_encoded.fit(data['class']).transform(data['class']) # Encode 1st column with labels\n#... .fit(...)\n#... .transfrom(..)\n# 2 rows will give same result.\ndisplay(data.head()) #Seems good","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"features = data.iloc[:,1:] #.loc and .iloc for data selecting. Choose all rows and columns from position 2\nX = features.values #  Extract features values\ny = data.iloc[:,0] # Extract labels (column number 0) values\nprint(\"Array form (X): {} Array form (y): {}\".format(X.shape, y.shape)) # Check for mistakes \n#22 features and 1 class. 8124 rows\n# Now split data with the simplest method (train_test_split):\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y) \n# Cells below are representing some models that can be used in binary classification","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"#Playground time!\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=0) \nlogreg.fit(X_train, y_train)\n\nprint(\"Accuracy, training set: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Accuracy, test set: {:.3f}\".format(logreg.score(X_test, y_test)))","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"#Gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy, training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy, test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n#Also we can visualizate feature importances (matplotlib help us)\ndef plot_feature_importances(model):\n    n_features = features.columns.size\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), features.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\nplot_feature_importances(gbrt)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=0) #try max_depth = 1..8, from rough to \n# (very) sensitive model\ntree.fit(X_train, y_train)\nprint(\"Accuracy, training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy, test set: {:.3f}\".format(tree.score(X_test, y_test)))\n# And visualizate how decision tree works","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names = [\"edible\", \"poisonous\"], #edible = 0,               \nfeature_names = features.columns, impurity=False, filled=True) # poisonous = 1, right? \nimport graphviz\nwith open(\"tree.dot\") as file:\n    dot_graph = file.read()\ngraphviz.Source(dot_graph)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# Knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=7) #n_neighbors=1..scary number\nknn.fit(X_train, y_train)\nprint(\"Accuracy, training set: {:.3f}\".format(knn.score(X_train, y_train)))\nprint(\"Accuracy, test set: {:.3f}\".format(knn.score(X_test, y_test)))","outputs":[]}],"nbformat_minor":1}