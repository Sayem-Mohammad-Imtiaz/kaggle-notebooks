{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Feature Engineering + Hyperparameter tuning*","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"****In this notebook, I have shared the techniques like feature engineering, hyperparameter tuning using Optuna.****","metadata":{}},{"cell_type":"markdown","source":"****Step-1 Importing Libraries.****","metadata":{}},{"cell_type":"code","source":"#importing necessary libararies\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',500)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:34:02.11721Z","iopub.execute_input":"2021-08-24T03:34:02.117672Z","iopub.status.idle":"2021-08-24T03:34:03.406964Z","shell.execute_reply.started":"2021-08-24T03:34:02.117571Z","shell.execute_reply":"2021-08-24T03:34:03.406095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Step-2 Importing the datasets.****\n\n****I have used datasets from 30 days of code and from 30days-folds by Abhishek Thakur.****","metadata":{}},{"cell_type":"code","source":"import optuna\ndf = pd.read_csv(\"../input/30days-folds/train_folds.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:34:03.410252Z","iopub.execute_input":"2021-08-24T03:34:03.410523Z","iopub.status.idle":"2021-08-24T03:34:07.37064Z","shell.execute_reply.started":"2021-08-24T03:34:03.410497Z","shell.execute_reply":"2021-08-24T03:34:07.36985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Step-3 Feature Engineering.****\n\nI have tried various techniques like OrdinalEncoder, One Hot Encoding ,Normalisation etc. \n\nThe best came out to be the OneHotEncoder on categorical variables and after that StandardScaler upon the whole dataset after one hot encoding.","metadata":{}},{"cell_type":"code","source":"imp_col = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\ncategorical_col = [col for col in imp_col if 'cat' in col]\nnumerical_col = [col for col in imp_col if col.startswith(\"cont\")]\ndf_test = df_test[imp_col]\nfor folds in range(5):\n    X_train=df[df.kfold!=folds].reset_index()\n    X_valid=df[df.kfold==folds].reset_index()\n    y_train=X_train.target\n    y_valid=X_valid.target\n    X_train=X_train[imp_col]\n    X_valid=X_valid[imp_col]\n    X_test=df_test.copy()\n\n    ohe=OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe=ohe.fit_transform(X_train[categorical_col])\n    X_valid_ohe=ohe.transform(X_valid[categorical_col])\n    X_test_ohe=ohe.transform(X_test[categorical_col])\n\n    X_train_ohe=pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\"for i in range(X_train_ohe.shape[1])]) \n    X_valid_ohe=pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\"for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe=pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\"for i in range(X_test_ohe.shape[1])]) \n    X_train=pd.concat([X_train,X_train_ohe],axis=1)\n    X_valid=pd.concat([X_valid,X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test,X_test_ohe], axis=1)\n\n    X_train = X_train.drop(categorical_col, axis=1)\n    X_valid = X_valid.drop(categorical_col, axis=1)\n    X_test = X_test.drop(categorical_col, axis=1)\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    X_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:34:07.372413Z","iopub.execute_input":"2021-08-24T03:34:07.372698Z","iopub.status.idle":"2021-08-24T03:34:23.106853Z","shell.execute_reply.started":"2021-08-24T03:34:07.372669Z","shell.execute_reply":"2021-08-24T03:34:23.105974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Step-4 Hyperparameter Tuning.****\n\nUsing Optuna:\n\nSteps-\n\n* Define objective function to be optimized.Here run.\n* Suggest hyperparameter values using trial object. \n* Create a study object and invoke the optimize method over 100 trials.","metadata":{}},{"cell_type":"code","source":"def run(trial):\n    fold=0\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n   \n    xgb_model = XGBRegressor(random_state=42,\n                tree_method=\"gpu_hist\",\n                gpu_id=1,\n                predictor=\"gpu_predictor\",\n                n_estimators=7000,\n                learning_rate=learning_rate,\n                reg_lambda=reg_lambda,\n                reg_alpha=reg_alpha,\n                subsample=subsample,\n                colsample_bytree=colsample_bytree,\n                max_depth=max_depth,)\n    xgb_model.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_valid, y_valid)], verbose=1000)\n    preds_valid = xgb_model.predict(X_valid) \n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    return rmse  ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:34:23.108388Z","iopub.execute_input":"2021-08-24T03:34:23.1087Z","iopub.status.idle":"2021-08-24T03:34:23.117836Z","shell.execute_reply.started":"2021-08-24T03:34:23.108667Z","shell.execute_reply":"2021-08-24T03:34:23.116868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(run, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:34:23.119194Z","iopub.execute_input":"2021-08-24T03:34:23.119647Z","iopub.status.idle":"2021-08-24T04:04:30.453718Z","shell.execute_reply.started":"2021-08-24T03:34:23.119609Z","shell.execute_reply":"2021-08-24T04:04:30.452904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***To check the Best parameters.***","metadata":{}},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2021-08-24T04:04:30.455105Z","iopub.execute_input":"2021-08-24T04:04:30.455452Z","iopub.status.idle":"2021-08-24T04:04:30.465316Z","shell.execute_reply.started":"2021-08-24T04:04:30.455414Z","shell.execute_reply":"2021-08-24T04:04:30.464266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: {}\".format(trial.value))\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T04:04:30.467011Z","iopub.execute_input":"2021-08-24T04:04:30.467365Z","iopub.status.idle":"2021-08-24T04:04:30.494549Z","shell.execute_reply.started":"2021-08-24T04:04:30.467326Z","shell.execute_reply":"2021-08-24T04:04:30.493598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the next notebook, I will share the optimized model.**\n\n**If you found my work helpful, please upvote!**\nThank You!","metadata":{}}]}