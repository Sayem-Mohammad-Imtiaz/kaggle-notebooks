{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix \nimport warnings as ws\nws.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns = \"PassengerId\", inplace = True)\ndef clean_name(name):\n    return name.lower().strip().replace(\" \", \"_\")\ndf.rename(columns = clean_name, inplace =True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(columns = \"survived\")\ny = df.survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing Some EDA to get more insights \nframe = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values \ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the firstname and lastname are the unique values for the every person it doesn't gives us any significancef\nframe.drop(columns  = [\"firstname\", \"lastname\"], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize = (9,9))\nplt.title(\"Coutrywise passengers count\", fontdict={'fontsize' : 20})\nsns.countplot(y = frame.country, palette=\"plasma\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like the swedish people are present on the deck as compared to other people. we have  to deal with this country column wisely as it bears more no. of catagroies we will handle it carefully","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(list(frame.country.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.sex.replace({'M': 1, 'F' : 0 }, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame[\"is_passenger\"] = pd.get_dummies(df.category, drop_first= True).rename(columns= {'P': \"is_passenger\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frame.head()\ntemp = pd.get_dummies(frame, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize = (20,20))\nsns.heatmap(temp.corr(), annot= True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the heatmap we can conclude that, addition of the dummy column for counties doesn't affects the overall correlatation. Just the 'swedish' columns add some. Hence we will finalize our columns now ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col_finalize = [\"sex\", \"age\", \"country_Sweden\"]\nX = temp[col_finalize]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splititng  the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42,  test_size = 0.2, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the Model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = [KNeighborsClassifier(), GaussianNB(), LogisticRegression(),DecisionTreeClassifier(), RandomForestClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfor i in classifier:\n    i.fit(X_train, y_train)\n    score = cross_val_score(i, X_train, y_train, cv =5)\n    y_pred = i.predict(X_test) \n    acc = accuracy_score(y_test, y_pred)\n    if(i.__class__.__name__ == \"GaussianNB\"):\n        print(i.__class__.__name__ , \"\\t\\t  Has accuracy of \", round(acc * 100, 3), \"\\t CV score is \", round(score.mean()*100, 3))\n    else:\n        print(i.__class__.__name__ , \"\\t  Has accuracy of \", round(acc * 100, 3), \"\\t CV score is \", round(score.mean()*100, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By looking at the  cv and accuracy_score  we will Logistic Regression for our further anlaysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit (X_train,y_train)\ny_predict = lr.predict(X_test)\nprint(\"Accuracy score is \",accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning the Logistic Regression\nfrom sklearn.model_selection import GridSearchCV\nlogit  = LogisticRegression()\nparm = {\n    'C': np.linspace(0.1, 1.0, 10),\n    'class_weight': ['balanced','None'],\n    'penalty': ['l1', 'l2', 'none'],\n    'solver' :['liblinear']\n}\ngsv = GridSearchCV(logit, parm, scoring = 'roc_auc', n_jobs= -1, cv = 5)\ngsv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsv.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Logistic Score is much lesser than that of the previous score hence it is not suitable as our assumtions was. Hence we will now try with the RandomForest as the tree based models perform well on the  data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RandomForestClassifier(max_depth  = 4, random_state=42)\nrfe.fit(X_train, y_train)\ny_pred_rfe = rfe.predict(X_test) \nprint(accuracy_score(y_test, y_pred_rfe))\nprint(confusion_matrix(y_test, y_pred_rfe))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Without tuning  the RandomForest Performed  well ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning the random forest\nforest = RandomForestClassifier()\nparam  = {\n    'n_estimators': [100, 300, 500, 800, 1200],\n    'max_depth' : [5, 8, 15, 25, 30],\n    'min_samples_split' : [2, 5, 10, 15, 100],\n    'min_samples_leaf' : [1, 2, 5, 10]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsv = GridSearchCV(forest, param, cv = 3, verbose = 1, \n                      n_jobs = -1)\ngsv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsv.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_param = gsv.best_params_\nfinal_param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest1 = RandomForestClassifier(**final_param, random_state=42)\nforest1.fit(X_train, y_train)\ny_pred = forest1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, y_pred))\ncnf = confusion_matrix(y_test, y_pred)\nprint(cnf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize = (6,6))\nsns.heatmap(cnf, annot = True, fmt = \"2g\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we are The accuracy is 0.86 and it has pretty low misclassification. We can furthur use the XGBoost  but the amount of data is low in amt. Hence we can also improve RFE by adding more Data ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div>\n    <p><i><strong>  UPVOTE IT  :>) </strong> </i></p>\n    </div> ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}