{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Hard Disk Drive Failures using Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"## This notebook contains:\n1. Performing EDA on  'Hard Disk Drive Failure' dataset provided on Kaggle\n2. Using ML to build a model which can predict whether a HDD is likely to fail or not. "},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#First we import all the libraries\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.utils import resample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'failure'   #defining a global variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train = pd.read_csv(\"/kaggle/input/hard-drive-data-and-stats/data_Q3_2019/data_Q3_2019/2019-07-09.csv\")  #Training Dataset\n\ndf_test = pd.read_csv(\"/kaggle/input/hard-drive-data-and-stats/data_Q3_2019/data_Q3_2019/2019-07-10.csv\")    #Test Dataset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\nprint('*'*50)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of target variable (Failure)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_train['failure'])    #Checking the distribution of the target variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##   Balancing the dataset"},{"metadata":{},"cell_type":"markdown","source":"*  Upsampling the minority class for train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalid = df_train[df_train['failure'] == 0]    #data of HDDs which do not indicate failure\nfailed = df_train[df_train['failure'] == 1]   #data of HDDs likely to fail\n\nprint(\"valid hdds:\",len(valid))      #storing the total number of valid HDDs\nprint(\"failing hdds:\",len(failed))    #storing the total number of HDDs likely to fail","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since the number of HDDs indicating failure are too low, we proceed to upsample the minority class viz.'failure'\n\n# We perform this step to prevent our final model from being biased\n\n   #Resampling of the failure class to match the length of valid HDDs\n\nfailed_up = resample(failed,replace=True,n_samples=len(valid),random_state=27)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally we concatenate our newly resampled classes with our training data\n\ndf_train = pd.concat([valid,failed_up])\ndf_train.failure.value_counts()       #Levelling the count of both classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_train['failure'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape  # You can notice the dimensions have doubled for our training dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ##   Feature Selection and filling of missing values"},{"metadata":{},"cell_type":"markdown","source":"###  For the training data:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  Selecting features with high correlation to hdd failure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features which highly correlate to HDD failure as per BackBlaze \n\n# https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/\n\n# SMART 5 \t\tReallocated Sectors Count\n# SMART 187 \t\tReported Uncorrectable Errors\n# SMART 188 \t\tCommand Timeout\n# SMART 197 \t\tCurrent Pending Sector Count\n# SMART 198 \t\tUncorrectable Sector Count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['date',\n 'serial_number',\n 'model',\n 'capacity_bytes',\n 'failure',\n'smart_5_raw','smart_187_raw','smart_188_raw','smart_197_raw','smart_198_raw']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"misc_feat = [fname for fname in df_train if fname not in features]  #misc features to be dropped \nmisc_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(misc_feat,inplace=True,axis=1)  #Dropping the misc features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our model cannot proccess string values, we remove the columns which contain string values/object values \n# to avoid errors\n\nobj = df_train.dtypes[df_train.dtypes == object ].index  \nobj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(obj,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  Handling missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()  #Total number of missing values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After going through the dataset,i found that the drives which were missing values did not correlate to its failure\n\n#  i.e all drives indicating failure did not contain missing values\n\n# Hence i replaced them with the most commonly occuring values for the respective SMART attributes\n\ndf_train['smart_187_raw'] = df_train['smart_187_raw'].fillna(0)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['smart_5_raw'] = df_train['smart_5_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['smart_188_raw'] = df_train['smart_188_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['smart_197_raw'] = df_train['smart_197_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['smart_198_raw'] = df_train['smart_198_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop('capacity_bytes',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*   Splitting the values for X_train and Y_train "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop('failure',axis=1)\nY_train = df_train['failure']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  For Test data:"},{"metadata":{},"cell_type":"markdown","source":"*    Upsampling of test data to match the dimensionality of the test and train data (optional)\n####  Note : You can skip this step if using TrainTestSplit function"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_test = df_test[df_test['failure'] == 0]\nfailed_test = df_test[df_test['failure'] == 1]\n\nprint(\"valid hdds:\",len(valid_test))\nprint(\"failing hdds:\",len(failed_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"failed_up_test = resample(failed,replace=True,n_samples=len(valid),random_state=27) #Same steps as in Training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.concat([valid_test,failed_up_test])\ndf_test.failure.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*   Feature Selection for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.drop(misc_feat,inplace=True,axis=1) #Since we have the imp features, we move ahead to drop the misc ones","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###    Filling out missing values for test data"},{"metadata":{},"cell_type":"markdown","source":" ####  We perform this step as our model cannot use NaN data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['smart_187_raw'] = df_test['smart_187_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['smart_5_raw'] = df_test['smart_5_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['smart_188_raw'] = df_test['smart_188_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['smart_197_raw'] =df_test['smart_197_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['smart_198_raw'] = df_test['smart_198_raw'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.drop(obj,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.drop('capacity_bytes',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Splitting values for X_test and Y_test (Optional)\n #### Note: Please skip this step if using TrainTestSpilt Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = df_test.drop('failure',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = df_test['failure']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Step 2: Building the model using Random Forest:"},{"metadata":{},"cell_type":"markdown","source":" ## Model 1: RF Using X_test and Y_test\n### Note : Please refer Model 2 if using Train_Test_Split"},{"metadata":{"trusted":true},"cell_type":"code","source":" #Building the Random Forest Classifier (RANDOM FOREST) \nfrom sklearn.ensemble import RandomForestClassifier \n\n# random forest model creation \nrfc = RandomForestClassifier() \nrfc.fit(X_train, Y_train) \n\n# predictions(Notice the caps'P' of yPred to differentiate between model 1 and 2) \nyPred = rfc.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Results of our predictions\n\nfrom sklearn.metrics import classification_report, accuracy_score  \nfrom sklearn.metrics import precision_score, recall_score \nfrom sklearn.metrics import f1_score, matthews_corrcoef \nfrom sklearn.metrics import confusion_matrix \n  \nn_outliers = len(failed) \nn_errors = (yPred != Y_test).sum()        #Notice the Y_test from iii) of Test Data\nprint(\"Model used is: Random Forest classifier\") \n  \nacc = accuracy_score(Y_test, yPred) \nprint(\"The accuracy is {}\".format(acc)) \n  \nprec = precision_score(Y_test, yPred) \nprint(\"The precision is {}\".format(prec)) \n  \nrec = recall_score(Y_test, yPred) \nprint(\"The recall is {}\".format(rec)) \n  \nf1 = f1_score(Y_test, yPred) \nprint(\"The F1-Score is {}\".format(f1)) \n  \nMCC = matthews_corrcoef(Y_test, yPred) \nprint(\"The Matthews correlation coefficient is {}\".format(MCC)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # confusion matrix \n\nLABELS = ['Healthy', 'Failed'] \nconf_matrix = confusion_matrix(Y_test, yPred) \nplt.figure(figsize =(12, 12)) \nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, fmt =\"d\"); \nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Model 2: RF Using the Train_Test_Split Method "},{"metadata":{"trusted":true},"cell_type":"code","source":"xData = X_train.values\nyData = Y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\n# Splitting of data into training and testing sets \nxTrain, xTest, yTrain, yTest = train_test_split( \n        xData, yData, test_size = 0.2, random_state = 42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Random Forest Classifier \nfrom sklearn.ensemble import RandomForestClassifier \n\n# RF model creation \nrfc = RandomForestClassifier() \nrfc.fit(xTrain, yTrain) \n\n# predictions (notice the small 'p' to differentiate from model 1) \nypred = rfc.predict(xTest) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score  \nfrom sklearn.metrics import precision_score, recall_score \nfrom sklearn.metrics import f1_score, matthews_corrcoef \nfrom sklearn.metrics import confusion_matrix \n  \nn_outliers = len(failed) \nn_errors = (ypred != yTest).sum()                             #yTest from the Train_Test_Split function\nprint(\"Model used is : Random Forest classifier\") \n  \nacc = accuracy_score(yTest, ypred) \nprint(\"The accuracy is {}\".format(acc)) \n  \nprec = precision_score(yTest, ypred) \nprint(\"The precision is {}\".format(prec)) \n  \nrec = recall_score(yTest, ypred) \nprint(\"The recall is {}\".format(rec)) \n  \nf1 = f1_score(yTest, ypred) \nprint(\"The F1-Score is {}\".format(f1)) \n  \nMCC = matthews_corrcoef(yTest, ypred) \nprint(\"The Matthews correlation coefficient is{}\".format(MCC)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix \n\nLABELS = ['Normal', 'Failed'] \nconf_matrix = confusion_matrix(yTest, ypred) \nplt.figure(figsize =(12, 12)) \nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, fmt =\"d\"); \nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## That's it folks! Thank you for your time!\n## Would love to hear your comments and valuable feedback!  :)"},{"metadata":{},"cell_type":"markdown","source":"### Dataset : https://www.kaggle.com/jackywangkaggle/hard-drive-data-and-stats"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}