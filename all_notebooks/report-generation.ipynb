{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Download necessary packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kulc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/ahmadelsallab/MultiCheXNet.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nimport math\nimport matplotlib.pyplot as plt\n\nimport re\nimport html\nimport string\nimport unicodedata\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  =pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = pd.read_csv('../input/chest-xrays-indiana-university/indiana_projections.csv')\ndff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Problems.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings'].iloc[0:10].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['impression'].unique().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MeSH'].unique().tolist()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('/kaggle/input/chest-xrays-indiana-university/images/images_normalized/1_IM-0001-3001.dcm.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\")\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.projection.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build vocab","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text cleaner**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    #words = text2words(text)\n    #stop_words = stopwords.words('english')\n    #words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    #words = lemmatize_words(words)\n    #words = lemmatize_verbs(words)\n\n    return text\n  \ndef normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna(subset=['findings'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings_cleaned'] = df['findings'].swifter.apply(normalize_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['findings_cleaned'] = 'startseq '+df['findings_cleaned']+' endseq'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = []\nfor row in df['findings_cleaned'].tolist():\n    num_words.append(len(word_tokenize(row)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words= np.array(num_words)\nprint(\"min length             : \", num_words.min())\nprint(\"max length             : \", num_words.max())\nprint(\"50th percentile length : \", np.percentile(num_words,50))\nprint(\"75th percentile length : \", np.percentile(num_words,75))\nprint(\"90th percentile length : \", np.percentile(num_words,90))\nprint(\"95th percentile length : \", np.percentile(num_words,95))\nprint(\"98th percentile length : \", np.percentile(num_words,98))\nprint(\"98th percentile length : \", np.percentile(num_words,99))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nmax_len = 100\n\ntok = Tokenizer(num_words=vocab_size,  oov_token='UNK' )\ntok.fit_on_texts(df['findings_cleaned'].tolist())\n\nvocab_size = len(tok.word_index) + 1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loader from Repo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.data_loader.indiana_dataloader import get_train_validation_generator\nfrom tensorflow.keras.applications.densenet import preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_vocab_size=10000\nmax_len=100\n\ncsv_path1  =\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\"\ncsv_path2  =\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\"\nimg_path   =\"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/\"\n\ntrain_dataloader, val_dataloader, vocab_size, tok = get_train_validation_generator(csv_path1,csv_path2,img_path, max_vocab_size,max_len,preprocess=vgg_preprocess_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y = next(enumerate(train_dataloader))[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X[0][0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.evaluation.report_gen_evaluation import evaluate_from_dataloader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            #weights=\"imagenet\",\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\nencoder_output = layers.Dropout(0.2)(encoder_output)\nencoder_output = layers.Dense(512,activation='relu')(encoder_output)\n\n##decoder ########################\n\n#layers\ngru_layer =  layers.GRU(512,activation='relu', return_sequences=True)\ndense_layer= layers.Dense(vocab_size,activation='softmax')\nembedding_layer = layers.Embedding(vocab_size, 300, mask_zero=True)\ndropout = layers.Dropout(0.2)\n\n# decoder model\nembedding_output = embedding_layer(report_input)\ngru_output = gru_layer(embedding_output, initial_state=encoder_output )\ngru_output = dropout(gru_output)\noutput  = dense_layer(gru_output)\nmodel = Model([img_input,report_input ],output)\n\n##Inference models ################\n\n#encoder_inference model\nencoder_model = Model(img_input,encoder_output)\n\n#decoder_inference model\nprev_hidden_state= layers.Input(shape= (512))\nreport_input2 = layers.Input(shape= (1,))\nembedding_output2= embedding_layer(report_input2)\ngru_output2 = gru_layer(embedding_output2, initial_state=prev_hidden_state )\noutput2 = dense_layer(gru_output2)\ndecoder_model = Model([report_input2,prev_hidden_state],[output2,gru_output2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs =5\nlr=1e-3\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator( train_dataloader,\n                    validation_data = val_dataloader,\n                    epochs = epochs\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= hist.history['loss']\ny_val = hist.history['val_loss']\n\nx = range(0,len(y))\nplt.plot(x,y)\nplt.plot(x,y_val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"weights.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.evaluation.report_gen_evaluation import get_predictions_from_data_loader\nfrom copy import deepcopy\n\nval_dataloader_tmp = deepcopy(val_dataloader)\nval_dataloader_tmp.nb_iteration  = 3\nGT , preds = get_predictions_from_data_loader(val_dataloader_tmp,tok,encoder_model, decoder_model,max_len,decoder_type='GRU')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=0\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=1\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=2\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=3\nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index=4 \nprint(GT[index])\nprint((\"=====================================\"))\nprint(preds[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_from_dataloader(val_dataloader,tok,encoder_model,decoder_model,max_len,decoder_type='GRU')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validation score: 0.6121\n\n\n# (0.1625980645932062,\n#  0.4032345032275961,\n#  0.5798753018059997,\n#  0.6350074828122863)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Model - Show and tell","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            weights=None,#\"imagenet\",\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\n\nX_img = layers.Dropout(0.5)(encoder_output)\nX_img = layers.Dense(300, use_bias = False,activation='relu' ,\n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name = 'dense_img')(X_img)\nX_img = layers.BatchNormalization(name='batch_normalization_img')(X_img)\nX_img = layers.Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n\n##decoder ########################\n\nX_text = layers.Embedding(vocab_size, 300, mask_zero = True, name = 'emb_text')(report_input)\nX_text = layers.Dropout(0.5)(X_text)\n\n# Initial States\n\n\nLSTMLayer = layers.LSTM(300, return_sequences = True, return_state = True, dropout=0.5, name = 'lstm')\noutput_layer = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax',\n                                 kernel_regularizer = regularizers.l2(1e-4), \n                                 bias_regularizer = regularizers.l2(1e-4)), name = 'time_distributed_softmax')\n# Take image embedding as the first input to LSTM\n_, a, c = LSTMLayer(X_img)\n\nA, _, _ = LSTMLayer(X_text, initial_state=[a, c])\noutput = output_layer(A)\n\n\n\nmodel  = Model(inputs=[img_input, report_input], outputs=output, name='NIC_greedy_inference_v2')\n\n\n##Inference models ################\n\n#encoder_inference model\nencoder_model = Model(img_input,[a,c])\n\n# Decoder model ###################\n\na0 = layers.Input(shape=(300,))\nc0 = layers.Input(shape=(300,))\n\nA, alast, clast = LSTMLayer(X_text, initial_state=[a0, c0])\noutput = output_layer(A)\n\n\n\ndecoder_model = Model([report_input,a0,c0],[output,alast,clast])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs =5\nlr=1e-4\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator( train_dataloader,\n                    validation_data = val_dataloader,\n                    epochs = epochs\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= hist.history['loss']\ny_val = hist.history['val_loss']\n\nx = range(0,len(y))\nplt.plot(x,y)\nplt.plot(x,y_val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save_weights(\"weights.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat0_1, feat0_2 = encoder_model(np.expand_dims(X0[0],axis=0))\nfeat1_1, feat1_2 = encoder_model(np.expand_dims(X0[1],axis=0))\nprint(np.array(feat0_1==feat1_1).sum())\nprint(np.array(feat0_2==feat1_2).sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.evaluation.report_gen_evaluation import get_predictions_from_data_loader\nfrom copy import deepcopy\n\nval_dataloader_tmp = deepcopy(val_dataloader)\nval_dataloader_tmp.nb_iteration  = 3\nGT , preds = get_predictions_from_data_loader(val_dataloader_tmp,tok,encoder_model, decoder_model,max_len,decoder_type='LSTM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(8):\n    print(GT[index])\n    print((\"=====================================\"))\n    print(preds[index])\n    print(\"\")\n    print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_from_dataloader(val_dataloader,tok,encoder_model,decoder_model,max_len,decoder_type='LSTM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_loss: 0.7740\n\n\n\n# (0.1420931304748732,\n#  0.37695242468363727,\n#  0.5568932125465332,\n#  0.6139645141892464)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dr's Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=(256,256,3),\n        )\n\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\n\nfe1 = layers.Dropout(0.5)(encoder_output)\nfe2 = layers.Dense(256, activation='relu')(fe1)\n\n# sequence model\nemb_layer =  layers.Embedding(vocab_size, 256, mask_zero=True)\ndropout1_layer= layers.Dropout(0.5)\nLSTM_layer=  layers.LSTM(256, return_sequences = True,return_state=True)\n\nse1 = emb_layer(report_input)\nse2 = dropout1_layer(se1)\nse3,_,_ = LSTM_layer(se2)\nD1  =layers.Dense(256 , activation='relu')\nD2 = layers.Dense(vocab_size, activation='softmax')\n\n# decoder model\ndecoder1 = layers.add([fe2, se3])\ndecoder2 = D1(decoder1)\noutputs = D2(decoder2)\n\n# tie it together [image, seq] [word]\nmodel = Model(inputs=[img_input, report_input], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# summarize model\nprint(model.summary())\nplot_model(model, to_file='model.png', show_shapes=True)\n\n\n# Encoder model\nencoder_model = Model(inputs=[img_input], outputs=fe2)\n\n#decoder model\nreport_input1 = layers.Input(shape= (1,))\nfe22 = layers.Input(shape=(256,))\n\nse11 = emb_layer(report_input1)\nse22 = dropout1_layer(se11)\nse33,_,c33 = LSTM_layer(se22)\n\ndecoder11 = layers.add([fe22, se33])\ndecoder22 = D1(decoder11)\noutputss = D2(decoder22)\n\ndecoder_model = Model(inputs=[fe22, report_input1 ], outputs=[outputss,se33,c33,fe22])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nlr=1e-3\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator( train_dataloader,\n                    validation_data = val_dataloader,\n                    epochs = epochs\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= hist.history['loss']\ny_val = hist.history['val_loss']\n\nx = range(0,len(y))\nplt.plot(x,y)\nplt.plot(x,y_val)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"initializer = tf.keras.initializers.GlorotNormal()\na0 = initializer(shape=(1,256))\nc0 = initializer(shape=(1,256))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef tokens_to_text(tokens,tok,end_token='endseq'):\n    sentence=\"\"\n    for token in tokens:\n        if token ==0:\n            break\n        \n        word = tok.index_word[token]\n        \n        if word==end_token:\n            break\n            \n        sentence+= word+\" \"\n        \n    sentence = sentence.strip()\n    \n    return sentence\n\n\ndef greedy_inference(input_img, tok,encoder_model, decoder_model,max_len,start_token=\"startseq\",end_token='endseq',decoder_type=\"LSTM\"):\n    if decoder_type=='LSTM':\n        a0,c0  =encoder_model(np.expand_dims(input_img,axis=0))\n    elif decoder_type=='GRU': \n        hidden_layer  =encoder_model(np.expand_dims(input_img,axis=0))\n        initializer = tf.keras.initializers.GlorotNormal()\n        a0 = initializer(shape=(1,256))\n        c0 = initializer(shape=(1,256))\n        \n    word = tok.word_index[start_token]\n    \n    words = []\n    \n    for index in range(max_len):\n        if decoder_type=='LSTM':\n            word_probs , a0,c0 = decoder_model.predict([[np.array([word]),a0,c0]])\n        elif decoder_type=='GRU': \n            word_probs , hidden_layer = decoder_model.predict([[np.array([word]),hidden_layer]])\n            hidden_layer=hidden_layer[0]\n        \n        word = np.argmax(word_probs)\n        \n        try:\n            if tok.index_word[word]==end_token:\n                break\n        except:\n            pass\n        \n        words.append(word)\n        \n    words = tokens_to_text(words,tok)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kk():\n    return 1,2,3\nl = kk()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ll(i , l1,l2,l3):\n    print(i)\n    print(l1)\n    print(l2)\n    print(l3)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_from_dataloader(val_dataloader,\n                         tok,\n                         encoder_model,\n                         decoder_model,\n                         max_len,\n                         decoder_type=\"GRU\"\n\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training without any pretrained weights, this model predicts all xxx\n\n(0.003765473458262969,\n 0.061363453767392925,\n 0.18738976959694617,\n 0.24771647859476956)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.evaluation.report_gen_evaluation import get_predictions_from_data_loader\nfrom copy import deepcopy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_dataloader_tmp = deepcopy(val_dataloader)\nval_dataloader_tmp.nb_iteration  = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GT , preds = get_predictions_from_data_loader(val_dataloader_tmp,tok,encoder_model, decoder_model,max_len,decoder_type='GRU')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 0\nprint(GT[index])\nprint(\"================================\")\nprint(preds[index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 1\nprint(GT[index])\nprint(\"================================\")\nprint(preds[index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 2\nprint(GT[index])\nprint(\"================================\")\nprint(preds[index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 3\nprint(GT[index])\nprint(\"================================\")\nprint(preds[index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 4\nprint(GT[index])\nprint(\"================================\")\nprint(preds[index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 5\nprint(GT[index])\nprint(\"================================\")\nprint(preds[index])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(decoder_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Attention Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import Layer, Dense, TimeDistributed, Concatenate, InputSpec,  RNN\nfrom keras.layers.wrappers import Wrapper\nimport numpy as np\nimport tensorflow as tf\n\nclass ScaledDotProductAttention(Layer):\n    \"\"\"\n        Implementation according to:\n            \"Attention is all you need\" by A Vaswani, N Shazeer, N Parmar (2017)\n    \"\"\"\n\n    def __init__(self, return_attention=False, **kwargs):    \n        self._return_attention = return_attention\n        self.supports_masking = True\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n    \n    def compute_output_shape(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        if not self._return_attention:\n            return input_shape[-1]\n        else:\n            return [input_shape[-1], [input_shape[0][0], input_shape[0][1], input_shape[1][2]]]\n    \n    def _validate_input_shape(self, input_shape):\n        if len(input_shape) != 3:\n            raise ValueError(\"Layer received an input shape {0} but expected three inputs (Q, V, K).\".format(input_shape))\n        else:\n            if input_shape[0][0] != input_shape[1][0] or input_shape[1][0] != input_shape[2][0]:\n                raise ValueError(\"All three inputs (Q, V, K) have to have the same batch size; received batch sizes: {0}, {1}, {2}\".format(input_shape[0][0], input_shape[1][0], input_shape[2][0]))\n            if input_shape[0][1] != input_shape[1][1] or input_shape[1][1] != input_shape[2][1]:\n                raise ValueError(\"All three inputs (Q, V, K) have to have the same length; received lengths: {0}, {1}, {2}\".format(input_shape[0][0], input_shape[1][0], input_shape[2][0]))\n            if input_shape[0][2] != input_shape[1][2]:\n                raise ValueError(\"Input shapes of Q {0} and V {1} do not match.\".format(input_shape[0], input_shape[1]))\n    \n    def build(self, input_shape):\n        self._validate_input_shape(input_shape)\n        \n        super(ScaledDotProductAttention, self).build(input_shape)\n    \n    def call(self, x, mask=None):\n        q, k, v = x\n        d_k = q.shape.as_list()[2]\n\n        # in pure tensorflow:\n        # weights = tf.matmul(x_batch, tf.transpose(y_batch, perm=[0, 2, 1]))\n        # normalized_weights = tf.nn.softmax(weights/scaling)\n        # output = tf.matmul(normalized_weights, x_batch)\n        \n        weights = K.batch_dot(q,  k, axes=[2, 2])\n\n        if mask is not None:\n            # add mask weights\n            if isinstance(mask, (list, tuple)):\n                if len(mask) > 0:\n                    raise ValueError(\"mask can only be a Tensor or a list of length 1 containing a tensor.\")\n\n                mask = mask[0]\n\n            weights += -1e10*(1-mask)\n\n        normalized_weights = K.softmax(weights / np.sqrt(d_k))\n        output = K.batch_dot(normalized_weights, v)\n        \n        if self._return_attention:\n            return [output, normalized_weights]\n        else:\n            return output\n\n    def get_config(self):\n        config = {'return_attention': self._return_attention}\n        base_config = super(ScaledDotProductAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nclass MultiHeadAttention():\n    \"\"\"\n        Implementation according to:\n            \"Attention is all you need\" by A Vaswani, N Shazeer, N Parmar (2017)\n    \"\"\"\n\n    def __init__(self, h, d_k=None, d_v=None, d_model=None, activation=None, return_attention=False, **kwargs):    \n        super(MultiHeadAttention, self).__init__(**kwargs)\n        \n        if (type(h) is not int or h < 2):\n            raise ValueError(\"You have to set `h` to an int >= 2.\")\n        self._h = h\n        \n        if d_model and (type(d_model) is not int or d_model < 1):\n                raise ValueError(\"You have to set `d_model` to an int >= 1.\")\n        self._d_model = d_model\n        \n        if d_k and int (type(d_k) is not int or d_k < 1):\n            raise ValueError(\"You have to set `d_k` to an int >= 1.\")\n        self._d_k = d_k\n        \n        if d_v and (type(d_v) is not int or d_v < 1):\n            raise ValueError(\"You have to set `d_v` to an int >= 1.\")\n        self._d_v = d_v\n        \n        self._activation = None\n        self._return_attention = return_attention\n    \n    def compute_output_shape(self, input_shape):\n        self._validate_input_shape(input_shape)\n        \n        if self._return_attention:\n            return [input_shape[-1], [input_shape[0][0], input_shape[1][1], self._h*input_shape[2][2]]]\n        else:\n            return input_shape[-1]\n    \n    def _validate_input_shape(self, input_shape):\n        if len(input_shape) != 3:\n            raise ValueError(\"Layer received an input shape {0} but expected three inputs (Q, V, K).\".format(input_shape))\n        else:\n            if input_shape[0][0] != input_shape[1][0] or input_shape[1][0] != input_shape[2][0]:\n                raise ValueError(\"All three inputs (Q, V, K) have to have the same batch size; received batch sizes: {0}, {1}, {2}\".format(input_shape[0][0], input_shape[1][0], input_shape[2][0]))\n            if input_shape[0][1] != input_shape[1][1] or input_shape[1][1] != input_shape[2][1]:\n                raise ValueError(\"All three inputs (Q, V, K) have to have the same length; received lengths: {0}, {1}, {2}\".format(input_shape[0][0], input_shape[1][0], input_shape[2][0]))\n            if input_shape[0][2] != input_shape[1][2]:\n                raise ValueError(\"Input shapes of Q {0} and V {1} do not match.\".format(input_shape[0], input_shape[1]))\n    \n    def build(self, input_shape):\n        self._validate_input_shape(input_shape)\n        \n        d_k = self._d_k if self._d_k else input_shape[1][-1]\n        d_model = self._d_model if self._d_model else input_shape[1][-1]\n        d_v = self._d_v\n\n        if type(d_k) == tf.Dimension:\n            d_k = d_k.value\n        if type(d_model) == tf.Dimension:\n            d_model = d_model.value\n        \n        self._q_layers = []\n        self._k_layers = []\n        self._v_layers = []\n        self._sdp_layer = ScaledDotProductAttention(return_attention=self._return_attention)\n    \n        for _ in range(self._h):\n            self._q_layers.append(\n                TimeDistributed(\n                    Dense(d_k, activation=self._activation, use_bias=False)\n                )\n            )\n            self._k_layers.append(\n                TimeDistributed(\n                    Dense(d_k, activation=self._activation, use_bias=False)\n                )\n            )\n            self._v_layers.append(\n                TimeDistributed(\n                    Dense(d_v, activation=self._activation, use_bias=False)\n                )\n            )\n        \n        self._output = TimeDistributed(Dense(d_model))\n        #if self._return_attention:\n        #    self._output = Concatenate()\n    \n    def __call__(self, x, mask=None):\n        if isinstance(x, (list, tuple)):\n            self.build([it.shape for it in x])\n        else:\n            self.build(x.shape)\n\n        q, k, v = x\n        \n        outputs = []\n        attentions = []\n        for i in range(self._h):\n            qi = self._q_layers[i](q)\n            ki = self._k_layers[i](k)\n            vi = self._v_layers[i](v)\n            \n            if self._return_attention:\n                output, attention = self._sdp_layer([qi, ki, vi], mask=mask)\n                outputs.append(output)\n                attentions.append(attention)\n            else:\n                output = self._sdp_layer([qi, ki, vi], mask=mask)\n                outputs.append(output)\n            \n        concatenated_outputs = Concatenate()(outputs)\n        output = self._output(concatenated_outputs)\n        \n        if self._return_attention:\n            attention = Concatenate()(attentions)\n            # print(\"attention\", attention, attention.shape)\n       \n        if self._return_attention:\n            return [output, attention]\n        else:\n            return output        \n\n# https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html\n# https://arxiv.org/pdf/1508.04025.pdf\nclass SequenceAttention(Layer):\n    \"\"\"\n        Takes two inputs of the shape (batch_size, T, dim1) and (batch_size, T, dim2),\n        whereby the first item is the source data and the second one the key data.\n        This layer then calculates for each batch's element and each time step a softmax attention \n        vector between the key data and the source data. Finally, this attention vector is multiplied\n        with the source data to obtain a weighted output. This means, that the key data is used to\n        interpret the source data in a special way to create an output of the same shape as the source data.\n    \"\"\"\n    def __init__(self, similarity, kernel_initializer=\"glorot_uniform\", **kwargs):\n        super(SequenceAttention, self).__init__(**kwargs)\n        if isinstance(similarity, str):\n            ALLOWED_SIMILARITIES = [\"additive\", \"multiplicative\" ]\n            if similarity not in ALLOWED_SIMILARITIES:\n                raise ValueError(\"`similarity` has to be either a callable or one of the following: {0}\".format(ALLOWED_SIMILARITIES))\n            else:\n                self._similarity = getattr(self, \"_\" + similarity + \"_similarity\")\n        elif callable(similarity):\n            self._similarity = similarity\n        else:\n            raise ValueError(\"`similarity` has to be either a callable or one of the following: {0}\".format(ALLOWED_SIMILARITIES))\n            \n        self._kernel_initializer = kernel_initializer\n            \n    def build(self, input_shape):\n        super(SequenceAttention, self).build(input_shape)\n        self._validate_input_shape(input_shape)\n        \n        self._weights = {}\n        if self._similarity == self._additive_similarity:\n            self._weights[\"w_a\"] = self.add_weight(\n                name='w_a', \n                shape=(input_shape[0][-1] + input_shape[1][-1], input_shape[0][-1]),\n                initializer=self._kernel_initializer,\n                trainable=True\n            )\n            \n            self._weights[\"v_a\"] = self.add_weight(\n                name='v_a', \n                shape=(1, input_shape[0][-1]),\n                initializer=self._kernel_initializer,\n                trainable=True\n            )\n            \n        elif self._similarity == self._multiplicative_similarity:\n            self._weights[\"w_a\"] = self.add_weight(\n                name='w_a', \n                shape=(input_shape[1][-1], input_shape[0][-1]),\n                initializer=self._kernel_initializer,\n                trainable=True\n            )\n\n        self.built = True\n        \n    def compute_output_shape(self, input_shape):\n        self._validate_input_shape(input_shape)\n        \n        return input_shape[0]\n            \n    def _validate_input_shape(self, input_shape):\n        if len(input_shape) != 2:\n            raise ValueError(\"Layer received an input shape {0} but expected two inputs (source, query).\".format(input_shape))\n        else:\n            if input_shape[0][0] != input_shape[1][0]:\n                raise ValueError(\"Both two inputs (source, query) have to have the same batch size; received batch sizes: {0}, {1}\".format(input_shape[0][0], input_shape[1][0]))\n            if input_shape[0][1] != input_shape[1][1]:\n                raise ValueError(\"Both inputs (source, query) have to have the same length; received lengths: {0}, {1}\".format(input_shape[0][0], input_shape[1][0]))\n        \n    def call(self, x):\n        source, query = x\n        \n        similarity = self._similarity(source, query)\n        expected_similarity_shape = [source.shape.as_list()[0], source.shape.as_list()[1], source.shape.as_list()[1]]\n       \n        if similarity.shape.as_list() != expected_similarity_shape:\n            raise RuntimeError(\"The similarity function has returned a similarity with shape {0}, but expected {1}\".format(similarity.shape.as_list()[:2], expected_similarity_shape))\n        \n        score = K.softmax(similarity)\n        output = K.batch_dot(score, source, axes=[1, 1])\n        \n        return output\n    \n    def _additive_similarity(self, source, query):\n        concatenation = K.concatenate([source, query], axis=2)\n        nonlinearity = K.tanh(K.dot(concatenation, self._weights[\"w_a\"]))\n        \n        # tile the weight vector (1, 1, dim) for each time step and each element of the batch -> (bs, T, dim)\n        source_shape = K.shape(source)\n        vaeff = K.tile(K.expand_dims(self._weights[\"v_a\"], 0), [source_shape[0], source_shape[1], 1])\n\n        similarity = K.batch_dot(K.permute_dimensions(vaeff, [0, 2, 1]), nonlinearity, axes=[1, 2])\n        \n        return similarity\n\n    def _multiplicative_similarity(self, source, query):\n        qp = K.dot(query, self._weights[\"w_a\"])\n        similarity = K.batch_dot(K.permute_dimensions(qp, [0, 2, 1]), source, axes=[1, 2])\n        \n        return similarity\n\n    def get_config(self):\n        config = {'similarity': self._similarity, 'kernel_initializer': self._kernel_initializer}\n        base_config = super(SequenceAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nclass AttentionRNNWrapper(Wrapper):\n    \"\"\"\n        The idea of the implementation is based on the paper:\n            \"Effective Approaches to Attention-based Neural Machine Translation\" by Luong et al.\n        This layer is an attention layer, which can be wrapped around arbitrary RNN layers.\n        This way, after each time step an attention vector is calculated\n        based on the current output of the LSTM and the entire input time series.\n        This attention vector is then used as a weight vector to choose special values\n        from the input data. This data is then finally concatenated to the next input\n        time step's data. On this a linear transformation in the same space as the input data's space\n        is performed before the data is fed into the RNN cell again.\n        This technique is similar to the input-feeding method described in the paper cited\n    \"\"\"\n\n    def __init__(self, layer, weight_initializer=\"glorot_uniform\", **kwargs):\n        assert isinstance(layer, RNN)\n        self.layer = layer\n        self.supports_masking = True\n        self.weight_initializer = weight_initializer\n        \n        super(AttentionRNNWrapper, self).__init__(layer, **kwargs)\n        \n    def _validate_input_shape(self, input_shape):\n        if len(input_shape) != 3:\n            raise ValueError(\"Layer received an input with shape {0} but expected a Tensor of rank 3.\".format(input_shape[0]))\n\n    def build(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        self.input_spec = InputSpec(shape=input_shape)\n        \n        if not self.layer.built:\n            self.layer.build(input_shape)\n            self.layer.built = True\n            \n        input_dim = input_shape[-1]\n\n        if self.layer.return_sequences:\n            output_dim = self.layer.compute_output_shape(input_shape)[0][-1]\n        else:\n            output_dim = self.layer.compute_output_shape(input_shape)[-1]\n      \n        self._W1 = self.add_weight(shape=(input_dim, input_dim), name=\"{}_W1\".format(self.name), initializer=self.weight_initializer)\n        self._W2 = self.add_weight(shape=(output_dim, input_dim), name=\"{}_W2\".format(self.name), initializer=self.weight_initializer)\n        self._W3 = self.add_weight(shape=(2*input_dim, input_dim), name=\"{}_W3\".format(self.name), initializer=self.weight_initializer)\n        self._b2 = self.add_weight(shape=(input_dim,), name=\"{}_b2\".format(self.name), initializer=self.weight_initializer)\n        self._b3 = self.add_weight(shape=(input_dim,), name=\"{}_b3\".format(self.name), initializer=self.weight_initializer)\n        self._V = self.add_weight(shape=(input_dim,1), name=\"{}_V\".format(self.name), initializer=self.weight_initializer)\n        \n        super(AttentionRNNWrapper, self).build()\n        \n    def compute_output_shape(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        return self.layer.compute_output_shape(input_shape)\n    \n    @property\n    def trainable_weights(self):\n        return self._trainable_weights + self.layer.trainable_weights\n\n    @property\n    def non_trainable_weights(self):\n        return self._non_trainable_weights + self.layer.non_trainable_weights\n\n    def step(self, x, states):   \n        h = states[0]\n        # states[1] necessary?\n\n        # equals K.dot(X, self._W1) + self._b2 with X.shape=[bs, T, input_dim]\n        total_x_prod = states[-1]\n        # comes from the constants (equals the input sequence)\n        X = states[-2]\n        \n        # expand dims to add the vector which is only valid for this time step\n        # to total_x_prod which is valid for all time steps\n        hw = K.expand_dims(K.dot(h, self._W2), 1)\n        additive_atn = total_x_prod + hw\n        attention = K.softmax(K.dot(additive_atn, self._V), axis=1)\n        x_weighted = K.sum(attention * X, [1])\n\n        x = K.dot(K.concatenate([x, x_weighted], 1), self._W3) + self._b3\n        \n        h, new_states = self.layer.cell.call(x, states[:-2])\n        \n        return h, new_states\n    \n    def call(self, x, constants=None, mask=None, initial_state=None):\n        # input shape: (n_samples, time (padded with zeros), input_dim)\n        input_shape = self.input_spec.shape\n\n        if self.layer.stateful:\n            initial_states = self.layer.states\n        elif initial_state is not None:\n            initial_states = initial_state\n            if not isinstance(initial_states, (list, tuple)):\n                initial_states = [initial_states]\n\n            base_initial_state = self.layer.get_initial_state(x)\n            if len(base_initial_state) != len(initial_states):\n                raise ValueError(\"initial_state does not have the correct length. Received length {0} but expected {1}\".format(len(initial_states), len(base_initial_state)))\n            else:\n                # check the state' shape\n                for i in range(len(initial_states)):\n                    if not initial_states[i].shape.is_compatible_with(base_initial_state[i].shape): #initial_states[i][j] != base_initial_state[i][j]:\n                        raise ValueError(\"initial_state does not match the default base state of the layer. Received {0} but expected {1}\".format([x.shape for x in initial_states], [x.shape for x in base_initial_state]))\n        else:\n            initial_states = self.layer.get_initial_state(x)\n            \n        if not constants:\n            constants = []\n            \n        constants += self.get_constants(x)\n        \n        last_output, outputs, states = K.rnn(\n            self.step,\n            x,\n            initial_states,\n            go_backwards=self.layer.go_backwards,\n            mask=mask,\n            constants=constants,\n            unroll=self.layer.unroll,\n            input_length=input_shape[1]\n        )\n        \n        if self.layer.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.layer.states[i], states[i]))\n\n        if self.layer.return_sequences:\n            output = outputs\n        else:\n            output = last_output \n\n        # Properly set learning phase\n        if getattr(last_output, '_uses_learning_phase', False):\n            output._uses_learning_phase = True\n            for state in states:\n                state._uses_learning_phase = True\n\n        if self.layer.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            return [output] + states\n        else:\n            return output\n\n    def get_constants(self, x):\n        # add constants to speed up calculation\n        constants = [x, K.dot(x, self._W1) + self._b2]\n        \n        return constants\n\n    def get_config(self):\n        config = {'weight_initializer': self.weight_initializer}\n        base_config = super(AttentionRNNWrapper, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nclass ExternalAttentionRNNWrapper(Wrapper):\n    \"\"\"\n        The basic idea of the implementation is based on the paper:\n            \"Effective Approaches to Attention-based Neural Machine Translation\" by Luong et al.\n        This layer is an attention layer, which can be wrapped around arbitrary RNN layers.\n        This way, after each time step an attention vector is calculated\n        based on the current output of the LSTM and the entire input time series.\n        This attention vector is then used as a weight vector to choose special values\n        from the input data. This data is then finally concatenated to the next input\n        time step's data. On this a linear transformation in the same space as the input data's space\n        is performed before the data is fed into the RNN cell again.\n        This technique is similar to the input-feeding method described in the paper cited.\n        The only difference compared to the AttentionRNNWrapper is, that this layer\n        applies the attention layer not on the time-depending input but on a second\n        time-independent input (like image clues) as described in:\n            Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\n            https://arxiv.org/abs/1502.03044\n    \"\"\"\n    def __init__(self, layer, weight_initializer=\"glorot_uniform\", return_attention=False, **kwargs):\n        assert isinstance(layer, RNN)\n        self.layer = layer\n        self.supports_masking = True\n        self.weight_initializer = weight_initializer\n        self.return_attention = return_attention\n        self._num_constants = None\n\n        super(ExternalAttentionRNNWrapper, self).__init__(layer, **kwargs)\n\n        self.input_spec = [InputSpec(ndim=3), InputSpec(ndim=3)]\n        \n    def _validate_input_shape(self, input_shape):\n        if len(input_shape) >= 2:\n            if len(input_shape[:2]) != 2:\n                raise ValueError(\"Layer has to receive two inputs: the temporal signal and the external signal which is constant for all time steps\")\n            if len(input_shape[0]) != 3:\n                raise ValueError(\"Layer received a temporal input with shape {0} but expected a Tensor of rank 3.\".format(input_shape[0]))\n            if len(input_shape[1]) != 3:\n                raise ValueError(\"Layer received a time-independent input with shape {0} but expected a Tensor of rank 3.\".format(input_shape[1]))\n        else:\n            raise ValueError(\"Layer has to receive at least 2 inputs: the temporal signal and the external signal which is constant for all time steps\")\n\n    def build(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        for i, x in enumerate(input_shape):\n            self.input_spec[i] = InputSpec(shape=x)\n        \n        if not self.layer.built:\n            self.layer.build(input_shape)\n            self.layer.built = True\n            \n        temporal_input_dim = input_shape[0][-1]\n        static_input_dim = input_shape[1][-1]\n\n        if self.layer.return_sequences:\n            output_dim = self.layer.compute_output_shape(input_shape[0])[0][-1]\n        else:\n            output_dim = self.layer.compute_output_shape(input_shape[0])[-1]\n      \n        self._W1 = self.add_weight(shape=(static_input_dim, temporal_input_dim), name=\"{}_W1\".format(self.name), initializer=self.weight_initializer)\n        self._W2 = self.add_weight(shape=(output_dim, temporal_input_dim), name=\"{}_W2\".format(self.name), initializer=self.weight_initializer)\n        self._W3 = self.add_weight(shape=(temporal_input_dim + static_input_dim, temporal_input_dim), name=\"{}_W3\".format(self.name), initializer=self.weight_initializer)\n        self._b2 = self.add_weight(shape=(temporal_input_dim,), name=\"{}_b2\".format(self.name), initializer=self.weight_initializer)\n        self._b3 = self.add_weight(shape=(temporal_input_dim,), name=\"{}_b3\".format(self.name), initializer=self.weight_initializer)\n        self._V = self.add_weight(shape=(temporal_input_dim, 1), name=\"{}_V\".format(self.name), initializer=self.weight_initializer)\n        \n        super(ExternalAttentionRNNWrapper, self).build()\n        \n    @property\n    def trainable_weights(self):\n        return self._trainable_weights + self.layer.trainable_weights\n\n    @property\n    def non_trainable_weights(self):\n        return self._non_trainable_weights + self.layer.non_trainable_weights\n\n    def compute_output_shape(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        output_shape =  self.layer.compute_output_shape(input_shape[0])\n\n        if self.return_attention:\n            if not isinstance(output_shape, list):\n                output_shape = [output_shape]\n\n            output_shape = output_shape + [(None, input_shape[1][1])]\n\n        return output_shape\n    \n    def step(self, x, states):  \n        h = states[0]\n        # states[1] necessary?\n        \n        # comes from the constants\n        X_static = states[-2]\n        # equals K.dot(static_x, self._W1) + self._b2 with X.shape=[bs, L, static_input_dim]\n        total_x_static_prod = states[-1]\n\n        # expand dims to add the vector which is only valid for this time step\n        # to total_x_prod which is valid for all time steps\n        hw = K.expand_dims(K.dot(h, self._W2), 1)\n        additive_atn = total_x_static_prod + hw\n        attention = K.softmax(K.dot(additive_atn, self._V), axis=1)\n        static_x_weighted = K.sum(attention * X_static, [1])\n        \n        x = K.dot(K.concatenate([x, static_x_weighted], 1), self._W3) + self._b3\n\n        h, new_states = self.layer.cell.call(x, states[:-2])\n        \n        # append attention to the states to \"smuggle\" it out of the RNN wrapper\n        attention = K.squeeze(attention, -1)\n        h = K.concatenate([h, attention])\n\n        return h, new_states\n    \n    def call(self, x, constants=None, mask=None, initial_state=None):\n        # input shape: (n_samples, time (padded with zeros), input_dim)\n        input_shape = self.input_spec[0].shape\n\n        if len(x) > 2:\n            initial_state = x[2:]\n            x = x[:2]\n            assert len(initial_state) >= 1\n\n        static_x = x[1]\n        x = x[0]\n\n        if self.layer.stateful:\n            initial_states = self.layer.states\n        elif initial_state is not None:\n            initial_states = initial_state\n            if not isinstance(initial_states, (list, tuple)):\n                initial_states = [initial_states]\n        else:\n            initial_states = self.layer.get_initial_state(x)\n            \n        if not constants:\n            constants = []\n        constants += self.get_constants(static_x)\n\n        last_output, outputs, states = K.rnn(\n            self.step,\n            x,\n            initial_states,\n            go_backwards=self.layer.go_backwards,\n            mask=mask,\n            constants=constants,\n            unroll=self.layer.unroll,\n            input_length=input_shape[1]\n        )\n\n        # output has at the moment the form:\n        # (real_output, attention)\n        # split this now up\n\n        output_dim = self.layer.compute_output_shape(input_shape)[0][-1]\n        last_output = last_output[:output_dim]\n\n        attentions = outputs[:, :, output_dim:]\n        outputs = outputs[:, :, :output_dim]\n        \n        if self.layer.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.layer.states[i], states[i]))\n\n        if self.layer.return_sequences:\n            output = outputs\n        else:\n            output = last_output \n\n        # Properly set learning phase\n        if getattr(last_output, '_uses_learning_phase', False):\n            output._uses_learning_phase = True\n            for state in states:\n                state._uses_learning_phase = True\n\n        if self.layer.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            output = [output] + states\n\n        if self.return_attention:\n            if not isinstance(output, list):\n                output = [output]\n            output = output + [attentions]\n\n        return output\n\n    def _standardize_args(self, inputs, initial_state, constants, num_constants):\n        \"\"\"Standardize `__call__` to a single list of tensor inputs.\n        When running a model loaded from file, the input tensors\n        `initial_state` and `constants` can be passed to `RNN.__call__` as part\n        of `inputs` instead of by the dedicated keyword arguments. This method\n        makes sure the arguments are separated and that `initial_state` and\n        `constants` are lists of tensors (or None).\n        # Arguments\n        inputs: tensor or list/tuple of tensors\n        initial_state: tensor or list of tensors or None\n        constants: tensor or list of tensors or None\n        # Returns\n        inputs: tensor\n        initial_state: list of tensors or None\n        constants: list of tensors or None\n        \"\"\"\n        inputs=inputs[:2]\n        if isinstance(inputs, list) and len(inputs) > 2:\n            assert initial_state is None and constants is None\n            if num_constants is not None:\n                constants = inputs[-num_constants:]\n                inputs = inputs[:-num_constants]\n            initial_state = inputs[2:]\n            inputs = inputs[:2]\n\n        def to_list_or_none(x):\n            if x is None or isinstance(x, list):\n                return x\n            if isinstance(x, tuple):\n                return list(x)\n            return [x]\n\n        initial_state = to_list_or_none(initial_state)\n        constants = to_list_or_none(constants)\n\n        return inputs, initial_state, constants\n\n    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n        inputs, initial_state, constants = self._standardize_args(\n            inputs, initial_state, constants, self._num_constants)\n\n        if initial_state is None and constants is None:\n            return super(ExternalAttentionRNNWrapper, self).__call__(inputs, **kwargs)\n\n        # If any of `initial_state` or `constants` are specified and are Keras\n        # tensors, then add them to the inputs and temporarily modify the\n        # input_spec to include them.\n\n        additional_inputs = []\n        additional_specs = []\n        if initial_state is not None:\n            kwargs['initial_state'] = initial_state\n            additional_inputs += initial_state\n            self.state_spec = [InputSpec(shape=K.int_shape(state))\n                               for state in initial_state]\n            additional_specs += self.state_spec\n        if constants is not None:\n            kwargs['constants'] = constants\n            additional_inputs += constants\n            self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n                                   for constant in constants]\n            self._num_constants = len(constants)\n            additional_specs += self.constants_spec\n        # at this point additional_inputs cannot be empty\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n        for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                raise ValueError('The initial state or constants of an ExternalAttentionRNNWrapper'\n                                 ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n\n        if is_keras_tensor:\n            # Compute the full input spec, including state and constants\n            full_input = inputs + additional_inputs\n            full_input_spec = self.input_spec + additional_specs\n            # Perform the call with temporarily replaced input_spec\n            original_input_spec = self.input_spec\n            self.input_spec = full_input_spec\n            output = super(ExternalAttentionRNNWrapper, self).__call__(full_input, **kwargs)\n            self.input_spec = self.input_spec[:len(original_input_spec)]\n            return output\n        else:\n            return super(ExternalAttentionRNNWrapper, self).__call__(inputs, **kwargs)\n\n    def get_constants(self, x):\n        # add constants to speed up calculation\n        constants = [x, K.dot(x, self._W1) + self._b2]\n        return constants\n\n    def get_config(self):\n        config = {'return_attention': self.return_attention, 'weight_initializer': self.weight_initializer}\n        base_config = super(ExternalAttentionRNNWrapper, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from kulc.attention import ExternalAttentionRNNWrapper\nfrom tensorflow.keras.layers import Input,Embedding,Lambda,Dense,TimeDistributed,LSTM,Reshape\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import plot_model\n\ndef create_model(vocabulary_size, embedding_size, T, L, D):\n    \n    image_input = Input(shape=(256,256,3), name=\"image_input\")\n    \n    image_model = tf.keras.applications.VGG16(include_top=False,\n                                          input_shape=(256,256,3),\n                                          weights='imagenet')\n    image_model = Model(image_model.input, image_model.layers[-2].output)\n    for layer in image_model.layers:\n        layer.trainable = False\n    \n    \n    image_features_input = image_model(image_input)\n    image_features_input = Reshape((16*16,512))(image_features_input)\n\n    captions_input = Input(shape=(T,), name=\"captions_input\")\n    captions = Embedding(vocabulary_size, embedding_size, input_length=T)(captions_input)\n\n    averaged_image_features = Lambda(lambda x: K.mean(x, axis=1))\n    averaged_image_features = averaged_image_features(image_features_input)\n    initial_state_h = Dense(embedding_size)(averaged_image_features)\n    initial_state_c = Dense(embedding_size)(averaged_image_features)\n\n    image_features = TimeDistributed(Dense(D, activation=\"relu\" ))(image_features_input)\n\n    encoder = LSTM(embedding_size, return_sequences=True, return_state=True, recurrent_dropout=0.1)\n    attented_encoder = ExternalAttentionRNNWrapper(encoder, return_attention=True)\n\n    output = TimeDistributed(Dense(vocabulary_size, activation=\"softmax\"), name=\"output\")\n\n    # for training purpose\n    attented_encoder_training_data, _, _ , _= attented_encoder([captions, image_features], initial_state=[initial_state_h, initial_state_c])\n    \n    \n    training_output_data = output(attented_encoder_training_data)\n\n    training_model = Model(inputs=[image_input,captions_input], outputs=training_output_data)\n    \n    initial_state_inference_model = Model(inputs=[image_input], outputs=[initial_state_h, initial_state_c])\n    \n    inference_initial_state_h = Input(shape=(embedding_size,))\n    inference_initial_state_c = Input(shape=(embedding_size,))\n    attented_encoder_inference_data, inference_encoder_state_h, inference_encoder_state_c, inference_attention = attented_encoder(\n        [captions, image_features],\n        initial_state=[inference_initial_state_h, inference_initial_state_c]\n        )\n   \n    inference_output_data = output(attented_encoder_inference_data)\n\n    inference_model = Model(\n        inputs=[image_input, captions_input, inference_initial_state_h, inference_initial_state_c],\n        outputs=[inference_output_data, inference_encoder_state_h, inference_encoder_state_c, inference_attention]\n    )\n    \n    return training_model, inference_model, initial_state_inference_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 512\nT= None\nL= 16*16\nD= 512\ntraining_model, inference_model, initial_state_inference_model = create_model(vocab_size, embedding_size, T, L, D)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(training_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nepochs = 5\nlr=1e-3\ntraining_model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = training_model.fit_generator( train_dataloader,\n                    validation_data = val_dataloader,\n                    epochs = epochs\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef tokens_to_text(tokens,tok,end_token='endseq'):\n    sentence=\"\"\n    for token in tokens:\n        if token ==0:\n            break\n        \n        word = tok.index_word[token]\n        \n        if word==end_token:\n            break\n            \n        sentence+= word+\" \"\n        \n    sentence = sentence.strip()\n    \n    return sentence\n\n\ndef greedy_inference(input_img, tok,encoder_model, decoder_model,max_len,start_token=\"startseq\",end_token='endseq',decoder_type=\"LSTM\"):\n    if decoder_type=='LSTM':\n        a0,c0  =encoder_model(np.expand_dims(input_img,axis=0))\n    elif decoder_type=='GRU': \n        hidden_layer  =encoder_model(np.expand_dims(input_img,axis=0))\n        \n    word = tok.word_index[start_token]\n    \n    words = []\n    \n    for index in range(max_len):\n        if decoder_type=='LSTM':\n            word_probs , a0,c0 = decoder_model.predict([[np.array([word]),a0,c0]])\n        elif decoder_type=='GRU': \n            word_probs , hidden_layer = decoder_model.predict([[np.array([word]),hidden_layer]])\n            hidden_layer=hidden_layer[0]\n        \n        word = np.argmax(word_probs)\n        \n        if tok.index_word[word]==end_token:\n            break\n        \n        words.append(word)\n        \n    words = tokens_to_text(words,tok)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"startseq\",end_token='endseq', inference_type='greedy',decoder_type='LSTM'):\n    \n    data_loader_iterator = data_loader.__iter__()\n    \n    pred_sentences = []\n    Gt_sentences = []\n    for index, (X,Y) in enumerate(data_loader_iterator):\n        for img,_,sample_y in zip(X[0],X[1],Y):\n            \n            if inference_type=='greedy':\n                pred_sentence = greedy_inference(img, tok,encoder_model, decoder_model,max_len,start_token=\"startseq\",end_token='endseq',decoder_type='LSTM')\n            \n            GT_sentence   = tokens_to_text(sample_y,tok)\n            \n            pred_sentences.append(pred_sentence)\n            Gt_sentences.append(GT_sentence)\n        \n        if index == data_loader.nb_iteration -1:\n            break\n        print(index)\n        \n    return Gt_sentences, pred_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n    BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=(1.0, 0, 0, 0))\n    BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.5, 0.5, 0, 0))\n    BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.3, 0.3, 0.3, 0))\n    BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_from_dataloader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"startseq\",end_token='endseq', inference_type='greedy',decoder_type=\"LSTM\"):\n    Gt_sentences, pred_sentences = get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=start_token,end_token=end_token, inference_type=inference_type,decoder_type=decoder_type)\n    BLEU_1,BLEU_2,BLEU_3,BLEU_4 = calculate_bleu_evaluation(Gt_sentences, pred_sentences)\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_from_dataloader(val_dataloader,tok,encoder_model, decoder_model,max_len,decoder_type=\"LSTM\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seq 2 Seq model\n\n(0.1388382130461867, 0.3726100012696743, 0.553035111006621, 0.6104178906861055)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dr's Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"layers.LSTM?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"define_model(vocab_size, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\n\nfrom tensorflow.keras.applications.vgg16 import preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from MultiCheXNet.data_loader.indiana_dataloader import get_train_validation_generator\n\nmax_vocab_size=10000\nmax_len=100\n\ncsv_path1  =\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\"\ncsv_path2  =\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\"\nimg_path   =\"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/\"\n\ntrain_dataloader, val_dataloader, vocab_size, tok = get_train_validation_generator(csv_path1,csv_path2,img_path, max_vocab_size,max_len,dim=(224,224), preprocess=preprocess_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y= next(enumerate(train_dataloader))[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the captioning model\ndef define_model(vocab_size, max_length):\n    \n    ## encoder\n    inputs1 = layers.Input(shape=(224,224,3))\n    VGG16 = tf.keras.applications.VGG16(\n            include_top=True,\n            weights=\"imagenet\",\n            input_shape=(224,224,3),\n        )\n    \n    VGG16_output = Model(inputs=VGG16.inputs, outputs=VGG16.layers[-2].output)(inputs1)\n    fe1 = layers.Dropout(0.5)(VGG16_output)\n    fe2 = layers.Dense(256, activation='relu')(fe1)\n    \n    \n    #decoder layers\n    embd_layer =  layers.Embedding(vocab_size, 256, mask_zero=True)\n    dropout_layer = layers.Dropout(0.5)\n    lstm_layer = layers.LSTM(256)\n    dense_1 = layers.Dense(256, activation='relu')\n    dense_2 = layers.Dense(vocab_size, activation='softmax')\n    \n    # sequence model\n    inputs2 = layers.Input(shape=(max_length,))\n    se1 = embd_layer(inputs2)\n    se2 = dropout_layer(se1)\n    se3 = lstm_layer(se2)\n    # decoder model\n    decoder1 = layers.add([fe2, se3])\n    decoder2 = dense_1(decoder1)\n    outputs = dense_2(decoder2)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    # summarize model\n    \n    \n    encoder_model = Model(inputs=inputs1, outputs=fe2)\n    \n    \n    #decoder_model \n    img_features = layers.Input(shape=(256,))\n    inputs2 = layers.Input(shape=(1,))\n    \n    se1 = embd_layer(inputs2)\n    se2 = dropout_layer(se1)\n    se3 = lstm_layer(se2)\n    \n    decoder1 = layers.add([img_features, se3])\n    decoder2 = dense_1(decoder1)\n    outputs = dense_2(decoder2) \n    \n    decoder_model = Model(inputs=[inputs2, img_features], outputs=outputs)\n    \n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model,encoder_model, decoder_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model,encoder_model, decoder_model = define_model(vocab_size, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=1e-3\nepochs = 5\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit_generator( \n                    train_dataloader,\n                    epochs = epochs\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}