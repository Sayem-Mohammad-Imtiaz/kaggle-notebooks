{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction of USA Car Accidents Severity\n### by Amirtha Rajan\n<p>Oct 3, 2020</p>"},{"metadata":{},"cell_type":"markdown","source":"<h1> 1 INTRODUCTION</h1>\n<h2> Business Problem </h2>\n<p>The first automobile crash in the United States occurred in New York City in 1896, when a motor vehicle collided with a pedalcycle rider. Later every year the accident count has been increasing but the number of death and number of drivers has never been lower. Now more than 38,000 people die every year in crashes on U.S. roadways. The U.S. traffic fatality rate is 12.4 deaths per 100,000 inhabitants. An additional 4.4 million are injured seriously enough to require medical attention.</p> \n<p>The develpment has lead to safer roads, quick emergency response, imposed traffic rules and of course faster cars nevertheless the rules of the road every drivers obeys are the laws of physics. Still nothing in the course of human revolution has prepared us for the forces involved during a reckless frontal car crash which happens over a tenth of a second And the reason for crash are limitless but some poses are more prevalent than others. Most occured are the speeding and other cases included alcohol.</p>\n<h2> Objective </h2>\n<p>There are ways in which this can be significantly reduced by prediction measures on analysing various range of factors, including weather conditions, special events, roadworks, traffic jams and bumpy and unsafty roads. Thus we can analyse on the patterns and events of occurance from history to predict the severity of accidents that could occur at duration of time at similar locations. This could be the key to provide well informed accurate prediction for the individual who travel though that road and provide better emergency respons, which could reduce significant amount of accidents per year. </p>\n<p> This analysis and accuracy measure that I am gonna build will provide the drivers/people(target audiance) of critical areas well cautioned and plan their route accordingly. By publishing this, the importance and actions plans to be take will be visible to governemt to take all the measures to develop safer roads.</p>"},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c9/Tokyo_traffic_%288231825234%29.jpg\" width=\"90%\"></center>"},{"metadata":{},"cell_type":"markdown","source":"<h1> 2 Data Undestanding</h1>\n<h2>Description</h2>\nThis is a countrywide car accident dataset, which covers 49 states of the USA. The accident data are collected from February 2016 to June 2020, using two APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 3.5 million accident records in this dataset. Check here to learn more about this dataset.\n\n<h3>Acknowledgements<h3>\n\n<p>\n<ul>\n<li>\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “A Countrywide Traffic Accident Dataset.”, 2019.\n</li>\n<li>\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n</li>\n</ul>\n</p>\n\n<h3> Key Findings </h3>\n\n<p> There are many features in the dataset that will be used for prediction :</p>\n<div><ul><li>Side</li>\n<li>Timezone</li>\n<li>Amenity</li>\n<li>Bump</li>\n<li>Crossing</li>\n<li>Give_Way</li>\n<li>Junction</li>\n<li>No_Exit</li>\n<li>Railway</li>\n<li>Roundabout</li>\n<li>Sevirity</li>\n<li>Station</li>\n<li>Stop</li>\n<li>Traffic_Calming</li>\n<li>Traffic_Signal</li>\n<li>Sunrise_Sunset</li>\n<li>Civil_Twilight</li>\n<li>Nautical_Twilight</li>\n<li>Astronomical_Twilight<li>...</ul></div>\n\n<p><center><img src=\"/kaggle/input/usaccidentprediction-data/img/graph.jpg\" width=\"80%\"></center></p>"},{"metadata":{},"cell_type":"markdown","source":"<h2> Dataset Overview </h2>\n<p>The data is provided in terms of a CSV file. Following table describes the data attributes:</p>"},{"metadata":{},"cell_type":"markdown","source":"<table>\n    <thead>\n        <tr>\n            <th style=\"text-align: center\">#</th>\n            <th style=\"text-align: center\">Attribute</th>\n            <th>Description</th>\n            <th style=\"text-align: center\">Nullable</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td style=\"text-align: center\">1</td>\n            <td style=\"text-align: center\">ID</td>\n            <td>This is a unique identifier of the accident record.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">2</td>\n            <td style=\"text-align: center\">Source</td>\n            <td>Indicates source of the accident report (i.e. the API which reported the accident).</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">3</td>\n            <td style=\"text-align: center\">TMC</td>\n            <td>A traffic accident may have a Traffic Message Channel (TMC) code which provides more detailed description of\n                the event.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">4</td>\n            <td style=\"text-align: center\">Severity</td>\n            <td>Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on\n                traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic\n                (i.e., long delay).</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">5</td>\n            <td style=\"text-align: center\">Start_Time</td>\n            <td>Shows start time of the accident in local time zone.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">6</td>\n            <td style=\"text-align: center\">End_Time</td>\n            <td>Shows end time of the accident in local time zone. End time here refers to when the impact of accident\n                on traffic flow was dismissed.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">7</td>\n            <td style=\"text-align: center\">Start_Lat</td>\n            <td>Shows latitude in GPS coordinate of the start point.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">8</td>\n            <td style=\"text-align: center\">Start_Lng</td>\n            <td>Shows longitude in GPS coordinate of the start point.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">9</td>\n            <td style=\"text-align: center\">End_Lat</td>\n            <td>Shows latitude in GPS coordinate of the end point.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">10</td>\n            <td style=\"text-align: center\">End_Lng</td>\n            <td>Shows longitude in GPS coordinate of the end point.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">11</td>\n            <td style=\"text-align: center\">Distance(mi)</td>\n            <td>The length of the road extent affected by the accident.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">12</td>\n            <td style=\"text-align: center\">Description</td>\n            <td>Shows natural language description of the accident.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">13</td>\n            <td style=\"text-align: center\">Number</td>\n            <td>Shows the street number in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">14</td>\n            <td style=\"text-align: center\">Street</td>\n            <td>Shows the street name in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">15</td>\n            <td style=\"text-align: center\">Side</td>\n            <td>Shows the relative side of the street (Right/Left) in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">16</td>\n            <td style=\"text-align: center\">City</td>\n            <td>Shows the city in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">17</td>\n            <td style=\"text-align: center\">County</td>\n            <td>Shows the county in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">18</td>\n            <td style=\"text-align: center\">State</td>\n            <td>Shows the state in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">19</td>\n            <td style=\"text-align: center\">Zipcode</td>\n            <td>Shows the zipcode in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">20</td>\n            <td style=\"text-align: center\">Country</td>\n            <td>Shows the country in address field.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">21</td>\n            <td style=\"text-align: center\">Timezone</td>\n            <td>Shows timezone based on the location of the accident (eastern, central, etc.).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">22</td>\n            <td style=\"text-align: center\">Airport_Code</td>\n            <td>Denotes an airport-based weather station which is the closest one to location of the accident.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">23</td>\n            <td style=\"text-align: center\">Weather_Timestamp</td>\n            <td>Shows the time-stamp of weather observation record (in local time).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">24</td>\n            <td style=\"text-align: center\">Temperature(F)</td>\n            <td>Shows the temperature (in Fahrenheit).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">25</td>\n            <td style=\"text-align: center\">Wind_Chill(F)</td>\n            <td>Shows the wind chill (in Fahrenheit).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">26</td>\n            <td style=\"text-align: center\">Humidity(%)</td>\n            <td>Shows the humidity (in percentage).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">27</td>\n            <td style=\"text-align: center\">Pressure(in)</td>\n            <td>Shows the air pressure (in inches).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">28</td>\n            <td style=\"text-align: center\">Visibility(mi)</td>\n            <td>Shows visibility (in miles).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">29</td>\n            <td style=\"text-align: center\">Wind_Direction</td>\n            <td>Shows wind direction.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">30</td>\n            <td style=\"text-align: center\">Wind_Speed(mph)</td>\n            <td>Shows wind speed (in miles per hour).</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">31</td>\n            <td style=\"text-align: center\">Precipitation(in)</td>\n            <td>Shows precipitation amount in inches, if there is any.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">32</td>\n            <td style=\"text-align: center\">Weather_Condition</td>\n            <td>Shows the weather condition (rain, snow, thunderstorm, fog, etc.)</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">33</td>\n            <td style=\"text-align: center\">Amenity</td>\n            <td>A POI annotation which indicates\n                presence of amenity in a nearby location.\n            </td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">34</td>\n            <td style=\"text-align: center\">Bump</td>\n            <td>A POI annotation which indicates presence of speed bump or hump in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">35</td>\n            <td style=\"text-align: center\">Crossing</td>\n            <td>A POI annotation which indicates presence of crossing in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">36</td>\n            <td style=\"text-align: center\">Give_Way</td>\n            <td>A POI annotation which indicates presence of give_way in a nearby location.\n            </td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">37</td>\n            <td style=\"text-align: center\">Junction</td>\n            <td>A POI annotation which indicates presence of junction in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">38</td>\n            <td style=\"text-align: center\">No_Exit</td>\n            <td>A POI annotation which indicates presence of no_exit in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">39</td>\n            <td style=\"text-align: center\">Railway</td>\n            <td>A POI annotation which indicates presence of railway in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">40</td>\n            <td style=\"text-align: center\">Roundabout</td>\n            <td>A POI annotation which indicates presence of roundabout in a nearby\n                location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">41</td>\n            <td style=\"text-align: center\">Station</td>\n            <td>A POI annotation which indicates presence of station in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">42</td>\n            <td style=\"text-align: center\">Stop</td>\n            <td>A POI annotation which indicates presence of stop in a nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">43</td>\n            <td style=\"text-align: center\">Traffic_Calming</td>\n            <td>A POI annotation which indicates presence of traffic_calming in a nearby\n                location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">44</td>\n            <td style=\"text-align: center\">Traffic_Signal</td>\n            <td>A POI annotation which indicates presence of traffic_signal in a\n                nearby location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">45</td>\n            <td style=\"text-align: center\">Turning_Loop</td>\n            <td>A POI annotation which indicates presence of turning_loop in a nearby\n                location.</td>\n            <td style=\"text-align: center\">No</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">46</td>\n            <td style=\"text-align: center\">Sunrise_Sunset</td>\n            <td>Shows the period of day (i.e. day or night) based on sunrise/sunset.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">47</td>\n            <td style=\"text-align: center\">Civil_Twilight</td>\n            <td>Shows the period of day (i.e. day or night) based on civil twilight.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">48</td>\n            <td style=\"text-align: center\">Nautical_Twilight</td>\n            <td>Shows the period of day (i.e. day or night) based on nautical twilight.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n        <tr>\n            <td style=\"text-align: center\">49</td>\n            <td style=\"text-align: center\">Astronomical_Twilight</td>\n            <td>Shows the period of day (i.e. day or night) based on astronomical twilight.</td>\n            <td style=\"text-align: center\">Yes</td>\n        </tr>\n    </tbody>\n</table>"},{"metadata":{},"cell_type":"markdown","source":"<h1> Data Cleaning and Processing </h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install folium","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nimport plotly.express as px\nimport json\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/us-accidents/US_Accidents_Dec20.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Understanding the Data </h2>"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print(\"The shape of the data is \",df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Checking Source data </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Severity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>We have three different sources MapQuest, Bing and MapQuest-Bing form which the data are gathered , so there are possibilities where a source might no have a categorical values. First lets check the severity based on the Source as Severity is our objective of our model. So lets check the target data column of our dataset. </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_source = df.groupby(['Severity','Source']).size().reset_index()\ndf_source = df_source.pivot(columns='Severity',index='Source',values=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_source","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> We can see that MapQuest has large dataset followed by Bing and lastly the MapQest-Bing with lowest dataset. Howerever each data source might have categorized based on different severity conditions, lets check the severity cnditions based on different features from our dataset. </p>"},{"metadata":{},"cell_type":"markdown","source":"<h2> Feature Identification </h2>\n<p> Now we have to identify the features from our data set that would fit for severity condition. Lets first categorize the features based on generalized categories. We should also remove the useless features which doesn't relate to our model.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Traffic_Attr = ['ID','Source','TMC','Severity','Start_Time','End_Time','Start_Lat','Start_Lng','End_Lat','End_Lng','Distance(mi)','Description']\nAddress_Attr = ['Number','Street','Side','City','County','State','Zipcode','Country','Timezone']\nWeather_Attr = ['Airport_Code','Weather_Timestamp','Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Direction','Wind_Speed(mph)','Precipitation(in)','Weather_Condition']\nPOI_Attr = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop']\nDayTime_Attr = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have undersood what the data has and what can be done with each features... Our next step will be cleaning and removing unnessery data and then see the realtion between the severiy and cleaned feature dataset."},{"metadata":{},"cell_type":"markdown","source":"<h1> Data Cleaning </h1>\nFor the Traffic category the feature <b>ID</b>(just a unique number), <b>Source</b> and <b>TMC</b> will not provide any information for accident prediction so lets drop these columns\n\n<b>Note: we are cleaning only the unnecessary columns for now, Other columns like description... etc will be used for Visualization and droped after visualizing</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['ID','TMC','Source'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Traffic_Attr = list(filter(lambda x: x not in ['ID','TMC','Source'],Traffic_Attr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can remove the country column as the data is based on single country... And we can also check if there are any other columns with single categorical value that can be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Country',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Clean the Categorical Features</h2>\n<p>Lets convert some fo the labeled fetures to categorical values as using Frequency encoding and log-transform:</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fre_list = ['Street', 'City', 'County', 'Zipcode', 'Airport_Code']\nfor i in fre_list:\n  newname = i + '_Fq'\n  df[newname] = df.groupby([i])[i].transform('count')\n  df[newname] = df[newname]/df.shape[0]*df[i].unique().size\n  df[newname] = df[newname].apply(lambda x: np.log(x+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have weather categorical features so lets clean it up "},{"metadata":{"trusted":true},"cell_type":"code","source":"Weather_Attr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wind direction \nThis data is a categorical Feature so lets check and clean the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Wind_Direction'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>After analysing all the weather categorical columns the <b>Wind_Direction</b> and <b>Weather_Condition</b> has some messy data which can be cleaned. So lets replace those categorical values to represent the wind direction of similar format</p>"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"%%time\ndf['Wind_Direction'] = df['Wind_Direction'].apply(lambda x : 'N' if str(x) in ['North','WNW','NNE'] else x)\ndf['Wind_Direction'] = df['Wind_Direction'].apply(lambda x : 'E' if str(x) in ['East','ESE','ENE'] else x)\ndf['Wind_Direction'] = df['Wind_Direction'].apply(lambda x : 'W' if str(x) in ['West','WSW','WNW'] else x)\ndf['Wind_Direction'] = df['Wind_Direction'].apply(lambda x : 'S' if str(x) in ['South','SSW','SSE'] else x)\ndf['Wind_Direction'] = df['Wind_Direction'].apply(lambda x : 'VAR' if str(x) == 'Variable' else x)\ndf['Wind_Direction'] = df['Wind_Direction'].apply(lambda x : 'CALM' if str(x) == 'Calm' else x)\nprint(\"Wind Direction after cleaning: \", df['Wind_Direction'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Weather Condition\nBad weathers increases the chances of an accident hence it would be a key factor in our dataset. Let look in to the features values of it"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Weather_Condition'] = df['Weather_Condition'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create features for all weather conditions"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"%%time\ndf['Clear'] = np.where(df['Weather_Condition'].str.contains('Clear|Fair', case=False, na = False), 1, 0)\ndf['Cloudy'] = np.where(df['Weather_Condition'].str.contains('Cloud|Overcast', case=False, na = False), 1, 0)\ndf['Light_Rain'] = np.where(df['Weather_Condition'].str.contains(r'(?=.*Light)(?=.*Rain)|(?=.*Light)(?=.*Drizzle)|(^(?!Light)(?!Heavy).*Drizzle)', case=False, na = False,regex=True), 1, 0)\ndf['Rain'] = np.where(df['Weather_Condition'].str.contains(r'(^(?!Light)(?!Heavy)(?!Snow)(?!Rain Shower).*Rain)|(^(?!Light)(.*Heavy).*Drizzle)$', case=False, na = False,regex=True), 1, 0)\ndf['Heavy_Rain'] = np.where(df['Weather_Condition'].str.contains(r'(^(.*Heavy).*Rain.*)|(^(.*Rain).Shower*)|(^.*Heavy T-Storm.*)$', case=False, na = False,regex=True), 1, 0)\ndf['Light_Snow'] = np.where(df['Weather_Condition'].str.contains(r'(?=.*Light)(?=.*Snow)|(?=.*Light)(?=.*Sleet)|(?=.*Light)(?=.*Hail)|(?=.*Light)(?=.*Ice)|(^(.*Wintry Mix.*))', case=False, na = False,regex=True), 1, 0)\ndf['Snow'] = np.where(df['Weather_Condition'].str.contains(r'(^(?!Light)(?!Heavy)(?!Rain).*Snow)|(^(?!Light)(?!Heavy).*Sleet)|(^(?!Light)(?!Heavy).*Hail)|(^(?!Light)(?!Heavy).*Ice.*)$', case=False, na = False,regex=True), 1, 0)\ndf['Heavy_Snow'] = np.where(df['Weather_Condition'].str.contains(r'(^(.*Heavy).*Snow.*)|(^(.*Havey).*Sleet.*)|(^.*Heavy.*Ice.*)|(^(?!Light).*Snow.*Shower.*)|(^(?!Light).*squalls.*)$', case=False, na = False,regex=True), 1, 0)\ndf['Fog'] = np.where(df['Weather_Condition'].str.contains('Fog|Mist|Haze|Smoke', case=False, na = False), 1, 0)\ndf['Windy'] = np.where(df['Weather_Condition'].str.contains('Windy|Dust|Sand|Tornado|Ash', case=False, na = False), 1, 0)\ndf['Thunderstrom'] = np.where(df['Weather_Condition'].str.contains('Thunder|T-Storm', case=False, na = False), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weathers = ['Clear','Cloudy','Light_Rain','Rain','Heavy_Rain','Light_Snow','Snow','Heavy_Snow','Fog','Windy','Thunderstrom']\nfor weather in weathers:\n    df.loc[df['Weather_Condition'].isnull(),weather] = df.loc[df['Weather_Condition'].isnull(),'Weather_Condition']\ndf.loc[:,['Weather_Condition'] + weathers]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding Missing Data\nLets find the missing percentage of all columns with isnull check and decide on which column can be removed or recovered with calculated data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_null_percentage():\n    null_data = pd.DataFrame(df.isnull().sum()).reset_index()\n    null_data.columns = ['Features','Missing_Percent']\n    null_data['Missing_Percent'] = null_data['Missing_Percent'].apply(lambda x: x / df.shape[0] * 100)\n    null_data.sort_values('Missing_Percent',ascending=False,inplace=True)\n    return null_data.loc[null_data['Missing_Percent']>0,:]","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"check_null_percentage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the columns <b>End_Lat</b> and <b>End_Lng</b> has 70% of missing data which can be removed as it's not much valuable for our prediction model, <b>Number</b>\thas 64% of missing data and can be removed as this would not be used for analysis. <b>Precipitation(in)</b> has 57% of missing data but this one is a key factor for rain/snow so can be recovered with these feature columns but the preciptation must be changed to categorical value. And for <b>Wind_Chill(F)</b> the percentage of missing data is 53, considering the fact of this feature, this can also be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['End_Lat','End_Lng','Number','Wind_Chill(F)']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping thse columns as it has more missing values\ndf.drop(['End_Lat','End_Lng','Number','Wind_Chill(F)'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filling Missing values of Precipitation depending on Weather Conditions\nSince Precipitation totally depends on Weather Condition we can use our categorical features of weather to predict and fill the precipitation\n<p>These are the <b>Precipitation(in)</b> based on rain\n<ul>\n<p>Light rain — when the precipitation rate is < 2.5 mm (0.098 in) per hour</p>\n<p>Moderate rain — when the precipitation rate is between 2.5 mm (0.098 in) - 7.6 mm (0.30 in) or 10 mm (0.39 in) per hour</p>\n<p>Heavy rain — when the precipitation rate is > 7.6 mm (0.30 in) per hour, or between 10 mm (0.39 in) and 50 mm (2.0 in) per hour</p>\n<p>Violent rain — when the precipitation rate is > 50 mm (2.0 in) per hour</p>\n</ul>\nFor each type of Rain we will find the mean and median and fill with random values between them</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the type os we hold weathers befoe perdicting preciptation\nweathers","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# Iterating over weather to find the missing preciptation at its respective weather condition\nfor weather in weathers:\n    precipitations = df.loc[(df['Precipitation(in)'] != 0.0) & (df[weather] == 1) & ~(df['Precipitation(in)'].isnull()),'Precipitation(in)']\n    # get the mean and standard deviation of available precipitation for the weather\n    mean = precipitations.mean()\n    std = precipitations.std()\n    missing_data_size = df.loc[(df['Precipitation(in)'] != 0.0) & (df[weather] == 1) & (df['Precipitation(in)'].isnull()),'Precipitation(in)'].shape[0]\n    # perform normal distribution between the mean and standard deviation to fill the values\n    df.loc[(df['Precipitation(in)'] != 0.0) & (df[weather] == 1) & (df['Precipitation(in)'].isnull()),'Precipitation(in)'] = np.random.normal(mean,std,missing_data_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding NAN values\nNow lets check for all nan values for other features columns as missing percentage value."},{"metadata":{"trusted":true},"cell_type":"code","source":"check_null_percentage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> All the nan values are part of <b>Weather_Attr</b> feature, And these features are continuous value as well with time.</p> For a time series values the nan values can be filled by taking the value of median.<p> To derive this calculation we should group the data by regions as the values differ for different locations thus by grouping these data by Airport Code we might get an approximate median of the data, as Airport_code is identical for data in similar region.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the null values of these two features to decide which ne to drop\ndf[[\"Weather_Timestamp\",'Start_Time']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Drop Weather_TimeStamp as Start_Time has all the necessary values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Weather_Timestamp',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling DateTime features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Start_Time and End_Time column to datetime format\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'],errors='coerce')\ndf['End_Time'] = pd.to_datetime(df['End_Time'],errors='coerce')\ndf.sort_values('Start_Time',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating features with respect to time\ndf['Year'] = df['Start_Time'].dt.year\ndf['Month'] = df['Start_Time'].dt.month\ndf['Hour'] = df['Start_Time'].dt.hour\ndf['Weekday']= df['Start_Time'].dt.weekday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Year','Month','Hour','Weekday']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling NAN for other Weather Attribute features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the weather attributes\ndf[['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']]","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"%%time\n# replace the missing nan values with normal distribution depending on different time period\nweather_props = ['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\nfor weather_prop in weather_props:\n    # Take the median of the weather condition for the hour in the month to fillna\n    df[weather_prop] = df.groupby(['Airport_Code','Year','Month','Hour'])[weather_prop].apply(lambda x: x.fillna(x.median()))\n    print( weather_prop + \" fill type 1 for Nan  : \" + df[weather_prop].isnull().sum().astype(str))\n    # Take the median of the weather condition for a month in the year to fillna\n    df[weather_prop] = df.groupby(['Airport_Code','Year','Month'])[weather_prop].apply(lambda x: x.fillna(x.median()))\n    print( weather_prop + \" fill type 2 for Nan : \" + df[weather_prop].isnull().sum().astype(str))\n    # Take the median of the weather condition for a seasonal month to fillna\n    df[weather_prop] = df.groupby(['Airport_Code','Month'])[weather_prop].apply(lambda x: x.fillna(np.random.normal(x.median() if x.mean() > x.median() else x.mean() ,x.std(),1)[0]))\n    print( weather_prop + \" fill type 3 for Nan : \" + df[weather_prop].isnull().sum().astype(str))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Weather direction is a cateorical feature with labels which needs a different approach to fill in the values, considering this scenario we can use most common function form counter to fill the values"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"%%time\n# grouping data to fill NAs with majority value\ndf['Wind_Direction'] = df.groupby(['Airport_Code','Year','Month','Hour'])['Wind_Direction'].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\nprint('Wind_Direction' + \" fill type 1 for Nan : \" + df['Wind_Direction'].isnull().sum().astype(str))\ndf['Wind_Direction'] = df.groupby(['Airport_Code','Year','Month'])['Wind_Direction'].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\nprint('Wind_Direction' + \" fill type 2 for Nan : \" + df['Wind_Direction'].isnull().sum().astype(str))\ndf['Wind_Direction'] = df.groupby(['Airport_Code','Month'])['Wind_Direction'].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\nprint('Wind_Direction' + \" fill type 3 for Nan : \" + df['Wind_Direction'].isnull().sum().astype(str))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still have few missing values but the nan count is less compared to the total so lets dropna for the features"},{"metadata":{},"cell_type":"markdown","source":"### It's a best practice to drop a row with a nan value for feature at the end of part of our Data Cleaning section so the remaing feature cloumns with values can be made use of for gettng mean , median etc on that feature column"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_null_percentage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets drop all the remaining NAN values as the percentage of missing data is less than 1%\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLets view the Severity counts from our dataset"},{"metadata":{},"cell_type":"markdown","source":"<h1>Visualizations</h1>\n<h2>Map Visualization</h2>\n\n## Accident Count Visualization by State"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the statesfroma JSON file containing info of US states name, abbr and code\nwith open(r'/kaggle/input/usaccidentprediction-data/us-states.json') as f:\n    geojson = json.load(f)\nstate_df = pd.read_csv(\"/kaggle/input/usaccidentprediction-data/states.csv\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# creating a new column for state name by getting the state name for its code from state_df \nfor ind in state_df.index:\n    df['State_Name'] = df['State'].replace(state_df.iloc[ind,2],state_df.iloc[ind,0])\nfor ind in state_df.index:\n    df['State_Name'] = df['State_Name'].replace(state_df.iloc[ind,2],state_df.iloc[ind,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a data for ploting collistion count based on states\nchrolopeth_data = df.groupby('State_Name')['State'].value_counts().to_frame()\nchrolopeth_data.columns = ['Count']\nchrolopeth_data.reset_index(inplace=True)\nchrolopeth_data.drop('State',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create figre for dimensions\nf = folium.Figure(width=1000, height=500)\n\n# Initialize the map\nchrolopeth_map = folium.Map(location=[39, -99.382017], zoom_start=4.5).add_to(f)\n \n# Add the color and otherattributes for the chloropleth:\nchrolopeth_map.choropleth(\n geo_data=geojson,\n name='choropleth',\n data=chrolopeth_data,\n columns=['State_Name','Count'],\n key_on='feature.properties.name',\n fill_color='Spectral',\n fill_opacity=0.7,\n line_opacity=0.2,\n legend_name='Accident Count'\n)\n\n# add the layout\nfolium.LayerControl().add_to(chrolopeth_map)\n\n#show\nchrolopeth_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HeatMap Visualization on US Accidents"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set the dimenstion for the map\nf = folium.Figure(width=1100, height=500)\n\n#Creating map of our locaiton of choice Fairfield County\ndf_map = folium.Map(location=[41.288241, -99.382017],tiles = 'Stamen Terrain', zoom_start=4).add_to(f)\n\n#Subsetting data for visualization\ndf_Heat=df.loc[df['Year'] == 2016,['Start_Lat','Start_Lng']]\n\n#Creating and Attaching heatmap to our map\nHeat_data=[[row['Start_Lat'],row['Start_Lng']] for index, row in df_Heat.iterrows()]\nHeatMap(Heat_data,blur=10,radius=15,gradient={0.4: 'green', 0.65: 'yellow', 1: 'red'}).add_to(df_map)\n\n#show\ndf_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accident Severity by County"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import geo-json data for county\nwith open(r'/kaggle/input/usaccidentprediction-data/geojson-counties.json') as f:\n    counties = json.load(f)\n\n# plot the map for collision count based on county\nfig = px.choropleth_mapbox(df, geojson=counties,locations='County',\n                            color='Severity',\n                            featureidkey=\"properties.NAME\",\n                            color_continuous_scale=\"Viridis\",\n                            range_color=(1, 4),\n                            mapbox_style=\"carto-positron\",\n                            zoom=3, center = {\"lat\": 37.0902, \"lon\": -95.7129},\n                            opacity=0.5,\n                            labels={'Severity':'Severity Level'},\n                            title=\"Accident Severity in County\"\n                            )\n# update layout for the map\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},width=1000,height=600)\n\n#show map\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Visualization Through Charts with Feature Attributes</h2>\n\n## HeatMap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# find correlation for all the features \nsns.heatmap(df[['Severity','Start_Lat','Start_Lng','Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)','Precipitation(in)','Weekday','Weather_Condition']].corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pie Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the figure dimentions\nf,ax=plt.subplots(1,2,figsize=(20,8))\n\n#get severity count and plot pir chart\ndf['Severity'].value_counts().plot.pie(explode=[0,0,0.1,0],autopct='%1.1f%%',ax=ax[0],shadow=True,colors=['#4daf4a','#377eb8','#ff7f00','#e41a1c'])\n\n# add title, labels and legends\nax[0].legend(['Severity 1','Severity 2','Severity 3','Severity 4'],\n          title=\"Level\",\n          loc=\"upper left\",\n          bbox_to_anchor=(1, 0, 0.5, 1))\nax[0].set_title('Percentage Severity Distribution')\nax[0].set_ylabel('Count')\n# plot count plot for severity\nsns.countplot('Severity',data=df,ax=ax[1],palette='Set1')\nax[1].set_title('Count of Severity')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accident Count in Time"},{"metadata":{},"cell_type":"markdown","source":"Visualizeing the accident for different time periods"},{"metadata":{"trusted":true},"cell_type":"code","source":"#group by year and get each month count \nyear_plot = df.groupby(['Year'])['Month'].value_counts().to_frame().rename(columns={'Month':'Count'})\nyear_plot.reset_index(inplace=True)\nplt.figure(figsize=(12,8))\n\n# plot the graph for the data\nax = sns.pointplot(x='Month', y='Count', hue='Year',data=year_plot)\n\n# set labels and title\nax.set_xlabel('Months')\nax.set_ylabel('Accident Count')\nplt.title('Accident Count in Months for Years 2016(Feb) - 2020(Jun)',fontdict={'size':16})\n\n#show graph\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accident Severity Counts in Year"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the figure size\nplt.figure(figsize=(15,5))\n\n# create count plot for severity for each year\nsns.countplot(x='Year', hue='Severity', data=df)\n\n# set the ticks, labels and title\nplt.title('Accident Severity Count by Year', size=15, y=1.05)\nplt.ylabel(\"Number of Accidents\")\nplt.yticks(np.linspace(25000,700000,8))\n#plt.yticks(np.power(2, np.linspace(14,19.5,8,endpoint=True)).astype(int))\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Severity Count by Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import calendar for getting the month and week names\nimport calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the plot size\nplt.figure(figsize=(20,5))\n\n# create count plot for severity for each month\nsns.countplot(x='Month', hue='Severity', data=df)\n\n# set itle, labels and ticks\nplt.title('Accident Severity Count by Month', size=15, y=1.05)\nplt.ylabel(\"Number of Accidents\")\nplt.xticks(np.arange(0,12,1),calendar.month_name[1:13])\nplt.yticks(np.linspace(25000,250000,6))\n\n#show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Severity Count by Weekdays"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set fig size\nplt.figure(figsize=(15,5))\n\n# create count plot for severity in the weekdays \nsns.countplot(x='Weekday', hue='Severity', data=df)\n\n# set labels, ticks and title\nplt.title('Accident Severity Count by Weekdays', size=15, y=1.05)\nplt.ylabel(\"Accident Counts\")\nplt.xticks(np.arange(7),calendar.day_name[0:7])\nplt.yticks(np.linspace(25000,400000,6))\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Severity Count by Hours of a Day"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the accident count for each hour\ndf_hour = df['Hour'].value_counts().to_frame().reset_index().sort_values('Hour')\ndf_hour.columns  = ['Hour','Counts']\n\n# set fig size\nplt.figure(figsize=(20,5))\n#set grid style\nsns.set_style(\"darkgrid\")\n# crete a line plot for accident count for each hours as a time series\nsns.lineplot(x='Hour', y='Counts', data=df_hour)\n# set label, ticks and title\nplt.xticks(df_hour['Hour'])\nplt.ylabel('Accident Count',size=15)\nplt.xlabel('Hour',size=15)\nplt.title('Number of Accidents by Hour', size=18, y=1.05)\n# show graph\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Severity Count by Twilight Hours"},{"metadata":{"trusted":true},"cell_type":"code","source":"# defie the list twilight hours\nperiod_features = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']\n\n# create sub plots for eahc twilight\nfig, axs = plt.subplots(ncols=1, nrows=4, figsize=(20, 5))\n\n# set spacing\nplt.subplots_adjust(wspace = 0.5)\n\n# Iterate thrugh each feature and create plot\nfor i, feature in enumerate(period_features, 1):    \n    plt.subplot(1, 4, i)\n    sns.countplot(x=feature, hue='Severity', data=df)\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count in Log Scale', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    # set the y axis to log scale as few categpries has very log value and hard to read\n    plt.yscale('log')\n    # set the legends for severity and locate it\n    plt.legend(['1','2','3','4'], loc='upper left', prop={'size': 8})\n    plt.title('Count of Severity in\\n{} Feature'.format(feature), size=13, y=1.05)\n# assige a main title\nfig.suptitle('Count of Accidents by Period-of-Day',y=1.08, fontsize=16)\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accident And Severity Count by Weather Conditions"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# get the collision count for each weather\nweather_count = [{ weather : df[weather].values.sum() for weather in weathers }][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the key and values seperatly\nkeys = [key for key in weather_count.keys()]\nvalues = [value for value in weather_count.values()]\n# set fig sze\nfig , ax = plt.subplots(figsize=(15,5))\n# create a bar plot\nbars = plt.bar(keys, [value for value in values],color=sns.color_palette(\"Spectral\", len(values)))\n# set count labels for each bar\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x()+0.4,yval, yval,ha='center')\n# set axis labels and title\nax.set_ylabel(\"Accident Counts in Log Scale\")\nax.set_xlabel(\"POIs\")\n# set the y scale to log as some catgeory has low values and makes it hard to view\nax.set(yscale=\"log\")\nax.set_title(\"Number of Accidents for Different Weather Conditions\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The collision is more on the cloudy days followed by clear day"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# define figure size with subplots for each weather\nfig, axs = plt.subplots(ncols=2, nrows=4, figsize=(20, 10))\n# set spaceing for the subplots\nplt.subplots_adjust(hspace=0.4,wspace = 0.6)\n# define the common weathers excluding thunderstrom and fog\ncommon_weathers = ['Clear','Cloudy','Light_Rain','Rain','Heavy_Rain','Light_Snow', 'Snow', 'Heavy_Snow']\n\n# Iterate through each weatehr and plot the counts\nfor i, feature in enumerate(common_weathers,1):  \n    plt.subplot(2, 4, i)\n    # create count plot\n    sns.countplot(x=feature, hue='Severity', data=df ,palette=\"Set1\")\n    # set labels , ticks , title and legends\n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count in log scale', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.legend(['1','2','3','4'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in \\n {} Weather'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents by Weather Features', fontsize=18)\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accident And Severity Count for POIs"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# define the poi features\nPOI_features = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal']\n# get the collision count for each feature\npoi_count = [{ poi : df[poi].values.sum() for poi in POI_features }][0]\n# key the keys and values seperately\nkeys = [key for key in poi_count.keys()]\nvalues = [value for value in poi_count.values()]\nfig , ax = plt.subplots(figsize=(18,6))\n# create a bar plot\nbars = plt.bar(keys, [value for value in values],color=sns.color_palette(\"Spectral\", len(values)))\n# iterate over each bar to label its value\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x()+0.4,yval, yval,ha='center')\n# set y axis to log scale\nplt.yscale('log')\n# set labels and title\nax.set_ylabel(\"Accident Counts\")\nax.set_xlabel(\"POIs\")\nax.set_title(\"Number of Accidents at Different POIs\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accidnet count is more at the traffic signals followed byt Junctions and Crossings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define sub plot for each poi\nfig, axs = plt.subplots(ncols=3, nrows=4, figsize=(20, 10))\n# set plot spacing\nplt.subplots_adjust(hspace=0.5,wspace = 0.5)\n# iterate through each feature and creat count plot for a subplot of the feature\nfor i, feature in enumerate(POI_features, 1):    \n    df_filter = df.loc[df[feature] == True,[feature,'Severity']]\n    plt.subplot(3, 4, i)\n    # create count plot\n    sns.countplot(x=feature, hue='Severity', data=df_filter ,palette=\"Set1\")\n    # set label, title , ticks and legernds\n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count in log scale', size=12, labelpad=3)    \n    plt.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)  \n    plt.tick_params(axis='y', labelsize=12)\n    plt.yscale(\"log\")\n    plt.legend(['1', '2','3','4'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in {}'.format(feature), size=14, y=1.05)\n# set main title\nfig.suptitle('Count of Accidents in POI Features',y=1.02, fontsize=16)\n\n# show plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set figure size\nplt.figure(figsize=(15,15))\n\n# plot correlation heatmap\nsns.heatmap(df.corr(),annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Turning_Loop',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Model & Evaluation </h1>\n<h2> Resample data </h2>\nWe will be using <b>100000</b> of the data for model as the original data has 4.1 million records which is a huge load to process the data for lower machines."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model = pd.concat([df[df['Severity']==4].sample(75000,replace=True,random_state=42),df[df['Severity']!=4].sample(75000)], axis=0)\ndf_model['Severity4'] = 0\ndf_model.loc[df_model['Severity'] == 4, 'Severity4'] = 1\ndf_model.drop('Severity',axis=1,inplace=True)\ndf_model.rename(columns={'Severity4':'Severity'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(df_model.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-hot Encoding for Categorical Data\nFrom the above visualization we can now build our Model but lastly lets categrize some of our Features with one-hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model = df_model.replace([True, False], [1,0])\n\n# one-hot encoding\ndf_model[period_features] = df_model[period_features].astype('category')\ndf_model = pd.get_dummies(df_model, columns=period_features, drop_first=True)\n\ncat = ['Side','State','Timezone','Wind_Direction', 'Weekday', 'Month', 'Hour']\ndf_model[cat] = df_model[cat].astype('category')\ndf_model = pd.get_dummies(df_model, columns=cat, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some features needs to be standardized based on unit variance.\n# Standardizing the features based on unit variance\n\ndf_model = df_model.drop(['Description','Street', 'City', 'County', 'Zipcode', 'Airport_Code','Weather_Condition','State_Name','Start_Time','End_Time','Distance(mi)'],axis=1)\n\nX = df_model.drop('Severity', axis=1)\ny= df_model['Severity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding numerical features to categrical if the unique value count in a feature is less tha are equal to 10\ndummy_feature = pd.Series(df_model.nunique().sort_values(),name='Count').to_frame().query('Count == 2').index.values\ndummy_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Train and Split data </h2>\nLets split the data with 80% train dataset and 20% for test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardizing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_scale = list(set(X_train.columns).difference(set(dummy_feature)))\nfeatures_to_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a StandardScaler object\nscaler = StandardScaler()\n\n# Fit and transform our train data with Standard Scaler\nX_train[features_to_scale] = StandardScaler().fit_transform(X_train[features_to_scale])\n\n# transform our test data with the same scaler object\nX_test[features_to_scale] = StandardScaler().fit_transform(X_test[features_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform correlation on the train data set for the predictor\ncorrelation_df = pd.concat([X_train,y_train],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a method to get the top correlated features\ndef get_top_corr_features():\n    # creating a correlation list for the top features with SalesPrice\n    corr_cols = correlation_df.corr().loc[:,'Severity'].sort_values(ascending=False)\n    corr_cols = corr_cols.reset_index()\n    # order the data with positive corr first and negative corr last\n    corr_cols = corr_cols[corr_cols.Severity>0].append(corr_cols[corr_cols.Severity<0])\n    return corr_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_top_corr_features()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get teh top 30 +ve corelation appended with top 25 most +ve corr freature and lastly the reaming features\ncorr_cols = get_top_corr_features().head(15).append(get_top_corr_features().tail(15).sort_values('Severity',ascending=True))[1:].reset_index(drop=True)\n# rename columns\ncorr_cols.columns = ['Corr Feature','Severity Corr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[corr_cols['Corr Feature']]\nX_test = X_test[corr_cols['Corr Feature']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install mlxtend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, jaccard_score, f1_score, log_loss, precision_score, recall_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree, neighbors\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.ensemble import RandomForestRegressor\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\nLets start predicting Accident with the classic model of regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_scores = []\njaccard_scores = []\nf1_scores = []\nprecision_scores = []\nrecall_scores = []\ntimes = []","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(max_iter=10000)\nclf.fit(X_train, y_train)\n\naccuracy_train = clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\naccuracy_test = clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))\n\ny_pred = clf.predict(X_test)\n\nconfmat = multilabel_confusion_matrix(y_true=y_test, y_pred=y_pred,labels=[1])\n\nconf_matrix = pd.DataFrame(data=confmat[0],columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\"Confusion Matrix\\n-Default Logistic Regression(resampled data)\", fontsize=16)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The grid search was performed over choices of 'penalty': {'none','l2'}, 'C': {0.001,.009,0.01,.09,1,5,10,25}, 'maximum iterations': {1000, 10000}"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"time_taken = time.time()\n\n# create params with list of values to perform gridsearchcv\nLR_grid = {\n           'penalty':['none','l2'],\n           'C':[0.001,.009,0.01,.09,1,5,10,25],\n           'max_iter': [100,1000]\n           }\nCV_LR = GridSearchCV(n_jobs=-1,estimator=LogisticRegression(random_state=42), param_grid = LR_grid,scoring = 'accuracy',cv=5)\n# fit the data\nCV_LR.fit(X_train, y_train)\n\nprint('Best Parameters: ', CV_LR.best_params_)\n\n# perform logistic regression with best params\nCV_LR_clf = LogisticRegression(C=CV_LR.best_params_['C'], max_iter=CV_LR.best_params_['max_iter'], penalty=CV_LR.best_params_['penalty'])\nCV_LR_clf.fit(X_train, y_train)\n\n# find the training and testing accuracy score\naccuracy_train = CV_LR_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\naccuracy_test = CV_LR.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))\ny_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jaccard_scores.append(jaccard_score(y_test, y_pred))\naccuracy_scores.append(accuracy_score(y_test, y_pred))\nf1_scores.append(f1_score(y_test, y_pred, average = 'weighted'))\nprecision_scores.append(precision_score(y_test, y_pred))\nrecall_scores.append(recall_score(y_test, y_pred))\ntimes.append(time.time() - time_taken)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even with the best parameter setting, logistic regression yielded very poor results for both training data and test data."},{"metadata":{},"cell_type":"markdown","source":"<h2> K-Nearest Neighbor Classifier</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_taken = time.time()\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\n# compress the data two 2 feature columns for plotting\nX_train2 = pca.fit_transform(X_train)\n\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X_train2,y_train)\nplot_decision_regions(X_train2, y_train.to_numpy(), clf=knn, legend=2)\n# Adding axes annotations\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Knn with K='+ str(6))\nplt.show()\n\nknn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\nprint('[K-Nearest Neighbors (KNN)] knn.score: {:.3f}.'.format(knn.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jaccard_scores.append(jaccard_score(y_test, y_pred))\naccuracy_scores.append(accuracy_score(y_test, y_pred))\nf1_scores.append(f1_score(y_test, y_pred, average = 'weighted'))\nprecision_scores.append(precision_score(y_test, y_pred))\nrecall_scores.append(recall_score(y_test, y_pred))\ntimes.append(time.time() - time_taken)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Decision Tree </h2>"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"time_taken = time.time()\nDT_grid = { 'min_samples_split': [5,10, 20, 30, 40], \n          'max_features': [None, 'log2', 'sqrt']}\nCV_DT = GridSearchCV(DecisionTreeClassifier(random_state=42), DT_grid, verbose=1, cv=3,n_jobs=-1)\nCV_DT.fit(X_train, y_train)\nprint('Best Parameters: ', CV_DT.best_params_)\n\n# Training step, on X_train with y_train\ntree_clf = tree.DecisionTreeClassifier(min_samples_split = CV_DT.best_params_['min_samples_split'], max_features=CV_DT.best_params_['max_features'])\ntree_clf = tree_clf.fit(X_train,y_train)\n\n# find the training and testing accuracy score\ntree_accuracy_train = tree_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (tree_accuracy_train*100))\ntree_accuracy_test = tree_clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (tree_accuracy_test*100))\n\n# predict the test data\ny_pred = tree_clf.predict(X_test)\n\n# create confiusion matric\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nconf_matrix = pd.DataFrame(data=confmat,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\"Confusion Matrix (resampled data)\\n Decision Tree\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jaccard_scores.append(jaccard_score(y_test, y_pred))\naccuracy_scores.append(accuracy_score(y_test, y_pred))\nf1_scores.append(f1_score(y_test, y_pred, average = 'weighted'))\nprecision_scores.append(precision_score(y_test, y_pred))\nrecall_scores.append(recall_score(y_test, y_pred))\ntimes.append(time.time() - time_taken)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cretae important dataframe with the severity counts\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], \n                           index=X_train.columns)\n\nimportances.iloc[:,0] = tree_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\n\n# defint the fig size\nplt.figure(figsize=(15, 10))\n\n# create barplot for feature and its importance\nsns.barplot(x='importance', y=importances.index, data=importances,palette=\"Spectral\")\n# set labels and title\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Decision Tree Classifier Feature Importance(resampled data)', size=15)\n# show graph\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature importance plot shows that high-resolution spatio-temporal patterns of accidents are the most useful features to predict severity. Among them, Airport frequency is far more important than any other feature. In addition to these spatio-temporal features, weather features like pressure, temperature, humidity, and wind speed are also very important."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create tree plot for decision tree\nfig, ax = plt.subplots(figsize=(20, 10))\ntree.plot_tree(tree_clf, max_depth=4, fontsize=10,\n               feature_names=df_model.drop('Severity',axis =1).columns.to_list(),\n               class_names = True, filled=True)\n# show tree structure plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\nFinally, random forest classifier was employied. The grid search was performed over choices of 'n_estimators': {30,40,50}, 'max_depth': {20,30,40}."},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"time_taken = time.time()\n# initialize params with list of values\nparam_grid = { \n    'n_estimators'     : [30,40,50],\n    'max_depth'        : [20,30,40]\n}\n# perform grid search for the params\nCV_clf = GridSearchCV(n_jobs=-1,estimator=RandomForestRegressor(), param_grid=param_grid,cv=4)\n# fit the data\nCV_clf.fit(X_train, y_train)\nprint('Best Parameters: ', CV_clf.best_params_)\n# perform Random forest regression with the best pram values\nrf_clf = RandomForestRegressor(n_jobs=-1,max_depth=CV_clf.best_params_['max_depth'],n_estimators=CV_clf.best_params_['n_estimators'])\nrf_clf.fit(X_train,y_train)\n\n# predict the test nad train set values\nf = lambda x: 1 if x>=0.5 else 0\ntrain_pred = np.array(list(map(f, rf_clf.predict(X_train))))\ntest_pred = np.array(list(map(f, rf_clf.predict(X_test))))\n\n# calculate the training and testing set accuracy\nrf_train_accuracy = accuracy_score(y_train, train_pred)\nprint(\"Train Accuracy: %.1f%%\"% (rf_train_accuracy*100))\nrf_test_accuracy = accuracy_score(y_test, test_pred)\nprint(\"Test Accuracy: %.1f%%\"% (rf_test_accuracy*100))\n\n# create confusion matric for random forest reuslts\nconfmat = confusion_matrix(y_true=y_test, y_pred=test_pred)\n\nconf_matrix = pd.DataFrame(data=confmat,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\n# creat heat map for confusion matrix\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\"Confusion Matrix \\n Random Forest(resampled data)\", fontsize=16)\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jaccard_scores.append(jaccard_score(y_test, test_pred))\naccuracy_scores.append(accuracy_score(y_test, test_pred))\nf1_scores.append(f1_score(y_test, test_pred, average = 'weighted'))\nprecision_scores.append(precision_score(y_test, test_pred))\nrecall_scores.append(recall_score(y_test, test_pred))\ntimes.append(time.time() - time_taken)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using random forest classifier, the model achieved 100.0% train accuracy and 85.3% test accuracy, which is even better than the results of decision tree classifier. But it took a much longer time to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a datarframe for imimportance feature for severities\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], index=X_train.columns)\n\nimportances.iloc[:,0] = rf_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportances30 = importances.head(30)\n# defint he plot size\nplt.figure(figsize=(15, 10))\n# create bar plot for each feature with the imprtance count for severity\nsns.barplot(x='importance', y=importances30.index, data=importances30,palette=\"Spectral\")\n# set labels, title and ticks\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Random Forest Classifier Feature Importance (resampled data)', size=15)\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Conclustion</h1>\nThe Random Forest performs better then other models but important features of random forest model are almost as same as decision tree model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a plot of the accuracy scores for different algorithms\nalgo_lst = ['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest']\n# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_scores,f1_scores,jaccard_scores,precision_scores,recall_scores,times)), columns=['Algorithm','Accuracy_Score','F1_Score','Jaccard_Score','Precision_Scores','Recall_Scores','Time Taken'])\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='0.5')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title('Comparing Prediction Models for resampled data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the final results of the models\ndf_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}