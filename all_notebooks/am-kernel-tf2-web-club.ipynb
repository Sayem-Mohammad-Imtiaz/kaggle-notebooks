{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# توابع مورد استفاده برای چاپ نمودار ها","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(history, label, n):\n  # Use a log scale to show the wide range of values.\n  plt.semilogy(history.epoch,  history.history['loss'],\n               color=colors[n], label='Train '+label)\n  plt.semilogy(history.epoch,  history.history['val_loss'],\n          color=colors[n], label='Val '+label,\n          linestyle=\"--\")\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  \n  plt.legend()\n\ndef plot_metrics(history):\n  metrics =  ['loss', 'auc', 'precision', 'recall']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.subplots_adjust(right=1.5)\n    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.7,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()\n    \nMETRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = pd.read_csv('/kaggle/input/web-club-recruitment-2018/train.csv')\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg, pos = np.bincount(dataframe['Y'])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos / total))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p lang=\"fa\" dir=\"rtl\" align=\"right\">\n    با توجه به خروجی بالا داده ها بالانس نیستند وقتی قبل از بالانس کردن داده ها مدل رو ترین کردیم مدل برای اکثریت داده ها خروجی رو دسته 0 تشخیص می داد\n</p>\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labels = np.array(dataframe['Y'])\nbool_data_labels = data_labels != 0\n\ndata_features = np.array(dataframe)\n\npos_features = data_features[bool_data_labels]\nneg_features = data_features[~bool_data_labels]\n\npos_labels = data_labels[bool_data_labels]\nneg_labels = data_labels[~bool_data_labels]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = np.arange(len(pos_features))\nchoices = np.random.choice(ids, len(neg_features))\n\nres_pos_features = pos_features[choices]\nres_pos_labels = pos_labels[choices]\n\nres_pos_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p lang=\"fa\" dir=\"rtl\" align=\"right\">\nبرای بالانس کردن داده ها به وسیله کد زیر سعی کردیم تعداد داده های 1 را به وسیله resample افزایش بدیم\n</p>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\nresampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n\norder = np.arange(len(resampled_labels))\nnp.random.shuffle(order)\nresampled_features = resampled_features[order]\nresampled_labels = resampled_labels[order]\n\nresampled_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg1, pos1 = np.bincount(resampled_labels)\ntotal1 = neg1 + pos1\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total1, pos1, 100 * pos1 / total1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = np.insert(resampled_features, 1, resampled_labels, axis=1)\npres = pd.DataFrame(data=resampled_features,columns=dataframe.columns)\npres.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(pres, test_size=0.08)\ntrain, val = train_test_split(train, test_size=0.08)\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = np.array(train['Y'])\nbool_train_labels = train_labels != 0\nval_labels = np.array(val['Y'])\n# test_labels = np.array(test['Y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n  dataframe = dataframe.copy()\n  labels = dataframe.pop('Y')\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n  ds = ds.batch(batch_size)\n  return ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def demo(feature_column):\n    train_ds = df_to_dataset(train, batch_size=5)\n    example_batch = next(iter(train_ds))[0]\n    feature_layer = layers.DenseFeatures(feature_column)\n    print(feature_layer(example_batch).numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p lang=\"fa\" dir=\"rtl\" align=\"right\">\n   بعد از بالانس کردن داده ها مدل شروع به تشخیص دو دسته کرد ولی مشکلی که پیش اومد دقت مدل خیلی کم بود و همچنین مدل مشکل overfit نیز داشت.\n    برای رفع این مشکلات اومدیم و داده ها رو با توابع زیر نرمال کردیم و دقت مدل رسید به 98 درصد.\n    دقت خیلی بهبود پیدا کرد ولی مدل عمیقا overfit شده بود.\n    نمودار history بر روی train و val از  هم فاصله می گرفت\n</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def zscore_x6(col):\n  mean = dataframe.describe()['X6']['mean']\n  std = dataframe.describe()['X6']['std']\n  return (col - mean)/std\n\ndef zscore_x7(col):\n  mean = dataframe.describe()['X7']['mean']\n  std = dataframe.describe()['X7']['std']\n  return (col - mean)/std\n\ndef zscore_x8(col):\n  mean = dataframe.describe()['X8']['mean']\n  std = dataframe.describe()['X8']['std']\n  return (col - mean)/std\n\ndef zscore_x9(col):\n  mean = dataframe.describe()['X9']['mean']\n  std = dataframe.describe()['X9']['std']\n  return (col - mean)/std\n\ndef zscore_x10(col):\n  mean = dataframe.describe()['X10']['mean']\n  std = dataframe.describe()['X10']['std']\n  return (col - mean)/std\n\ndef zscore_x11(col):\n  mean = dataframe.describe()['X11']['mean']\n  std = dataframe.describe()['X11']['std']\n  return (col - mean)/std\n\ndef zscore_x12(col):\n  mean = dataframe.describe()['X12']['mean']\n  std = dataframe.describe()['X12']['std']\n  return (col - mean)/std\n\ndef zscore_x13(col):\n  mean = dataframe.describe()['X13']['mean']\n  std = dataframe.describe()['X13']['std']\n  return (col - mean)/std\n\ndef zscore_x14(col):\n  mean = dataframe.describe()['X14']['mean']\n  std = dataframe.describe()['X14']['std']\n  return (col - mean)/std\n\ndef zscore_x15(col):\n  mean = dataframe.describe()['X15']['mean']\n  std = dataframe.describe()['X15']['std']\n  return (col - mean)/std\n\ndef zscore_x16(col):\n  mean = dataframe.describe()['X16']['mean']\n  std = dataframe.describe()['X16']['std']\n  return (col - mean)/std\ndef zscore_x17(col):\n  mean = dataframe.describe()['X17']['mean']\n  std = dataframe.describe()['X17']['std']\n  return (col - mean)/std\n\ndef zscore_x18(col):\n  mean = dataframe.describe()['X18']['mean']\n  std = dataframe.describe()['X18']['std']\n  return (col - mean)/std\n\ndef zscore_x19(col):\n  mean = dataframe.describe()['X19']['mean']\n  std = dataframe.describe()['X19']['std']\n  return (col - mean)/std\n\ndef zscore_x20(col):\n  mean = dataframe.describe()['X20']['mean']\n  std = dataframe.describe()['X20']['std']\n  return (col - mean)/std\n\ndef zscore_x21(col):\n  mean = dataframe.describe()['X21']['mean']\n  std = dataframe.describe()['X21']['std']\n  return (col - mean)/std\n\ndef zscore_x22(col):\n  mean = dataframe.describe()['X22']['mean']\n  std = dataframe.describe()['X22']['std']\n  return (col - mean)/std\n\ndef zscore_x23(col):\n  mean = dataframe.describe()['X23']['mean']\n  std = dataframe.describe()['X23']['std']\n  return (col - mean)/std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = []\n\nX1_buckets = feature_column.bucketized_column(feature_column.numeric_column('X1'), boundaries=[i*10000 for i in range(1, 100)])\n# demo(X1_buckets)\nfeature_columns.append(X1_buckets)\n\nX2_categories = feature_column.indicator_column(feature_column.categorical_column_with_identity('X2', 3))\n# demo(X2_categories)\nfeature_columns.append(X2_categories)\n\nX3_categories = feature_column.indicator_column(feature_column.categorical_column_with_identity('X3', 7))\n# demo(X3_categories)\nfeature_columns.append(X3_categories)\n\nX4_categories = feature_column.indicator_column(feature_column.categorical_column_with_identity('X4', 4))\n# demo(X4_categories)\nfeature_columns.append(X4_categories)\n\n\nX5_buckets = feature_column.bucketized_column(feature_column.numeric_column('X5'), boundaries=[25, 30, 35, 40, 45, 50, 55, 60, 65, 70 , 75])\n# demo(X5_buckets)\nfeature_columns.append(X5_buckets)\n\n\n\nfeature_columns.append(feature_column.numeric_column('X6', normalizer_fn=zscore_x6))\nfeature_columns.append(feature_column.numeric_column('X7', normalizer_fn=zscore_x7))\nfeature_columns.append(feature_column.numeric_column('X8', normalizer_fn=zscore_x8))\nfeature_columns.append(feature_column.numeric_column('X9', normalizer_fn=zscore_x9))\nfeature_columns.append(feature_column.numeric_column('X10', normalizer_fn=zscore_x10))\nfeature_columns.append(feature_column.numeric_column('X11', normalizer_fn=zscore_x11))\nfeature_columns.append(feature_column.numeric_column('X12', normalizer_fn=zscore_x12))\nfeature_columns.append(feature_column.numeric_column('X13', normalizer_fn=zscore_x13))\nfeature_columns.append(feature_column.numeric_column('X14', normalizer_fn=zscore_x14))\nfeature_columns.append(feature_column.numeric_column('X15', normalizer_fn=zscore_x15))\nfeature_columns.append(feature_column.numeric_column('X16', normalizer_fn=zscore_x16))\nfeature_columns.append(feature_column.numeric_column('X17', normalizer_fn=zscore_x17))\nfeature_columns.append(feature_column.numeric_column('X18', normalizer_fn=zscore_x18))\nfeature_columns.append(feature_column.numeric_column('X19', normalizer_fn=zscore_x19))\nfeature_columns.append(feature_column.numeric_column('X20', normalizer_fn=zscore_x20))\nfeature_columns.append(feature_column.numeric_column('X21', normalizer_fn=zscore_x21))\nfeature_columns.append(feature_column.numeric_column('X22', normalizer_fn=zscore_x22))\nfeature_columns.append(feature_column.numeric_column('X23', normalizer_fn=zscore_x23))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 200\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p lang=\"fa\" dir=\"rtl\" align=\"right\">\n   برای رفع مشکل overfit از لایه های dropout استفاده کردیم و مشکل حل شد ولی دقت مدل به ۸۳ کاهش پیدا کرد.\n    همچنین رگرسیون خطی L2 رو هم تست کردیم ولی نتیجه dropput خیلی بهتر بود\n</p>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(200, activation='relu'),\n  layers.Dropout(0.5),\n  layers.Dense(128, activation='relu'),\n  layers.Dropout(0.5),\n  layers.Dense(100, activation='relu'),\n  layers.Dropout(0.5),\n  layers.Dense(80, activation='relu'),\n  layers.Dropout(0.5),\n  layers.Dense(40, activation='relu'),\n  layers.Dropout(0.5),\n  layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=METRICS)\n\n\nhistory = model.fit(train_ds,\n          validation_data=val_ds,\n          epochs=150)\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss(history, \"history\", 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_metrics(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p lang=\"fa\" dir=\"rtl\" align=\"right\">\n  در آخر در نمودار خطا می بینیم که مدل مشکل high variance نداره\n    همچنین دقت و recal و precision مدل به حدود ۸۰ رسیده\n    بنظر می رسه که برای بهبود بیشتر دقت و سایر معیار های مدل نیاز به داده های بیشتر هست چونکه با کاهش سایز ولیدیشن و تست ست دقت مدل بیشتر می شه ولی چون که دوست نداریم مدل احیانا آورفیت بشه بیشتر از این اندازه مجموعه داده های تست و ولیدیت رو کاهش نمی دیم\n    نکته مهم بعدی معیار ROC هست که با مقدار 0.9 نشون میده مدل با احتمال خوبی دسته بندی رو انجام میده\n</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataframe = pd.read_csv('/kaggle/input/web-club-recruitment-2018/test.csv')\nkaggle_test_ds = tf.data.Dataset.from_tensor_slices((dict(test_dataframe))).batch(32)\nprint(kaggle_test_ds)\nprint(train_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred= model.predict(kaggle_test_ds).round().astype(int)\ndf = pd.DataFrame(pred)\ndf.index.name = 'id'\ndf.columns = ['predicted_val']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('output.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}