{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Training Transformers with Custom Data </p> \nHugging face transformers are wrappers that help in several tasks like Sentiment Analysis, Question Answering Etc.  \nSome more are given in [this](https://www.kaggle.com/kabirnagpal/vaccine-tweet-analysis-with-hugging-face) notebook.   \nHowever you can use same architecture to train on your own dataset and fine tune it.  \nThis is available both in Tensorflow and PyTorch and is easy to use.  \nIn this Notebook I've used TF for the same purpose and tried to predict rating by a user based on review. \nYou can refer here for some more codes:\n1. [GitHub](https://github.com/katanaml/sample-apps/blob/master/02/sentiment-fine-tuning-huggingface.ipynb)\n2. [TF Docs](https://www.tensorflow.org/tutorials/text/classify_text_with_bert)"},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Importing Packages </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\n!pip install contractions\nimport contractions\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport transformers\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nlem = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Loading Dataa</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/flipkart-customer-review-and-rating/data.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Preprocessing </p>\n\nPreprocessing is a necessary steps, as it helps in removing errors, emojis and other unnecessary words/symbols.  \nI've created a separate notebook, for most used preprocessing methods.  \n[[Tips] List of preprocessing techniques in NLP](https://www.kaggle.com/kabirnagpal/tips-list-of-preprocessing-techniques-in-nlp)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x):\n    x = x.replace(\"READ MORE\",\"\")\n    x = x.encode('ascii','ignore')\n    x = x.decode()\n    x = x.lower()\n    x = contractions.fix(x)\n    x = ' '.join([word for word in x.split() if not word in set(stopwords.words('english'))])\n    x =  re.sub('[^a-zA-Z0-9]', ' ', x)\n    x = ' '.join(x.split())\n    x = lem.lemmatize(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'] = data['review'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Train Test Split</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = list(data['review'].values)\ny = pd.get_dummies(data['rating']).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Tokenising with Hugging Face</p> \nTokenising Converts words to numbers.  \nIt is highly recommeded to use these methods rather than numbering yourself, becuase,  \nthe pretrained models are trained on a huge dataset and autmatically adjusted numbers to be given to words.  \nHence Good and Awesome will lie closer tha Good and Bad.  \nI've used a length of 30 words only, howver it's a hyperparamter and can be later tuned.  \nAlong with numerical values, it also provided mask, which are used by attention layer.  \n( don't worry, you'll understand this while learning aout transformers. )"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\nX_train = tokenizer(X_train, truncation=True, padding=True,max_length=30)\nX_test = tokenizer(X_test, truncation=True, padding=True,max_length = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(X_train),\n    y_train\n))\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(X_test),\n    y_test\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Bert Model</p> "},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification\n\n\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=10e-5)\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\nmodel.fit(train_dataset.shuffle(42).batch(512),\n          epochs=5,\n          batch_size=512,\n          validation_data=val_dataset.shuffle(42).batch(512))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of hyperparamters here can be tuned to increase the score,  \nhowever the motive of the notebook was to get you familiar with the method.  \n**num_labels** is the number of classses we've, i.e. 5 ( rating 1 - 5 )  \nYou can also used different pretrained methods given [here](https://huggingface.co/models).  \n## Happy Learning"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}