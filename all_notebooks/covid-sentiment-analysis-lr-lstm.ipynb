{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import TweetTokenizer \nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import Counter\n\nimport keras\nfrom keras.models import Sequential\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras.preprocessing.sequence import pad_sequences\nimport re\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Reading In and Exploratory analysis"},{"metadata":{},"cell_type":"markdown","source":"### Reading the data in"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv',encoding='latin_1')\ntest = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv',encoding='latin_1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### initial checks on the data set, missing data and null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.iloc[16])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the fields except the Location field have no missing values.\n\nThe UserName and ScreenName columns have been anonymized for privacy so they are not useful to our analysis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(labels = ['UserName','ScreenName'],axis=1, inplace=True)\ntest.drop(labels = ['UserName','ScreenName'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dupicates and Null Values "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(test.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Null values are mostly in the Loction variable, hence the loacation variable will be utilized for EDA and dropped before modelling begins. Dupilcates will be dropped to reduce Variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates(inplace= True)\ntest.drop_duplicates(inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train data shape: ',train.shape)\nprint('Test data shape: ',test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the distibrution of Dependent Variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nsns.set(rc={'figure.figsize':(10,5)})\nsns.countplot(train['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The other variables will now be analysed in reference to the Target variable, starting with the Location. We will examine the top 20 locations; But before we do that we have to change the Extremely Positive and Extremely negative classes to positive and negative classes respectively using a string function to make future tasks easier \n\nI attempted devising an ordinal scale system to rank the sentiment but couldn't deal with the bugs within the time specified "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sentiment'] = train.Sentiment.str.replace('Extremely Positive', 'Positive')\ntrain['Sentiment'] = train.Sentiment.str.replace('Extremely Negative', 'Negative')\n\ntest['Sentiment'] = test.Sentiment.str.replace('Extremely Positive', 'Positive')\ntest['Sentiment'] = test.Sentiment.str.replace('Extremely Negative', 'Negative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Negative and Positive classes are fairly balanced, with the neutral variable with a much lower percentage. Undersampling or Oversampling techniques will be applied if the models to be built are skewed by this. \n\nThe methodology that was used to determine the sentiment of the tweets wasn't published on the dataset page so certain assumptions cannot be made\n\n----------------------------------------------------------------------------------------------------------------------\n\nNow that the sentiment class has been simplified, we can begin analysing the other Variable in respect to it; the Location, Time and ultimately the Tweet corpus that will require alot more cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Location Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Location'].value_counts(dropna = False)[:40]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A large section of the location data is missing. The rest of the data doesn't follow a consistent pattern because twitter allows for a flexible location setting. So we start by attempting to make the Location bar as consistent as possible by splitting word pairs; e.g changing 'Nairobi, Kenya' to Nairobi "},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting location into word pairs\ntrain['Location'] = train['Location'].str.split(\",\").str[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Location'].value_counts()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting the location and sentiment columns in to a df for plotting\nplot_df = train.iloc[:,[0,3]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,6)})\ngg = train.Location.value_counts()[:10].index\nplt.title(\"Tweet count across top  cities\")\nsns.countplot(x = \"Location\", hue = \"Sentiment\", data = plot_df, order = gg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insight\nAll major cities follow the same trend across the sentiments except England, where Negative sentiments outnumber the positive and neutral.\n\nThe graph shows that most tweets contain positive content accross the board except England as a country, which is interesting because the location with the highest tweet count is London and the distribution there follows the major trend. \n\nThe top locations are located in the United Kingdom, America, Canada and India. The dataset description does not say much about how the data was gathered. However most top cities follow the same trend.\n\nAnother caveat: the location data is not representative of the entire dataset due to missing data."},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------------------------------------------------------------------------------------------\n### Time / Date Analysis\nWe examine the tweets across different time periods"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the time column to date time for easy analysis\ntrain[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Earliest date: ', train['TweetAt'].min())\nprint('Latest Date', train['TweetAt'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tweets were collected across an 8 month range"},{"metadata":{},"cell_type":"markdown","source":"Next, we examine the tweet count across the days of the week"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])\ntrain[\"day\"] = train[\"TweetAt\"].apply(lambda x : x.dayofweek)\nkey = {0: 'Monday', 1: 'Tuesday', 2:'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\ntrain[\"day\"] = train[\"day\"].map(key)\nplt.title(\"Tweet count across days\")\nsns.countplot(train[\"day\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(17,6)})\ntrain[\"TweetAt\"] = pd.to_datetime(train[\"TweetAt\"])\ntrain[\"month\"] = train[\"TweetAt\"].apply(lambda x : x.month)\nkey = {0: 'January ', 1: 'February', 2:'March', 3: 'April', 4: 'May', 5: 'June', 6: 'July',\n      7:'August', 8:'September', 9: 'October', 10: 'November', 11: 'December'  }\ntrain[\"month\"] = train[\"month\"].map(key)\nplt.title(\"Tweet count across months\")\nsns.countplot(train[\"month\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insight\nThe distribution is heavily skewed to the right when referenced with the death toll image below... "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Paste Image Here\nhttps://ourworldindata.org/covid-deaths?country=IND~USA~GBR~CAN~DEU~FRA"},{"metadata":{},"cell_type":"markdown","source":"Image Citation: \nMax Roser, Hannah Ritchie, Esteban Ortiz-Ospina and Joe Hasell (2020) - \"Coronavirus Pandemic (COVID-19)\". Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/coronavirus' [Online Resource]"},{"metadata":{},"cell_type":"markdown","source":"...it is noticed that it follows a very similar trend. Perhaps the high number of tweets in April came as result of the high number of cases and deaths in cities such as London and New York."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['month'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Corpus Analysis\n\nTime to examine the tweets; Hashtags, mentions and top words "},{"metadata":{"trusted":true},"cell_type":"code","source":"#examination of tweets\nfor x in train.loc[:50, 'OriginalTweet']:\n    print(x)\n    print('\\n')\n    print('***********************************\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tweets contain mentions, hashtags, links, numbers and non english characters(take the cell below)"},{"metadata":{"trusted":true},"cell_type":"code","source":" train.loc[16, 'OriginalTweet']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The top 20 hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting top hashtags using regex\nhashtags=train['OriginalTweet'].str.extractall(r\"(#\\S+)\")\nfreqs = hashtags[0].value_counts()\nfreqs[:20]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets look at the sentiments of the tweets containing the most prominent hashtag"},{"metadata":{"trusted":true},"cell_type":"code","source":"#regex function to find all rows that contain #coronavirus hashtag\nhashtag = train[train.OriginalTweet.str.contains(pat ='#coronavirus ')]\nplt.title(\"Sentiment in tweets that trend #coronavirus\")\nsns.countplot(hashtag.Sentiment)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insight\n\nRepresentative of the general trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top Mentions"},{"metadata":{"trusted":true},"cell_type":"code","source":"mentions = train['OriginalTweet'].str.extractall(r\"(@\\S+)\")\nmentions = mentions[0].value_counts()\nmentions[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#regex function to find all rows that contain trump\nresult = train.OriginalTweet.str.contains(pat ='@realDonaldTrump')\ntrump=train[result]\nplt.title(\"Sentiment in tweets that mention Trump\")\nsns.countplot(trump.Sentiment)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insight\n\nAlmosts follows the general trend but the sentiments are more polarized. The lower neutral sentiment and very close Positive and Negative sentiments are indicative of polarization. "},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the data\nIn order to perform text mining, the following processes have to be applied.\n* Clean data, remove links and numbers\n* Remove Stopwords\n* Vectorize words"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to clean data\n\nstop_word = stopwords.words('english')\ndef clean_data(df): \n    df.OriginalTweet = df.OriginalTweet.str.replace(r'(@\\w*)','')\n\n    #Removes URLs in the tweets\n    df.OriginalTweet = df.OriginalTweet.str.replace(r\"http\\S+\", \"\")\n\n    #Remove hashtags\n    df.OriginalTweet = df.OriginalTweet.str.replace(r'#\\w+',\"\")\n\n    #Removes uniques characters\n    df.OriginalTweet = df.OriginalTweet.str.replace(r\"[^a-zA-Z ]\",\"\")\n\n    # Remove all extra spaces\n    df.OriginalTweet = df.OriginalTweet.str.replace(r'( +)',\" \")\n    df.OriginalTweet = df.OriginalTweet.str.strip()\n\n    # Changes characters to lowercase\n    df.OriginalTweet = df.OriginalTweet.str.lower()\n    \n    return df\n    \n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying cleaning function on train and test dataframes\n\ntemptr = train.iloc[:,[2,3]]\nclean_train = clean_data(temptr)\ntempte = test.iloc[:,[2,3]]\nclean_test = clean_data(tempte)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(corpus):\n    corpus = corpus.split()\n    corpus = \" \".join([word for word in corpus if not word in stop_word])\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lambda function to remove stopwords\nclean_train['OriginalTweet'] = clean_train['OriginalTweet'].apply(lambda x: remove_stopwords(x))\nclean_test['OriginalTweet'] = clean_test['OriginalTweet'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing empty strings with Nan\nclean_train = clean_train.replace(r'^\\s*$', np.NaN, regex=True)\nclean_test = clean_test.replace(r'^\\s*$', np.NaN, regex=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train.OriginalTweet[16]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping nan\nclean_train.dropna(inplace = True)\nclean_test.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#examination of tweets\nfor x in clean_train.loc[:50, 'OriginalTweet']:\n    print(x)\n    print('\\n')\n    print('***********************************\\n')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is free from links, hashtags, mentions and figures. We can begin modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#token = nltk.word_tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus = ' '.join([i for i in clean_train['OriginalTweet']]).split() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TweetTokenizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unigrams, Bigrams and Trigrams\nNgrams show the relationships and probabilistic tendences that certain words appear together"},{"metadata":{"trusted":true},"cell_type":"code","source":"unigram = pd.Series(nltk.ngrams(Corpus, 1)).value_counts()[:15]\nunigram = pd.DataFrame(unigram)\nunigram['sn'] = unigram.index\nunigram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram = pd.Series(nltk.ngrams(Corpus, 2)).value_counts()[:15]\nbigram = pd.DataFrame(bigram)\nbigram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram = pd.Series(nltk.ngrams(Corpus, 3)).value_counts()[:15]\ntrigram = pd.DataFrame(trigram)\ntrigram","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n### Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(df):\n    nltk_tokens = nltk.word_tokenize(df)\n    return nltk_tokens\n\nclean_train['tokens'] =  clean_train['OriginalTweet'].apply(lambda x: tokenize(x))\nclean_train.tokens = clean_train.apply(lambda x: \" \".join(x.tokens),axis=1)\n\n\nclean_test['tokens'] =  clean_test['OriginalTweet'].apply(lambda x: tokenize(x))\nclean_test.tokens = clean_test.apply(lambda x: \" \".join(x.tokens),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nord_enc = OrdinalEncoder()\nclean_train[\"y_nominal\"] = ord_enc.fit_transform(clean_train[[\"Sentiment\"]])\nclean_test[\"y_nominal\"] = ord_enc.fit_transform(clean_test[[\"Sentiment\"]])\n\nX_train, X_test, y_train, y_test = train_test_split(clean_train['tokens'],\n                                                    clean_train['y_nominal'], test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize vectorizer\nvector = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5).fit(clean_train['tokens'])\n#initially min_df = 5\n\n# Transforms a collection of text documents into a matrix of token counts\nx_train_vectorized = vector.transform(X_train)\nx_test_vectorized = vector.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_enc.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n### Logistic Regression\n\nUsually a model selection techniques would occur here, but since the model to be used was explicitly stated; we jump straight into training and parameter tuning using Gridsearch."},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\n\n# Create regularization penalty space\npenalty = ['l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create grid search using 5-fold cross validation\nclf = GridSearchCV(LR, hyperparameters, cv=5, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit grid search to find the best model\nbest_model = clf.fit(x_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View best hyperparameters\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification Report\ny_pred = best_model.predict(x_test_vectorized)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report = classification_report(y_test, y_pred)\nprint(report)\n\n#confusion matrix\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC CURVE\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_probas = best_model.predict_proba(x_test_vectorized)\nskplt.metrics.plot_roc(y_test, y_probas)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results Discussion\nThe metric of choice for evaluation is \n> F1 Score\n\nit is the harmonic mean between precision and recall and gives us a better measure.\nThe positive class was the most accurately predicted, then Negative and Neutral across all metrics Precision, Recall and F1. \n\nThe results are consistent with the amount of tweets available per class. Positive class highest accuracy, followed by Negative and Neutral. It is a simply a matter or more data. To get better scores on the neutral class or overall, the following options can be considerered\n\n* **Collect more data**: more \n* **Sampling**: This can be done in a number of ways.The majority classes (in this case Positive and Negative) can be undersampled. Equally, the minority class can be Oversampled to match the majority. The weights of the different classes can also be modified for uniformity. Weighing the input of the minority class higher than the majority classes or vice versa. \n* Utilize other word vectorizers: Vectorizers like TFid, Word2Vec or frameworks like Gensim can be utilized. \n* More Parameter tuning: The model can be finetuned further to slightly improve scores \n\n"},{"metadata":{},"cell_type":"markdown","source":"### LSTM\nA seperate tokenizer was used to fit the LSTM. The sequences were padded with zeros for uniformity across the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizerlstm = Tokenizer( split=' ')\ntokenizerlstm.fit_on_texts(clean_train['OriginalTweet'].values)\n\nX = tokenizerlstm.texts_to_sequences(clean_train['OriginalTweet'])\n\nX = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabSize = len(tokenizerlstm.word_index) + 1\nvocabSize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LSTM model was created with a sequential Keras object. It contains an Embedded layer, LSTM and sense layer for output."},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, embed_dim,input_length = 28))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable was binarized to correspond with the logit pairs for the LSTM model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\n\nLe = LabelEncoder()\nlb = preprocessing.LabelBinarizer()\n\ny_train = lb.fit_transform(clean_train['Sentiment'])\n\n#y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n#y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = lb.fit_transform(clean_test['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#atempted memory error fix\n'''\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trainl, X_testl, y_trainl, y_testl = train_test_split(X,y_train, test_size = 0.15, random_state = 42)\n\nmodel.fit(X_trainl, y_trainl,validation_data = (X_testl,y_testl),epochs = 10, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizerlstm = Tokenizer( split=' ')\n#tokenizerlstm.fit_on_texts(clean_train['OriginalTweet'].values)\n\ntest_lstm = tokenizerlstm.texts_to_sequences(clean_test['OriginalTweet'])\n\ntest_lstm = pad_sequences(test_lstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_pred = model.predict_classes(test_lstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(np.argmax(y_test,1), lstm_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(np.argmax(y_test,1), lstm_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results Discussion\nInterstingly the Logistic Regression Classifier performed almost the same as the LSTM Model.\nThe results are near identical except the macro average recall where the LSTM model edged out the LR model by 0.01. However the LSTM model used was not state of the art, presumably the results would have been better. \n\nSimilar recommendations given to LR model can be applied to the LSTM model, such as gathering more data, sampling and other word vectorizers.\n\nMost importantly, the model structure. No single LSTM is the best for text processing as finding the best model is a [leading topic in the literature](https://scholar.google.com/scholar?q=lstm+for+nlp+papers&hl=en&as_sdt=0&as_vis=1&oi=scholart). Hence other model structures can be experimented with in the future.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}