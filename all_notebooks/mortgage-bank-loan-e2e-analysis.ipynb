{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Mortgage Bank Loan E2E Analysis**"},{"metadata":{},"cell_type":"markdown","source":"Mortgage Bank Loan Analytics with ARIMA and Machine Learning Mortgage Loans Analytics Banks can now use mortgage loan analytics using Data Science techniques. The system can provide detail information of the mortgage loans and the mortgage loan markets. It is a powerful tool for mortgage brokers to seek counterparties and generate trading interests and is useful for the CFOs to conduct what-ifs scenarios on the balance sheets."},{"metadata":{},"cell_type":"markdown","source":"Loan file template requires below details: \n- Loan ID: to identify the special loan \n- Loan Type: to indicate the loan if fixed rate, or balloon loan , or ARM, or AMP (alternative mortgage product).\n- Balance: \n- Loan program type: to indicate conforming loan, FHA/VA loan, Jumbo loan or sub-prime loan \n- Current coupon rate: \n- Amortization type: the original amortization term \n- Maturity: the maturity loan (the remaining term of the loan) \n- FICO Score: the updated fico score \n- LTV: the current loan to value ratio \n- Loan Size: the loan amount of the loan \n- Loan origination location (City & Zip) \n- Unit Types (Types of property)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os                             # Library to do things on the filesystem\nimport pandas as pd                   # Super cool general purpose data handling library\nimport matplotlib.pyplot as plt       # Standard plotting library\nimport seaborn as sns\nimport numpy as np                    # General purpose math library\nfrom IPython.display import display   # A notebook function to display more complex data (like tables)\nimport scipy.stats as stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', header=0, encoding='cp1252')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the 5th and 95th percentiles\ndata.quantile([0.05, 0.95])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Loan Amount'].quantile([0.05, 0.95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Loan Amount'].quantile([0.1, 0.90])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"St=['CT', 'FL', 'NJ', 'NY', 'PA']\nprint(St)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DataFrame subsets by State\nct_data = data[data['State'].isin(['CT'])]\nfl_data = data[data['State'].isin(['FL'])]\nny_data = data[data['State'].isin(['NY'])]\nnj_data = data[data['State'].isin(['NJ'])]\npa_data = data[data['State'].isin(['PA'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting data per column\nfiltered_columns =('Created Date', 'First Name', 'Last Name', 'Loan Amount', 'City', 'unit_type_code', 'Loan Type')\nct_lo_amount = ct_data.reindex(columns=filtered_columns)\nfl_lo_amount = fl_data.reindex(columns=filtered_columns)\nnj_lo_amount = nj_data.reindex(columns=filtered_columns)\nny_lo_amount = ny_data.reindex(columns=filtered_columns)\npa_lo_amount = pa_data.reindex(columns=filtered_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating boxplot to see the distribution median and outliers\nplt.figure(figsize=(10,5))\nsns.violinplot(y=data['Loan Amount'],inner=\"quartile\")\nplt.title(\"Boxplot showing distribution of index of accessbility of Loan Amount\")\nplt.ylim(0.1,1000000)\nplt.ylabel('price ($)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that mid 50% loan amount between USD 340,000 &  USD 570,000"},{"metadata":{},"cell_type":"markdown","source":"# Mortgage interest rates"},{"metadata":{},"cell_type":"markdown","source":"Mortgage interest rates have a significant impact on the amount of mortgage applications. If the interest rates are low, the mortgages are relatively cheaper for the borrower as they have to pay less interest, which leads to an increased amount of mortgage applications. A high mortgage interest rate means the mortgage borrower pays a high amount of interest to the lender, which makes the mortgage less attractive for the borrower. Interest rate changes have a significant impact on mortgage applications, as was seen in November of last year, where a sudden increase in interest rates led to a large peak in mortgage applications. The main difference between the mortgages offered by these types of companies lies in the mortgage interest rates. Even a small difference in mortgage interest rates can often save or cost the borrower a vast amount of money, due to the large sum of a mortgage.\n\nIn general, there are two types of mortgage interest rate: variable rates (ARM) and fixed rates. Variable interest rates are generally lower than fixed interest rates, but can change every month. Fixed interest rates are slightly higher, but are fixed for a certain period of time. A fixed interest rate is generally preferred when the mortgage interest rates are expected to rise, or when the borrower wants to know its monthly expenses upfront. A variable interest rate (ARM) is preferred when interest rates are expected to decrease. If a financial institution has a significantly higher interest rate than its competitors, it will generally receive fewer mortgage applications as the independent mortgage advisors will forward its customers to a different mortgage lender.\n\nFinancial institutions sometimes increase their interest rates during the summer months, and at the end of the year, as there is less personnel available to handle the requests due to vacations and holidays. With less personnel available they can handle less mortgage requests, so in order to keep the processing time the same they choose to reduce the input, by increasing the interest rates. Financial institutions may also specifically keep interest rates low for mortgages with a certain fixed interest period. Interest rate changes are not always directly influenced by changes in the cost of lending, but can have numerous reasons.\n"},{"metadata":{},"cell_type":"markdown","source":"## Load US 10-Years Treasury Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y = pd.read_csv('../input/mortgage-bank-loan/US10Y.csv', header=0, index_col='DATE', encoding='cp1252')\nUS10Y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.RATE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.replace(\".\", value=np.nan, inplace=True)\nUS10Y= US10Y.replace(to_replace=-1, value=np.nan)\nUS10Y.RATE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y= US10Y.fillna(method='ffill')\nUS10Y= US10Y[['RATE']].astype('float64')\nUS10Y.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(US10Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing_rate=US10Y.isnull().sum() #Checking for missing values\ntotal_missing_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.index = pd.to_datetime(US10Y.index)\nax=monthly_rate_data=US10Y.resample('M').mean().plot(title=\"Interest Rate for 10 Years Treasury - \",figsize=(20,5))\nplt.ylabel('RATE')\nax.get_legend().remove()\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display Monthly US Treasury 10 Years Interest Rate"},{"metadata":{},"cell_type":"markdown","source":"Generally, when US 10 Years Treasury Rate fluctuates, that leads lenders to adjust their internal bank rates accordingly. Also interest rates for consumers varies on several risk factors, such as DTI (Debt to Income Ratio), FICO Scores, Recent derogatory events on their credit history, stable job history, W2 or 1099, Stated Income, Profit or Loss Statements, Student Loans, Auto Payments, Credit utilization, Property types, number of households, rental history, etc.\n\nInterest Rate is currently historical low. In the short run rate may go ups and down but in the long run rate will go up. As housing price goes up, interest rate will go up to control the housing price."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os                             # Library to do things on the filesystem\nimport pandas as pd                   # Super cool general purpose data handling library\nimport matplotlib.pyplot as plt       # Standard plotting library\nimport seaborn as sns\nimport numpy as np                    # General purpose math library\nfrom IPython.display import display   # A notebook function to display more complex data (like tables)\nimport scipy.stats as stats  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns = data[['Loan Amount', 'Created Date']]\nloan_patterns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nWe can create a histogram with 20 bins to show the distribution of purchasing patterns.'''\n\nloan_patterns_plot = loan_patterns['Loan Amount'].hist(alpha=0.6, bins=40, grid=True,figsize=(20,5))\n\nloan_patterns_plot = loan_patterns['Loan Amount'].apply(np.sqrt)\n\nparam = stats.norm.fit(loan_patterns_plot) \nx = np.linspace(0, 100000, 1250000)      # Linear spacing of 100 elements between 0 and 20.\npdf_fitted = stats.norm.pdf(x, *param)    # Use the fitted paramters to \nloan_patterns_plot.plot.hist(alpha=0.6, bins=40, grid=True, density=True, legend=None,figsize=(20,5))\n# Fit a normal distribution to the data\n# Plot the histogram again\n# Plot some fancy text to show us what the paramters of the distribution are (mean and standard deviation)\nplt.text(x=np.min(loan_patterns_plot), y=800, s=r\"  $\\mu=%0.2f$\" % param[0] + \"\\n\" \n         + r\"  $\\sigma=%0.2f$\" % param[1], color='b')\n\n# Plot a line of the fitted distribution over the top\n# Standard plot stuff\nplt.xticks(rotation=75)\nplt.ylim((1,900))\nplt.xlim((1,1210000))\nplt.xlabel(\"Loan Amount($)\")\nplt.ylabel(\"Loan frequency\")\nplt.title(\"Histogram with fitted normal distribution for Mortgage Bank Loan\")\n\n# Density Plot with Rug Plot\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( 'excess kurtosis of normal distribution (should be 0): {}'.format(round((kurtosis(loan_patterns_plot)),2)))\nprint( 'skewness of normal distribution (should be 0):        {}'.format(round((skew(loan_patterns_plot)),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean : \", round(np.mean(loan_patterns_plot),2))\nprint(\"var  : \", round(np.var(loan_patterns_plot),2))\nprint(\"skew : \",round(skew(loan_patterns_plot),2))\nprint(\"kurt : \",round(kurtosis(loan_patterns_plot),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will compare Monthly Revenue, Monthly Closed Loan Number and Active Mortgage Loan Originators. We will count number of MLO actively closing loans on any given month."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns = data[['Loan Amount', 'Created Date']]\nloan_patterns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlo_num=data[['Loan Officer Name']]\nmlo_num['date'] = pd.DatetimeIndex(data['Created Date'])\nmlo_num = mlo_num.set_index('date')\nmonthly_mlo_num=mlo_num.resample('M').nunique()\n\nmonthly_loan_num=data[['LoanInMonth']]\nmonthly_loan_num.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_loan_num['date'] = pd.DatetimeIndex(data['Created Date'])\nmonthly_loan_num = monthly_loan_num.set_index('date')\nmonthly_loan_num_data=monthly_loan_num.resample('M').last().plot(title=\"Mortgage Bank - Total Sales by Month\",\n                                                                 legend=None,grid=True,figsize=(20,5))\nplt.ylabel(\"Loan in Months\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summer seems to be high sales seasons for the Mortgage bank. Numbers of loans closed per months varies between 20 & 80."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_rev_data=data[['Loan Amount']]\nloan_rev_data['date'] = pd.DatetimeIndex(data['Created Date'])\nloan_rev_data = loan_rev_data.set_index('date')\nmonthly_loan_rev_data=loan_rev_data.resample('M').sum().plot(title=\"Mortgage Bank - Total Sales by Month\",\n                                                             legend=None,grid=True,figsize=(20,5))\nplt.ylabel(\"Loan Amount\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mortgage Bank Monthly Sales Since October 2014. Sales varies between 12M & 33M per months."},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_loan_num_data= monthly_loan_num.resample('M').last()\nplt.figure(figsize=(20,5))\nplt.xlabel('Loan origination Months')\nplt.xticks(rotation=60)\nplt.ylabel('Loans per Months')\nplt.title('Mortgage Bank Monthly Loan numbers')\n\n\nfrom matplotlib.lines import Line2D\ncolors = ['red', 'green', 'blue']\nlines = [Line2D([0], [0], color=c, linewidth=3, linestyle='--') for c in colors]\nlabels = ['Monthly closed loans', 'Monthly Loan Revenue / 400000', 'Monthly Active MLO * 4']\nplt.plot(monthly_loan_num_data, color='red')\nplt.plot(loan_rev_data.resample('M').sum() /400000, color='green')\nplt.plot(monthly_mlo_num*4, color='blue')\nplt.legend(lines, labels)\nplt.title('Comparing Monthly: Revenue, Closed_Loan_Num, Active_MLO_Num')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we know that number of producer is essential component in any given business. MLO (Mortgage loan Originator) is core component in Mortgage business. Many MLO works indedendently and interect directly to clients, involve in marketting and grow their business. There could be many MLO in Mortgage Bank, but active MLO generate more reverue for the bank. As number of active MLO goes up, which will directly and positively impact numbers of loan closed per month, eventually mortgage revenue will go up. On the other hand, once number of active MLO goes down, mortgage revenue and number of loan per month goes down as well. By visualizing the graphs, we can see that monthly data of closed loan numbers , monthly revenue and active MLO numbers ber months, all moving at the same direction.\n\nLet’s find out interest rate effect on Monthly Closed Loans and Monthly Revenue."},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.index = pd.to_datetime(US10Y.index)\nmonthly_rate_data=US10Y.resample('M').mean().plot(title=\"Interest Rate for 10 Years Treasury - \",figsize=(20,5)\n                                                  ,legend=None,grid=True)\nplt.ylabel('RATE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_rate_data=US10Y.resample('M').mean()\nfrom matplotlib.lines import Line2D\nplt.figure(figsize=(20,5))\ncolors = ['red', 'green', 'blue']\nlines = [Line2D([0], [0], color=c, linewidth=3, linestyle='--') for c in colors]\nlabels = ['Monthly closed loans', 'Monthly Loan Revenue / 400000', '10 Years Interest Rate * 20']\n\nplt.plot(monthly_loan_num_data, color='red')\nplt.plot(loan_rev_data.resample('M').sum()/400000, color='green')\nplt.plot(monthly_rate_data*20, color='blue')\nplt.legend(lines, labels)\nplt.title('Comparing Monthly: Revenue, Closed_Loan_Num, VS US 10 Year Treasury Rates')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the strong positive correlation between Monthly Closed Loans and Monthly Revenue. This graph also suggest that, as interest rates goes down, banks monthly revenue and numbers of loans increases, and when the Rates goes up, both Monthly Closed Loans and Monthly Revenue for the Mortgage bank decline. Pearson correlation coefficient between Monthly Interest & Monthly loans Closed Data is -0.334, which clearly proves that Monthly Interest Rates & Monthly loans Closed Data is negatively correlated."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n    # Compute correlation matrix: corr_mat\n    corr_mat = scipy.stats.pearsonr()\n\n    # Return entry [0,1]\n    return corr_mat[0, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \n\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport sklearn\n\nfico = data['Qualification FICO']\nloan_amount=data['Loan Amount']\ncltv_data = data['CLTV']\nscipy.stats.pearsonr(cltv_data, fico)\nr_loan_amount_fico = scipy.stats.pearsonr(loan_amount, fico)\n# Print the result\nprint('Pearson correlation coefficient between FICO and Loan_Amount: ', r_loan_amount_fico)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_cltv_data_fico = scipy.stats.pearsonr(cltv_data, fico)\nprint('Pearson correlation coefficient between FICO and CLTV: ', r_cltv_data_fico)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_loan_amount_cltv_data = scipy.stats.pearsonr(loan_amount, cltv_data)\nprint('Pearson correlation coefficient between Loan Amount and CLTV: ', r_loan_amount_cltv_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_loan_num=np.array(monthly_loan_num_data,dtype=np.float)\nmonthly_loan_num=monthly_loan_num.flatten()\nmonthly_loan_rev=np.array(loan_rev_data.resample('M').sum(),dtype=np.float)\nmonthly_loan_rev=monthly_loan_rev.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_monthly_loan_num_data_monthly_loan_rev = scipy.stats.pearsonr(monthly_loan_num,monthly_loan_rev)\nprint('Pearson correlation coefficient between Loan number and Loan Revenue: ', r_monthly_loan_num_data_monthly_loan_rev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Acquire 1000 pairs bootstrap replicates of the Pearson correlation coefficient using the draw_bs_pairs() function you wrote in the previous exercise for CLTV data VS Qualification FICO Data and Monthly Loan_num_data VS. Monthly_loan_rev. Compute the 95% confidence interval for both using your bootstrap replicates. -We have created a NumPy array of percentiles to compute. These are the 2.5th, and 97.5th. By creating a list and convert the list to a NumPy array using np.array(). For example, np.array([2.5, 97.5]) would create an array consisting of the 2.5th and 97.5th percentiles."},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_r0=scipy.stats.pearsonr(cltv_data, fico)\npearson_r1=scipy.stats.pearsonr(loan_amount, fico)\n# Print results\nprint('CLTV data VS Qualification FICO Data       :', pearson_r0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Monthly Loan_Amount VS. FICO Data          :', pearson_r1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Walk"},{"metadata":{},"cell_type":"markdown","source":"Are Interest Rates or Monthly Loan Returns Prices a Random Walk?\n\nMost returns prices follow a random walk (perhaps with a drift). We will look at a time series of Monthly Sales Revenue, and run the 'Augmented Dickey-Fuller Test' from the statsmodels library to show that it does indeed follow a random walk. With the ADF test, the \"null hypothesis\" (the hypothesis that we either reject or fail to reject) is that the series follows a random walk. Therefore, a low p-value (say less than 5%) means we can reject the null hypothesis that the series is a random walk. Print out just the p-value of the test (adfuller_loan_rev_data[0] is the test statistic, and adfuller_loan_rev_data[1] is the p-value). Print out the entire output, which includes the test statistic, the p-values, and the critical values for tests with 1%, 10%, and 5% levels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the adfuller module from statsmodels\nfrom statsmodels.tsa.stattools import adfuller\nloan_rev_data=data[['Loan Amount']]\nloan_rev_data['date'] = pd.DatetimeIndex(data['Created Date'])\nloan_rev_data = loan_rev_data.set_index('date')\nmonthly_loan_rev_data= loan_rev_data.resample('M').sum()\nmonthly_loan_rev_data[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the ADF test on the monthly_loan_rev_data series and print out the results\nadfuller_loan_rev_data = adfuller(monthly_loan_rev_data['Loan Amount'], autolag='AIC')\nprint(adfuller_loan_rev_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just print out the p-value\nprint('The p-value of the test on loan_rev is: ' + str(adfuller_loan_rev_data[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Print in different format')\nprint('Results of Dickey-Fuller Test:')\ndfoutput = pd.Series(adfuller_loan_rev_data[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in adfuller_loan_rev_data[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to this test, p-value is very low (lower than 0.05). We reject the hypothesis that monthly_loan_rev_data follow a random walk."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_rev_data=data[['Loan Amount']]\nloan_rev_data['date'] = pd.DatetimeIndex(data['Created Date'])\nloan_rev_data = loan_rev_data.set_index('date')\nmonthly_loan_rev_data= loan_rev_data.resample('M').sum()\nmonthly_loan_rev_data[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the ADF test on the monthly_loan_rev_data series and print out the results\nadfuller_loan_rev_data = adfuller(monthly_loan_rev_data['Loan Amount'], autolag='AIC')\n\nprint(adfuller_loan_rev_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just print out the p-value\nprint('The p-value of the test on loan_rev is: ' + str(adfuller_loan_rev_data[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Print in different format')\nprint('Results of Dickey-Fuller Test:')\ndfoutput = pd.Series(adfuller_loan_rev_data[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in adfuller_loan_rev_data[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_rate=US10Y.resample('M').mean()\nmonthly_rate_data=monthly_rate['RATE']\nmonthly_rate_data[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the ADF test on the monthly_rate_data series and print out the results\nadfuller_monthly_rate_data = adfuller(monthly_rate_data)\nprint(adfuller_monthly_rate_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just print out the p-value\nprint('The p-value of the test on monthly_rate_data is: ' + str(adfuller_monthly_rate_data[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Results of Dickey-Fuller Test:')\ndfoutput = pd.Series(adfuller_monthly_rate_data[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in adfuller_monthly_rate_data[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\nprint(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Are Interest Rates Autocorrelated?"},{"metadata":{},"cell_type":"markdown","source":"When we look at daily changes in interest rates, the autocorrelation is close to zero. However, if we resample the data and look at annual changes, the autocorrelation is negative. This implies that while short term changes in interest rates may be uncorrelated, long term changes in interest rates are negatively autocorrelated. A daily move up or down in interest rates is unlikely to tell us anything about interest rates tomorrow, but a move in interest rates over a year can tell us something about where interest rates are going over the next year. And this makes some economic sense: over long horizons, when interest rates go up, the economy tends to slow down, which consequently causes interest rates to fall, and vice versa.\n\nOne of the really cool things that pandas allows us to do is resample the data. If we want to look at the data by monthly and anually. We can easily resample and sum it up. I’m using ‘M’ as the period for resampling which means the data should be resampled on a month boundary and 'A' for annual data'. Finally find the The autocorrelation of annual interest rate changes'''"},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y['change_rates'] = US10Y.diff()\nUS10Y['change_rates'] = US10Y['change_rates'].dropna()\nUS10Y.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute and print the autocorrelation of daily changes\nautocorrelation_daily = US10Y['change_rates'].autocorr()\nprint(\"The autocorrelation of daily interest rate changes is %4.2f\" %(autocorrelation_daily))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.index = pd.to_datetime(US10Y.index)\nmonthly_rate_data = US10Y['RATE'].resample(rule='M').last()\n#annual_data = annual_data.dropna()\n# Repeat above for annual data\nmonthly_rate_data['diff_rates'] = monthly_rate_data.diff()\nmonthly_rate_data['diff_rates'] = monthly_rate_data['diff_rates'].dropna()\nmonthly_rate_data['diff_rates'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrelation_monthly = monthly_rate_data['diff_rates'].autocorr()\nprint(\"The autocorrelation of monthly interest rate changes is %4.2f\" %(autocorrelation_monthly))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.index = pd.to_datetime(US10Y.index)\nannual_rate_data = US10Y['RATE'].resample(rule='A').last()\n# Repeat above for annual data\nannual_rate_data['diff_rates'] = annual_rate_data.diff()\nannual_rate_data['diff_rates'] = annual_rate_data['diff_rates'].dropna()\nannual_rate_data['diff_rates']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrelation_annual = annual_rate_data['diff_rates'].autocorr()\nprint(\"The autocorrelation of annual interest rate changes is %4.2f\" %(autocorrelation_annual))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Daily and monthly autocorrelation is small but the annual autocorrelation is large and negative"},{"metadata":{},"cell_type":"markdown","source":"Visual exploration is the most effective way to extract information between variables.\n\nWe can plot a barplot of the frequency distribution of a categorical feature using the seaborn package, which shows the frequency distribution of the mortgage dataset column"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nloan_type_count = data['Loan Type'].value_counts()\nsns.set(style=\"darkgrid\")\nax=sns.barplot(x=loan_type_count.index,y= loan_type_count.values, alpha=0.9)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.title('Frequency Distribution of Loan Types')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Loan Types', fontsize=12)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conventional loan type is top market for the Bank, secound is FHA Loan Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nunit_type_count = data['Unit Type'].value_counts()\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(20,5))\nax=sns.barplot(x=unit_type_count.index, y=unit_type_count.values, alpha=0.9)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.title('Frequency Distribution of Loan Types')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Loan Types', fontsize=12)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One Family, Two Family and Condos are the top unit types for the Bank"},{"metadata":{},"cell_type":"markdown","source":"# Data Collection"},{"metadata":{},"cell_type":"markdown","source":"Since we are only interested in the event log data we will only be using one of the tables. This table contains data about every mortgage application. Every action performed by the system or by a user on a mortgage application is logged, and the status before and after that specific action is logged. For our analysis we are mainly interested in the date and time at which each of the mortgage applications have entered the system. Besides Mortgage Application DataSet, we have joined two separate (10 Years US Treasury Rate, Home Supply Index) with our existing Mortgage Application DataSet to enhance predictive power of our model."},{"metadata":{},"cell_type":"markdown","source":"# Encoding Categorical Data"},{"metadata":{},"cell_type":"markdown","source":"There are different techniques to encode the categorical features to numeric quantities.\n\nThe techniques are as following:\n- Replacing values\n- Encoding labels\n- One-Hot encoding\n- Binary encoding\n- Backward difference encoding\n- Miscellaneous features\n- Replace Values\n\nLet's start with the most basic method, which is just replacing the categories with the desired numbers. This can be achieved with the help of the replace() function in pandas. The idea is that you have the liberty to choose whatever numbers we want to assign to the categories according to the business use case.\n\nIt's a good practice to typecast categorical features to a category dtype because they make the operations on such columns much faster than the object dtype. You can do the typecasting by using .astype() method on your columns like shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', index_col='Created Date', header=0, encoding='cp1252')\ndata_lc = data.copy()\ndata_lc['City'] = data_lc['City'].astype('category')\ndata_lc['Zip'] = data_lc['Zip'].astype('category')\ndata_lc['Loan Type'] = data_lc['Loan Type'].astype('category')\ndata_lc['Unit Type'] = data_lc['Unit Type'].astype('category')\ndata_lc['loan_purpose_code'] = data_lc['Loan Purpose'].astype('category')\ndata_lc.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['lo_code'] = data_lc['Loan Officer Name'].astype('category')\nlo_code =data_lc['lo_code']\ndata_lc.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding"},{"metadata":{},"cell_type":"markdown","source":"We can achieve the label encoding using scikit-learn's LabelEncoder:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\ndata_lc['loan_purpose_code'] = lb_make.fit_transform(data_lc['Loan Purpose'])\ndata_lc['loan_type_code'] = lb_make.fit_transform(data_lc['Loan Type'])\ndata_lc['unit_type_code'] = lb_make.fit_transform(data_lc['Unit Type'])\ndata_lc.head() #Results in appending a new column to df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label Encoding Another approach is to encode categorical values with a technique called \"label encoding\", which allows you to convert each value in a column to a number. Numerical labels are always between 0 and n_categories-1.\n\nWe can do label encoding via attributes .cat.codes on your DataFrame's column."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['lo_code'] = data_lc['Loan Officer Name'].astype('category')\nlo_code =data_lc['lo_code']\ndata_lc['city_code'] = data_lc['City'].cat.codes\ndata_lc['zip_code'] = data_lc['Zip'].cat.codes\nlo_code = data_lc['lo_code'].cat.codes\nlo_code[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One-Hot encoding"},{"metadata":{},"cell_type":"markdown","source":"The basic strategy is to convert each category value into a new column and assign a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc = pd.get_dummies(data_lc, columns=['Fix'], prefix = ['Fix'])\ndata_lc['Fix_True']\ndata_lc['Fix_False']\ndata_lc['Fix_True'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['Fix_True'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc[['Unit Type', 'unit_type_code']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['Unit Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" data_lc['unit_type_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc[['Loan Type', 'loan_type_code']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['Loan Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['unit_type_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc[['Loan Purpose', 'loan_purpose_code']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['loan_purpose_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Forward Fill Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Categorical value 'loan_purpose_code' has been update based on frequency of the value_counts\ndata_lc=data_lc.fillna(method='ffill')\ntotal_missing_data_lc=data_lc.isnull().sum()\nUS10Y= US10Y.fillna(method='ffill')\ndata_lc.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Machine Learning Modeling, we need to create new Data Frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data1 = data_lc[['Loan Amount', 'city_code', 'zip_code', \n                       'loan_purpose_code', 'Qualification FICO', 'unit_type_code', \n                       'loan_type_code', 'Fix_True', 'CLTV', 'LoanInMonth']]\nmodel_data1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data1.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing_mode_data1=model_data1.isnull().sum()\ntotal_missing_mode_data1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('model_data1 Keys: \\n',model_data1.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('model_data1 shape: ',model_data1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nloan_amt=model_data1[['Loan Amount']]\npd.plotting.autocorrelation_plot(loan_amt[::30])\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joining DataFrame"},{"metadata":{},"cell_type":"markdown","source":"Join two DataFrames model_data1 & US10Y save the results in model_data2"},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y = pd.read_csv('../input/mortgage-bank-loan/US10Y.csv', header=0, index_col='DATE', encoding='cp1252')\n#index_col='DATE'\nUS10Y.replace('.', -1, inplace=True)\nUS10Y= US10Y.replace(to_replace=[-1], value=[np.nan])\nUS10Y.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y= US10Y.fillna(method='ffill')\nUS10Y= US10Y[['RATE']].astype('float64')\ntotal_missing_rate=US10Y.isnull().sum()\ntotal_missing_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data2 = model_data1.join(US10Y)\ntotal_missing=model_data2.isnull().sum()\nmodel_data2= model_data2.fillna(method='ffill')\ntotal_missing_model_data2=model_data2.isnull().sum()\ntotal_missing_model_data2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Home Supply"},{"metadata":{},"cell_type":"markdown","source":"US Home Supply directly impact Housing Market and Mortgage Market. We have collected the data from FRED Integrate Monthly housing supply index data and merging with current dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"home_supply = pd.read_csv('../input/mortgage-bank-loan/MonHouseSupply.csv', header=0, index_col='DATE', encoding='cp1252')\nhome_supply.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing_home_supply=home_supply.isnull().sum()\nmodel_data = model_data2.join(home_supply)\ntotal_missing=model_data.isnull().sum()\nmodel_data= model_data.fillna(method='ffill')\ntotal_missing_home_supply=model_data.isnull().sum()\ntotal_missing_home_supply","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data3=model_data.copy()\nmodel_data3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA EXPLORATION"},{"metadata":{},"cell_type":"markdown","source":"Since our dataset can be grouped per day to create meaningful visualizations. The dataset contains data from October 2014 until December 2018. In order to get a feel of the amount of mortgage applications per day and the distribution of the mortgage applications, different visualizations can be made using Python. Two graphs have been created, which can be found in Figure 1 and Figure 2. Both of these graphs only contain the amount of mortgage applications on the weekdays. As there are almost no applications coming in on the weekends they have been excluded from the graphs. As can be seen from the graphs, there seems to be a seasonal pattern on a monthly level, but from these graphs it is not very clear. It also seems like there are some outliers, so these data points will have to be investigated to see if they will have to be included in our model, as there can be multiple underlying reasons for outliers in our dataset. It also seems there is an increase in mortgage applications during the last few months of each year. The amount of applications per day during these months is higher compared to the other months. This can have multiple explanations so this will have to be accounted for in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_amount_data = data[data['Loan Amount'].isin(St)]\nloan_amount_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct_loan_amount=sum(ct_data['Loan Amount'])\nfl_loan_amount=sum(fl_data['Loan Amount'])\nny_loan_amount=sum(ny_data['Loan Amount'])\nnj_loan_amount=sum(nj_data['Loan Amount'])\npa_loan_amount=sum(pa_data['Loan Amount'])\n\nloan_amount_per_state = [ct_loan_amount, fl_loan_amount, nj_loan_amount, ny_loan_amount, pa_loan_amount]\n\nprint('====================================================')\nprint('=========    Total Sales by State     ==============')\nprint(' ')\nprint('Total Sales in Cunnecticut   : $', ct_loan_amount)\nprint('Total Sales in Florida       : $', fl_loan_amount)\nprint('Total Sales in New York      : $', ny_loan_amount)\nprint('Total Sales in New Jersey    : $', nj_loan_amount)\nprint('Total Sales in Pennsylvania  : $', pa_loan_amount)\nprint(' ')\nprint('====================================================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_types=data['Loan Type'].unique()\ngroup_loan_types=data.groupby(data['Loan Type']).size()\nprint('Unique Loan Types        : ', loan_types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(' ')\nprint('====================================================')\n\nprint('Number of loan per Types : ', group_loan_types)\n\nprint(' ')\nprint('====================================================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\nx = np.arange(5)\nmoney = [1.5e5, 2.5e6, 5.5e6, 1.0e7, 2.0e7, 3.0e7, 4.0e7, 5.0e7, 6.0e7]\ndef millions(x, pos):\n    'The two args are the value and tick position'\n    return '$%1.1fM' % (x * 1e-6)\nformatter = FuncFormatter(millions)\nfig, ax = plt.subplots(figsize=(20, 5))\nax.yaxis.set_major_formatter(formatter)\na=sns.barplot(x=St, y=loan_amount_per_state)\nfor p in a.patches:\n    a.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(x, ('Connecticut', 'Florida', 'New Jersey', 'New York', 'Pennsylvania'))\nplt.ylabel('Loan Amount')\nplt.xlabel('Loan Origination per State')\nplt.xticks(rotation=60)\nplt.title('Mortgage Bank Loans per State')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eventhough Bank is Licensed for business in NY, NJ, CT, PA and FL, their strong mortgage market is in NY & NI"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', header=0, encoding='cp1252')\ndata.quantile([0.05, 0.95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Loan Amount'].quantile([0.05, 0.95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Loan Amount'].quantile([0.1, 0.90])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"St=['CT', 'FL', 'NJ', 'NY', 'PA']\nprint(St)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DataFrame subsets by State\nct_data = data[data['State'].isin(['CT'])]\nfl_data = data[data['State'].isin(['FL'])]\nny_data = data[data['State'].isin(['NY'])]\nnj_data = data[data['State'].isin(['NJ'])]\npa_data = data[data['State'].isin(['PA'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting data per column\nct_lo_amount = ct_data.reindex(('Created Date', 'First Name', 'Last Name', 'Loan Amount', 'City', 'unit_type_code', 'Loan Type'))\nfl_lo_amount = fl_data.reindex(('Created Date', 'First Name', 'Last Name', 'Loan Amount', 'City', 'Unit Type', 'Loan Type'))\nnj_lo_amount = nj_data.reindex(('Created Date', 'First Name', 'Last Name', 'Loan Amount', 'City', 'Unit Type', 'Loan Type'))\nny_lo_amount = ny_data.reindex(('Created Date', 'First Name', 'Last Name', 'Loan Amount', 'City', 'Unit Type', 'Loan Type'))\npa_lo_amount = pa_data.reindex(('Created Date', 'First Name', 'Last Name', 'Loan Amount', 'City', 'Unit Type', 'Loan Type'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xlabel('Loan origination Data')\nplt.xticks(rotation=60)\nplt.ylabel('Loan Amount')\nplt.title('MWB Loan Data for CT')\nplt.plot(ct_data['Created Date'], ct_data['Loan Amount'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the Loan amount Distribution for CT State"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('==============================')\nprint(' ')\n#Unit Type Loan Data\ntotal_unit_type = data.groupby(data['Unit Type']).size()\nprint('Loan originated in all States per unit types : \\n', total_unit_type)\n\nprint('==============================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unit Type Loan Data for CT\nprint('==============================')\nprint(' ')\nct_data_unit_type=ct_data['Loan Amount'].groupby(data['Unit Type']).size()\nprint('Loan originated in Cunnecticut per unit types : \\n', ct_data_unit_type)\n\nprint('==============================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unit Type Loan Data for FL\nprint('==============================')\nprint(' ')\nfl_data_unit_type=fl_data['Loan Amount'].groupby(data['Unit Type']).size()\nprint('Loan originated in Florida per unit types : \\n', fl_data_unit_type)\nprint('==============================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unit Type Loan Data for NJ\nprint('==============================')\nprint(' ')\nnj_data_unit_type=nj_data['Loan Amount'].groupby(data['Unit Type']).size()\nprint('Loan originated in New Jersey per unit types : \\n', nj_data_unit_type)\n\nprint('==============================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unit Type Loan Data for NY\nprint('==============================')\nprint(' ')\nny_data_unit_type=ny_data['Loan Amount'].groupby(data['Unit Type']).size()\nprint('Loan originated in New York per unit types : \\n', ny_data_unit_type)\n\nprint('==============================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unit Type Loan Data for PA\nprint('==============================')\nprint(' ')\npa_data_unit_type=pa_data['Loan Amount'].groupby(data['Unit Type']).size()\nprint('Loan originated in Pennsylvania per unit types : \\n', pa_data_unit_type)\n\nprint('==============================')\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unit Type Loan Data for CT\nct_data['Loan Amount'].groupby(data['Unit Type']).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loan data based on Unit Types (One Fami, Two Family etc) per State"},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_data = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', header=0, index_col = 'Loan Officer Name', encoding='cp1252')\n\n# Create a separate dataframe with the columns ['', 'total', 'voters']: results\nlo_df = lo_data[['Created Date', 'Loan Amount', 'Unit Type', 'Loan Type', 'City', 'Zip']]\n\n# Print the output of results.head()\nlo_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('=======================================================================')\nprint('**********************  Loan Statistics for Mortgage Bank ***********************')\nprint(' ')\n\n\nfrom scipy.stats import scoreatpercentile\nimport numpy as np\n\nq0 = scoreatpercentile(lo_df['Loan Amount'],10)\nq1 = scoreatpercentile(lo_df['Loan Amount'],25)\nq2 = scoreatpercentile(lo_df['Loan Amount'],55)\nq3 = scoreatpercentile(lo_df['Loan Amount'],75)\nq4 = scoreatpercentile(lo_df['Loan Amount'],90)\n\n\nprint('Average Loan Amount is               :  $', '%.2f' %lo_df['Loan Amount'].mean())\nprint('Median Loan Amount is                :  $', '%.2f' %lo_df['Loan Amount'].median())\nprint(' ')\nprint('Standard deviation of Loan Amount is :  $', '%.2f' %lo_df['Loan Amount'].std())\nprint(' ')\nprint('Minimum Loan Amount is               :  $', '%.2f' %lo_df['Loan Amount'].min())\nprint('Maximum Loan Amount is               :  $', '%.2f' %lo_df['Loan Amount'].max())\nprint(' ')\nprint('Total of Loan Amount is              :  $', '%.2f' %lo_df['Loan Amount'].sum())\nprint(' ')\nprint('10% of Loan Amount is below          :  $', '%.2f' %q0)\nprint('25% of Loan Amount is below          :  $', '%.2f' %q1)\nprint('50% of Loan Amount is below          :  $', '%.2f' %q2)\nprint('75% of Loan Amount is below          :  $', '%.2f' %q3)\nprint('90% of Loan Amount is below          :  $', '%.2f' %q4)\nprint(' ')\nprint('==========================================================================')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loan Statistics for the Bank"},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fico = lo_data[['Qualification FICO']]\nfico.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('=======================================================================')\nprint('**************** Qualification FICO  Statistics for Mortgage Bank ***************')\nprint(' ')\nfrom scipy.stats import scoreatpercentile\nimport numpy as np\n\nfico_q7 = scoreatpercentile(lo_data['Qualification FICO'],5)\nfico_q0 = scoreatpercentile(lo_data['Qualification FICO'],10)\nfico_q1 = scoreatpercentile(lo_data['Qualification FICO'],25)\nfico_q5 = scoreatpercentile(lo_data['Qualification FICO'],40)\nfico_q2 = scoreatpercentile(lo_data['Qualification FICO'],50)\nfico_q6 = scoreatpercentile(lo_data['Qualification FICO'],65)\nfico_q3 = scoreatpercentile(lo_data['Qualification FICO'],75)\nfico_q4 = scoreatpercentile(lo_data['Qualification FICO'],90)\nfico_q8 = scoreatpercentile(lo_data['Qualification FICO'],95)\n\n\nprint('Average FICO is                  :  $', '%.2f' %lo_data['Qualification FICO'].mean())\nprint('Median FICO is                   :  $', '%.2f' %lo_data['Qualification FICO'].median())\nprint('Standard deviation is            :  $', '%.2f' %lo_data['Qualification FICO'].std())\nprint('Minimum FICO is                  :  $', '%.2f' %lo_data['Qualification FICO'].min())\nprint('Maximum FICO is                  :  $', '%.2f' %lo_data['Qualification FICO'].max())\nprint(' ')\nprint('5% of FICO is below              :  $', '%.2f' %fico_q7)\nprint('10% of FICO is below             :  $', '%.2f' %fico_q0)\nprint('25% of FICO is below             :  $', '%.2f' %fico_q1)\nprint('40% of FICO is below             :  $', '%.2f' %fico_q5)\nprint('50% of FICO is below             :  $', '%.2f' %fico_q2)\nprint('65% of FICO is below             :  $', '%.2f' %fico_q6)\nprint('75% of FICO is below             :  $', '%.2f' %fico_q3)\nprint('90% of FICO is below             :  $', '%.2f' %fico_q4)\nprint('95% of FICO is below             :  $', '%.2f' %fico_q8)\n\nprint(' ')\nprint('==========================================================================')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fico_score = [fico_q7, fico_q0, fico_q1, fico_q5, fico_q2, fico_q6, fico_q3, fico_q4, fico_q8]\nfico_pct=['5% Loan Below', '10% Loan Below', '25% Loan Below', '40% Loan Below', '50% Loan Below', '65% Loan Below', '75% Loan Below','90% Loan Below', '95% Loan Below']\nplt.figure(figsize=(20,5))\nplt.xticks(rotation=75)\nplt.ylim((500,850))\na=sns.barplot(x=fico_pct, y=fico_score)\nfor p in a.patches:\n    a.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.ylabel('Qualification FICO Scores')\nplt.xlabel('Qualification FICO (%)')\nplt.title('Mortgage Loans Qualification FICO  Statistics')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mortgage Bank FICO Score Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('==============================================================================')\nprint('**************** Mortgage Loan Ofiicer'' Sales Volume per Loan Type ***************')\nprint(' ')\n\n\n###########################\nlo_loan = data[['Loan Officer Name', 'Loan Type', 'Created Date', 'Loan Amount']]\n\n#We can use groupby to organize the data by category and name.\nlo_loan_group = lo_loan.groupby(['Loan Officer Name', 'Loan Type']).sum()\n\nlo_loan_group_count = lo_loan.groupby(['Loan Officer Name', 'Loan Type']).count()\n\nlo_loan_group\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_loan_group_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_loan['Loan Type'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''The category representation looks good but we need to break it apart\nto graph it as a stacked bar graph. unstack can do this for us.'''\nlo_loan_group.unstack().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_loan_group_plot = lo_loan_group.unstack().plot(kind='bar',stacked=True,\n                                                  title=\"Total Sales by Loan Officers by Loan Type\",figsize=(20,5))\nlo_loan_group_plot.set_xlabel(\"Loan Officers\")\nlo_loan_group_plot.set_ylabel(\"Sales per Loan Type\")\nlo_loan_group_plot.legend([\"Commercial\",\"Conventional\",\"FHA\",\"Other\",\"VA\"], loc=2,ncol=1)\nplt.ylim(10000000, 140000000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('==============================================================================')\nprint('**************** MWB Loan Ofiicers Sales Volume per Unit Type ***************')\nprint(' ')\nlo_unit = data[['Loan Officer Name', 'Unit Type', 'Created Date', 'Loan Amount']]\nlo_unit_group = lo_unit.groupby(['Loan Officer Name', 'Unit Type']).sum()\nlo_unit_group_count = lo_unit.groupby(['Loan Officer Name', 'Unit Type']).count()\nlo_unit_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_unit_group_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_unit['Unit Type'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_unit_group.unstack().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_unit_group_plot = lo_unit_group.unstack().plot(kind='bar',stacked=True,title=\"Total Sales by Loan Officers by Unit Type\",figsize=(20, 5))\nlo_unit_group_plot.set_xlabel(\"Loan Officers\")\nlo_loan_group_plot.set_ylabel(\"Sales per Unit Type\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MWB Loan Officers Sales Volume per Unit Type"},{"metadata":{},"cell_type":"markdown","source":"The category representation looks good also displayed stacked bar graph and. unstack can do this for us.''' Mortgage Loan Ofiicer'' Sales Volume per Loan Type displayed and grahically displayed Loan Officer’s Sales Volume per Unit Type."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing data with date as without index\ndata = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', header=0, encoding='cp1252')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the really cool things that Pandas allows us to do is resample the data. We want to look at the data by month, we can easily resample and sum it all up. We're using ‘M’ as the period for resampling which means the data should be resampled on a month boundary."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns = data[['Loan Amount', 'Created Date']]\nloan_patterns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nIf we want to analyze the data by date,\nwe need to set the date column as the index using set_index .\n'''\n#Convert Date Index\nloan_patterns['date'] = pd.DatetimeIndex(data['Created Date'])\n\nloan_patterns = loan_patterns.set_index('date')\nloan_patterns.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_loan_rev=loan_patterns.resample('M').sum()\nloan_patterns.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns_month_plot = loan_patterns.resample('M').sum().plot(title=\"Mortgage Bank - Total Sales by Month\",\n                                                                  legend=False,figsize=(20,5))\nloan_patterns_month_plot.set_xlabel(\"Months\")\nloan_patterns_month_plot.set_ylabel(\"Monthly Slaes\")\nplt.xticks(rotation=45)\nplt.ylim((12000000, 35000000))\nfig = loan_patterns_month_plot.get_figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that monthly mortgage loan sales volume varies between 15M and 32M. Another interesting find is Loan sales are at the peak during summer seasons. Winter sales are normally slow but 2018 was an exception."},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns.resample('Q').sum()\nmonthly_loan_rev=loan_patterns.resample('Q').sum()\nloan_patterns.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oan_patterns_month_plot = loan_patterns.resample('Q').sum().plot(title=\"Mortgage Bank - Total Sales by Quaters\",\n                                                                 legend=False,figsize=(20,5))\nloan_patterns_month_plot.set_xlabel(\"Quaters\")\nloan_patterns_month_plot.set_ylabel(\"Quaterly Slaes\")\nplt.xticks(rotation=45)\nplt.ylim((50000000, 80000000))\nfig = loan_patterns_month_plot.get_figure()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig.savefig(\"./loan_patterns_month_plot.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mortgage Bank's Quarterly Sales Revenue varies between 50 (millions) and 77 (millions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthy Sales Sorted\n'''\nGrouping on a function of the index\nGroupby operations can also be performed on transformations\nof the index values. In the case of a DateTimeIndex,\nwe can extract portions of the datetime over which to group.\n'''\n\ndata = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', index_col='Created Date', parse_dates=True, encoding='cp1252')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a groupby object: by_day\nby_month = data.groupby(data.index.strftime('%B'))\nby_year = data.groupby(data.index.strftime('Y'))\nby_day = data.groupby(data.index.strftime('%a'))\n\n'''\n%a - day\n%m - month (01 to 12)\n%b - abbreviated month name\n%B - full month name\n%y - year without a century (range 00 to 99)\n%Y - year including the century\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create sum: units_sum\nmonthly_loan_amount_sum = by_month['Loan Amount'].sum()\nmonthly_loan_amount_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_loan_amount_sum = by_day['Loan Amount'].sum()\ndaily_loan_amount_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearly_loan_amount_sum = by_year['Loan Amount'].sum()\nyearly_loan_amount_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"formatter = FuncFormatter(millions)\nfig, ax = plt.subplots(figsize=(20,5))\nax.yaxis.set_major_formatter(formatter)\nplt.xlabel('Loan Origination Month')\nplt.ylabel('Loan Amount')\nplt.title('Mortgage Bank : 48 Months Total - Monthly Loan Sales')\nplt.xticks(rotation=90)\nplt.plot(monthly_loan_amount_sum)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_loan_amount_sum_sorted= monthly_loan_amount_sum.sort_values()\n# Print units_sum\nmonthly_loan_amount_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yearly_loan_amount_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"formatter = FuncFormatter(millions)\nfig, ax = plt.subplots(figsize=(20,5))\nax.yaxis.set_major_formatter(formatter)\nplt.xlabel('Loan Origination Month')\nplt.ylabel('Loan Amount')\nplt.title('MWB : 24 Months Total - Monthly Loan Sales (Sorted)')\nplt.xticks(rotation=90)\nplt.plot(monthly_loan_amount_sum_sorted)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lo_zip=data.groupby(['Loan Officer Name', 'Zip'])\nlo_zip_count = lo_zip['Zip'].count()\nlo_zip_filt = lo_zip.filter(lambda c:c['Zip'].count() > 3)\nlo_zip_filt_top10= (lo_zip_filt.groupby(['Zip']).size()).sort_values(ascending=False)\nprint('Top 10 marketing location by Zip Codes :', lo_zip_filt_top10.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top Ten Loan Origination Zip Code. Bank can focus more on marketting to generate more revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport calendar\nfrom time import strptime\ndata = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', header=0, encoding='cp1252')\nM_loan_city_num=data[['Created Date', 'LoanInMonth']]\nM_loan_city_num['date'] = pd.DatetimeIndex(data['Created Date'])\nM_loan_city_num = M_loan_city_num.set_index('date')\nM_loan_city_num_2017 = M_loan_city_num.loc['2017-1-1':'2017-9-16']\nprint('2017 : ')\nM_loan_city_num_2017.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_loan_city_num_2018 = M_loan_city_num.loc['2018-1-1':'2018-9-16']\nprint('2018 :')\nM_loan_city_num_2018.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M17= M_loan_city_num_2017.resample('M').count()\nM17.index=M17.index.month\nM_loan_city_num_2017.resample('M').count().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M18=M_loan_city_num_2018.resample('M').count()\nM18.index=M18.index.month\nM_loan_city_num_2018.resample('M').count().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_loan_city_num_2017_plot = M_loan_city_num_2017.resample('M').count().plot(title=\"MWB Total Monthly Sales for 2017\",\n                                                                            legend=False,figsize=(20,5))\nplt.xlabel('Loan origination Months')\nplt.xticks(rotation=60)\nplt.ylabel('Loans per Monthly')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_loan_city_num_2018_plot = M_loan_city_num_2018.resample('M').count().plot(title=\"MWB Total Monthly Sales for 2018\",\n                                                                            legend=False,figsize=(20,5))\nplt.xlabel('Loan origination Months')\nplt.xticks(rotation=60)\nplt.ylabel('Loans per Months')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see number of loan closed per month (Comparing data for 2017 & 2018)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as mpatches\ndata = pd.read_csv('../input/mortgage-bank-loan/mwb2014.csv', encoding='cp1252')\nM_loan_amount_num=data[['Created Date', 'Loan Amount']]\nM_loan_amount_num['date'] = pd.DatetimeIndex(data['Created Date'])\nM_loan_amount_num = M_loan_amount_num.set_index('date')\nM_loan_amount_num_2017 = M_loan_amount_num.loc['2017-1-1':'2017-9-30']\nM_loan_amount_num_2018 = M_loan_amount_num.loc['2018-1-1':'2018-9-30']\nM_amount_17= M_loan_amount_num_2017.resample('M').sum()\nM_amount_17.index=M_amount_17.index.month\nM_amount_18= M_loan_amount_num_2018.resample('M').sum()\nM_amount_18.index=M_amount_18.index.month\nM_amount_17.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_amount_18.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"formatter = FuncFormatter(millions)\nfig, ax = plt.subplots(figsize=(20,5))\nax.yaxis.set_major_formatter(formatter)\nplt.plot(M_amount_17, color='red')\nplt.plot(M_amount_18, color='blue')\nred_patch = mpatches.Patch(color='red', label=('2017 Loans (Jan - Sep) Total: ', M_amount_17['Loan Amount'].sum()) )\nblue_patch = mpatches.Patch(color='blue', label=('2018 Loans (Jan - Sep) Total: ',  M_amount_18['Loan Amount'].sum()) )\nplt.legend(handles=[red_patch, blue_patch])\nplt.xticks(range(len(M17)), [calendar.month_name[month] for month in M17.index], rotation=60)\nplt.xlabel('Loan origination Months')\nplt.ylabel('Loans per Months')\nplt.title('Mortgage Monthly Loan numbers 2017 & 2018: ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Mortgage Monthly Loan numbers 2017 & 2018"},{"metadata":{"trusted":true},"cell_type":"code","source":"M_amount_17= M_loan_amount_num_2017.resample('M').sum()\nM_amount_17.index=M_amount_17.index.month\nM_amount_18= M_loan_amount_num_2018.resample('M').sum()\nM_amount_18.index=M_amount_18.index.month\nM_amount_17.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_amount_18.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\nx = np.arange(5)\nmoney = [1.5e5, 2.5e6, 5.5e6, 1.0e7, 2.0e7, 3.0e7, 4.0e7, 5.0e7, 6.0e7]\ndef millions(x, pos):\n    'The two args are the value and tick position'\n    return '$%1.1fM' % (x * 1e-6)\n\nformatter = FuncFormatter(millions)\nfig, ax = plt.subplots(figsize=(20,5))\nax.yaxis.set_major_formatter(formatter)\nplt.plot(M_amount_17, color='red')\nplt.plot(M_amount_18, color='blue')\nred_patch = mpatches.Patch(color='red', label=('2017 Loans (Jan - Sep) Total: ', M_amount_17['Loan Amount'].sum()) )\nblue_patch = mpatches.Patch(color='blue', label=('2018 Loans (Jan - Sep) Total: ',  M_amount_18['Loan Amount'].sum()) )\nplt.legend(handles=[red_patch, blue_patch])\nplt.xticks(range(len(M17)), [calendar.month_name[month] for month in M17.index], rotation=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In 2017, Mortgage Bank sales revenue was (M_amount_17.sum()) = $180M\n\nIn 2018, Mortgage Bank sales revenue was (M_amount_18.sum()) = $197M\n\nWe have seen numbers of loan closed went down, but sales revenue went up. Main reason behind this, housing price went up, which drive average loan amount to increase. As a result total sales went up for 2018 comparing 2017"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Pearson correlation coefficient between Loan number and Loan Revenue: ', r_monthly_loan_num_data_monthly_loan_rev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen earlier earlier, there is positive strong correlation (0.844) between Loan number and Loan Revenue.\n\nMonthly Loan numbers and Monthly Sales volumes are strongly positively correlated. Once we analyze number of the loans closed per month, we find the similarity between sales volume and loan numbers. As average loan numbers goes up, total monthly sales volume goes up. Any given months, if the number of loans closed are higher; we find that average loan amounts are low for that particular months. In other words, this may suggest that loan processing requirements and guidelines loans with higher loan amount take longer time to close the loans with lower loan amounts.\n\nVisual exploration is the most effective way to extract information between variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab  #Plotting\nimport scipy.stats as stats # scintific calculation\nplt.figure(figsize=(20,5))\nstats.probplot(model_data['Loan Amount'], dist=\"norm\", plot=pylab)\npylab.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(model_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(model_data.drop(['Loan Amount'],1))\ny = np.array(model_data['Loan Amount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, columns in enumerate(X[1:5]):\n    plt.figure(figsize=(20, 5))\n    plt.scatter(X[:, index], y, color='g')\n    plt.ylabel('Loan Amount', size=10)\n    plt.xlabel(columns, size=10)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter plot for each (Column 1-5) feature with respect to Loan Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, columns in enumerate(X[6:10]):\n    plt.figure(figsize=(20, 5))\n    plt.scatter(X[:, index], y, color='b')\n    plt.ylabel('Loan Amount', size=15)\n    plt.xlabel(columns, size=15)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter plot for each (Column 6-10) feature with respect to Loan Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.hist(model_data['RATE'])\nplt.title(\"RATE\")\nplt.xlabel(\"US10Y Rate Distribution\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This distribution is somewhat normal distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nplt.hist(model_data['Home'])\nplt.title(\"New Home Supply\")\nplt.xlabel(\"Housing Supply\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is somewhat left skewed with a mean to the left"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nax=sns.distplot( model_data['RATE'] , color=\"blue\", label=\"US 10 Years Treasury Rate\")\nax=sns.distplot( model_data['Home'] , color=\"green\", label=\"New Home Supply\")\nplt.legend()\nplt.title(\"US 10 Years Treasury Rate vs. New Home Supply\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Home Supply goes up, RATE goes up**\n\nThe distribution plot comparing US 10 Years Treasury Rate & New Home Supply shows that US 10Y RATE is normally distributed and New Home Supply is skewed to the right. Rate is the key component for the government to keep rising housing price in check. Once rate goes up, borrowers buying power will go down, that will keep housing price in check."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_data = model_data.drop(['loan_purpose_code', 'Qualification FICO' ],1)\nsns.pairplot(fit_data, hue = 'loan_type_code',corner=True,palette='Set2',diag_kind=\"hist\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot colored by continent for years 2000-2007\nsns.pairplot(fit_data[fit_data['CLTV'] >= 8],\n             vars = ['Loan Amount', 'loan_type_code', 'LoanInMonth'],\n             hue = 'unit_type_code',corner=True,palette='Set2',diag_kind=\"hist\")\n# Title\nplt.suptitle('Pair Plot of Mortgage Data for 2014-2018 for CLTV over 80%',\n             size = 12);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate correlation coefficient between two arrays\ndef corr(x, y, **kwargs):\n    # Calculate the value\n    coef = np.corrcoef(x, y)[0][1]\n    # Make the label\n    label = r'$\\rho$ = ' + str(round(coef, 2))\n\n    # Add the label to the plot\n    ax = plt.gca()\n    ax.annotate(label, xy = (0.2, 0.95), size = 20, xycoords = ax.transAxes)\n# Create a pair grid instance\ngrid = sns.PairGrid(data= fit_data[fit_data['CLTV'] > 8],\n                    vars = ['Loan Amount', 'unit_type_code',\n       'loan_type_code', 'LoanInMonth'], height = 5)\n# Map the plots to the locations\ngrid = grid.map_upper(plt.scatter, color = 'darkred')\ngrid = grid.map_upper(corr)\ngrid = grid.map_lower(sns.kdeplot, cmap = 'Reds')\ngrid = grid.map_diag(plt.hist, bins = 10, edgecolor =  'k', color = 'darkred')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculated correlation coefficient between two arrays: data and vars=['Loan Amount', 'unit_type_code', 'loan_type_code', 'LoanInMonth']\n\n**FEATURE SELECTION** Feature Selection is one of the core concepts in machine learning which hugely impacts the performance of your model. The data features that you use to train your machine learning models have a huge influence on the performance you can achieve. How to select features and what are Benefits of performing feature selection before modeling your data? Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise. Improves Accuracy: Less misleading data means modeling accuracy improves.Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster. Feature Selection Methods: \n\nWe will share 3 Feature selection techniques that are easy to use and also gives good results. \n-  Univariate Selection \n- Feature Importance \n- Correlation Matrix with Heatmap"},{"metadata":{},"cell_type":"markdown","source":"# Univariate Selection"},{"metadata":{},"cell_type":"markdown","source":"Statistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\nThe example below uses the chi-squared (chi²) statistical test for non-negative features to select 10 of the best features from the Mobile Price Range Prediction Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data=model_data3.copy()\nmodel_data['Qualification FICO']=model_data['Qualification FICO'].astype('int64')\nmodel_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making sure Target Variable (Qualification FICO) is integer. In order to reduce the weight, we will be scalling."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scalling data\nmodel_data['Loan Amount']=model_data['Loan Amount']/100000\nmodel_data['Loan Amount']=model_data['Loan Amount'].astype('float64')\nmodel_data['city_code']=model_data['city_code']/100\nmodel_data['city_code']=model_data['city_code'].astype('float64')\nmodel_data['zip_code']=model_data['zip_code']/100\nmodel_data['zip_code']=model_data['zip_code'].astype('float64')\nmodel_data['Qualification FICO']=model_data['Qualification FICO']/100\nmodel_data['CLTV']=model_data['CLTV']/10\nmodel_data['CLTV']=model_data['CLTV'].astype('float64')\nmodel_data['LoanInMonth']=model_data['LoanInMonth']/10\nmodel_data['LoanInMonth']=model_data['LoanInMonth'].astype('float64')\nmodel_data['Qualification FICO']=model_data['Qualification FICO'].astype('float64')\nmodel_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nmodel_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(model_data.drop(['Fix_True'],1))\ny = np.array(model_data['Fix_True'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(model_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concat two dataframes for better visualization\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores.nlargest(11,'Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(20,5))\nax=sns.barplot(x=featureScores['Specs'], y=round((featureScores['Score']),2), alpha=0.9)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.title('Frequency Distribution of Features')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Specs', fontsize=12)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"The feature importance of each feature of our dataset by using the feature importance property of the model. Feature importance gives us a score for each feature of our data, the higher the score more important or relevant is the feature towards your output variable. Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data['CLTV']=model_data['CLTV'].astype('int64')\nimport pandas as pd\nimport numpy as np\n\nX = np.array(model_data.drop(['CLTV'],1))\ny = np.array(model_data['CLTV'])    #target column\nZ = model_data.drop(['loan_purpose_code'],1) #Max Col = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nmodel.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot graph of feature importances for better visualization\nplt.figure(figsize=(20,5))\nfeat_importances = round((pd.Series(model.feature_importances_, index=Z.columns)),3)\nax=feat_importances.nlargest(11).plot(kind='bar')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix with Heatmap"},{"metadata":{},"cell_type":"markdown","source":"Correlation states how the features are related to each other or the target variable.\n\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nHeatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nX = np.array(model_data.drop(['CLTV'],1))\ny = np.array(model_data['CLTV'])    #target column\n#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Let's Drop Fretures that are not important '''\nfit_data = model_data.drop(['loan_purpose_code', 'Qualification FICO' ],1)\nfit_data.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['Fix_True'],1))\ny = np.array(fit_data['Fix_True'])    #target column\nX[:5,] #print 1st 5 row of input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Target variable is based on the five input rows above. \\nFix Mortgage = 1 & ARM (Adjustable Rate Mortgage) = 0 : \\n',y[:5,])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA (Principal Component Analysis)"},{"metadata":{},"cell_type":"markdown","source":"PCA use PCA to de-correlate these measurements, then plot the de-correlated points and measure their Pearson correlation. Compute Pearson correlation coefficient between two arrays."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pearson_r(x, y):\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x, y)\n    # Return entry [0,1]\n    return corr_mat[0, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import PCA\nfrom sklearn.decomposition import PCA\n# Create PCA instance: model\nmodel = PCA()\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(model_data)\n# Assign 0th column of pca_features: xs\nxs = pca_features[:,0]\n# Assign 1st column of pca_features: ys\nys = pca_features[:,1]\nplt.figure(figsize=(20,5))\nplt.scatter(xs, ys) # Scatter plot xs vs ys\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Variance of the PCA features"},{"metadata":{},"cell_type":"markdown","source":"The dataset is 10-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA features to find out. As before, samples is a 2D array, where each row represents a fish.\n\nWe'll need to standardize the features first. Tthe use of principal component analysis for dimensionality reduction, for visualization of high-dimensional data, for noise filtering, and for feature selection within high-dimensional data. Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines.\n\nGiven any high-dimensional dataset, I tend to start with PCA in order to visualize the relationship between points ), to understand the main variance in the data and to understand the intrinsic dimensionality (by plotting the explained variance ratio)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\nimport numpy as np\nX = np.array(fit_data.drop(['Fix_True'],1))\ny = np.array(fit_data['Fix_True'])\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create scaler: scaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a PCA instance: pca\npca = PCA()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the pipeline to 'samples'\npipeline.fit(fit_data)\npca.n_components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the explained variances\ncolumns = ['Loan Amount', 'zip_code', 'loan_purpose_code', 'Qualification FICO', 'unit_type_code',\n       'loan_type_code', 'Fix_True', 'CLTV', 'RATE', 'Home']\nfor x in ax.get_xticklabels(minor=True):\n    columns.set_rotation(45)\n    print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we want to know how many principal components we can choose for our new feature subspace?\n\nA useful measure is the so-called “explained variance ratio“. \n\nThe explained variance ratio tells us how much information (variance) can be attributed to each of the principal components. We can plot bar graph between no. of features on X axis and variance ratio on Y axis"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = range(pca.n_components_)\nfeature_names = features = range(pca.n_components_)\nplt.figure(figsize=(20,5))\nax=plt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(feature_names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit_transform(X)\nprint(pca.mean_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.singular_values_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.n_components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.noise_variance_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features with high variance ratio\nText(0,0,'Loan Amount') = 4.45 \nText(3,0,'unit_type_code') = 7.1\nText(5,0,'Fix_True') = 4.2 \n\nFinding Correlation between Features and Target Variable in mortgage Dataset using Heatmap\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = model_data.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')\nplt.title('Correlation between Features and Target Variable in mortgage Dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us load the basic packages needed for the PCA analysis\npca = PCA().fit(fit_data)\nplt.figure(figsize=(20,5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can reduce to six to get the 98% accurary"},{"metadata":{},"cell_type":"markdown","source":"# PCA as dimensionality reduction"},{"metadata":{},"cell_type":"markdown","source":"Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n\nHere is an example of using PCA as a dimensionality reduction transform:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=6)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"transformed shape:\", X_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformed data has been reduced to a 6 dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = pca.inverse_transform(X_pca)\nplt.figure(figsize=(20,5))\nplt.scatter(X[:, 0], X[:, 7], alpha=0.7, c='red')\nplt.scatter(X_new[:, 0], X_new[:, 5], alpha=0.5, c='blue')\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n\nThis reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."},{"metadata":{},"cell_type":"markdown","source":"# MODELING"},{"metadata":{},"cell_type":"markdown","source":"In the Modeling stage we discuss the activities related to the model building part of our project. A selection of five modeling techniques is made that are applicable to our capstone project. From each of these five modeling techniques, a model is built with the feature set provided earlier, and the models are validated using repeated cross-validation."},{"metadata":{},"cell_type":"markdown","source":"## SELECTION OF MODELING TECHNIQUES"},{"metadata":{},"cell_type":"markdown","source":"For our modeling we use a combination of predictive techniques. Multiple techniques are selected and applied on the data. For the non-linear regression techniques, we use Support Vector Regression (SVR) and Neural Networks (NN). SVR has shown to obtain excellent performances in regression and time series applications. Neural Networks are a widely used method for time series data that generally gives mixed results.\n\nAnother technique we use is Classification and Decession Trees, which is a simple technique that is easy to visualize. Also two ensemble techniques are included, in order to improve the performance of the Classification and Regression Trees. These ensemble techniques are Gradient Boosting Machines (GBM) and Random Forests (RF). These techniques create a multitude of regression trees and select a combination of them in order to maximize the performance."},{"metadata":{},"cell_type":"markdown","source":"## MODEL BUILDING"},{"metadata":{},"cell_type":"markdown","source":"Using these five techniques (ARIMA, Linear Regression, Logistic Regression, SVM, SVR, Decision Tree, RF, and KNN) we can create five models. We use the list of features mentioned in section as input for our models. A total of 12 features are included, the remaining features were excluded after performing feature selection. For each of the five models hyperparameters were tuned, using grid search. Hyperparameters are the model-specific parameters that are used for optimizing the model. They generally have to be tuned in order to optimize the model’s performance, and reduce the variance and bias of the model. By training the model with different values of the size and the decay, and evaluating its performance, we can select the hyperparameters that result in the best performing model in terms of predictive power."},{"metadata":{},"cell_type":"markdown","source":"# ARMA Model"},{"metadata":{},"cell_type":"markdown","source":"Estimating an AR Model\n\nWe will estimate the AR(1) parameter, ϕ, of one of the Rate, Revenue, Loan_num, series that generated in the earlier . Since the parameters are known for a series, it is a good way to understand the stimation routines before applying it to real data. For monthly_rate_data with a true ϕ of 0.9, we will print out the estimate of ϕ. In addition, we will also print out the entire output that is produced when you fit a time series, so we can get an idea of what other tests and summary statistics are available in statsmodels"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n                        FutureWarning)\nimport warnings\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n                        FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US10Y.index = pd.to_datetime(US10Y.index)\nmonthly_rate_data = US10Y['RATE'].resample(rule='M').last()\n# Import the ARMA module from statsmodels\nfrom statsmodels.tsa.arima_model import ARMA\n# Fit an AR(1) model to the first simulated data\nmod_rate = ARMA(np.asarray(monthly_rate_data), order=(1,0))\nres_rate = mod_rate.fit()\n# Print out summary information on the fit\nprint(res_rate.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print out the estimate for the constant and for phi\nprint(\"When the true phi=0.9, the estimate of phi (and the constant) are:\")\nprint(res_rate.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Forecasting with an AR Model\n\nIn addition to estimating the parameters of a model, we can also do forecasting using statsmodels. The in-sample is a forecast of the next data point using the data up to that point, and the out-of-sample forecasts any number of data points in the future. These forecasts can be made using either the predict() method if we want the forecasts in the form of a series of data, or using the plot_predict() method we you want a plot of the forecasted data. We will supply the starting point for forecasting and the ending point, which can be any number of data points after the data set ends. For the simulated series Monthly Interest Rate with ϕ=0.9, we will plot in-sample and out-of-sample forecasts.\n\nBeing able to forecast interest rates is of enormous importance, not only for bond investors but also for individuals like new homeowners who must decide between fixed and floating rate mortgages.\n\nThere is some mean reversion in interest rates over long horizons. In other words, when interest rates are high, they tend to drop and when they are low, they tend to rise over time. Currently they are below long-term rates, so they are expected to rise, but an AR model attempts to quantify how much they are expected to rise."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARMA\n# Forecast interest rates using an AR(1) model\nmod_monthly_rate_data = ARMA(monthly_rate_data, order=(1,0))\nres = mod_monthly_rate_data.fit()\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nax=res.plot_predict(start=0, end='2021',ax=ax)\nplt.legend(fontsize=8)\nplt.ylabel('Rate')\nplt.xlabel('Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have only used only FOUR years of Monthly Interest Rate Data, we can see the short term downward momentum on the interest rate\n\nA daily move up or down in interest rates is unlikely to tell us anything about interest rates tomorrow, but a move in interest rates over a year can tell us something about where interest rates are going over the next year. The DataFrame daily_data contains daily data of 10-year interest rates from 1962 to 2017"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_data = pd.read_csv('../input/mortgage-bank-loan/DGS10.csv', index_col='DATE')\n#Data Cleaning\ndaily_data.replace('.', -1, inplace=True)\ndaily_data= daily_data.replace(to_replace=[-1], value=[np.nan])\ndaily_data = daily_data.dropna()\ndaily_data['DGS10']= daily_data['DGS10'].astype('float64')\ndaily_data['change_rates'] = daily_data.diff()\ndaily_data = daily_data.dropna()\n#Convert index to datetime\ndaily_data.index = pd.to_datetime(daily_data.index)\nannual_data = daily_data['DGS10'].resample(rule='A').last()\nannual_data[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annual_data = annual_data.dropna()\n# Repeat above for annual data\nannual_data['diff_rates'] = annual_data.diff()\nannual_data['diff_rates'] = annual_data['diff_rates'].dropna()\nannual_data['diff_rates'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute and print the autocorrelation of daily changes\nautocorrelation_daily = daily_data['change_rates'].autocorr()\nprint(\"The autocorrelation of daily interest rate changes is %4.2f\" %(autocorrelation_daily))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute and print the autocorrelation of annual changes\nautocorrelation_annual = annual_data['diff_rates'].autocorr()\nprint(\"The autocorrelation of annual interest rate changes is %4.2f\" %(autocorrelation_annual))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how the daily autocorrelation is small (0.07) but the annual autocorrelation is large and negative (-0.22)"},{"metadata":{"trusted":true},"cell_type":"code","source":"annual_rate=daily_data.resample('A').mean()\nannual_rate_data=annual_rate['DGS10']\n# Import the ARMA module from statsmodels\nfrom statsmodels.tsa.arima_model import ARMA\n# Forecast interest rates using an AR(1) model\nmod_annual_rate = ARMA(annual_rate_data, order=(1,0))\nres_annual_rate = mod_annual_rate.fit()\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nres_annual_rate.plot_predict(start=0, end='2024',ax=ax)\nplt.ylabel('Rate')\nplt.xlabel('Year')\nplt.legend(fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over long horizons, when interest rates go up, the economy tends to slow down, which consequently causes interest rates to fall, and vice versa.\n\nAccording to an AR(1) model, 10-year interest rates are forecasted to rise from 2.16%, towards the end of 2017 to 3.35% in five years"},{"metadata":{},"cell_type":"markdown","source":"# Forecast expected monthly closed loans"},{"metadata":{},"cell_type":"markdown","source":"Our Mortgage DataSet contains data from Oct 2014 to December 2018. Let’s forecast Monthly Closed Loans and Monthly Revenue. We will forecast monthly_loan_num_data using an AR(1) model"},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_rate=daily_data.resample('M').sum()\nmonthly_rate_data=monthly_rate['DGS10']\n# Import the ARMA module from statsmodels\nfrom statsmodels.tsa.arima_model import ARMA\n# Forecast interest rates using an AR(1) model\nmod_monthly_rate = ARMA(monthly_rate_data, order=(1,0))\nres_monthly_rate = mod_monthly_rate.fit()\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nres_annual_rate.plot_predict(start=0, end='2024',ax=ax)\nplt.legend(fontsize=8)\nplt.ylabel('Rate')\nplt.xlabel('Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over long horizons, when interest rates go up, the economy tends to slow down, which consequently causes interest rates to fall, and vice versa.\n\nAccording to an AR(1) model, 10-year interest rates are forecasted to rise from 2.16%, towards the end of 2017 to 3.35% in five years"},{"metadata":{},"cell_type":"markdown","source":"# Forecast expected monthly closed loans"},{"metadata":{},"cell_type":"markdown","source":"Our Mortgage DataSet contains data from Oct 2014 to December 2018. Let’s forecast Monthly Closed Loans and Monthly Revenue. We will forecast monthly_loan_num_data using an AR(1) model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import the ARMA module from statsmodels\nfrom statsmodels.tsa.arima_model import ARMA\nM_loan_city_num.resample('M').last()\nmonthly_loan_num_data= M_loan_city_num.resample('M').last()\nmonthly_loan_num_data=monthly_loan_num_data['LoanInMonth']\n# Forecast monthly_loan_num_data using an AR(1) model\nmod_monthly_loan_num_data = ARMA(monthly_loan_num_data, order=(1,0))\nres_monthly_loan_num_data = mod_monthly_loan_num_data.fit()\n\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nax=res_monthly_loan_num_data.plot_predict(start=0, end='2022',ax=ax)\nplt.legend(fontsize=12)\nplt.title('Monthly Loan Application Forecast')\nplt.ylabel('Loan in Month')\nplt.xlabel('Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we have plotted the original series and the forecasted series. With 95% confidence interval, Expected Loans per month will be around 50. Low end is 28 & High End is 70"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import the ARMA module from statsmodels\nfrom statsmodels.tsa.arima_model import ARMA\nM_loan_city_num.resample('Q').last()\nQ_loan_num_data= M_loan_city_num.resample('Q').last()\nQ_loan_num_data=Q_loan_num_data['LoanInMonth']\n# Forecast quaterly_loan_num_data using an AR(1) model\nmod_Q_loan_num_data = ARMA(Q_loan_num_data, order=(1,0))\nres_mod_Q_loan_num_data = mod_Q_loan_num_data.fit()\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nres_mod_Q_loan_num_data.plot_predict(start=0, end='2020',ax=ax)\nplt.legend(fontsize=14)\nplt.title('Quaterly Loan Application Forecast')\nplt.ylabel('Loan in Month')\nplt.xlabel('Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we have plotted the original series and the forecasted series. With 95% confidence interval, Expected Loans per month will be around 47. Low end is 20 & High End is 75"},{"metadata":{},"cell_type":"markdown","source":"# Forecast expected monthly revenue"},{"metadata":{},"cell_type":"markdown","source":"![](http://)![](http://)![](http://)![](http://)![](http://)Similarly, we ma[](http://)y forecast expected monthly revenue and plot original series and the forecasted series"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod_monthly_loan_rev_data = ARMA(monthly_loan_rev_data, order=(1,0))\n\nres_monthly_loan_rev_data = mod_monthly_loan_rev_data.fit()\nprint(\"The AIC for an AR(1) is: \", res_monthly_loan_rev_data.aic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nres_monthly_loan_rev_data.plot_predict(start=0, end='2022',ax=ax)\nplt.legend(fontsize=16)\nplt.title('Monthly Sales Revenue Forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 95% confidence interval, Expected Revenue per month will be around 22M. Low end is 13M & High End is 32M"},{"metadata":{},"cell_type":"markdown","source":"# Forecasting Quarterly Sales Revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_patterns['date']= pd.DatetimeIndex(data['Created Date'])\nloan_patterns = loan_patterns.set_index('date')\nloan_patterns.index\nquarterly_revenue_data = loan_patterns['Loan Amount'].resample(rule='Q').sum()\nmod_quarterly_loan_rev_data = ARMA(quarterly_revenue_data, order=(1,0))\nres_quarterly_loan_rev_data = mod_quarterly_loan_rev_data.fit()\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nres_quarterly_loan_rev_data.plot_predict(start=0, end='2022',ax=ax)\nplt.legend(fontsize=10)\nplt.title('Quarterly Sales Revenue Forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 95% confidence interval, Expected Revenue per Quarters will be around 67M. Low end is 52M & High End is 80M"},{"metadata":{},"cell_type":"markdown","source":"# Forecasting Annual Sales Revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"annual_revenue_data = loan_patterns['Loan Amount'].resample(rule='A').sum()\nmod_annual_loan_rev_data = ARMA(annual_revenue_data, order=(1,0))\nres_annual_loan_rev_data = mod_annual_loan_rev_data.fit()\n# Plot the original series and the forecasted series\nfig, ax = plt.subplots(figsize=(20,5))\nres_annual_loan_rev_data.plot_predict(start=0, end='2022',ax=ax)\nplt.legend(fontsize=14)\nplt.title('Annual Sales Revenue Forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graph, we have plotted the original series and the forecasted series. We are expecting little drop in annual mortgage revene. Our current annual revenue is $270M. By end of 2021, with 95% confidence interval, Expected Revenue per Years will be around 220M. Low end is 70M & High End is 360M. With more annual data, we should be able to do better annual revenue prediction."},{"metadata":{},"cell_type":"markdown","source":"# Linear regression"},{"metadata":{},"cell_type":"markdown","source":"Purpose of linear regression: Given a dataset containing predictor variables X and outcome/response variable Y, linear regression can be used to:\n\nBuild a predictive model to predict future values, using new data X where Y is unknown. Model the strength of the relationship between each independent variable X_i and Y Many times, only a subset of independent variables X_i will have a linear relationship with Y Need to figure out which X_i contributes most information to predict Y It is in many cases, the first pass prediction algorithm for continuous outcomes.\n\nLinear Regression is a method to model the relationship between a set of independent variables X (also knowns as explanatory variables, features, predictors) and a dependent variable Y. This method assumes the relationship between each predictor X is linearly related to the dependent variable Y.\n\nIndependence means that the residuals are not correlated -- the residual from one prediction has no effect on the residual from another prediction. Correlated errors are common in time series analysis and spatial analyses."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split # for train and test set split\nfrom sklearn.model_selection import cross_val_score #Sklearn.model_seletion is used instead of sklearn.cross_validation to avoid\n#warning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(model_data.drop(['Loan Amount'],1))\ny = np.array(model_data['Loan Amount'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"size of the training feature set is\",X_train.shape)\nprint(\"size of the test feature set is\",X_test.shape)\nprint(\"size of the training Target set is\",y_train.shape)\nprint(\"size of the test Target set is\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear regression\nfrom sklearn.linear_model import LinearRegression #import from sklearn\nlinear_reg= LinearRegression() # instantiated linreg\nlinear_reg.fit(X_train,y_train) #fit the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict using X_test\npredicted_train= linear_reg.predict(X_train)\npredicted_test= linear_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error # import mse from sklearn\n#calculate root mean squarred error\nrmse_train=np.sqrt(mean_squared_error(y_train, predicted_train))\nrmse_test=np.sqrt(mean_squared_error(y_test, predicted_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The train root mean squarred error is :', rmse_train)\nprint('The test root mean squarred error is  :', rmse_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Linear Regression coefficient parameters are :', linear_reg.coef_ )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The Linear Regression intercept value is :', linear_reg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE of the test data is closer to the training RMSE (and lower) if you have a well trained model. It will be higher if we have an overfitted model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics # import metrics from sklearn\nRsquared=linear_reg.score(X_train,y_train) # to determine r square Goodness of fit\n# how good the model fits the training data can be determined by R squared metric which is here 0.12\nRsquared\nprint('The R squared metric is :', Rsquared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R^2 = 0.12\n\nThe R^2 in scikit learn is the coefficient of determination. It is 1 - residual sum of square / total sum of squares.\n\nSince R^2 = 1 - RSS/TSS, the only case where RSS/TSS > 1 happens when our model is even worse than the worst model assumed (which is the absolute mean model).\n\nhere RSS = sum of squares of difference between actual values(yi) and predicted values(yi^) and TSS = sum of squares of difference between actual values (yi) and mean value (Before applying Regression). So you can imagine TSS representing the best(actual) model, and RSS being in between our best model and the worst absolute mean model in which case we'll get RSS/TSS < 1. If our model is even worse than the worst mean model then in that case RSS > TSS(Since difference between actual observation and mean value < difference predicted value and actual observation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# K fold cross validation\n# cross validation score\ncv_score= cross_val_score(LinearRegression(),X,y,scoring='neg_mean_squared_error', cv=10) # k =10\nprint('cv_score is :\\n', cv_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean squared error\nprint('cv_score is :', cv_score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Root mean squared error\nrmse_cv= np.sqrt(cv_score.mean() * -1)\nprint('The cross validation root mean squarred error is :', rmse_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With Linear regressor we are able to predict the model with RMSE: \n- train RMSE                                : 2.349923482270037 \n- test RMSE                                 : 3.9655849638809144\n- R squared                                 : 0.12 \n- cross validation root mean squarred error : 2.75"},{"metadata":{},"cell_type":"markdown","source":"# Fitting Linear Regression using statsmodels"},{"metadata":{},"cell_type":"markdown","source":"Statsmodels is a great Python library for a lot of basic and inferential statistics. It also provides basic regression functions using an R-like syntax, so it's commonly used by statisticians. The version of least-squares we will use in statsmodels is called ordinary least-squares (OLS). There are many other versions of least-squares such as partial least squares (PLS) and weighted least squares (WLS)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n# statsmodels works nicely with pandas dataframes\n# The thing inside the \"quotes\" is called a formula, a bit on that below\nm_rate = ols('y ~ RATE',model_data).fit()\nprint(m_rate.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_home = ols('y ~ Home',model_data).fit()\nprint(m_home.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_cltv = ols('y ~ CLTV',model_data).fit()\nprint(m_cltv.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_zip = ols('y ~ zip_code',model_data).fit()\nprint(m_zip.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_Fix_True = ols('y ~ Fix_True',model_data).fit()\nprint(m_Fix_True.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_loaninmonths = ols('y ~ LoanInMonth',model_data).fit()\nprint(m_loaninmonths.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_rcpi = ols('y ~ LoanInMonth + RATE + Home + CLTV ',model_data).fit()\nprint(m_rcpi.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nfdval = m_rcpi.fittedvalues\nplt.scatter(fdval, y)\nplt.ylabel('Predicted prices')\nplt.xlabel('Original Prices')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.regplot(x=fdval, y=\"Loan Amount\", data=model_data, fit_reg = True, color='g')\nplt.xlabel('Original Prices')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- RATE R^2 = 0.12 \n- Home R^2 = 0.12 \n- CLTV R^2 = 0.00 "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting Linear Regression using sklearn"},{"metadata":{},"cell_type":"markdown","source":"Look inside lm object using dir(lm):\n\n- lm.predit() \n- lm.fit()\n- lm.score()\n- lm.coef\n- lm.intercept\n\nFit a linear model: The lm.fit() function estimates the coefficients the linear regression using least square"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nX = np.array(fit_data.drop(['Fix_True'],1))\ny = np.array(fit_data['Fix_True'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This creates a LinearRegression object\nlm = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use all 13 predictors to fit linear regression model\nlm.fit(X, y)\nlm.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean squared error\nprint(\"Mean squared error (Fix_True Rate): %.2f\" % np.mean((lm.predict(X) - y) ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['loan_type_code'],1))\ny = np.array(fit_data['loan_type_code'])\nlm.fit(X, y)\nlm.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean squared erro\nprint(\"Mean squared error (loan_type_code): %.2f\" % np.mean((lm.predict(X) - y) ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['Loan Amount'],1))\ny = np.array(fit_data['Loan Amount'])\nlm.fit(X, y)\nlm.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean squared error\nprint(\"Mean squared error (Loan Amount): %.2f\" % np.mean((lm.predict(X) - y) ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['unit_type_code'],1))\ny = np.array(fit_data['unit_type_code'])\nlm.fit(X, y)\nlm.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The mean squared error\nprint(\"Mean squared error (unit_type_code): %.2f\" % np.mean((lm.predict(X) - y) ** 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try Regression: with Scale"},{"metadata":{},"cell_type":"markdown","source":"Here is the different types of regression we have used with StandardScaler and GridSearchCV:\n- LinearRegression\n- Lasso\n- Ridge\n- ElasticNet\n- SGDRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import DataFrame,Series\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pretty_print_linear(coefs, names = None, sort = False):\n    if names is None:\n        names = [\"X%s\" % x for x in range(len(coefs))]\n    lst = zip(coefs, names)\n    if sort:\n        lst = sorted(lst, key = lambda x:-np.abs(x[0]))\n    return \" + \".join(\"%s * %s\" % (round(coef, 3), name) for coef, name in lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(X):\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(X,Y):\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n    return X_train, X_test, Y_train, Y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def root_mean_square_error(y_pred,y_test):\n    rmse_train = np.sqrt(np.dot(abs(y_pred-y_test),abs(y_pred-y_test))/len(y_test))\n    return rmse_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_real_vs_predicted(y_pred,y_test):\n    plt.figure(figsize=(20,5))\n    plt.plot(y_pred,y_test,'ro')\n    plt.plot([0,50],[0,50], 'g-')\n    plt.xlabel('predicted')\n    plt.ylabel('real')\n    plt.title('real_vs_predicted')\n    plt.show()\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions(precision=2, linewidth=100, suppress=True, edgeitems=2)\nX = np.array(fit_data.drop(['Fix_True'],1))\ny = np.array(fit_data['Fix_True'])\nX = scale_data(X)  #Scalling Input Data\nX_train, X_test, y_train, y_test = split_data(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try Regular Linear Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create linear regression object\nlinreg = LinearRegression()\n# Train the model using the training sets\nlinreg.fit(X_train,y_train)\nprint(\"Linear model: \", pretty_print_linear(linreg.coef_, sort = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the values using the model\ny_lin_predict = linreg.predict(X_test)\n# Print the root mean square error\nprint(\"Linear Regression - Root Mean Square Error: \", root_mean_square_error(y_lin_predict,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_real_vs_predicted(y_test,y_lin_predict)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try Lasso Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create lasso regression object\nlasso = Lasso(alpha=.3)\n# Train the model using the training sets\nlasso.fit(X_train, y_train)\nprint(\"Lasso model: \", pretty_print_linear(lasso.coef_, sort = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the values using the model\ny_lasso_predict = lasso.predict(X_test)\n# Print the root mean square error\nprint(\"Lasso model - Root Mean Square Error: \", root_mean_square_error(y_lasso_predict,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_real_vs_predicted(y_test,y_lasso_predict)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try Ridge Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(fit_intercept=True, alpha=.3)\n# Train the model using the training sets\nridge.fit(X_train, y_train)\nprint(\"Ridge model: \", pretty_print_linear(ridge.coef_, sort = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the values using the model\ny_ridge_predict = ridge.predict(X_test)\n# Print the root mean square error\nprint(\"Ridge Regression - Root Mean Square Error: \", root_mean_square_error(y_ridge_predict,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_real_vs_predicted(y_test,y_ridge_predict)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now let's try to do regression via Elastic Net."},{"metadata":{"trusted":true},"cell_type":"code","source":"elnet = ElasticNet(fit_intercept=True, alpha=.3)\n# Train the model using the training sets\nelnet.fit(X_train, y_train)\nprint(\"Elastic Net model: \", pretty_print_linear(elnet.coef_, sort = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the values using the model\ny_elnet_predict = elnet.predict(X_test)\n# Print the root mean square error\nprint(\"Elastic Net - Root Mean Square Error: \", root_mean_square_error(y_elnet_predict,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_real_vs_predicted(y_test,y_elnet_predict)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now let's try to do regression via Stochastic Gradient Descent."},{"metadata":{"trusted":true},"cell_type":"code","source":"sgdreg = SGDRegressor(penalty='l2', alpha=0.15, max_iter=200)\n# Train the model using the training sets\nsgdreg.fit(X_train, y_train)\nprint(\"Stochastic Gradient Descent model: \", pretty_print_linear(sgdreg.coef_, sort = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the values using the model\ny_sgdreg_predict = sgdreg.predict(X_test)\n# Print the root mean square error\nprint(\"Stochastic Gradient Descent - Root Mean Square Error: \", root_mean_square_error(y_sgdreg_predict,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_real_vs_predicted(y_test,y_sgdreg_predict)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression Summary Report"},{"metadata":{},"cell_type":"markdown","source":"- Linear Regression - Root Mean Square Error : 0.298\n- Lasso model - Root Mean Square Error : 0.308\n- Ridge Regression - Root Mean Square Error : 0.298\n- Elastic Net - Root Mean Square Error : 0.308\n- Stochastic Gradient Descent - Root Mean Square Error : 0.298\n\nLinear Regression, Ridge Regression, Stochastic Gradient Descent all have - Root Mean Square Error: 0.298 are performing best with low Root Mean Square Error"},{"metadata":{},"cell_type":"markdown","source":"Now we have a pandas DataFrame called model_data containing all the data we want to use to predict Mortgage Loan prices. Let's create a variable called 'Loan Amount' which will contain the prices.\n\nThis information is contained in the target data."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data=model_data3.copy()\nmodel_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n%matplotlib inline\ng1=sns.pairplot(model_data, x_vars=['RATE', 'Home'], y_vars='Loan Amount', height=8, aspect=0.9, kind='reg')\ng1.axes[0,1].set_ylim(300000,600000)\ng1.axes[0,0].set_xlim(1.5,3.5)\ng1.axes[0,1].set_xlim(4.5,8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# US 10-Years Interest Rate goes up loan amount increases"},{"metadata":{},"cell_type":"markdown","source":"Home Supply increases, loan amount also increases slightly but eventually housing market will slow down\n\nInterest Rate Chage has bigger impact on Loan Amount compare to Home Supply"},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.pairplot(model_data, x_vars=['Qualification FICO', 'CLTV'], y_vars='Loan Amount', size=8, aspect=0.9, kind='reg')\ng.axes[0,1].set_ylim(400000,500000)\ng.axes[0,0].set_xlim(600, 820)\ng.axes[0,1].set_xlim(30,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Majority of the FICO scores between 600 and 820\n- Majority of the CLTV scores between 30% and 100%\n- FICO goes up, Loan Amount goes up\n- CLTV goes up, Loan Amount Goes down"},{"metadata":{"trusted":true},"cell_type":"code","source":"g2=sns.pairplot(model_data, x_vars=['unit_type_code',\n       'loan_type_code', 'RATE'], y_vars='Loan Amount', size=8, aspect=0.9, kind='reg')\ng2.axes[0,1].set_ylim(300000,600000)\ng2.axes[0,0].set_xlim(1.0,12.1)\ng2.axes[0,1].set_xlim(1,5)\ng2.axes[0,2].set_xlim(1.5,3.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['Unit Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['unit_type_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['Loan Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lc['loan_type_code'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As number of unit decreases ave Loan Amount also decreases\n\nloan_type_code: Conventional is high volume but Slightly low average loan amount\n\nARM (Adjustable Rate Mortgage) has higher loan amount thn Fix_True Rate mortgage Two Family (Code = 10) has higher Loan Amount that three Family (Code = 9)\n\nConventional Mortgage has Higher Loan Amount FHA ARM has higher Loan Amount compare to Fix Rate Mortgage"},{"metadata":{"trusted":true},"cell_type":"code","source":"g3=sns.pairplot(model_data, x_vars=[ 'CLTV', 'Home', 'LoanInMonth'], y_vars='Loan Amount', size=8, aspect=1.2, kind='reg')\ng3.axes[0,1].set_ylim(300000,600000)\ng3.axes[0,0].set_xlim(30,100)\ng2.axes[0,1].set_xlim(4.5,7.5)\ng3.axes[0,2].set_xlim(0,80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NYC NJ (zip_code between 7 & 11) seems to be closing more loans and generation more revenues for the Mortgage Bank. As number of loans per month inceases, loan amount decrease"},{"metadata":{"trusted":true},"cell_type":"code","source":"g4=sns.pairplot(model_data, x_vars=[ 'zip_code', 'unit_type_code'], y_vars='Loan Amount', size=8, aspect=1.2, kind='reg')\ng4.axes[0,1].set_ylim(300000,600000)\ng4.axes[0,0].set_xlim(6,500)\ng4.axes[0,1].set_xlim(1,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NYC NJ (zip_code 1st digit starting with 7 & 11) seems to be closing more loans and generation more revenues for the Mortgage Bank. City shows similar results. Population density plays bigger role on Loan Amount. As number of loans per month increases, loan amount decreases"},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing, neighbors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['loan_type_code'],1))\ny = np.array(fit_data['loan_type_code'])\nX_train, X_test, y_train, y_test_knn = train_test_split(X, y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the DataFrame: numeric_data_only\nnumeric_data_only = model_data[0:10].fillna(-1)\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the classifier: clf\nclf = OneVsRestClassifier(LogisticRegression())\n# Fit the classifier to the training data\nclf.fit(X_train, y_train)\n# Print the accuracy\nprint(\"Logistic Regression Accuracy: {}\".format(clf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression Model Accuracy for Loan Types: 67.46%"},{"metadata":{},"cell_type":"markdown","source":"**> Now Centering, Scaling and Logistic Regression and look at the model accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary packages\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn import datasets\nfrom sklearn import linear_model\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\nX = np.array(fit_data.drop(['loan_type_code'],1))\ny = np.array(fit_data['loan_type_code'])\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into test and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train logistic regression model and print performance on the test set\nlr = linear_model.LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nlr = lr.fit(X_train, y_train)\nprint('Logistic Regression score for training set: %f' % lr.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_true, y_pred = y_test, lr.predict(X_test)\nprint(classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale\nXs = scale(X)\nXs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2)\nlr_2 = lr.fit(Xs_train, y_train)\nprint('Scaled Logistic Regression score for test set: %f' % lr_2.score(Xs_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true, y_pred = y_test, lr_2.predict(Xs_test)\nprint(classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is very interesting! The performance of logistic regression did not improve with data scaling. Why not, particularly when we saw that k-Nearest Neigbours performance improved substantially with scaling? The reason is that, if there predictor variables with large ranges that do not effect the target variable, a regression algorithm will make the corresponding coefficients ai small so that they do not effect predictions so much. K-nearest neighbours does not have such an inbuilt strategy and so we very much needed to scale the data."},{"metadata":{},"cell_type":"markdown","source":"# Scaling Synthesized Data"},{"metadata":{},"cell_type":"markdown","source":"Scaling numerical data (that is, multiplying all instances of a variable by a constant in order to change that variable’s range) has two related purposes: i) if your measurements are in different currencies and, then, if we both scale our data, they end up being the same & ii) if two variables have vastly different ranges, the one with the larger range may dominate your predictive model, even though it may be less important to our target variable than the variable with the smaller range. What we saw is that this problem identified in ii) occurs with k-NN, which explicitly looks at how close data are to one another but not in logistic regression which, when being trained, will shrink the relevant coefficient to account for the lack of scaling. We can see was how the models performed before and after scaling.\n\nLet’s now split into testing & training sets & plot both sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nplt.figure(figsize=(20,5));\nplt.subplot(1, 2, 1 );\nplt.title('training set')\nplt.scatter(X_train[:,0] , X_train[:,1],  c = y_train, alpha = 0.7);\nplt.subplot(1, 2, 2);\nplt.scatter(X_test[:,0] , X_test[:,1],  c = y_test, alpha = 0.7);\nplt.title('test set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking good! Now let’s instantiate a k-Nearest Neighbors voting classifier & train it on our training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors, linear_model\nknn = neighbors.KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\nprint('k-NN score for test set: %f' % knn_model.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('k-NN score for training set: %f' % knn_model.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_true, y_pred = y_test, knn_model.predict(X_test)\nprint(classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice the improvement for KNN compare to Logistic Regression Now with scaling KNN: We’ll now scale the predictor variables and then use k-NN again:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale\nXs = scale(X)\nXs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2)\nplt.figure(figsize=(20,5));\nplt.subplot(1, 2, 1 );\nplt.scatter(Xs_train[:,0] , Xs_train[:,1],  c = y_train, alpha = 0.7);\nplt.title('scaled training set')\nplt.subplot(1, 2, 2);\nplt.scatter(Xs_test[:,0] , Xs_test[:,1],  c = y_test, alpha = 0.7);\nplt.title('scaled test set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model_s = knn.fit(Xs_train, y_train)\nprint('k-NN score for test set: %f' % knn_model_s.score(Xs_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k-NN score for test set: 0.71 \n\nIt doesn’t perform any better with scaling! This is most likely because both features were already around the same range. It really makes sense to scale when variables have widely varying ranges. To see this in action, we’re going to add another feature. Moreover, this feature will bear no relevance to the target variable: it will be mere noise."},{"metadata":{},"cell_type":"markdown","source":"# Adding Gaussian noise to the signal (KNN):"},{"metadata":{},"cell_type":"markdown","source":"We add a third variable of Gaussian noise with mean 0 and variable standard deviation σ. We’ll call σ the strength of the noise and we’ll see that the stronger the noise, the worse the performance of k-Nearest Neighbours"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['loan_type_code'],1))\ny = np.array(fit_data['loan_type_code'])\n# Generate some clustered data (blobs!)\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nn_samples=2000\nX, y = make_blobs(n_samples, centers=4, n_features=2,\n                  random_state=42)\n# Add noise column to predictor variables\nns = 10**(3) # Strength of noise term\nnewcol = np.transpose([ns*np.random.randn(n_samples)])\nXn = np.concatenate((X, newcol), axis = 1)\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(15,15));\nax = fig.add_subplot(111, projection='3d' , alpha = 0.5);\nax.scatter(Xn[:,0], Xn[:,1], Xn[:,2], c = y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xn_train, Xn_test, y_train, y_test = train_test_split(Xn, y, test_size=0.2, random_state=42)\nknn = neighbors.KNeighborsClassifier(n_neighbors=6)\nknn_model = knn.fit(Xn_train, y_train)\nprint('k-NN score for test set: %f' % knn_model.score(Xn_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k-NN score for test set: 0.58\n\nThis is a horrible model! How about we scale and check out performance?"},{"metadata":{"trusted":true},"cell_type":"code","source":"Xns = scale(Xn)\ns = int(.2*n_samples)\nXns_train = Xns[s:]\ny_train = y[s:]\nXns_test = Xns[:s]\ny_test = y[:s]\nknn_models = knn.fit(Xns_train, y_train)\nknn_accuracy = knn_models.score(Xns_test, y_test)\nprint('knn_accuracy for test set: ' , knn_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_prediction = knn.predict(Xns[1515:1535])\nprint('KNN : - Output of Real Data : Conv=5, FHA=4, Res=3, Comm=2: ', (y[1515:1535]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('KNN : - Output of prediction: Conv=5, FHA=4, Res=3, Comm=2: ',knn_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With Scale and Synthesize the data we can see huge improvement on the model accuracy.. 36% to 100% Let’s do same for Logistic Regression and check out the performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set sc = True if you want to scale your features\nsc = False\n#Import packages\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neighbors, linear_model\nfrom sklearn.preprocessing import scale\nfrom sklearn.datasets import make_blobs\n    #Generate some data\nn_samples=2000\nX, y = make_blobs(n_samples, centers=4, n_features=2, random_state=0)\n# Add noise column to predictor variables\nnewcol = np.transpose([ns*np.random.randn(n_samples)])\nXn = np.concatenate((X, newcol), axis = 1)\n#Scale if desired\nif sc == True:\n    Xn = scale(Xn)    \n#Train model and test after splitting\nXn_train, Xn_test, y_train, y_test = train_test_split(Xn, y, test_size=0.2, random_state=42)\nlr = linear_model.LogisticRegression()\nlr_model = lr.fit(Xn_train, y_train)\nprint('logistic regression score for test set: %f' % lr_model.score(Xn_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"logistic regression score for test set has improved from 70% to 0.87 or 87%\n\nWe can see big improvement. We have seen the essential place in the data scientific pipeline by preprocessing, in its scaling and centering incarnation, and we have done so to promote a holistic approach to minimize the challenges of machine learning.\n\nTo conclude, we have seen the essential place occupied in the data scientific pipeline by preprocessing, in its scaling and centering incarnation, and we have done so to promote a holistic approach to the challenges of machine learning."},{"metadata":{},"cell_type":"markdown","source":"# Random Forests (RF)"},{"metadata":{},"cell_type":"markdown","source":"Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance. Random forests has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases.\n\n- Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n\n- It does not suffer from the over fitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.\n\n- The algorithm can be used in both classification and regression problems.\n\n- Random forests can also handle missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing, neighbors, svm \nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nX = np.array(fit_data.drop(['loan_type_code'],1))\ny = np.array(fit_data['loan_type_code'])\nX_train, X_test, y_train, y_test_rfc = train_test_split(X, y, test_size = 0.2)\nrfc = RandomForestClassifier(n_jobs=-1,max_features= 'auto' ,n_estimators=200, oob_score = True) \nrfc.fit(X_train, y_train)\nrfc_accuracy = rfc.score(X_test, y_test_rfc)\nprint('Unscaled: Random Forest Classifier Accuracy : ', rfc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(fit_data.drop(['loan_type_code'],1))\ny = np.array(fit_data['loan_type_code'])\nfrom sklearn.preprocessing import scale\nX_scaled = scale(X)\nX_train, X_test, y_train, y_test_rfc = train_test_split(X, y, test_size = 0.2)\nrfc = RandomForestClassifier(n_jobs=-1,max_features= 'auto' ,n_estimators=200, oob_score = True) \nrfc.fit(X_train, y_train)\nrfc_accuracy = rfc.score(X_test, y_test_rfc)\nprint('Scaled: Random Forest Classifier Accuracy   : ', rfc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look for the confussion Matrix\nfrom sklearn.metrics import confusion_matrix\n#confusion_matrix?\nrfc.predict(X_test)\ny_pred_rfc = rfc.predict(X_test)\ny_pred_rfc_out = rfc.predict(X[975:995,])\nprint('RFC Confussion Matrix: ')\nconfusion_matrix(y_test_rfc, y_pred_rfc, labels=None, sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('KNeighborsClassifier Accuracy : ', rfc_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier :Input Real Data : Conv=5, FHA=4, Res=3, Comm=2:\\n', X[975:995,] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier :Output of X[975:995]:Conv=5, FHA=4, Res=3, Comm=0:' , y[975:995])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier :Output of prediction: Conv=5, FHA=4, Res=3, Comm=:', y_pred_rfc_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier : - Output of prediction: Fix_True =1 & ARM =0:\\n', y_pred_rfc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unscaled: Random Forest Classifier Accuracy : 76%\n\nScaled: Random Forest Classifier Accuracy : 79%\n\nBoth unscalled and scalled for Random Forest Model accuray is almost close"},{"metadata":{},"cell_type":"markdown","source":"# Let’s analyze Interest Rate types using Random Forest Classifier."},{"metadata":{},"cell_type":"markdown","source":"We have achieved greater accuracy compare to Loan Types. We have Fixed Rate mortgage where Fix_True = 1 & Fix_True = 0 for ARM (Adjustable Rate Mortgage)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn import preprocessing, neighbors, svm\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nX = np.array(model_data.drop(['Fix_True'],1))\ny = np.array(model_data['Fix_True'])\nfrom sklearn.preprocessing import scale\n# Scale the features: X_scaled\nX_scaled = scale(X)\nX_train, X_test, y_train, y_test_rfc = train_test_split(X, y, test_size = 0.2)\nrfc = RandomForestClassifier(n_jobs=-1,max_features= 'auto' ,n_estimators=200, oob_score = True)\nrfc.fit(X_train, y_train)\nrfc_accuracy = rfc.score(X_test, y_test_rfc)\nprint('Random Forest Classifier Accuracy : ', rfc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look for the confussion Matrix\nfrom sklearn.metrics import confusion_matrix\n#confusion_matrix? # Hit enter\nrfc.predict(X_test)\ny_pred_rfc = rfc.predict(X_test)\ny_pred_rfc_out = rfc.predict(X[975:995,])\nprint('RFC Confussion Matrix: ')\nprint(confusion_matrix(y_test_rfc, y_pred_rfc, labels=None, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier Accuracy : ', rfc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier : - Input of Real Data : Fix_True =1 & ARM =0:\\n ', X[975:995,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier : - Output of X[975:995]: Fix_True =1 & ARM =0:' , y[975:995])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RandomForestClassifier : - Output of prediction: Fix_True =1 & ARM =0:', y_pred_rfc_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n############ Prediction  ######################')\nprint('RandomForestClassifier : - Output of prediction: Fix_True =1 & ARM =0:\\n', y_pred_rfc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier Accuracy for Fix or ARM is : 0.87 or 87%"},{"metadata":{},"cell_type":"markdown","source":"# Using Random Forest finding Important Features in Scikit-learn"},{"metadata":{},"cell_type":"markdown","source":"Here, we are finding important features or selecting features in the Mortgage Loan dataset. In scikit-learn, we can perform this task in the following steps:\n\n- First, we need to create a random forests model.\n\n- Second, use the feature importance variable to see feature importance scores.\n\n- Third, visualize these scores using the seaborn library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing, neighbors, svm\nX = np.array(model_data.drop(['Fix_True'],1))\ny = np.array(model_data['Fix_True'])\nfrom sklearn.preprocessing import scale\n# Scale the features: X_scaled\nX_scaled = scale(X)\nX_train, X_test, y_train, y_test_rfc = train_test_split(X, y, test_size = 0.2)\nrfc = RandomForestClassifier(n_jobs=-1,max_features= 'auto' ,n_estimators=200, oob_score = True)\nrfc.fit(X_train, y_train)\nrfc_accuracy = rfc.score(X_test, y_test_rfc)\nprint('Random Forest Classifier Accuracy : ', rfc_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nimport pandas as pd\n#col_names = (fit_data.columns)\nX = (model_data.drop(['Qualification FICO'],1))\nfeature_cols = fit_data.columns\nmodel_data.columns\nfeature_names = feature_cols\nfeature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also visualize the feature importance. Visualizations are easy to understand and interpretable. For visualization, we have used a combination of matplotlib and seaborn"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.figure(figsize=(20,5))\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Higher the value, greater the feature importance"},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Regression (SVR)"},{"metadata":{},"cell_type":"markdown","source":"**SUPPORT VECTOR REGRESSION.** Those who are in Machine Learning or Data Science are quite familiar with the term SVM or Support Vector Machine. But SVR is a bit different from SVM. As the name suggest the SVR is an regression algorithm, so we can use SVR for working with continuous Values instead of Classification which is SVM."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(model_data.drop(['Loan Amount'],1))\ny = np.array(model_data['Loan Amount'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# We want to use svm now\nfrom sklearn import preprocessing,svm\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nclf = svm.SVR()\nclf.fit(X_train, y_train)\naccuracy = clf.score(X_test, y_test)\nprint('SVM accuracy: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM-SVR accuracy: -0.026 which is unacceptable for our Mortgage Loan Data Sets"},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine (SVM)"},{"metadata":{},"cell_type":"markdown","source":"A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimensional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing, neighbors, svm\nX = np.array(model_data.drop(['Fix_True'],1))\ny = np.array(model_data['Fix_True'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#Define SVM support vector classifier\nsvmc = svm.SVC(kernel='rbf', C=10, gamma=1)\nsvmc.fit(X_train, y_train)\nsvm_accuracy = svmc.score(X_test, y_test)\nprint('Support Vector Classifier Accuracy : ', svm_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look for the confussion Matrix\nfrom sklearn.metrics import confusion_matrix\n#confusion_matrix? # Hit enter\nsvmc.predict(X_test)\ny_pred_svm = svmc.predict(X_test)\nprint('SVC Confussion Matrix: ')\nprint(confusion_matrix(y_test, y_pred, labels=None, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nsvm_scores = cross_val_score(svmc, X, y, cv=7, scoring='accuracy')\nprint('SVM: cross_val_score accuracy : ', svm_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Classifier Accuracy : 0.896 or 90%"},{"metadata":{},"cell_type":"markdown","source":"# k-Nearest Neighbors (KNN)"},{"metadata":{},"cell_type":"markdown","source":"k-Nearest Neighbors: FIT\n\nHaving explored the Congressional mortgage records dataset, we have build our classifier. Here, we will fit a k-Nearest Neighbors classifier to the mortgage dataset. The features need to be in an array where each column is a feature and each row a different observation or data point. The target needs to be a single column with the same number of observations as the feature data. Notice we named the feature array X and response variable y: This is in accordance with the common scikit-learn practice. We need create an instance of a k-NN classifier with 6 neighbors (by specifying the n_neighbors parameter) and then fit it to the data."},{"metadata":{},"cell_type":"markdown","source":"# k-Nearest Neighbors: Predict"},{"metadata":{},"cell_type":"markdown","source":"Having fit a k-NN classifier, we can use it to predict the label of a new data point. However, there is no unlabeled data available since all of it was used to fit the model! We will use your classifier to predict the label for this new data point, as well as on the training data X that the model has already seen."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import preprocessing,neighbors\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split # for train and test set split\nfrom sklearn.model_selection import cross_val_score\nX = np.array(model_data.drop(['Fix_True'],1))\ny = np.array(model_data['Fix_True'])\nX_train, X_test, y_train, y_test_knn = train_test_split(X, y, test_size = 0.2)\nfrom sklearn.neighbors import KNeighborsClassifier\nprint(\"size of the training feature set is\",X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"size of the test feature set is\",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"size of the training Target set is\",y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"size of the test Target set is\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import scale\nfrom sklearn.preprocessing import scale\n# Scale the features: X_scaled\nX_scaled = scale(X)\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nknn_accuracy = knn.score(X_test, y_test_knn)\nprint('KNeighborsClassifier Accuracy : ', knn_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_prediction = knn.predict(X[1200:1220,])\nprint('KNN : - Input of Real Data :: \\n', X[1200:1220,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('KNN : - Output of Real Data :: ', y[1200:1220,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('KNN : - Output of prediction:: ', knn_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNeighborsClassifier Accuracy : 0.882 or 88%"},{"metadata":{},"cell_type":"markdown","source":"# How do we improve the KNN model"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing: scaling"},{"metadata":{},"cell_type":"markdown","source":"Here below I will take following steps to improve the model.\n\n(i) scale the data,\n\n(ii) use k-Nearest Neighbors\n\n(iii) check the model performance.\n\nWe'll use scikit-learn's scale function, which standardizes all features (columns) in the array passed to it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale\nXs = scale(X)\nfrom sklearn.model_selection import train_test_split\nXs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2)\nknn_model_2 = knn.fit(Xs_train, y_train)\nprint('k-NN score for test set: %f' % knn_model_2.score(Xs_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('k-NN score for training set: %f' % knn_model_2.score(Xs_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true, y_pred = y_test, knn_model_2.predict(Xs_test)\nprint(classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true, y_pred = y_test, knn_model_2.predict(Xs_test)\nprint(classification_report(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All these measures improved by 3.25% improvement and significant! As hinted at above, before scaling there were a number of predictor variables with ranges of different order of magnitudes, meaning that one or two of them could dominate in the context of an algorithm such as k-NN. The two main reasons for scaling our data are\n\nOur predictor variables may have significantly different ranges and, in certain situations, such as when implementing k-NN, this needs to be mitigated so that certain features do not dominate the algorithm; We want our features to be unit-independent, that is, not reliant on the scale of the measurement involved. If we both scale our respective data, this feature will be the same for each of us."},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{},"cell_type":"markdown","source":"Using Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning. In the following the example, we can plot a decision tree on the same data with max_depth=4. Other than pre-pruning parameters, We have also tried other attribute selection measure such as entropy This pruned model is less complex, explainable, and easy to understand than the previous decision tree model plot.\n\n**Pros**\n\n- Decision trees are easy to interpret and visualize.\n- It can easily capture Non-linear patterns.\n- It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n- It can be used for feature engineering such as predicting missing values, suitable for variable selection.\n- The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm.\n\n**Cons**\n\n- Sensitive to noisy data. It can overfit noisy data.\n- The small variation(or variance) in data can result in the different decision tree.\n- Decision trees are biased with imbalance dataset, so we can balance out the dataset before creating the decision tree."},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Algorithm"},{"metadata":{},"cell_type":"markdown","source":"A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n# Split dataset into training set and test set\nX = np.array(fit_data.drop(['Fix_True'],1))\ny = np.array(fit_data['Fix_True'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 70% training and 30% test\n#Building Decision Tree Model Let's create a Decision Tree Model using Scikit-learn.\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import scale\nfrom sklearn.preprocessing import scale\n# Scale the features: X_scaled\nX_scaled = scale(X)\nclf.predict(X_test)\ny_pred_dt = clf.predict(X_test)\ny_pred_dt_out = clf.predict(X[975:995,])\nprint('DecisionTre Confussion Matrix: ')\nprint(confusion_matrix(y_test, y_pred_dt, labels=None, sample_weight=None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_accuracy = clf.score(X_test, y_test)\nprint('Decision Tree Classifier Accuracy : ', dt_accuracy) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree Classifier : - Input of Real Data : Fix_True =1 & ARM =0:\\n ', X[975:995,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree Classifier : - Output of X[975:995]: Fix_True =1 & ARM =0:' , y[975:995])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree Classifier : - Output of prediction: Fix_True =1 & ARM =0:', y_pred_dt_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Classifier Accuracy : 0.840 or 84%"},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Decision Trees"},{"metadata":{},"cell_type":"markdown","source":"We have used Scikit-learn's export_graphviz function for display the tree within a Jupyter notebook. For plotting tree, you also need to install graphviz and pydotplus. export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter Notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom six import StringIO\nfrom IPython.display import Image\nimport pydotplus\ncol_names = (fit_data.columns)\nX = (fit_data.drop(['Loan Amount'],1))\ny = (fit_data['Loan Amount'])\nfeature_cols = X.columns\nfit_data.columns\ndot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,\n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('./mortgage_dt.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning.\n\nWe can plot a decision tree on the same data with max_depth=3. Other than pre-pruning parameters, We can also try other attribute selection measure such as entropy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing Decision Trees\ndot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,\n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1','2'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('./mortgage_dt.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotted a decision tree on the same data with max_depth=3. Easy to visualize.\n\nDecision tree model accuracy has also improved."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}