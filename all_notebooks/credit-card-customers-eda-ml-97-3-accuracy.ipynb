{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What will you find here?\n\n* Exploring & Visualising the data\n* Transform the data for building better models\n* Comparing the results and easily to choose the best model\n* Trying Dimension Reduction, Scaling\n* Mix of PCA and original features\n* Explanation of the correlations and other plots"},{"metadata":{},"cell_type":"markdown","source":"This notebook is an extended version of the original posted in kaggle. You can find it [here](https://www.kaggle.com/alpertml/credit-card-customers-eda-ml-97-5-accuracy)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing neccesary packages\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nplt.style.use('ggplot') # default plot style.\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\n\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer \n\nimport pickle\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjusting the plotting style\nsns.set(color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we have?, let's see"},{"metadata":{},"cell_type":"markdown","source":"So.. we have only one dataset which in total 10127 observations 21 features. Our target column is **Attrition_Flag(binary)** and we will try to predict it. Dataset is not include any missing values(NaN/Null/NotANumber), it's a good new. We see that the dataset consists mostly of numerical data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"full_df = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\n\ndisplay(full_df.shape)\n# display 5 sample randomly\nfull_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We don't need the unique ids'\nfull_df.drop(columns=['CLIENTNUM',\n                      'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                      'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1, inplace=True)\n\ndisplay(full_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns were dropped succesfully!"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df[full_df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's no duplicated data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking dtypes\ndisplay(full_df.info())\n# Checking numeric values stats\ndisplay(full_df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing data or NaN values, and the types of the data consist on objects and mostly numeric values.\n\nAttrition Flag is the target variable, we should encode it to a binary form to correct train a classification model."},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Data"},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features"},{"metadata":{},"cell_type":"markdown","source":"* **Attrition_Flag** (1: Existing Customer, 0: Attrited Customer): The Customer leave or not\n* **Gender** (1: Male, 0: Female)\n* **Education_Level** (Graduate , High School, Unknown, Uneducated, College, Post-Graduate, Doctorate)\n* **Marital_Status** (Married, Single, Unknown, Divorced)\n* **Income_Category** (Less than 40K, 40K - 60K, 80K - 120K, 60K - 80K, Unknown, 120K +) in dollar\n* **Card_Category** (Blue, Silver, Gold, Platinum)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cats = ['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\n\ndef pltCountplot(cats):\n    \n    fig, axis = plt.subplots(len(cats) // 3,3, figsize=(20,12))  \n\n    index = 0\n    for i in range(len(cats) // 3):\n        for j in range(3):\n            \n            ax = sns.countplot(cats[index], data=full_df, ax=axis[i][j])\n            \n            if cats[index] in ['Education_Level', 'Income_Category']:\n                for item in ax.get_xticklabels():\n                    item.set_rotation(15)\n                \n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width()/2.,\n                        height + 3,\n                        '{:1.2f}%'.format(height/len(full_df)*100),\n                        ha=\"center\") \n            index += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pltCountplot(cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n\n* We can see that the dataset is not equally distribute according to Attrition_Flag. We have samples which are mostly Existing.\n* We can say that if education level is improved, using the credit card is decresing.\n* Generally people use blue card, it's must be correlated with income."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def pltCountplotHueTarget(cats, target):\n    \n    fig, axis = plt.subplots(len(cats) // 3,3, figsize=(20,12))  \n\n    index = 0\n    for i in range(len(cats) // 3):\n        for j in range(3):\n            \n            ax = sns.countplot(cats[index], data=full_df, hue=target, ax=axis[i][j])\n            \n            ax.legend(title='Customer exit?',\n                      loc='upper right',\n                      labels=['Yes', 'No'])\n            \n            if cats[index] in ['Education_Level', 'Income_Category']:\n                for item in ax.get_xticklabels():\n                    item.set_rotation(15)\n                \n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width()/2.,\n                        height + 3,\n                        '{:1.2f}%'.format(height/len(full_df)*100),\n                        ha=\"center\") \n            index += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pltCountplotHueTarget(cats, 'Attrition_Flag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The higher the income, the less likely is the person to use a credit card, and if they use it, it is more likely to keep using it.\n\nPeople with the lowest income apparently uses more the blue card. The causes of this massive churn could be a change in the finantial situation of the country, or the creation of a more suitable way of payment."},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{},"cell_type":"markdown","source":"* **Customer_Age**: Customer's Age in Years\n* **Dependent_count:** Number of dependents\n* **Months_on_book:** Period of relationship with bank\n* **Total_Relationship_Count:** Total no. of products held by the customer\n* **Months_Inactive_12_mon:** No. of months inactive in the last 12 months\n* **Contacts_Count_12_mon:** No. of Contacts in the last 12 months\n* **Credit_Limit:** Credit Limit on the Credit Card\n* **Total_Revolving_Bal:** Total Revolving Balance on the Credit Card\n* **Avg_Open_To_Buy:** Open to Buy Credit Line (Average of last 12 months)\n* **Total_Amt_Chng_Q4_Q1:** Change in Transaction Amount (Q4 over Q1)\n* **Total_Trans_Amt:** Total Transaction Amount (Last 12 months)\n* **Total_Trans_Ct:** Total Transaction Count (Last 12 months)\n* **Total_Ct_Chng_Q4_Q1:** Change in Transaction Count (Q4 over Q1)\n* **Avg_Utilization_Ratio:** Average Card Utilization Ratio"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"numeric_columns = ['Customer_Age','Credit_Limit','Months_on_book','Avg_Utilization_Ratio','Avg_Open_To_Buy','Total_Trans_Amt','Dependent_count',\n                  'Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon','Total_Revolving_Bal',\n                  'Total_Amt_Chng_Q4_Q1','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1']\n\nsome_columns = ['Customer_Age','Credit_Limit','Months_on_book','Avg_Utilization_Ratio','Avg_Open_To_Buy','Total_Trans_Amt']\n\n\ndef plotDistPlot(columns):\n    fig, ax = plt.subplots(len(columns)//3, 3,figsize=(20, 12))\n    \n    index = 0\n    for i in range(2):\n        for j in range(3):\n            sns.distplot(full_df.loc[:, columns[index]],\n                         hist=True,\n                         fit=norm,\n                         kde=True,\n                         ax=ax[i][j])\n            ax[i][j].set_title(columns[index])\n            ax[i][j].legend(labels=['Normal', 'Actual'])\n            index += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotDistPlot(some_columns)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"corr_data = full_df.loc[:, numeric_columns].corr()\n\nplt.figure(figsize=(20,12))\nsns.heatmap(corr_data.abs(), annot=True, fmt='.3f',cmap='coolwarm',square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NOTICE:  \nGenerally features have not strong correlation with each other. This is not mean they are not correleted. In corr matrix, we can see linearly corelated features. Maybe our features are correlated quadratic or n-degree polynomial. We can't see if features are correlated n-degree polynomial in the corr matrix."},{"metadata":{},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{},"cell_type":"markdown","source":"In the beginning of the notebook, i indicated that the dataset has not include missing values (If you check dataset page in kaggle, you see it). But we should check the dataset again, We should ensure. Heatmap is all dark. It's mean there is no missing data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# detecting the missing data\n\nfig, ax = plt.subplots(figsize=(20, 6))\n\nax.set_title('Train Data Missing Values')\nplt.xticks(rotation=90)\n\nsns.heatmap(full_df.iloc[:,:-2].isnull(),\n            yticklabels=False,\n            cbar=False,\n            cmap='magma',\n            ax=ax)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time to Feature Engineering!!"},{"metadata":{},"cell_type":"markdown","source":"We will play with the data."},{"metadata":{},"cell_type":"markdown","source":"## Object, Category to Numeric, Encode"},{"metadata":{},"cell_type":"markdown","source":"ML algorithms works on numeric values. That's why we should transform Object, Category, etc. values to numeric values."},{"metadata":{},"cell_type":"markdown","source":"### Binary Flags"},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_df = pd.DataFrame()\n\ndef tobinary():\n    \n    # full_df['Attrition_Flag'] = full_df.Attrition_Flag // same thing\n    updated_df['Attrition'] = full_df.Attrition_Flag.map({'Existing Customer':1, 'Attrited Customer':0})\n    \n    updated_df['Gender'] = full_df.Gender.map({'M':1, 'F':0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### String to integer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stringtoint():\n        #organized in such way that follows the probability trend of the plots\n\n    income_data = full_df['Income_Category'].replace({ 'Less than $40K':0, '$40K - $60K':1, '$60K - $80K':2,\n                                                      '$80K - $120K':3,'Unknown': 4 , '$120K +':5})\n    education_data = full_df['Education_Level'].replace({'Uneducated': 0, 'High School':1, 'Graduate':2, 'Unknown':3,\n                                                         'College':4,'Post-Graduate':5,'Doctorate':6})\n    \n    updated_df['Income_Category'] = income_data\n    updated_df['Education_Level'] = education_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode():\n    global updated_df\n    card_dummies = pd.get_dummies(full_df['Card_Category'], prefix='Card')\n    marital_dummies = pd.get_dummies(full_df['Marital_Status'], prefix='Marital')\n    updated_df = pd.concat([updated_df, marital_dummies, card_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concat_with_numerics():\n    global updated_df\n    updated_df = pd.concat([updated_df, full_df.loc[:, numeric_columns]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's excecute all the previus functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"tobinary()\nstringtoint()\nencode()\nconcat_with_numerics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data shapes \"\"\"including target value\"\"\"')\nprint(f'Old shape : {full_df.shape}')\nprint(f'Updated shape : {updated_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Look updated data"},{"metadata":{},"cell_type":"markdown","source":"We're going to make sure the data is ready for modeling. Let's see the updated data with big picture."},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the Dataframe for future works"},{"metadata":{"trusted":true},"cell_type":"code","source":"updated_df.to_csv('./BankChurners_all_numeric.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing packages for modelling.\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def estimates(X_data, y_data, models, cv):\n    \n    train_acc_dict = dict()\n    test_acc_dict = dict()\n    time_dict = dict()\n    \n    for model in models:\n        \n        current_model_name = model.__class__.__name__\n        \n        cv_results = cross_validate(model, X_data, y_data, cv=cv,\n                                    return_train_score=True, scoring='accuracy')\n        \n        train_acc_dict[current_model_name] = cv_results['train_score'].mean()\n        test_acc_dict[current_model_name] = cv_results['test_score'].mean()\n        time_dict[current_model_name] = cv_results['fit_time'].mean()\n        \n    return train_acc_dict, test_acc_dict, time_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#m_logreg = LogisticRegression()\n\nm_gbc = GradientBoostingClassifier(random_state=14)\n\n#m_rfc = RandomForestClassifier(criterion='gini', n_estimators=999,\n                            #max_depth=4, random_state=14)\n\nm_lgb = lgb.LGBMClassifier(num_iterations=550, learning_rate=0.01055,\n                        max_depth=3, random_state=14)\n\nm_xgb = xgb.XGBClassifier(n_estimators=2250,\n                       max_depth=2, random_state=14)\n\n#m_gnb = GaussianNB()\n\n#m_mlpc = MLPClassifier(random_state=14)\n\n#m_svc = SVC(probability=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(11, shuffle=True, random_state=14)\n#will use only the 3 best models of the example\nmodels = [m_gbc, m_lgb, m_xgb]\n\nX = updated_df.drop('Attrition', axis=1)\ny = updated_df['Attrition']\n\nprint(X.shape)\nprint(y.shape)\n\ntrain_acc_dict, test_acc_dict, time_dict = estimates(X, y, models, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model results"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training accuracy\nfor key, value in train_acc_dict.items():\n    print('{} - {:.1f}%'.format(key, value*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test accuracy\nfor key, value in test_acc_dict.items():\n    print('{} - {:.1f}%'.format(key, value*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting time\nfor key, value in time_dict.items():\n    print('{} - {:.1f} seconds'.format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **NOTICE:** Fitting time can be changed according to your process unit. TPU & GPU faster than CPU. So, Fitting time can be different."},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_importance_features(models, X, y):\n    \n    fig, axes = plt.subplots(3, len(models) // 2, figsize=(23, 12))\n\n    for ax, model in zip(axes.flatten(), models):\n        try:\n            model.fit(X, y)\n            importance_features = pd.DataFrame(sorted(\n                zip(model.feature_importances_, X.columns)),\n                                       columns=['Value', 'Feature'])\n\n            importance_features = importance_features.sort_values('Value', ascending=False)\n            sns.barplot(y=\"Feature\", x=\"Value\", ax=ax,\n                        data=importance_features)\n            current_model_name = model.__class__.__name__\n            ax.set(title=f'{current_model_name} Feature Importances')\n            ax.xaxis.set_major_locator(MaxNLocator(nbins=11))\n        except:\n            pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some estimators don't have feature_importance that's why i choosed the estimators which are include feature_importance\nplot_importance_features(models[0:3], X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that each model uses a different set of variables for the prediction.\n\n**Total revolving balance of the credit card** and **total transaction count** seems to be 2 powerfull predictors. Let's check. "},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_data = updated_df.corr()\n\nplt.figure(figsize=(30,22))\nsns.heatmap(corr_data, annot=True, fmt='.3f',cmap='coolwarm',square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The updated dataframe with the categorical variables in a numerical format, and the churn included, shows a potential correlation with some variables with the Attrition, despite the fact that correlations plot only look for linear correlations.\n\nIn this plot we have removed the absolute value calculation in order to understand if the correlation was positive, or negative. Now we can see that the more the people used this credit card, it is more likely to **quit** using it.\n\nIn the same manner we can see that the more contacts the people made in the last 12 months, the less likely is to quit."},{"metadata":{},"cell_type":"markdown","source":"### Dimensional Reduction & Fit models again"},{"metadata":{},"cell_type":"markdown","source":"Maybe some features are decreasing our models' accuracy. We try to reduce dimension then check accuracy again. Also, we try to improve models' accuracy using StandartScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creates pipeline\nmy_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=2)),\n])\n\nX_red = my_pipe.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit again**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc_dict_red, test_acc_dict_red, time_dict_red = estimates(X_red, y, models, cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Print & Plot the model's accuracy again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training accuracy\nfor key, value in train_acc_dict_red.items():\n    print('{} - {:.1f}%'.format(key, value*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test accuracy\nfor key, value in test_acc_dict_red.items():\n    print('{} - {:.1f}%'.format(key, value*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting time\nfor key, value in time_dict.items():\n    print('{} - {:.1f} seconds'.format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding all up\nMix of PCA with previous features."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full = pd.concat([pd.DataFrame(X_red),X], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc_dict_full, test_acc_dict_full, time_dict_full = estimates(X_full, y, models, cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training accuracy\nfor key, value in train_acc_dict_full.items():\n    print('{} - {:.1f}%'.format(key, value*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test accuracy\nfor key, value in test_acc_dict_full.items():\n    print('{} - {:.1f}%'.format(key, value*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting time\nfor key, value in time_dict.items():\n    print('{} - {:.1f} seconds'.format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that combining the features generated with PCA with the others its what gives the better results, this can be due to higher degrees of freedom for the model.\n\nThis implementation was based on a previus Kaggle Implementation of the member **Alper Temel**, you can find [here](https://www.kaggle.com/alpertml/credit-card-customers-eda-ml-97-5-accuracy) the original implementarion."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}