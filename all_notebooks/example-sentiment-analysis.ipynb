{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport string\nimport re\nfrom collections import Counter\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df = pd.read_csv('../input/sentiment-analysis-for-financial-news/all-data.csv', sep=',', encoding='latin-1',names = [\"category\",\"comment\"])\nprint(tweet_df.shape)\nprint(\"COLUMN NAMES\" , tweet_df.columns)\nprint(tweet_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"category\",data=tweet_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stopwords removal\nnltk.download('stopwords')\nstopword = nltk.corpus.stopwords.words('english')\nprint(stopword)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove punctuations\ndef remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ndef remove_stopwords(text, STOPWORDS):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndef stem_words(text, stemmer):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndef remove_freqwords(text, FREQWORDS):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndef lemmatize_words(text, lemmatizer, wordnet_map ):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndef clean_review(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return clean_text\n\ndef join_text(text):\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df['comment'] = tweet_df['comment'].apply(lambda x: remove_punct(x))\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\ntweet_df[\"text_wo_stop\"] = tweet_df[\"comment\"].apply(lambda text: remove_stopwords(text, STOPWORDS))\ntweet_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer()\ntweet_df[\"text_stemmed\"] = tweet_df[\"text_wo_stop\"].apply(lambda text: stem_words(text, stemmer))\ntweet_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove frequent words - countvectorization\ncnt = Counter()\nfor text in tweet_df[\"text_stemmed\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ntweet_df[\"text__stopfreq\"] = tweet_df[\"text_stemmed\"].apply(lambda text: remove_freqwords(text, FREQWORDS))\ntweet_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ntweet_df[\"text_lemmatized\"] = tweet_df[\"text__stopfreq\"].apply(lambda text: lemmatize_words(text, lemmatizer, wordnet_map))\ntweet_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df=tweet_df.drop([\"text_stemmed\",\"text__stopfreq\"],axis=1)\ntweet_df['encoded_category'] = LabelEncoder().fit_transform(tweet_df['category'])\ntweet_df[[\"category\", \"encoded_category\"]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df=tweet_df.drop([\"category\",\"text_wo_stop\",\"comment\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(tweet_df.text_lemmatized,tweet_df.encoded_category,test_size = 0.3 , random_state = 0)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('tfidf', TfidfVectorizer()),\n                 ('model', LinearSVC())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MODEL - LINEAR SVC\")\nprint(\"accuracy score: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MODEL - LOGISTIC REGRESSION\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', MultinomialNB())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MULTINOMIAL NAIVE BAYES\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', BernoulliNB())])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"BERNOULLIS NAIVE BAYES\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=55))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"GRADIENT BOOST\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', XGBClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=2020))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"XGBOOST\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = 10, \n                                           splitter='best', \n                                           random_state=2020))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"DECISION TREE\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"K NEAREST NEIGHBOR\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}