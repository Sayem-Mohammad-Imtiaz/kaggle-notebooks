{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INFO T780: Applied Machine Learning\n\n## Week 1: Introduction to ML\n### Prof. Y. An, PhD\n### College of Computing and Informatics, Drexel University","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For a Machine Learning Project, we usually need to go through these main steps:\n\n- Understanding the Objective\n- Data Acquisition\n- Data Pre-processing\n- Gain insights from Exploratory Data Analysis\n- Data preparation for Machine Learning\n- Select proper model(s)\n    * supervised or unsupervised?\n    * regression or classification?\n    * if with label, univariate regression/classification or multivariate regression/classification?\n    * what performance measure(s) to use?\n- Train model with hyperparameter tuning\n- Prediction & performance evaluation\n- Present Solution\n- Lauch, Monitor and maintain the system","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Setup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common imports\nimport numpy as np\nimport pandas as pd\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# to make this notebook's output identical at every run\nnp.random.seed(42)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the data\n\nWe will use California Housing data as example. It contains data drawn from the 1990 U.S. Census: related literature: Pace, R. Kelley, and Ronald Barry, \"Sparse Spatial Autoregressions,\" Statistics and Probability Letters, Volume 33, Number 3, May 5 1997, p. 291-297.*\n>We collected information on the variables using all the block groups in California from the 1990 Census. In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. Naturally, the geographical area included varies inversely with the population density. We computed distances among the centroids of each block group as measured in latitude and longitude. We excluded all the block groups reporting zero entries for the independent and dependent variables. The final data contained 20,640 observations on 9 characteristics.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = pd.read_csv('../input/hands-on-machine-learning-housing-dataset/housing.csv')\nhousing.head()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column `total_bedrooms` seem to have about 200 missing values; `ocean_proximity` is not numerical data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look how many districts belong to each category\nhousing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a histogram for each numerical attribute to get a feel of data\n%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\n- These attributes have very different scales.\n- The `housing_median_age` and the `median_house_value` were capped. The `median_house_value` may be a serious problem since it is the label to predict. The Machine Learning algorithms may learn that prices never go beyond that limit. You need to check to see if this is a problem or not. If precise predictions even beyond 500,000 is needed, then you have two options:\n    * Option 1: Collect proper labels for the districts whose labels were capped.\n    * Option 2: Remove those districts from the dataset.\n    \n- Many attributes are right skewed. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes to have more bell-shaped distributions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Split the data\nScikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is `train_test_split()`, which provides a couple of additional features. \n- First, there is a random_state parameter that allows you to set the random generator seed. \n- Second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"test_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far we have considered purely random sampling methods. This is generally fine if the dataset is large enough (especially relative to the number of attributes), but if it is not, will face the risk of introducing a significant sampling bias. \n\nWhen a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population. \n\n### **For example, the US population is 51.3% females and 48.7% males, so a well-conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called **stratified sampling**: the population is divided into homogeneous subgroups called **strata**, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Suppose `median_income` is a very important attribute to predict median housing prices. We want to ensure that the test set is representative of the various categories of incomes in the whole dataset. \n\nSince the `median_income` is a continuous numerical attribute, we first need to create an income category attribute. It is important to have a sufficient number of instances in each stratum, or else the estimate of a stratum’s importance may be biased. This means that we should not have too many strata, and each stratum should be large enough.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"median_income\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `stratify` within `train_test_split` offers an option for stratified sampling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, \n                                         stratify = housing[\"income_cat\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also use Scikit-Learn’s `StratifiedShuffleSplit` to realize stratified sampling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set1 = housing.loc[train_index]\n    strat_test_set1 = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can take a look at the comparison of stratified sampling and random sampling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set), # train_test_split\n    \"Stratified1\": income_cat_proportions(strat_test_set1), #StratifiedShuffleSplit\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n\ncompare_props","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the income_cat attribute so the data is back to its original\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gain insights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = train.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=train['population']/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"california_img = mpimg.imread(\"../input/california-housing-feature-engineering/california.png\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ax = train.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=train['population']/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = train[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar()\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking for correlations\n\nSince the dataset is not too large, we can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the `corr()` method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation coefficient ranges from –1 to 1. \n- When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up. \n- When the coefficient is close to –1, it means that there is a strong negative correlation; we can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when head to north of California). \n- Finally, coefficients close to 0 mean that there is no linear correlation.\n\n<img src=\"https://i.imgur.com/8McsYNO.png\" width=\"600\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another way to check for correlation between attributes is to use the pandas `scatter_matrix()` function, which plots every numerical attribute against every other numerical attribute. \n\nWe will just focus on a few promising attributes that seem most correlated with the `median_housing_value`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The most promising attribute to predict the `median_house` value is the median income.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(train[attributes], figsize=(12, 12))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot reveals a few things. \n- First, the correlation is indeed very strong: upward trend can be clearly seen, and the points are not too dispersed. \n- Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500,000. But this plot reveals other less obvious straight lines: a horizontal line around 450,000, another around 350,000, perhaps one around 280,000, and a few more below that. \n\nWe may want to try removing the corresponding districts to prevent algorithms from learning to reproduce these data quirks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfor i in range(0, 3):\n    for j in range(0, 3):\n        axes[i, j].scatter(train[predictors[i*3+j]], train['median_house_value'])\n        axes[i, j].set_xlabel(predictors[i*3+j])\n        axes[i, j].set_ylabel('median_house_value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attribute Combinations\n\nOne more thing you to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. \n\n- For example, the total number of rooms in a district is not very useful if we don’t know how many households there are. What we really want is the number of rooms per household. \n- Similarly, the total number of bedrooms by itself is not very useful: we probably want to compare it to the number of rooms. \n- The population per household also seems like an interesting attribute combination to look at. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"rooms_per_household\"] = train[\"total_rooms\"]/train[\"households\"]\ntrain[\"bedrooms_per_room\"] = train[\"total_bedrooms\"]/train[\"total_rooms\"]\ntrain[\"population_per_household\"]=train[\"population\"]/train[\"households\"]\n\n# let’s look at the correlation matrix again\ncorr_matrix = train.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The new `bedrooms_per_room` attribute is much more correlated with the median house value than the `total_rooms` or `total_bedrooms`. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The `rooms_per_household` is also more informative than `total_rooms` in a district—obviously the larger the houses, the more expensive they are.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation for Machine Learning algorithms\n\nIt’s time to prepare the data for Machine Learning algorithms. Instead of doing this manually, we should write functions for this purpose, for several good reasons:\n\n- This will allow reproduce these transformations easily on any dataset (e.g., the next time get a fresh dataset).\n- We can gradually build a library of transformation functions that you can reuse in future projects.\n- We can use these functions in your live system to transform the new data before feeding it to ML algorithms.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# revert to a clean training set \n# separate the predictors and the labels\ntrain = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\ntrain_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning\n\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. We saw earlier that the `total_bedrooms` attribute has some missing values, so let’s fix this with three options:\n1. Get rid of the corresponding districts.\n2. Get rid of the whole attribute.\n3. Set the values to some value (zero, the mean, the median, etc.).\n\nWe can accomplish these easily using DataFrame’s `dropna()`, `drop()`, and `fillna()`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def option_for_NA(df, col_name = \"total_bedrooms\", option=3):\n    if option == 1:\n        return df.dropna(subset=[col_name])\n    elif option == 2:\n        return df.drop(col_name, axis=1)\n    elif option == 3:\n        median = df[col_name].median()\n        df[col_name].fillna(median, inplace=True) \n        return df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true},"cell_type":"markdown","source":"If choose option 3, DO NOT forget to save the median value computed. We will need it later to replace missing values in the test set when evaluate the system, and also once the system goes live to replace missing values in new data.\n\nScikit-Learn provides a handy class to take care of missing values: `SimpleImputer`. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\n\n# Remove the text attribute because median can only be calculated on numerical attributes\ntrain_num = train.drop(\"ocean_proximity\", axis=1)\n\n# fit the imputer instance to the training data\nimputer.fit(train_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The imputer has simply computed the median of each attribute and stored the result in its `statistics_` instance variable. It is usually safer to apply the imputer to all the numerical attributes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check this is the same as manually computing the median of each attribute\nimputer.statistics_ == train_num.median().values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the training set with imputer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.strategy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = imputer.transform(train_num)\n\ntrain_tr = pd.DataFrame(X, columns=train_num.columns,\n                          index=train_num.index)\ntrain_tr.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical attributes\n\nSo far we have only dealt with numerical attributes. Now let's preprocess the categorical input feature, `ocean_proximity`.\n\nMost Machine Learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s `OrdinalEncoder` class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\ntrain_cat = train[[\"ocean_proximity\"]]\ntrain_cat_encoded = ordinal_encoder.fit_transform(train_cat)\ntrain_cat_encoded[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can get the list of categories using the `categories_` instance variable. It is a list containing a 1D array of categories for each categorical attribute.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the problem here??\n\nOne issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obviously not the case for the `ocean_proximity` column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category. The new attributes are sometimes called *dummy attributes*. Scikit-Learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors.\n\nBy default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\ntrain_cat_1hot = cat_encoder.fit_transform(train_cat)\ntrain_cat_1hot.toarray()\n\n# alternatively, set sparse=False\n# cat_encoder = OneHotEncoder(sparse=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the list of categories\ncat_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Transformer\n\nAlthough Scikit-Learn provides many useful transformers, we will need to write our own for tasks such as custom cleanup operations or combining specific attributes. We will want our transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all we need to do is create a class and implement three methods: `fit()` (returning self), `transform()`, and `fit_transform()`.\n\nWe can get the last one for free by simply adding `TransformerMixin` as a base class. If add `BaseEstimator` as a base class (and avoid `*args` and `**kargs` in the constructor), we will also get two extra methods (`get_params()` and `set_params()`) that will be useful for automatic hyperparameter tuning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\ntrain_extra_attribs = attr_adder.transform(train.values)\ntrain_extra_attribs = pd.DataFrame(\n    train_extra_attribs,\n    columns=list(train.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=train.index)\ntrain_extra_attribs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above example the transformer has one hyperparameter, `add_bedrooms_per_room`, set to `True` by default. This hyperparameter will allow us to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, we can add a hyperparameter to gate any data preparation step that you are not 100% sure about. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling\n\nOne of the most important transformations you need to apply to your data is **feature scaling**. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. *Note that scaling the target values is generally not required*.\n\nThere are two common ways to get all attributes to have the same scale: **min-max scaling** and **standardization**.\n- Min-max scaling (many people call this normalization) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the minimum value and dividing by (maximum - minimum). Scikit-Learn provides a transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that allow to  change the range.\n- Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called `StandardScaler` for standardization.\n- other scalling: \n    * `MaxAbsScaler`: differs from the previous scaler such that the absolute values are mapped in the range [0, 1]. On positive only data, this scaler behaves similarly to MinMaxScaler and therefore also suffers from the presence of large outliers.\n    * `RobustScaler`: uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. After Robust scaling, the distributions are brought into the same scale and overlap, but the outliers remain outside of bulk of the new distributions.\n    * `Normalizer`: points are all brought within a sphere that is at most 1 away from the origin at any point. Also, the axes that were previously different scales are now all one scale.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Transformation Pipelines\n\nAs you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the `Pipeline` class to help with such sequences of transformations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\ntrain_num_tr = num_pipeline.fit_transform(train_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. \n\nIn version 0.20, Scikit-Learn introduced the `ColumnTransformer` for this purpose.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(train_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\ntrain_prepared = full_pipeline.fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_pipeline.get_feature_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the `OneHotEncoder` returns a sparse matrix, while the `num_pipeline` returns a dense matrix. When there is such a mix of sparse and dense matrices, the `ColumnTransformer` estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold (by default, sparse_threshold=0.3). ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Select and train a model \n\nWe are now ready to select and train a Machine Learning model!\n\n### Linear Regression\n\nLet's first try on Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(train_prepared, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare against the actual values:\n\nA typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors.\n\n$$RMSE = \\sqrt{\\frac{1}{m}\\sum_{i = 1}^m(h(x^{(i)})-y^{(i)})^2}$$\n\nEven though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, suppose that there are many outlier districts. In that case, you may consider using the mean absolute error (MAE).\n\n$$MAE(X,h)=\\frac{1}{m}\\sum_{i=1}^m|h(x^{(i)})-y^{(i)}|$$\n\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. RMSE corresponds to the Euclidean distance, MAE corresponds to the Manhattan distance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ntrain_predictions = lin_reg.predict(train_prepared)\nlin_mse = mean_squared_error(train_labels, train_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(train_labels, train_predictions)\nlin_mae","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree\nA prediction error of 68,628 of `median_housing_values` which range between 120,000 and 265,000 is not very satisfying. Let's try a more powerful model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(train_prepared, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predictions = tree_reg.predict(train_prepared)\ntree_mse = mean_squared_error(train_labels, train_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No error at all? Could this model really be absolutely perfect? ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}