{"cells":[{"metadata":{},"cell_type":"markdown","source":"To predict fake news, trained a pretrained Bert model and CNN model. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.preprocessing.text import Tokenizer\n\nfrom keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\nfrom keras.models import Sequential,Model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.datasets import fetch_20newsgroups\nfrom keras.layers.merge import Concatenate\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\nfrom keras.models import Sequential,Model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.datasets import fetch_20newsgroups\nfrom keras.layers.merge import Concatenate\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom keras.backend import concatenate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for bert model\n!pip install ktrain\nimport ktrain\nfrom ktrain import text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/a-fake-news-dataset-around-the-syrian-war/FA-KES-Dataset.csv',encoding='latin1')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define feature text and target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['article_title'].apply(lambda x: ' ') is used to create a space between two column text\ntexts = np.array(df['article_title'] + df['article_title'].apply(lambda x: ' ') + df['article_content'])\ntarget = df['labels']\n# texts[0], target[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = np.unique(target).tolist()\ntarget_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bert Model"},{"metadata":{},"cell_type":"markdown","source":"### Split Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\ntexts, target, test_size=0.3, random_state=42)\n\nX_train = X_train.tolist()\nX_test = X_test.tolist()\ny_train = y_train.tolist()\ny_test = y_test.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess Data and Extract Features for Bert Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train,  y_train), (X_test, y_test), preproc = text.texts_from_array(x_train=X_train, y_train=y_train,\n                                                                       x_test=X_test, y_test=y_test,\n                                                                       class_names=target_names,\n                                                                       preprocess_mode='bert',\n                                                                       maxlen=350, \n                                                                       max_features=35000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build and Train Bert Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can disregard the deprecation warnings arising from using Keras 2.2.4 with TensorFlow 1.14.\nmodel = text.text_classifier('bert', train_data=(X_train, y_train), preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=(X_train, y_train), batch_size=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_onecycle(2e-5, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.validate(val_data=(X_test, y_test), class_names=target_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN Model"},{"metadata":{},"cell_type":"markdown","source":"### Split Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(texts, target, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess Data and Extract Features for CNN Model\n1. tokenization\n2. word indexing\n3. vectorization\n4. padding\n5. assignment of word embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenization\nMAX_NB_WORDS = 3000\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS) # based on the word frequency, num_words-1 words will be kept\ntokenizer.fit_on_texts(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word indexing\nword_index = tokenizer.word_index\nprint(\"Data type of word index: {} and length of dictionary {}\".format(type(word_index),\n                                                                       len(word_index)) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vectorization\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\nprint(\"Sequence: \", sequences_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# padding\nMAX_SEQUENCE_LENGTH = 1600\ntexts_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH) # default value truncate the previous sequence\ntexts_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH) # default value truncate the previous sequence\nprint(\"Padding Result: \", texts_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assignment of word embedding\n## access pretrained word embedding file\npath='/kaggle/input/gloveicg/glove/Glove/glove.6B.300d.txt'\nembeddings_index = {}\nf = open(path, encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    try:\n        coefs = np.asarray(values[1:], dtype='float32')\n    except:\n        pass\n    embeddings_index[word] = coefs\nf.close()\n\n## build word embedding matrix based on word index\nEMBEDDING_DIM = 300\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        if len(embedding_matrix[i]) !=len(embedding_vector):\n            print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n            exit(1)\n        embedding_matrix[i] = embedding_vector\n\nembedding_matrix.shape # words, vector length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of samples, Max sequence length, embedding dim\")\nlen(df), MAX_SEQUENCE_LENGTH, EMBEDDING_DIM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build and Train CNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n# embedded_sequences = embedding_layer(sequence_input)\n# flat_1 = Flatten()(embedded_sequences)\n# flat_1.shape\n# embedded_sequences.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropout=0.5\nnclasses = 2\n\n# input layer and embedding layer\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_embedding')\nembedded_sequences = embedding_layer(sequence_input)\nflat_1 = Flatten(name='flatten_1')(embedded_sequences)\n# dense layer and dropout layer\nl_dense = Dense(512, activation='relu', name='dense_layer_1')(flat_1)\nl_dropout = Dropout(dropout)(l_dense)\n\n### prediction\npreds = Dense(nclasses, activation='softmax', name='prediction_layer')(l_dropout)\n\nmodel = Model([sequence_input], preds)\nmodel.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(texts_train, y_train,\n              batch_size=128,\n              epochs=5,\n          validation_data =(texts_test, y_test),\n          verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(texts_test, y_test, verbose=0)\nprint(\"Loss value: {}, accuracy:{}\".format(loss, accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}