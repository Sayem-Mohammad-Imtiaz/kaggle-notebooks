{"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"e9c20dc3-4e0f-4853-af09-c1d38f4f50e5","_uuid":"ecf02f60179b0a22b2ad63246b62e3eacc0d77da"},"source":">\"Então você não se lembra de um mundo sem robôs.  Houve um tempo quando a humanidade enfrentou o universo sozinha e sem amigo. Agora ela tem criaturas para ajudá-la; criaturas mais fortes que si mesma, mais confiáveis,  mais úteis e absolutamente devotas. A humanidade não mais está sozinha. Você já pensou nisso desta forma?\" <br>\nI, Robot - Issac Asimov, 1950\n\n\n# Uma breve história dos algoritmos que aprendem\n\n<br><br>\n**Bem-vindos ao Laboratório Introdutório de Machine Learning!**\n<br><br>\n\nEsse é um dos livros que vamos  usar como referência [Python Machine Learning - Second Edition,\nRaschka & Mirjalili, September-2017](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition)\n\n\nO primeiro passo para iniciar nossos estudos  é compreender que **Machine Learning (ML)** é um sub-campo de pesquisa da **Inteleligência Artificial (IA)** e,  portanto, não é necessariamente seu sinônimo como erroneamente sugerem alguns desavisados por ai.  ** Deep Learning (DL)** é um dos tópicos de **Redes Neurais (NN's)** que por sua vez são uma das sub-áreas de **ML**.  Não cometa o erro de confundir indistintamente Deep Learning com Machine Learning.\n![](https://blogs.nvidia.com/wp-content/uploads/2016/07/Deep_Learning_Icons_R5_PNG.jpg.png)\n\n\n\n# Peceptron\nAlgoritmos de aprendizagem não são um tema novo.  A definição de neurônio artificial, o **perceptron**, foi estabelecida no final da década de 50 (The Perceptron: A Perceiving and Recognizing Automaton, F. Rosenblatt, Cornell Aeronautical Laboratory, 1957) e pode ser resumida na função abaixo:\n\n![](https://www.dropbox.com/s/s0uvoloszvkg83x/00-Perceptron.jpg?dl=1)\n\nDe forma simplificada, a saída de um neurônio artificial é igual a soma do produto das entradas ***x*** pelos pesos ***w*** aplicados a cada entrada.   As entradas ***x*** de um neurônio artificial equivalem aos dentritos de um neurônio biológico e a soma **$\\sum_{j=0}^m x_{j}w_{j}$** é o estímulo resultante no axônio, definido por um limiar interno do neurônio (**threshold**) que vai determinar a sua \"sensibilidade\" ou quando será ativado ou não. Em ML preferencialmente utilizaremos a notação matricial  **$w^Tx$** onde o produto é dado pela transposta de *w* por *x*. A utilização de matrizes permite maior eficiência computacional e simplificação dos códigos de ML. \n\n<br><BR>\nAqui temos a representação gráfica do perceptron:\n\n![](https://www.dropbox.com/s/yxvrkm7kk1r991e/01-Perceptron.jpg?dl=1)","cell_type":"markdown"},{"metadata":{"_cell_guid":"22040ec2-d174-4010-bb6c-5df66f5bf8bd","_uuid":"bd1a899f2ca43537a082c6914b680037529c1cd5"},"source":"![](https://www.sololearn.com/avatars/b2b6905b-4e53-412a-bcb8-22bfef2bcec5.jpg)\n\n# Quando as máquinas aprendem...\n\nA aprendizagem se traduz em encontrar pesos que aplicados aos valores de entrada resultem em um determinado valor de saída esperado.  Ainda analisando o gráfico do perceptron acima, vale notar que por questões de convenção e cálculo a entrada **$x_{0} $** é fixada com o valor ***1*** e seu o peso **$w_{0} $**  é chamado de **bias**.   Em uma rede neural de apenas uma entrada teríamos a seguinte equação equivalente z =  $w_{0}$ + $w_{1}x_{1}$.  Se voltarmos às aulas de matemática fundamental veremos que essa é exatamente uma **equação reduzida da reta **, onde $w_{0}$ define a \"altura\" da reta e  $w_{1}$ define sua inclinação no gráfico.   \n\n![](https://www.dropbox.com/s/cdai4n28jp5m5wp/simple_regression.png?dl=1)\n\n\nO que os algoritmos de ML fazem é buscar de forma automática a equação que melhor representa o conjunto verdade (**y**) para um conjunto de observações ou amostras de entradas.  Uma forma de encontrar a melhor equação é através do cálculo sucessivo da diferença entre os valores gerados pela equação \"aprendida\" (**ŷ**) e os valores reais observados (**y**). Essa diferença chamamos de **Erro** ou **Perda**. As funções de perda  ou **loss functions** são um importante elemento na construção de algoritmos inteligentes. Em outras palavras, podemos afirmar que a aprendizagem de máquina é essencialmente uma tarefa de otimização de funções.  Atualmente os principais frameworks de ML implementam diversos algoritmos de otimização, sendo o Gradiente Descendente Estocástico ([SGD](http://ruder.io/optimizing-gradient-descent)) um dos mais populares.\n\nO processo de ajustar pesos através de algoritmos de otimização de função é denominado **fit (treino)**, e cada rodada de ajustes é chamada de **Epoch (Época)**. O ajuste geralmente é feito usando um determinado número de amostras por vez que chamamos de **Batch (Lote)** .\n\nNo gráfico acima temos um problema onde os valores de solução podem ser linearmente correlacionadas com as amostras de entrada. Neste caso um algoritmo de ** Regressão ** poderia ser aplicado, mas existem varios tipos de algoritmos de ML e cada um vai funcionar melhor em determinados cenários.  Daí o teorema  **No Free Lunch**  em Machine Learning de David H. Wolpert, que nos recorda de que nenhum algoritmo de ML é universalmente melhor que todos os outros em todos cenários (The Lack of A Priori Distinctions Between Learning Algorithms, Wolpert and David H, 1996).\n\n\n\n# Rolling in the Deep\n\nAs redes profundas conhecidas como Deep Learning(DL) ganharam maior destaque a partir de 2012 com a vitória de um time universitário canadense em uma competição de classificação de Imagens, a [ImageNet](http://www.image-net.org/).\n\nMas a vitória deste time canadense está intimamente relacionada com avanços da década de 80, sendo um de seus expoentes o cientista de computação e psicologia cognitiva **Geoffrey Hinton** da Universidade de Toronto. Hinton é conhecido por temas como **Propagação Reversa (Backpropagation)**, **Máquina de Boltzman (Boltzmann Machine** e **Deep Learning**. \n\nEmbora o termo Deep Learning já havia sido aplicado a redes neurais artificiais por Igor Aizenberg em 2000,  foi uma plublicação de Geoffrey Hinton e Ruslan Salakhutdinov em 2006 que chamou mais atenção ao mostrar como redes neurais poderiam ser pré-treinadas uma camada por vez, e então fazer ajustes finos por meio de Backpropagation.  Esse avanço contribuiu fortemente para a viabilidade das redes DL como hoje conhecemos.\n\nEm 2012, Hinton e seus dois alunos Alex Krizhevsky e Ilya Sutskever entraram na competição ImageNet e ao fazerem uso de redes densas convolucionais (CNN's) e técnicas avançadas para reduzir overfitting (ajuste excessivo aos dados de treino que resulta em baixa generalização) conseguiram atingir um incrível patamar de erro de 16% contra os 25% alcançados até então com algoritmos classificadores existentes. Hinton e seus alunos criaram uma empresa que seria adquirida posteriormente pelo Google.\n\nAbaixo o gráfico da arquitetura de sua rede **[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)** . Esta rede foi treinada por cerca de 5 a 6 dias usando um dataset de 15 milhões de imagens classificadas com cerca de 22.000 classes. A equipe da AlexNet além da arquitetura inovadora utilizou duas placas de video GTX 580 (GPU) para poder suportar a alta demanda de processamento desse tipo de rede. O poder de manipulação de matrizes de uma GPU é muito bemvindo com algoritmos de ML já que no final das contas toda informação e aprendizagem resultam em matrizes de dados e pesos.\n\n\n![](https://image.slidesharecdn.com/dlcvd2l4imagenet-160802094728/95/deep-learning-for-computer-vision-imagenet-challenge-upc-2016-7-638.jpg?cb=1470131387)\n\nNos anos seguintes empresas como Nvidia, Google, Microsoft, Baidu, Amazon, IBM, Ubber, Facebook e Tesla  entrariam de forma ainda mais agressiva na corrida tecnológica por plataformas de inteligência artificial mudando o nível do jogo para uma aposta de trilhões de dólares,  e criando com o apoio das diversas comunidades de código aberto os frameworks poderosos que estão hoje ao alcance de alguns cliques. Abaixo algum dos principais frameworks da atualidade:\n\n![](https://www.dropbox.com/s/lv9ooa3ur8pxc33/deep-learning-developer-frameworks-407.png?dl=1)","cell_type":"markdown"},{"metadata":{"_cell_guid":"6d21cbbf-95b9-4e29-bdc7-26df3ca0dfce","_uuid":"adff681816b86fee2a4d3d647c12af834c37a2ba"},"source":"# O que é Machine Learning?\n<br>\nA seguir vamos começar a entender um pouco mais sobre como funciona os principais tipos de algoritmos de ML,  quais são as estratégias de treino e etapas para construção destes algorítimos.\n\nEntão, uma pergunta importante a fazer é: o que é um algorítimo de aprendizagem de máquina? Uma boa definição , emprestada de [*Deep Learning*](http://www.deeplearningbook.org) (Goodfellow-et-al-2016) ,  seria ***\"um algorítmo de aprendizagem de máquina é um algorítmo capaz de aprender com os dados\"* **.\n\nOk, mas o que significa aprender? Tom Mitchell em seu livro* Machine Learning* (McGraw-Hill, New York. 97) nos ajuda com uma definição bem sucinta: *** “Um programa de computador é dito aprender de uma experiência E em respeito a alguma classe de tarefa T e medida de performance P, se sua performance em tarefas T, como medido por P, melhora com a experiência E\".*** \n\nTodavia não podemos esquecer que Machine Learning é um campo em construção e muitos dos conceitos que hoje consideramos  verdade serão descartados nos próximos anos. O próprio[ Geoffrey Hinton em entrevista com Andrew NG](https://www.youtube.com/watch?v=-eyhCTvrEtE) (outro nome bastante conhecido da galera de ML) diz *\"Meu conselho é que leia alguma literatura (*de ML*) mas não leia demais... alguns dizem que você deveria passar vários anos lendo a literatura e só então começar a trabalhar em suas próprias idéias e isso pode ser verdade para alguns pesquisadores, mas para pesquisadores criativos eu penso que o que você quer é estudar uma parte da literatura e, então, notar o que todos estão fazendo errado... aquilo que você sente que não está correto, e ao contrário imaginar um jeito de fazer certo... e quando os outros disserem que não serve, apenas continue... tenho um bom princípio para ajudar as pessoas a continuarem que é: ou suas intuições são boas ou não, se são boas você deveria seguir-las e ao final terá sucesso, se não são boas não importa o que você faça... você deveria confiar nas suas intuições não há razão para não fazê-lo...\"* (tradução livre)\n\n\nPortanto, a seguir veremos três grandes grupos de algoritimos de ML, mas utilize essa divisão apenas como ferramenta de compreensão já que alguns algoritmos atuais extravasam essas classificações.\n\n\n# Os três tipos de Machine Learning\n\nOs algoritmos de Machine Learning podem ser agrupados em três tipos principais:\n\n\n![](https://www.dropbox.com/s/btluyzv2e08djan/02-MLTipos.jpg?dl=1)\n\n## Supervised Learning\nO principal objetivo na **aprendizagem supervisionada** é \"aprender\" um modelo com base nos dados de treino rotulados que seja capaz fazer predições a respeito de dados novos ou de dados futuros. \n\nQuando os valores esperados são discretos, como por exemplo um algoritmo capaz de reconhecer se uma imagem é de um gato ou cachorro, dizemos que se trata de uma **Tarefa de Classificação** ou seja buscamos um modelo classificador.  Classificação é uma subcategoria da aprendizagem supervisionada na qual o foco é prever rótulos categóricos de novas instâncias baseado nas observações do passado.\n\nA predição de  valores contínuos, como por exemplo o preço de venda de um imóvel, é tratada por outra subcategoria de aprendizagem supervisionada a **Regressão**.  \n\n\n## Reinforcement Learning\nOutro tipo de aprendizagem de máquina é o aprendizado por reforço. Em **reinforcement learning** o objetivo é desenvolver um **agente** que melhora sua performance baseado em sucessivas interações com o ambiente.  Diferentemente das funções de perda (loss functions) das técnicas supervisioanadas, aqui o feedback é dado por um sistema de recompensas que pune ou premia certos resultados (**reward function**) com base em certos estados do ambiente.\n\nUm exemplo popular desta arquitetura de aprendizagem é uma Engine de Xadrez. Nela o agente decide uma série de movimentos de acordo com o estado do tabuleiro, a recompensa pode ser definida com base em diversos resultados como sobrepor uma peça inimiga ou tomar sua rainha, ou mesmo a vitória ou derrota final.\n\n\n\n\n## Unsupervised Learning\nNa aprendizagem não supervisionada lidamos com dados não rótulados ou com informação cuja estrutura não é exatamente conhecida.   \n\nAo usarmos  técnicas de aprendizagem não supervisionada somos capazes de explorar a estrutura de nossas amostras e extrair informação significativa de como essas amostras se relacionam.  Uma das aplicações práticas deste tipo de aprendizagem é a segmentação (**clustering**) de clientes de acordo com suas preferências ou quaisquer outras características que tenhamos à disposição. \n\nOutro campo de aplicação da aprendizagem não supervisionada é redução de dimensionalidade de dados. A redução de dimensionalidade permite eliminar ruídos e  comprimir informação resultando em economia de processamento e armazenamento de dados. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"a59f9d08-6aee-4165-baa6-2ddfa9346d79","_uuid":"a219f277dbabeea4e3da3a879e6c2446a5e0e0e3"},"source":"# Botando a mão na massa!\n\nAgora que vimos em linhas gerais o que são algorítmos de ML, vamos começar com o primeiro passo no desenvolvimento de um sistema de ML:   A preparação e exploração de dados ou **exploratory data analysis (EDA)** termo também emprestado do campo de estatística.  \n\nNos exemplos vou usar um famoso dataset chamado Iris que possui 150 amostras de 3 tipos de flores e os tamanhos de suas pétalas.  Em ML essas características ou dados de entrada denominamos **features**.\n\n*Você deve executar cada célula de código . Use Ctrl + Enter para executar e Shift + Enter para criar uma nova célula*\n\n## 1 - Bibliotecas ","cell_type":"markdown"},{"source":"#Usamos import para importar as bibliotecas e pacotes que vamos utilizar\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # library for draring charts\n\n# A tag '%' é uma magic cell que permite alterar o comportamento de uma célula de um notebook\n# a magic cell abaixo permite exibir automaticamente os graficos criados com matplotlib \n%matplotlib inline\n\n# Exibe a versão das biblioteca. Em alguns casos é importante em que versão está trabalhando\nprint(\"Numpy Version {}\".format(np.__version__))\nprint(\"Pandas Version {}\".format(pd.__version__))","metadata":{"_cell_guid":"3b7d9c6c-12aa-41b4-953c-cbef8810029b","_uuid":"5de0e1e74b4ee6b4f300fabcbbb109c82083143c","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"0eed9cc0-2888-46f2-a5e4-86a016637de1","_uuid":"88185240749e454cca2cd636fc9c6a88a5198227"},"source":"## 2 - Caminho do Dataset","cell_type":"markdown"},{"source":"'''\nO dataset que vamos usar foi adicionado automaticamente pelo Kaggle na parte superior \ndeste notebook na área \"Input Files\". Lá é possível adicionar qualquer dataset público \ndo Kaggle com o botão \"Add a Data Source\" ou o seu próprio com \"Upload a Dataset\"\n\nOs dataset adicionados serão postos no caminho \"../input\".\nAbaixo executamos o comando linux ls através do python para listar os arquivos desta pasta:\n'''\n\nfrom subprocess import check_outpu\nprint('Arquivos Iris:')\nprint(check_output([\"ls\", \"../input/iris\"]).decode(\"utf8\"))","metadata":{"_cell_guid":"8d6c3d95-8a72-43ba-b42e-a1f7b1d24072","_uuid":"1ac38464d1588421a7b2db1f3f3d290a27c1500d","_kg_hide-output":false,"collapsed":true,"_kg_hide-input":false},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"08b39ae4-1338-45dd-a0be-d2d50c647fae","_uuid":"4744c41851afcb76ae5dbacc7f254cac02db8b0f"},"source":"## 3 - Carregando o Dataset","cell_type":"markdown"},{"source":"# Existem várias formas de se carregar um dataset para uso em ML as duas mais comuns:\n# usar a biblioteca numpy ou carregar um data frame do pandas como abaixo\ndf_iris = pd.read_csv('../input/iris/Iris.csv')","metadata":{"_cell_guid":"668ab5a8-6bf3-460c-9752-3884ce911bbe","_uuid":"becdadf42dde8a7ce06bbe24a96c75aee321c005","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"# Exibe as primeiras 5 linhas do dataframe\ndf_iris.head(5)","metadata":{"_cell_guid":"166b2060-a74d-49a7-bd28-9c3b4e260ca8","_uuid":"3074786f42f081c0570bf094b5349cfa76037896","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"134bea85-22c3-4b00-91fc-ed64661364cc","_uuid":"f2bb120e5624ffbb39a21f25dc25b3a92c5021aa"},"source":"## 4 - Explorando os dados com gráficos do matplotlib\n\nNo dataset Iris temos na coluna Species os tipos das flores que vamos analisar. Para isso precisamos transformar as classes de flores em números, para podermos seguir com as análise","cell_type":"markdown"},{"metadata":{"_cell_guid":"39bec785-3f85-4291-b366-9071d83d5b45","_uuid":"8dbdfeb0a42bbf94cb047a5e0feacb7ae2638d39"},"source":"**Exibindo a distribuição das classes**","cell_type":"markdown"},{"source":"# Verificamos os valores únicos para as espécies\nprint(df_iris['Species'].unique())\n\n# Adicionamos uma nova coluna no data frame e mapeamos com um valor númerico por classe\ndf_iris['y'] = df_iris['Species'].map({'Iris-setosa': 1, 'Iris-versicolor': 2, 'Iris-virginica' : 3})\n\n#plt.scatter(x,y, c=color)\nplt.xlabel('Sepal Length Cm')\nplt.ylabel('Petal Length Cm')\nplt.scatter(df_iris['SepalLengthCm'],df_iris['PetalLengthCm'], c=df_iris['y'])\nplt.show()","metadata":{"_cell_guid":"50dd38b5-517e-4cd9-90c1-095d43f1ab5c","_uuid":"ce82bc08306759116599b056bcec4e82f42983e2","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"7a14520e-9736-43cc-a00a-7cc6677b7db4","_uuid":"133d1553976102f155e19ef0ab88c409734b3382"},"source":"Observando o gráfico acima podemos verificar que com apenas duas features do dataset é possível separar as classes com uma amostra como de exceção.  Esse tipo de feature é muito útil para construirmos nossos modelos de aprendizagem.","cell_type":"markdown"},{"metadata":{"_cell_guid":"e2b61d08-95bb-40ad-984e-c226e1f1f6e1","_uuid":"71ec0edcfe30ba10198f769333934c2fd37bef1b"},"source":"**Usando funções plot e hist do matplotlib para compreender melhor os dados:**","cell_type":"markdown"},{"source":"# Nesse grafico podemos ver que nosso dataset é bastante balanceado\nplt.title('Histograma das Classes - Cada Classe tem 50 ocorrências')\nplt.hist(df_iris['y'])\nplt.show()\n \nplt.title('Histograma da Propriedade Sepal Length\\n Maior número de amostras com valor entre 5 e 7 cm')\nplt.hist(df_iris['SepalLengthCm'], bins=6)\nplt.show()","metadata":{"_cell_guid":"9774c342-0200-4527-8984-cb6bcf41fbe4","_uuid":"6190ec7b0a5afa94bb7f7cc9d11a38c8c235ca7a","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"plt.figure(figsize=(15,10))\nplt.title('Exibindo as medidas por amostras')\nplt.plot ( df_iris['SepalLengthCm'], c='blue', ) \nplt.plot ( df_iris['SepalWidthCm'], c= 'red')\nplt.plot ( df_iris['PetalLengthCm'], c= 'green')\nplt.plot ( df_iris['PetalWidthCm'], c= 'yellow')\nplt.show()","metadata":{"_cell_guid":"aa7f6cd1-b23a-411b-a949-fd37163bc8e9","_uuid":"136177e252e6a8803f7ce7b302e8c9a499abee8b","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"053fce27-5c1f-4216-96b1-296b051a8fe7","_uuid":"6862058fd499459657c45a7c41b7732b253921bc"},"source":"##  5 - Selecionando um algoritmo de ML\nEste é um Dataset bem pequeno, porém bastante balanceado pois temos 50 amostras de cada classe possível.  Vimos que as duas features  SepalLengthCm e PetalLengthCm sozinhas praticamente conseguem definir a separação das três classes mas queremos um classificador que faça o maior acerto possível. Então vamos usar as 4 features que dispomos, nesse caso é importante notar que o **campo Id ** será descartado para análises.  Vamos usar a biblioteca sckit-learn muito útil para preparação de dados e também para implementação de algoritmos que não envolvam redes neurais.\n\nPelas características desse dataset poderíamos ter um bom resultado mesmo usando algoritmos lineares, mas para que você possa verificar o poder de uma rede neural, vamos utilizar 3 neurônios de saída (um para cada tipo de flor) com 4 entradas cada (uma para cada feature), em algorítmos de classificação o número de saídas deve ser igual ao número de classes quando tivermos mais que duas classes.  \n\nNão se preocupe se não entender alguma parte do código, vamos explorar todos detalhes nos próximos Labs, o objetivo aqui é você ver uma solução completa usando Keras e TensorFlow.","cell_type":"markdown"},{"source":"#veja que ao importar o keras que é um wrapper, o TensorFlow será exibido como backend\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils","metadata":{"_cell_guid":"5ab8f9f6-1d50-4349-9682-0a78bf917427","_uuid":"ff9782560b819cc7e0019f975ced9ded9ba07a69","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"e77ec279-25d0-4b58-8b5d-5b424a154e0f","_uuid":"102b151d344b62798f8ce5e8bd985fa99c962752"},"source":"## 6 - Feature Engineering \n\nNesta fase temos a seleção das features que vão compor nosso Modelo, e seu ajuste para que sejam compatível com o formato de entrada do tipo de algorítm selecionado.  Nossa coluna Y, por exemplo contém valores de 1 a 3, esses valores serão transformados para 0 a 2 e convertidos em três colunas no formato** One-Hot**, que estudaremos nos próximos labs. Além disso em muitos casos vamos normalizar os valores de entrada antes de aplicar a uma rede neural ou qualquer outro tipo de algoritmo de ML.","cell_type":"markdown"},{"source":"#Número de classes possíveis\nn_classes = len(df_iris['Species'].unique()) # 3 classes\n\n#Fazemos slice do Dataframe e as convertemos em matrizes do NumPy\nx_full = np.array(df_iris.iloc[:, 1:5].values) # selecionamos as colunas de features e todas linhas\ny_full = np.array(df_iris.iloc[:, 6].values) # selecionamos todas linhas mas apenas a coluna 'y' \n\n# Para algorítimos de classificação com mais de duas classes temos que usar one-hot\n# aqui uso uma simples subtração para alterar os valores de todas a linhas y\ny_full = np_utils.to_categorical(y_full - 1, n_classes) \n\nprint(\"Vericamos se as matrizes de entrada possuem o formato correto\")\nprint(\"x_full.shape ={}    y_full.shape ={}\".format(x_full.shape, y_full.shape))","metadata":{"_cell_guid":"7fa2bfb2-c80e-4dbe-a7c3-e0eeed8b5abd","_uuid":"d5f22767cfb9017d2e8ca21d4bf8a27ea2874c90","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"31fab741-8f16-46aa-b23b-c59c6c9b4916","_uuid":"a82aeaafb6b2d7189c0ebe4489589b940121eeef"},"source":"## 7 - Split do dataset em treino e validação","cell_type":"markdown"},{"source":"seed = 42 # aqui ficamos o seed randômico, para garantir a reprodução de resultados\n\n# A separação do dataset é uma técnica muito importante para maior eficiência\n# da validação da eficácia de um modelo e veremos em maior detalhe nos próximos labs. \nX_train, X_val, y_train, y_val = train_test_split(x_full, y_full,\n                                                test_size=0.2, random_state=seed)\n\n# A classe train_test_split faz o embaralhamento dos dados antes\nprint(\"Novamente validamos os formatos do split:\")\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","metadata":{"_cell_guid":"4617dc7b-fef8-4088-a072-2a6bf9d58af9","_uuid":"116ba880aa00fac1840df107fa19fefcf3de62ab","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"a558d1a8-e2ff-4a83-965a-3c2142af4c0f","_uuid":"462e3e4f585bd536164f2dc0faac375df460aee8"},"source":"## 8 - Definindo o Modelo para nossa Rede Neural","cell_type":"markdown"},{"source":"# Fixamor o seed permite reproduzir o mesmo resultado e é muito importante \n#com algumas estratégias de validação de treino\nnp.random.seed(seed) \n\n#Cada implementação de um algorítimo de ML chamamos de modelo\nmodel = Sequential()\nmodel.add(Dense(n_classes, input_shape=(4,)))\nmodel.add(Activation('softmax')) # vamos usar um ativação de threshold conhecida como softmax\nmodel.summary()\n","metadata":{"_cell_guid":"ca8199df-a145-4e44-9852-e2c16f44bedf","_uuid":"2a984442cb7a96bd4711f5e9ad8dfaa753845d1e","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"33e1e6af-14b5-445b-af6c-0000406d08e9","_uuid":"bb415711369af975c5b4856a603b89bae51fe89b"},"source":"Se olharmos acima notamos a existência de 15 parâmetros treináveis. Cada um dos três neurônios possuem 4 entradas, uma para cada feature, portanto teremos 4 x 3  = 12, ou seja 12 pesos que devem ser treinados. E de onde vem os 15?  \n\nLembra que para cada neurônio vamos ter uma entrada $x_{0}$ igual a 1 e um peso peso $w_{0}$ que será o **bias**?   Então como temos 3 neurônios teremos 3 biases a serem treinados. Dai 12 pesos + 3 biases, resultando em 15 parâmetros treináveis.","cell_type":"markdown"},{"metadata":{"_cell_guid":"8de1489d-4d99-458d-9675-706b007f842b","_uuid":"4f814ddcb923ad53b9318fb703cb0946fb3b7846"},"source":"## 9 - Compilando e Treinando nosso Modelo (Finalmente!!)","cell_type":"markdown"},{"source":"import timeit\n\nn_epoch = 500 # Número de Épocas\nbatch_size = 10 #tamanho do Batch (quantidade de amostras por lote de treino)\n\n#Aqui vamos usar o Gradiente Descendente Estocástico, que é um tipo de otimizador\nsgd = SGD(lr= 0.1) # lr é o Learning Rate conceito que vamos ver nos próximos labs.\n\n#Todo modelo precisa ser compilado, veja que no parâmetro loss informamos a função de erro\nmodel.compile(optimizer=sgd, loss='categorical_crossentropy') \n\n#Inicia contagem do tempo\nstart = timeit.default_timer()\n\n# Aqui fazemos o fit do modelo e salvamos o resultado de cada epoca em history\nhistory = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epoch,verbose=0) \n\n#Inicia contagem do tempo\nelapsed = timeit.default_timer() - start\n\nprint(\"Rede Treinada em {} épocas durante {:.4f} segundos\".format(n_epoch, elapsed))","metadata":{"_cell_guid":"66ab352b-e584-460e-ac35-22242b61a7dd","_uuid":"1fdbb011bf6cbfa62d63703b90c368fd90e5ef1e","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"45c37822-556d-4593-a15b-14d21b61fab2","_uuid":"0164ed11a70db538dbd0d3bd40fe19f5a2ba817c"},"source":"## 10 - Avaliando o quão inteligente é nosso algorítimo","cell_type":"markdown"},{"source":"train_loss = model.evaluate(X_train, y_train, verbose=0)\nval_loss = model.evaluate(X_val, y_val, verbose=0)\n\nprint('Perda no Treino: {:.4f}%'.format(train_loss))\nprint('Perda na Validação: {:.4f}%'.format(val_loss))","metadata":{"_cell_guid":"6d01c39c-be39-444a-819a-ac1c2f9ece5c","_uuid":"890e9912573e11576699dd1ed44fba4a52f634c3","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"b3006d12-8b2a-48c5-a1a7-16c77dc6f40c","_uuid":"383c47dd1c254283d91d395c1c1f7b6d52331fbe"},"source":"Uau!!! Um excelente resultado com cerca de 5 segundos de treino e uma rede de somente 3 neurônios e um mínimo ajuste de **hiperparâmetro**, o **Learning Rate** (LR=0.1). \n\nUma perda de 0.077% siginifica que na validação essa foi nossa margem de erro. \n\nHiperparâmetros serão tema para um próximo lab. Fiquem a vontade para fazer Fork desse Kernel e testar valores diferentes para número de épocas, batch size e tipo de otimizador.  ","cell_type":"markdown"},{"metadata":{"_cell_guid":"7d391804-5ad3-4862-ac1a-6bd0a1084259","_uuid":"fce00b6bf7d44230168a1a6bb5e5f6d3eb24fc56"},"source":"** Matrix de Confusão** \n\nEsta é uma outra forma de visualizar a acurácia de uma rede, geralmente aplicamos somente no dataset de validação.\nAqui apliquei nos dois para poder exibir onde nosso algoritmo errou.","cell_type":"markdown"},{"source":"y_hat_train = model.predict_classes(X_train)\npd.crosstab(y_hat_train, np.argmax(y_train, axis=1))","metadata":{"_cell_guid":"10763f0b-de86-48bf-8f2a-57f0d334a870","_uuid":"70f22a551dcbe502e3d3b1e4b12c0ed6de05f655","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"y_hat_val = model.predict_classes(X_val)\npd.crosstab(y_hat_val, np.argmax(y_val, axis=1))","metadata":{"_cell_guid":"9f844bab-6495-409b-a828-e3e2910350c9","_uuid":"97b413a3aebcd4e10e83b2d2f5c07eac73657fcf","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"dc3abad2-a075-4c54-a915-2238e5613cc7","_uuid":"95f422209b27d49d931a7e81562bdcebb17a0ff1"},"source":"Se olharmos as duas matrizes de confusão veremos que nosso modelo errou apenas 3 amostras das 150. Um ótimo feito para uma rede de penas uma camada densa de 3 neurônios.","cell_type":"markdown"},{"metadata":{"_cell_guid":"364abd4c-b777-4428-b12b-f933b4570d4a","_uuid":"9d5b4c5a42516db7ceaea03c750a86f240b6dba7"},"source":"## 11 - Verificando a curva de aprendizagem de sua rede\nÉ possível verificar que a partir da época 300 não há grande melhoria (diminuição do erro)","cell_type":"markdown"},{"source":"# a impressão dos valores de perda a cada época de treinamento\n# permite ter valiosos insights sobre como seu modelo se comporta durante o treinamento\nplt.figure()\nplt.plot(history.history['loss'])\nplt.show()","metadata":{"_cell_guid":"600211d0-24bd-4952-ad34-c3fbcf3d4214","_uuid":"5aea86729bdf8af4ef5002764ac3cdf421620cb8","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"4f47816b-49a8-4f11-9ea5-1ef0c553e6e3","_uuid":"61f1dfc72ed385a8e6d25d0c9fb30efe43053704"},"source":"## Tarefas do Lab\n\nCrie um fork deste notebook em sua conta (assim você trabalha em sua própria cópia), e nos quadros abaixo escreva código para carregar o dataset  **House Sales in King County** (kc_house_data.csv) que já está copiado na pasta **../input/housesalesprediction** .  Crie uma abaixo de cada tarefa solicitada e selecione o tipo Code na parte superior da célula. Alguns blocos eu já deixei as células criadas.\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"e5cb005a-3d2a-41b3-b087-065bece8279d","_uuid":"9bcb0d95a0e511f2d50c206a3b48f172560744ff"},"source":"### 1 - Carregar o Dataset House Sales de King County","cell_type":"markdown"},{"source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output \nprint('Arquivo House Sales:')\nprint(check_output([\"ls\", \"../input/housesalesprediction\"]).decode(\"utf8\"))","metadata":{"_cell_guid":"1a879fa8-53bf-4cdd-9b0c-056ee60d244a","_uuid":"6f925b6f5bff3b5357e214823ee0d73203817d0d","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"#Usando pandas crie um dataframe para armazenar o dataset House Sales\n#df_house = pd.read_csv...\ndf_house = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')","metadata":{"_cell_guid":"f551a2d5-8603-472e-842a-74ed4ec00ed7","_uuid":"a55666ece632d9e0b24f5609e104d0df3b632682","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"f38993f6-3131-4113-9921-81f0571ce2fb","_uuid":"44a73dd42341518d9fe4ff3d43745552a9850f8b"},"source":"### 2 - Exibir as 20 primeiras linhas e as últimas 5 do data frame df_house","cell_type":"markdown"},{"source":"#df_house.head(20)\npd.concat([df_house.head(20), df_house.tail(5)])","metadata":{"_cell_guid":"f78c774b-52ca-434d-bd0a-be348bd2481b","_uuid":"863edc2e51463d31e64443b6f99cdfc9040caf48"},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"9c7e80ac-eb0c-4b92-9b58-8c2c35c46f52","_uuid":"e4a781de3c99d499cd976a827633c91f8b285ef6"},"source":"### 3 - Adicione uma nova coluna no dataframe com o nome 'yearsale' (o campo date possui a data da venda)","cell_type":"markdown"},{"source":"df_house['yearsale'] = pd.to_datetime(df_house['date']).dt.year","metadata":{"_cell_guid":"66cf2c8e-313e-4d74-a431-a2738fb10d17","_uuid":"f5f412e4c53bbe11b67f5136db1caed46fe62f53","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"43d8e5be-cb51-45da-a49f-8a4cfb903c1e","_uuid":"a8b2095abae571a3e39b9efa94645560979aeb42"},"source":"### 4 - Criar um gráfico que relacione o ano de cosntrução (yr_built) com o valor da venda (price)\nAqui é possível utilizar a função plot ou scatter, veja qual funciona melhor.","cell_type":"markdown"},{"source":"\nplt.scatter(df_house['yr_built'],df_house['price'])\nplt.show()\n","metadata":{"_cell_guid":"7af68ab5-af4d-46ce-a068-2f7cce256936","_uuid":"580e19a95618b803d4dac815c2b8c3f7226c4ed7","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"94a74408-a60b-434a-b199-95ca4e89a9ac","_uuid":"34947ff1940dbb158271b62985475aedc0e179c5"},"source":"### 5 - Mostrar o histograma de distribuição das vendas de acordo com o local (zipcode), preço de venda (price) e tamanho das casas (sqft_living)","cell_type":"markdown"},{"source":"plt.title('Histograma')\nplt.hist(df_house['zipcode'] + df_house['price'])\nplt.show()\n\nplt.title('Histograma')\nplt.hist(df_house['price'] + df_house['sqft_living'])\nplt.show()","metadata":{"_cell_guid":"4d57ffad-142e-4d99-a2cf-a45fc329097a","_uuid":"dd8bd08b798c0e1f97a742a14549dc136365d5a9","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"2186a790-4454-42f7-a550-c9a80f56541b","_uuid":"e70997d98ef472905dba09848a54c1c3fed0f44c","collapsed":true},"source":"\n### 6 - Avaliando de forma geral o conteúdo deste dataset qual ou quais colunas você acredita que tenha maior impacto sobre o valor da venda do imóvel?  Correlacione essas colunas com a coluna price. Plote gráficos que justifiquem sua resposta.","cell_type":"markdown"},{"source":"df_house.corr()\n","metadata":{"_cell_guid":"e679adb3-e9e3-462e-94ce-a179b2421af5","_uuid":"6e60708643dcd4c8fee286b7c25ad3b3b1d0ca6d"},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"ca1702ae-246c-4809-b3ed-0b0aad2784f9","_uuid":"0af99625cc78c7b9a91a3b4f4ece858e0a8e2ad2","collapsed":true},"source":"### 7 - Em ML recorremos ao conceito estatístico *Outlier*. Dada uma série de dados uma amostra que possua um valor muito destoante do restante é considerado um *outlier*.   Em algumas análises reconhecer outliers pode ser de grande ajuda para entender a natureza dos dados a serem explorados.  Como você faria para identificar a existência de outliers ao verificarmos o valor das vendas deste dataset?  Dica tente usar gráficos scatter e hist.  ","cell_type":"markdown"},{"source":"","metadata":{"_cell_guid":"bb081b8a-612f-45d3-ad12-ac94e798b2ce","_uuid":"60cdf8ac474dc0b623e904c0e2d35c21bb65598a","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"21793bcd-e055-43ec-bb24-7660836276e0","_uuid":"9079e398a98364e06e1d446aa4cb883c101ed936"},"source":"### 8 - Usando Python e NumPy calcule o valor médio do square feet (pode utilizar a coluna sqft_living para o cálculo) e crie um gráfico para exibir todas as amostras cujo valor do square feet de venda seja maior que o valor médio.","cell_type":"markdown"},{"source":"","metadata":{"_cell_guid":"08421456-e32d-4a66-ad08-a5196af05f90","_uuid":"fe1758103e7657fcd99ff80674ee10ca2369b248","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"bf7bc6ff-215c-4c69-a840-38c517ea947c","_uuid":"529d079719954ca12dc4575d2b35b4db0b2aec34"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"1a7cd8e8-366c-41a5-962c-d8f3d421270b","_uuid":"a4c65152d0c0bc252add40d99e5350e2ffedb82e"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"a90425a1-667b-48b1-ac4c-cfb70f2a3275","_uuid":"0f3f25a1c2e6aec9141df9b86b77a28c3c1e44c7"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"be16178b-0763-4e5a-884e-9054bd229ed1","_uuid":"48b768481690874073541cae39e540bc2ad51f3b"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"2e063052-e001-401f-a111-5836f7b0ab3a","_uuid":"38c0f67a4915f489e3a08a654d670a63f9ed3380"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"e6b714c1-3208-407c-9ff7-a338078e7e9b","_uuid":"4d5d423f6984850f1fb184bfc789f10f50edeed7"},"source":"","cell_type":"markdown"}],"nbformat_minor":1}