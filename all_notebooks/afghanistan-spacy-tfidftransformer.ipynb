{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-13T02:43:18.434919Z","iopub.execute_input":"2021-07-13T02:43:18.43537Z","iopub.status.idle":"2021-07-13T02:43:20.165371Z","shell.execute_reply.started":"2021-07-13T02:43:18.435293Z","shell.execute_reply":"2021-07-13T02:43:20.164219Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://pbs.twimg.com/media/E6EFuIPXEAA9IXP.jpg)twitter.com","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/sentiment-dataset-for-afghanistan-sd4a/SD4A-clean.csv')\ntrain.head().style.set_properties(**{'background-color':'black',\n                                     'color': '#03e8fc'})","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:43:28.302242Z","iopub.execute_input":"2021-07-13T02:43:28.302761Z","iopub.status.idle":"2021-07-13T02:43:28.489801Z","shell.execute_reply.started":"2021-07-13T02:43:28.302737Z","shell.execute_reply":"2021-07-13T02:43:28.488549Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T01:36:57.270033Z","iopub.execute_input":"2021-07-13T01:36:57.270373Z","iopub.status.idle":"2021-07-13T01:36:57.287284Z","shell.execute_reply.started":"2021-07-13T01:36:57.270332Z","shell.execute_reply":"2021-07-13T01:36:57.286395Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.rename(columns={'Wed Jun 13 06:41:31 +0000 2018':'date', '1': 'polarity', 'assure afghans of bright future once u.s invaders by reuters news': 'text'})","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:43:51.056423Z","iopub.execute_input":"2021-07-13T02:43:51.056787Z","iopub.status.idle":"2021-07-13T02:43:51.061378Z","shell.execute_reply.started":"2021-07-13T02:43:51.056765Z","shell.execute_reply":"2021-07-13T02:43:51.060794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:44:09.260495Z","iopub.execute_input":"2021-07-13T02:44:09.260775Z","iopub.status.idle":"2021-07-13T02:44:09.276568Z","shell.execute_reply.started":"2021-07-13T02:44:09.260752Z","shell.execute_reply":"2021-07-13T02:44:09.27536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Code by James McNeill  https://www.kaggle.com/datajmcn/nlp-read-eda/notebook","metadata":{}},{"cell_type":"code","source":"# Extract insights from the excerpt variable\nimport spacy","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:44:15.705024Z","iopub.execute_input":"2021-07-13T02:44:15.705383Z","iopub.status.idle":"2021-07-13T02:44:16.363968Z","shell.execute_reply.started":"2021-07-13T02:44:15.705354Z","shell.execute_reply":"2021-07-13T02:44:16.362547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialise spacy\nnlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:44:22.004553Z","iopub.execute_input":"2021-07-13T02:44:22.004893Z","iopub.status.idle":"2021-07-13T02:44:23.420455Z","shell.execute_reply.started":"2021-07-13T02:44:22.004864Z","shell.execute_reply":"2021-07-13T02:44:23.419444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform initial test on one text\nsample = train.head(5)\nsample\nsample1 = train.loc[0, 'text']","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:44:48.806129Z","iopub.execute_input":"2021-07-13T02:44:48.806445Z","iopub.status.idle":"2021-07-13T02:44:48.8132Z","shell.execute_reply.started":"2021-07-13T02:44:48.806421Z","shell.execute_reply":"2021-07-13T02:44:48.812063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the spacy doc item for review\ndoc = nlp(sample1)\ndoc","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:44:55.582211Z","iopub.execute_input":"2021-07-13T02:44:55.582581Z","iopub.status.idle":"2021-07-13T02:44:55.617985Z","shell.execute_reply.started":"2021-07-13T02:44:55.582552Z","shell.execute_reply":"2021-07-13T02:44:55.617126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reviewing the token, lemma and stopword for each token (item)\nprint(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\n# Review the first 20 values to test the output\nfor token in doc[:20]:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\\t\\t{len(token)}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:45:11.92443Z","iopub.execute_input":"2021-07-13T02:45:11.924776Z","iopub.status.idle":"2021-07-13T02:45:11.930621Z","shell.execute_reply.started":"2021-07-13T02:45:11.924745Z","shell.execute_reply":"2021-07-13T02:45:11.929777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Review stop words</span></h1><br>","metadata":{}},{"cell_type":"code","source":"# A few different options for stopwords, spacy and nltk. Lets compare\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:45:17.656448Z","iopub.execute_input":"2021-07-13T02:45:17.656796Z","iopub.status.idle":"2021-07-13T02:45:19.203657Z","shell.execute_reply.started":"2021-07-13T02:45:17.656764Z","shell.execute_reply":"2021-07-13T02:45:19.202722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparison of the stop words available\nprint(f\"NLTK : {len(stopwords.words('english'))} \\n {stopwords.words('english')}\")\nprint(f\"Spacy : {len(nlp.Defaults.stop_words)} \\n {nlp.Defaults.stop_words}\")\n\n# Compare the differences\nnltk_set = set(stopwords.words('english'))\nspacy_set = set(nlp.Defaults.stop_words)\n\n# Union - all values\nunion = nltk_set.union(spacy_set)\n# Intersection - seen in both sets\ninter = nltk_set.intersection(spacy_set)\nprint(f\"Seen in both : {len(inter)} \\n {inter}\")\n# Remainder - differences between sets\nnltk_extra = nltk_set - inter\nspacy_extra = spacy_set - inter\nprint(f\"Extra NLTK : {len(nltk_extra)} \\n {nltk_extra}\")\nprint(f\"Extra Spacy : {len(spacy_extra)} \\n {spacy_extra}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:45:27.974681Z","iopub.execute_input":"2021-07-13T02:45:27.975021Z","iopub.status.idle":"2021-07-13T02:45:27.991193Z","shell.execute_reply.started":"2021-07-13T02:45:27.974991Z","shell.execute_reply":"2021-07-13T02:45:27.989976Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Review Tfidftransformer and Tfidfvectorizer</span></h1><br>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:45:34.631667Z","iopub.execute_input":"2021-07-13T02:45:34.632013Z","iopub.status.idle":"2021-07-13T02:45:34.637472Z","shell.execute_reply.started":"2021-07-13T02:45:34.631984Z","shell.execute_reply":"2021-07-13T02:45:34.636066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#instantiate CountVectorizer() \ncv=CountVectorizer() \n\n# this steps generates word counts for the words in the sample doc\nword_count_vector=cv.fit_transform(sample.text)\n\nword_count_vector.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:45:45.175792Z","iopub.execute_input":"2021-07-13T02:45:45.176133Z","iopub.status.idle":"2021-07-13T02:45:45.186577Z","shell.execute_reply.started":"2021-07-13T02:45:45.176103Z","shell.execute_reply":"2021-07-13T02:45:45.185287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the IDF values\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \ntfidf_transformer.fit(word_count_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:46:28.639295Z","iopub.execute_input":"2021-07-13T02:46:28.639684Z","iopub.status.idle":"2021-07-13T02:46:28.651627Z","shell.execute_reply.started":"2021-07-13T02:46:28.639653Z","shell.execute_reply":"2021-07-13T02:46:28.650059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print idf values \ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n \n# sort ascending \ndf_idf.sort_values(by=['idf_weights'])\n\ndf_idf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:46:44.461854Z","iopub.execute_input":"2021-07-13T02:46:44.462196Z","iopub.status.idle":"2021-07-13T02:46:44.489654Z","shell.execute_reply.started":"2021-07-13T02:46:44.462168Z","shell.execute_reply":"2021-07-13T02:46:44.488073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Lower the IDF value the more common the value is","metadata":{}},{"cell_type":"code","source":"# Time to compute the TFIDF\n# count matrix \ncount_vector=cv.transform(sample.text) \n \n# tf-idf scores \ntf_idf_vector=tfidf_transformer.transform(count_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:47:07.160088Z","iopub.execute_input":"2021-07-13T02:47:07.160375Z","iopub.status.idle":"2021-07-13T02:47:07.170024Z","shell.execute_reply.started":"2021-07-13T02:47:07.160351Z","shell.execute_reply":"2021-07-13T02:47:07.168669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = cv.get_feature_names() \n \n#get tfidf vector for first document \nfirst_document_vector=tf_idf_vector[0] \n \n#print the scores \ndf = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \ndf.sort_values(by=[\"tfidf\"],ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:47:25.618891Z","iopub.execute_input":"2021-07-13T02:47:25.619144Z","iopub.status.idle":"2021-07-13T02:47:25.637454Z","shell.execute_reply.started":"2021-07-13T02:47:25.619121Z","shell.execute_reply":"2021-07-13T02:47:25.636102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Tfidfvectorizer Usage</span></h1><br>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# settings that you use for count vectorizer will go here \ntfidf_vectorizer=TfidfVectorizer(use_idf=True) \n \n# just send in all your docs here \ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(sample.text)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:47:54.168279Z","iopub.execute_input":"2021-07-13T02:47:54.168666Z","iopub.status.idle":"2021-07-13T02:47:54.178306Z","shell.execute_reply.started":"2021-07-13T02:47:54.168634Z","shell.execute_reply":"2021-07-13T02:47:54.177209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the first vector out (for the first document) \nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n \n# place tf-idf values in a pandas data frame \ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \ndf.sort_values(by=[\"tfidf\"],ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:48:49.389985Z","iopub.execute_input":"2021-07-13T02:48:49.390342Z","iopub.status.idle":"2021-07-13T02:48:49.40626Z","shell.execute_reply.started":"2021-07-13T02:48:49.390312Z","shell.execute_reply":"2021-07-13T02:48:49.405522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here. Here is Sample\nfitted_vectorizer=tfidf_vectorizer.fit(sample.text)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(sample.text)\ndf = pd.DataFrame(tfidf_vectorizer_vectors.T.todense(), index=fitted_vectorizer.get_feature_names(), columns=sample['date'])\ndf.head()\ndf.columns\ndf.shape\ndf_out = df.loc[:, ['Wed Jun 13 06:42:06 +0000 2018']]\ndf_out.sort_values(by=['Wed Jun 13 06:42:06 +0000 2018'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:49:30.218855Z","iopub.execute_input":"2021-07-13T02:49:30.219195Z","iopub.status.idle":"2021-07-13T02:49:30.24151Z","shell.execute_reply.started":"2021-07-13T02:49:30.219164Z","shell.execute_reply":"2021-07-13T02:49:30.240549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test on all training data (=df)\n# just send in all your docs here\nfitted_vectorizer=tfidf_vectorizer.fit(train.text)\ntfidf_vectorizer_vectors=fitted_vectorizer.transform(train.text)\ndf = pd.DataFrame(tfidf_vectorizer_vectors.T.todense(), index=fitted_vectorizer.get_feature_names(), columns=train['date'])\ndf.head()\ndf.columns\ndf.shape\ndf_out = df.loc[:, ['Wed Jun 13 06:42:06 +0000 2018']]\ndf_out.sort_values(by=['Wed Jun 13 06:42:06 +0000 2018'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:51:24.317709Z","iopub.execute_input":"2021-07-13T02:51:24.318148Z","iopub.status.idle":"2021-07-13T02:51:26.82724Z","shell.execute_reply.started":"2021-07-13T02:51:24.31812Z","shell.execute_reply":"2021-07-13T02:51:26.826039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets create a dictionary to review the key phrase outputs\nfrom collections import defaultdict, Counter\n\n# Returns integers that map to parts of speech\ncounts_dict = doc.count_by(spacy.attrs.IDS['POS'])\n\n# Print the human readable part of speech tags\nfor pos, count in counts_dict.items():\n    human_readable_tag = doc.vocab[pos].text\n    print(human_readable_tag, count)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:52:15.482628Z","iopub.execute_input":"2021-07-13T02:52:15.482895Z","iopub.status.idle":"2021-07-13T02:52:15.490765Z","shell.execute_reply.started":"2021-07-13T02:52:15.482872Z","shell.execute_reply":"2021-07-13T02:52:15.489536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_counts = defaultdict(Counter)\nfor token in doc:\n    pos_counts[token.pos][token.orth] += 1\n    \nfor pos_id, counts in sorted(pos_counts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common():\n        print(pos, count, doc.vocab.strings[orth_id], len(doc.vocab.strings[orth_id]))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:52:37.581125Z","iopub.execute_input":"2021-07-13T02:52:37.581395Z","iopub.status.idle":"2021-07-13T02:52:37.591712Z","shell.execute_reply.started":"2021-07-13T02:52:37.581372Z","shell.execute_reply":"2021-07-13T02:52:37.59062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#SPACE value appears to correspond to the new line.","metadata":{}},{"cell_type":"code","source":"# Expanding named entities\nfor entity in doc.ents:\n    print(entity.text, entity.label_)\n\n# Analyze syntax\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\nprint(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\nprint(\"Number of sentences\", len([*doc.sents]))\nprint(\"Sentiment\", doc.sentiment)\n\n# Understand the length of sentences\nfor sent in doc.sents:\n    print(sent.start_char, sent.end_char, (sent.end_char - sent.start_char))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:53:28.685673Z","iopub.execute_input":"2021-07-13T02:53:28.685946Z","iopub.status.idle":"2021-07-13T02:53:28.695597Z","shell.execute_reply.started":"2021-07-13T02:53:28.685922Z","shell.execute_reply":"2021-07-13T02:53:28.694579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy import displacy\n\n# Display the entities within a sentence\ndisplacy.render(doc, style='ent', jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:53:50.829121Z","iopub.execute_input":"2021-07-13T02:53:50.829677Z","iopub.status.idle":"2021-07-13T02:53:50.836297Z","shell.execute_reply.started":"2021-07-13T02:53:50.82965Z","shell.execute_reply":"2021-07-13T02:53:50.835302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise the dependencies within a sentence\ndisplacy.render(doc, style='dep', jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:54:38.371943Z","iopub.execute_input":"2021-07-13T02:54:38.372223Z","iopub.status.idle":"2021-07-13T02:54:38.380673Z","shell.execute_reply.started":"2021-07-13T02:54:38.3722Z","shell.execute_reply":"2021-07-13T02:54:38.379445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Create the datasets for training the models</span></h1><br>","metadata":{}},{"cell_type":"markdown","source":"#That below will take some time.","metadata":{}},{"cell_type":"code","source":"# Lets apply the nlp instance to each text\ntrain['text_scy'] = train['text'].apply(nlp)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T02:55:18.035316Z","iopub.execute_input":"2021-07-13T02:55:18.035705Z","iopub.status.idle":"2021-07-13T02:59:52.902557Z","shell.execute_reply.started":"2021-07-13T02:55:18.035679Z","shell.execute_reply":"2021-07-13T02:59:52.901671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the data type for the updated column\ntype(train.loc[0, 'text_scy'])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:01:21.924106Z","iopub.execute_input":"2021-07-13T03:01:21.924387Z","iopub.status.idle":"2021-07-13T03:01:21.942185Z","shell.execute_reply.started":"2021-07-13T03:01:21.924363Z","shell.execute_reply":"2021-07-13T03:01:21.941346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reviewing a different row\ntrain.loc[1,'text_scy']","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:01:30.091457Z","iopub.execute_input":"2021-07-13T03:01:30.091937Z","iopub.status.idle":"2021-07-13T03:01:30.100233Z","shell.execute_reply.started":"2021-07-13T03:01:30.091906Z","shell.execute_reply":"2021-07-13T03:01:30.09931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the class methods required to run the analysis\nclass NLPMethods():\n    # Create constructor for the class\n#     def __init__():\n    \n    # Number of sentences\n    def number_sentences(self, nlp_text):\n        return len([*nlp_text.sents])\n    \n    # Average length of sentence\n    def average_sentence_length(self, nlp_text):\n        sent_length = list()\n        for sent in nlp_text.sents:\n            sent_length.append(sent.end_char - sent.start_char)\n        return np.mean(sent_length)\n    \n    # Part of speech tags\n    def part_of_speech_tags(self, nlp_text):\n        counts_dict = nlp_text.count_by(spacy.attrs.IDS['POS'])\n        counts_dict1 = {}\n        # Extract the text that matches to the POS value\n        for k, v in counts_dict.items():\n            counts_dict1[nlp_text.vocab[k].text] = v\n        return counts_dict1\n    \n    # Number of spaces\n    def number_spaces(self, nlp_text):\n        dict_pos = self.part_of_speech_tags(nlp_text)\n        if dict_pos.get('SPACE') != None:\n            space = dict_pos.get('SPACE')\n        else:\n            space = 0\n        return space\n    \n    # Part of speech tags - including the word counts\n    def word_counts(self, nlp_text):\n        pos_counts = defaultdict(Counter)\n        for token in nlp_text:\n            pos_counts[token.pos][token.orth] += 1\n        \n        # Create dictionary for the word counts\n        word_counts_dict = {}\n        for pos_id, counts in sorted(pos_counts.items()):\n            pos = nlp_text.vocab.strings[pos_id]\n            for orth_id, count in counts.most_common():\n                word_counts_dict[nlp_text.vocab.strings[orth_id]] = {'count':count, \n                                                                     'length':len(nlp_text.vocab.strings[orth_id]), \n                                                                     'pos':pos}\n        return word_counts_dict\n    \n    # Number of words\n    def number_words(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        return len(dict_word_counts.items())\n    \n    # Longest word\n    def longest_word(self, nlp_text):\n        dict_word_counts = self.word_counts(nlp_text)\n        df = pd.DataFrame(dict_word_counts).T.reset_index().rename(columns={'index':'variable'})\n        return max(df['length'])","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:02:20.766025Z","iopub.execute_input":"2021-07-13T03:02:20.766325Z","iopub.status.idle":"2021-07-13T03:02:20.77732Z","shell.execute_reply.started":"2021-07-13T03:02:20.766291Z","shell.execute_reply":"2021-07-13T03:02:20.776629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Another one snippet that takes time","metadata":{}},{"cell_type":"code","source":"# Add columns for the spacy doc\ntrain['num_sentences'] = train['text_scy'].apply(NLPMethods().number_sentences)\ntrain['avg_sentence_length'] = train['text_scy'].apply(NLPMethods().average_sentence_length)\ntrain['pos_dict'] = train['text_scy'].apply(NLPMethods().part_of_speech_tags)\ntrain['num_space'] = train['text_scy'].apply(NLPMethods().number_spaces)\ntrain['wc_dict'] = train['text_scy'].apply(NLPMethods().word_counts)\ntrain['num_words'] = train['text_scy'].apply(NLPMethods().number_words)\ntrain['longest_word'] = train['text_scy'].apply(NLPMethods().longest_word)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:03:44.676939Z","iopub.execute_input":"2021-07-13T03:03:44.677224Z","iopub.status.idle":"2021-07-13T03:05:29.241601Z","shell.execute_reply.started":"2021-07-13T03:03:44.6772Z","shell.execute_reply":"2021-07-13T03:05:29.240446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:05:42.365582Z","iopub.execute_input":"2021-07-13T03:05:42.366104Z","iopub.status.idle":"2021-07-13T03:05:42.444855Z","shell.execute_reply.started":"2021-07-13T03:05:42.366047Z","shell.execute_reply":"2021-07-13T03:05:42.443894Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review the max value target variable. We don't target to return a single line max val\n#max_val = np.max(train['polarity'])\n#train_max = train.loc[(train['polarity']==max_val), :]\n#train_max","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:09:45.751697Z","iopub.execute_input":"2021-07-13T03:09:45.752056Z","iopub.status.idle":"2021-07-13T03:09:45.757132Z","shell.execute_reply.started":"2021-07-13T03:09:45.752026Z","shell.execute_reply":"2021-07-13T03:09:45.755453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the reason for the largest target value\n#We don't have largest target. Hence, I kept the same value 2829 \ntype(train.loc[2829, 'text_scy'])\ntrain.loc[2829, 'text_scy']\ntrain.loc[2829, 'text']","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:10:00.705936Z","iopub.execute_input":"2021-07-13T03:10:00.706453Z","iopub.status.idle":"2021-07-13T03:10:00.712671Z","shell.execute_reply.started":"2021-07-13T03:10:00.706423Z","shell.execute_reply":"2021-07-13T03:10:00.712066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I commented since we don't have target or min value target.\n# Review the min value target variable\n#min_val = np.min(train['target'])\n#train_min = train.loc[(train['target']==min_val), :]\n#train_min","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Though we don't have smallest target I kept the same value\n# Check the reason for the smallest target value\ntype(train.loc[1705, 'text_scy'])\ntrain.loc[1705, 'text_scy']","metadata":{"execution":{"iopub.status.busy":"2021-07-13T03:14:19.864418Z","iopub.execute_input":"2021-07-13T03:14:19.864887Z","iopub.status.idle":"2021-07-13T03:14:19.872607Z","shell.execute_reply.started":"2021-07-13T03:14:19.864857Z","shell.execute_reply":"2021-07-13T03:14:19.871583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Thanks James McNeill for the code https://www.kaggle.com/datajmcn/nlp-read-eda/notebook","metadata":{"execution":{"iopub.status.busy":"2021-07-13T01:37:12.429322Z","iopub.execute_input":"2021-07-13T01:37:12.429739Z","iopub.status.idle":"2021-07-13T01:37:12.437959Z","shell.execute_reply.started":"2021-07-13T01:37:12.429707Z","shell.execute_reply":"2021-07-13T01:37:12.43694Z"}}}]}