{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Hello There**\n\nIn this notebook I am going to create a linear SVM model to predict the authenticity of a news. If you found any mistake feel free to drop a comment. Cheers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we create the fake and true dataset, we need to add a \"status\" column to both of the datasets. Where the value will be 1 if it belongs to the fake class and 0 otherwise\n\nThen we combine both as one dataframe, called it df"},{"metadata":{"trusted":true},"cell_type":"code","source":"true = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\ntrue['status'] = 0\nfake['status'] = 1\n\ndf = pd.concat([true, fake])\ndf = df.sample(frac = 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice at the end of the line, i do a sampling by fraction of 0.5. This is due to the ram capacity. When i tried this locally on my computer just use the original dataframe is fine, however here i need to scale it down by half so that the ram can finish the computation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we are going to convert all capitals in both title and text columns into lower case characters\nAnd drop the subject and date, as we are going to make use only the words fron the text and title"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.title = df.title.str.lower()\ndf.text = df.text.str.lower()\ndf = df.drop(columns = ['subject','date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I merge together the title and text and add a spcae between these two"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.title + ' ' + df.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns = ['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I created two dataframe, one for training and one for testing with test size 0.3"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df, test_size = 0.3, random_state = 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to extract the words from all the news, I am going to make use of count vectorizer function"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(stop_words = 'english')\nfitting = list(train.text)\ncv.fit(fitting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = cv.transform(fitting).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_vocab = {v: k for k, v in cv.vocabulary_.items()}\nvocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = pd.DataFrame(features, columns = vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the least 80000 used words to be removed from the training dataset\nto_remove = list(new_train.sum(axis = 0).sort_values()[:65000].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the words appear less than 5 times in the whole news and hence it is un necessary for the training model, thus I decided to remove the least 65000 keywords appeared in the whole news. This will also fasten the training time"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = new_train.drop(columns = to_remove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVC()\nsvc.fit(new_train, train.status)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the test dataframe, we need to create the exact features as the train dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features = cv.transform(list(test.text)).toarray()\nnew_test = pd.DataFrame(test_features, columns = vocabulary)\nnew_test = new_test.drop(columns = to_remove)\nnew_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = svc.predict(new_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And Voila! We achieved 99.4% of accuracy in the test set. This is because SVM model is a very good model when we are working with data that has a lot of features such as in this case"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(ans, test.status)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}