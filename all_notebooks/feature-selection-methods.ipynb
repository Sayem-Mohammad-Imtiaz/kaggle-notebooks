{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature selection\nPer feature selection si intende un processo attraverso il quale restringere la dimensionalità degli elementi su insiemi di dati. Grazie ad esso è possibile ridurre l'overfitting, permettendo di prendere decisioni con meno \"rumore\", migliorare l'accuratezza e ridurre i tempi di training dell'algoritmo.  \n\nIn questo notebook, relativo al bike sharing nella città di Londra, analizzo i metodi di feature selection di sklearn. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.linear_model import LassoCV, Lasso\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndataset = pd.read_csv('/kaggle/input/london-bike-sharing-dataset/london_merged.csv').drop(['timestamp', 't1', 't2'], axis=1) # Dropping columns with negative values\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Unvariate selection (metodo Filter)**\n\nIl primo metodo di feature selection è chiamato *unvariate selection*. Queste metodo utilizza test statistici per selezionare le caratteristiche che hanno relazioni più forti con la variabile di uscita.\nLa libreria scikit-learn fornisce la classe SelectKBest, che può essere utilizzata con una suite di test statistici diversi per selezionare un numero specifico di caratteristiche. In questo esempio utilizzo il test *Chi squared (Chi^2)* ."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset_values = dataset.values # Extract values from our dataset.\n\nX = dataset_values[:,0:7] # Input\nY = dataset_values[:,6] # Target\n\ntest = SelectKBest(score_func=chi2, k=3) # Extract features, setting k value equal to 3. \nfit = test.fit(X, Y) \nprint(fit.scores_) # Scores for each feature\nfeatures = fit.transform(X) # Apply the transformation\nprint(features[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come possiamo vedere, l'*unvariate selection* con il test *Chi squared* estrae le tre migliori features del dataset (il parametro k è infatti impostato a 3), poiché presentano gli *scores* più alti."},{"metadata":{},"cell_type":"markdown","source":"**Recursive Feature Elimination - RFE (metodo Wrapper)**\n\nCome è possibile intuire dall'acronimo, l'RFE consiste nel selezionare i dati considerando in modo ricorsivo insiemi sempre più piccoli di informazioni. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3) # Select 3 features\nfit = rfe.fit(X, Y)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come avviene la selezione di una feature?\n\nIn primo luogo, il predittore è allenato sull'insieme iniziale di elementi e l'importanza di ogni elemento è ottenuta sia attraverso un attributo *coef_* o attraverso un attributo *feature_importances_*. Quindi, le informazioni meno importanti vengono rimosse dall'insieme corrente, procedura che viene ripetuta ricorsivamente sull'insieme di elementi da selezionare fino a raggiungere il numero desiderato."},{"metadata":{},"cell_type":"markdown","source":"**Selection from model (metodo Embedded)**\n\nI metodi Embedded sono metodi iterativi. Attraverso ogni iterazione del processo di formazione del modello, essi estraggono con attenzione quelle informazioni che contribuiscono maggiormente alla formazione per una particolare iterazione. I metodi di regolarizzazione sono i metodi embedded più comunemente usati che valutano un'informazione dato un coefficiente *treshold*.\n\nIn questo esempio viene utilizzata la regolarizzazione LASSO. Se l'informazione è irrilevante, il LASSO penalizza il suo coefficiente e lo rende 0. Tutto ciò con coefficiente = 0 viene rimosso mentre il resto viene preso. "},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LassoCV()\nreg.fit(X, Y)\nprint(\"Best alpha: %f\" % reg.alpha_)\nprint(\"Best score: %f\" %reg.score(X,Y))\ncoef = pd.Series(reg.coef_, index = list(dataset.columns.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_coef = coef.sort_values()\nmatplotlib.rcParams['figure.figsize'] = (15.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusioni\nCome possiamo vedere, sono disponibili diversi metodi di feature selection, ognuno dei quali adatto per una specifica situazione. I metodi Filter utilizzano una *scoring function* per determinare l'utilità di un dato ma generalmente sono più imprecisi rispetto ai metodi Wrapper e Embedded. Questi ultimi, tuttavia, richiedono un tempo di esecuzione più lungo. Sono, quindi, più adatti quando il numero di features è ragionevolmente contenuto.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}