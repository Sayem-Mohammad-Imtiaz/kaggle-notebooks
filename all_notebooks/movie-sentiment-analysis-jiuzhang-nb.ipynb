{"cells":[{"metadata":{"_uuid":"cd044aaad17e6e7e064a73ac0104a2eff0f9ab6d"},"cell_type":"markdown","source":"# Movie Sentiment Analysis\nhttps://www.kaggle.com/c/word2vec-nlp-tutorial/"},{"metadata":{"_uuid":"a580102b324c7a02adf7a2f506f0b16296b4100a"},"cell_type":"markdown","source":" 拿到数据首先读入拿到数据"},{"metadata":{"trusted":true,"_uuid":"fe37903f3d3876260b9cf35aa4a6498976070ab8"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk \n#nltk.download('wordnet')  -only need to do once\n#nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a557045341211cf7b285e1dd9685db8072514a03"},"cell_type":"code","source":"dir=\"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/\"\ntrainData=pd.read_csv(dir+\"labeledTrainData.tsv\", delimiter='\\t')\ntestData=pd.read_csv(dir+\"testData.tsv\", delimiter='\\t')\ntrainData.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba5a34bf540fe0e1e07f6b8167555014dde367e1"},"cell_type":"markdown","source":"分离标签信息"},{"metadata":{"trusted":true,"_uuid":"128d99f85ee21c7103c7e228e2d4d61e8baed4eb"},"cell_type":"code","source":"trainY=trainData['sentiment'].values   \ntrainX=trainData['review'].values     \ntestX=testData['review'].values\nprint(trainY.shape, trainX.shape, testX.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"515a18d9089d3851d2c46bd6d4071877de8330e3"},"cell_type":"markdown","source":"# data Preprocessing"},{"metadata":{"_uuid":"129f1d350420ced0f622d0fd47feab2ebaa06e63"},"cell_type":"markdown","source":"Remove punctuation, and non-words"},{"metadata":{"trusted":true,"_uuid":"9a0d57fa7a480970eaa54a5867533e74d3264fdd"},"cell_type":"code","source":"from tqdm import tqdm\n\nimport re\n\n#remove html <labels>, change n't to not, and remove 's, punctuation and nonwords\ndef clean_data(data_in):\n    data=data_in.copy()\n    for idx, line in tqdm(enumerate(data)):\n        a=re.sub(r\"n\\'t\",\" not\", line)\n        data[idx]=re.sub(r\"<.*?/>|\\'s|[^A-Za-z]\",\" \",a).lower()\n    return data\ntrainX_after_clean=clean_data(trainX)\ntestX_after_clean=clean_data(testX)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b218fecaba0b9d0bc5729864b77d4ef78290cf89"},"cell_type":"markdown","source":"Tokenize, Lemmatize and Class Mapping"},{"metadata":{"trusted":true,"_uuid":"e21ad3a8f975c1db51573f3052c1ee25f55435b3"},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\n\n#tokenize\ndef tokenize(data_in):\n    res=[]\n    for line in tqdm(data_in):\n        line_tok=line.split()\n        res.append(line_tok)\n    return res\n\n#lematize and remove stop word\ndef lemmatize_remove_stop(data_in):\n    res=[]\n    lemmatizer=WordNetLemmatizer()\n    stop_words=set(stopwords.words('english'))\n\n    for data in tqdm(data_in): \n#        res.append([lemmatizer.lemmatize(i,pos='v') for i in data if not i in stop_words and len(i)>=2])\n         res.append([i for i in data if not i in stop_words and len(i)>=2])\n    return res\n\n#load human names corpus on Kaggle\n#https://www.kaggle.com/nltkdata/names\nname_copus1=pd.read_csv('../input/names/names/female.txt', header=None)\nname_copus2=pd.read_csv('../input/names/names/male.txt',header=None)\nhuman_names=set((name_copus1.append(name_copus2))[0].str.lower().values.ravel())\n\ndef class_mapping(data_in, name_copus):\n    res=[]\n    for data in tqdm(data_in):\n        a=[(lambda x:(\"_humanname\" if x in name_copus else x))(i) for i in data]\n        res.append(a)\n    return res\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e739c281ab1e3add36052a0bca42bd9a06873bb5"},"cell_type":"code","source":"trainX_after_tok=tokenize(trainX_after_clean)\ntestX_after_tok=tokenize(testX_after_clean)\ntrainX_after_lem=lemmatize_remove_stop(trainX_after_tok)\ntestX_after_lem=lemmatize_remove_stop(testX_after_tok)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"535ad94e6516ae66c0b68abfd1d89ed0009711fa"},"cell_type":"code","source":"trainX_after_map=class_mapping(trainX_after_lem, human_names)\ntestX_after_map=class_mapping(testX_after_lem, human_names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37ec6eeda9b4680c209e3c0616230041cdd4d8a3"},"cell_type":"markdown","source":"add 3-gram"},{"metadata":{"trusted":false,"_uuid":"67de5a539e87a21a6f5099fa725241e01252c6c1"},"cell_type":"code","source":"import copy\ndef add_ngram(data_in):\n    data_copy=copy.deepcopy(data_in);\n    for line in tqdm(data_copy):\n        n=len(line)\n        line.extend([\"\".join(line[i:i+2]) for i in range(n-1)])\n        line.extend([\"\".join(line[i:i+3]) for i in range(n-2)])\n    return data_copy\n\ntrainX_after_ngram=add_ngram(trainX_after_map)\ntestX_after_ngram=add_ngram(testX_after_map)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bd8f3549d00880020b98298e9f236ab65103a2a"},"cell_type":"markdown","source":"把文本转换成 Vector"},{"metadata":{"trusted":false,"_uuid":"1dfb12bcd6e6b3600050ab387082e1237e8eea0d"},"cell_type":"code","source":"def gen_vocabulary(data_in):\n    #data_in is a list of lists\n    dictionary={}\n    word_id=0\n    for line in tqdm(data_in):\n        for word in line:\n            if word not in dictionary:\n                dictionary[word]=word_id\n                word_id+=1\n    return dictionary\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fa847f1bc4d1be6442dfc4a8fca2bd3de4827da3"},"cell_type":"code","source":"from scipy.sparse import csr_matrix\ndef td_idf_vectorize(data_in, dictionary):\n    m=len(dictionary)\n    n=len(data_in)\n    col_idx=[]\n    indptr=[0]\n    data=[]\n    word_doc_count=np.array([0]*(m+1))\n    \n    #generate idf matirx\n    for doc in tqdm(data_in):\n        term_set=set([])\n        for term in doc:\n            term_set.add(term)\n        for term in term_set:\n            word_doc_count[dictionary.get(term,m)]+=1\n    idf=np.log((n+1)/(word_doc_count+1))+1  #+1 for smoothing\n    \n    #generate td matrix\n    for doc in tqdm(data_in):\n        word_count=len(doc)\n        for term in doc:\n            term_idx=dictionary.get(term,m)\n            col_idx.append(term_idx)\n            data.append(1/word_count*idf[term_idx])\n        \n        indptr.append(len(col_idx))\n        \n    td_matrix=csr_matrix((data,col_idx,indptr),shape=[n,m+1])    \n    return td_matrix\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"956c0781a36cf5e495e9a3b68afe5533e62b6273"},"cell_type":"code","source":"dict_gen=gen_vocabulary(trainX_after_ngram)      \nprint(len(dict_gen))\ntrainX_vector=td_idf_vectorize(trainX_after_ngram,dict_gen)\ntestX_vector=td_idf_vectorize(testX_after_ngram,dict_gen)\nprint(trainX_vector.shape, testX_vector.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b4f12402715a5de1f47e5cbd3bc9bb48e918818"},"cell_type":"markdown","source":"# 建立模型"},{"metadata":{"trusted":false,"_uuid":"6599195006f31b81c47af6df3e259289ad64d934"},"cell_type":"code","source":"class NBClassifier():\n    \"\"\"\n    class variables:\n        class_labels\n        prior_matrix : num_classes x 1\n        likely_hood_matrix : num_classes x feature_dim\n    \"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, Xtrain, y):\n        num_data,num_features=Xtrain.shape\n        \n        class_labels, count=np.unique(y,return_counts=True)\n        self.class_labels=class_labels\n        num_classes=class_labels.shape[0]\n        \n        #calculate priorMatrix\n        self.log_prior=np.log(count/sum(count))\n        \n        #calculate likelyhood\n        self.log_likely_hood=np.ones((num_classes,num_features))  # includes 1 for smoothing\n        for idx, class_label in enumerate(class_labels):\n            row_idx=np.nonzero(y==class_label)[0]\n            self.log_likely_hood[idx]+=np.asarray(Xtrain[row_idx,:].sum(axis=0)).ravel()\n            \n        self.log_likely_hood=np.log(self.log_likely_hood/(self.log_likely_hood.sum(axis=1).reshape(2,1)-1))  \n         \n    def predict(self,Xtest):\n        numData=Xtest.shape[0]\n        num_classes=self.class_labels.shape[0]\n        posterior=np.zeros((numData,num_classes))\n        for idx, xtest in tqdm(enumerate(Xtest)):\n            for idx2 in range(self.log_likely_hood.shape[0]):\n                posterior[idx,idx2]=np.sum(xtest*self.log_likely_hood[idx2,:])+self.log_prior[idx2]\n\n                              \n        ytest=np.array([self.class_labels[i] for i in np.argmax(posterior, axis=1)])\n        return ytest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e513fd3b8eb814ce105c7e884111ff7a07084820"},"cell_type":"code","source":"#run model check\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score,confusion_matrix\n\ntrainX_split, valiX_split, trainY_split, valiY_split=train_test_split(trainX_vector, trainY, test_size=0.3, random_state=1)\n\nnbc=NBClassifier()\nnbc.fit(trainX_split,trainY_split)\nypred=nbc.predict(valiX_split)\n\nprint(accuracy_score(ypred,valiY_split))\nprint(f1_score(ypred,valiY_split))\nprint(confusion_matrix(ypred,valiY_split))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e698fc35072ad41114c02f2980bd5adf44be431"},"cell_type":"markdown","source":"# 在测试集上做预测并写出答案"},{"metadata":{"trusted":false,"_uuid":"ba6ff0d40cb3754d3d971861a5eb257c0b543de1"},"cell_type":"code","source":"#train the model \nnbc2=NBClassifier()\nnbc2.fit(trainX_vector,trainY)\nytestpred=nbc2.predict(testX_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f133c4fdd8e779325ecd7803200a390c0805d096"},"cell_type":"code","source":"df=pd.DataFrame({'id':testData['id'],'sentiment':ytestpred})\ndf.to_csv('submission.csv',index=False,header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"84e36c80a08bec4dba6e1c5d4b1b844e79eabf0e"},"cell_type":"code","source":"#base line: test score = 0.81020, \n#using TF-IDF, lemmatizaing and stop word removal, test score=0.83812\n#adding n-gram (1-3): test score=0.85168\n#adding class mapping: test score=0.868\n#remove lemmatization: test score=0.86992","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}