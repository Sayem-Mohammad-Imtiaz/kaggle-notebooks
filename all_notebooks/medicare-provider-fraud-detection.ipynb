{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings     # for supressing a warning when importing large files\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix,roc_curve,accuracy_score,roc_auc_score,classification_report\nimport pickle\nfrom scipy import stats\nimport time\nfrom sklearn.model_selection import GridSearchCV,KFold\n\nfrom pylab import rcParams\n\n%matplotlib inline\nsns.set(style='whitegrid', palette='muted', font_scale=1.5)\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Train Dataset\n\nTrain=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Train-1542865627584.csv\")\nTrain_Beneficiarydata=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Train_Beneficiarydata-1542865627584.csv\")\nTrain_Inpatientdata=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Train_Inpatientdata-1542865627584.csv\")\nTrain_Outpatientdata=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Train_Outpatientdata-1542865627584.csv\")\n\n# Load Test Dataset\n\nTest=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Test-1542969243754.csv\")\nTest_Beneficiarydata=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Test_Beneficiarydata-1542969243754.csv\")\nTest_Inpatientdata=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Test_Inpatientdata-1542969243754.csv\")\nTest_Outpatientdata=pd.read_csv(\"/kaggle/input/healthcare-provider-fraud-detection-analysis/Test_Outpatientdata-1542969243754.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets Check Shape of datasets \n\nprint('Shape of Train data :',Train.shape)\nprint('Shape of Train_Beneficiarydata data :',Train_Beneficiarydata.shape)\nprint('Shape of Train_Inpatientdata data :',Train_Inpatientdata.shape)\nprint('Shape of Train_Outpatientdata data :',Train_Outpatientdata.shape)\n\nprint('Shape of Test data :',Test.shape)\nprint('Shape of Test_Beneficiarydata data :',Test_Beneficiarydata.shape)\nprint('Shape of Test_Inpatientdata data :',Test_Inpatientdata.shape)\nprint('Shape of Test_Outpatientdata data :',Test_Outpatientdata.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train and Test Dataset understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1m'\"Train Dataset\"+ \"\\033[0m\",\"\\n\",Train.head(4),'\\n')\n\nprint('\\033[1m'+\"Test Dataset\"+ \"\\033[0m\")\n\nprint(Test.head(4)) # We don't have Target Variable Fraud in the test dataset and this target variable we need to predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To Check the summary of the train dataset\n\nTrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets check whether  providers details are unique or not in train data\nprint(Train.Provider.value_counts(sort=True,ascending=False).head(5))  # number of unique providers in train data.Check for duplicates\n\nprint('\\n Total missing values in Train :',Train.isna().sum().sum())\n\nprint('\\n Total missing values in Train :',Test.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing on Beneficiary Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1m'+\"Train Dataset\"+ \"\\033[0m\")\n\ndisplay(Train_Beneficiarydata.head(5))\n\nprint('\\033[1m'+\"Test Dataset\"+ \"\\033[0m\")\n\ndisplay(Test_Beneficiarydata.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets Check missing values in each column in beneficiary data :\n\n\nprint('\\033[1m'+\"Train Beneficiary Dataset\"+ \"\\033[0m\")\n\nprint(Train_Beneficiarydata.isna().sum())\n\nprint('\\033[1m'+\"Test Beneficiary Dataset\"+ \"\\033[0m\")\n\nprint(Train_Beneficiarydata.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check data types of each column in beneficiary data\n\nTrain_Beneficiarydata.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Beneficiarydata.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Replacing 2 with 0 for chronic conditions ,that means chronic condition No is 0 and yes is 1\n\nTrain_Beneficiarydata = Train_Beneficiarydata.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n                           'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n                           'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2, \n                           'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2 }, 0)\n\nTrain_Beneficiarydata = Train_Beneficiarydata.replace({'RenalDiseaseIndicator': 'Y'}, 1)\n\n\n## Same thing do in the Test Dataset also \nTest_Beneficiarydata = Test_Beneficiarydata.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n                           'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n                           'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2, \n                           'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2 }, 0)\n\nTest_Beneficiarydata = Test_Beneficiarydata.replace({'RenalDiseaseIndicator': 'Y'}, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Engineering on Beneficiary Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets Create Age column to the Train and Test dataset\n\nTrain_Beneficiarydata['DOB'] = pd.to_datetime(Train_Beneficiarydata['DOB'] )\nTrain_Beneficiarydata['DOD'] = pd.to_datetime(Train_Beneficiarydata['DOD'],errors='ignore')\nTrain_Beneficiarydata['Age'] = round(((Train_Beneficiarydata['DOD'] - Train_Beneficiarydata['DOB']).dt.days)/365)\n\n\nTest_Beneficiarydata['DOB'] = pd.to_datetime(Test_Beneficiarydata['DOB'])\nTest_Beneficiarydata['DOD'] = pd.to_datetime(Test_Beneficiarydata['DOD'],errors='ignore')\nTest_Beneficiarydata['Age'] = round(((Test_Beneficiarydata['DOD'] - Test_Beneficiarydata['DOB']).dt.days)/365)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Beneficiarydata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## As we can see above Age column have some Nan values, This is due to DOD is Nan for that record.\n## As we see that last DOD value is 2017-12-01 ,which means Beneficiary Details data is of year 2017.\n## so we will calculate age of other benficiaries for year 2017.\n\nTrain_Beneficiarydata.Age.fillna(round(((pd.to_datetime('2017-12-01' ) - Train_Beneficiarydata['DOB']).dt.days)/365),\n                                 inplace=True)\n\n\nTest_Beneficiarydata.Age.fillna(round(((pd.to_datetime('2017-12-01') - Test_Beneficiarydata['DOB']).dt.days)/365),\n                                 inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Beneficiarydata.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Add Flag column 'WhetherDead' using DOD values to tell whether beneficiary is dead on not"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets create a new variable 'WhetherDead' with flag 1 means Dead and 0 means not Dead\n\nTrain_Beneficiarydata.loc[Train_Beneficiarydata.DOD.isna(),'WhetherDead']=0\nTrain_Beneficiarydata.loc[Train_Beneficiarydata.DOD.notna(),'WhetherDead']=1\n\n\n\nTest_Beneficiarydata.loc[Test_Beneficiarydata.DOD.isna(),'WhetherDead']=0\nTest_Beneficiarydata.loc[Test_Beneficiarydata.DOD.notna(),'WhetherDead']=1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1m'+\"Train Dataset\"+ \"\\033[0m\")\n\nprint(Train_Beneficiarydata.loc[:,'WhetherDead'].head(7))\n\nprint('\\033[1m'+\"Test Dataset\"+ \"\\033[0m\")\n\nprint(Train_Beneficiarydata.loc[:,'WhetherDead'].head(7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing on Inpatient Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of Inpatient Dataset\n\nprint('\\033[1m'+\"Train Inpatient Dataset\"+ \"\\033[0m\")\n\ndisplay(Train_Inpatientdata.head(5))\n\nprint('\\033[1m'+\"Test Inpatient Dataset\"+ \"\\033[0m\")\n\ndisplay(Train_Inpatientdata.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check missing values in each column in inpatient data\n\nprint('\\033[1m'+\"Train Inpatient Dataset\"+ \"\\033[0m\")\n\nprint(Train_Inpatientdata.isna().sum())\n\nprint('\\033[1m'+\"Test Inpatient Dataset\"+ \"\\033[0m\")\n\nprint(Test_Inpatientdata.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Engineering on Inpatient Dataset"},{"metadata":{},"cell_type":"markdown","source":"Create new column 'AdmitForDays' indicating number of days patient was admitted in hospital"},{"metadata":{"trusted":true},"cell_type":"code","source":"## As patient can be admitted for only for 1 day,we will add 1 to the difference of Discharge Date and Admission Date \n\nTrain_Inpatientdata['AdmissionDt'] = pd.to_datetime(Train_Inpatientdata['AdmissionDt'])\nTrain_Inpatientdata['DischargeDt'] = pd.to_datetime(Train_Inpatientdata['DischargeDt'])\nTrain_Inpatientdata['AdmitForDays'] = ((Train_Inpatientdata['DischargeDt'] - Train_Inpatientdata['AdmissionDt']).dt.days.abs())+1\n\n\nTest_Inpatientdata['AdmissionDt'] = pd.to_datetime(Test_Inpatientdata['AdmissionDt'])\nTest_Inpatientdata['DischargeDt'] = pd.to_datetime(Test_Inpatientdata['DischargeDt'])\nTest_Inpatientdata['AdmitForDays'] = ((Test_Inpatientdata['DischargeDt'] - Test_Inpatientdata['AdmissionDt']).dt.days.abs())+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Inpatientdata.loc[:,['AdmissionDt','DischargeDt','AdmitForDays']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets check Min and Max values of AdmitforDays column in Train and Test.\nprint('Min AdmitForDays Train:- ',Train_Inpatientdata.AdmitForDays.min())\nprint('Max AdmitForDays Train:- ',Train_Inpatientdata.AdmitForDays.max())\nprint(Train_Inpatientdata.AdmitForDays.isnull().sum() )  #Check Null values.\n\nprint('Min AdmitForDays Test:- ',Test_Inpatientdata.AdmitForDays.min())\nprint('Max AdmitForDays Test:- ',Test_Inpatientdata.AdmitForDays.max())\nprint(Test_Inpatientdata.AdmitForDays.isnull().sum())   #Check Null values.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing on Outpatient Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of Outpatient Dataset\n\nprint('\\033[1m'+\"Train Outpatient Dataset\"+ \"\\033[0m\")\n\ndisplay(Train_Outpatientdata.head(5))\n\nprint('\\033[1m'+\"Test Outpatient Dataset\"+ \"\\033[0m\")\n\ndisplay(Train_Outpatientdata.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check the null values in each column of Outpatient Dataset\n\nprint('\\033[1m'+\"Train Outpatient Dataset\"+ \"\\033[0m\")\n\nprint(Train_Outpatientdata.isna().sum())\n\nprint('\\033[1m'+\"Test Outpatient Dataset\"+ \"\\033[0m\")\n\nprint(Test_Outpatientdata.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets Check Shape of datasets after adding new variables\n\nprint('Shape of Train data :',Train.shape)\nprint('Shape of Train_Beneficiarydata data :',Train_Beneficiarydata.shape)\nprint('Shape of Train_Inpatientdata data :',Train_Inpatientdata.shape)\nprint('Shape of Train_Outpatientdata data :',Train_Outpatientdata.shape)\n\nprint('Shape of Test data :',Test.shape)\nprint('Shape of Test_Beneficiarydata data :',Test_Beneficiarydata.shape)\nprint('Shape of Test_Inpatientdata data :',Test_Inpatientdata.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge Beneficiary, Inpatient and Outpatient Dataset into a single dataset \n"},{"metadata":{},"cell_type":"markdown","source":"#### Merging of Train Datasets "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTrain_patient_merge_id = [i for i in Train_Outpatientdata.columns if i in Train_Inpatientdata.columns]\n\n# Merge Inpatient, Outpatient and beneficiary dataframe into a single patient dataset\nTrain_Patient_data = pd.merge(Train_Inpatientdata, Train_Outpatientdata,\n                    left_on = Train_patient_merge_id,\n                    right_on = Train_patient_merge_id,\n                    how = 'outer').\\\n          merge(Train_Beneficiarydata,left_on='BeneID',right_on='BeneID',how='inner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Merging of Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_patient_merge_id = [i for i in Test_Outpatientdata.columns if i in Test_Inpatientdata.columns]\n\n# Merge Inpatient, Outpatient and beneficiary dataframe into a single patient dataset\nTest_Patient_data = pd.merge(Test_Inpatientdata, Test_Outpatientdata,\n                    left_on = Test_patient_merge_id,\n                    right_on = Test_patient_merge_id,\n                    how = 'outer').\\\n          merge(Test_Beneficiarydata,left_on='BeneID',right_on='BeneID',how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of Merging Dataset \n\nprint(\"Train Dataset Shape after merge:\",Train_Patient_data.shape)\n\nprint(\"Test Dataset Shape after merge:\",Test_Patient_data.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis on Train_Patient_data dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"# To check the number of missing values in the Train_Pateint_data\n\nTrain_Patient_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### There are missing values in AttendingPhysician, OperatingPhysician and OtherPhysician columns, so we need to handle these varaibles \n\nTrain_Patient_data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data[['AttendingPhysician','OperatingPhysician', 'OtherPhysician']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We are replacing these columns value with 0 and 1 where we have value we are replacing it with 1 and in place of null value we replace it with 0.\n\n\nTrain_Patient_data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']] = np.where(Train_Patient_data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']].isnull(), 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Add a new variable in which it tells us how many total types of physicians used for the particular claim or patient.\n\n\nTrain_Patient_data['N_Types_Physicians'] = Train_Patient_data['AttendingPhysician'] +  Train_Patient_data['OperatingPhysician'] + Train_Patient_data['OtherPhysician']\n\nTrain_Patient_data['N_Types_Physicians']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data.isnull().sum() #We can see here new variable \"N_Type_Physicians\" is added","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Handling Missing values on\"DiagnosisGroupCode\"\n\nTrain_Patient_data['DiagnosisGroupCode'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we are finding out each DignosisGroupCode Count\n\nCount_DiagnosisGroupCode=Train_Patient_data['DiagnosisGroupCode'].value_counts()\nCount_DiagnosisGroupCode=Count_DiagnosisGroupCode[:20] # To show only top 20 codes \nCount_DiagnosisGroupCode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Visualization of top 20 DignosisGroupCode\n\nfig=plt.figure(figsize=(20,8))\nsns.barplot(Count_DiagnosisGroupCode.index,Count_DiagnosisGroupCode.values)\nfig.tight_layout()\n\n## From here we can see that DignosisGroupCode 882 has maximum count that is 179 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Since in this columns we have maximum values as null, so we are handling this by creating a new column\n## so we are creating a new variable/column \"IsDiagnosisCode\" in which value will either \"1\" or \"0\" \n## if in a claim there is a groupDiagnosiscode  has null value then in \"IsDiagnosisCode\" column value is 0 otherwise 1\n\nTrain_Patient_data['IsDiagnosisCode'] = np.where(Train_Patient_data.DiagnosisGroupCode.notnull(), 1, 0)\nTrain_Patient_data = Train_Patient_data.drop(['DiagnosisGroupCode'], axis = 1) # We are droping the column \"DiagnosisGroupCode\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data['IsDiagnosisCode']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Handling missing values for \"DeductibleamtPaid\" column\n\nTrain_Patient_data['DeductibleAmtPaid'].isnull().sum()  #Check number of missing values in this variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describing this column by omiting the Nan, to check mean , variance , skewness etc\n\nsc.stats.describe(Train_Patient_data['DeductibleAmtPaid'],nan_policy='omit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Count Plot of \"DeductibleAmtPaid\" maximum values are 0 in this \n\nfig=plt.figure(figsize=(15,10))\nsns.countplot(Train_Patient_data['DeductibleAmtPaid'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Box plot of this \"DeductibleAmtPaid\", maximum values are 0 that shows here.\n\nfig=plt.figure(figsize=(8,6))\nsns.boxplot(Train_Patient_data['DeductibleAmtPaid'])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## So from the above analysis we can reach to the conclusion that we replace missing values with 0 \n\nTrain_Patient_data['DeductibleAmtPaid'].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### We are also creating one new variable \"IsDeductibleAmtPaid\" which tells us that particular claim has any DeductibleAmtPaid or not\n\nTrain_Patient_data['IsDeductibleAmtPaid']=np.where(Train_Patient_data['DeductibleAmtPaid']==0,0,1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So from this plot we can say that maximum claims doesn't have any \"DeductibleAmtPaid\"\n\nfig=plt.figure(figsize=(8,6))\nsns.countplot(Train_Patient_data['IsDeductibleAmtPaid'])\n\nprint(Train_Patient_data['IsDeductibleAmtPaid'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Handling missing values for \"AdmitForDays\" column\n\nTrain_Patient_data['AdmitForDays'].isnull().sum() # Count of missing values in this column\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace all value with 0 as these all are the patients that didn't admit in the hospital\n\nTrain_Patient_data['AdmitForDays'].fillna(0,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data['AdmitForDays'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In this dataset now we have some Date columns in which missing values are there, which we do not need to handle and we can drop those columns also. \n\nTrain_Patient_data.isnull().sum() \n\n\n# Now we need to handle missing values of ClmDiagnosisCodes and ClmProcedureCode columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## First we handle ClmProcedureCodes variables \n\nClmProcedure_vars = ['ClmProcedureCode_{}'.format(x) for x in range(1,7)]\nClmProcedure_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data[ClmProcedure_vars]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To Check how many null values are in each Clmprocedurecodes\n## By this we find out that in code_6 column all are Nan values \n\nTrain_Patient_data[ClmProcedure_vars].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data[ClmProcedure_vars].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function helps us find the length of unique values in each row/record\ndef N_unique_values(df):\n    return np.array([len(set([i for i in x[~pd.isnull(x)]])) for x in df.values])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We count the number of procedureCode for each claim and store these value in a new variable\nTrain_Patient_data['N_Procedure'] = N_unique_values(Train_Patient_data[ClmProcedure_vars])\n\n## So from here we get to know that 534901 claims/records has 0 claim procedure codes, 17820 claims/records has 1 claimprocedurecodes and so on\n\nTrain_Patient_data['N_Procedure'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Handling of 'ClmDiagnosisCode'\n\n# We count the number of claims\nClmDiagnosisCode_vars =['ClmAdmitDiagnosisCode'] + ['ClmDiagnosisCode_{}'.format(x) for x in range(1, 11)]\n\n\nClmDiagnosisCode_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We count the number of CLMDiagnosisCode for each claim and store these value in a new variable\n\nTrain_Patient_data['N_UniqueDiagnosis_Claims'] = N_unique_values(Train_Patient_data[ClmDiagnosisCode_vars])\n\n\nTrain_Patient_data['N_UniqueDiagnosis_Claims'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### EDA on other remaining variables \n\n#### 1.Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data.Gender.describe()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data.Gender.value_counts() # here we have only 1 and 2, so we can change it to binary as 0 or 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data['Gender']=Train_Patient_data['Gender'].replace(2,0) # replacing 2 with 0 \n\n\n\n## Countplot of Gender Column, Here we can consider 0 as Female and 1 as Male\n\nfig=plt.figure(figsize=(8,6))\nsns.countplot(Train_Patient_data['Gender'])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.Race"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data['Race'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Countplot of Race variable \n### From here we can find out that majority of claims are from Race 1\nfig=plt.figure(figsize=(8,6))\nsns.countplot(Train_Patient_data['Race'])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Now in Race column we do 'one hot encoding' so that ranking of values doesn't occur here \n\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\nx = onehotencoder.fit_transform(Train_Patient_data.Race.values.reshape(-1, 1)).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_OneHot = pd.DataFrame(x, columns = [\"Race_\"+str(int(i)) for i in range(1,5)]) \ndf_OneHot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_OneHot.drop('Race_1',axis=1,inplace=True) ## Drop the first column \"Race_1\" this we need to drop when we do oneHotEncoding\ndf_OneHot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Concatenation of dataframe \"df_oneHot\" that we created above in our main dataset\n\nTrain_Patient_data = pd.concat([Train_Patient_data, df_OneHot], axis=1)\n\n\nTrain_Patient_data.drop(['Race'], axis=1,inplace=True)  #So now we do not need this race column so we are droping this also ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. RenealDiseaseIndicator"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data['RenalDiseaseIndicator'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Countplot of \"RenalDiseaseIndicator\" variable from here we can findout that maximu disease doesn't have any RenalDisease\nfig=plt.figure(figsize=(8,6))\nsns.countplot(Train_Patient_data['RenalDiseaseIndicator'])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data['RenalDiseaseIndicator']=Train_Patient_data.RenalDiseaseIndicator.astype(int) # Change of datatype from object to int\n\n\nTrain_Patient_data['RenalDiseaseIndicator'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. State and County"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Patient_data[['State','County']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find out which state has maximum count of claims\n\nstate_count=Train_Patient_data['State'].value_counts()\nstate_count=state_count[:20]\nstate_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Count plot of top 20 states which have maximum claims  \n\n## from here we can see that state code 5 has maximum number of claims \n\nfig=plt.figure(figsize=(10,6))\nsns.barplot(state_count.index,state_count.values,order=state_count.index)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find out which County has maximum count of claims\ncounty_count=Train_Patient_data['County'].value_counts()\ncounty_count=county_count[:20]\ncounty_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Count plot of top 20 County which have maximum claims  \n\n## from here we can see that County code 200 has maximum number of claims \n\n\nfig=plt.figure(figsize=(12,6))\nsns.barplot(county_count.index,county_count.values,order=county_count.index)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 5. Chronic_cond","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Visulization of ChronicCond Variables \n\n## From this we can findout that how many claims has ChronicCond diseases, for eg: In ChronicCond_Alzheimer more than 3 lacs claims doesn't have this and remaining claims approx( 2 lacs) have ChronicCond_Alzheimer\n\nfig=plt.figure(figsize=(20,20))\n\nfor col in range(1,12):\n    plt.subplot(6,2,col)\n    sns.countplot(Train_Patient_data.iloc[:,37+col])\n    \nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Boxplots of some numerical features to check the distribution of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Boxplot of \"IPAnnualReimbursementAmt\" and we can see in this boxplot data is not normally distributed and it is left skewed \n\nfig=plt.figure(figsize=(8,6))\nsns.boxplot(Train_Patient_data['IPAnnualReimbursementAmt'])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## Boxplot of \"IPAnnualDeductibleAmt\" and we can see in this boxplot data is not normally distributed and it is left skewed\n\n\nfig=plt.figure(figsize=(8,6))\nsns.boxplot(Train_Patient_data['IPAnnualDeductibleAmt'])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling  Missing values and add new features in Test_Patient_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We are replacing these columns value with 0 and 1 where we have value we are replacing it with 1 and in place of null value we replace it with 0.\n\n\nTest_Patient_data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']] = np.where(Test_Patient_data[['AttendingPhysician', 'OperatingPhysician', 'OtherPhysician']].isnull(), 0, 1)\n\nTest_Patient_data['N_Types_Physicians'] = Test_Patient_data['AttendingPhysician'] +  Test_Patient_data['OperatingPhysician'] + Test_Patient_data['OtherPhysician']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['IsDiagnosisCode'] = np.where(Test_Patient_data.DiagnosisGroupCode.notnull(), 1, 0)\nTest_Patient_data = Test_Patient_data.drop(['DiagnosisGroupCode'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['DeductibleAmtPaid'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['DeductibleAmtPaid'].fillna(0,inplace=True)\n\nTest_Patient_data['IsDeductibleAmtPaid']=np.where(Test_Patient_data['DeductibleAmtPaid']==0,0,1) \n\n\nTest_Patient_data['IsDeductibleAmtPaid'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['AdmitForDays'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['AdmitForDays'].fillna(0,inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data.Gender.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['Gender']=Test_Patient_data['Gender'].replace(2,0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['Race'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehotencoder = OneHotEncoder()\nx = onehotencoder.fit_transform(Test_Patient_data.Race.values.reshape(-1, 1)).toarray()\n\ndf_test_OneHot = pd.DataFrame(x, columns = [\"Race_\"+str(int(i)) for i in range(1,5)]) \ndf_test_OneHot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_OneHot.drop('Race_1',axis=1,inplace=True)\n\n\nTest_Patient_data = pd.concat([Test_Patient_data, df_test_OneHot], axis=1)\n\n#droping the country column \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data.drop(['Race'], axis=1,inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['RenalDiseaseIndicator'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['RenalDiseaseIndicator']=Test_Patient_data.RenalDiseaseIndicator.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data[ClmProcedure_vars].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We count the number of procedures for each claim\nTest_Patient_data['N_Procedure'] = N_unique_values(Test_Patient_data[ClmProcedure_vars])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Patient_data['N_Procedure'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We count the number of CLMDiagnosisCode for each claim and store these value in a new variable\n\nTest_Patient_data['N_UniqueDiagnosis_Claims'] = N_unique_values(Test_Patient_data[ClmDiagnosisCode_vars])\n\nTest_Patient_data['N_UniqueDiagnosis_Claims'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\033[1m'+\"Train Patient Dataset\"+ \"\\033[0m\")\n\nprint(Train_Patient_data.info())\n\nprint('\\033[1m'+\"Test Patient Dataset\"+ \"\\033[0m\")\n\nprint(Test_Patient_data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging of Train and Test dataframe with Train_Patient_data and Test_Patient_data respectively to create a Final Dataframe for Train and Test for modelling  "},{"metadata":{"trusted":true},"cell_type":"code","source":"### Count number of records\n## From here we get the count of BeneID and ClaimId for each provider\n\n## For Train \nTrain_Count = Train_Patient_data[['BeneID', 'ClaimID']].groupby(Train_Patient_data['Provider']).nunique().reset_index()\nTrain_Count.rename(columns={'BeneID':'BeneID_count','ClaimID':'ClaimID_count'},inplace=True)\n\n\n## For Test\nTest_Count = Test_Patient_data[['BeneID', 'ClaimID']].groupby(Test_Patient_data['Provider']).nunique().reset_index()\nTest_Count.rename(columns={'BeneID':'BeneID_count','ClaimID':'ClaimID_count'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Data_Sum = Train_Patient_data.groupby(['Provider'], as_index = False)[['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'RenalDiseaseIndicator', \n                                                     'AttendingPhysician','OperatingPhysician','OtherPhysician','AdmitForDays',\n                                                    'ChronicCond_Alzheimer', 'ChronicCond_Heartfailure','ChronicCond_Cancer', \n                                                    'ChronicCond_KidneyDisease', 'ChronicCond_ObstrPulmonary',\n                                                   'ChronicCond_Depression','ChronicCond_Diabetes', 'ChronicCond_IschemicHeart',   \n                                                    'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis',\n                                                    'ChronicCond_stroke', 'IPAnnualReimbursementAmt','IPAnnualDeductibleAmt',\n                                                    'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt', 'WhetherDead',\n                                                    'N_Types_Physicians','IsDiagnosisCode', 'N_Procedure', 'N_UniqueDiagnosis_Claims']].sum()\n\nTest_Data_Sum = Test_Patient_data.groupby(['Provider'], as_index = False)[['InscClaimAmtReimbursed', 'DeductibleAmtPaid', 'RenalDiseaseIndicator', \n                                                     'AttendingPhysician','OperatingPhysician','OtherPhysician','AdmitForDays',\n                                                    'ChronicCond_Alzheimer', 'ChronicCond_Heartfailure','ChronicCond_Cancer', \n                                                    'ChronicCond_KidneyDisease', 'ChronicCond_ObstrPulmonary',\n                                                   'ChronicCond_Depression','ChronicCond_Diabetes', 'ChronicCond_IschemicHeart',   \n                                                    'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis',\n                                                    'ChronicCond_stroke', 'IPAnnualReimbursementAmt','IPAnnualDeductibleAmt',\n                                                    'OPAnnualReimbursementAmt', 'OPAnnualDeductibleAmt', 'WhetherDead',\n                                                    'N_Types_Physicians','IsDiagnosisCode', 'N_Procedure', 'N_UniqueDiagnosis_Claims']].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Data_Sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Data_Sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Here we are calculating the mean of values for some variables for each unique provider.\n\nTrain_Data_Mean=round(Train_Patient_data.groupby(['Provider'], as_index = False)[['NoOfMonths_PartACov', 'NoOfMonths_PartBCov',\n                                                                            'Age']].mean())\n\n\nTest_Data_Mean=round(Test_Patient_data.groupby(['Provider'], as_index = False)[['NoOfMonths_PartACov', 'NoOfMonths_PartBCov',\n                                                                            'Age']].mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Data_Mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Now we merge Count,sum and mean dataframes with the main train dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Merging of Train Datasets\nTrain_df=pd.merge(Train_Count,Train_Data_Sum,on='Provider',how='left').\\\n                merge(Train_Data_Mean,on='Provider',how='left').\\\n                merge(Train,on='Provider',how='left')\n\n## Merging of Test Datasets\n\nTest_df=pd.merge(Test_Count,Test_Data_Sum,on='Provider',how='left').\\\n                merge(Test_Data_Mean,on='Provider',how='left').\\\n                merge(Test,on='Provider',how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_df #Target column PotentialFraud is avaialble here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_df #Target column PotentialFraud is not avaialble here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_df.isnull().sum() ## No null value is present in this dataset ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In Train Dataset Target variable PotentialFraud has value in category i.e \"Yes\" and \"No\" need to replace with 1 and 0.\n\nTrain_df['PotentialFraud']=np.where(Train_df.PotentialFraud == \"Yes\", 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we can the count of Dependent variable values \nplt.figure(figsize=(10,8))\nsns.countplot(Train_df.PotentialFraud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Bivariant Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Here we can se the barplot of PotentialFraud v/s BeneID_Count and here bar shows mean of BeneID_Count for Potential Fraud value 1 and 0\n## From this barplot we can conclude that there is a Potential Fraud when the BeneID_Count is more as its mean is more as shown.\n\nplt.figure(figsize=(12,8))\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"BeneID_count\"], hue=Train_df[\"PotentialFraud\"])\nplt.suptitle('PotentialFraud v/s BeneID_count')\nplt.xlabel('PotentialFraud')\nplt.ylabel('BeneID_count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see Fraudulant claims have higher number of Beneficiary ID as they tend to commit fraud with multiple beneficiary id."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Here we can se the barplot of PotentialFraud v/s ClaimID_Count and here bar shows mean of ClaimID_Count for Potential Fraud value 1 and 0\n## From this barplot we can conclude that there is a Potential Fraud when the ClaimID_Count is more as its mean is more as shown.\n\nplt.figure(figsize=(12,8))\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"ClaimID_count\"], hue=Train_df[\"PotentialFraud\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same as the above observation, potential fraud claims tend to have higher number of Claim ID."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Here we can se the barplot of PotentialFraud v/s InscClaimAmtReimbursed and here bar shows mean of InscClaimAmtReimbursed for Potential Fraud value 1 and 0\n## From this barplot we can conclude that there is a Potential Fraud when the InscClaimAmtReimbursed is more as its mean is more as shown.\n\nplt.figure(figsize=(12,8))\n\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"InscClaimAmtReimbursed\"], hue=Train_df[\"PotentialFraud\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"DeductibleAmtPaid\"], hue=Train_df[\"PotentialFraud\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have observed both in InscClaimAmtReimbursed and DeductibleAmtPaid are way higher than the legitimate claims."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,8))\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"RenalDiseaseIndicator\"], hue=Train_df[\"PotentialFraud\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,8))\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"AdmitForDays\"], hue=Train_df[\"PotentialFraud\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.barplot(Train_df[\"PotentialFraud\"],Train_df[\"WhetherDead\"], hue=Train_df[\"PotentialFraud\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In category 0, the bar is between 0 and 1 because there are some people who are dead and some are alive, but in category 1 the bar has gone above 3 that means fraudulant claims are more likely to happen where people are dead."},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nTrain_corr=Train_df.corr()\nsns.heatmap(Train_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_corr=Train_df.corr()\nTrain_corr['PotentialFraud']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from here we can see that Age, NoOfMonths_PartBCov and NoOfMonths_PartACov are not making any pattern/relationship with dependent variable 'PotentialFraud', hence we will not consider these variables in our model "},{"metadata":{},"cell_type":"markdown","source":"We will make a final dataset on which we will do modelling,In this dataset we keep only those variable which we will use in our machine learning modelling algorithms. So from our Train_df dataset we will remove all ID type variables like Provider,BeneID_count and ClaimID_count and also remove those variable which are not making any pattern with the dependent variable this we can see correlation matrix that is shown above "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clf=Train_df.iloc[:,3:]\ndf_clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clf.drop(['NoOfMonths_PartACov','NoOfMonths_PartBCov','Age'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Train Dataset on which we trained our model "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clf #This is final Trained Dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Test Dataset on which we will do final Prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(test_data):\n    test_data=test_data.iloc[:,3:]\n    test_data=test_data.drop(['NoOfMonths_PartACov','NoOfMonths_PartBCov','Age'],axis=1)\n    return test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data=test(Test_df)\nTest_data ## In this target varaible is not there we need to predict this after we trained our model ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Working on our Train Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the dataset into Independent and Dependent Features\n\nx=df_clf.drop(\"PotentialFraud\",axis=1)\ny=df_clf.PotentialFraud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Independent Variable shape:\",x.shape)\nprint(\"Dependent Variable shape:\",y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Train Test Split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7,random_state=42)\n\n\nprint(\"Independent variables train:\",x_train.shape)\nprint(\"Target variable train:\",y_train.shape)\nprint(\"Independent variables test:\",x_test.shape)\nprint(\"Target variables test:\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we can see that our target vairable is imbalanced as \"0\" class is is majority and \"1\" class is in minority \nplt.figure(figsize=(10,8))\nsns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since our target variable is imbalanced hence we need to do the modelling using sampling techniques "},{"metadata":{},"cell_type":"markdown","source":"### Sampling Techniques "},{"metadata":{},"cell_type":"markdown","source":"#### 1. Under Sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom imblearn.under_sampling import NearMiss\n\n\n\nns=NearMiss(0.8)\nx_train_ns,y_train_ns=ns.fit_sample(x_train,y_train) ## Create new train dataset after fitting undersmapling\nprint(\"The number of classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of classes after fit {}\".format(Counter(y_train_ns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(y_train_ns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of independent trained dataset after under sampling\",x_train_ns.shape)\nprint(\"Shape of dependent trained dataset after under sampling\", y_train_ns.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling on Under sampled Dataset  "},{"metadata":{},"cell_type":"markdown","source":"### Random Forest on Undersampled Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n\nclf=RandomForestClassifier()\nclf_fit=clf.fit(x_train_ns,y_train_ns)\n\ny_pred_rf=clf_fit.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_rf))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_rf))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that this model accuracy is low or not good.This model is able to classify \"1\" class but not able to classify \"0\" class "},{"metadata":{},"cell_type":"markdown","source":"### SVM on Undersampled Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n\nclf_svc=SVC()\nclf_svc_fit=clf_svc.fit(x_train_ns,y_train_ns)\n\ny_pred_svc=clf_svc_fit.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_svc))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_svc))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In SVM also accuracy score is okay, but here are many miscalssification in \"0\" class, Hence we go for another sampling technique \n"},{"metadata":{},"cell_type":"markdown","source":"### 2. Over Sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\n\nos=RandomOverSampler(0.75)\nx_train_os,y_train_os=os.fit_sample(x_train,y_train)\nprint(\"The number of classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of classes after fit {}\".format(Counter(y_train_os)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(y_train_os)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of independent trained dataset after Over sampling\",x_train_os.shape)\nprint(\"Shape of dependent trained dataset after Over sampling\", y_train_os.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest on Over Sampled Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n\nos_clf=RandomForestClassifier()\nos_clf_fit=os_clf.fit(x_train_os,y_train_os)\n\ny_pred_rf_os=os_clf_fit.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_rf_os))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_rf_os))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_rf_os))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here Random Forest gives us good accuracy score but it is not able to classify majority of \"1\" class correctly  "},{"metadata":{},"cell_type":"markdown","source":"### SVM on Over Sampled Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n\nclf_svc_os=SVC()\nclf_svc_fit_os=clf_svc_os.fit(x_train_os,y_train_os)\n\ny_pred_svc_os=clf_svc_fit_os.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_svc_os))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_svc_os))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_svc_os))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here SVM gives us good accuracy score but there are some misclassification in \"1\" class, Now we want to minimize this misclassification, Hence we go to another sampling technique "},{"metadata":{},"cell_type":"markdown","source":"### 3. Synthetic Minority Oversampling Technique (SMOTE) "},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.combine import SMOTETomek\n\n\n\nos=SMOTETomek(0.75)\nx_train_st,y_train_st=os.fit_sample(x_train,y_train)\nprint(\"The number of classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of classes after fit {}\".format(Counter(y_train_st)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(y_train_st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of independent trained dataset after SMOTE sampling\",x_train_st.shape)\nprint(\"Shape of dependent trained dataset after SMOTE sampling\", y_train_st.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree on SMOTE Sampled Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n\nclf_dt=DecisionTreeClassifier()\nmodel_dt=clf_dt.fit(x_train_st,y_train_st)\ny_pred_dt=model_dt.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_dt))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_dt))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Decision Tree model accuracy is good but there are many misclassification for both the classes."},{"metadata":{},"cell_type":"markdown","source":"### Naive bayes "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB,BernoulliNB\n\n\nclf_nb=GaussianNB()\nmodel_nb=clf_nb.fit(x_train_st,y_train_st)\ny_pred_nb=model_nb.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_nb))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_nb))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_nb))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf_GB=GradientBoostingClassifier()\nmodel_GB=clf_GB.fit(x_train_st,y_train_st)\ny_pred_GB=model_GB.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_GB))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_GB))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_GB))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest on SMOTE sampled Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_st=RandomForestClassifier()\nmodel_rf=clf_st.fit(x_train_st,y_train_st)\ny_pred_rf=model_rf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_rf))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_rf))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = model_rf.predict_proba(x_test)\nprobs = probs[:, 1]\nprobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr1, tpr1, thresholds = roc_curve(y_test, probs)\n\nplot_roc_curve(fpr1, tpr1)\n\n\nprint('\\033[1m'+\"AUC Score \\n\"+'\\033[0m', roc_auc_score(y_test, probs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM on SMOTE Sampled Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_svm=SVC(probability=True)\nmodel_svm=clf_svm.fit(x_train_st,y_train_st)\ny_pred_svm=model_svm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_svm))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_svm))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_svm))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM is working good here as we can see misclassification for classes is also less "},{"metadata":{},"cell_type":"markdown","source":"#### ROC Curve for SVM "},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr2, tpr2, thresholds = roc_curve(y_test, probs_svm)\n\nplot_roc_curve(fpr2, tpr2)\n\n\nprint('\\033[1m'+\"AUC Score \\n\"+'\\033[0m', round(roc_auc_score(y_test, probs_svm),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of Models Accuracy "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_accuracy=round(accuracy_score(y_test,y_pred_rf),4)\nsvm_accuracy=round(accuracy_score(y_test,y_pred_svm),4)\nGB_accuracy=round(accuracy_score(y_test,y_pred_GB),4)\nDT_accuracy=round(accuracy_score(y_test,y_pred_dt),4)\nNB_accuracy=round(accuracy_score(y_test,y_pred_nb),4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy=pd.DataFrame({\"Model\":[\"Decision Tree\",\"Naive Bayes\",\"Random Forest\",\"SVM\",\"Gradient Boosting\"],\"Accuracy\":[DT_accuracy,NB_accuracy,rf_accuracy,svm_accuracy,GB_accuracy]})\nAccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=Accuracy.Model,y=Accuracy.Accuracy,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of ROC Curve between RandomForest and SVM "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(fpr2, tpr2,color='orange',label='SVM')\nplt.plot(fpr1, tpr1,color='green',label='Random Forest')\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hence our final model will be SVM."},{"metadata":{},"cell_type":"markdown","source":"### Prediction of Potential Fraud (Target Variable) on our main Test Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PotentialFraud=model_svm.predict(Test_data)\n\nPotential_Fraud=pd.DataFrame(PotentialFraud,columns=['PotentialFraud'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predicted_Test_data=pd.concat([Test_df,Potential_Fraud],axis=1)\n\nPredicted_Test_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance "},{"metadata":{},"cell_type":"markdown","source":"#### Get important features from RandomForest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances from Random Forest model\nimportances = list(model_rf.feature_importances_)\nprint(importances)\n\nfeature_list=list(df_clf.columns)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n#print(feature_importances)\n# Print out the feature and importances \nprint([print('Variable: {:20} Importance: {} '.format(*pair))  for pair in feature_importances])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get important Features from Gradient Boosting Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances from Gradient Boosting model\nimportances = list(model_GB.feature_importances_)\nprint(importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list=list(df_clf.columns)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n#print(feature_importances)\n# Print out the feature and importances \nprint([print('Variable: {:20} Importance: {} '.format(*pair))  for pair in feature_importances])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\n\nxg_model=XGBClassifier()\nsg_model_fit=xg_model.fit(x_train_st,y_train_st)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nplot_importance(sg_model_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Hence from above we can conclude that important features are :\n\nx_train_imp=x_train_st[['InscClaimAmtReimbursed','AdmitForDays','DeductibleAmtPaid','N_Procedure','IsDiagnosisCode']]\nx_test_imp=x_test[['InscClaimAmtReimbursed','AdmitForDays','DeductibleAmtPaid','N_Procedure','IsDiagnosisCode']]\nx_train_imp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we will do the modelling with important features only"},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM model\n\nsvm_imp=SVC(probability=True)\nsvm_imp=svm_imp.fit(x_train_imp,y_train_st)\ny_pred_svm2=svm_imp.predict(x_test_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('\\033[1m'+\"Confusion Matrix \\n\"+'\\033[0m',confusion_matrix(y_test,y_pred_svm2))\nprint('\\033[1m'+\"\\n Accuracy Score \\n\"+'\\033[0m',accuracy_score(y_test,y_pred_svm2))\nprint('\\033[1m'+\"\\n Classification Report \\n\"+'\\033[0m',classification_report(y_test,y_pred_svm2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ROC Curve of SVM Model on Important Features "},{"metadata":{"trusted":true},"cell_type":"code","source":"probs_new = svm_imp.predict_proba(x_test_imp)\nprobs_new = probs_new[:, 1]\n\n\nfpr, tpr, thresholds = roc_curve(y_test, probs_new)\n\nplot_roc_curve(fpr, tpr)\n\n\nprint('\\033[1m'+\"AUC Score \\n\"+'\\033[0m', round(roc_auc_score(y_test, probs_new),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}