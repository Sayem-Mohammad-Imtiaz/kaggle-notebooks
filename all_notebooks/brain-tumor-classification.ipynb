{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to prevent unnecessary warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\n\nfrom pathlib import Path\n\n#import useful module for keras library\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard,EarlyStopping\n# get modules from sklearn library\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report \n\n#import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tumor_dir=r'../input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set/Brain Tumor'\nhealthy_dir=r'../input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set/Healthy'\nfilepaths=[]\nlabels=[]\ndir_list=[tumor_dir, healthy_dir]\nfor i ,d in enumerate(dir_list): \n    flist=os.listdir(d)\n    for f in flist:\n        fpath=os.path.join(d,f)\n        filepaths.append(fpath)\n        if i==0:\n          labels.append('cancer')\n        else:\n          labels.append('healthy')            \nFseries = pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')    \ntumor_data = pd.concat([Fseries, Lseries], axis=1)\ntumor_df = pd.DataFrame(tumor_data)\nprint (tumor_data.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tumor_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tumor_df['labels'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, test_images = train_test_split(tumor_df, test_size = 0.3, random_state = 42)\n\ntrain_set, val_set = train_test_split(tumor_df, test_size = 0.2, random_state = 42)\nprint(train_set.shape)\nprint(test_images.shape)\nprint(val_set.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_gen = ImageDataGenerator(preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input, rescale=1/255)\n\n# img_gen cannot take in an array, so ensure the data that is been passed is a dataframe\ntrain = img_gen.flow_from_dataframe(dataframe = train_set,\n    x_col = 'filepaths', #name of the column containing the image in the train set\n    y_col ='labels', #name of column containing the target in the train set\n    target_size = (224, 224),\n    color_mode = 'rgb',\n    class_mode = 'categorical',#the class mode here and that for the model_loss(when using sequential model)\n                                    #should be the same\n    batch_size = 32,\n    shuffle = False #not to shuffle the given data\n)\n\ntest = img_gen.flow_from_dataframe(dataframe = test_images,\n    x_col = 'filepaths', #name of the column containing the image in the test set\n    y_col ='labels', #name of column containing the target in the test set\n    target_size =(224, 224),\n    color_mode ='rgb',\n    class_mode ='categorical',\n    batch_size = 32,\n    shuffle = False # not to shuffle the give#check the range of pixel values for the first element in the train data\n                                  )\n\nval = img_gen.flow_from_dataframe(dataframe = val_set,\n    x_col = 'filepaths', #name of the column containing the image in the test set\n    y_col ='labels', #name of column containing the target in the test set\n    target_size =(224, 224),\n    color_mode ='rgb',\n    class_mode ='categorical',\n    batch_size = 32,\n    shuffle = False # not to shuffle the give#check the range of pixel values for the first element in the train data\n                                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define sequential model\nmodel = tf.keras.models.Sequential()\n# define conv-pool layers - set 1\nmodel.add(tf.keras.layers.Conv2D(filters = 32, kernel_size=(3, 3), strides=(1, 1), \n                                activation='relu', padding='valid', input_shape = (224, 224, 3)))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n# add flatten layer\nmodel.add(tf.keras.layers.Flatten())\n\n# add dense layers with some dropout\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate = 0.3))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\n\n# add output layer\nmodel.add(tf.keras.layers.Dense(2, activation='sigmoid')) #use softmax as activation in the output layer\n#for multiclass. Sigmoid activation is used for binary and 'relu' shouldnt be use for output layer\n\n\n# view model layers\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile model\nmodel.compile(optimizer='adam', # optimize the model with adam optimizer\n              loss=\"categorical_crossentropy\", \n              metrics=['accuracy']) #to get accuracy of the model in each run","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train, #fit the model on the training set\n                    validation_data = val, #add the validation set to evaluate the performance in each run\n                    epochs = 11, #train in 10 epochs\n                    verbose = 1)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy'] # get history report of the model\n\nval_acc = history.history['val_accuracy'] # get history of the validation set\n\nloss = history.history['loss'] #get the history of the lossses recorded on the train set\nval_loss = history.history['val_loss'] #get the history of the lossses recorded on the validation set\n\nplt.figure(figsize=(8, 8)) # set figure size for the plot generated\nplt.subplot(2, 1, 1) # a sup plot with 2 rows and 1 column\n\nplt.plot(acc, label='Training Accuracy') #plot accuracy curve for each train run\nplt.plot(val_acc, label='Validation Accuracy') #plot accuracy curve for each validation run\n\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy') #label name for y axis\nplt.ylim([min(plt.ylim()),1]) #set limit for y axis\nplt.title('Training and Validation Accuracy') #set title for the plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8)) # set figure size for the plot generated\nplt.subplot(2, 1, 1) # a sup plot with 2 rows and 1 column\n\nplt.plot(loss, label='Training Loss') #plot loss curve for each train run\nplt.plot(val_loss, label='Validation Loss') #plot loss curve for each validation run\n\nplt.legend(loc='lower right')\nplt.ylabel('Loss') #label name for y axis\nplt.ylim([min(plt.ylim()),1]) #set limit for y axis\nplt.title('Training and Validation Loss') #set title for the plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test, verbose= 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the label of the test_images\npred = model.predict(test)\npred = np.argmax(pred,axis = 1) # pick the class with highest probability\n# sequential model predicts by given probability for each of the classes\n#np.argmax is called on the prediction to choose the class with the highest probability\n\n# Map the label\nlabels = (train.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred2 = [labels[k] for k in pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix # import metrics for evaluation\n\ny_test = test_images.labels # set y_test to the expected output\n\nprint(classification_report(y_test, pred2)) # print the classification report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf = confusion_matrix(y_test, pred2) #confusion matrix for the classification\nprint(cf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,5))\nsns.heatmap(cf, annot = True, fmt = 'g')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}