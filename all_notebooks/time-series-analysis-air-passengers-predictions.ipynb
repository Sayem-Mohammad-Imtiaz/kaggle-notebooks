{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=datetime.datetime.strptime('2019-10-08', '%Y-%m-%d')\nprint(a)\nprint(type(a))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parser(x):\n    return datetime.datetime.strptime(x, '%Y-%m')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= pd.read_csv(\"../input/AirPassengers.csv\", date_parser=parser, parse_dates=[0], index_col=[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#plotting the data\nplt.plot(data['#Passengers'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#determining the rolling stats of data\n#Rolmean is used to smoothen the graph. Weactually deal with noice using rolling mean.\nrolmean= data.rolling(window=12).mean()\nrolstd= data.rolling(window=12).std()\n\norig= plt.plot(data, color='blue', label='original')\nrolmean= plt.plot(rolmean, color='red', label='rolmean')\nrolstd= plt.plot(rolstd, color='black', label='rolstd')\nplt.legend(loc='best')\nplt.title('Plot for original data')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Here we find out that From the above graph, we see that rolling mean itself has a trend component even though rolling standard deviation is fairly constant with time. For our time series to be stationary, we need to ensure that both the rolling statistics ie: mean & std. dev. remain time invariant or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so."},{"metadata":{},"cell_type":"raw","source":"Stationarty Check :-\nMean = constant over all intervals.\nVariance = constant over all intervals.\n\nDickey Fuller Test\nIn this test null hypothesis is that the TimeSeries is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. \n\n##If the Test Statistic is less than the Critical Value, we can reject the null hypothesis and say that the series is stationary."},{"metadata":{"trusted":false},"cell_type":"code","source":"#performing the dickey fuller test on original data\nfrom statsmodels.tsa.stattools import adfuller\norg_test= adfuller(data['#Passengers'])\norg_output=pd.Series(org_test[0:4], index=['TestStatistics', 'P-value', '#lagsUsed', '#obs used'])\nfor key, value in org_test[4].items():\n    org_output['critical_value {}'.format(key)]=value\norg_output\n#the stats came out to be non stationary\n\n#The Time Series is still not stationary here \n#Trend – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n#Seasonality – variations at specific time-frames. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"For a Time series to be stationary, its ADCF test should have:\n\np-value to be low (according to the null hypothesis)\nThe critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\nFrom the above ADCF test result, we see that p-value(at max can be 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is not stationary"},{"metadata":{"trusted":false},"cell_type":"code","source":"#using log of data and check stationarity\ndata_log_value= np.log(data)\nmov_avg= data_log_value.rolling(window=12).mean()\nmov_std= data_log_value.rolling(window=12).std()\n\nplt.plot(data_log_value, label='data_log_value')\nplt.plot(mov_avg, color='red', label='mov_avg')\nplt.plot(mov_std, color='green', label='mov_std')\nplt.legend()\n\n#still the plot is not stationary\n\n#performing the dickey fuller test on original data\nfrom statsmodels.tsa.stattools import adfuller\norg_test= adfuller(data_log_value['#Passengers'])\norg_output=pd.Series(org_test[0:4], index=['TestStatistics', 'P-value', '#lagsUsed', '#obs used'])\nfor key, value in org_test[4].items():\n    org_output['critical_value {}'.format(key)]=value\norg_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets define a function to check stationarity further\ndef check_stationarity(dataset):\n    rolling_mean= dataset.rolling(window=12).mean()\n    rolling_std= dataset.rolling(window=12).std()\n    plt.plot(dataset, color='blue', label='original')\n    plt.plot(rolling_mean, color='red', label='rolling_mean')\n    plt.plot(rolling_std, color='green', label='rolling_std')\n    plt.legend(loc='best')\n    plt.show()\n    \n    from statsmodels.tsa.stattools import adfuller\n    test= adfuller(dataset['#Passengers'], autolag='AIC')\n    output=pd.Series(test[0:4], index=['TestStatistics', 'P-value', '#lagsUsed','#ObsUsed'])\n    for key,value in test[4].items():\n        output['Critical_value {}'.format(key)]= value\n   \n    display(output)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets shift the data by one and check the stationarity for data without log\ndata_1Lag= data.shift(periods=1)\ndataMinusdata1Lag= data-data_1Lag\ndataMinusdata1Lag.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"check_stationarity(dataMinusdata1Lag)\n\n#seems like the rolling mean is constant but still variance is there so it cant be stationary.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#1 Checking stationarity for #data_log_valueMinusmov_avg \n#It is taking rolling mean(12)of log data and then subratcting from original log data\n\ndata_log_value= np.log(data)\nmov_avg= data_log_value.rolling(window=12).mean()\nmov_std= data_log_value.rolling(window=12).std()\n\ndata_log_valueMinusmov_avg= data_log_value-mov_avg\ndata_log_valueMinusmov_avg.dropna(inplace=True)\n\ncheck_stationarity(data_log_valueMinusmov_avg)\n\n#Here the result came out is stationary data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#2 (Without subtraction)\n#Exponential weighted average\nexponentialDecayWeightedAvg= data_log_value.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(data_log_value)\nplt.plot(exponentialDecayWeightedAvg, color='red')\n\n#The Exponential weighted average data is not stationary","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Now taking the subtracted data for Exponential weighted average\n\ndata_log_valueMinusexponentialDecayWeightedAvg= data_log_value-exponentialDecayWeightedAvg\ncheck_stationarity(data_log_valueMinusexponentialDecayWeightedAvg)\n\n#The data comes out here is stationary ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#3 Differencing\n#Shifting Log value by one\ndata_log_value_shift= data_log_value-data_log_value.shift(1)\ndata_log_value_shift.dropna(inplace=True)\nplt.plot(data_log_value_shift)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"check_stationarity(data_log_value_shift)\n#the data comes out here is also stationary.\n\n#Lets take this to build our ARIMA model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Components of Timeseries\n#Decomposition\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecompose= seasonal_decompose(data_log_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trend= decompose.trend\nseasonal= decompose.seasonal\nresidual=decompose.resid\n\nplt.subplot(411)\nplt.plot(data_log_value, label='original')\nplt.legend()\n\nplt.subplot(412)\nplt.plot(trend, color='red', label='trend')\nplt.legend()\n\nplt.subplot(413)\nplt.plot(seasonal, color='green', label='seasonality')\nplt.legend()\n\nplt.subplot(414)\nplt.plot(residual, color='red', label='residual')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"residual.dropna(inplace=True)\ncheck_stationarity(residual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets build our model using differencing as it is very popular technique. Apart from that its easier to add noice and seasonality\n#back into predicted value","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#ACF and #PACF value\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nacf= acf(data_log_value_shift, nlags=10)\npacf=pacf(data_log_value_shift, nlags=10, method='ols')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot ACF\nplt.figure(figsize=(10,6))\nplt.subplot(121)\nplt.plot(acf)\nplt.axhline(y=0, linestyle='--', color='black')\nplt.axhline(y=-1.96/np.sqrt(len(data_log_value_shift)), linestyle='--', color='black')\nplt.axhline(y=1.96/np.sqrt(len(data_log_value_shift)), linestyle='--', color='black')\nplt.axvline(x=0, linestyle='--', color='grey')\nplt.axvline(x=1, linestyle='--', color='grey')\nplt.axvline(x=2, linestyle='--', color='grey')\nplt.axvline(x=3, linestyle='--', color='grey')\nplt.axvline(x=4, linestyle='--', color='grey')\nplt.title('AutoCorrelation Function')\n\n\nplt.subplot(122)\nplt.plot(pacf)\nplt.axhline(y=0, linestyle='--', color='black')\nplt.axhline(y=-1.96/np.sqrt(len(data_log_value_shift)), linestyle='--', color='black')\nplt.axhline(y=1.96/np.sqrt(len(data_log_value_shift)), linestyle='--', color='black')\nplt.axvline(x=0, linestyle='--', color='grey')\nplt.axvline(x=1, linestyle='--', color='grey')\nplt.axvline(x=2, linestyle='--', color='grey')\nplt.axvline(x=3, linestyle='--', color='grey')\nplt.title('PartialAutoCorrelation Function')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_acf(data_log_value_shift, lags=10)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(data_log_value_shift)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(data_log_value_shift)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\nplot_pacf(data_log_value_shift, lags=10)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(data_log_value_shift)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(data_log_value_shift)),linestyle='--',color='gray')\nplt.title('PartialAutocorrelation Function')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#we can take (1,1) or (2,2) value as both lies above the boundary line(dashed line)\n#ARIMA Model\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\nmodel_ARIMA1= ARIMA(data_log_value, order=(2,1,2))\nresult1= model_ARIMA1.fit()\nprint(result1.aic)\n\nplt.plot(data_log_value_shift, color='blue', alpha=0.5 )\nplt.plot(result1.fittedvalues, color='red')\n\n\nfrom sklearn.metrics import mean_squared_error\nprint(np.sqrt(mean_squared_error(data_log_value_shift, result1.fittedvalues)))\nprint('RMSE value is {}'.format(sum((data_log_value_shift['#Passengers']-result1.fittedvalues)**2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets use AIC to find the best values\nimport itertools\np=d=q=range(0,5)\npdq= list(itertools.product(p,d,q))\n\nfor i in pdq:\n    try:\n         model_ARIMA1= ARIMA(data_log_value, order=i)\n         result1= model_ARIMA1.fit()\n         print(i, result1.aic)\n    except:\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_ARIMA_diff= result1.fittedvalues\npredicted_ARIMA_diff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_ARIMA_diff_cumsum= np.cumsum(predicted_ARIMA_diff)\npredicted_ARIMA_diff_cumsum.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_log_value= pd.Series(data_log_value['#Passengers'].iloc[0], index=data_log_value.index).add(predicted_ARIMA_diff_cumsum\n                                                                                                   ,fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_log_value.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"prediction_ARIMA= np.exp(predicted_log_value)\nplt.plot(data)\nplt.plot(prediction_ARIMA, color='red', alpha=0.7)\nplt.show()\n\nfrom sklearn import metrics\nmetrics.r2_score(data, prediction_ARIMA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#lets predict for next 10 years\nresult1.plot_predict(start='1953-07-01', end='1962-12-01')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#forcasting the log value for next 10 years\nresult1.forecast(120)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}