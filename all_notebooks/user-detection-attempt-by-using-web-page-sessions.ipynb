{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"[](http://) INTRODUCTION"},{"metadata":{},"cell_type":"markdown","source":"This dataset contains a user's sequence of web pages and the times between them in their web session. The objective is to take a webpage session (a sequence of webpages attended consequently by the same person) and predict whether it belongs to Alice or somebody else.\n\nMore information can be obtained here: https://www.kaggle.com/danielkurniadi/catch-me-if-you-can\n\nIdentifying a user could be useful for detecting like fraud or some other anomalous behavior. It is common to use a user's webpage sessions to identify them, but additional information would be helpful (geographical location, devices, etc.) to distinguish users individually. \n\nI originally used the dataset from Catch Me If You Can, but had to use the mlcourse ai 4 dataset to get the labels for the webpages. In addition, I used the following kernels to assist me in data preparation and exploratory data analysis:\n* https://www.kaggle.com/kerneler/starter-catch-me-if-you-can-7181e865-4\n* https://www.kaggle.com/dariavol/alice-baseline\n\nI am assuming that target = 1 is Alice's sessions and target = 0 is someone else's.\nThis is a classification problem. Our primary question we want to ask is, using the dataset, can we predict which sessions belongs to Alice?"},{"metadata":{},"cell_type":"markdown","source":"Below are the packages used for this kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport collections\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport datetime\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is a list of functions used for this kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function that to get a list of the sites by mapping the key to the webpage\ndef get_site_name(site_key, site_dictionary):\n    site_list = []\n    for i in range(len(site_key)):\n        for key, value in site_dictionary.items():\n            if value == site_key[i]:\n                site_list.append(key)\n    return site_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATA PREPARATION"},{"metadata":{},"cell_type":"markdown","source":"In this section, I will examine the contents of the datasets. Then, I checked to see if there were missing values and replaced them. Lastly, I standardize the data types of the columns so that there are in an appropriate format for data analysis."},{"metadata":{},"cell_type":"markdown","source":"The command below will display the number of files in the input directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get an understanding of the columns that make up the dataset, I displayed the first five rows along with the number of rows and columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train_sessions.csv\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrow, ncol = df_train.shape\nprint('There are %i rows and %i columns in the train dataset.' % (nrow, ncol))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test_sessions.csv\")\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrow, ncol = df_test.shape\nprint('There are %i rows and %i columns in the test dataset.' % (nrow, ncol))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the site_dic.pkl file and save the contents to the variable site_dic. The pkl file contains the key-value pairs for the sites."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/site_dic.pkl', 'rb') as f:\n    site_dic = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While looking at the train dataset, there were quite a few Not a Number (NaN) values. We will explore how many are present in each column of the datasets."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"Nan_num_train= df_train.isna().sum()\nNan_num_test= df_test.isna().sum()\nprint('The number of NaNs in the training set per column \\n', Nan_num_train)\nprint('The number of NaNs in the testing set per column \\n ', Nan_num_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There could a few reasons why there are so many NaN values. For now, I am assuming that they visited fewer 10 sites so I replaced the NaN values with zeros."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_train = df_train.fillna(0)\ndf_test = df_test.fillna(0)\n\nNan_num_train= df_train.isna().sum()\nNan_num_test= df_test.isna().sum()\nprint('The number of NaNs in the training set per column \\n', Nan_num_train)\nprint('The number of NaNs in the testing set per column \\n ', Nan_num_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used the dtype command to display the data types of the training and test sets. The session_id, site1, and target are integers. The time columns are objects. The rest of the site columsns are floats. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_test.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert all the time columns from text to datetime. This step comes in handy when using the describe function."},{"metadata":{"trusted":true},"cell_type":"code","source":"timelist = ['time%s' % i for i in range(1, 11)]\ndf_train[timelist] = df_train[timelist].apply(pd.to_datetime)\ndf_test[timelist] = df_test[timelist].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the dtype command to display the data types of the training and test sets. We see that all of the the time columns are now in datetime format."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_test.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EXPLORATORY DATA ANALYSIS"},{"metadata":{},"cell_type":"markdown","source":"In this section, I am primarily analyzing the training dataset. It is quite long, but I wanted to be a bit thorough with my analysis. \n\nHere are some of the questions of interest:\n* Are the features correlated?\n* What are the most popular pages?\n* Which time period (year, month, day of the week, etc.) had the most web page views?"},{"metadata":{},"cell_type":"markdown","source":"Use the describe function to look at descriptive statistics. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_train[list(df_train)].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"df_test[list(df_test)].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When plotting the counts of the target values, the figure below shows there are more sessions that do not belong to Alice (target = 0). This means that the data is skewwed and the classes are unbalanced. \n\nThis could happen for a number of reasons. Maybe Alice was not as active as a user as this other person. Maybe there was not as much data that was collected for Alice, which common for new users. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nsns.countplot(x=\"target\", data=df_train).set_title('Counts per Target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have created a sub-dataframe called sites_df. This dataframe has the columns target, variable (site1,...,site10), and sites (site key).\nIt will be used to create a distribution plot of the sites and a couple of plots of the most popular websites. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"sites = [c for c in df_train if c.startswith('site')]\nsites_df = pd.melt(df_train, id_vars='target', value_vars=sites, value_name='sites')\nsites_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"sns.distplot(sites_df['sites']).set_title('Sites Distribution Plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_sites = collections.Counter(sites_df['sites']).most_common(11)\nprint(popular_sites)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove the order pair (0.0, 122730) from the list as 0.0 is not a real site. "},{"metadata":{"trusted":true},"cell_type":"code","source":"popular_sites.remove((0.0, 122730))\n#print(popular_sites)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below shows the popular sites for the overall population, Alice, and a user that are not Alice."},{"metadata":{"trusted":true},"cell_type":"code","source":"site, count = zip(*popular_sites)\nsite_labels = get_site_name(site,site_dic)\n#print(site_labels)\n\ny_pos = np.arange(len(site_labels))\nplt.barh(y_pos, count, align='center', alpha=0.5)\nplt.yticks(y_pos, site_labels)\nplt.xlabel('Count')\nplt.title('Top %i Sites Overall' % len(site_labels))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alice_sites = sites_df[sites_df['target'] == 1]\nalice_sites.head(10)\n#sns.distplot(alice_sites['sites']).set_title('Alice\\'s Sites Distribution Plot')\nalice_popular_sites = collections.Counter(alice_sites['sites']).most_common(11)\n#print(alice_popular_sites)\n\nsite, count = zip(*alice_popular_sites)\nsite_labels = get_site_name(site,site_dic)\n#print(site_labels)\n\ny_pos = np.arange(len(site_labels))\nplt.barh(y_pos, count, align='center', alpha=0.5)\nplt.yticks(y_pos, site_labels)\nplt.xlabel('Count')\nplt.title('Alice\\'s Top %i Sites Overall' % len(site_labels))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"notalice_sites = sites_df[sites_df['target'] == 0]\nnotalice_popular_sites = collections.Counter(notalice_sites['sites']).most_common(11)\n#print(notalice_popular_sites)\nnotalice_popular_sites.remove((0.0, 122529))\n\nsite, count = zip(*notalice_popular_sites)\nsite_labels = get_site_name(site,site_dic)\n#print(site_labels)\n\ny_pos = np.arange(len(site_labels))\nplt.barh(y_pos, count, align='center', alpha=0.5)\nplt.yticks(y_pos, site_labels)\nplt.xlabel('Count')\nplt.title('Not Alice Top %i Sites Overall' % len(site_labels))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next lines of code, I am creating dataframes of just the time columns per webpage session for the general population called time_df. It has the columns target, variable (time1,...,time10), and times (timestamp).\n\nFrom that set, I will generate dataframes for alice and this unamed person. This dataframe will be used to create smaller dataframes for visualizing trends for a specific measurement of time. \n\nHere are a couple of resources I used for creatng the sub-dataframes: \n* https://docs.python.org/2/library/datetime.html\n* https://www.programiz.com/python-programming/datetime"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"times = [c for c in df_train if c.startswith('time')]\ntimes_df = pd.melt(df_train, id_vars='target', value_vars=times, value_name='times')\ntimes_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alice_timesdf = times_df[times_df['target']==1]\nnotalice_timesdf = times_df[times_df['target']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_df = times_df['times'].dt.year\nyear_df.value_counts().plot('bar').set_title('Counts per Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_alice = alice_timesdf['times'].dt.year\nyear_alice.value_counts().plot('bar').set_title('Counts per Year For Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_notalice = notalice_timesdf['times'].dt.year\nyear_notalice.value_counts().plot('bar').set_title('Counts per Year For Not Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_df = times_df['times'].dt.month\nmonth_df.value_counts().plot('bar').set_title('Counts per Month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_alice = alice_timesdf['times'].dt.month\nmonth_alice.value_counts().plot('bar').set_title('Counts per Month For Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_notalice = notalice_timesdf['times'].dt.month\nmonth_notalice.value_counts().plot('bar').set_title('Counts per Month For Not Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_df = times_df['times'].map(lambda x: x.strftime('%H'))\nhour_df.value_counts().plot('bar').set_title('Counts per Hour')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_alice = alice_timesdf['times'].map(lambda x: x.strftime('%H'))\nhour_alice.value_counts().plot('bar').set_title('Counts per Hour For Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_notalice = notalice_timesdf['times'].map(lambda x: x.strftime('%H'))\nhour_notalice.value_counts().plot('bar').set_title('Counts per Hour For Not Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myr_df = times_df['times'].map(lambda x: x.strftime('%m-%Y'))\nmyr_df.value_counts().plot('bar').set_title('Counts per Month-Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myr_alice = alice_timesdf['times'].map(lambda x: x.strftime('%m-%Y'))\nmyr_alice.value_counts().plot('bar').set_title('Counts per Month-Year For Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myr_notalice = notalice_timesdf['times'].map(lambda x: x.strftime('%m-%Y'))\nmyr_notalice.value_counts().plot('bar').set_title('Counts per Month-Year For Not Alice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wkday_df = times_df['times'].map(lambda x: x.weekday())\nwkday_df.value_counts().plot('bar').set_title('Counts per Day of the Week')\n#Saturday and Sunday are, respectively. the less active days of the week","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wkday_alice = alice_timesdf['times'].map(lambda x: x.weekday())\nwkday_alice.value_counts().plot('bar').set_title('Counts per Day of the Week For Alice')\n#Saturday, Wednesday, and Sunday are, respectively, the less active days of the week","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wkday_notalice = notalice_timesdf['times'].map(lambda x: x.weekday())\nwkday_notalice.value_counts().plot('bar').set_title('Counts per Day of the Week For Not Alice')\n#Saturday and Sunday are, respectively, the less active days of the week","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am creating a dataframe of time deltas. The columns of this new dataset are the difference in seconds between each web page. "},{"metadata":{"trusted":true},"cell_type":"code","source":"timedelta = np.zeros((df_train.shape[0], len(timelist)+1))\n#len with be 11, columns 0 to 10\ntimedelta[:,len(timelist)] = df_train['target']\n#column 10 is the target\nfor i in range(len(timelist)-2):\n    timedelta[:,i] = (df_train[timelist[i]] - df_train[timelist[i+1]]).abs().dt.seconds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#timedelta.mean(axis=1)\n#row mean\nalice_timedelta = timedelta[timedelta[:,10] == 1]\nalice_avgtimedelta = alice_timedelta.mean(axis=1)\nplt.hist(alice_avgtimedelta, 4, facecolor='blue', alpha=0.5)\nplt.xlabel('Seconds')\nplt.ylabel('Count')\nplt.title('Alice\\'s Average Time Between Sites')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"notalice_timedelta = timedelta[timedelta[:,10] == 0]\nnotalice_avgtimedelta = notalice_timedelta.mean(axis=1)\nplt.hist(notalice_avgtimedelta, 4, facecolor='blue', alpha=0.5)\nplt.xlabel('Seconds')\nplt.ylabel('Count')\nplt.title('Not Alice Average Time Between Sites')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FEATURE ENGINEERING"},{"metadata":{},"cell_type":"markdown","source":"The time columns need to be transformed before passing the dataset into a machine learning algorithm.\n\nThere are many different ways to encode the time columns (see link below). I have chosen the day of the week. \n\nhttps://stackoverflow.com/questions/46428870/how-to-handle-date-variable-in-machine-learning-data-pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new_train = df_train\ndf_new_train[timelist] = df_new_train[timelist].applymap(lambda x: x.weekday())\ndf_new_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new_test = df_test\ndf_new_test[timelist] = df_new_test[timelist].applymap(lambda x: x.weekday())\ndf_new_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MAKING PREDICATIONS"},{"metadata":{},"cell_type":"markdown","source":"The new test and train sets are reassigned to the variables: X_train (features set), y_train (labels), X_test (test set). They will be passed into scikit learn's Logistic Regression algorithm to make predications on the user (target) given a vector webpages and timestamps. \n\nIts seems there wasn't a target column for df_test to create the variable y_test. This makes a little difficult to check the accuracy/performance of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_new_train[df_new_train.columns[1:20]]\ny_train = df_new_train[df_new_train.columns[21]]\nX_test = df_new_test[df_new_test.columns[1:20]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(random_state=3, solver='lbfgs').fit(X_train, y_train)\npredictions = logreg.predict(X_test)\npredictions_probabilities = logreg.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values, counts = np.unique(predictions, return_counts=True)\nprint(values)\nprint(counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Possible next steps:\n* Split the original train set into cross validation sets and test sets to correctly measure model's accuracy\n* Explore other machine learning algorithms\n* Research and apply methods to deal with unbalanced classes\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}