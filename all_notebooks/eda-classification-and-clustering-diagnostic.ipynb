{"cells":[{"metadata":{"_cell_guid":"3807cf76-b310-41b1-bddd-7c5500f914d4","_uuid":"1245ddf84e60fba2da240853f069b858546562c4","colab_type":"text","id":"hndBHlM9zBNY"},"cell_type":"markdown","source":"# Breast Cancer Wisconsin (Diagnostic) \nThis is my first kernel on kaggle , I hope this kernel will  be helpful "},{"metadata":{"_cell_guid":"e4484223-bf1d-4b4e-969c-7c1c3cf22175","_uuid":"01691b8ff549695c7fa149051267ed52fc2a1c46","colab_type":"toc","id":"g_2X4uTSrvoN"},"cell_type":"markdown","source":">[Breast Cancer Wisconsin (Diagnostic)](#scrollTo=hndBHlM9zBNY)\n\n>[Exploratory Data Analysis](#scrollTo=Y7Hoah7Qzr9e)  \n\n>>[Load Libraries](#scrollTo=VMiGkUAR0yke)\n\n>>[Load Data](#scrollTo=78nQ3i230vZN)\n\n>>[Data cleaning](#scrollTo=1k2un17s1imZ)\n\n>>[Are there outliers?](#scrollTo=DeLC8yiWIez_)\n\n>>[Correlation between features](#scrollTo=j0ar9UxxzIYz)\n\n>[Feature engineering](#scrollTo=EWxSUWbkqzEU)\n\n>>[What are the most important features](#scrollTo=Ph5LJxBaZCPG)\n\n>>[PCA,TSE, Visualizziamo il dataset](#scrollTo=cq6t-EbPtv_k)\n\n>[Classification](#scrollTo=s-BrG81hz8XX)\n\n>>[Plotting functions](#scrollTo=0y6zRuG8BMDZ)\n\n>>[LogisticRegression](#scrollTo=hIGngCe0BzWU)\n\n>>[K-Nearest Neighbor](#scrollTo=-AlWlgU3rYku)\n\n>>[Random Forest](#scrollTo=q62LTQISuCFE)\n\n>>[Support Vector - linear](#scrollTo=EdK6pp23wCna)\n\n>>[SVM - rbf](#scrollTo=yjCVaYwmy-zG)\n\n>>[Support Vector - polynomial](#scrollTo=-jbfWrkC1ISR)\n\n>>[AdaBoostClassifier](#scrollTo=cbWIjC7f25cB)\n\n>>[Bagging](#scrollTo=sW2L1pNT8Qxp)\n\n>>[Neural Network](#scrollTo=PiLDKoNZ-nSf)\n\n>>[Ranking](#scrollTo=AuSn2_Opz94T)\n\n>[Clustering](#scrollTo=ljq5av_2z_xS)\n\n>>[Hierarchical clustering](#scrollTo=CZ-Sz2QK0Xr4)\n\n>>>[Functions Utils](#scrollTo=F6h0d_7A5NXW)\n\n>>>[Linkage functions.](#scrollTo=8Ty9jfTC5u2M)\n\n>>>[Ward Linkage K = 2](#scrollTo=iiBcdEzW0pia)\n\n>>>[Ward K=3 (Elbow Method)](#scrollTo=8i88Cxve1pFk)\n\n>>[K Meas](#scrollTo=0RtwOd1Q2AHS)\n\n>>>[Silhouette analysis](#scrollTo=7hcPAb4_2LBS)\n\n"},{"metadata":{"_cell_guid":"cf98633b-3114-44e7-8fbb-28b5b478caa6","_uuid":"887d331885210d26fbeb2aac40d7e9f6e9f74f74","colab_type":"text","id":"Y7Hoah7Qzr9e"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_cell_guid":"2fc5ce6a-8d25-4092-b8ee-3368f9fb6f42","_uuid":"ec3327077f88874e68a40a276adf88ccd1d5f924","colab_type":"text","id":"i8JNAdP1qPpK"},"cell_type":"markdown","source":"**Description of the Dataset.**\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. \n![alt text](http://thenurseszone.com/wp-content/uploads/2017/03/Breast-Biopsy-2.jpg)\n\nThis dataset is also available via the ftp server UW CS: http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/\n\nAttributes information:\n* ID number\n* Diagnosis (M = malignant, B = benign)\n\n\n\n\n\n![alt text](http://img.technews.tw/wp-content/uploads/2013/07/fna-benign1.png)\n![alt text](http://img.technews.tw/wp-content/uploads/2013/07/fna-malignant1.png)\n\nTen characteristics with true values are calculated for each cell nucleus:\n* Radius (Average of distances from center to points on the perimeter)\n* Consistency (standard deviation of grayscale values)\n* Perimeter\n* Smoothness (local variation in radius lengths)\n* Compactness (perimeter ^ 2 / area - 1.0)\n* Concavity (severity of the concave parts of the contour)\n* Concave points (number of concave parts of the contour)\n* Symmetry\n* fractal dimension ('coastline approximation' - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant\n"},{"metadata":{"_cell_guid":"b4a412f8-b023-4623-9967-d35bc22833a0","_uuid":"eb36c126a50447374a8e4f13099f1f8f1234ee2d","colab_type":"text","id":"VMiGkUAR0yke"},"cell_type":"markdown","source":"## Load Libraries"},{"metadata":{"_cell_guid":"b09e815a-3e08-464b-a4dd-7a3ba6f0d02d","_uuid":"0deb99666d6dec37b1c05bcd6274b43cf96583c9","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"a2jPersv04he","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# keeps the plots in one place. calls image as static pngs\n%matplotlib inline \nimport matplotlib.pyplot as plt # side-stepping mpl backend\nimport matplotlib.gridspec as gridspec # subplots\nimport seaborn as sns \n\n#Import models from scikit learn module:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics \n\n#colab\n#from google.colab import files\n\nfrom sklearn.decomposition import PCA,KernelPCA # Principal Component Analysis module\nfrom sklearn.manifold import TSNE, MDS # TSNE module","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ea6d7d7-b8c2-43b3-8be9-78eee8c5b404","_uuid":"1def6474ef1b6aeef5858995e9409f4a69e0d097","colab_type":"text","id":"78nQ3i230vZN"},"cell_type":"markdown","source":"## Load Data "},{"metadata":{"_cell_guid":"a8f88e58-605f-4b99-a711-cfca60fa41e8","_uuid":"4ae03c2054720bdb67ae185448f90e04fb4e53a9","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":89,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":7885,"status":"ok","timestamp":1526573124520,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"z9nvUSKLzvKZ","outputId":"c0a6d2aa-715b-41be-917d-06393ee37d69","trusted":true},"cell_type":"code","source":"#uploaded = files.upload()\n\n#for fn in uploaded.keys():\n#  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n#      name=fn, length=len(uploaded[fn])))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6c8396f-8df6-4161-a101-4569178f9051","_uuid":"298dc00f95dbfd6b33bd9c4d2a8db81fa34bde21","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":270},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":477,"status":"ok","timestamp":1526573125122,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"FMaHO-Mc0OSS","outputId":"da507e16-45e3-4331-cb3a-16aedc2cad9f","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/data.csv\",header = 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"37791460-6cd8-4eb1-86e8-6f6e96eedf03","_uuid":"2ac50bbeef3ae057715d55638797f80a4f19155d","colab_type":"text","id":"1k2un17s1imZ"},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"_cell_guid":"4b130b9e-e1b8-4767-8906-42c691613096","_uuid":"e258e16523f145c9ac80baefa687b1d74fa0c861","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":663},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":592,"status":"ok","timestamp":1526572140425,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"amKMS71E4qu1","outputId":"bdebfaee-60f3-461d-e93d-a096040ee717","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92899a20-b017-4f3b-bc52-047d1c2c9df1","_uuid":"e28003e6eaf79822f01b3afbc67d41a8cd73d4e3","colab_type":"text","id":"w2UfBCBT5Eol"},"cell_type":"markdown","source":"Delete the unuseful columns : 'id' and 'Unnamed: 32' (it is empty) are not needed for the data analysis."},{"metadata":{"_cell_guid":"f0d85999-dfde-4642-aa59-f38d9f78a6e6","_uuid":"5f5b973e3cc235692bcc3035a410613ecba48715","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":788,"status":"ok","timestamp":1526573128586,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"wgP21ovu5GGo","outputId":"559c73e9-59fa-4225-d9e2-168a71fbee35","trusted":true},"cell_type":"code","source":"df.drop('id',axis=1,inplace=True)\ndf.drop('Unnamed: 32',axis=1,inplace=True)\nprint('Are there null values? ',df.isnull().values.any())\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e8be4e67-761e-4392-8b8c-e02b73240d87","_uuid":"5e6024e3e68fef2a2597aa68414a4ea0e6da11d2","colab_type":"text","id":"Uydoq3wMuL3b"},"cell_type":"markdown","source":"There aren't null values! let's see the number of samples in the two classes and the percentages"},{"metadata":{"_cell_guid":"cdc041d7-7a12-46da-8e2c-720def1d8e8e","_uuid":"f563d89e58c2bfe1d0740bcc932b79446b03add9","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":381},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":506,"status":"ok","timestamp":1526486418018,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"1nJQsBPu2_Ir","outputId":"ad32591e-e0ec-46d3-c085-a0fa16aa5a3f","trusted":true},"cell_type":"code","source":"sns.countplot(df['diagnosis'],label=\"Count\")    \nB, M = df['diagnosis'].value_counts()\n\nprint('Number of Benign\\t:\\t ',B)\nprint('Number of Malignant\\t:\\t ',M)\nprint('Percentage Benign\\t:\\t % 2.2f %%' % (B/(B+M)*100))\nprint('Percentage Malignant\\t:\\t % 2.2f %%' % (M/(B+M)*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7f15595-e23b-4d1e-bf0b-e0091b857c62","_uuid":"3cc3fe56a1963dc77225681f5b51cbeecbbd6d97","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":363},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":586,"status":"ok","timestamp":1526573131277,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"pAX7YNTG2lvK","outputId":"aeb8760f-16f5-4844-e02d-1359c1eb8096","trusted":true},"cell_type":"code","source":"df_features = df[df.columns[1:31]]\ny =   df[df.columns[0]]\ndf_features.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c12f9dd-4a46-428b-93c3-d8e88fe1d75e","_uuid":"178fc26921aad9af6f7f1c0386cc7bdeadafea4f","colab_type":"text","id":"DeLC8yiWIez_"},"cell_type":"markdown","source":"## Are there outliers?\nAfter a StandardScaler,  in order to easily compare the data, I want to determine if there are any outliers in the data set."},{"metadata":{"_cell_guid":"2f283d36-53b1-42de-aeb9-6b21f8b67ee8","_uuid":"8d6587605be117f3281f0bddb00d05f4e7b5cfcc","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"224seATHLH-e","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_features =scaler.fit_transform(df_features.values)\ndf_features_scaled = pd.DataFrame(scaled_features, index=df_features.index, columns=df_features.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8c82a55-fd96-4616-b670-5264824284e6","_uuid":"b55b5ef8f41e06d8567f5592979f49b63c5df1ad","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":619},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1070,"status":"ok","timestamp":1526573135305,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"H_TFWZgvZAbF","outputId":"13d26ee4-0c0d-48c3-c651-75c93fe903fd","trusted":true},"cell_type":"code","source":"# ci sono outlier?\n\nplt.figure(figsize=(14,8))\nsns.boxplot( data = df_features_scaled )\nplt.xticks(rotation=90)  \n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d800109-833e-48fc-8246-53004f0f5fa5","_uuid":"f1a03cf526b445a64e5f36288c625472b0e77587","colab_type":"text","id":"iAQBqXBYYznj"},"cell_type":"markdown","source":"A robust statistical approach would have considered outliers all the values plotted over the value 3 (or 2.5) on the y axis. \nhowever, given the limited range of data distribution (from about -2.5 to about +11) and the availability of data is not very high (only 569 observations), it has been preferred to use an \"visual\" approach for detection (based on density of the points above a specific threshold) and remotion  the outliers.I consider only the observations plotted under  the 6 value  (very far from the mean).\n\nNote. Outliers could be indicative of incorrect data, erroneous procedures or experimental areas where some theories may not be valid\nBefore removing them we should discuss with domino experts to understand why these points are not valid (for example, the measuring equipment failed, the measurement method was unreliable for some reason, there were contaminants, etc ... ).\n\n\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.2"},{"metadata":{"_cell_guid":"6e272c1d-be34-4b5e-9a27-a1b09ebdf4d1","_uuid":"e7d890ec23ec14574462fd137402f64ecf107a63","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":636},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1601,"status":"ok","timestamp":1526573138681,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"iFyDcpNUDLzO","outputId":"7cc6e675-9c80-4758-d844-3c9eb4349504","trusted":true},"cell_type":"code","source":"df_features_clean=df_features_scaled[df_features_scaled.apply(lambda x: np.abs(x - x.mean()) / x.std() < 6).all(axis=1)]\nprint(df_features_clean.shape)\nplt.figure(figsize=(14,8))\nsns.boxplot( data = df_features_clean )\nplt.xticks(rotation=90)  \nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10eb69a1-2926-4b21-9f31-7ebb33137d25","_uuid":"7418b68b99d638d87d20df79ceef76c5660ff284","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":469},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":571,"status":"ok","timestamp":1526573139740,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"82-mgj57KY2w","outputId":"ae8b6eb0-d7cc-4100-de13-19f503e6df96","trusted":true},"cell_type":"code","source":"#############################\ndf_features_y_clean = pd.concat([df_features_clean, y], axis=1, join_axes=[df_features_clean.index])\n#################################\ndf_features_clean = df_features_y_clean.iloc[:,0:30]\ndf_y = df_features_y_clean.iloc[:,30]\n\nB, M = df['diagnosis'].value_counts()   \nB_c, M_c = df_features_y_clean['diagnosis'].value_counts()\nsns.countplot(df_features_y_clean['diagnosis'],label=\"Count\")    \n\nprint('Number of Malignant              : ',M)\nprint('Number of Benign                 : ',B)\nprint('Number of Malignant              : % 2.2f %%' % (M/(B+M)*100))\nprint('Number of Benign                 : % 2.2f %%' % (B/(B+M)*100))\nprint('Number of Malignant (no outliers): ',M_c)\nprint('Number of Benign    (no outliers): ',B_c)\nprint('Number of Benign    (no outliers): % 2.2f %%' % (B_c/(B_c+M_c)*100))\nprint('Number of Malignant (no outliers): % 2.2f %%' % (M_c/(B_c+M_c)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"776dcc0e-0fc3-4f3f-bb74-88592c5f6798","_uuid":"7448137077b14998fa8b0fef2d2b0fca99014f48","colab_type":"text","id":"43iXjm2Ayaxq"},"cell_type":"markdown","source":"After  removing of the outliers, the dataset is composed as follows:\n\nTotal observations of the dataset: 557;\n\nMalignant Class: 205;\n\nBenign Class: 352.\n\nThe proportion is maintained"},{"metadata":{"_cell_guid":"3fc24b45-af48-4918-ad6a-3f67bc9cecf8","_uuid":"779f8a267d79cb6cece7cce8df86179b934b587d","colab_type":"text","id":"j0ar9UxxzIYz"},"cell_type":"markdown","source":"## Correlation between features"},{"metadata":{"_cell_guid":"c8e9b59d-013b-4cac-ae30-5799e384fc6d","_uuid":"fe6646a07777ed597f161b2c045b875f3c1c3ca4","colab_type":"text","id":"HJ8AC1GOF8WM"},"cell_type":"markdown","source":"We study the correlation dividing the features into three groups: (The mean, standard error and \"worst\" or largest )"},{"metadata":{"_cell_guid":"759ab126-aeac-403f-a610-cb44519c4427","_uuid":"868762a8faebbd4e02941ee75103403b39b8cbdf","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"TBoIreX6Xham","trusted":true},"cell_type":"code","source":"df_features_mean= df_features[df_features.columns[0:10]]\ndf_features_se= df_features[df_features.columns[10:20]]\ndf_features_worst = df_features[df_features.columns[20:30]]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0cb3a0e8-b058-4d6b-906b-b6ddd92444f7","_uuid":"5b429bf58c0145959da8016b68d15c8a9e758a9d","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":461},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":3357,"status":"ok","timestamp":1526061725230,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"8PIdE1rLRoGv","outputId":"27b73590-c34f-4be4-98be-699a7e60d160","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,5))\nplt.subplot(1, 3, 1)\nsns.heatmap( df_features_mean.corr(), cbar = False,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},cmap= 'coolwarm')\nplt.subplot(1, 3, 2)\nsns.heatmap(df_features_worst.corr(), cbar = False,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},cmap= 'coolwarm')\nplt.subplot(1, 3, 3)\nsns.heatmap(df_features_se.corr(), cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95a152f3-b8c3-415f-a38d-9de3e0b7a352","_uuid":"8143c9c584d58c85543234cd00421d1150974e16","colab_type":"text","id":"OOeSPpnr0weP"},"cell_type":"markdown","source":"radius, area and perimeter (mean, the wrost, and the error standard) are closely correlated to each other, the same for   the characteristics of compactness, concave points and concavity.\n"},{"metadata":{"_cell_guid":"e5b44595-7016-4ecc-a057-adf5ee3912c2","_uuid":"aee59b2e43dc7f3111c32eec8b674c79556c6c57","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1844},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":20395,"status":"ok","timestamp":1526061660848,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"49-u0mf0kGUX","outputId":"3e79ccc7-0773-467a-f1b7-ec893668fe84","trusted":true},"cell_type":"code","source":"\nsns.pairplot(  pd.concat([df_features_mean,y], axis=1),  hue='diagnosis', diag_kind=\"kde\",diag_kws=dict(shade=True))\n# sns.pairplot(  pd.concat([df_features_se,y], axis=1),    hue='diagnosis', diag_kind=\"kde\",diag_kws=dict(shade=True))\n# sns.pairplot(  pd.concat([df_features_worst,df_y], axis=1), hue='diagnosis', diag_kind=\"kde\",diag_kws=dict(shade=True))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9958609f-cc0b-400d-95dd-0b97118209ff","_uuid":"d3d1e138c274b1f5d6bd09f752ca72a52cde54c3","colab_type":"text","id":"ASvOMnSf1TF8"},"cell_type":"markdown","source":"Observations: The radius, area and perimeter characteristics are closely related and have higher values and binomial distribution. In malignant tumors, the benign tumors have smaller cell nuclei and Gausssian distribution. HWHM is bigger in the class of malignants, but broadening is narrower in the benign"},{"metadata":{"_cell_guid":"4e4382b4-576e-43c6-86a8-0e9d86258433","_uuid":"e6eb38e420a51937cabe975f65b724407d0cbc09","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"64etBBDZsw2G","trusted":true},"cell_type":"code","source":"# I change the order to better compare\norder_cols = ['radius_mean', 'radius_se', 'radius_worst',\n 'texture_mean', 'texture_se', 'texture_worst',\n 'perimeter_mean', 'perimeter_se', 'perimeter_worst',\n 'area_mean','area_se','area_worst',\n 'smoothness_mean', 'smoothness_se', 'smoothness_worst',\n 'compactness_mean', 'compactness_se', 'compactness_worst',\n 'concavity_mean', 'concavity_se', 'concavity_worst',\n 'concave points_mean','concave points_se','concave points_worst',\n 'symmetry_mean', 'symmetry_se', 'symmetry_worst',\n 'fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst','diagnosis']\ndf_features_y_clean = df_features_y_clean[order_cols]\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f786737b-bd12-4ce6-96c7-bdd18eb5b32b","_uuid":"52dfc238cd7af956b8bb42dd1592840ba9002fef","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":707},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":2528,"status":"ok","timestamp":1526202120236,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"c3u2vUwHrRVc","outputId":"467b8e9d-a3b6-4238-c6e5-e987ed1e3be2","trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(18,8))\ndata = pd.melt(df_features_y_clean,id_vars=\"diagnosis\",var_name=\"features\", value_name='value')\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90) \nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e00dbe8-10b3-4329-b199-50f398ba8c0d","_uuid":"e0787aae257d6799df220339e7a44e6a29381919","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":605},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":61798,"status":"ok","timestamp":1526061875895,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"fduseK5zvzn1","outputId":"e4dd4744-53b2-4ed1-bf4d-4cc092a8f430","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,8))\ndata = pd.melt(df_features_y_clean,id_vars=\"diagnosis\",var_name=\"features\", value_name='value')\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90) \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"775c62b3-4047-4a0b-9f01-4ca4b7969c7f","_uuid":"cdc2b7bedf400bcb71819879d9d6e70c493d7b5f","colab_type":"text","id":"EWxSUWbkqzEU"},"cell_type":"markdown","source":"#  Feature engineering\nI proceeded to select the features that are most discriminating (see swarmplot) among the most correlated features (see correlation plot). the following considerations were therefore made, \n\n* from  the correlated group [compactness_se, concavity_se  concave pointsse], I select **compactness_se**.\n* from the correlated group [compactness_worst, concavity_worst and concave], I select points_worst **concavity_point_worst**)\n* rom the correlated group [concavity_mean, compactness_mean, concavity_mean and concave points_mean] , I select **concavity_mean**)\n* from the correlated group [radius_worst, area_worst, perimeter_worst, radius_worst ] , I select **radius_worst**)\n* from the correlated group [perimeter_mean, area_mean, perimeter_mean, radius_mean ]  , I select **perimeter_worst**)\n* from the correlated group [area_se, area_se, perimeter_se, radius_se]  , I select **area_se**)"},{"metadata":{"_cell_guid":"6a809975-33b2-4bca-9221-51162b1611ba","_uuid":"f97ad4709f07ddafc16276ae308f482f09206dcf","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"-ibh-wbeAd_K","trusted":true},"cell_type":"code","source":"my_selected_features = ['compactness_se', \n                        'radius_worst', 'perimeter_mean', 'area_se',\n                         'texture_mean', 'texture_se', 'texture_worst',\n                        'concavity_mean', 'concavity_se', 'concavity_worst', 'concave points_mean','concave points_se','concave points_worst', \n                        'symmetry_mean', 'symmetry_se', 'symmetry_worst', 'fractal_dimension_mean', 'fractal_dimension_se', 'fractal_dimension_worst']\ndf_my_selected_features = df_features_clean[my_selected_features]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9592a88a-03f0-41a1-8725-1d175c6695b1","_uuid":"303e040b6cf763e6582931c4ddef7ad264cace32","colab_type":"text","id":"fA1axZrZ5UeY"},"cell_type":"markdown","source":"Although still under data exploration, once our dataset was simplified, it was already possible to obtain useful information.\nTherefore, a decision tree was created (depth 4 and Gini criterion), from the observation of which it was possible to make some important considerations."},{"metadata":{"_cell_guid":"a2a08855-ff5d-4f87-96b6-9470f43fc9fd","_uuid":"f4456aa7b7224f66cd8bea16a080f7c7e30d7665","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":5562,"status":"ok","timestamp":1526205301213,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"9pYXMAt0qE-9","outputId":"b50abe45-f274-451b-e371-9849a632639a","trusted":true},"cell_type":"code","source":"!pip install graphviz\n!apt-get -qq install -y graphviz && pip install -q pydot","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07e478d7-d166-46f2-b561-0004b9adbcf1","_uuid":"ab66cef92e55834ea51477f72b37f65dd5e900a1","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"q7pRDynp3Gyi","trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz \n\ndef plot_decision_tree(model, columns):\n    dot_data = tree.export_graphviz(model, out_file=None, \n                             feature_names=columns,  \n                             class_names=['Malignant','Benign'],  \n                             filled=False, rounded=True,  \n                             special_characters=False)  \n    graph = graphviz.Source(dot_data)  \n    return graph \n  \n    \ndef plot_importances_features(model, columns):\n  indices = np.argsort(model.feature_importances_)[::-1]\n  feat_imp = pd.DataFrame({'Feature':columns.values[indices],\n                        'Feature ranking':model.feature_importances_[indices]})\n  plt.rcParams['figure.figsize']=(8,12)\n  sns.set_style('whitegrid')\n  ax = sns.barplot(x='Feature ranking', y='Feature', data=feat_imp)\n  ax.set(xlabel='Feature ranking')\n  plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8e312a4-b980-46ad-858c-f9120d4427e1","_uuid":"aac0cc4058c5cf8be1c1b61b4c02a45b57519b0f","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":618},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":748,"status":"ok","timestamp":1526205302793,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"sVqf4VnapVzN","outputId":"57fa306d-22a7-415e-f4ea-138b8b5a2be8","trusted":true},"cell_type":"code","source":" # vediamo l'albero con le mie features\nclf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=8)  \nclf1.fit(df_my_selected_features.values, df_y.values)\nplot_decision_tree(clf1,my_selected_features)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b58f9b1f-7385-404e-82eb-386f3c74a881","_uuid":"831bc0c796e66c2cf7808036c98eaf47ff902e18","colab_type":"text","id":"ca_S--Bv5AmA"},"cell_type":"markdown","source":"the first consideration to make is that, as we hypothesized, the size of the nucleus is very discriminating for the purposes of classification;\n\nIn the decision tree, you can see the role of individual features in discriminating tumors.\n"},{"metadata":{"_cell_guid":"4dfd55ff-b02a-45bb-b862-e39ff0fee2c0","_uuid":"1457ca7702bb640438e7c6ed0673e87186cba088","colab_type":"text","id":"Ph5LJxBaZCPG"},"cell_type":"markdown","source":"## What are the most important features\n\nRandom Forests are often used for feature selection in a data science workflow. "},{"metadata":{"_cell_guid":"0abfd993-42b9-4a39-afe0-6fb03ace9e62","_uuid":"9f27fc3d9d4f4d19cc05e1a68420c30ac799e06f","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":748},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":2700,"status":"ok","timestamp":1526321681329,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"unLmGaG2u56L","outputId":"881c30d5-1b75-45ca-c4df-216dfbf92df8","scrolled":true,"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=1000)\n#Fit the model:\nmodel.fit(df_my_selected_features.values, df_y.values)\nplot_importances_features(model,df_my_selected_features.columns) # on my_selected_features\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f73c9809-b2f3-4dce-a530-a846977b506d","_uuid":"343c2123d0a33a467682760d2378a957f9298a6f","colab_type":"text","id":"L0eqkdbGY4gi"},"cell_type":"markdown","source":"the most important features for reduced dataset are: radius_worst, concave points_worst,radius_worst,concave points_mean, perimeter_mean, concavity_mean.\n\nlet's see the most important features on the whole dataset:\n\n"},{"metadata":{"_cell_guid":"e3ff7053-2512-4f58-85ad-f7c7f0a12d88","_uuid":"dc46e959fea2a37f11d54ba5d07cb7265ff9504e","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":748},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":3036,"status":"ok","timestamp":1526321692914,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"JJrCwVxy2YZx","outputId":"feb939a4-a824-41f4-fc1b-0a538b9f6aee","trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=1000)\n#Fit the model:\nmodel.fit(df_features_clean.values, df_y.values) \nplot_importances_features(model, df_features.columns)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8f0931e-21e5-44db-b6ff-496d278096af","_uuid":"bb695fa1443806b3a115ab3bc3d4d6dee68329c3","colab_type":"text","id":"3aQxI-G77UZf"},"cell_type":"markdown","source":"The most important features are: perimeter_worst, area_worst, concave points_worst, radius_worst,concave points_mean \n"},{"metadata":{"_cell_guid":"1ae1d527-b9fc-408a-8d3b-c841ff7784c5","_uuid":"381311ef1fe36f1db08601a64c37f6d8e283308e","colab_type":"text","id":"cq6t-EbPtv_k"},"cell_type":"markdown","source":"## PCA,TSE, MDS Dimensionality Reduction"},{"metadata":{"_cell_guid":"36f6332d-c027-4d2c-9387-c7261ef9a85e","_uuid":"9d4c2660c0a1b4ac2a5dcf41f1c20d873ab8eb8a","colab_type":"text","id":"sSEIxfvPywfV"},"cell_type":"markdown","source":"Why is dimensionality reduction useful? \n* Too many variables can cause such problems below\n* Increased computer throughput\n* Too complex visualization problems\n* Decrease efficiency by including variables that have no effect on the analysis\n* Make data interpretation difficult\n\nContinuing the exploratory process aimed at understanding how to simplify the dataset, without losing relevant information, the following processes were applied: PCA, Cosine PCA, TSNE and MDS, which are techniques for reducing complexity; in particular:\n\n*  PCA (Principal Component Analysis) is based on two basic considerations: high correlation between variables indicates redundancy in the data; the most important variables express higher variance. Based on these considerations, the model simplifies the complexity of the variables;\n* TSNE (T-distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique that is particularly suited to reducing the complexity of multidimensional datasets in a two- or three-dimensional model;\n* MDS (MultiDimensional Scaling) is a statistical analysis technique often used to graphically show differences or similarities between elements of a collection. Starting from a square matrix, containing the \"similarity\" of each row element with each column element, the multidimensional scaling algorithm assigns each element a position in a N-dimensional space, with N established a priori. This technique therefore starts with a system as large as the elements of the system, and reduces the dimensions by up to a certain number.\n\n\n"},{"metadata":{"_cell_guid":"6943e932-741a-4596-b142-6e0840f8e3fe","_uuid":"8e9a0bad981628ad59841ce3c6a61a3387268590","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"AzLuWvhQ-pHF","trusted":true},"cell_type":"code","source":"# Turn dataframe into arrays\nX = df_features_clean.values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bd58f433-9f48-4d08-a0b3-9c6f7ec4b9bf","_uuid":"c009baf8ccc03b82f38602c25c4d198d30b3919d","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":555},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":28485,"status":"ok","timestamp":1526121276528,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"MK7IszJMT8TD","outputId":"2ded68db-b6e2-42a5-ab72-5211dfdf1df4","trusted":true},"cell_type":"code","source":"# Invoke the PCA method. Since this is a binary classification problem\n# let's call n_components = 2\npca_2d = PCA(n_components=2)\npca_2d_results = pca_2d.fit_transform(X)\n\n# Invoke the TSNE method\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=2000)\ntsne_results = tsne.fit_transform(X)\n\npca_cos_2d = KernelPCA(n_components=2, kernel = 'cosine')\npca_cos_2d_results = pca_cos_2d.fit_transform(X)\n\n\nmds = MDS(2, max_iter=100, n_init=1)\nmds_results = mds.fit_transform(X)\n\n# Plot the TSNE and PCA visuals side-by-side\ncmap = {'B':'green','M':'blue'}\nfig = plt.figure(figsize = (15,9))\n\n\nplt.subplot(2, 2, 1)\nplt.title('PCA Scatter Plot')\nplt.scatter( pca_2d_results[:,0], pca_2d_results[:,1], c = [cmap[x] for x in df_y.values] ,alpha=0.75)\n\nplt.subplot(2, 2, 2)\nplt.title('Kernel Cosine PCA Scatter Plot')\nplt.scatter( pca_cos_2d_results[:,0], pca_cos_2d_results[:,1], c =[cmap[x] for x in df_y.values] ,alpha=0.75)\n\nplt.subplot(2, 2, 3);\nplt.title('TSNE Scatter Plot')\nplt.scatter(tsne_results[:,0], tsne_results[:,1],  c =[cmap[x] for x in df_y.values] , alpha=0.75)\n\n\nplt.subplot(2, 2, 4)\nplt.title('MDS Scatter Plot')\nplt.scatter(mds_results[:,0], mds_results[:,1],  c =[cmap[x] for x in df_y.values] , alpha=0.75)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e919eef3-daaf-41ae-9696-2e88366a9817","_uuid":"48ba19cd21d3588219f3792aad3647af9d9e1d8b","colab_type":"text","id":"iwTkf9ZS9Cpy"},"cell_type":"markdown","source":"From a first observation of the results of the plots, it can be deduced that the data are linearly separable and therefore a linear model could be an excellent solution to solve a classification problem.\n"},{"metadata":{"_cell_guid":"241695a7-371e-40b3-ac5e-94b0bdd5c4f4","_uuid":"19d20a9e86815682cbb660b23571c4a00ce3433b","colab_type":"text","id":"fsm9WnrEamXo"},"cell_type":"markdown","source":"**In the PCA, the next question is “how many principal components are we going to choose for our new feature subspace?\n\nAnother question that arose was how many main components we should have chosen for our new function subspace. Usually, in order to identify the number of components to be used, we try to follow the two criteria together.\n\nAccording to the first criterion, a number of principal components must be considered such that they take into account a sufficiently high percentage of total variance (at least 70%, for example). When defining the minimum percentage of acceptable variance, the number of original variables should be taken into account, so that as the number of variables increases, a lower percentage of explained variance may be accepted.\n\n\n\n"},{"metadata":{"_cell_guid":"4dc41695-f8e7-4b49-b7ae-d7cf9fee8511","_uuid":"b823db3eeb01e416e4b90d64375a38819109c242","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":513},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":779,"status":"ok","timestamp":1526491433576,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"E1CAjMoSsxKs","outputId":"b1f9ca84-2d29-4766-94f3-eb3d66ff01b7","trusted":true},"cell_type":"code","source":"\"\"\"e next question is “how many principal components are we going to choose for our new feature subspace?” \nA useful measure is the so-called “explained variance,” which can be calculated from the eigenvalues.\nThe explained variance tells us how much information (variance) can be attributed to each of the principal components.\"\"\"\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(X)\n\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(1, figsize=(14, 7))\n    plt.bar(range(1,31,1), pca.explained_variance_ratio_, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(1,31,1),pca.explained_variance_ratio_.cumsum(), where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.title(\"Around 95% of variance is explained by the Fisrt 10 colmns \");\n    plt.legend(loc='best')\n    plt.axhline(y=0.7, color='r', linestyle='-')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45991dcf-22cf-4aad-9c6c-6b2d3ce85702","_uuid":"bf1e3f4bb3f519a60e2fad0351da30afc5ed676c","colab_type":"text","id":"vdMBpPhNvYl8"},"cell_type":"markdown","source":"The second criterion uses a graph called scree-graph of eigenvalues as a function of the number of principal components. As the eigenvalues are decreasing, the graph takes the form of a broken graph with always negative slope. Analyzing the graph it will be possible to notice a point in which a sudden variation of slope occurs, in correspondence with which the number k of principal components to consider is identified. \n"},{"metadata":{"_cell_guid":"8f10b241-05c1-46fb-97af-5e0770722126","_uuid":"3f51ea15f20b1c4402d7999aec71a92fe88e7555","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":349},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":545,"status":"ok","timestamp":1526491502788,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"e00BYvpYjGvR","outputId":"6ef4e258-2925-4e6e-a921-42ca3bd9cf36","trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n#Make a random array and then make it positive-definite\nnum_vars = 30\n\n\nA = X\nU, S, V = np.linalg.svd(X) \neigvals = S**2 / np.cumsum(S)[-1]\n\nfig = plt.figure(figsize=(8,5))\nsing_vals = np.arange(num_vars) + 1\nplt.plot(sing_vals, eigvals, 'ro-', linewidth=2)\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Eigenvalue')\n\nleg = plt.legend(['Eigenvalues from SVD'], loc='best', borderpad=0.3, \n                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n                 markerscale=0.4)\nleg.get_frame().set_alpha(0.4)\nleg.draggable(state=True)\nplt.axvline(x=3, color='b', linestyle='-')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e43594ba-9aaf-4cc2-a9b4-23b516423ed4","_uuid":"cc0d88534e54067ecdf52f0778a37dbf4cc5c438","colab_type":"text","id":"pLECzaQa_CP9"},"cell_type":"markdown","source":"A good  number of principal component is 3 with the explained variance of 74%"},{"metadata":{"_cell_guid":"afc2e91c-01d6-483b-a66c-5a6a61bf4df1","_uuid":"5d9e2a01476c74b48f624f3df86b0e7e8004962a","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"MAaQE9GrkN5_","trusted":true},"cell_type":"code","source":"import math\n\ndef get_important_features(transformed_features, components_, columns):\n    \"\"\"\n    This function will return the most \"important\" \n    features so we can determine which have the most\n    effect on multi-dimensional scaling\n    \"\"\"\n    num_columns = len(columns)\n\n    # Scale the principal components by the max value in\n    # the transformed set belonging to that component\n    xvector = components_[0] * max(transformed_features[:,0])\n    yvector = components_[1] * max(transformed_features[:,1])\n\n    # Sort each column by it's length. These are your *original*\n    # columns, not the principal components.\n    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n    print (\"Features by importance:\\n\", important_features)\n    return important_features\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f6d10e4-9d77-433c-835f-3a08975d8c3f","_uuid":"7eea6c6009ef82a7b2a5e255c6d95e1842ca9680","colab_type":"text","id":"dJ_KadSl_lQ7"},"cell_type":"markdown","source":"No, I want to know what are the most import features for PCA\n"},{"metadata":{"_cell_guid":"87c02f91-d86e-490e-bb1d-64b029586a15","_uuid":"2a702a6d207fa592f967075313998fb0d2dd0774","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":74},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":538,"status":"ok","timestamp":1526121328571,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"wIHKsSnOkV_9","outputId":"2d585b37-b80d-43d2-8e58-014bc97635d0","trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2, svd_solver='full')\npca_2d_results = pca.fit_transform(X)\n\ncomponents = pd.DataFrame(pca.components_ , columns = df_features.columns, index=[1, 2])\n\nimportant_features = get_important_features(pca_2d_results, pca.components_, df_features.columns.values)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ed8a55e-c97f-40f2-a15f-2b459d2d8b23","_uuid":"9aa800368428623562f7d8668c20df3b0809748b","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":773},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":957,"status":"ok","timestamp":1526121497394,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"pj081dkcbtc6","outputId":"5b369e47-5d15-464a-d51b-bb87f2793f4a","trusted":true},"cell_type":"code","source":"r,f = zip(*important_features)\n\nfeat_imp = pd.DataFrame({'Feature':f,\n                        'Feature ranking':r})\nplt.rcParams['figure.figsize']=(8,12)\nsns.set_style('whitegrid')\nax = sns.barplot(x='Feature ranking', y='Feature', data=feat_imp)\nax.set(xlabel='Feature ranking')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f7c2687-eec9-496a-b1cd-33fd3866d507","_uuid":"4c1cc8147a61619f53aa46ba4cbc833a3c400ecc","colab_type":"text","id":"_5cfRLwHyBSp"},"cell_type":"markdown","source":"   This funtion will project your *original* features\n    onto your principal component feature-space, so that you can\n    visualize how \"important\" each one was in the\n    multi-dimensional scaling"},{"metadata":{"_cell_guid":"332cfa3f-36a6-44e0-b671-46b84f57a39e","_uuid":"03d2fec109b1f4f4175a66b4d0c8986a2edc6dc5","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"qzx1si9ft_JO","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\ndef draw_vectors(transformed_features, components_, columns, lenght):\n    \"\"\"\n    This funtion will project your *original* features\n    onto your principal component feature-space, so that you can\n    visualize how \"important\" each one was in the\n    multi-dimensional scaling\n    \"\"\"\n    \n    num_columns = len(columns)\n\n    # Scale the principal components by the max value in\n    # the transformed set belonging to that component\n    xvector = components_[0] * max(transformed_features[:,0])\n    yvector = components_[1] * max(transformed_features[:,1])\n\n    ax = plt.axes()\n\n    for i in range(num_columns):\n    # Use an arrow to project each original feature as a\n    # labeled vector on your principal component axes\n        if math.sqrt(xvector[i]**2 + yvector[i]**2) >lenght: # solo vettori maggiori di lenght\n          plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0005, head_width=0.02, alpha=0.75)\n          plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='b', alpha=0.45)\n\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2970b7f1-a463-48a6-ba42-afe8e8f1d94c","_uuid":"ff255b70bc3a082c042a6323f4e45cbd881a3df9","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":499},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1255,"status":"ok","timestamp":1526121557891,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"QUqJiaE1uAV5","outputId":"c662072e-300f-4612-c44c-8cc6aedcf0d7","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,8))\n\n\nax = draw_vectors(pca_2d_results, pca.components_, df_features.columns.values,3.0)\n#ax.set_ylim([-3,-1])\n\nT_df = pd.DataFrame(pca_2d_results)\nT_df.columns = ['component1', 'component2']\n\nT_df['color'] = 'y'\n\n\nplt.xlabel('Principle Component 1')\nplt.ylabel('Principle Component 2')\nplt.scatter(T_df['component1'], T_df['component2'], color=T_df['color'], alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7daa6dff-42db-4412-b6fe-2eb01ca8c698","_uuid":"ce98a6ef33a177ff81f0844003cbaf57167f643f","colab_type":"text","id":"VGUHmZH6ACG2"},"cell_type":"markdown","source":"The most important features for PCA are: radius worst, perimeter worst, perimeter mean, radius mean and area worst\n"},{"metadata":{"_cell_guid":"9ede6f6d-4d20-421e-b064-db41ff5f9ad3","_uuid":"5fd964234cedb76eb508f496d2adfdb2906c6607","colab_type":"text","id":"s-BrG81hz8XX"},"cell_type":"markdown","source":"# Classification\n\nThis section will describe the procedures and the various models used for the classification the type of tumor.\nIn particular, after having divided the data into two set (70%/30% for the training and testing) for each model I analyzed:\n-\tthe validation curve plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values. If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well [http://scikit-learn.org/stable/modules/learning_curve.html].\n-\tthe learning curve that compares the performance of a model on training and testing data over a varying number of training instances [http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html] ;\n-\tI trained the model using the entire dataset (which we called Real Data) and a reduced dataset composed of 3 principal component (which we called PCA 3D)\n-\t Boundary Line and Boundary Line Probabilistic, illustrate the nature of decision boundaries of different classifiers . [http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html]\n-\tthe performance and validated the model with the Cross-Validation. \nFinally I plot Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality [http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html].\n\nIn the medical field, the value of Recall is considered a primary value, as the goal is to identify all cases of really positive (malignant tumor) minimizing false negative (e.g. I predict benign tumor but the patient has a malignant tumor). In our case the malignant M tumors have positive class and those benign B negative class.\n"},{"metadata":{"_cell_guid":"3a912198-0f71-4421-8b15-98bce803fd03","_uuid":"0e22b64a5c08c6f9c215f14cf77e6ca0a69e7ada","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"toIY4YXNAsx0","trusted":true},"cell_type":"code","source":"#Learning curve\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom matplotlib.colors import ListedColormap\n\nfrom matplotlib import cm\nfrom sklearn.decomposition import PCA,KernelPCA # Principal Component Analysis module\n\n\ndf_features = df_features_clean\ny = df_y\n# 70/30 \ny = df_y.map({'M':1,'B':0})\nx_train, x_test, y_train, y_test = train_test_split(df_features.values,y.values, test_size = .3, stratify =y, random_state=34)\n\n#70/30 cross validation\ncv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=34)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ff6c81f-e2e3-4ec5-932a-049eb8a779bd","_uuid":"f21e4d76adf57f61ca6a60ac93256bd1f8d4cc0b","colab_type":"text","id":"0y6zRuG8BMDZ"},"cell_type":"markdown","source":"## Plotting functions\n\nHere, I defined some functions to evaluate the performance of a choosen model (roc_curve_model, validation_curve_model,learning_curve_model,scores_plot, boundaryline, plot_confusion_matrix)"},{"metadata":{"_cell_guid":"186c219c-7d6f-4f98-aa5c-7e2f81f57314","_uuid":"200d8264f2291819071a377bd02219119214c467","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"ankSqHiFBQj0","trusted":true},"cell_type":"code","source":"\n#validation curve\ndef validation_curve_model(X, Y, model, param_name, parameters, cv, ylim, log=True):\n\n    train_scores, test_scores = validation_curve(model, X, Y, param_name=param_name, param_range=parameters,cv=cv, scoring=\"accuracy\")\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n   \n\n    plt.figure(figsize = (6,4))\n    plt.title(\"Validation curve\")\n    plt.fill_between(parameters, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(parameters, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n\n    if log==True:\n        plt.semilogx(parameters, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n        plt.semilogx(parameters, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n    else:\n        plt.plot(parameters, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n        plt.plot(parameters, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n\n    if ylim is not None:\n        plt.ylim(*ylim)\n\n    plt.ylabel('Score')\n    plt.xlabel('Parameter '+param_name)\n    plt.legend(loc=\"best\")\n    \n    return plt\n  \n# Learning curve\ndef learning_curve_model(X, Y, model, cv, train_sizes, ylim):\n\n    plt.figure(figsize = (6,4))\n    plt.title(\"Learning curve\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n\n    train_sizes, train_scores, test_scores = learning_curve(model, X, Y, cv=cv, n_jobs=4, train_sizes=train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    \n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n    if ylim is not None:\n        plt.ylim(*ylim)                  \n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42961b30-46a0-4c2d-ae92-a65a97f8a646","_uuid":"9170b5c193193160617d8ac53c82d6b84e1277ce","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":241},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":2955,"status":"ok","timestamp":1526573189123,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"dPrsvtK6BZJW","outputId":"631687d7-8593-4999-ef35-fbc2bd4d663e","trusted":true},"cell_type":"code","source":"!pip install mlxtend #plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2022a51a-946e-433d-bd22-680ca28a4deb","_uuid":"d22f285e6a10bbd06d92f47b0c67015a7667a840","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"upu_oZKpBm7z","trusted":true},"cell_type":"code","source":"from mlxtend.plotting import plot_confusion_matrix\n# for visualization purpose\ndef BoundaryLine(kernel, algo, algo_name):\n    reduction = KernelPCA(n_components=2, kernel = kernel)\n    x_train_reduced = reduction.fit_transform(x_train)\n    x_test_reduced = reduction.transform(x_test)\n    \n    classifier = algo\n    classifier.fit(x_train_reduced, y_train)\n    \n    y_pred_reduced = classifier.predict(x_test_reduced)\n\n    \n    plt.figure(figsize = (14,6))\n    plt.subplot(221)\n    #Train set boundary\n    X_set, y_set = x_train_reduced, y_train\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                         np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n                 alpha = 0.6, cmap = ListedColormap(( 'green','red')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    \n    \n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                    c =  ListedColormap(( 'green','red'))(i), label = j)\n        \n    plt.title('{} Boundary Line with {} PCA (Train Set)' .format(algo_name, kernel))\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.legend()\n    \n    \n    plt.subplot(222)\n    #Test set boundary\n    X_set, y_set = x_test_reduced, y_test\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                         np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n                 alpha = 0.6, cmap = ListedColormap(( 'green','red')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                    c =  ListedColormap(( 'green','red'))(i), label = j)\n        \n    plt.title('{} Boundary Line with {} PCA (Test Set)' .format(algo_name, kernel))\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.legend()\n    \n    \n   \n    \n    plt.subplot(223)\n    cmap  = cm.get_cmap('RdYlGn_r')\n    \n    \n    x_min, x_max = x_train_reduced[:, 0].min() - .5, x_train_reduced[:, 0].max() + .5\n    y_min, y_max = x_train_reduced[:, 1].min() - .5, x_train_reduced[:, 1].max() + .5\n    X1, X2 = np.meshgrid(np.arange(x_min, x_max, .02),\n                     np.arange(y_min, y_max, .02))\n    \n    \n    Z = classifier.predict_proba(np.c_[X1.ravel(), X2.ravel()])[:, 1].reshape(X1.shape)\n\n    plt.contourf(X1, X2, Z, cmap = cmap, alpha=.6)\n\n    \n    for i, j in enumerate(np.unique(y_train)):\n      plt.scatter(x_train_reduced[y_train == j, 0], x_train_reduced[y_train == j, 1],\n                  c =  ListedColormap(( 'green','red'))(i), label = j)\n\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n            \n    plt.title('{} Boundary Line Proba with {} PCA (Train Set)' .format(algo_name, kernel))\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.legend()\n    \n    plt.subplot(224)\n    plt.contourf(X1, X2, Z, cmap = cmap, alpha=.6)\n    for i, j in enumerate(np.unique(y_test)):\n      plt.scatter(x_test_reduced[y_test == j, 0], x_test_reduced[y_test == j, 1],\n                  c =  ListedColormap(( 'green','red'))(i), label = j)\n      \n      \n\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n            \n    plt.title('{} Boundary Line Proba with {} PCA (Test Set)' .format(algo_name, kernel))\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.legend()\n    \n    plt.tight_layout(pad=2.4, w_pad=2.5, h_pad=1.0)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e8ab4b7-65ab-4a7e-a11a-a07a0747cada","_uuid":"5873f046e77879da18311d7f1ab91b59991108df","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"waXdca-NBv2a","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n\n#roc curve_model\ndef roc_curve_model(model,x_test,y_test, algo_name=''):\n  y_prob = model.predict_proba(x_test)[:,1] # This will give you positive class prediction probabilitie\n  false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\n  roc_auc = auc(false_positive_rate, true_positive_rate)\n\n  plt.figure(figsize=(5,5))\n  plt.title('ROC '.format(algo_name))\n  plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC '+algo_name+'= %0.2f' % roc_auc)\n  plt.legend(loc = 'lower right')\n  plt.plot([0, 1], [0, 1],linestyle='--')\n  plt.axis('tight')\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')\n  \n  return plt\n\ndef evaluate_cv(xvalues,yvalues, algo):\n    scores = cross_val_score(algo, xvalues ,yvalues, cv=10) #accuracy\n    scores_recall = cross_val_score(algo, xvalues,yvalues, cv=10,scoring='recall')\n    scores_f1 = cross_val_score(algo, xvalues,yvalues, cv=10,scoring='f1')\n    scores_average_precision= cross_val_score(algo,xvalues,yvalues, cv=10,scoring='average_precision')\n    print(\"Cross validation score           : \",scores.mean())  #accuracy\n    print(\"Cross validation recall          : \",scores_recall.mean()) \n    print(\"Cross validation f1 score        : \",scores_f1.mean())     \n    print(\"Cross validation precision score : \",scores_average_precision.mean())\n    \n    return scores_recall, scores\n\n  # plot Classifires' performances on real data and PCA 3D \ndef evalute_realdata_PCAreduction(algo, algo_name,npca=3, plot_cf=False):\n    reduction = PCA(n_components=npca) # in EDA we found that 3 PC is a good value\n    x_train_reduced = reduction.fit_transform(x_train)\n    x_test_reduced = reduction.transform(x_test)\n    \n   \n    algo.fit(x_train_reduced, y_train)\n    \n    y_pred_reduced = algo.predict(x_test_reduced)\n    \n      \n   \n    roc_curve_model(algo,x_test_reduced,y_test,' PCA' )\n   \n    print(\"\\t------\\t Reduced Data \"+algo_name+\" \\t-------\")\n    print(\"Confusion matrix PCA (3D):\\n\", confusion_matrix(y_test, y_pred_reduced))\n    print(classification_report(y_test, y_pred_reduced))\n    \n\n    scores_recall_pca, scores_accuracy_pca = evaluate_cv(reduction.fit_transform(df_features.values) ,y.values,algo)\n    \n    algo.fit(x_train, y_train)\n    y_pred = algo.predict(x_test)\n    print(\"\\n\\t------\\t Real Data \"+algo_name+\" \\t-------\")\n    print(\"Confusion matrix :\\n \", confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    \n    scores_recall, scores_accuracy = evaluate_cv(df_features.values ,y.values,algo)\n    \n     \n    \n    y_prob = algo.predict_proba(x_test)[:,1] # This will give you positive class prediction probabilitie\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    \n    plt.plot(false_positive_rate,true_positive_rate, color='green',label = 'AUC = %0.2f' % roc_auc)\n    \n    plt.legend()\n  \n    if plot_cf:\n      fig, ax =  plot_confusion_matrix(conf_mat= confusion_matrix(y_test, y_pred_reduced))  #PCA\n      ax.set_title(\"Confusion mat PCA\")\n      fig, ax =plot_confusion_matrix(conf_mat= confusion_matrix(y_test, y_pred))    #real data\n      ax.set_title(\"Confusion mat Real \")\n\n    return [scores_recall_pca, scores_accuracy_pca, scores_recall, scores_accuracy]\n    \ndef plot_scores(scores, algo_name):\n  n_groups = 2\n  pca_means = (scores[0].mean(),scores[1].mean())\n  real_means = (scores[2].mean(),scores[3].mean())\n  pca_std = (scores[0].std(),scores[1].std())\n  real_std = (scores[2].std(),scores[3].std())\n\n  # create plot\n  fig, ax = plt.subplots()\n  index = np.arange(n_groups)\n  bar_width = 0.35\n  opacity = 0.8\n\n  rects1 = plt.bar(index, pca_means, bar_width,\n                   alpha=opacity,\n                    yerr=pca_std,\n                   color='b',\n                   label='PCA 3D')\n\n  rects2 = plt.bar(index + bar_width, real_means, bar_width,\n                   alpha=opacity,\n                    yerr=real_std,\n                   color='g',\n                   label='Real Data')\n  for rects in [rects1,rects2]:\n    for rect in rects:\n          height = rect.get_height()\n          ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,                \n                   str(round((height)*100, 2))+'%',\n                  ha='center', va='bottom')\n\n  plt.xlabel('Metrics')\n  plt.ylabel('Scores')\n  plt.title('Scores '+algo_name)\n  plt.xticks(index + bar_width, ('Recall', 'Accuracy'))\n  \n  plt.legend((rects1[0], rects2[0]), ('PCA', 'Real'))\n\n  plt.tight_layout()\n  plt.show()      \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a8714bca-f8fd-45d4-8e14-7f5043916719","_uuid":"7ccffcfd7e4ec0d9a24762769149a0069f8132d6","colab_type":"text","id":"hIGngCe0BzWU"},"cell_type":"markdown","source":"## LogisticRegression\n\nFrom the previuos exploratory data  analysis, we  have seen that our data-set is linearly separable, so the first attempt at classification was made using the algorithm Logistic Regression.\n\n\"In statistics, the logistic model (or logit model) is a statistical model with input (independent variable) a continuous variable and output (dependent variable) a binary variable\" [wiki](https://en.wikipedia.org/wiki/Logistic_regression)"},{"metadata":{"_cell_guid":"03630c96-abfe-4fd3-8bcc-ad8ee08c1b54","_uuid":"4721980fb9d32fb8babbc98983a7a8cff9e3a68c","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"H41vXZChB2pj","trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\n#Import model\nfrom sklearn.linear_model import LogisticRegression #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cd2031d0-4d0b-46c4-8aec-40aa9f3425aa","_uuid":"ff0fca2ff64da5cfb5f1022500bb4c98c7cb0cf2","colab_type":"text","id":"mkmLjvY7B3BI"},"cell_type":"markdown","source":"Now, we analyze the influence of a single hyperparameter C (Inverse of regularization strength) on the training score and the validation score to find out whether the estimator is overfitting or underfitting. For C>0.5 the estimator is overfitting (as shown in the validation plot)"},{"metadata":{"_cell_guid":"4d6eb1cc-67bd-4228-9024-10d394a1fe21","_uuid":"020d131e22acf01f60bb25b04a652415a970d63e","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":315},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":6604,"status":"ok","timestamp":1526573203857,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"evEGmeUbB6jP","outputId":"a6e6b02b-c0a6-48cf-a226-277e8dcc92b5","trusted":true},"cell_type":"code","source":"logreg =  LogisticRegression()\nparam_range = np.logspace(-6, 3, 10)\nparam_name=\"C\"\nylim=[0.85, 1.02]\nvalidation_curve_model(df_features.values,df_y.values, logreg, \"C\", param_range, cv, ylim=ylim,log=True)\nplt.axvline(x=0.5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8bdf2a9c-6a42-4051-8571-355afc1dd4f0","_uuid":"55f31c34e987398be07b6f4ec15bcc54167f30d2","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":5252,"status":"ok","timestamp":1526572397417,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"mdea8pJiB9xR","outputId":"539fa8e8-4464-4e17-b340-963948cd4b83","trusted":true},"cell_type":"code","source":"logreg =  LogisticRegression(C=0.5)\ntrain_size=np.linspace(.1, 1.0, 15)\n\nlearning_curve_model(df_features.values,df_y.values, logreg, cv, train_size, ylim=ylim)\nplt.axhline(y=0.97)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"261ad55a-f1d0-4dcc-9e4e-87f96ff5e828","_uuid":"94accb487bfeef6af094d08e377e7d144e745d2d","colab_type":"text","id":"j5c9QgPsCBt-"},"cell_type":"markdown","source":"the Learning curve shows a convergence of the two curves around 150 examples with validation scores over 97%"},{"metadata":{"_cell_guid":"d816a43d-bf4d-4d61-86f0-557e3c446482","_uuid":"502fbd3a907e7cf09e91de0fd1a018ac176c182f","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":415},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":3921,"status":"ok","timestamp":1526492323042,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"af5LMw1LCBCI","outputId":"225be794-3e1f-41e4-bf6d-5859a9f63278","trusted":true},"cell_type":"code","source":"BoundaryLine('linear', logreg, \"Logistic Regression\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fccbc30e-f6b0-44bc-9bc6-f6ccac5fc9e8","_uuid":"eebb97476b6a41013d0b8f04cac4eadfda4f3822","colab_type":"text","id":"aeCrcUPBCKxj"},"cell_type":"markdown","source":"Boundary Line  show the results on he training  (left column) and  test set (right column). \nIn the yellow area, in the Boundary Line Probabilistic plot, the model has the same probability tho assign B(green) or M class (red) "},{"metadata":{"_cell_guid":"4c190486-2ab8-4a39-abb8-38261edf0338","_uuid":"07a0caa637463f7af7a260b99e12df7d2c734c26","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1156},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1122,"status":"ok","timestamp":1526576244835,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"VHmmC79gCQSp","outputId":"b3443194-a708-4604-b071-9177759d8b2f","trusted":true},"cell_type":"code","source":"logreg =  LogisticRegression(C=0.5)\nlogreg_scores = evalute_realdata_PCAreduction( logreg, \"Logistic Regression\")\nplot_scores(logreg_scores,\"Logistic Regression\")\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b06216e-3fcc-4d86-91f6-9a7ededf9580","_uuid":"1d33dbec9d10c18057f672457d5fd44f9834ca7e","colab_type":"text","id":"1BmB7FS2C60X"},"cell_type":"markdown","source":"With Logistic Regression on Real Data we obtained:\n* Cross validation score           : ** 0.985**\n* Cross validation recall          :  **0.97**"},{"metadata":{"_cell_guid":"617d1008-6b93-4156-a7a6-8196e9d364b0","_uuid":"6ba49f66558d373fc74ac4c778e7694b882f9b84","colab_type":"text","id":"-AlWlgU3rYku"},"cell_type":"markdown","source":"## K-Nearest Neighbor\n\n\"In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\" [ wiki](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n"},{"metadata":{"_cell_guid":"748cacf6-8282-41e5-ac4a-04100bf530f4","_uuid":"c43c71c5564904b2ff709e178abbbea951f568f1","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"uTZwMYyrrilW","trusted":true},"cell_type":"code","source":"#K-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier #http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b560358c-1ec0-4628-8402-d6e2eb26de57","_uuid":"0ada8741bfc6ec8b0feb80473b2a6402bd147453","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":13122,"status":"ok","timestamp":1526503480115,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"Iq-AXYyDC6ah","outputId":"4a7fb34c-c7a2-4582-eb44-62dd189818a4","trusted":true},"cell_type":"code","source":"kneigh =  KNeighborsClassifier()\n\nparam_range = range(1,10,1)\n\nparam_name=\"n_neighbors\"\n\nvalidation_curve_model(df_features.values,df_y.values, kneigh, param_name, param_range, cv, ylim=ylim,log=False)\n\nplt.axvline(x=7)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e5f2fda-c069-4743-8254-c33c0862afd5","_uuid":"469f387dca9c85a55ba66de1ece55886eb5f0647","colab_type":"text","id":"8YTc6eQTs9Iv"},"cell_type":"markdown","source":"Results on training and on the validation set are similar. From the point highlighted on the graph,   curves tend to converge (parameter number of neighbors = 7); "},{"metadata":{"_cell_guid":"e82b2b9f-6b79-4e4f-9f37-db77551214ea","_uuid":"22cc775a88a9b6c7e37afb8d7605933de2983a1a","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":9309,"status":"ok","timestamp":1526503637080,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"XsuhCCXBrx6u","outputId":"4f060d8e-27c4-41ae-b453-b2e63303dbd0","trusted":true},"cell_type":"code","source":"kneigh =  KNeighborsClassifier(n_neighbors=7)\ntrain_size=np.linspace(.1, 1.0, 15)\nlearning_curve_model(df_features.values,df_y.values, kneigh, cv, train_size, ylim=ylim)\nplt.axhline(y=0.96)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b1bccc1-1ad4-4b6b-8f78-ce8d251fadbf","_uuid":"0ec3ba2c34dde7f4bd9e5bc6729296158baee52b","colab_type":"text","id":"HXNLtahTtKfI"},"cell_type":"markdown","source":"the performances are high (96%) and do not change from 250 "},{"metadata":{"_cell_guid":"481f5d7d-38ce-42da-8ac9-ead2f07b6609","_uuid":"59d010b5c040ba29770acb0363bf55f633653389","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":415},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":13581,"status":"ok","timestamp":1526503755499,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"_4L4XGm7tTir","outputId":"0aba35c5-0405-4cdf-ef9b-afefd35af07b","trusted":true},"cell_type":"code","source":" BoundaryLine(\"linear\", kneigh, \"KNN\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc1f827d-3730-4934-a39e-b168f857cf2e","_uuid":"2bd64b20d23928d3520c381bcfa163a70ae6dc03","colab_type":"text","id":"x4QMyUB6tqpm"},"cell_type":"markdown","source":"Boundary Line is not regular because it labels an element based on the characteristics of the neighboring elements."},{"metadata":{"_cell_guid":"6e150bd5-b56c-4d58-90f5-26968de54c04","_uuid":"263a927c45e46ae82e2251def5535a4e9d1b8190","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1156},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1134,"status":"ok","timestamp":1526575255697,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"r_YJKQL9snf4","outputId":"ee7dad24-c2a7-46f8-d3af-64edb6593b0d","trusted":true},"cell_type":"code","source":"kneigh =  KNeighborsClassifier(n_neighbors=7)\nkneigh_scores = evalute_realdata_PCAreduction( kneigh, \"KNN\")\nplot_scores(kneigh_scores,\"KNN\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"062a0a9b-0218-41a0-8067-52396c09ffea","_uuid":"1dbb7c4bdbd5a059c2a4b67a0ab0406bc3e2a1f2","colab_type":"text","id":"dSwRJ94puBWa"},"cell_type":"markdown","source":"There is a big difference in performance between PCA and Real Data\n\nWith K-Nearest Neighbor on Real Data we obtained:\n\n* Cross validation score : **0.967**\n* Cross validation recall : **0.927**"},{"metadata":{"_cell_guid":"7f263108-ef15-4ffc-94c2-fa0752faa359","_uuid":"09405610ab8a21d64f5e187e3e7fbe0877c43aab","colab_type":"text","id":"q62LTQISuCFE"},"cell_type":"markdown","source":"## Random Forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. [wiki](https://en.wikipedia.org/wiki/Random_forest)"},{"metadata":{"_cell_guid":"51c32453-f2f8-411b-968d-6019fd8873ae","_uuid":"2160bb63c3b6ade712483c2dba67c862ed51e31c","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"GNVZMztM-e8F","trusted":true},"cell_type":"code","source":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a0cf11c2-5ca0-46a4-b98d-02551beef3d7","_uuid":"634ecaf4cc48ed2b111b42e7e94c1fdd6475ab76","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":127338,"status":"ok","timestamp":1526504248970,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"mYoyUqEXuA20","outputId":"718f8b34-af3b-4d3f-efb1-6b5c69d355d2","trusted":true},"cell_type":"code","source":"r_forest = RandomForestClassifier(n_estimators=80) \nparam_range = np.linspace(1, 21, 10).astype(int)\nparam_name=\"min_samples_leaf\"\nvalidation_curve_model(df_features.values,df_y.values, r_forest, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=11)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9503438-f809-463d-b76e-df1162e61755","_uuid":"fd6ee2f70b767858d56c162e567e6646bea9a57e","colab_type":"text","id":"_2GH5nJ5u7Uz"},"cell_type":"markdown","source":"I evaluated the fitting by varying a pruning parameter. A good value is 11. \n\nWe note that as the \"min_samples_leaf\" parameter grows we may encounter underfitting"},{"metadata":{"_cell_guid":"f4ec800b-3e6f-45e7-bd67-349a81d0bfa8","_uuid":"90e6cf02dd1b96223a235bcc7435bd5d70985eaa","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":138421,"status":"ok","timestamp":1526504559333,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"2XXRQdgmuP_G","outputId":"cfa7df31-b438-44aa-e454-475713fbdee9","trusted":true},"cell_type":"code","source":"r_forest = RandomForestClassifier(n_estimators=80,min_samples_leaf=11)\nlearning_curve_model(df_features.values,df_y.values, r_forest , cv, train_size, ylim=ylim)\nplt.axhline(y=0.94)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70a467d0-9c7e-4a3b-ab40-dfc23503e0a3","_uuid":"069c978629ab59f8f8fb52d602199b0afdf7b93e","colab_type":"text","id":"KLYvFX7hvlsS"},"cell_type":"markdown","source":"learning curve: training and validation sets converge for high values"},{"metadata":{"_cell_guid":"845c9278-69ed-4b42-a811-69ecd00f921a","_uuid":"b232a5c7769410a91de5736e54c463d7a2f365e7","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":415},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":23553,"status":"ok","timestamp":1526504410438,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"M2Lts93YvRK_","outputId":"697992aa-ca41-4477-edcb-7a59a2e76efe","trusted":true},"cell_type":"code","source":" BoundaryLine('linear',r_forest, \"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fa5e02f0-fdfd-450f-acc0-fc42fa22cbce","_uuid":"1e2bb8c07a359472c90dbb36a25ffc1b086b9e79","colab_type":"text","id":"5M4FzLpxv_ki"},"cell_type":"markdown","source":"The boundary demarcation line is the clear sign of the superposition of n models (70) "},{"metadata":{"_cell_guid":"8200973a-46b5-43ca-8ac5-6fa82499a49e","_uuid":"21bd631602f96fa83a62cf5523866b6eb500a268","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1156},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":10276,"status":"ok","timestamp":1526575297498,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"3vdxw5g1vjj3","outputId":"d74ccd8f-0d73-485f-d15a-2e46fb084a22","trusted":true},"cell_type":"code","source":"r_forest = RandomForestClassifier(n_estimators=80,min_samples_leaf=11)\nr_forest_scores = evalute_realdata_PCAreduction( r_forest, \"Random Forest\")\nplot_scores(r_forest_scores,\"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a10d6f4-f48e-4a5f-b4d6-c788a4d4afa6","_uuid":"919bff293a0ad0e0e10d11aa0bea9680bc3b316c","colab_type":"text","id":"xcKIaVWxgbBZ"},"cell_type":"markdown","source":"With Random Forest on Real Data we obtained:\n\n* Cross validation score : **0.95**\n* Cross validation recall : **0.92**"},{"metadata":{"_cell_guid":"e51a0346-5b88-441a-b467-92af3998b43e","_uuid":"d038a151c09f2cfd220fe123e95974233d6210f6","colab_type":"text","id":"EdK6pp23wCna"},"cell_type":"markdown","source":"## Support Vector - linear"},{"metadata":{"_cell_guid":"49484ff6-dc21-4afd-ae53-a2ff5ecb8c0a","_uuid":"e94aa42f90f9a091c9ad008a013d022d24570afc","colab_type":"text","id":"3edfr2Q2bY2k"},"cell_type":"markdown","source":"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting).  [wiki](https://en.wikipedia.org/wiki/Support_vector_machine)"},{"metadata":{"_cell_guid":"7daa8ce8-396e-4b8d-b883-a9aae8054583","_uuid":"e68c7a302c586aa31efa3c3c843f4303dcb3e9a0","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"RXEQBXh9wHf9","trusted":true},"cell_type":"code","source":"#Support Vector - linear\nfrom sklearn.svm import SVC # http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a53e9d2-ba8f-475c-b501-58b453ea30af","_uuid":"27ac033d0f09d7da844e6c5736e64742af20476f","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":7980,"status":"ok","timestamp":1526504609002,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"VowNqaoQwUw3","outputId":"d543a634-8a1f-4078-fb70-8bef59593f6d","trusted":true},"cell_type":"code","source":"param_range = np.linspace(0.01, 5, 15)\nparam_name=\"C\"\nscvl = SVC(kernel = 'linear')\nvalidation_curve_model(df_features.values,df_y.values,scvl, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=0.01)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e5f8d04-b0a7-4b39-9fbe-936ccdde72f6","_uuid":"7a6b1298ace6bc89cd183badb9c680f25f9e1f22","colab_type":"text","id":"TuvUwMAhcK9w"},"cell_type":"markdown","source":"a good value of the parameter C is 0.01. increasing the value of the parameter we obtain the overfitting"},{"metadata":{"_cell_guid":"14261770-1868-4785-b014-9d9172292e0d","_uuid":"162dd572905d0a2b99b9aecf7f09cb4fc726174e","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":4029,"status":"ok","timestamp":1526504669120,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"O3JfwUOJwhda","outputId":"7397ef21-2f45-4891-ffc8-c87596ec6aea","trusted":true},"cell_type":"code","source":"scvl = SVC(kernel = 'linear', C=0.01)\nlearning_curve_model(df_features.values,df_y.values, scvl, cv, train_size, ylim=ylim)\nplt.axhline(y=0.96)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d08057e3-7537-4bb2-aa56-8c8cc7d5b7eb","_uuid":"14a8c8a1f44ba28e9390e26ec607f4883083f916","colab_type":"text","id":"Ah99xS6IcUX0"},"cell_type":"markdown","source":"learning curve : already with 250 we have high score"},{"metadata":{"_cell_guid":"bafc879d-e6ee-4f27-b914-a90242a8f7f8","_uuid":"dc1545bcab36d8affebce8f55a0c0213a8df0af4","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":415},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":8251,"status":"ok","timestamp":1526504897633,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"FbKDFK-OxWq3","outputId":"d3ee047e-0115-425a-8992-2ad19c67aac2","trusted":true},"cell_type":"code","source":"svcl = SVC(kernel = 'linear',C=0.01, probability=True)\nBoundaryLine('linear',svcl, \"Support Vector - linear\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ab3f928-04a8-4294-bdf9-3552b5a82830","_uuid":"049820c90139603baa7c6f2b2dcc77c396691c20","colab_type":"text","id":"cQCNO-QjcucL"},"cell_type":"markdown","source":"An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall."},{"metadata":{"_cell_guid":"b610b6fe-ff35-4b48-8060-d2c01f69bac5","_uuid":"f6fe219a0199c888f28290dee14f3bd13b624d25","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1156},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1745,"status":"ok","timestamp":1526576345974,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"RhomZyNnx0fL","outputId":"757bd7c0-7de6-4093-d71a-5eecfca5574d","trusted":true},"cell_type":"code","source":"svcl_scores = evalute_realdata_PCAreduction( svcl, \"Support Vector - linear\")\nplot_scores(svcl_scores,\"Support Vector - linear\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24ae4872-0c4a-4ee6-8de5-d90e1a101806","_uuid":"c0badf20e08f788caca8a4a33ea64bb375384995","colab_type":"text","id":"gA3tQIIkcxQY"},"cell_type":"markdown","source":"Performance does not change much between PCA and real data (as you seen in the ROC curves).\n\nWith Support Vector Linear on Real Data we obtained:\n\n* Cross validation score : **0.966**\n* Cross validation recall : **0.91**"},{"metadata":{"_cell_guid":"cc45a857-3f76-4480-bbd1-b5a1637a3f71","_uuid":"344bcc52bf65bacf65f7b570274b2cbfe5650a63","colab_type":"text","id":"yjCVaYwmy-zG"},"cell_type":"markdown","source":"## SVM - rbf\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n\nThe validation curve illustrates the effect of the parameter C of the Radial Basis Function (RBF) kernel SVM."},{"metadata":{"_cell_guid":"8c675705-cc64-48cb-8857-389afcccc7b1","_uuid":"0e825b1a5567e17fc800752dd78883bc8beef083","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":14052,"status":"ok","timestamp":1526505335319,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"5idb58L0y9xl","outputId":"9dd96b42-dce9-49f1-aaed-8bf6c603a53b","trusted":true},"cell_type":"code","source":"param_range = np.linspace(0.1, 8, 15)\nparam_name=\"C\"\nscvr = SVC(kernel = 'rbf')\nvalidation_curve_model(df_features.values,df_y.values,scvr, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39e5135e-2323-4f4a-9f1b-6b1121cf484c","_uuid":"3d23636da162c9bc8aca72d91909c17fa3b1993b","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":5020,"status":"ok","timestamp":1526505514630,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"CY65ktUxzU2D","outputId":"1c0e10d3-d37d-4607-dc69-45a4ec617ad8","trusted":true},"cell_type":"code","source":"svcrbf = SVC(kernel = 'rbf', C=5)\nlearning_curve_model(df_features.values,df_y.values, svcrbf, cv, train_size, ylim=ylim)\nplt.axhline(y=0.97)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc13e0ef-5f39-4f0e-a80a-0747fa356a10","_uuid":"4ec9bb5b42b06ea8b34125104cc88bad945e96b5","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":415},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":28707,"status":"ok","timestamp":1526575389259,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"Y2JJ0cvL0ooU","outputId":"c76991c3-aba7-4828-9527-e47900a050b4","trusted":true},"cell_type":"code","source":"svcrbf = SVC(kernel = 'rbf',probability=True, C=5)\nBoundaryLine('linear',svcrbf, \"Support Vector - rbf\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b559327-823e-4f63-a0ce-4934ce96b2e5","_uuid":"79a23d5e723f48818e9269f9d8f801d1d6f74d90","colab_type":"text","id":"VfIhitwolSeg"},"cell_type":"markdown","source":"in BoundaryLine you can see as exponential function  works [wiki](https://en.wikipedia.org/wiki/Radial_basis_function)"},{"metadata":{"_cell_guid":"fd946b62-732e-4beb-80e6-5dc937c0ca65","_uuid":"3c88562307b58f831d0d71c1c12e60a098bb5b8f","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1156},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":3054,"status":"ok","timestamp":1526576409785,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"rDOO-b1T0BL-","outputId":"7627d8fe-ee22-4a07-fbee-c4340b49d145","trusted":true},"cell_type":"code","source":"svcrbf_scores = evalute_realdata_PCAreduction( svcrbf, \"Support Vector - rbf\")\nplot_scores(svcrbf_scores,\"Support Vector - rbf\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06cb493d-77a7-4b19-b11a-da526f23f82c","_uuid":"2a342a17aa22d899ebc12b57a820692da7b81607","colab_type":"text","id":"SGKpXCWPkYoA"},"cell_type":"markdown","source":"There is a big difference in performance between PCA and Real Data (as shown in ROC curves)\n\nWith Suppport Vector RBF on Real Data we obtained:\n\n* Cross validation score : **0.98**\n* Cross validation recall : **0.956**"},{"metadata":{"_cell_guid":"9f2298c5-3c9d-455c-b13b-583decb37542","_uuid":"fe133b304151527adcc76fd22be8907adfac128e","colab_type":"text","id":"-jbfWrkC1ISR"},"cell_type":"markdown","source":"## Support Vector - polynomial\n\nLet's try Support Vector - polynomial"},{"metadata":{"_cell_guid":"f898b78c-54fe-4277-a318-e5c5f6b1d266","_uuid":"f7a1d8604e35b40a6ee595aa8654a87ab4132382","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":14074,"status":"ok","timestamp":1526505917226,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"CP42kdcs04Ph","outputId":"a202c21f-84c9-40b0-93e3-ebb20dbf5b74","trusted":true},"cell_type":"code","source":"param_range = np.linspace(0.1, 20, 20)\nparam_name=\"C\"\nscvpoly = SVC(kernel = 'poly')\nvalidation_curve_model(df_features.values,df_y.values,scvpoly, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=12.5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ca133e58-fe02-491d-bf5b-0ed4d2c5e0e4","_uuid":"4b5be19c21849e029bdc1eca080cd703d9345a2a","colab_type":"text","id":"yVBK1GZ3mzdS"},"cell_type":"markdown","source":"After the value 12.5 the curve diverges"},{"metadata":{"_cell_guid":"d9b23815-542a-46c8-9401-906dcf4efe23","_uuid":"e8d81339003afad28661d0ed8bc994e3afce8ecb","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":8706,"status":"ok","timestamp":1526575459050,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"qHluCas94B-O","outputId":"46daed48-ee00-4b8d-feca-ddae7b7b6548","trusted":true},"cell_type":"code","source":"svcpoly = SVC(kernel = 'poly',probability=True, C=12.5)\nlearning_curve_model(df_features.values,df_y.values, svcpoly, cv, train_size, ylim=ylim)\nplt.axhline(y=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1fbdeaa-b14b-44e3-ae2a-19660c02af8a","_uuid":"aaa1ee80bc5f8c6420cdababd97b8be8ace3bfc8","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1554},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":14948,"status":"ok","timestamp":1526576426937,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"iJszw_io1gYf","outputId":"0d7edd69-ed23-4cea-a3fd-d00fd660fb94","trusted":true},"cell_type":"code","source":"BoundaryLine('linear',svcpoly, \"Support Vector - poly\")\nsvcpoly_scores = evalute_realdata_PCAreduction( svcpoly, \"Support Vector - poly\")\nplot_scores(svcpoly_scores,\"Support Vector - poly\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"748321dc-6dfd-4e11-aa18-481a2a7331c4","_uuid":"c88590cc4909363db31272bd64bb729617f1d54c","colab_type":"text","id":"-moA76kinFL1"},"cell_type":"markdown","source":"Performance is lower than other models\n\nWith Suppport Vector Poly on Real Data we obtained:\n\n* Cross validation score : **0.95**\n* Cross validation recall : **0.898**"},{"metadata":{"_cell_guid":"f5b27640-e505-4e42-8911-5b5fa96eb0eb","_uuid":"8ab420c4cc17aeede2c7d8c5415e36cdc9989fd9","colab_type":"text","id":"cbWIjC7f25cB"},"cell_type":"markdown","source":"## AdaBoostClassifier\n\nAdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. [wiki](https://en.wikipedia.org/wiki/Radial_basis_function)"},{"metadata":{"_cell_guid":"d626a608-8d0b-42cb-8174-ec4b72c28044","_uuid":"fbc560fa743f300554fd2066e03b48a85bc6b351","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"38CKg2Is_RxK","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8c376ac-eb38-4142-b15d-36f563d4750e","_uuid":"16ecb19799a0376d25a07802a87bd0546e68c23c","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":278976,"status":"ok","timestamp":1526507454606,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"4VgEVUWa24ce","outputId":"ebe0644c-e4af-4eff-bc6c-2a87af3e6f8c","trusted":true},"cell_type":"code","source":"ABC = AdaBoostClassifier()\n\nparam_range = np.linspace(0.01, 0.2, 20)\nparam_name=\"learning_rate\"\nvalidation_curve_model(df_features.values,df_y.values, ABC, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b59a8a8-413e-45c6-ac3f-d26194ac1323","_uuid":"20000a336d1953060388f17400fe3874feddd065","colab_type":"text","id":"teDPTD9lo91U"},"cell_type":"markdown","source":"The curve diverges after the value 0.1 (overfitting)\n\nLearning rate shrinks the contribution of each classifier by learning_rate."},{"metadata":{"_cell_guid":"0c60bb72-64de-4e03-8a65-f2342d68c374","_uuid":"13f7dd47d9b7650d7111aec226965aed8a31f8ed","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":125555,"status":"ok","timestamp":1526507580219,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"-04cXW_u3w3U","outputId":"2399ace4-008e-4a2f-95e3-361c2131c5f4","trusted":true},"cell_type":"code","source":"ada =  AdaBoostClassifier(learning_rate=  0.1)\nlearning_curve_model(df_features.values,df_y.values, ada, cv, train_size,ylim=ylim)\nplt.axhline(y=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"791f6e01-666d-45bc-b9dc-fc8390d504bf","_uuid":"4a2b9cfbc4f6dfb3ce675e462dc71d05d1804051","colab_type":"text","id":"F7Rj0EQPpTFp"},"cell_type":"markdown","source":"The score doesn't change much after 200"},{"metadata":{"_cell_guid":"9615aa3f-8666-4ec5-a912-19e4be9030ee","_uuid":"1196084f5716a51d92b68e0f0b4ec675486318ea","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1554},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":48766,"status":"ok","timestamp":1526576515243,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"izUpE4aw7BGS","outputId":"95d88b4e-8949-419c-b554-dbdc7ae38e5f","trusted":true},"cell_type":"code","source":"BoundaryLine('linear',ada, \"Ada Boost\")\nada_scores = evalute_realdata_PCAreduction( ada, \"Ada Boost\")\nplot_scores(ada_scores,\"Ada Boost\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ccd96f94-4fdd-4ba9-a0eb-d6755ada6f0a","_uuid":"01dd5794630c43f5722a37c49e8b29426e37a4ef","colab_type":"text","id":"6z-23v35pqDs"},"cell_type":"markdown","source":"With Ada Boost on Real Data we obtained:\n\n* Cross validation score : **0.955**\n* Cross validation recall : **0.922**"},{"metadata":{"_cell_guid":"44e3a0ef-76f7-40dc-9da2-9e6e77aaae05","_uuid":"0f45dec4e7a18815e85c86da4b7393a52d058f95","colab_type":"text","id":"sW2L1pNT8Qxp"},"cell_type":"markdown","source":"## Bagging\n\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."},{"metadata":{"_cell_guid":"cc5ed790-caa1-4224-8253-a3da0231befa","_uuid":"0da52a4d3c6a979bdaa83c037823e1ffb3c3f335","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"TmaEnVWl_Z-4","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a1f13f0-8c34-46e7-a8dc-47811794f088","_uuid":"186c2b4091ff0fc6d891a70518b69dafddd2bdcb","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":28414,"status":"ok","timestamp":1526507706330,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"bO11zSsp8S9b","outputId":"fe591039-428c-4a27-c077-f6235416b155","trusted":true},"cell_type":"code","source":"bagging = BaggingClassifier()\nparam_name=\"max_samples\"\nparam_range = np.linspace(0.1, 1.0, 10)\nvalidation_curve_model(df_features.values,df_y.values, bagging, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f49ad1b4-35b6-4172-b1be-10c84faf7669","_uuid":"71c049655bb93690aa4238985cb02d9726c5ced0","colab_type":"text","id":"dtwA0GLDvbIF"},"cell_type":"markdown","source":"max_sample = the number of samples to be extracted X to train each base estimator\n"},{"metadata":{"_cell_guid":"1c118f19-5758-4b70-b9d6-14b773627869","_uuid":"3f2a37f5f776681fb1e0d00d1424f967c06b3698","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":19142,"status":"ok","timestamp":1526507979122,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"InmMFW-h8xQw","outputId":"87e14b79-269d-4923-d48f-a5c40f43b1cb","trusted":true},"cell_type":"code","source":"bagging = BaggingClassifier(max_samples=0.2)\nlearning_curve_model(df_features.values,df_y.values, bagging, cv, train_size,ylim=ylim)\nplt.axhline(y=0.94)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2d7006b-2c60-45cc-b4a5-11c97aeb6ca9","_uuid":"4a98d37a4c2d6b28fb286efd9aceb7a757be5756","colab_type":"text","id":"4dkeC5Elq_ma"},"cell_type":"markdown","source":"The score doesn't change much after 300"},{"metadata":{"_cell_guid":"ee42ad1b-d40a-4a3a-98bc-99e5d1802cc4","_uuid":"98b15c83045587327530591f2bbb4577bb720940","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1554},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":7064,"status":"ok","timestamp":1526575551105,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"B0aeCzF29z55","outputId":"d226ff27-8eac-4b3a-bb7d-8784d35436f8","trusted":true},"cell_type":"code","source":"BoundaryLine('linear',bagging, \"Bagging\")\nbagging_scores = evalute_realdata_PCAreduction( bagging, \"Bagging\")\nplot_scores(bagging_scores,\"Bagging\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d07795c-754d-42cf-8071-2158327e6284","_uuid":"d03a2a19ca51993ed08e4db8a9933a7384a251c6","colab_type":"text","id":"znK-9BeVvnX0"},"cell_type":"markdown","source":"With Ada Boost on Real Data we obtained:\n\n* Cross validation score : **0.946**\n* Cross validation recall : **0.89**"},{"metadata":{"_cell_guid":"250215c8-d406-4d81-8011-df7a2d80cd50","_uuid":"b315fc70b633df23f8f9b82cd41e647305cb8328","colab_type":"text","id":"PiLDKoNZ-nSf"},"cell_type":"markdown","source":"## Neural Network\n\nA multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable."},{"metadata":{"_cell_guid":"951a6374-28ce-4eae-b4f0-1eae0a306cc9","_uuid":"b41a553c350c0d283af8c94e60d46bbc6fa1b519","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"FFx6DC7NzbMf","trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier #http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23624392-8831-48a9-828e-c2478418a8cc","_uuid":"f7eb4b2c7cb1ffe5ccbf04391e96ea99a6216205","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":612309,"status":"ok","timestamp":1526509049135,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"YANWsV0I-plv","outputId":"1b1a8f4e-2419-4263-d4ba-98e1fe6aa2f8","trusted":true},"cell_type":"code","source":"mlp = MLPClassifier(max_iter=1000)\nparam_range = np.logspace(-5, 2, 10)\n\nparam_name=\"alpha\" # L2 penalty (regularization term) parameter\nvalidation_curve_model(df_features.values,df_y.values, mlp, param_name, param_range, cv, ylim=ylim,log=False)\nplt.axvline(x=15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c6f4f4f-be5b-47ed-959e-0c50a2b432f9","_uuid":"2a6b50497bdf554748562d22598bf129a685f1e7","colab_type":"text","id":"q-KIj4O6yFF7"},"cell_type":"markdown","source":"A good value is 15 for alpha.  Alpha is L2 penalty (regularization term) parameter"},{"metadata":{"_cell_guid":"891ca6ca-c0b4-4420-9ed3-b916f78a79e8","_uuid":"36a3579d17b84f775eb280daed1688ef999f9f07","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":311},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":701322,"status":"ok","timestamp":1526573982080,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"fRW2zeMH_qYU","outputId":"e5d41f7d-ce08-482c-a02d-102c51cf41c1","trusted":true},"cell_type":"code","source":"mlp = MLPClassifier(max_iter=1000, alpha=15)\nlearning_curve_model(df_features.values,df_y.values, mlp, cv, train_size, ylim=ylim)\nplt.axhline(y=0.97)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b3eed11-c418-4af9-af38-513a59ec6e13","_uuid":"4356b1487f89f4ad482c148557c97546f642b794","colab_type":"text","id":"FNxyZNW5zws-"},"cell_type":"markdown","source":"there is a rapid decline in performance around 220"},{"metadata":{"_cell_guid":"437ecc95-c498-4485-80d1-b3c911be32d5","_uuid":"5657cf62eb825f634f3e4ba7e689a9ef63e0522d","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1554},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":52979,"status":"ok","timestamp":1526575617454,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"PHPobD1S_8nv","outputId":"bd6d6287-b6f2-483f-9100-303da5e6bf40","trusted":true},"cell_type":"code","source":"BoundaryLine('linear',mlp, \"MLP Classifier\")\nmlp_scores = evalute_realdata_PCAreduction( mlp, \"MLP Classifier\")\nplot_scores(mlp_scores,\"MLP Classifier\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"393bda15-9fb4-4af7-be1b-d69b13d9a31e","_uuid":"c1e1898b613750fc22f2562f084aa886ea900e22","colab_type":"text","id":"7iPGyZ2xyVGo"},"cell_type":"markdown","source":"With MLP on Real Data we obtained:\n\n* Cross validation score : **0.97**\n* Cross validation recall : **0.917**"},{"metadata":{"_cell_guid":"fef90ceb-e34b-4ac2-aca0-c8d8b6a7d759","_uuid":"927054c84ed400a5d938ec8c7695ff74fc5af79c","colab_type":"text","id":"AuSn2_Opz94T"},"cell_type":"markdown","source":"## Ranking"},{"metadata":{"_cell_guid":"ab686ce4-df8e-4eba-873b-d63eca273a33","_uuid":"d4f937df89f3e72b25c29e0735aac963f8f17a64","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":328},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":570,"status":"ok","timestamp":1526576570752,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"BCHL4qw10ANK","outputId":"05489bf7-5bd7-4f2a-d9d8-c30c2038101e","trusted":true},"cell_type":"code","source":"models = pd.DataFrame({'Model': [\"Logistic Regression\",\"K-Nearest Neighbors\", \"Random Forest\", \"Support Vector - linear\", \n                                \"Support Vector -  rbf\", \"Support Vector -  poly\",\"Ada Boost\", \"Bagging Classifier\",\"MLP Classifier \"],\n                      'KFoldScore':\n                      [logreg_scores[3].mean(),  kneigh_scores[3].mean(), r_forest_scores[3].mean(),  svcl_scores[3].mean(),\n                       svcrbf_scores[3].mean(), svcpoly_scores[3].mean(), ada_scores[3].mean(), bagging_scores[3].mean(), mlp_scores[3].mean()],\n                      'Std (Score)': \n                      [logreg_scores[3].std(),  kneigh_scores[3].std(), r_forest_scores[3].std(), svcl_scores[3].std(), \n                       svcrbf_scores[3].std(), svcpoly_scores[3].std(), ada_scores[3].std(), bagging_scores[3].std(), mlp_scores[3].std()],\n                'KFoldRecall':\n                      [logreg_scores[2].mean(),  kneigh_scores[2].mean(), r_forest_scores[2].mean(),  svcl_scores[2].mean(),\n                       svcrbf_scores[2].mean(), svcpoly_scores[2].mean(), ada_scores[2].mean(), bagging_scores[2].mean(), mlp_scores[2].mean()],\n                      'Std (Recall)': \n                      [logreg_scores[2].std(), kneigh_scores[2].std(), r_forest_scores[2].std(), svcl_scores[2].std(), \n                       svcrbf_scores[2].std(), svcpoly_scores[2].std(), ada_scores[2].std(), bagging_scores[2].std(), mlp_scores[2].std()]})\n\nmodels.sort_values(by='KFoldRecall', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79e384d5-1403-45cb-9215-9b04f196b378","_uuid":"fd753028ac914458abc2889ce2a66096b865e71f","colab_type":"text","id":"7TrYwCOvDjfz"},"cell_type":"markdown","source":"The winner is **Logistic Regression** \n\n![alt text](https://chemicalstatistician.files.wordpress.com/2014/05/pregnant.jpg?w=500&h=374)"},{"metadata":{"_cell_guid":"a1f08fcd-a58c-4c72-a9d5-3ed33c9cc907","_uuid":"b50fb09e3243463a240a062e65c5d8039432584c","colab_type":"text","id":"ljq5av_2z_xS"},"cell_type":"markdown","source":"# Clustering"},{"metadata":{"_cell_guid":"10589acc-75e6-4004-97c4-0b64c381cb83","_uuid":"2c7f68af79a3e297d2d5e86529618437c82676ab","colab_type":"text","id":"QQ6hod1U3pG7"},"cell_type":"markdown","source":"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n\n\nClustering algorithms can be categorized based on their cluster model. The following we will see Hierarchical and K-means clustering"},{"metadata":{"_cell_guid":"f4fed850-5d58-4289-bc45-53caca2188e4","_uuid":"ce751a62a58616171e2f5c73de4ffcbdfe5b903b","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"jV0x1-eq0KbM","trusted":true},"cell_type":"code","source":"# Import the neccessary modules for data manipulation and visual representation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport matplotlib.gridspec as gridspec # subplots\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram, linkage,median  # linkage analysis and dendrogram for visualization\nfrom scipy.cluster.hierarchy import fcluster  # simple clustering\nfrom scipy.spatial.distance import pdist, squareform # metric\nfrom sklearn import datasets, metrics\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ef12f89-2e73-40e5-a6c6-361dc8324adc","_uuid":"d380538c1d106274e475d205423ec068e6dcb442","colab_type":"text","id":"CZ-Sz2QK0Xr4"},"cell_type":"markdown","source":"## Hierarchical clustering\n\nIn data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. [wiki](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\nIn the first instance, there was the need to identify, among others, the most appropriate linkage function."},{"metadata":{"_cell_guid":"8349e9f8-f032-42e7-a0dd-2f07437dc361","_uuid":"c0ebddd03d79f16fcbda0ca6e3aa73010ead27e0","colab_type":"text","id":"F6h0d_7A5NXW"},"cell_type":"markdown","source":"### Functions Utils"},{"metadata":{"_cell_guid":"aac0ce50-0794-47c5-83c5-23d7ea43ea32","_uuid":"a3df41db38949bff480c385bdac9c7ec3ebf4d01","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"EoOwGB4X07Qe","trusted":true},"cell_type":"code","source":"def fancy_dendrogram(*args, **kwargs):\n    max_d = kwargs.pop('max_d', None)\n    if max_d and 'color_threshold' not in kwargs:\n        kwargs['color_threshold'] = max_d\n    annotate_above = kwargs.pop('annotate_above', 0)\n\n    ddata = dendrogram(*args, **kwargs)\n\n    if not kwargs.get('no_plot', False):\n        plt.title('Hierarchical Clustering Dendrogram ')\n        plt.xlabel('sample index')\n        plt.ylabel('distance')\n        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n            x = 0.5 * sum(i[1:3])\n            y = d[1]\n            if y > annotate_above:\n                plt.plot(x, y, 'o', c=c)\n                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n                             textcoords='offset points',\n                             va='top', ha='center')\n        if max_d:\n            plt.axhline(y=max_d, c='k')\n    return ddata\n\ndef getCenter(D,clusters): # distance matrix and clusters\n  err = .0\n  centers = []\n  contr = []\n  for i in range(len(set(clusters))):\n    id_pts = [index for index,value in enumerate(clusters) if value == i+1] #ids cluster i-th\n    sub_ms = D[id_pts,:][:,id_pts] #sub distance matrix\n    err = err + np.sum(D[np.argmin(np.mean(sub_ms, axis=0)), :])  # errore assoluto  +=  somma riga centroide della matrice D\n\n    beta = 1\n    index = np.exp(-beta * sub_ms / sub_ms.std()).sum(axis=1).argmax()\n    centers.append(id_pts[index])\n    contr.append(float(\"{0:.2f}\".format((len(sub_ms) * 100) / len(D))))\n    \n  return contr,err,centers \n    \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cc5b1c60-f31b-4227-b20f-f58747c09363","_uuid":"4eb9d663baa90a673a904cb40977e2202c98bdd9","colab_type":"text","id":"8Ty9jfTC5u2M"},"cell_type":"markdown","source":"### Linkage functions.\nAs evidenced by the plot, among the various linkage functions selected, the ward method was the most suitable, as it allowed to create clusters and well separated clusters."},{"metadata":{"_cell_guid":"c473ab23-3003-46e8-b0c1-9a12b02cc4ca","_uuid":"0d528f05ebc45e9ea990646a7ffc234ea8f33b53","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":605},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":2989,"status":"ok","timestamp":1526576763416,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"O6DTXaD00ZJ0","outputId":"5e9e9a41-4c02-49b4-b11b-bb80b3436e4e","trusted":true},"cell_type":"code","source":"D = df_features.values\n\n#ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\nmethods = ['single','complete','average','weighted','median','ward']\n\n\nplt.figure(figsize=(25, 8))\nfor i in range(len(methods)):\n  plt.subplot(231+i)\n  Z = linkage(D, method=methods[i]) #Perform hierarchical/agglomerative clustering. \n  de = dendrogram(\n      Z,\n      leaf_rotation=90.,\n      leaf_font_size=11.,\n      distance_sort='descending',\n      truncate_mode = 'lastp',\n      p=50\n      \n  )\n  plt.title(methods[i])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91762cd6-dae5-480e-b240-5046740a123e","_uuid":"3dc59cf5b1a6055bca28a4a1580840ed11fb6347","colab_type":"text","id":"mGeffOvKET21"},"cell_type":"markdown","source":"Ward suggests 2 clusters by default (different color)"},{"metadata":{"_cell_guid":"b78252db-de4c-4fd1-a5be-f694c1abdd4c","_uuid":"7ffd11adef8340d790640872dd6f65d4071aac8a","colab_type":"text","id":"iiBcdEzW0pia"},"cell_type":"markdown","source":"### Ward Linkage K = 2\n\n"},{"metadata":{"_cell_guid":"a56b6295-fc8c-4b90-9b6f-22c4c267e5cb","_uuid":"cb39f2e2703ad96e64049bb7069880a0e6caaed5","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":513},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1310,"status":"ok","timestamp":1526577079438,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"MwAjTH190wQL","outputId":"625d31b3-46fd-48dd-c801-f6b6a8aa2b64","trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid') \nD = df_features.values\n\nZ = linkage(D, method='ward', metric='euclidean') #Perform hierarchical/agglomerative clustering. ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\n \nplt.figure(figsize=(15, 7))\ndendrogram(\n    Z,\n    \n    leaf_rotation=90.,\n    leaf_font_size=11.,\n    show_contracted=True,\n    distance_sort='descending',\n    truncate_mode = 'lastp',\n    p=50\n)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"506d2213-478b-47c5-a8cb-07c4b23ee1e2","_uuid":"368dd4a7214122bc2da0d9b3e332354cbf0f14e4","colab_type":"text","id":"FKtito-402QT"},"cell_type":"markdown","source":"the dendrogram function with ward divides the data into 2 groups (it cuts to 70% of the maximum length) by default \n\nhttps://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html"},{"metadata":{"_cell_guid":"99d96e61-34f3-482b-9962-6d8cffd22184","_uuid":"5f9c49f7b2038019cae0ddcfa1ab70f1fc814355","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":500},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":928,"status":"ok","timestamp":1526577274830,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"HgbNkz_41HDN","outputId":"3ae6fe57-ad3f-4b72-c96f-c0bbd997feed","trusted":true},"cell_type":"code","source":"pca_2d = PCA(n_components=2)\nX = pca_2d.fit_transform(D)\nY = pdist(D, 'euclidean')\nY = squareform(Y)\nY.shape\nk=2\nclusters = fcluster(Z, k, criterion='maxclust')\ncontr, err,centers = getCenter(Y,clusters)\nprint('centroid: ',centers,'\\t %items ',contr)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X[:,0], X[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\nplt.scatter([X[504,0], X[74,0]],[X[504,1], X[74,1]], c='black', cmap='prism',marker='x',s=50,\n            label=\"centroid\")  # plot points with cluster dependent colors\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65a0c09c-b727-4be0-a500-fc33ddfd5ab7","_uuid":"5d7f514df8b179eab0e6c5f781627e3d33ea054e","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"Y-4oGRSy1WZL","trusted":true},"cell_type":"code","source":"noscaled_features =scaler.inverse_transform(df_features.values)# de normalizzo per vedere i valori reali dei centroidi\ndf_features_noscaled = pd.DataFrame(noscaled_features, index=df_features.index, columns=df_features.columns)\ndf_features_y =  pd.concat([df_features_noscaled,df_y], axis=1)\nprint(\"----------------- Centroide Cluster 1 ----------------- \")\nprint(df_features_y.iloc[70,:])\nprint(\"----------------- Centroide Cluster 2 ----------------- \")\nprint(df_features_y.iloc[504,:])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b9033b4-eca1-4c23-953c-a72ab8857b1e","_uuid":"27749cb10d8c4ab75b2d8dd7eee626b740c85076","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":571},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":791,"status":"ok","timestamp":1526577280816,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"xclyl1qV1cil","outputId":"ef0c7c8c-4f29-42a6-cbd2-9e05f57b539d","trusted":true},"cell_type":"code","source":"\nmean_pca_M = pca_2d.transform(df_features_y_clean[ df_features_y_clean['diagnosis']=='M'].mean().reshape(1, -1))\n\nmean_pca_B = pca_2d.transform(df_features_y_clean[ df_features_y_clean['diagnosis']=='B'].mean().reshape(1, -1))\n\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X[:,0], X[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\nplt.scatter([mean_pca_M[:,0], mean_pca_B[:,0]],[mean_pca_M[:,1], mean_pca_B[:,1]], c='blue', cmap='prism',marker='+',s=50,\n            label=\"centroid diagnosis class (b,m)\")  # plot points with cluster dependent colors\nplt.scatter([X[504,0], X[74,0]],[X[504,1], X[74,1]], c='black', cmap='prism',marker='x',s=50,\n            label=\"centroid cluster\")  # plot points with cluster dependent colors\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9915e96f-2b6c-489c-a3bc-a91f806a2592","_uuid":"dc20ec0f7a59491b61c7336fd7d3a4545f36e77c","colab_type":"text","id":"bT39wOy1GUTN"},"cell_type":"markdown","source":"It is interesting to note how the centroids of the clusters fall very close to the average values of the two classes of tumors (benign and malignant). Therefore It is possible to note that, If we did not have a labeled dataset (with well-defined classes B and M) we would still be able to determine (with good probability) the class of belonging of the dataset elements, through an unsupervised clustering process ."},{"metadata":{"_cell_guid":"c6e6a564-eec2-4dcd-90cf-1327fcb0768f","_uuid":"0d550f1fb18c680b66c5183f8de4f48ecd66954e","colab_type":"text","id":"8i88Cxve1pFk"},"cell_type":"markdown","source":"### Ward K=3 (Elbow Method)\n\n\n\nAnother thing you might see out there is a variant of the \"elbow method\". It tries to find the clustering step where the acceleration of distance growth is the biggest (the \"strongest elbow\" of the blue line graph below, which is the highest value of the green graph below):\n\nhttps://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/"},{"metadata":{"_cell_guid":"5aec68b7-f40b-4814-b503-ea356dc2ad45","_uuid":"d39d66426549b07139910c32f456f7cef4ff70e9","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":282},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":558,"status":"ok","timestamp":1526577380127,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"35SmBQyH1n9Z","outputId":"e38a0a01-9fe6-4806-daed-617a42022a91","trusted":true},"cell_type":"code","source":"last = Z[-20:, 2]\nlast_rev = last[::-1]\nidxs = np.arange(1, len(last) + 1)\nplt.plot(idxs, last_rev)\n\nacceleration = np.diff(last, 2)  # 2nd derivative of the distances\nacceleration_rev = acceleration[::-1]\nplt.plot(idxs[:-2] + 1, acceleration_rev)\nplt.show()\nk = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\nprint (\"clusters:\", k)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"67a6f38c-7059-4c74-b017-d29ec9957e93","_uuid":"6850d5c407f9057b6dbdc54cc82a91093dfd4c00","colab_type":"text","id":"iutixTX0G1u4"},"cell_type":"markdown","source":"Elbow represented on the blue line corresponds to the maximum acceleration of distance growth (acceleration plotted by the green line) which then identifies the number of clusters in 3."},{"metadata":{"_cell_guid":"20f5f1c6-9c4c-426e-bc07-e769d227b740","_uuid":"9e18dc76b8f958b8f74c6d0c9b484ab1790eb3d1","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":467},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":1560,"status":"ok","timestamp":1526577383736,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"VKkF_noA1wkc","outputId":"4b9dfa2b-1ee8-4e6e-c093-f94f3b88c99f","trusted":true},"cell_type":"code","source":"\n#D = df_features.values\n\nZ = linkage(D, method='ward', metric='euclidean') #Perform hierarchical/agglomerative clustering. ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\n\nmax_d =35.2 #The common practice to flatten dendrograms in $k$ clusters is to cut them off at constant height $k-1$. \nplt.figure(figsize=(15, 7))\nfancy_dendrogram(\n    Z,\n    \n    leaf_rotation=90.,\n    leaf_font_size=11.,\n    show_contracted=True,\n    annotate_above=10,  # useful in small plots so annotations don't overlap\n    color_threshold=max_d,\n    max_d = max_d,\n    distance_sort='descending',\n    truncate_mode = 'lastp',\n      p=50\n)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1cc92996-8a92-4274-a0c3-7a61068ba388","_uuid":"f9b0597d74fe2c998909574fba10ae7f5c9994b9","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":567,"status":"ok","timestamp":1526577491415,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"x_B3alIA15kU","outputId":"118be808-6dce-405e-afe7-7339c9f3d234","trusted":true},"cell_type":"code","source":"k=3\nclusters = fcluster(Z, k, criterion='maxclust')\ncontr, err,centers = getCenter(Y,clusters)\nprint('centroid: ',centers,'\\t %items ',contr)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0aaaaa1-7c1c-46c1-817b-a508629010e4","_uuid":"f726172f687c81bbe07a5ceff87d02d1e264d60c","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":483},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":677,"status":"ok","timestamp":1526577494107,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"WhZFfdGT19NB","outputId":"578edee1-7270-45fe-911f-a94fcce057e8","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.scatter(X[:,0], X[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\nplt.scatter([X[504,0], X[70,0],X[41,0]],[X[504,1], X[70,1],X[41,1]], c='black', cmap='prism',marker='x',s=50,\n            label=\"centroid\")  # plot points with cluster dependent colors\n\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"031295ea-f60b-4dd9-98a5-3c03844993e4","_uuid":"d2d1d8586d6123c4d5e20539358faca8e1e41795","colab_type":"text","id":"0RtwOd1Q2AHS"},"cell_type":"markdown","source":"## K Meas\n\nk-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. "},{"metadata":{"_cell_guid":"fb82e7ee-f34f-4666-aa7c-d85270d7b2a7","_uuid":"b11d06e05ade2f02dfa1e78c6c4ed5f1bf74cd71","colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"uac_BGDm2HHO","trusted":true},"cell_type":"code","source":"D = df_features.values\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 30):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(D)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 30), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9da0846f-424a-45ff-a0ec-165b507c315a","_uuid":"f3cb1e22a0b0dc4de2ad7bf2411e4786cde3fab9","colab_type":"text","id":"7hcPAb4_2LBS"},"cell_type":"markdown","source":"### Silhouette analysis\nSilhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n\nSilhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n\n\nhttp://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"},{"metadata":{"_cell_guid":"8a8a36be-086d-4339-b2de-10515f07c964","_uuid":"3e08e7c65f0b6616d888e58d9fc51f3448b93506","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1884},"colab_type":"code","collapsed":true,"executionInfo":{"elapsed":2945,"status":"ok","timestamp":1526577525759,"user":{"displayName":"Daniele Licari","photoUrl":"//lh6.googleusercontent.com/-h8R-x0sf1G8/AAAAAAAAAAI/AAAAAAAABG4/wJizoTKm2YA/s50-c-k-no/photo.jpg","userId":"104589627174468553732"},"user_tz":-120},"id":"b_L7mDJx2HeQ","outputId":"aef96747-9305-46d6-be33-5092b01292fe","trusted":true},"cell_type":"code","source":"\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX = df_features.values\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\npca_2d = PCA(n_components=2)\npca_2d_r = pca_2d.fit_transform(X)\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 5)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10, max_iter=9000)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n \n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(pca_2d_r[:, 0], pca_2d_r[:, 1], marker='.', s=90, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n    \n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    centers = pca_2d.transform(centers)\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=250, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=100, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"PC1\")\n    ax2.set_ylabel(\"PC2\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b5d0ff3-ceaf-4b68-9d66-76c96fe375e6","_uuid":"770a57d69e502d4033517197e68c33336f920830","colab_type":"text","id":"mUqaSIDwHnmI"},"cell_type":"markdown","source":"In our example, the analysis of the silhouette is used to choose an optimal value for the number of clusters. The silhouette plot shows that a n_clusters value of 5 and 6 is not good because they have clusters with lower than average scores, many negative values and also large fluctuations in the size of the silhouette plot. From the analysis of the silhouette, a good number of k clusters appears to be 2, 3 or 4, since it confirms what has already been expressed by the elbow method applied to hierarchical clustering."}],"metadata":{"colab":{"collapsed_sections":[],"default_view":{},"name":"Breast Cancer Kaggle","provenance":[{"file_id":"10aYhs4Fh0eXJnEIQSFkDlDHwKq9EGrzU","timestamp":1526485216033},{"file_id":"1xjypwIexpnBD3Ro4x5nnkHl-93vQQ8qN","timestamp":1526063245562}],"version":"0.3.2","views":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}