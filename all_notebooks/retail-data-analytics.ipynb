{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade storyscience","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import storyscience\n# storyscience.Shree()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\na=pd.read_csv(\"/kaggle/input/retaildataset/sales data-set.csv\")\nfea=pd.read_csv(\"/kaggle/input/retaildataset/stores data-set.csv\")\nabc=pd.read_csv(\"/kaggle/input/retaildataset/Features data set.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----Count----->\n#<-----Subbhashit----->\nfrom collections import Counter\ndef Count(x):\n    dictionary = dict()\n    array = list(x)\n    countArray = dict(Counter(array).most_common(1))\n    return countArray\nb=Count(a['IsHoliday'])\nb,list(b.keys())[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----Count----->\n#<-----Subbhashit----->\nimport storyscience as ss\n# ss.Count(a['IsHoliday'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----Impute----->\n#<-----Subbhashit----->\na.iloc[0,1]=np.nan\nprint(a.head())\n\n\nfrom collections import Counter\n\ndef impute(array,method='mean'):\n    arr = list(array)\n    pos = []\n    for i in range(len(arr)):\n        if np.isnan(arr[i]):\n            pos.append(i)\n    for i in pos:\n        arr.remove(arr[i])\n    #<-----mean----->\n    if method=='mean':\n        for i in pos:\n            key = int(sum(arr)/len(arr))\n            arr.insert(i,key)\n     #<-----mode----->\n    elif method=='mode':\n        for i in pos:\n            dictionary = dict(Counter(arr).most_common(1))\n            key = int(list(dictionary.keys())[0])\n            arr.insert(i,key)\n    return arr     \nb=impute(a['Dept'],'mode')\nprint(\"b------>\",b[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----Zscore----->\n#<-----Subbhashit----->\ndef zscore(data,threshold=1):\n    threshold = 3\n    outliers = []\n    arr = list(data)\n    mean = np.mean(arr)\n    std = np.std(arr)\n    for i in arr:\n        #zscore formula\n        z = (i-mean)/std\n        if z > threshold:\n            outliers.append(i)\n    return outliers\n\nb=zscore(a['Weekly_Sales'],15981.258123467243)    \nprint(b[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----SinglePlOT----->\n#<-----Subbhashit----->\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef SinglePlot(arr):\n    #Plots initialization\n    fig, ax =plt.subplots(2,2)\n    fig.set_size_inches(12.7, 10.27)\n    \n    #Pie plot\n    plt.subplot(2,2,1)\n    arr.value_counts().tail().plot(kind='pie',figsize=(15,10))\n    \n    #Histogram\n    sns.distplot(arr,ax=ax[0,1])\n    \n    #Bar plot\n    plt.subplot(2, 2,3)\n    arr.value_counts().tail().plot(kind='bar',color=['c','y','r'],figsize=(15,10))\n    \n    #Box plot\n    sns.boxplot(arr,ax=ax[1,1])\n    \n    \n    fig.show()\nSinglePlot(a['Dept'])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----IQR----->\n#<-----Subbhashit----->\nimport numpy as np\ndef IQR(data,arg1=75,arg2=25):\n    q3, q1 = np.percentile(data, [arg1 ,arg2])\n    iqr = q3 - q1\n    return iqr\n\nran = IQR(a['Store'])\nran","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----IQR----->\n#<-----Subbhashit----->\n# rangeIQR = ss.IQR(a['Store'])\n# ranngeIQR","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----Describe----->\n#<-----Subbhashit----->\ndef Describe(data):\n    l = list(data.columns)\n    length = []\n    mini = []\n    maxi =[]\n    mean = []\n    median = []\n    mode = []\n    typ =[]\n    std =[]\n    std=[]\n    types = ['float64','int64']\n    for  i in l:\n        typ.append(data[i].dtype)\n        length.append(len(data[i]))\n        mini.append(min(data[i]))\n        maxi.append(max(data[i]))\n        if data[i].dtype in types:\n            mean.append(data[i].mean())\n            median.append(data[i].median())\n            mode.append(data[i].mode()[0])\n            std.append(np.std(data[i]))\n            \n        else:\n            mean.append(np.nan)\n            median.append(np.nan)\n            mode.append(np.nan)\n            std.append(np.nan)\n            \n        \n    df = pd.DataFrame([typ,length,mini,maxi,mean,median,mode,std], index=['Type','Length','Minimum','Maximum','Mean','Median','Mode','STD'] ,columns = l)\n    return df\n        \n    \ndf=Describe(abc)\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----Describe----->\n#<-----Subbhashit----->\n# df=ss.Describe(abc)\n# df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tabulate ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate as tb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----suggest_cats----->\n#<-----SR----->\n\ndef suggest_cats(data, th=40):\n    dtb = []\n    print('Following columns might be considered to be changed as categorical\\nTaking', th, \n          '% as Threshold for uniqueness percentage determination\\nLength of the dataset is:', len(data))\n    ln = len(data)\n    \n    for i in data.columns:\n        unique_vals = data[i].nunique()\n        total_percent = (unique_vals/ln) * 100\n        eff_percent = (data[i].dropna().nunique()/ln) * 100\n        avg_percent = (total_percent + eff_percent)/2\n        if avg_percent <= th:\n            dtb.append([i, round(unique_vals,5), round(total_percent,5), round(eff_percent,5), round(avg_percent,5)])\n            \n    print(tb(dtb, headers=['Column name', 'Number of unique values', 'Total uniqueness percent', \n                           'Effective uniqueness percent', 'Average uniqueness percentage'], \n            tablefmt=\"fancy_grid\"))\n\nsuggest_cats(abc, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----suggest_drops----->\n#<-----SR----->\n\ndef suggest_drops(data, th=60):\n    dtb = []\n    print('Following columns might be considered to be dropped as percent of missing values are greater than the threshold-',th, \n          '%\\nLength of the dataset is:', len(data))\n    ln = len(data)\n    \n    for i in data.columns:\n        nans = data[i].isna().sum()\n        nan_percent = (nans/ln)*100\n        if nan_percent >= th:\n            dtb.append([i, round(nans, 5), round(nan_percent, 5)])\n    \n    print(tb(dtb, headers=['Column name', 'Number of nulls', 'Percent of null values'],\n             tablefmt=\"fancy_grid\"))\n    \nsuggest_drops(abc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----suggest_fillers----->\n#<-----SR----->\n\ndef suggest_fillers(data, th=40):\n    dtb = []\n    print('Following columns might be considered to be imputed as percent of missing values are less than the threshold-',th, \n          '%\\nLength of the dataset is:', len(data))\n    ln = len(data)\n    \n    for i in data.columns:\n        nans = data[i].isna().sum()\n        nan_percent = (nans/ln)*100\n        if nan_percent <= th and nan_percent != 0:\n            dtb.append([i, round(nans, 5), round(nan_percent, 5)])\n    \n    print(tb(dtb, headers=['Column name', 'Number of nulls', 'Percent of null values'],\n             tablefmt=\"fancy_grid\"))\n    \nsuggest_fillers(abc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----suggest_quants----->\n#<-----SR----->\n\ndef suggest_quants(data, th=60):\n    dtb = []\n    print('Following columns might be considered to be converted as categorical as \\nthe column is numerical and the uniqueness percent is greater than the threshold-',th, \n          '%\\nLength of the dataset is:', len(data))\n    ln = len(data)\n    numer = data.select_dtypes(include=np.number).columns.tolist()\n\n    for i in numer:\n        unique_vals = data[i].nunique()\n        total_percent = (unique_vals/ln) * 100\n        if total_percent >= 60:\n            dtb.append([i])\n            \n    \n    print(tb(dtb, headers=['Column name'],\n             tablefmt=\"fancy_grid\"))\n    \nsuggest_quants(a)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----create_quants----->\n#<-----SR----->\n\ndef create_quants(data, cols):\n    dtb = []\n    print('Creating Quantile columns...')\n\n    for col in cols:\n        low = np.percentile(data[col], 25)\n        mid = np.percentile(data[col], 50)\n        high = np.percentile(data[col], 75)\n        data[col + '_quant'] = data[col].apply(\n            lambda i: 0 if low > i else (1 if mid > i else (2 if high > i else 3)))\n        print(col + '_quant'+' has been created using column '+col)\n\n            \n    \n    print('completed!')\n    \ncreate_quants(a, ['Weekly_Sales'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<-----date_me----->\n#<-----SR----->\ndef date_me(data, cols):\n    from datetime import date\n    today = date.today()\n    dtb = []\n    print('Starting feature extraction from date column...')\n\n    for col in cols:\n        data['age'] = today.year - data[col].dt.year\n        data['months'] = data['age']*12 + data[col].dt.month\n        data['days'] = data['months']*30 + data[col].dt.day\n        data['season'] = data['months'].apply(lambda i:\n                                           'Winter' if i in [1,2,12] else(\n                                           'Spring' if i in [4,5,6] else(\n                                           'Summer' if i in [7,8,9] else \n                                           'Spring' )))\n        data['weekday'] = data[col].dt.day_name()\n        print('Features Extracted from column', col+'.....')\n            \n    \n    print('completed!')\n    \ndate_me(a, ['Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}