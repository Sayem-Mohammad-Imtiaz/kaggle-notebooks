{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Personal Loan Campaign Modelling","metadata":{}},{"cell_type":"markdown","source":"## Objective :\n1. To predict whether a liability customer will buy a personal loan or not.\n2. Which variables are most significant.\n3. Which segment of customers should be targeted more.","metadata":{}},{"cell_type":"markdown","source":"## Key Questions:\n1. What are the Key variables that have a strong relationships with the dependent variable?\n2. Which metric is right for model performance evaluation and why?\n3. How accurate are the Model predictions and can it be improved?\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\npd.set_option(\"display.max_columns\", None)\n# pd.set_option('display.max_rows', None)\npd.set_option(\"display.max_rows\", 200)\n\nimport statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.491757Z","iopub.execute_input":"2021-07-10T19:33:06.492177Z","iopub.status.idle":"2021-07-10T19:33:06.500501Z","shell.execute_reply.started":"2021-07-10T19:33:06.492151Z","shell.execute_reply":"2021-07-10T19:33:06.498918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and Explore the Data","metadata":{}},{"cell_type":"code","source":"data = '../input/personal-loan-modeling/Bank_Personal_Loan_Modelling.csv'\ndata_frame = pd.read_csv(data) #load and read the csv file\ndf= data_frame.copy() #making a copy to avoid changes to data\nprint(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")\n#checking the shape of the dataset\nnp.random.seed(85) \ndf.sample(10) #loading random 10 rows","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.511131Z","iopub.execute_input":"2021-07-10T19:33:06.51142Z","iopub.status.idle":"2021-07-10T19:33:06.550906Z","shell.execute_reply.started":"2021-07-10T19:33:06.511396Z","shell.execute_reply":"2021-07-10T19:33:06.549793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() # looking at the structure of the data","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.552132Z","iopub.execute_input":"2021-07-10T19:33:06.552367Z","iopub.status.idle":"2021-07-10T19:33:06.57343Z","shell.execute_reply.started":"2021-07-10T19:33:06.552341Z","shell.execute_reply":"2021-07-10T19:33:06.571869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* All the variables are of numerical datatype and there are no missing values\n* The Dependent variable is Personal Loan. We see that it has only two values; `0 & 1`. As it is a binary class variable, we will convert to category for further processing.\n* Education and Family have numerical inputs label-coded from differnet categories and should be in category datatype\n* Securities_account, CD_Account, Online and CreditCard are int datatype but with Binary inputs\n    * {i.e 0 = No and 1 = Yes} \n* The inputs can also be considered as two categories and hence for better model and analysis, we will convert them to categorical datatype    \n* Zipcode signifies the area where the customer lives and we cannot use it in its Numerical Form. Hence we will extract relevant information from it and drop the column\n","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering:","metadata":{}},{"cell_type":"code","source":"#remove the spaces in the columns\ndf.rename(columns={\"ZIP Code\":\"ZIPCode\",\"Personal Loan\":\"Personal_Loan\",\n                        \"Securities Account\":\"Securities_Account\",\"CD Account\":'CD_Account'},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.57613Z","iopub.execute_input":"2021-07-10T19:33:06.576472Z","iopub.status.idle":"2021-07-10T19:33:06.582914Z","shell.execute_reply.started":"2021-07-10T19:33:06.576442Z","shell.execute_reply":"2021-07-10T19:33:06.581631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Checking the different values in the ZIPCode variable, we conclude that all the customers in this dataset are from the State of California. \n* We will extract the county where the customer is currently residing.","metadata":{}},{"cell_type":"code","source":"# checking the number of uniques in the zip code\ndf['ZIPCode'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.584922Z","iopub.execute_input":"2021-07-10T19:33:06.585303Z","iopub.status.idle":"2021-07-10T19:33:06.604163Z","shell.execute_reply.started":"2021-07-10T19:33:06.585267Z","shell.execute_reply":"2021-07-10T19:33:06.603404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are 467 unique values in zip code.\n* In US, The first digit of a PIN indicates the zone or a region, the second indicates the sub-zone, and the third, combined with the first two, indicates the sorting district within that zone. The final three digits are assigned to individual post offices within the sorting district. \n* Hence we will group them based on the first two digits\n","metadata":{}},{"cell_type":"code","source":"df['ZIPCode'] = df['ZIPCode'].astype(str)\ndf['ZIPCode'] = df['ZIPCode'].str[0:2]\ndf['ZIPCode'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.607431Z","iopub.execute_input":"2021-07-10T19:33:06.607734Z","iopub.status.idle":"2021-07-10T19:33:06.631839Z","shell.execute_reply.started":"2021-07-10T19:33:06.607706Z","shell.execute_reply":"2021-07-10T19:33:06.630963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now the unique ZIPCodes are reduces to seven groups","metadata":{}},{"cell_type":"markdown","source":"## Fixing DataTypes","metadata":{}},{"cell_type":"code","source":"df.drop(['ID'],axis=1,inplace=True)\n#Dropping ID as its not relevant","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.632825Z","iopub.execute_input":"2021-07-10T19:33:06.633198Z","iopub.status.idle":"2021-07-10T19:33:06.642195Z","shell.execute_reply.started":"2021-07-10T19:33:06.633166Z","shell.execute_reply":"2021-07-10T19:33:06.640792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Education'] = df['Education'].astype('category')\ndf['Family'] = df['Family'].astype('category')\ndf['Personal_Loan'] = df['Personal_Loan'].astype('category')\ndf['Securities_Account'] = df['Securities_Account'].astype('category')\ndf['CD_Account'] = df['CD_Account'].astype('category')\ndf['Online'] = df['Online'].astype('category')\ndf['CreditCard'] = df['CreditCard'].astype('category')\ndf['ZIPCode'] = df['ZIPCode'].astype('category')\n\ndf.info() #rechecking the datatypes ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.643477Z","iopub.execute_input":"2021-07-10T19:33:06.64374Z","iopub.status.idle":"2021-07-10T19:33:06.680173Z","shell.execute_reply.started":"2021-07-10T19:33:06.643716Z","shell.execute_reply":"2021-07-10T19:33:06.679197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We now have 5000 rows and 13 columns and we see that the memory has also reduced\n\n## Summary of Numerical Columns","metadata":{}},{"cell_type":"code","source":"df.describe().T ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.682775Z","iopub.execute_input":"2021-07-10T19:33:06.68311Z","iopub.status.idle":"2021-07-10T19:33:06.71678Z","shell.execute_reply.started":"2021-07-10T19:33:06.683066Z","shell.execute_reply":"2021-07-10T19:33:06.71625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Mean and Median for Age is almost equal ie approx 45 yrs \n* Experience Column has a min value -3, which is could be an error and needs to be checked.\n* Mean Income is greater than median income indicating skewness. We also see a very high Max value suggesting outliers.\n* CCavg minimum value is  0.0 dollars; suggesting that the customer may not have any credit cards. The Mean and median for this variable are fairly close.\n* Similarly, the Minimum value for Mortgage is 0.0 for atleast 50% of the customers; this could mean the customers dont own a home.\n","metadata":{}},{"cell_type":"markdown","source":"## Processing Columns","metadata":{}},{"cell_type":"code","source":"df[df['Experience'] < 0]['Experience'].count() #finding columns with -ve experience values","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.717983Z","iopub.execute_input":"2021-07-10T19:33:06.71832Z","iopub.status.idle":"2021-07-10T19:33:06.744541Z","shell.execute_reply.started":"2021-07-10T19:33:06.71829Z","shell.execute_reply":"2021-07-10T19:33:06.743639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=df[(df.Experience<0)] \nprint(f\"The unique Negative Experience Array= {df1['Experience'].unique()}\")\ndf1['Age'].value_counts(ascending=True)#finding the mean and median for w.r.t Age ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.745548Z","iopub.execute_input":"2021-07-10T19:33:06.745885Z","iopub.status.idle":"2021-07-10T19:33:06.754821Z","shell.execute_reply.started":"2021-07-10T19:33:06.745856Z","shell.execute_reply":"2021-07-10T19:33:06.754311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's check the actual experience distribution for the ages above\ndf2=df[(df.Experience>=0)][df.Age<30] #Since the age for -ve experience values is less than 30 yrs\ndf2.groupby(['Age']).agg([np.mean,np.median]).Experience","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.755775Z","iopub.execute_input":"2021-07-10T19:33:06.75615Z","iopub.status.idle":"2021-07-10T19:33:06.800647Z","shell.execute_reply.started":"2021-07-10T19:33:06.756078Z","shell.execute_reply":"2021-07-10T19:33:06.799273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We see that the mean and median for the above ages are approximately equal.\n* We are missing information for Age 23 yrs old, suggesting that the data has only -ve values for experience. \n* Replacing with either Mean or Median for corresponding ages will lose information for ages 23 & 24. \n* Hence we will consider this issue as an data entry error and remove the negative sign from the values, thus making them positive experience values","metadata":{}},{"cell_type":"code","source":"multiplier = -1\nfor i in range(len(df)):\n    if df.Experience[i]<0:\n        df.Experience[i]=(df.Experience[i]*multiplier)\n(df.Experience<0).value_counts()         ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.801975Z","iopub.execute_input":"2021-07-10T19:33:06.802336Z","iopub.status.idle":"2021-07-10T19:33:06.865Z","shell.execute_reply.started":"2021-07-10T19:33:06.802304Z","shell.execute_reply":"2021-07-10T19:33:06.863719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* All negative experience values have been corrected.","metadata":{}},{"cell_type":"markdown","source":"## Missing Values :","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.866672Z","iopub.execute_input":"2021-07-10T19:33:06.866954Z","iopub.status.idle":"2021-07-10T19:33:06.880562Z","shell.execute_reply.started":"2021-07-10T19:33:06.866928Z","shell.execute_reply":"2021-07-10T19:33:06.879319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are no missing values in this dataset","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis:\n### Univariate Analysis - Numerical Columns:","metadata":{}},{"cell_type":"code","source":"#Performing Univariate Analysis to study the central tendency and dispersion\n#Plotting histogram to study distribution\nUni_num = df.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(17,75))\nfor i in range(len(Uni_num)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    sns.histplot(df[Uni_num[i]],kde=False)\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:06.881677Z","iopub.execute_input":"2021-07-10T19:33:06.881889Z","iopub.status.idle":"2021-07-10T19:33:08.341506Z","shell.execute_reply.started":"2021-07-10T19:33:06.881866Z","shell.execute_reply":"2021-07-10T19:33:08.340322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,35))\nfor i in range(len(Uni_num)):\n    plt.subplot(10,3,i+1)\n    sns.boxplot(df[Uni_num[i]],showmeans=True, color='yellow')\n    plt.tight_layout()\n    plt.title(Uni_num[i],fontsize=25)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:08.342536Z","iopub.execute_input":"2021-07-10T19:33:08.342749Z","iopub.status.idle":"2021-07-10T19:33:08.877266Z","shell.execute_reply.started":"2021-07-10T19:33:08.342727Z","shell.execute_reply":"2021-07-10T19:33:08.876437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* Age and Experience are almost normally distributed and look quite similar. This suggests a correlation between the two.\n* There is skewness is the other three variables:\n* Income:\n    - Income shows the annual salary earned by the customer and its right-skewed in distribution.Majority of customers have income less than 100K, but there are several observation in the higher end.\n\n* Credit Card Average: \n    - CCAvg has several outliers in the higher end and is heavily right-skewed. Almost 75% of customers have an average of less than 2.5(in thousand dollars). This suggests that some customers have very high charges compared to the rest\n\n* Mortgage \n    - The distribution in Mortgage variable is also heavily skewed. Almost 50 % of customers dont have a mortgage,indicating they dont own a home. We will have to analyse the mortgage for customers who only own a home to understand the distributions","metadata":{}},{"cell_type":"code","source":"df3= df[(df.Mortgage>0)]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:08.878349Z","iopub.execute_input":"2021-07-10T19:33:08.878556Z","iopub.status.idle":"2021-07-10T19:33:08.884924Z","shell.execute_reply.started":"2021-07-10T19:33:08.878535Z","shell.execute_reply":"2021-07-10T19:33:08.883908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax_box,ax_hist) = plt.subplots(2,1,sharex=True ,\n                                        figsize=(10,9),\n                                        gridspec_kw = {\"height_ratios\": (.35, .65)})\nsns.boxplot(df3.Mortgage, ax=ax_box, showmeans=True, color='orange')\nsns.histplot(df3.Mortgage, ax=ax_hist,kde=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:08.888061Z","iopub.execute_input":"2021-07-10T19:33:08.888354Z","iopub.status.idle":"2021-07-10T19:33:09.367561Z","shell.execute_reply.started":"2021-07-10T19:33:08.888325Z","shell.execute_reply":"2021-07-10T19:33:09.366292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights**:\n* The distribution is again right-skewed with an increased Mean of around 183K dollars.\n* There are again several outliers in the higher end. We suspect this could be due to the location of the homes, as higher land value could mean higher mortgage price. Over 75% of the customers have Mortgageare below 230K dollars.","metadata":{}},{"cell_type":"markdown","source":"### Univariate Analysis - Categorical Columns:","metadata":{}},{"cell_type":"code","source":"categorical_val = df.select_dtypes(exclude=np.number).columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:09.369673Z","iopub.execute_input":"2021-07-10T19:33:09.369907Z","iopub.status.idle":"2021-07-10T19:33:09.379532Z","shell.execute_reply.started":"2021-07-10T19:33:09.369884Z","shell.execute_reply":"2021-07-10T19:33:09.378621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,75))\nfor i in range(len(categorical_val)):     #creating a loop that will show the plots for the columns in one plot\n    plt.subplot(18,3,i+1)\n    ax=sns.countplot(df[categorical_val[i]],palette='Spectral')\n    plt.tight_layout()\n    plt.title(categorical_val[i],fontsize=25)\n    total = len (df[categorical_val[i]])\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()/total) # percentage of each class of the category\n        x = p.get_x() + (p.get_width() / 2)-0.1  # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        ax.annotate(percentage, (x, y), size = 12.5,color='black') # To annonate\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:09.380641Z","iopub.execute_input":"2021-07-10T19:33:09.380888Z","iopub.status.idle":"2021-07-10T19:33:10.822222Z","shell.execute_reply.started":"2021-07-10T19:33:09.380864Z","shell.execute_reply":"2021-07-10T19:33:10.820667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* 29.4% of customers are from region 94 followed by reion 92 at19.8%\n* 29.4 % of customers are of single-family household, with Family variable having four unique values.\n* Education has three unique values with 41.9% of at Undergrad level(1).\n* Personal_Loans is the Dependent variable and we see that there is heavy imbalance. Only 9.6% of customers in the data have accepted a loan from the previous campaign\n* 89.6% of customers dont have a Securities account whereas 94% of customers dont have a CD account.\n* We that 59.7 % of customers use the bank's online facilities and about 70.6% dont have credit cards issue by another bank.","metadata":{}},{"cell_type":"markdown","source":"### Correlation Matrix","metadata":{}},{"cell_type":"code","source":"corr= df.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr,annot= True,vmin=0,vmax=1, cmap='RdYlGn_r',linewidths=0.75)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:10.823533Z","iopub.execute_input":"2021-07-10T19:33:10.823844Z","iopub.status.idle":"2021-07-10T19:33:11.101568Z","shell.execute_reply.started":"2021-07-10T19:33:10.823811Z","shell.execute_reply":"2021-07-10T19:33:11.100433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* Age and Experience have the highest correlation at 0.99. We suspect multi-collinearity between these variables\n* Income and CC_Avg have the next highest positive correlation at 0.65. This suggests that customers with higher income have higher Credit card charges.\n* Income and CCAvg have a positive correlation with Mortgage.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data=df,hue='Personal_Loan')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-10T19:33:11.103586Z","iopub.execute_input":"2021-07-10T19:33:11.10388Z","iopub.status.idle":"2021-07-10T19:33:23.08308Z","shell.execute_reply.started":"2021-07-10T19:33:11.103852Z","shell.execute_reply":"2021-07-10T19:33:23.081852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* The pair plot shows a more varying distribution in the variables between customers who took a loan and those who didnt. \n* We see again that the distribution for Age and Experience is very similar. This could suggest possible multicollinearity\n* There are overlaps that make it difficults to interpret who has personal loans and who doesnt, hence we will analyse futher with other plots","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"# For all numerical variables with Personal_Loan\nplt.figure(figsize=(20,10))\nfor i, variable in enumerate(Uni_num):\n                     plt.subplot(3,2,i+1)\n                     sns.boxplot(df['Personal_Loan'],df[variable],palette=\"Dark2\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:23.084327Z","iopub.execute_input":"2021-07-10T19:33:23.084619Z","iopub.status.idle":"2021-07-10T19:33:23.950355Z","shell.execute_reply.started":"2021-07-10T19:33:23.084589Z","shell.execute_reply":"2021-07-10T19:33:23.9494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The mean values for Age is the same for both categories of Personal Loans\n* Similarly the mean values for Experience is also almost equal for both categories of Personal Loan. Both these variables dont have any outliers\n\n* Customers who have Personal Loans also have high Mean **Income and CreditCard Average** compared to customers who dont have a loan. Interesting we see several outliers in the higher end for both these variables in Class **0**. \n\n* The mean value for Mortgage at both levels in 0.0(in dollars). This is because majority of the customers dont have Mortgages. However, we see that customers with higher mortgages have Personal loans. But, we also see that there are several outliers in the high end again for customers who dont have a loan.\n\n* The above plot, suggests a correlation between Income,CCavg and Mortgage. Customers with high values for these variables have taken loans. This could suggests them as possible features of customers that can be targeted.","metadata":{}},{"cell_type":"code","source":"#Stacked plot of categorical variables with Personal Loans\ndef stacked_plot(x):\n    sns.set(palette='Accent')\n    tab1 = pd.crosstab(x,df['Personal_Loan'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df['Personal_Loan'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(10,5))\n    plt.legend(loc='lower left', frameon=True)\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n    plt.ylabel('Percentage')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:23.951563Z","iopub.execute_input":"2021-07-10T19:33:23.951809Z","iopub.status.idle":"2021-07-10T19:33:23.958736Z","shell.execute_reply.started":"2021-07-10T19:33:23.951783Z","shell.execute_reply":"2021-07-10T19:33:23.957764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_plot(df.ZIPCode)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:23.959719Z","iopub.execute_input":"2021-07-10T19:33:23.959928Z","iopub.status.idle":"2021-07-10T19:33:24.220616Z","shell.execute_reply.started":"2021-07-10T19:33:23.959906Z","shell.execute_reply":"2021-07-10T19:33:24.220106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* All the sub-regions show fairly equal distribution among customers who purchased a loan","metadata":{}},{"cell_type":"code","source":"stacked_plot(df.Family)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:24.221525Z","iopub.execute_input":"2021-07-10T19:33:24.221947Z","iopub.status.idle":"2021-07-10T19:33:24.420801Z","shell.execute_reply.started":"2021-07-10T19:33:24.221916Z","shell.execute_reply":"2021-07-10T19:33:24.420252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n- More Customers with larger family size(3&4) have Personal Loans. ","metadata":{}},{"cell_type":"code","source":"stacked_plot(df.Education)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:24.421757Z","iopub.execute_input":"2021-07-10T19:33:24.422231Z","iopub.status.idle":"2021-07-10T19:33:24.609985Z","shell.execute_reply.started":"2021-07-10T19:33:24.422198Z","shell.execute_reply":"2021-07-10T19:33:24.608883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Customers with higher Education levels have taken Personal Loans.","metadata":{}},{"cell_type":"code","source":"stacked_plot(df.Securities_Account)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:24.610967Z","iopub.execute_input":"2021-07-10T19:33:24.611199Z","iopub.status.idle":"2021-07-10T19:33:24.797695Z","shell.execute_reply.started":"2021-07-10T19:33:24.611174Z","shell.execute_reply":"2021-07-10T19:33:24.796616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Majority of the customers dont have a Securities Account out of which 420 have Personal loans\n* Remaining customers who do have an account; only 60 have loans. ","metadata":{}},{"cell_type":"code","source":"stacked_plot(df.CD_Account)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:24.798763Z","iopub.execute_input":"2021-07-10T19:33:24.798955Z","iopub.status.idle":"2021-07-10T19:33:24.974468Z","shell.execute_reply.started":"2021-07-10T19:33:24.798934Z","shell.execute_reply":"2021-07-10T19:33:24.973363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In the 302 customers have a CD_account, almost 50% have a Personal Loan\n* This suggests that customers who have a CD_account are likely to buy loans and can be a possible target feature.","metadata":{}},{"cell_type":"code","source":"stacked_plot(df.Online)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:24.975758Z","iopub.execute_input":"2021-07-10T19:33:24.975995Z","iopub.status.idle":"2021-07-10T19:33:25.189229Z","shell.execute_reply.started":"2021-07-10T19:33:24.975971Z","shell.execute_reply":"2021-07-10T19:33:25.188259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 10% of customers in both classes of Online variable have purchased loans","metadata":{}},{"cell_type":"code","source":"stacked_plot(df.CreditCard)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:25.190215Z","iopub.execute_input":"2021-07-10T19:33:25.190406Z","iopub.status.idle":"2021-07-10T19:33:25.379981Z","shell.execute_reply.started":"2021-07-10T19:33:25.190385Z","shell.execute_reply":"2021-07-10T19:33:25.379097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have more customers who dont have Credit card with other banks\n* Again 10% of customers in both Credit Card classes have purchased loans","metadata":{}},{"cell_type":"markdown","source":"## Multi-variate Analysis","metadata":{}},{"cell_type":"code","source":"#Income Vs Education Vs Personal_Loan\nplt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='Income',x='Education',hue='Personal_Loan')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:25.380838Z","iopub.execute_input":"2021-07-10T19:33:25.381016Z","iopub.status.idle":"2021-07-10T19:33:25.611899Z","shell.execute_reply.started":"2021-07-10T19:33:25.380996Z","shell.execute_reply":"2021-07-10T19:33:25.610495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As Education level increases, Mean Income also increases.\n* Customers with Education level 2 and 3 who have personal loans have a much higher mean income than Education level 1 customers","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='Income',x='Family',hue='Personal_Loan')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:25.61466Z","iopub.execute_input":"2021-07-10T19:33:25.614883Z","iopub.status.idle":"2021-07-10T19:33:25.870393Z","shell.execute_reply.started":"2021-07-10T19:33:25.61486Z","shell.execute_reply":"2021-07-10T19:33:25.869324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Income level among all Family groups is significantly higher for customers who have a Personal Loan. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='Mortgage',x='Family',hue='Personal_Loan')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:25.871499Z","iopub.execute_input":"2021-07-10T19:33:25.871721Z","iopub.status.idle":"2021-07-10T19:33:26.238452Z","shell.execute_reply.started":"2021-07-10T19:33:25.871698Z","shell.execute_reply":"2021-07-10T19:33:26.237399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are several outliers in Family size 1 and 2 for customers who dont have a Personal loan compares to the rest.\n* We also see that as Family size increases, the Mortgage value also rises and the customers have Personal Loans","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df,y='CCAvg',x='CreditCard',hue='Personal_Loan')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:26.239582Z","iopub.execute_input":"2021-07-10T19:33:26.239805Z","iopub.status.idle":"2021-07-10T19:33:26.428995Z","shell.execute_reply.started":"2021-07-10T19:33:26.239783Z","shell.execute_reply":"2021-07-10T19:33:26.427967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Customers who have Personal loans have a higher credit card Average.\n* There are several outliers in customers who dont have personal loans. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.scatterplot(data=df,y='Income',x='CCAvg',hue='Personal_Loan')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:26.430183Z","iopub.execute_input":"2021-07-10T19:33:26.430539Z","iopub.status.idle":"2021-07-10T19:33:26.775048Z","shell.execute_reply.started":"2021-07-10T19:33:26.430506Z","shell.execute_reply":"2021-07-10T19:33:26.774114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* More Customers with higher income and CCAvg `>2.5(in thousand dollars)` have personal loans.","metadata":{}},{"cell_type":"markdown","source":"## Data Pre-Processing:\n\n### Outliers Treatment:\n* Income, CCAvg and Mortgage have very high outliers in the higher end and must be treated.\n* Since we will also be creating a Decision Tree model(Decision Trees are not influenced by outliers) \n  we will make a copy of the dataset before proceeding with outlier treatment.","metadata":{}},{"cell_type":"code","source":"df1=df.copy() # new copy for Decision Tree model","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:26.776402Z","iopub.execute_input":"2021-07-10T19:33:26.776653Z","iopub.status.idle":"2021-07-10T19:33:26.781844Z","shell.execute_reply.started":"2021-07-10T19:33:26.776627Z","shell.execute_reply":"2021-07-10T19:33:26.780968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets treat outliers by flooring and capping\ndef treat_outliers(df,col):\n   \n    Q1=df[col].quantile(0.25) # 25th quantile\n    Q3=df[col].quantile(0.75)  # 75th quantile\n    IQR=Q3-Q1\n    Lower_Whisker = Q1 - 1.5*IQR \n    Upper_Whisker = Q3 + 1.5*IQR\n    df[col] = np.clip(df[col], Lower_Whisker, Upper_Whisker) # all the values samller than Lower_Whisker will be assigned value of Lower_whisker \n                                                            # and all the values above upper_whisker will be assigned value of upper_Whisker \n    return df\n\ndef treat_outliers_all(df, col_list): # treat outliers in numerical column of Dataframe\n    \n    for c in col_list:\n        df = treat_outliers(df,c)\n        \n        \n    return df ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:26.782727Z","iopub.execute_input":"2021-07-10T19:33:26.782927Z","iopub.status.idle":"2021-07-10T19:33:26.798106Z","shell.execute_reply.started":"2021-07-10T19:33:26.782905Z","shell.execute_reply":"2021-07-10T19:33:26.797474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_treatment = {'Age','Experience'} # These two variables dont have outliers\nnumerical_col = [ele for ele in Uni_num if ele not in no_treatment] \n#Applying outlier treatment\ndf = treat_outliers_all(df,numerical_col)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:26.798942Z","iopub.execute_input":"2021-07-10T19:33:26.799162Z","iopub.status.idle":"2021-07-10T19:33:26.827495Z","shell.execute_reply.started":"2021-07-10T19:33:26.799141Z","shell.execute_reply":"2021-07-10T19:33:26.825727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* All Outliers are treated","metadata":{}},{"cell_type":"markdown","source":"## Model Building \n\n### Model Evaluation Criterion \n#### Model can make two kinds of wrong predictions: \n1. Wrongly Identify customers as loan borrowers but they are not - False Positive\n2. Wrongly identifying customers as not borrowers but they actually buy loans - False Negative\n\n* Since the Banks wants to identify all potential customers who will purchase a loan, the False Negative value must be less.\n\n#### How to reduce losses\n* Recall is the Performance metric that must be improved.\n* The Recall score must be maximised and greater the score the less the chance of missing potential customers.\n\n\n#### Creating a Confusion Matrix","metadata":{}},{"cell_type":"code","source":"#Defining a function for Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nsns.set(font_scale=2.0) # to set font size for the matrix\ndef make_confusion_matrix(y_actual,y_predict):\n    '''\n    y_predict: prediction of class\n    y_actual : ground truth  \n    '''\n    cm=confusion_matrix(y_actual,y_predict)\n    group_names = ['True -ve','False +ve','False -ve','True +ve']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(cm, annot=labels,fmt='',cmap='Blues')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:26.828811Z","iopub.execute_input":"2021-07-10T19:33:26.829053Z","iopub.status.idle":"2021-07-10T19:33:27.148359Z","shell.execute_reply.started":"2021-07-10T19:33:26.829031Z","shell.execute_reply":"2021-07-10T19:33:27.146957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing all necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nfrom sklearn import metrics #accuracy,confusion metrics, etc\nfrom sklearn import datasets \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:27.149798Z","iopub.execute_input":"2021-07-10T19:33:27.150148Z","iopub.status.idle":"2021-07-10T19:33:27.343005Z","shell.execute_reply.started":"2021-07-10T19:33:27.150116Z","shell.execute_reply":"2021-07-10T19:33:27.342346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Defining X and Y variables\nX = df.drop(['Personal_Loan'], axis=1) #dropping the dependent variable\nY = df[['Personal_Loan']]\n\n#Convert categorical variables to dummy variables\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30,random_state=29) # 70% train set and 30% test set\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:27.344114Z","iopub.execute_input":"2021-07-10T19:33:27.344435Z","iopub.status.idle":"2021-07-10T19:33:27.3651Z","shell.execute_reply.started":"2021-07-10T19:33:27.344405Z","shell.execute_reply":"2021-07-10T19:33:27.364021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression (with Sklearn library)","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(solver='saga',max_iter=1000,penalty='none',verbose=True,n_jobs=1,random_state=29)\n\n# There arae several optimizer, we are using optimizer called as 'saga' with max_iter equal to 1000 \n# max_iter indicates number of iteration needed to converge\n\nlogreg.fit(X_train, y_train)\npred_train = logreg.predict(X_train)\npred_test = logreg.predict(X_test)\n\n#Checking the Accuracy of the model:\nprint('\\nAccuracy on train data:%.6f'%accuracy_score(y_train, pred_train) )\nprint('Accuracy on test data:%.6f' %accuracy_score(y_test, pred_test))\n#checking the Recall metrics of the model:\nprint('\\nRecall on train data:%.6f'%recall_score(y_train, pred_train) )\nprint('Recall on test data:%.6f'%recall_score(y_test, pred_test))\n#checking the Precision metrics of model:\nprint(\"\\nPrecision on training set : \",precision_score(y_train, pred_train))\nprint(\"Precision on test set : \",precision_score(y_test, pred_test))\n\nprint(\"\\nF1 Score on training set : \",f1_score(y_train, pred_train))\nprint(\"F1 Score on test set : \",f1_score(y_test, pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:27.365988Z","iopub.execute_input":"2021-07-10T19:33:27.366196Z","iopub.status.idle":"2021-07-10T19:33:28.479797Z","shell.execute_reply.started":"2021-07-10T19:33:27.366175Z","shell.execute_reply":"2021-07-10T19:33:28.478731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_confusion_matrix(y_test,pred_test) #display confusion matrix for test set","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:28.48618Z","iopub.execute_input":"2021-07-10T19:33:28.486521Z","iopub.status.idle":"2021-07-10T19:33:28.732133Z","shell.execute_reply.started":"2021-07-10T19:33:28.486489Z","shell.execute_reply":"2021-07-10T19:33:28.730343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n* The Logistic Regression model has good accuracy by poor Recall values.\n* This could be due to multi-collinearity and insignificant values in the model.\n* To check this we will build a model using statsmodels library","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression Using Stats Model:\n* Using Stats Model in Python, we will get an list of statistical results for each estimator.\n* Stats Model is also used to further conduct tests and statistical data exploration","metadata":{}},{"cell_type":"code","source":"# adding constant to training and test set\nX_train = sm.add_constant(X_train)\nX_test = sm.add_constant(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:28.735968Z","iopub.execute_input":"2021-07-10T19:33:28.736224Z","iopub.status.idle":"2021-07-10T19:33:28.750745Z","shell.execute_reply.started":"2021-07-10T19:33:28.7362Z","shell.execute_reply":"2021-07-10T19:33:28.749582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining a funciton to call all the performance metrics scores\ndef metrics_score(model,train,test,train_y,test_y):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable '''\n     \n    pred = model.predict(train)\n    pred_train = list(map(round,pred))\n    pred1 = model.predict(test)\n    pred_test = list(map(round,pred1))\n   \n    print(\"Accuracy on training set : \",accuracy_score(train_y,pred_train))\n    print(\"Accuracy on test set : \",accuracy_score(test_y,pred_test))\n    print(\"Recall on training set : \",recall_score(train_y,pred_train))\n    print(\"Recall on test set : \",recall_score(test_y,pred_test))\n    print(\"Precision on training set : \",precision_score(train_y,pred_train))\n    print(\"Precision on test set : \",precision_score(test_y,pred_test))\n    print(\"F1 on training set : \",f1_score(train_y,pred_train))\n    print(\"F1 on test set : \",f1_score(test_y,pred_test))\n        \n  ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:28.751967Z","iopub.execute_input":"2021-07-10T19:33:28.752268Z","iopub.status.idle":"2021-07-10T19:33:28.763305Z","shell.execute_reply.started":"2021-07-10T19:33:28.752181Z","shell.execute_reply":"2021-07-10T19:33:28.762435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit = sm.Logit(y_train, X_train) #logistic regression\nlg = logit.fit(warn_convergence =False) \n#Checking model performance \nmetrics_score(lg,X_train,X_test,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:28.764776Z","iopub.execute_input":"2021-07-10T19:33:28.76512Z","iopub.status.idle":"2021-07-10T19:33:28.872687Z","shell.execute_reply.started":"2021-07-10T19:33:28.765072Z","shell.execute_reply":"2021-07-10T19:33:28.8721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The Accuracy for the test set is 0.96 which looks good\n* But the Recall for the test set is only 0.68\n* We must further analyse this model and check if the perfomance can be improved.","metadata":{}},{"cell_type":"code","source":"cm_pred = lg.predict(X_test)\npred_test = list(map(round,cm_pred))\nmake_confusion_matrix(y_test,pred_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:28.873845Z","iopub.execute_input":"2021-07-10T19:33:28.874291Z","iopub.status.idle":"2021-07-10T19:33:29.093891Z","shell.execute_reply.started":"2021-07-10T19:33:28.874258Z","shell.execute_reply":"2021-07-10T19:33:29.09336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The True positive values('ie predicting customers who will purchase loan) is only 6.2% .\n* The False negative is at 2.93%. We have to check if we can bring this lower","metadata":{}},{"cell_type":"markdown","source":"## Checking for Multicollinearity using VIF Scores:\n* Multicollinearity occurs when there is correlation between the predictor variables.\n* Variance Inflation factor: Variance inflation factors measure the inflation in the variances of the regression coefficients estimates due to collinearities that exist among the predictors.\n* If VIF is 1 then there is no correlation among the predictor variables. Whereas if VIF exceeds 5, we say there is moderate multi-collinearity and if it is 10 or exceeding 10, it shows signs of high multi-collinearity. \n* Alternatively we can check the significance of a variable to the model with the P-value","metadata":{}},{"cell_type":"code","source":"#checking the VIF scores for X_train set\nvif_series1 = pd.Series([variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])],index=X_train.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series1))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.094888Z","iopub.execute_input":"2021-07-10T19:33:29.095255Z","iopub.status.idle":"2021-07-10T19:33:29.183882Z","shell.execute_reply.started":"2021-07-10T19:33:29.095231Z","shell.execute_reply":"2021-07-10T19:33:29.183311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n* Age and Experience have the highest VIF values.\n* We already suspected that these variables might have multicollinearity which is proven true with the above values.\n* We will remove Experience column to remove multi-collinearity","metadata":{}},{"cell_type":"code","source":"X_train1 = X_train.drop('Experience', axis=1)\nX_test1 = X_test.drop('Experience', axis=1)\nvif_series2 = pd.Series([variance_inflation_factor(X_train1.values,i) for i in range(X_train1.shape[1])],index=X_train1.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series2))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.184932Z","iopub.execute_input":"2021-07-10T19:33:29.185357Z","iopub.status.idle":"2021-07-10T19:33:29.278434Z","shell.execute_reply.started":"2021-07-10T19:33:29.185328Z","shell.execute_reply":"2021-07-10T19:33:29.277738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The VIF scores for all the variables is less than 5 and there is no more multi-collinearity in the model.\n* Let's check the model performance","metadata":{}},{"cell_type":"code","source":"logit1=sm.Logit(y_train,X_train1)\nlg1=logit1.fit()\nmetrics_score(lg1,X_train1,X_test1,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.279647Z","iopub.execute_input":"2021-07-10T19:33:29.280147Z","iopub.status.idle":"2021-07-10T19:33:29.361357Z","shell.execute_reply.started":"2021-07-10T19:33:29.280111Z","shell.execute_reply":"2021-07-10T19:33:29.360621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The Accuracy and Recall values for test set has a slight increase\n\n\n**Variable Significance:**\n- The P-Value of the variable indicates if the predictor variable is significant or not.\n- The level of significance is 0.05 and any p-value less than 0.05 , then that variable would be considered significant.","metadata":{}},{"cell_type":"code","source":"print(lg1.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.362688Z","iopub.execute_input":"2021-07-10T19:33:29.363205Z","iopub.status.idle":"2021-07-10T19:33:29.421933Z","shell.execute_reply.started":"2021-07-10T19:33:29.363167Z","shell.execute_reply":"2021-07-10T19:33:29.421215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Insights**\n- In the above model the following variables have p-value >0.05:\n    - Age, Mortgage, Family2, SecuritiesAccount_1 and all the Dummy variables of the Variable ZIPCode.\n- We know that Mortgage has a positive correlation to Personal Loan and Family2 despite having high p-value cannot be removed as its part of the Family category variable.\n- Let's remove all the dummy variables of Category Region and check model performance","metadata":{}},{"cell_type":"code","source":"#dropping all dummy variables of Region\nX_train2 = X_train1.drop(['ZIPCode_91','ZIPCode_92','ZIPCode_93','ZIPCode_94','ZIPCode_95','ZIPCode_96'], axis=1)\nX_test2 = X_test1.drop(['ZIPCode_91','ZIPCode_92','ZIPCode_93','ZIPCode_94','ZIPCode_95','ZIPCode_96'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.423255Z","iopub.execute_input":"2021-07-10T19:33:29.423732Z","iopub.status.idle":"2021-07-10T19:33:29.431248Z","shell.execute_reply.started":"2021-07-10T19:33:29.423695Z","shell.execute_reply":"2021-07-10T19:33:29.43045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit2=sm.Logit(y_train,X_train2)\nlg2=logit2.fit()\n#print(lg2.summary())\n\n#Lets look at model performance \nmetrics_score(lg2,X_train2,X_test2,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.432583Z","iopub.execute_input":"2021-07-10T19:33:29.433068Z","iopub.status.idle":"2021-07-10T19:33:29.504302Z","shell.execute_reply.started":"2021-07-10T19:33:29.433031Z","shell.execute_reply":"2021-07-10T19:33:29.502463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Recall for test set has dropped to 0.686\n* Next let's drop Age variable and check model performance ","metadata":{}},{"cell_type":"code","source":"#Let's drop Age \nX_train3 = X_train2.drop(['Age'],axis=1)\nX_test3 = X_test2.drop(['Age'],axis=1)\nlogit3=sm.Logit(y_train,X_train3)\nlg3=logit3.fit()\n\nmetrics_score(lg3,X_train3,X_test3,y_train,y_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-10T19:33:29.5055Z","iopub.execute_input":"2021-07-10T19:33:29.505758Z","iopub.status.idle":"2021-07-10T19:33:29.632106Z","shell.execute_reply.started":"2021-07-10T19:33:29.505731Z","shell.execute_reply":"2021-07-10T19:33:29.631033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Recall for test set did not change.\n* Next let's drop Mortgage variable for this model and check performance again.\n* Even though Mortgage has a positive coefficient, it is compartitively low to the rest of the variables.","metadata":{}},{"cell_type":"code","source":"#Let's drop Mortgage \nX_train4 = X_train3.drop(['Mortgage'],axis=1)\nX_test4 = X_test3.drop(['Mortgage'],axis=1)\nlogit4=sm.Logit(y_train,X_train4)\nlg4=logit4.fit()\nmetrics_score(lg4,X_train4,X_test4,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.635206Z","iopub.execute_input":"2021-07-10T19:33:29.635509Z","iopub.status.idle":"2021-07-10T19:33:29.704742Z","shell.execute_reply.started":"2021-07-10T19:33:29.635481Z","shell.execute_reply":"2021-07-10T19:33:29.70405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Recall for test set improved to 0.6934\n* Let's check the model summary","metadata":{}},{"cell_type":"code","source":"print(lg4.summary())","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.707873Z","iopub.execute_input":"2021-07-10T19:33:29.709566Z","iopub.status.idle":"2021-07-10T19:33:29.747459Z","shell.execute_reply.started":"2021-07-10T19:33:29.709519Z","shell.execute_reply":"2021-07-10T19:33:29.746814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are no more insignificant variables \n\n**Hence, we will use lg4 as the final model**\n\n## Observations from Model:\n\n### Coefficient Interpretations:\n\n* Income, CCAvg, Mortgage, Family_3, Family_4, Both Education variables and CD_account1 have positive co-efficients; which indicate that an increase in their values will increase the probability of customers having Personal loans\n\n\n* Family2, Securities_Account_1,Online_1 and CreditCard_1 have a negative co-efficient; Which indicates that an increase in their value would decrease the probability  Customer's having Personal loans\n\n\n### Converting Coefficients to odds: \n* In a Logistic Regression model, the coefficients of the variable is the Log of odds. \n* We will calculate the  odds ration to quantify the strength of the assosiation between the dependent and independent variables\n\n**Odds ratio =  Exp(coef)**\n\n**Probability  = odds/(1+odds)**","metadata":{}},{"cell_type":"code","source":"#Calculate Odds Ratio, probability\n##create a data frame to collate Odds ratio, probability and p-value of the coef\nlgcoef = pd.DataFrame(lg4.params, columns=['coef']) #getting the coefficent from lg4 model\nlgcoef.loc[:, \"Odds_ratio\"] = np.exp(lgcoef.coef) #calculate the odds ratio\n\nlgcoef['probability'] = lgcoef['Odds_ratio']/(1+lgcoef['Odds_ratio']) #calculate the probability \nlgcoef['pval']=lg.pvalues\npd.options.display.float_format = '{:.2f}'.format","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.750458Z","iopub.execute_input":"2021-07-10T19:33:29.751961Z","iopub.status.idle":"2021-07-10T19:33:29.763382Z","shell.execute_reply.started":"2021-07-10T19:33:29.751927Z","shell.execute_reply":"2021-07-10T19:33:29.762609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter by significant p-value (pval <0.005) and sort descending by Odds ratio\nlgcoef = lgcoef.sort_values(by=\"Odds_ratio\", ascending=False)\npval_filter = lgcoef['pval']<=0.005\nlgcoef[pval_filter]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.767448Z","iopub.execute_input":"2021-07-10T19:33:29.769301Z","iopub.status.idle":"2021-07-10T19:33:29.787811Z","shell.execute_reply.started":"2021-07-10T19:33:29.76926Z","shell.execute_reply":"2021-07-10T19:33:29.786911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* Customers with higher Education level, ie, Graduate and Post-Graduate level and with a CD Account have a 98% probability of having personal loans\n* Customers with larger family size of 3 and 4 have higher probabilities 91% and 82% respectively of having a personal loan.\n* Other Significant Variables that have moderate to high probability are Income , Customers who use the Bank's online services and those who have addditional credit cards from Other Banks.\n\n\n### Identifying Key Variables:\n* The model indicates the following key variables to have a strong relationship with the dependent variable Personal_Loan\n    - Education \n    - CD_Account \n    - Family\n    - CCAvg\n    - Income\n    - Online and\n    - CreditCard","metadata":{}},{"cell_type":"markdown","source":"**Confusion matrix Prediction on lg4 model Test Data**","metadata":{}},{"cell_type":"code","source":"pred1 = lg4.predict(X_test4)\npred_test1 = list(map(round,pred1))\nmake_confusion_matrix(y_test,pred_test1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:29.791582Z","iopub.execute_input":"2021-07-10T19:33:29.793377Z","iopub.status.idle":"2021-07-10T19:33:30.037852Z","shell.execute_reply.started":"2021-07-10T19:33:29.793339Z","shell.execute_reply":"2021-07-10T19:33:30.036698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n- In the lg4 model:\n    - True positive is 6.33% \n    - True Negative is 90.33%\n    - False Positive is 0.53% \n    - False Negative is 2.8%\n- We need to improve the True Positive and reduce False Negative values.\n- Let's check for model improvement","metadata":{}},{"cell_type":"markdown","source":"## Model Performance Improvement\n\n###  AUC-ROC curve:\n* This is a performance measurement for classification problems at various threshold settings","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(y_test, lg4.predict(X_test4))\nfpr, tpr, thresholds = roc_curve(y_test, lg4.predict(X_test4))\nplt.figure(figsize=(13,8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.038958Z","iopub.execute_input":"2021-07-10T19:33:30.039284Z","iopub.status.idle":"2021-07-10T19:33:30.35599Z","shell.execute_reply.started":"2021-07-10T19:33:30.039252Z","shell.execute_reply":"2021-07-10T19:33:30.354787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Optimal Threshold from AUC-ROC**\n\n* The optimal threshold cut off will be where True Positive Rate is high and False Positive Rate is low","metadata":{}},{"cell_type":"code","source":"# Optimal threshold as per AUC-ROC curve\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal = thresholds[optimal_idx]\nprint(optimal)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.356954Z","iopub.execute_input":"2021-07-10T19:33:30.357195Z","iopub.status.idle":"2021-07-10T19:33:30.362633Z","shell.execute_reply.started":"2021-07-10T19:33:30.357172Z","shell.execute_reply":"2021-07-10T19:33:30.361458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the optimal threshold to predict model for test data\ny_pred_train = (lg4.predict(X_train4)>optimal).astype(int)\ny_pred_test = (lg4.predict(X_test4)>optimal).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.364021Z","iopub.execute_input":"2021-07-10T19:33:30.364328Z","iopub.status.idle":"2021-07-10T19:33:30.382169Z","shell.execute_reply.started":"2021-07-10T19:33:30.364304Z","shell.execute_reply":"2021-07-10T19:33:30.38121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix for test set for lg4 model\nmake_confusion_matrix(y_test,y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.383331Z","iopub.execute_input":"2021-07-10T19:33:30.383607Z","iopub.status.idle":"2021-07-10T19:33:30.61611Z","shell.execute_reply.started":"2021-07-10T19:33:30.38358Z","shell.execute_reply":"2021-07-10T19:33:30.615621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy on training set : \",accuracy_score(y_train,y_pred_train))\nprint(\"Accuracy on test set : \",accuracy_score(y_test,y_pred_test))\nprint(\"\\nRecall on training set : \",recall_score(y_train,y_pred_train))\nprint(\"Recall on test set : \",recall_score(y_test,y_pred_test))\nprint(\"\\nPrecision on training set : \",precision_score(y_train,y_pred_train))\nprint(\"Precision on test set : \",precision_score(y_test, y_pred_test))\n\nprint(\"\\nF1 Score on training set : \",f1_score(y_train,y_pred_train))\nprint(\"F1 Score on test set : \",f1_score(y_test, y_pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.616866Z","iopub.execute_input":"2021-07-10T19:33:30.617165Z","iopub.status.idle":"2021-07-10T19:33:30.63904Z","shell.execute_reply.started":"2021-07-10T19:33:30.617144Z","shell.execute_reply":"2021-07-10T19:33:30.638211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n* At Optimal Threshold, the Accuracy of the test set has reduced to 0.94 \n* But the Recall score for Test set has rised significantly to 0.8321\n\n\n### Percision-Recall Curve\n\n* This curve will plot the precision and Recall values for the lg4 model.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\ny_PresRec=lg4.predict(X_train4)\nprec, rec, tre = precision_recall_curve(y_train, y_PresRec)\n\ndef plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')\n    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.ylim([0,1])\nplt.figure(figsize=(10,7))\nplot_prec_recall_vs_tresh(prec, rec, tre)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.639964Z","iopub.execute_input":"2021-07-10T19:33:30.640288Z","iopub.status.idle":"2021-07-10T19:33:30.835743Z","shell.execute_reply.started":"2021-07-10T19:33:30.640264Z","shell.execute_reply":"2021-07-10T19:33:30.834994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the optimal threshold to predict model for test data\noptimal_threshold = 0.25 # we get a balanced recall and precision at this threshold\n\ny_pred_train1 = (lg4.predict(X_train4)>optimal_threshold).astype(int)\ny_pred_test1 = (lg4.predict(X_test4)>optimal_threshold).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.837053Z","iopub.execute_input":"2021-07-10T19:33:30.837419Z","iopub.status.idle":"2021-07-10T19:33:30.844952Z","shell.execute_reply.started":"2021-07-10T19:33:30.837383Z","shell.execute_reply":"2021-07-10T19:33:30.844122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix for test set for lg4 model\nmake_confusion_matrix(y_test,y_pred_test1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:30.846195Z","iopub.execute_input":"2021-07-10T19:33:30.846459Z","iopub.status.idle":"2021-07-10T19:33:31.076762Z","shell.execute_reply.started":"2021-07-10T19:33:30.846433Z","shell.execute_reply":"2021-07-10T19:33:31.076045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy on training set : \",accuracy_score(y_train,y_pred_train1))\nprint(\"Accuracy on test set : \",accuracy_score(y_test,y_pred_test1))\nprint(\"\\nRecall on training set : \",recall_score(y_train,y_pred_train1))\nprint(\"Recall on test set : \",recall_score(y_test,y_pred_test1))\nprint(\"\\nPrecision on training set : \",precision_score(y_train,y_pred_train1))\nprint(\"Precision on test set : \",precision_score(y_test, y_pred_test1))\n\nprint(\"\\nF1 Score on training set : \",f1_score(y_train,y_pred_train1))\nprint(\"F1 Score on test set : \",f1_score(y_test, y_pred_test1))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:31.07765Z","iopub.execute_input":"2021-07-10T19:33:31.07794Z","iopub.status.idle":"2021-07-10T19:33:31.103228Z","shell.execute_reply.started":"2021-07-10T19:33:31.077917Z","shell.execute_reply":"2021-07-10T19:33:31.102176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Observations**\n* At 0.25 threshold, Recall has dropped to 0.810 for test set \n* But the Precision value has increased and Accuracy remains same.\n* Since Precision is not the defining metric; AUC-ROC threshold value has a better model ","metadata":{}},{"cell_type":"markdown","source":"# Sequential Feature Selector method:\n* This method will begin with an empty model and will add in each forward step the one variable that gives the maximum improvement to the model.\n* The aim of this method is to discrad deceptive features and also speed training and testing process","metadata":{}},{"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt ","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:31.104202Z","iopub.execute_input":"2021-07-10T19:33:31.104431Z","iopub.status.idle":"2021-07-10T19:33:31.166737Z","shell.execute_reply.started":"2021-07-10T19:33:31.104405Z","shell.execute_reply":"2021-07-10T19:33:31.165679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Defining X and Y variables\nX = df.drop(['Personal_Loan'], axis=1)\nY = df[['Personal_Loan']]\n\n#Convert categorical variables to dummy variables\nX = pd.get_dummies(X, drop_first=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30)\n# Fit the model on train\nm = LogisticRegression(solver='newton-cg',n_jobs=-1,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:31.168351Z","iopub.execute_input":"2021-07-10T19:33:31.168594Z","iopub.status.idle":"2021-07-10T19:33:31.195354Z","shell.execute_reply.started":"2021-07-10T19:33:31.168561Z","shell.execute_reply":"2021-07-10T19:33:31.194614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will first build model with all \nsfs = SFS(m, k_features=19, forward=True, floating=False, scoring='recall', verbose=2, cv=5)\nsfs = sfs.fit(X_train, y_train)\nfig = plot_sfs(sfs.get_metric_dict(),kind='std_dev')\nplt.ylim([0, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:33:31.196259Z","iopub.execute_input":"2021-07-10T19:33:31.196546Z","iopub.status.idle":"2021-07-10T19:34:54.626817Z","shell.execute_reply.started":"2021-07-10T19:33:31.196523Z","shell.execute_reply":"2021-07-10T19:34:54.625793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Since the recall value only stopped rising after 10th feature, we will proceed only with the best 10 features","metadata":{}},{"cell_type":"code","source":"sfs1 = SFS(m,k_features=10, forward=True, floating=False, scoring='recall', verbose=2, cv=5)\n\nsfs1 = sfs1.fit(X_train, y_train)\nfig1 = plot_sfs(sfs1.get_metric_dict(),kind='std_dev')\nplt.ylim([0, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:34:54.62791Z","iopub.execute_input":"2021-07-10T19:34:54.628176Z","iopub.status.idle":"2021-07-10T19:35:44.537626Z","shell.execute_reply.started":"2021-07-10T19:34:54.628144Z","shell.execute_reply":"2021-07-10T19:35:44.536551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Which are the important features?\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:44.538896Z","iopub.execute_input":"2021-07-10T19:35:44.539202Z","iopub.status.idle":"2021-07-10T19:35:44.543801Z","shell.execute_reply.started":"2021-07-10T19:35:44.539172Z","shell.execute_reply":"2021-07-10T19:35:44.542811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at the column names\nX_train.columns[feat_cols]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:44.545025Z","iopub.execute_input":"2021-07-10T19:35:44.545363Z","iopub.status.idle":"2021-07-10T19:35:44.568825Z","shell.execute_reply.started":"2021-07-10T19:35:44.545331Z","shell.execute_reply":"2021-07-10T19:35:44.568036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating new X_train and X_test with the selected columns\nX_train_final = X_train[X_train.columns[feat_cols]]\nX_test_final = X_test[X_train_final.columns]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:44.570272Z","iopub.execute_input":"2021-07-10T19:35:44.570619Z","iopub.status.idle":"2021-07-10T19:35:44.587886Z","shell.execute_reply.started":"2021-07-10T19:35:44.570589Z","shell.execute_reply":"2021-07-10T19:35:44.586729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fitting logistic regression model\nlogreg1 = LogisticRegression(solver='saga',max_iter=1000,penalty='none',verbose=True,n_jobs=1,random_state=29)\nlogreg1.fit(X_train_final, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:44.58931Z","iopub.execute_input":"2021-07-10T19:35:44.589588Z","iopub.status.idle":"2021-07-10T19:35:45.491629Z","shell.execute_reply.started":"2021-07-10T19:35:44.589561Z","shell.execute_reply":"2021-07-10T19:35:45.490867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check the model performance\nmetrics_score(logreg1,X_train_final,X_test_final,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:45.492935Z","iopub.execute_input":"2021-07-10T19:35:45.493285Z","iopub.status.idle":"2021-07-10T19:35:45.541066Z","shell.execute_reply.started":"2021-07-10T19:35:45.493253Z","shell.execute_reply":"2021-07-10T19:35:45.540375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test2 = logreg1.predict(X_test_final)\n\nprint(\"confusion matrix = \\n\")\nmake_confusion_matrix(y_test,pred_test2)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:45.54449Z","iopub.execute_input":"2021-07-10T19:35:45.546417Z","iopub.status.idle":"2021-07-10T19:35:45.788208Z","shell.execute_reply.started":"2021-07-10T19:35:45.546366Z","shell.execute_reply":"2021-07-10T19:35:45.787343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AOC-RUC Curve\n\nSFS_roc_auc = roc_auc_score(y_test, logreg1.predict_proba(X_test_final)[:,1])\nfpr, tpr, thresholds = roc_curve(y_test, logreg1.predict_proba(X_test_final)[:,1])\nplt.figure(figsize=(13,8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % SFS_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:45.789493Z","iopub.execute_input":"2021-07-10T19:35:45.78983Z","iopub.status.idle":"2021-07-10T19:35:46.114908Z","shell.execute_reply.started":"2021-07-10T19:35:45.789794Z","shell.execute_reply":"2021-07-10T19:35:46.114421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_idx1 = np.argmax(tpr - fpr)\noptimal_threshold1 = thresholds[optimal_idx1]\nprint(optimal_threshold1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.11578Z","iopub.execute_input":"2021-07-10T19:35:46.116045Z","iopub.status.idle":"2021-07-10T19:35:46.120079Z","shell.execute_reply.started":"2021-07-10T19:35:46.116023Z","shell.execute_reply":"2021-07-10T19:35:46.119441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_trn = (logreg1.predict(X_train_final)>optimal_threshold)\ny_pred_tst = (logreg1.predict(X_test_final)>optimal_threshold)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.120915Z","iopub.execute_input":"2021-07-10T19:35:46.121236Z","iopub.status.idle":"2021-07-10T19:35:46.140508Z","shell.execute_reply.started":"2021-07-10T19:35:46.121206Z","shell.execute_reply":"2021-07-10T19:35:46.139821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us make confusion matrix after optimal threshold has been choosen\nmake_confusion_matrix(y_test,y_pred_tst)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.141563Z","iopub.execute_input":"2021-07-10T19:35:46.142027Z","iopub.status.idle":"2021-07-10T19:35:46.391297Z","shell.execute_reply.started":"2021-07-10T19:35:46.141999Z","shell.execute_reply":"2021-07-10T19:35:46.390498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy on train data:',accuracy_score(y_train, y_pred_trn) )\nprint('Accuracy on test data:',accuracy_score(y_test, y_pred_tst))\n\nprint('\\nRcall on train data:',recall_score(y_train, y_pred_trn) )\nprint('Recall on test data:',recall_score(y_test, y_pred_tst))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.392207Z","iopub.execute_input":"2021-07-10T19:35:46.392415Z","iopub.status.idle":"2021-07-10T19:35:46.404109Z","shell.execute_reply.started":"2021-07-10T19:35:46.392393Z","shell.execute_reply":"2021-07-10T19:35:46.402911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The Accuracy is at 0.94 for the test set but the Recall is only 0.513 \n* The recall value is only slightly better than the Sklearn logistic regression model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building - Decision Tree:\n### Approach\n1. Data preparation\n2. Partition the data into train and test set.\n3. Built a CART model on the train data.\n4. Tune the model and prune the tree, if required.\n5. Test the data on test set.","metadata":{}},{"cell_type":"code","source":"df1.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.405666Z","iopub.execute_input":"2021-07-10T19:35:46.405921Z","iopub.status.idle":"2021-07-10T19:35:46.433333Z","shell.execute_reply.started":"2021-07-10T19:35:46.405897Z","shell.execute_reply":"2021-07-10T19:35:46.431812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Data","metadata":{}},{"cell_type":"code","source":"X= df1.drop(['Personal_Loan'],axis=1)\ny=df1['Personal_Loan']","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.434791Z","iopub.execute_input":"2021-07-10T19:35:46.435072Z","iopub.status.idle":"2021-07-10T19:35:46.44119Z","shell.execute_reply.started":"2021-07-10T19:35:46.435049Z","shell.execute_reply":"2021-07-10T19:35:46.440019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoding the categorical variables\nX = pd.get_dummies(X, drop_first=True)\n# Splitting data into training and test set:\nX_train,X_test, y_train, y_test =train_test_split(X,y, test_size=0.3, random_state=29)\nprint(X_train.shape,X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.442376Z","iopub.execute_input":"2021-07-10T19:35:46.442633Z","iopub.status.idle":"2021-07-10T19:35:46.47933Z","shell.execute_reply.started":"2021-07-10T19:35:46.442611Z","shell.execute_reply":"2021-07-10T19:35:46.477862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\n* We will build the Decision Tree model using the default 'gini' criteria to split.\n* In our dataset, we know that there is an imbalance in the Dependent variable Personal_Loan. ie. 90.4% of frequency is for 0 and 9.6% is for 1.\n* This might cause the Decision Tree model to become biased towards the dominant class\n* Hence we will add a class_weight hyperparameter as a dictionary {0:0.15,1:0.85} to the model to specify the weight of each class and the decision tree will give more weightage to class 1","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nDt = DecisionTreeClassifier(criterion='gini',class_weight={0:0.15,1:0.85},random_state=29)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.480636Z","iopub.execute_input":"2021-07-10T19:35:46.480886Z","iopub.status.idle":"2021-07-10T19:35:46.607871Z","shell.execute_reply.started":"2021-07-10T19:35:46.480862Z","shell.execute_reply":"2021-07-10T19:35:46.606821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dt.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.608884Z","iopub.execute_input":"2021-07-10T19:35:46.609111Z","iopub.status.idle":"2021-07-10T19:35:46.626766Z","shell.execute_reply.started":"2021-07-10T19:35:46.609073Z","shell.execute_reply":"2021-07-10T19:35:46.625572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = Dt.predict(X_test)\nmake_confusion_matrix(y_test,y_predict)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.627926Z","iopub.execute_input":"2021-07-10T19:35:46.628153Z","iopub.status.idle":"2021-07-10T19:35:46.855939Z","shell.execute_reply.started":"2021-07-10T19:35:46.628132Z","shell.execute_reply":"2021-07-10T19:35:46.85523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.856972Z","iopub.execute_input":"2021-07-10T19:35:46.85721Z","iopub.status.idle":"2021-07-10T19:35:46.866137Z","shell.execute_reply.started":"2021-07-10T19:35:46.857184Z","shell.execute_reply":"2021-07-10T19:35:46.865163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* True Positive - 8.20%\n* False Positive - 0.27%\n* False Negative - 0.93%\n* True Negative - 90.6%\n\n* We also see that there are only 9.8% of the Class '1'.\n\n## Model Evaluation Criteria - Recall\n* As discussed earlier, the Bank wants to predict and identify all potential customers who will buy a Personal loan.\n* We want to maintain the False Negative ie. wrongly identifying customers as non-buyers but they actually purchase a loan as low as possible.\n\n* Hence Recall is the metric to be used\n","metadata":{}},{"cell_type":"code","source":"def scores(model):\n    \"\"\" model : classifier to predict X values \"\"\"\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    print(\"Accuracy on training set : \",metrics.accuracy_score(y_train,y_pred_train))\n    print(\"Accuracy on test set : \",metrics.accuracy_score(y_test,y_pred_test))\n\n    print(\"\\nRecall on training set : \",metrics.recall_score(y_train,y_pred_train))\n    print(\"Recall on test set : \",metrics.recall_score(y_test,y_pred_test))\n    \n    print(\"\\nPrecision on training set : \",metrics.precision_score(y_train,y_pred_train))\n    print(\"Precision on test set : \",metrics.precision_score(y_test,y_pred_test))\n    \n    print(\"\\nF1 on training set : \",metrics.f1_score(y_train,y_pred_train))\n    print(\"F1 on test set : \",metrics.f1_score(y_test,y_pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.86763Z","iopub.execute_input":"2021-07-10T19:35:46.86803Z","iopub.status.idle":"2021-07-10T19:35:46.88489Z","shell.execute_reply.started":"2021-07-10T19:35:46.86799Z","shell.execute_reply":"2021-07-10T19:35:46.883466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's calculate the Accuracy and Recall Score of the model\nscores(Dt)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.886527Z","iopub.execute_input":"2021-07-10T19:35:46.886893Z","iopub.status.idle":"2021-07-10T19:35:46.91876Z","shell.execute_reply.started":"2021-07-10T19:35:46.886867Z","shell.execute_reply":"2021-07-10T19:35:46.917237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Accuracy values for both Train and Test set are very close.\n* But there is huge difference in the Recall Scores for train and test set. \n* This suggests that the model is overfitting.\n\n## Visualizing the Decision Tree","metadata":{}},{"cell_type":"code","source":"column_names = list(X.columns)\nprint(column_names)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:46.920014Z","iopub.execute_input":"2021-07-10T19:35:46.920458Z","iopub.status.idle":"2021-07-10T19:35:46.924781Z","shell.execute_reply.started":"2021-07-10T19:35:46.92043Z","shell.execute_reply":"2021-07-10T19:35:46.923854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,30))\n\nout = tree.plot_tree(Dt,feature_names=column_names,filled=True,fontsize=8,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-10T19:35:46.926017Z","iopub.execute_input":"2021-07-10T19:35:46.926322Z","iopub.status.idle":"2021-07-10T19:35:55.078322Z","shell.execute_reply.started":"2021-07-10T19:35:46.926295Z","shell.execute_reply":"2021-07-10T19:35:55.077004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(Dt,feature_names=column_names,show_weights=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:55.079662Z","iopub.execute_input":"2021-07-10T19:35:55.079931Z","iopub.status.idle":"2021-07-10T19:35:55.091578Z","shell.execute_reply.started":"2021-07-10T19:35:55.079905Z","shell.execute_reply":"2021-07-10T19:35:55.090286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* There are close to a 100 nodes in the tree with the smallest sample size = 2 and the Gini value for the last node is 0.0\n* This is surely a overfitted Decision Tree model \n* Let's check the important features in the tree. This is also called the Gini importance","metadata":{}},{"cell_type":"code","source":"importance = Dt.feature_importances_\nindices = np.argsort(importance)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [column_names[i] for i in indices])\nplt.xlabel('Importance Value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:55.09258Z","iopub.execute_input":"2021-07-10T19:35:55.092797Z","iopub.status.idle":"2021-07-10T19:35:55.372597Z","shell.execute_reply.started":"2021-07-10T19:35:55.092774Z","shell.execute_reply":"2021-07-10T19:35:55.371307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The top five important features are:\n    - Income\n    - Education_2\n    - Family_4\n    - CCAvg\n    - Family_3\n    \n* The above tree is complex to interpret\n* Since there is suspicions of over-fitting we must prune the tree(reduce overfit) for better model performance. \n\n\n## Reduce Over-Fitting:\n\n\n### GridSearch for Hyperparameter tuning of Tree Model\n* Hyperparameters are variables that control the network structure of the Decision tree.\n* As there is no direct way to calculate the effects of value change in hyperparamter has on the model, we will use a GridSearch\n* This is a tuning technique that will compute the optimum values of specific hyperparamters of the model\n* The parameters are optimized using a cross-validated GridSearch over a parameter grid\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Choose the type of classifier. \nclassifier = DecisionTreeClassifier(random_state=29,class_weight = {0:.15,1:.85}) #adding classweights \n\n#Defining the Hyperparameters\n\nparameters = {'max_depth': np.arange(1,11), \n            'criterion': ['gini'],\n            'splitter': ['best','random'],\n            'max_features': ['log2','sqrt']}\n\n# Type of scoring used to compare parameter combinations\nrecall_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search with the above parameters\ngrid_obj = GridSearchCV(classifier, parameters, scoring=recall_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set to the best combination of parameters\nclassifier = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:55.373554Z","iopub.execute_input":"2021-07-10T19:35:55.373763Z","iopub.status.idle":"2021-07-10T19:35:57.039507Z","shell.execute_reply.started":"2021-07-10T19:35:55.373741Z","shell.execute_reply":"2021-07-10T19:35:57.038353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test2 = classifier.predict(X_test)\nmake_confusion_matrix(y_test,pred_test2)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:57.041075Z","iopub.execute_input":"2021-07-10T19:35:57.041387Z","iopub.status.idle":"2021-07-10T19:35:57.272847Z","shell.execute_reply.started":"2021-07-10T19:35:57.041363Z","shell.execute_reply":"2021-07-10T19:35:57.271953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores(classifier)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:57.274008Z","iopub.execute_input":"2021-07-10T19:35:57.274484Z","iopub.status.idle":"2021-07-10T19:35:57.298531Z","shell.execute_reply.started":"2021-07-10T19:35:57.274451Z","shell.execute_reply":"2021-07-10T19:35:57.297433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The Recall for test set has improved to 0.876 after the hyperparameter tuning.\n\n### Visualizing the Tree","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,30))\n\nout = tree.plot_tree(classifier,feature_names=column_names,filled=True,fontsize=11,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:35:57.299556Z","iopub.execute_input":"2021-07-10T19:35:57.299773Z","iopub.status.idle":"2021-07-10T19:36:00.281213Z","shell.execute_reply.started":"2021-07-10T19:35:57.29975Z","shell.execute_reply":"2021-07-10T19:36:00.280078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(classifier,feature_names=column_names,show_weights=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:00.282357Z","iopub.execute_input":"2021-07-10T19:36:00.28259Z","iopub.status.idle":"2021-07-10T19:36:00.290425Z","shell.execute_reply.started":"2021-07-10T19:36:00.282564Z","shell.execute_reply":"2021-07-10T19:36:00.289383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = classifier.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [column_names[i] for i in indices])\nplt.xlabel('Importance Value')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:00.291428Z","iopub.execute_input":"2021-07-10T19:36:00.291636Z","iopub.status.idle":"2021-07-10T19:36:00.580274Z","shell.execute_reply.started":"2021-07-10T19:36:00.291613Z","shell.execute_reply":"2021-07-10T19:36:00.579043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The order importance of features has changed.\n* The value for Income has increased and CCAvg is now the second most important feature.","metadata":{}},{"cell_type":"markdown","source":"## Cost Complexity Pruning\n* This is another method to reduce and control the size of the Tree. This method is called Post-Pruning \n* Here, we use the Cost complexity Parameter `ccp_alpha` to prune the tree\n* We will remove each possible nodes based on the alpha value. The greater the `ccp_alpha`value, higher number of nodes will be pruned and the total impurity will also increase\n\n**Finding the `ccp_alpha` values**","metadata":{}},{"cell_type":"code","source":"ccp = DecisionTreeClassifier(random_state=29,class_weight = {0:0.15,1:0.85})\nccp.fit(X_train,y_train)\npath = ccp.cost_complexity_pruning_path(X_train, y_train) #finding the alpha and impurity values\nccp_alphas, impurities = path.ccp_alphas, path.impurities","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:00.581443Z","iopub.execute_input":"2021-07-10T19:36:00.581746Z","iopub.status.idle":"2021-07-10T19:36:00.604415Z","shell.execute_reply.started":"2021-07-10T19:36:00.581713Z","shell.execute_reply":"2021-07-10T19:36:00.603576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(path)\n #display as a dataframe","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:00.605465Z","iopub.execute_input":"2021-07-10T19:36:00.605797Z","iopub.status.idle":"2021-07-10T19:36:00.618341Z","shell.execute_reply.started":"2021-07-10T19:36:00.60577Z","shell.execute_reply":"2021-07-10T19:36:00.61736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting alpha vs impurities\nfig, ax = plt.subplots(figsize=(15,5))\nax.plot(ccp_alphas, impurities, marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"Alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Alpha for training set\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:00.619579Z","iopub.execute_input":"2021-07-10T19:36:00.619854Z","iopub.status.idle":"2021-07-10T19:36:00.84315Z","shell.execute_reply.started":"2021-07-10T19:36:00.619825Z","shell.execute_reply":"2021-07-10T19:36:00.841658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The impurity values increases till ~ 0.03 of alpha value and remains constant till alpha ~ 0.22 before rising sharply","metadata":{}},{"cell_type":"code","source":"# Finding the number of nodes in the last tree and the corresponding alpha value\nclfs = [] #creating a empty list \nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=29, ccp_alpha=ccp_alpha,class_weight = {0:0.15,1:0.85})\n    clf.fit(X_train, y_train) #apply classifier model with alpha values  \n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1])) #finding the last node and its corresponding alpha","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:00.844543Z","iopub.execute_input":"2021-07-10T19:36:00.844871Z","iopub.status.idle":"2021-07-10T19:36:01.470274Z","shell.execute_reply.started":"2021-07-10T19:36:00.844831Z","shell.execute_reply":"2021-07-10T19:36:01.468526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's plot the Recall Vs Alpha values for both Train and Test set**","metadata":{}},{"cell_type":"code","source":"#Creating empty lists for train and test recall\nrecall_train=[]\nrecall_test=[]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:01.471873Z","iopub.execute_input":"2021-07-10T19:36:01.472222Z","iopub.status.idle":"2021-07-10T19:36:01.478139Z","shell.execute_reply.started":"2021-07-10T19:36:01.472192Z","shell.execute_reply":"2021-07-10T19:36:01.476562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run a loop to appead all recall scores for train and test at the alpha values\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=29, ccp_alpha=ccp_alpha,class_weight = {0:0.15,1:0.85})\n    clf.fit(X_train, y_train)\n    y_pred_train1 = clf.predict(X_train)\n    y_pred_test1 = clf.predict(X_test)\n    values_train = metrics.recall_score(y_train,y_pred_train1)\n    values_test= metrics.recall_score(y_test,y_pred_test1)\n    recall_train.append(values_train)\n    recall_test.append(values_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:01.479595Z","iopub.execute_input":"2021-07-10T19:36:01.479914Z","iopub.status.idle":"2021-07-10T19:36:02.373165Z","shell.execute_reply.started":"2021-07-10T19:36:01.479881Z","shell.execute_reply":"2021-07-10T19:36:02.371585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the recall VS alpha \nfig, ax = plt.subplots(figsize=(9,10))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker='o', label=\"train\",\n        drawstyle=\"steps-post\",)\nax.plot(ccp_alphas, recall_test, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend(loc='lower left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:02.374518Z","iopub.execute_input":"2021-07-10T19:36:02.374853Z","iopub.status.idle":"2021-07-10T19:36:02.558513Z","shell.execute_reply.started":"2021-07-10T19:36:02.374821Z","shell.execute_reply":"2021-07-10T19:36:02.557686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's find the best alpha threshold for max recall\nindex_best_alpha = np.argmax(recall_test)\nbest_model = clfs[index_best_alpha]\nprint(best_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:02.559431Z","iopub.execute_input":"2021-07-10T19:36:02.559616Z","iopub.status.idle":"2021-07-10T19:36:02.565381Z","shell.execute_reply.started":"2021-07-10T19:36:02.559595Z","shell.execute_reply":"2021-07-10T19:36:02.563734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Maximum Recall value is at alpha 0.0042. But at this alpha we will lose valuable business information and the decision tree might have very less nodes.** \n\n**Hence we will use the point where the Recall values just begins to drop first; at alpha = 0.003.\nThis will ensure we are retaining information and also get a high recall value.**","metadata":{}},{"cell_type":"code","source":"#at alpha = 0.003\nbest_model2 = DecisionTreeClassifier(ccp_alpha=0.003,\n                       class_weight={0: 0.15, 1: 0.85}, random_state=29)\nbest_model2.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:02.566489Z","iopub.execute_input":"2021-07-10T19:36:02.566781Z","iopub.status.idle":"2021-07-10T19:36:02.595865Z","shell.execute_reply.started":"2021-07-10T19:36:02.566751Z","shell.execute_reply":"2021-07-10T19:36:02.594652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test3=best_model2.predict(X_test)\nmake_confusion_matrix(y_test,pred_test3)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:02.596719Z","iopub.execute_input":"2021-07-10T19:36:02.596907Z","iopub.status.idle":"2021-07-10T19:36:02.837254Z","shell.execute_reply.started":"2021-07-10T19:36:02.596886Z","shell.execute_reply":"2021-07-10T19:36:02.835869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n* The True positive has increased to 8.53% and the False negative has decreased to 0.6%.","metadata":{}},{"cell_type":"code","source":"scores(best_model2)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:02.839817Z","iopub.execute_input":"2021-07-10T19:36:02.84008Z","iopub.status.idle":"2021-07-10T19:36:02.864017Z","shell.execute_reply.started":"2021-07-10T19:36:02.840056Z","shell.execute_reply":"2021-07-10T19:36:02.862461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n\n* The overall results for recall has increased from the initial model and its also higher than the Hypertuned model.\n* The performance for both train(0.9417 and test (0.9343) recall is close and comparable\n\n\n### Visualizing Decision Tree for best_model2","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\n\nout = tree.plot_tree(best_model2,feature_names=column_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:02.86539Z","iopub.execute_input":"2021-07-10T19:36:02.865616Z","iopub.status.idle":"2021-07-10T19:36:03.925483Z","shell.execute_reply.started":"2021-07-10T19:36:02.865593Z","shell.execute_reply":"2021-07-10T19:36:03.924336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(best_model2,feature_names=column_names,show_weights=True))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:03.926957Z","iopub.execute_input":"2021-07-10T19:36:03.927263Z","iopub.status.idle":"2021-07-10T19:36:03.933348Z","shell.execute_reply.started":"2021-07-10T19:36:03.927237Z","shell.execute_reply":"2021-07-10T19:36:03.932302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances2 = best_model2.feature_importances_\nindices = np.argsort(importances2)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances2[indices], color='green', align='center')\nplt.yticks(range(len(indices)), [column_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:03.934419Z","iopub.execute_input":"2021-07-10T19:36:03.934619Z","iopub.status.idle":"2021-07-10T19:36:04.210604Z","shell.execute_reply.started":"2021-07-10T19:36:03.934599Z","shell.execute_reply":"2021-07-10T19:36:04.209507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Income is the most important feature to predict if the customer bought a personal loan. \n* Education2, Family4 and CCAvg are the next most important predictor features.","metadata":{}},{"cell_type":"markdown","source":"## Comparison of all Models for Personal_Loan prediction","metadata":{}},{"cell_type":"code","source":"All_models = {'Model':['Logistic Regression Model-sklearn','Logistic Regression-Statsmodel-mutlicollinearity remvo','Logistic Regression-Optimal Threshold =0.2017','Logistic Regression-Optimal Threshold =0.25','Sequential Feature Selction Method','Initial Decision Tree','Decision treee- hyperparameter tuning(pre-pruning)',\n                                          'Decision tree- Cost Complexity post-pruning'],'Train_Accuracy':[0.9380,0.9640,0.9497,0.9560,0.944,1.0,0.8070,0.9749],'Test_Accuracy':[0.9387,0.9660,0.9400,0.9493,0.944,0.9880,0.80,0.972],'Train_Recall':[0.5245,0.7289,0.8484,0.8367,0.4817,1.0,0.9854,0.9417], 'Test_Recall':[0.4599,0.6788,0.8321,0.8102,0.5131,0.9051,0.9343,0.9343]}\ncomparison = pd.DataFrame(All_models)\n\ncomparison","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.211677Z","iopub.execute_input":"2021-07-10T19:36:04.211902Z","iopub.status.idle":"2021-07-10T19:36:04.225504Z","shell.execute_reply.started":"2021-07-10T19:36:04.211881Z","shell.execute_reply":"2021-07-10T19:36:04.224544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Misclassification of model:\n## Analysing predictions that were off the mark","metadata":{}},{"cell_type":"code","source":"df2=df.copy() # making a new copy from the dataset without outlier treatment\nA = df2.drop(['Personal_Loan'], axis=1) #dropping the dependent variable\nB = df2[['Personal_Loan']]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.226725Z","iopub.execute_input":"2021-07-10T19:36:04.227073Z","iopub.status.idle":"2021-07-10T19:36:04.243596Z","shell.execute_reply.started":"2021-07-10T19:36:04.227039Z","shell.execute_reply":"2021-07-10T19:36:04.242701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A = pd.get_dummies(A, drop_first=True) #creat dummy variables \n# Splitting data into training and test set:\nA_train,A_test, B_train, B_test =train_test_split(A,B, test_size=0.3,random_state=1)\n#split data\nprint(A_train.shape,A_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.244955Z","iopub.execute_input":"2021-07-10T19:36:04.245368Z","iopub.status.idle":"2021-07-10T19:36:04.283236Z","shell.execute_reply.started":"2021-07-10T19:36:04.24534Z","shell.execute_reply":"2021-07-10T19:36:04.281591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.284763Z","iopub.execute_input":"2021-07-10T19:36:04.285156Z","iopub.status.idle":"2021-07-10T19:36:04.306068Z","shell.execute_reply.started":"2021-07-10T19:36:04.285118Z","shell.execute_reply":"2021-07-10T19:36:04.305203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply the final model best_model2 to the train and test set\nfinal_pred_test = best_model2.predict(A_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.312251Z","iopub.execute_input":"2021-07-10T19:36:04.312506Z","iopub.status.idle":"2021-07-10T19:36:04.322055Z","shell.execute_reply.started":"2021-07-10T19:36:04.312483Z","shell.execute_reply":"2021-07-10T19:36:04.321291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df2.loc[A_test.index] #selecting rows with same index as test set\ndata['Predicted'] = final_pred_test\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.323147Z","iopub.execute_input":"2021-07-10T19:36:04.323352Z","iopub.status.idle":"2021-07-10T19:36:04.350722Z","shell.execute_reply.started":"2021-07-10T19:36:04.323331Z","shell.execute_reply":"2021-07-10T19:36:04.349658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_column = np.where(data[\"Predicted\"] == data[\"Personal_Loan\"], True, False) #identifying the misclassification\ndata['Misclassification'] = comparison_column\ndata['Misclassification'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.351943Z","iopub.execute_input":"2021-07-10T19:36:04.352168Z","iopub.status.idle":"2021-07-10T19:36:04.36094Z","shell.execute_reply.started":"2021-07-10T19:36:04.352148Z","shell.execute_reply":"2021-07-10T19:36:04.359939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There are 49 misclassified data in the test set","metadata":{}},{"cell_type":"code","source":"incorrect =data[data['Misclassification']== False] # Grouping only the misidentified rows \nincorrect.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.362172Z","iopub.execute_input":"2021-07-10T19:36:04.362443Z","iopub.status.idle":"2021-07-10T19:36:04.400002Z","shell.execute_reply.started":"2021-07-10T19:36:04.362416Z","shell.execute_reply":"2021-07-10T19:36:04.399036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Crearting a Pandas Profile report to identify pattern\nfrom pandas_profiling import ProfileReport\nprofile  = ProfileReport(incorrect,title = 'Misclassification Pattern Profile',minimal=True) \nprofile.to_widgets()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T19:36:04.401135Z","iopub.execute_input":"2021-07-10T19:36:04.401336Z","iopub.status.idle":"2021-07-10T19:36:20.835884Z","shell.execute_reply.started":"2021-07-10T19:36:04.401315Z","shell.execute_reply":"2021-07-10T19:36:20.834888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OBSERVATIONS**:\n* About 3% of the data from the test set has been misclassified i.e The Predicted value of the model was not the same as Personal_loan variable in the dataset.\n\n* The miscalssifcation seeems to spread across all variables. But its significant on some\n* Income and CCAvg have high misclassifications. This is understandable as the model highlighted these two features as very important. Hence the model seems to have classified customers with high income and CCavg as potential loan borrowers\n\n* Among the categorical variables; again the misclassification is high for customer with CD_account; an important feature for the model. \n* The model has targeted all its important feature combinations as potential loan borrowing customers.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}