{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook will show an example of text classification using a standard LSTM, followed by using LIME library((https://github.com/marcotcr/lime))"},{"metadata":{},"cell_type":"markdown","source":"Lime is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a probability for each class. Support for scikit-learn classifiers is built-in."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\ndf = pd.read_csv('../input/nlp-tweet-sentiment-analysis/bitcointweets.csv', header=None)\ndf = df[[1,7]]\ndf.columns = ['tweet','label']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect sentiment\nsns.countplot(df['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of tweets are neutral and positive. Looks like there are not much negative tweets on Bitcoin! No wonder the price is skyrocketing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# text length\ndf['text_length'] = df['tweet'].apply(len)\ndf[['label','text_length','tweet']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df,col='label')\ng.map(plt.hist,'text_length')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, most tweets are very short in length."},{"metadata":{},"cell_type":"markdown","source":"We are going to clean up the tweets, remove special chars, stop words, URL links, etc.."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport re\n\ndef clean_text(s):\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n    s = re.sub(r'@\\S+', '', s)\n    s = re.sub('&amp', ' ', s)\n    return s\ndf['clean_tweet'] = df['tweet'].apply(clean_text)\n\ntext = df['clean_tweet'].to_string().lower()    \nwordcloud = WordCloud(\n    collocations=False,\n    relative_scaling=0.5,\n    stopwords=set(stopwords.words('english'))).generate(text)\n\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode Categorical Variable\nX = df['clean_tweet']\n# y = pd.get_dummies(df['label']).values\nencode_cat = {\"label\":     {\"['neutral']\": 0, \"['positive']\": 1, \"['negative']\": 2},\n             }\ny_df = df.replace(encode_cat)\ny = y_df['label']\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 101 # fix random seed for reproducibility\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Train Test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 20000  # Max number of different word, i.e. model input dimension\nmaxlen = 80  # Max number of words kept at the end of each text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here, we need to create our LSTM model and use the KerasClassifier in keras.wrappers.scikit_learn."},{"metadata":{},"cell_type":"markdown","source":"Reason is that to use LIME text explainer, we need to use a sklearn pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.pipeline import TransformerMixin\nfrom sklearn.base import BaseEstimator\n\nclass TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    \"\"\" Sklearn transformer to convert texts to indices list \n    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n    def __init__(self,  **kwargs):\n        super().__init__(**kwargs)\n        \n    def fit(self, texts, y=None):\n        self.fit_on_texts(texts)\n        return self\n    \n    def transform(self, texts, y=None):\n        return np.array(self.texts_to_sequences(texts))\n        \nsequencer = TextsToSequences(num_words=vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Padder(BaseEstimator, TransformerMixin):\n    \"\"\" Pad and crop uneven lists to the same length. \n    Only the end of lists longernthan the maxlen attribute are\n    kept, and lists shorter than maxlen are left-padded with zeros\n    \n    Attributes\n    ----------\n    maxlen: int\n        sizes of sequences after padding\n    max_index: int\n        maximum index known by the Padder, if a higher index is met during \n        transform it is transformed to a 0\n    \"\"\"\n    def __init__(self, maxlen=500):\n        self.maxlen = maxlen\n        self.max_index = None\n        \n    def fit(self, X, y=None):\n        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n        return self\n    \n    def transform(self, X, y=None):\n        X = pad_sequences(X, maxlen=self.maxlen)\n        X[X > self.max_index] = 0\n        return X\n\npadder = Padder(maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.pipeline import make_pipeline\n\nbatch_size = 128\nmax_features = vocab_size + 1\n\nimport tensorflow as tf \ntf.random.set_seed(seed)\n\ndef create_model(max_features):\n    \"\"\" Model creation function: returns a compiled LSTM\"\"\"\n    model = Sequential()\n    model.add(Embedding(max_features, 128))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Keras Scikit-learn wrapper to instantiate a LSTM with all methods\n# required by Scikit-learn for the last step of a Pipeline\nsklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=batch_size, \n                               max_features=max_features, verbose=1)\n\n# Build the Scikit-learn pipeline\npipeline = make_pipeline(sequencer, padder, sklearn_lstm)\n\npipeline.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Computing predictions on test set...')\n\ny_preds = pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\ndef model_evaluate(): \n    \n    print('Test Accuracy:\\t{:0.1f}%'.format(accuracy_score(y_test,y_preds)*100))\n    \n    #classification report\n    print('\\n')\n    print(classification_report(y_test, y_preds))\n\n    #confusion matrix\n    confmat = confusion_matrix(y_test, y_preds)\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_evaluate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good, our LSTM model seems very accurate on the test set. Now for the interesting part in using LIME."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We choose a sample from test set\nidx = 15\ntest_text = np.array(X_test)\ntest_class = np.array(y_test)\ntext_sample = test_text[idx]\nclass_names = ['neutral', 'positive', 'negative']\nprint(text_sample)\nprint('Probability =', pipeline.predict_proba([text_sample]).round(3))\nprint('True class: %s' % class_names[test_class[idx]])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, so good. Our LSTM model predicts the correct class for this sample text."},{"metadata":{},"cell_type":"markdown","source":"Now let's run LIME!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=class_names)\nexp = explainer.explain_instance(text_sample, pipeline.predict_proba, num_features=6, top_labels=2)\nexp.show_in_notebook(text=text_sample)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok...now to interpret! You can see all the words highlighted in orange which contributes to the positive classification. The decimal values show the proportion of how much that word contributes to the class label."},{"metadata":{},"cell_type":"markdown","source":"The word 'successful' is a major reason for the positive classification, let's see what happens when we remove it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_sample2 = re.sub('successful', ' ', text_sample)\nprint(text_sample2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's rerun the prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Probability =', pipeline.predict_proba([text_sample2]).round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, as you can see, when you remove 'successful', the LSTM model class probabilities have indeed changed!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}