{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T11:32:12.38315Z","iopub.execute_input":"2021-08-16T11:32:12.383526Z","iopub.status.idle":"2021-08-16T11:32:12.409419Z","shell.execute_reply.started":"2021-08-16T11:32:12.383445Z","shell.execute_reply":"2021-08-16T11:32:12.408374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:12.410799Z","iopub.execute_input":"2021-08-16T11:32:12.41109Z","iopub.status.idle":"2021-08-16T11:32:13.387503Z","shell.execute_reply.started":"2021-08-16T11:32:12.411059Z","shell.execute_reply":"2021-08-16T11:32:13.386599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/shark-attacks/attacks.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.389057Z","iopub.execute_input":"2021-08-16T11:32:13.389314Z","iopub.status.idle":"2021-08-16T11:32:13.549958Z","shell.execute_reply.started":"2021-08-16T11:32:13.389289Z","shell.execute_reply":"2021-08-16T11:32:13.548725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()\n# right off, it seems the Case Number and Year columns are redundant... we'll drop them","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.551447Z","iopub.execute_input":"2021-08-16T11:32:13.551801Z","iopub.status.idle":"2021-08-16T11:32:13.599602Z","shell.execute_reply.started":"2021-08-16T11:32:13.551771Z","shell.execute_reply":"2021-08-16T11:32:13.59861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Case Number', 'Year'], axis =1, inplace= True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.601165Z","iopub.execute_input":"2021-08-16T11:32:13.601552Z","iopub.status.idle":"2021-08-16T11:32:13.614088Z","shell.execute_reply.started":"2021-08-16T11:32:13.601511Z","shell.execute_reply":"2021-08-16T11:32:13.61315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.615406Z","iopub.execute_input":"2021-08-16T11:32:13.615975Z","iopub.status.idle":"2021-08-16T11:32:13.669521Z","shell.execute_reply.started":"2021-08-16T11:32:13.615944Z","shell.execute_reply":"2021-08-16T11:32:13.668605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similarly, the columns : 'Investigator' 'pdf', 'href formula', 'href', 'Case Number.1', 'Case Number.2'\n# and 'original order' contain either redundant or irrelevant information for prediction \n# purposes. The columns: 'Investigator' 'pdf', 'href formula', 'href',\n# would be helpful to dig into the dataset more\n# For exploratory purposes, we drop these columns.\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.672354Z","iopub.execute_input":"2021-08-16T11:32:13.672786Z","iopub.status.idle":"2021-08-16T11:32:13.678602Z","shell.execute_reply.started":"2021-08-16T11:32:13.672742Z","shell.execute_reply":"2021-08-16T11:32:13.677804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Name','Investigator or Source','pdf', 'href formula', 'href',\n       'Case Number.1', 'Case Number.2', 'original order'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.680577Z","iopub.execute_input":"2021-08-16T11:32:13.681147Z","iopub.status.idle":"2021-08-16T11:32:13.695197Z","shell.execute_reply.started":"2021-08-16T11:32:13.681117Z","shell.execute_reply":"2021-08-16T11:32:13.693888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this dataset is full of null values. We cannot impute as the data is text based. \ndf.isnull().sum()/len(df)\n#Let's see where the null values lie in the data","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.696391Z","iopub.execute_input":"2021-08-16T11:32:13.696688Z","iopub.status.idle":"2021-08-16T11:32:13.747068Z","shell.execute_reply.started":"2021-08-16T11:32:13.696653Z","shell.execute_reply":"2021-08-16T11:32:13.745915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df.isnull(), yticklabels = False,cbar = False, cmap = 'viridis')\n# Yellow is NaN here. A huge swath of this data is missing. Let's first just remove all rows\n # that only contain the enrty Nan","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:13.748729Z","iopub.execute_input":"2021-08-16T11:32:13.749118Z","iopub.status.idle":"2021-08-16T11:32:14.399801Z","shell.execute_reply.started":"2021-08-16T11:32:13.749077Z","shell.execute_reply":"2021-08-16T11:32:14.398865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(axis = 0, how = 'all', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:14.401024Z","iopub.execute_input":"2021-08-16T11:32:14.401266Z","iopub.status.idle":"2021-08-16T11:32:14.42744Z","shell.execute_reply.started":"2021-08-16T11:32:14.401242Z","shell.execute_reply":"2021-08-16T11:32:14.426468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df.isnull(), yticklabels = False,cbar = False, cmap = 'viridis')\n# Now it is clear that the 'Age', 'Time' and 'Species' columns are missing a lot information","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:14.42901Z","iopub.execute_input":"2021-08-16T11:32:14.429327Z","iopub.status.idle":"2021-08-16T11:32:14.742604Z","shell.execute_reply.started":"2021-08-16T11:32:14.4293Z","shell.execute_reply":"2021-08-16T11:32:14.741651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are now at least two approaches. The first approach is to delete the 'Age','Time' and\n#'Species' column, then delete remaining rows with NaN and proceed. \n\n# The species and time columns are interesting features...is there a shark that attacks \n#humans more frequently? What time do these attacks occur?\n\n# For now, we'll take the first approach for sanity sake!","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:14.743891Z","iopub.execute_input":"2021-08-16T11:32:14.74422Z","iopub.status.idle":"2021-08-16T11:32:14.748382Z","shell.execute_reply.started":"2021-08-16T11:32:14.744176Z","shell.execute_reply":"2021-08-16T11:32:14.747362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copies of dataframe up until now\n# data is dropped according to discussion above\ndf1 = df.copy(deep = True)\ndf1.drop(['Age', 'Time', 'Species '], axis = 1, inplace = True)\ndf1.dropna(axis = 0, how = 'any', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:14.749577Z","iopub.execute_input":"2021-08-16T11:32:14.749908Z","iopub.status.idle":"2021-08-16T11:32:14.771142Z","shell.execute_reply.started":"2021-08-16T11:32:14.749878Z","shell.execute_reply":"2021-08-16T11:32:14.7696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1 now has no missing data (it is not clean though!)\nsns.heatmap(df1.isnull(), yticklabels= False, cbar = False, cmap = 'viridis')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:14.772416Z","iopub.execute_input":"2021-08-16T11:32:14.772754Z","iopub.status.idle":"2021-08-16T11:32:15.024423Z","shell.execute_reply.started":"2021-08-16T11:32:14.77272Z","shell.execute_reply":"2021-08-16T11:32:15.022842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.info()\n# 4601 entries by first removing 'Age', 'Time' and 'Species' columns","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.025589Z","iopub.execute_input":"2021-08-16T11:32:15.025945Z","iopub.status.idle":"2021-08-16T11:32:15.044787Z","shell.execute_reply.started":"2021-08-16T11:32:15.025916Z","shell.execute_reply":"2021-08-16T11:32:15.044008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning up the data features columns...","metadata":{}},{"cell_type":"code","source":"# fatal data column is not neatly classified\nsns.countplot(x = df1['Fatal (Y/N)'], data = df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.045838Z","iopub.execute_input":"2021-08-16T11:32:15.046242Z","iopub.status.idle":"2021-08-16T11:32:15.220375Z","shell.execute_reply.started":"2021-08-16T11:32:15.046208Z","shell.execute_reply":"2021-08-16T11:32:15.21968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# appears that six non-fatal encounters are classified as ' N'. we need to group \n# these with the 'N' category and get rid of the 'UNKNOWN' and '2017' entrys.\ndf1['Fatal (Y/N)'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.22142Z","iopub.execute_input":"2021-08-16T11:32:15.221918Z","iopub.status.idle":"2021-08-16T11:32:15.230664Z","shell.execute_reply.started":"2021-08-16T11:32:15.221879Z","shell.execute_reply":"2021-08-16T11:32:15.229506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1[df1['Fatal (Y/N)'] == ' N']['Fatal (Y/N)'].iloc[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.235291Z","iopub.execute_input":"2021-08-16T11:32:15.235739Z","iopub.status.idle":"2021-08-16T11:32:15.248642Z","shell.execute_reply.started":"2021-08-16T11:32:15.235694Z","shell.execute_reply":"2021-08-16T11:32:15.247584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replacing ' N' values with 'N' in fatal column\nif df1[df1['Fatal (Y/N)'] == ' N']['Fatal (Y/N)'].iloc[0]:\n    df1.replace(to_replace = df1[df1['Fatal (Y/N)'] == ' N']['Fatal (Y/N)'].iloc[0],\\\n    value = 'N',inplace = True)\n    \ndf1['Fatal (Y/N)'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.252226Z","iopub.execute_input":"2021-08-16T11:32:15.25252Z","iopub.status.idle":"2021-08-16T11:32:15.270937Z","shell.execute_reply.started":"2021-08-16T11:32:15.252493Z","shell.execute_reply":"2021-08-16T11:32:15.269789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1[df1['Fatal (Y/N)'] == 'UNKNOWN']['Fatal (Y/N)'].index.values","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.272195Z","iopub.execute_input":"2021-08-16T11:32:15.27248Z","iopub.status.idle":"2021-08-16T11:32:15.284426Z","shell.execute_reply.started":"2021-08-16T11:32:15.272453Z","shell.execute_reply":"2021-08-16T11:32:15.283555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we do not know what the 'UNKNOWN' and '2017' labels correspond to, we'll have to\n# drop these rows\ndf1.drop(index = df1[df1['Fatal (Y/N)'] == 'UNKNOWN']['Fatal (Y/N)'].index.values,\\\n        inplace = True)\n\ndf1.drop(index = df1[df1['Fatal (Y/N)'] == '2017']['Fatal (Y/N)'].index.values,\\\n        inplace = True)\n\ndf1['Fatal (Y/N)'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.285783Z","iopub.execute_input":"2021-08-16T11:32:15.286118Z","iopub.status.idle":"2021-08-16T11:32:15.307039Z","shell.execute_reply.started":"2021-08-16T11:32:15.286088Z","shell.execute_reply":"2021-08-16T11:32:15.306098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'Fatal (Y/N)', data = df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.308493Z","iopub.execute_input":"2021-08-16T11:32:15.308862Z","iopub.status.idle":"2021-08-16T11:32:15.45034Z","shell.execute_reply.started":"2021-08-16T11:32:15.308832Z","shell.execute_reply":"2021-08-16T11:32:15.44942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dummy variables for Fatal (Y/N) column\nfatal = pd.get_dummies(df1['Fatal (Y/N)'], drop_first = True)\ndf1.drop('Fatal (Y/N)', axis = 1, inplace = True)\ndf1 = pd.concat([df1, fatal], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.451642Z","iopub.execute_input":"2021-08-16T11:32:15.451999Z","iopub.status.idle":"2021-08-16T11:32:15.462027Z","shell.execute_reply.started":"2021-08-16T11:32:15.451957Z","shell.execute_reply":"2021-08-16T11:32:15.461124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can use the same approach for the 'Type', 'Activity', 'Sex' and 'Area' columns:\n\n#we'll work on the 'Sex column first'\nsns.countplot(x = 'Sex ', data = df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.463365Z","iopub.execute_input":"2021-08-16T11:32:15.463747Z","iopub.status.idle":"2021-08-16T11:32:15.649427Z","shell.execute_reply.started":"2021-08-16T11:32:15.46366Z","shell.execute_reply":"2021-08-16T11:32:15.648443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['Sex '].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.650875Z","iopub.execute_input":"2021-08-16T11:32:15.651143Z","iopub.status.idle":"2021-08-16T11:32:15.660142Z","shell.execute_reply.started":"2021-08-16T11:32:15.651116Z","shell.execute_reply":"2021-08-16T11:32:15.659161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if df1[df1['Sex '] == 'M ']['Sex '].iloc[0]:\n    df1.replace(to_replace = df1[df1['Sex '] == 'M ']['Sex '].iloc[0], value = 'M',\\\n               inplace = True)\n    \ndf1['Sex '].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.661546Z","iopub.execute_input":"2021-08-16T11:32:15.661988Z","iopub.status.idle":"2021-08-16T11:32:15.681803Z","shell.execute_reply.started":"2021-08-16T11:32:15.661959Z","shell.execute_reply":"2021-08-16T11:32:15.68102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping remaing ambiguous 'Sex' labels\ndf1.drop(df1[df1['Sex '] == 'lli']['Sex '].index.values, inplace = True)\ndf1.drop(df1[df1['Sex '] == '.']['Sex '].index.values, inplace = True)\ndf1.drop(df1[df1['Sex '] == 'N']['Sex '].index.values, inplace = True)\ndf1['Sex '].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.682946Z","iopub.execute_input":"2021-08-16T11:32:15.683229Z","iopub.status.idle":"2021-08-16T11:32:15.705874Z","shell.execute_reply.started":"2021-08-16T11:32:15.683202Z","shell.execute_reply":"2021-08-16T11:32:15.704871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'Sex ', data = df1, hue = 'Y')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.707006Z","iopub.execute_input":"2021-08-16T11:32:15.70728Z","iopub.status.idle":"2021-08-16T11:32:15.880557Z","shell.execute_reply.started":"2021-08-16T11:32:15.707254Z","shell.execute_reply":"2021-08-16T11:32:15.879875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sex = pd.get_dummies(df1['Sex '], drop_first = True)\ndf1.drop('Sex ', axis = 1, inplace = True)\ndf1 = pd.concat([df1, sex], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.881995Z","iopub.execute_input":"2021-08-16T11:32:15.882461Z","iopub.status.idle":"2021-08-16T11:32:15.891216Z","shell.execute_reply.started":"2021-08-16T11:32:15.882428Z","shell.execute_reply":"2021-08-16T11:32:15.890127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# moving on to the 'Type' column\ndf1['Type'].value_counts()\n# 'Boat' and 'Boating' columns can be merged","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.892542Z","iopub.execute_input":"2021-08-16T11:32:15.89283Z","iopub.status.idle":"2021-08-16T11:32:15.907286Z","shell.execute_reply.started":"2021-08-16T11:32:15.892804Z","shell.execute_reply":"2021-08-16T11:32:15.906274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging Boat and Boating columns\nif df1[df1['Type'] == 'Boat']['Type'].iloc[0]:\n    df1.replace(to_replace = df1[df1['Type'] == 'Boat']['Type'].iloc[0], value = 'Boating',\\\n               inplace = True)\ndf1['Type'].value_counts()\n#we'll keep the invalid column as we lack additional information to change it","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.908518Z","iopub.execute_input":"2021-08-16T11:32:15.908834Z","iopub.status.idle":"2021-08-16T11:32:15.930283Z","shell.execute_reply.started":"2021-08-16T11:32:15.908805Z","shell.execute_reply":"2021-08-16T11:32:15.929316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'Type', data = df1, hue = 'Y')\n# sea diaster entries are most fatal. Possibly due to time in the water/presence of blood\n# number of people...","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:15.931512Z","iopub.execute_input":"2021-08-16T11:32:15.931865Z","iopub.status.idle":"2021-08-16T11:32:16.257583Z","shell.execute_reply.started":"2021-08-16T11:32:15.931836Z","shell.execute_reply":"2021-08-16T11:32:16.256713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type = pd.get_dummies(df1['Type'], drop_first= True)\ndf1.drop('Type', axis = 1, inplace = True)\ndf1 = pd.concat([df1, type], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:16.258837Z","iopub.execute_input":"2021-08-16T11:32:16.259152Z","iopub.status.idle":"2021-08-16T11:32:16.269913Z","shell.execute_reply.started":"2021-08-16T11:32:16.259125Z","shell.execute_reply":"2021-08-16T11:32:16.268914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now the 'Activity column'\n# some simple typos to correct and then some larger explainations...\n\n# first we make everything lowercase\nfor i in range(len(df1['Activity'])):\n    df1['Activity'].iloc[i] = df1['Activity'].iloc[i].lower()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:16.271375Z","iopub.execute_input":"2021-08-16T11:32:16.271874Z","iopub.status.idle":"2021-08-16T11:32:18.854946Z","shell.execute_reply.started":"2021-08-16T11:32:16.271833Z","shell.execute_reply":"2021-08-16T11:32:18.854001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# capturing and classifiying as many 'activities' as possible\n\n# some activities are lumped together for processing \nfor i in range(len(df1['Activity'])):\n    \n    if 'surfing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surfing', inplace = True)\n    elif 'surfing,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surfing', inplace = True)\n    elif 'surfboard' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surfing', inplace = True)\n    elif 'swimming' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'swimming,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'bathing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'floating' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'water' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'swimming', inplace = True)\n    elif 'fishing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'fishing', inplace = True)\n    elif 'fishing,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'fishing', inplace = True)\n    elif 'wading' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'wading', inplace = True)\n    elif 'standing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'wading', inplace = True)\n    elif 'boogie' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'body boarding', inplace = True)\n    elif 'body-boarding' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'body boarding', inplace = True)\n    elif 'bodyboarding' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'body boarding', inplace = True)\n    elif 'spearfishing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'spearfishing', inplace = True)\n    elif 'spearfishing,' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'spearfishing', inplace = True)\n    elif 'diving' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'diving', inplace = True)\n    elif 'freediving' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'diving', inplace = True)\n    elif 'skindiving' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'diving', inplace = True)\n    elif 'snorkeling' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'snorkeling', inplace = True)\n    elif 'surf-skiing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surf skiing', inplace = True)\n    elif 'skiing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'surf skiing', inplace = True)\n    elif 'canoeing' in df1['Activity'].iloc[i].split():\n        df1.replace(to_replace = df1['Activity'].iloc[i],value = 'kayaking', inplace = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:18.856028Z","iopub.execute_input":"2021-08-16T11:32:18.856289Z","iopub.status.idle":"2021-08-16T11:32:30.551186Z","shell.execute_reply.started":"2021-08-16T11:32:18.856262Z","shell.execute_reply":"2021-08-16T11:32:30.550141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('max_rows', None)\nprint(df1['Activity'].value_counts()[:13].sum())\nprint('\\n')\ndf1['Activity'].value_counts()[:13].sum()/df1['Activity'].value_counts().sum()\n#we'll drop all activities that have less than ten entrys. \n#We're keeping 88% with this cut off","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:30.552686Z","iopub.execute_input":"2021-08-16T11:32:30.553295Z","iopub.status.idle":"2021-08-16T11:32:30.570836Z","shell.execute_reply.started":"2021-08-16T11:32:30.553247Z","shell.execute_reply":"2021-08-16T11:32:30.569854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_list = df1['Activity'].value_counts()[:13].index.values","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:30.57236Z","iopub.execute_input":"2021-08-16T11:32:30.572767Z","iopub.status.idle":"2021-08-16T11:32:30.58204Z","shell.execute_reply.started":"2021-08-16T11:32:30.57273Z","shell.execute_reply":"2021-08-16T11:32:30.58107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_act = []\nfor i in range(len(df1)):\n    if df1['Activity'].iloc[i] not in act_list: \n        drop_act = np.append(drop_act,  int(df1['Activity'].index[i]))\n\ndrop_act = list(map(int, drop_act))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:30.583454Z","iopub.execute_input":"2021-08-16T11:32:30.584196Z","iopub.status.idle":"2021-08-16T11:32:30.729921Z","shell.execute_reply.started":"2021-08-16T11:32:30.584161Z","shell.execute_reply":"2021-08-16T11:32:30.728475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.drop(index = drop_act, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:30.731539Z","iopub.execute_input":"2021-08-16T11:32:30.731887Z","iopub.status.idle":"2021-08-16T11:32:30.750913Z","shell.execute_reply.started":"2021-08-16T11:32:30.731856Z","shell.execute_reply":"2021-08-16T11:32:30.749691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.countplot(x = 'Activity', data = df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:30.752455Z","iopub.execute_input":"2021-08-16T11:32:30.752836Z","iopub.status.idle":"2021-08-16T11:32:31.068685Z","shell.execute_reply.started":"2021-08-16T11:32:30.752802Z","shell.execute_reply":"2021-08-16T11:32:31.067634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.countplot(x = 'Activity', data = df1, hue = 'Y')\n# swimming (and the activities that are lumped there) have the greatest fatality rate","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.069664Z","iopub.execute_input":"2021-08-16T11:32:31.069939Z","iopub.status.idle":"2021-08-16T11:32:31.516038Z","shell.execute_reply.started":"2021-08-16T11:32:31.069911Z","shell.execute_reply":"2021-08-16T11:32:31.514836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_dum = pd.get_dummies(df1['Activity'], drop_first=True)\ndf1.drop('Activity', axis =1, inplace = True)\ndf1 = pd.concat([df1, act_dum], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.517292Z","iopub.execute_input":"2021-08-16T11:32:31.517591Z","iopub.status.idle":"2021-08-16T11:32:31.527037Z","shell.execute_reply.started":"2021-08-16T11:32:31.517546Z","shell.execute_reply":"2021-08-16T11:32:31.525934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's remove the injury as it does not help with predictive properties. Also, we'll drop \n# 'Country' and 'Location' Columns and focus on the 'Area'\ndf1.drop(['Injury', 'Country', 'Location'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.528376Z","iopub.execute_input":"2021-08-16T11:32:31.528776Z","iopub.status.idle":"2021-08-16T11:32:31.539055Z","shell.execute_reply.started":"2021-08-16T11:32:31.52874Z","shell.execute_reply":"2021-08-16T11:32:31.537983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again, collecting 'Area' entrys with value counts greater than 9\narea_list = df1['Area'].value_counts()[:32].index.values","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.540584Z","iopub.execute_input":"2021-08-16T11:32:31.540966Z","iopub.status.idle":"2021-08-16T11:32:31.555071Z","shell.execute_reply.started":"2021-08-16T11:32:31.540929Z","shell.execute_reply":"2021-08-16T11:32:31.55375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_area = []\nfor i in range(len(df1)):\n    if df1['Area'].iloc[i] not in area_list: \n        drop_area = np.append(drop_area,  int(df1['Area'].index[i]))\n\ndrop_area = list(map(int, drop_area))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.556406Z","iopub.execute_input":"2021-08-16T11:32:31.55674Z","iopub.status.idle":"2021-08-16T11:32:31.693805Z","shell.execute_reply.started":"2021-08-16T11:32:31.556709Z","shell.execute_reply":"2021-08-16T11:32:31.692763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.drop(index = drop_area, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.695063Z","iopub.execute_input":"2021-08-16T11:32:31.695359Z","iopub.status.idle":"2021-08-16T11:32:31.700567Z","shell.execute_reply.started":"2021-08-16T11:32:31.695331Z","shell.execute_reply":"2021-08-16T11:32:31.699823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (50,5))\nsns.countplot(x = 'Area', data = df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:31.701772Z","iopub.execute_input":"2021-08-16T11:32:31.702223Z","iopub.status.idle":"2021-08-16T11:32:32.230186Z","shell.execute_reply.started":"2021-08-16T11:32:31.702192Z","shell.execute_reply":"2021-08-16T11:32:32.229137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (50,5))\nsns.countplot(x = 'Area', data = df1, hue = 'Y')\n# New South Wales has the greatest fatality rate per shark attack.","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:32.23141Z","iopub.execute_input":"2021-08-16T11:32:32.231718Z","iopub.status.idle":"2021-08-16T11:32:32.966893Z","shell.execute_reply.started":"2021-08-16T11:32:32.231687Z","shell.execute_reply":"2021-08-16T11:32:32.965747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"area_dum = pd.get_dummies(df1['Area'], drop_first=True)\ndf1.drop('Area', axis = 1, inplace = True)\ndf1 = pd.concat([df1, area_dum], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:32.968263Z","iopub.execute_input":"2021-08-16T11:32:32.968572Z","iopub.status.idle":"2021-08-16T11:32:32.977042Z","shell.execute_reply.started":"2021-08-16T11:32:32.968535Z","shell.execute_reply":"2021-08-16T11:32:32.976026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now for the Date Column...","metadata":{}},{"cell_type":"code","source":"# We'll keep only the year\n\n# year column\ndf1['Year'] = df1['Date'].apply(lambda x:x.split('-')[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:32.978772Z","iopub.execute_input":"2021-08-16T11:32:32.97927Z","iopub.status.idle":"2021-08-16T11:32:32.990885Z","shell.execute_reply.started":"2021-08-16T11:32:32.979215Z","shell.execute_reply":"2021-08-16T11:32:32.989713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correcting the nonyear values\n\nwrong_year_val = []\nwrong_year_ind = []\nfor i in range(len(df1['Year'])):\n    if len(df1['Year'].iloc[i]) > 4:\n        wrong_year_val = np.append(wrong_year_val, df1['Date'].iloc[i])\n        wrong_year_ind = np.append(wrong_year_ind, df1['Date'].index[i])\n    elif len(df1['Year'].iloc[i]) < 4:\n        wrong_year_val = np.append(wrong_year_val, df1['Date'].iloc[i])\n        wrong_year_ind = np.append(wrong_year_ind, df1['Date'].index[i])\n\nwrong_year_ind = list(map(int, wrong_year_ind))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:32.992489Z","iopub.execute_input":"2021-08-16T11:32:32.992905Z","iopub.status.idle":"2021-08-16T11:32:33.099649Z","shell.execute_reply.started":"2021-08-16T11:32:32.992862Z","shell.execute_reply":"2021-08-16T11:32:33.098769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replacing nonyear values\n\ndrop_wrongdf = pd.DataFrame(wrong_year_val)\nsplit_year = drop_wrongdf[0].apply(lambda x:x.split('-'))\n\nfor i in range(len(split_year)):\n    df1['Year'].at[wrong_year_ind[i]] = split_year[i][-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:33.101161Z","iopub.execute_input":"2021-08-16T11:32:33.101598Z","iopub.status.idle":"2021-08-16T11:32:33.116211Z","shell.execute_reply.started":"2021-08-16T11:32:33.101556Z","shell.execute_reply":"2021-08-16T11:32:33.115161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final cleaning and dropping remaining erroneous values\ndf1['Year'] = df1['Year'].apply(lambda x:x.split()[-1])\ndf1.drop(df1[df1['Year'].map(len)!=4].index, inplace = True)\ndf1.drop(df1[df1['Year']> '2021'].index, inplace = True)\ndf1['Year'] = list(map(int, df1['Year']))\nlen(df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:32:33.117838Z","iopub.execute_input":"2021-08-16T11:32:33.118444Z","iopub.status.idle":"2021-08-16T11:32:33.140132Z","shell.execute_reply.started":"2021-08-16T11:32:33.1184Z","shell.execute_reply":"2021-08-16T11:32:33.138938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reported shark attacks have certainly increased...what happened in 1905?\nplt.figure(figsize=(100,5))\nyear_order = sorted(df1['Year'].unique(), reverse = True)\nsns.countplot(x = 'Year',order = year_order, data = df1)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T11:55:23.940531Z","iopub.execute_input":"2021-08-16T11:55:23.940923Z","iopub.status.idle":"2021-08-16T11:55:26.660961Z","shell.execute_reply.started":"2021-08-16T11:55:23.940889Z","shell.execute_reply":"2021-08-16T11:55:26.659806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing the 'Date' column\ndf1.drop('Date', axis =1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:03:42.037935Z","iopub.execute_input":"2021-08-16T12:03:42.038334Z","iopub.status.idle":"2021-08-16T12:03:42.046791Z","shell.execute_reply.started":"2021-08-16T12:03:42.038297Z","shell.execute_reply":"2021-08-16T12:03:42.045793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:03:57.437863Z","iopub.execute_input":"2021-08-16T12:03:57.438247Z","iopub.status.idle":"2021-08-16T12:03:57.445272Z","shell.execute_reply.started":"2021-08-16T12:03:57.438212Z","shell.execute_reply":"2021-08-16T12:03:57.444239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we are now ready to train a classification model! \ndf1.select_dtypes(['object']).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:05:36.709429Z","iopub.execute_input":"2021-08-16T12:05:36.709963Z","iopub.status.idle":"2021-08-16T12:05:36.717701Z","shell.execute_reply.started":"2021-08-16T12:05:36.709928Z","shell.execute_reply":"2021-08-16T12:05:36.716753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll attempt to predict whether or not an attack will be fatal\nfrom sklearn.model_selection import train_test_split\nX = df1.drop('Y', axis = 1).values\ny = df1['Y'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:28:21.118375Z","iopub.execute_input":"2021-08-16T12:28:21.118754Z","iopub.status.idle":"2021-08-16T12:28:21.125613Z","shell.execute_reply.started":"2021-08-16T12:28:21.118722Z","shell.execute_reply":"2021-08-16T12:28:21.124689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:28:24.341068Z","iopub.execute_input":"2021-08-16T12:28:24.341559Z","iopub.status.idle":"2021-08-16T12:28:24.347489Z","shell.execute_reply.started":"2021-08-16T12:28:24.341527Z","shell.execute_reply":"2021-08-16T12:28:24.346754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Models","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"# We'll train three classification models: Logistic Regression, Decision tree classifier \n# and random forest classifer.\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver ='liblinear')\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:33:43.668615Z","iopub.execute_input":"2021-08-16T12:33:43.669004Z","iopub.status.idle":"2021-08-16T12:33:43.687534Z","shell.execute_reply.started":"2021-08-16T12:33:43.66897Z","shell.execute_reply":"2021-08-16T12:33:43.686061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(classification_report(y_test, lr_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, lr_pred))\nprint('\\n')\nprint(accuracy_score(y_test, lr_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:42:15.565295Z","iopub.execute_input":"2021-08-16T12:42:15.565878Z","iopub.status.idle":"2021-08-16T12:42:15.582393Z","shell.execute_reply.started":"2021-08-16T12:42:15.565828Z","shell.execute_reply":"2021-08-16T12:42:15.581432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ndtc_pred = dtc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:37:32.475164Z","iopub.execute_input":"2021-08-16T12:37:32.475759Z","iopub.status.idle":"2021-08-16T12:37:32.493283Z","shell.execute_reply.started":"2021-08-16T12:37:32.475706Z","shell.execute_reply":"2021-08-16T12:37:32.492266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, dtc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, dtc_pred))\nprint('\\n')\nprint(accuracy_score(y_test, dtc_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:42:28.099975Z","iopub.execute_input":"2021-08-16T12:42:28.100336Z","iopub.status.idle":"2021-08-16T12:42:28.114838Z","shell.execute_reply.started":"2021-08-16T12:42:28.100306Z","shell.execute_reply":"2021-08-16T12:42:28.113717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifer","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:39:42.383793Z","iopub.execute_input":"2021-08-16T12:39:42.38418Z","iopub.status.idle":"2021-08-16T12:39:42.792615Z","shell.execute_reply.started":"2021-08-16T12:39:42.384146Z","shell.execute_reply":"2021-08-16T12:39:42.791691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, rfc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, rfc_pred))\nprint('\\n')\nprint(accuracy_score(y_test, rfc_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T12:43:10.913035Z","iopub.execute_input":"2021-08-16T12:43:10.913605Z","iopub.status.idle":"2021-08-16T12:43:10.930666Z","shell.execute_reply.started":"2021-08-16T12:43:10.913557Z","shell.execute_reply":"2021-08-16T12:43:10.929773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}