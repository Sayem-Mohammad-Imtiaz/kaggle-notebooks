{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This code has a custom callback that is a combination of early stopping, reduce learning rate on plateau \n### and model checkpoint. It has the following input parameters\ncallbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n            factor=factor,dwell=dwell, model_name=model_name, freeze=freeze, batches=batches,initial_epoch=0 )] \nThe callback initially monitor the training accuracy. If the training accuracy fails to improve for 'patience'\nnumber of epochs the learning rate is reduced by to the value new lr= current lr * factor' Once the training\naccuracy exceeds the level set by the parameter 'threshold' the callback switches to monitor validation loss.\nAgain if validation loss fails to decrease for 'patience' number of consecutive epochs the learning rate is\nreduced as discribed above. Parameter 'stop_patience' is used to set how many consecutive adjustments of the\nlearning rate can occur without metric improvement before training is terminated.Parameter model is the model\nyou created with model.compile in your code. Parameter model_name is a string for the name of your model for\nexample 'Mobilenet'. Parameter 'dwell' is a boolean. If set to True then if the current epoch resulted in no\nimprovement of the metric, the models weights are set back to the weights of the model in the previous epoch \nand the learning rate is reduced. Logic here is if the metric did not improve you moved to a location in N space\n(N being the number of trainable parameters in the model) that is not as favorable as the position you were in\nfor the previous epoch. So why stay in this less favorable location, rather go back to the previous location by setting the model weights back to those of the previous epoch. Parameter 'Freeze' is just for display purpose. See the\nimage below. It simply tells the callback to print 'training all layers' Batches is an interger that is the\nvalue of the number of training steps. For example if your traning set has 10000 samples and you set the batch\nsize to 100 than set batch= samples/batch_size. In the code below jog down to where the callback prints out\nthe training data to see how the information is displayed. Not batch count data disappears when training is completed.\nyou see the running batch count and total number of batches to be processed per epoch. This is a substitute for\nthe model.fit progress bar. In model.fit I set verbose=0 to stop the model,fit printout and let the callback\nprint the result for each epoch as shown in the image below.","metadata":{"papermill":{"duration":0.022332,"end_time":"2021-05-27T22:59:56.352141","exception":false,"start_time":"2021-05-27T22:59:56.329809","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport time\nimport cv2 as cv2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport os\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom IPython.core.display import display, HTML","metadata":{"execution":{"iopub.status.busy":"2021-05-29T01:33:04.34917Z","iopub.execute_input":"2021-05-29T01:33:04.349615Z","iopub.status.idle":"2021-05-29T01:33:09.977581Z","shell.execute_reply.started":"2021-05-29T01:33:04.34951Z","shell.execute_reply":"2021-05-29T01:33:09.976281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unused code experiment with masking\nimgpath=r'../input/ham1000-segmentation-and-classification/images/ISIC_0024306.jpg'\nmaskpath=r'../input/ham1000-segmentation-and-classification/masks/ISIC_0024306_segmentation.png'\n\nplt.figure(figsize=(10, 20))\nplt.subplot(5, 2,  1)\nimg=plt.imread(imgpath)\nplt.title('input image')\nplt.imshow(img/255)\nmask=plt.imread(maskpath)\nmaxpix=np.max(mask)\nprint ('mask maximum pixel value= ',maxpix)\nplt.subplot(5, 2,  2)\nplt.title('mask')\nplt.imshow(mask)\nb, g, r = cv2.split(img)\nprint ('blue channel max pixel value= ', np.max(b))\nplt.subplot(5,2,3)\nplt.title('blue channel')\nplt.imshow(b)\nplt.subplot(5,2,4)\nplt.title('bm=blue * mask')\nbm=b*mask\nprint('blue channel masked max pixel value= ', np.max(bm))\nplt.imshow(bm)\ngm=g*mask\nplt.subplot(5, 2,  5)\nplt.title('green channel')\nplt.imshow(g)\nplt.subplot(5, 2,  6)\nplt.title('gm=green * mask')\nplt.imshow(gm)\nrm=g * mask\nplt.subplot(5,2,7)\nplt.title('red channel')\nplt.imshow(r)\nplt.subplot(5,2,8)\nplt.title('rm=red * mask')\nplt.imshow(rm)\nmasked_img= cv2.merge([bm, gm, rm])/255\nprint('max pixel value of result= ', np.max(masked_img))\nplt.subplot(5,2,9)\nplt.title('merged(bm,gm,rm)')\nplt.imshow(masked_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T00:35:00.40842Z","iopub.execute_input":"2021-05-29T00:35:00.408892Z","iopub.status.idle":"2021-05-29T00:35:02.481939Z","shell.execute_reply.started":"2021-05-29T00:35:00.408838Z","shell.execute_reply":"2021-05-29T00:35:02.480751Z"}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport time\nimport cv2 as cv2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport os\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom IPython.core.display import display, HTML","metadata":{"papermill":{"duration":5.205912,"end_time":"2021-05-27T23:00:01.579855","exception":false,"start_time":"2021-05-27T22:59:56.373943","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-29T01:33:09.979248Z","iopub.execute_input":"2021-05-29T01:33:09.979814Z","iopub.status.idle":"2021-05-29T01:33:09.991919Z","shell.execute_reply.started":"2021-05-29T01:33:09.979778Z","shell.execute_reply":"2021-05-29T01:33:09.990922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### input an image and get the shape","metadata":{"execution":{"iopub.execute_input":"2021-05-24T02:41:06.896292Z","iopub.status.busy":"2021-05-24T02:41:06.895551Z","iopub.status.idle":"2021-05-24T02:41:06.898243Z","shell.execute_reply":"2021-05-24T02:41:06.897841Z","shell.execute_reply.started":"2021-05-24T02:24:22.496614Z"},"papermill":{"duration":0.021899,"end_time":"2021-05-27T23:00:01.623248","exception":false,"start_time":"2021-05-27T23:00:01.601349","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fpath=r'../input/ham1000-segmentation-and-classification/images/ISIC_0024306.jpg'\nimg=plt.imread(fpath)\nprint (img.shape)\nplt.imshow(img)","metadata":{"papermill":{"duration":0.407452,"end_time":"2021-05-27T23:00:02.052352","exception":false,"start_time":"2021-05-27T23:00:01.6449","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:09.994667Z","iopub.execute_input":"2021-05-29T01:33:09.995172Z","iopub.status.idle":"2021-05-29T01:33:10.537046Z","shell.execute_reply.started":"2021-05-29T01:33:09.995136Z","shell.execute_reply":"2021-05-29T01:33:10.536238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### read in the csv file","metadata":{"papermill":{"duration":0.024421,"end_time":"2021-05-27T23:00:02.101014","exception":false,"start_time":"2021-05-27T23:00:02.076593","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df=pd.read_csv(r'../input/ham1000-segmentation-and-classification/GroundTruth.csv')\nprint (df.head())\nprint (len(df))\nprint (df.columns)\nlabels=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']","metadata":{"papermill":{"duration":0.062899,"end_time":"2021-05-27T23:00:02.188554","exception":false,"start_time":"2021-05-27T23:00:02.125655","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:10.538448Z","iopub.execute_input":"2021-05-29T01:33:10.538797Z","iopub.status.idle":"2021-05-29T01:33:10.583171Z","shell.execute_reply.started":"2021-05-29T01:33:10.53876Z","shell.execute_reply":"2021-05-29T01:33:10.581991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Warning the column image does NOT include the extension '.jpg' so to work need to modify the image column","metadata":{}},{"cell_type":"code","source":"df['image']=df['image'].apply(lambda x: x+ '.jpg')\nprint (df.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-29T01:33:10.584646Z","iopub.execute_input":"2021-05-29T01:33:10.584992Z","iopub.status.idle":"2021-05-29T01:33:10.601247Z","shell.execute_reply.started":"2021-05-29T01:33:10.584932Z","shell.execute_reply":"2021-05-29T01:33:10.600237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### one hot encoding caused problem when trying to create ImageDataGenerator.flow_from_dataframe even with  class_mode='raw'\nas a work around create a new column in df labels that contains the string version of the label, like \"MEL\" , \"NV\" etc\nthen you can use this as y_col=labels with class_mode='categorical","metadata":{}},{"cell_type":"code","source":"labels=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']\nlabel_list=[]\nfor i in range (len(df)):\n    row= list(df.iloc[i])\n    del row[0]\n    index=np.argmax(row)\n    label=labels[index]\n    label_list.append(label)\ndf['label']= label_list\ndf=df.drop(labels, axis=1)\nprint (df.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-29T01:33:10.602668Z","iopub.execute_input":"2021-05-29T01:33:10.603058Z","iopub.status.idle":"2021-05-29T01:33:12.35133Z","shell.execute_reply.started":"2021-05-29T01:33:10.603021Z","shell.execute_reply":"2021-05-29T01:33:12.350452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### split df into train, test and valid dataframes","metadata":{"papermill":{"duration":0.025153,"end_time":"2021-05-27T23:00:02.352477","exception":false,"start_time":"2021-05-27T23:00:02.327324","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_split=.95 # set this to the percentof the data you want to use for training\nvalid_split=.025 # set this to the percent of the data you want to use for validation\n# Note percent of data sed for test is 1-train_split-valid_split\ndummy_split=valid_split/(1-train_split)\ntrain_df, dummy_df=train_test_split(df, train_size=train_split, shuffle=True, random_state=123)\nvalid_df, test_df=train_test_split(dummy_df, train_size=dummy_split, shuffle=True, random_state=123)\nprint(' train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))  \nprint (train_df.head())\nprint (train_df['label'].value_counts())","metadata":{"papermill":{"duration":0.024367,"end_time":"2021-05-27T23:00:02.40139","exception":false,"start_time":"2021-05-27T23:00:02.377023","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:12.352455Z","iopub.execute_input":"2021-05-29T01:33:12.352853Z","iopub.status.idle":"2021-05-29T01:33:12.381209Z","shell.execute_reply.started":"2021-05-29T01:33:12.352775Z","shell.execute_reply":"2021-05-29T01:33:12.380379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### dataset is highly out of balance limit maximum samples per class to 300 samples to help balance it","metadata":{}},{"cell_type":"code","source":"print ('original number of classes: ', len(df['label'].unique()))     \nsize=300 # set number of samples for each class\nsamples=[]\ngroup=df.groupby('label')\nfor label in df['label'].unique():\n    Lgroup=group.get_group(label)\n    count=int(Lgroup['label'].value_counts())    \n    if count>=size:\n        sample=Lgroup.sample(size, axis=0)        \n    else:        \n        sample=Lgroup.sample(frac=1, axis=0)\n    samples.append(sample) \ntrain_df=pd.concat(samples, axis=0).reset_index(drop=True)\nprint (len(train_df))\nprint ('final number of classes: ', len(train_df['label'].unique()))       \nprint (train_df['label'].value_counts())  ","metadata":{"execution":{"iopub.status.busy":"2021-05-29T01:33:12.384082Z","iopub.execute_input":"2021-05-29T01:33:12.384499Z","iopub.status.idle":"2021-05-29T01:33:12.410797Z","shell.execute_reply.started":"2021-05-29T01:33:12.384465Z","shell.execute_reply":"2021-05-29T01:33:12.409975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create train, test, valid  generators","metadata":{"papermill":{"duration":0.024424,"end_time":"2021-05-27T23:00:02.450632","exception":false,"start_time":"2021-05-27T23:00:02.426208","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sdir=r'../input/ham1000-segmentation-and-classification/images' # main directory where data is stored\nsave_dir=r'./' # output directory where model will be saved\nsubject='cancer' # part of the name of the saved model\nheight=224  # image height\nwidth=224   # image width\nchannels=3  # number of coloor channels\nbatch_size=40  # model batch size for training and evaluation\nimg_shape=(height, width, channels)\nimg_size=(height, width)\n# code below determines test generator batch size and steps so that test_batch_size X test_steps = number of test samples\n# ensures that for predictions you go through the test set exactly once\nlength=len(test_df)\ntest_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \ntest_steps=int(length/test_batch_size)\nprint ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n#\ndef scalar(img): # unused for efficientnet model\n    return img/127.5-1  # scale pixel between -1 and +1\ngen=ImageDataGenerator() # no scaling of pixels is need for efficientnet\ntrain_gen=gen.flow_from_dataframe( train_df, sdir, x_col='image', y_col='label', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=gen.flow_from_dataframe( test_df, sdir, x_col='image', y_col='label', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\nvalid_gen=gen.flow_from_dataframe( valid_df, sdir, x_col='image', y_col='label', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n\nclasses=list(train_gen.class_indices.keys())\n\nclass_count=len(classes)\ntrain_steps=int(len(train_gen.labels)/batch_size)","metadata":{"papermill":{"duration":8.468849,"end_time":"2021-05-27T23:00:10.944471","exception":false,"start_time":"2021-05-27T23:00:02.475622","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:12.412468Z","iopub.execute_input":"2021-05-29T01:33:12.41306Z","iopub.status.idle":"2021-05-29T01:33:19.090135Z","shell.execute_reply.started":"2021-05-29T01:33:12.413024Z","shell.execute_reply":"2021-05-29T01:33:19.088644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train data is still misblanced but not as bad use class_weight  to help with this","metadata":{}},{"cell_type":"code","source":"class_weight={}\ntrain_dict=train_gen.class_indices\nclasses=list(train_dict.keys())\nclass_count=len(classes)\nlabels=train_gen.labels\ncount_array=np.zeros((class_count))\nfor value in train_dict.values(): # these are the integer values of the labels\n    for label in labels: # iterate through the train_gen labels   \n        if label==value:\n            count_array[value] +=1    \n#print (count_array)\nmax_samples=np.max(count_array)\nmax_index=np.argmax(count_array)\nmax_class=classes[max_index]\nprint('class ', max_class,' with ', max_samples, 'samples has the largest sample size')\nmsg='{0:^30s}{1:^10s}{2:^9s}'.format('Class', 'Samples', 'Weight')\nprint(msg)\nfor i in range (class_count):\n    class_weight[i]= max_samples/count_array[i]\n    msg=f'{classes[i]:^30s}{str(count_array[i]):^10s}{class_weight[i]:^9.5f}'\n    print (msg)","metadata":{"papermill":{"duration":0.113042,"end_time":"2021-05-27T23:00:11.083783","exception":false,"start_time":"2021-05-27T23:00:10.970741","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:19.091478Z","iopub.execute_input":"2021-05-29T01:33:19.091875Z","iopub.status.idle":"2021-05-29T01:33:19.106764Z","shell.execute_reply.started":"2021-05-29T01:33:19.091839Z","shell.execute_reply":"2021-05-29T01:33:19.104383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create function to show some image examples","metadata":{"papermill":{"duration":0.025855,"end_time":"2021-05-27T23:00:11.135783","exception":false,"start_time":"2021-05-27T23:00:11.109928","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def show_image_samples(gen ):\n    test_dict=test_gen.class_indices\n    classes=list(test_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    plt.figure(figsize=(20, 20))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):\n        plt.subplot(5, 5, i + 1)\n        image=images[i]/255 # scale images between 0 and 1 becaue no preprocessing scaling was done for efficientnet\n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color='blue', fontsize=16)\n        plt.axis('off')\n    plt.show()","metadata":{"papermill":{"duration":0.035605,"end_time":"2021-05-27T23:00:11.197615","exception":false,"start_time":"2021-05-27T23:00:11.16201","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:19.108246Z","iopub.execute_input":"2021-05-29T01:33:19.108649Z","iopub.status.idle":"2021-05-29T01:33:19.118899Z","shell.execute_reply.started":"2021-05-29T01:33:19.108585Z","shell.execute_reply":"2021-05-29T01:33:19.118093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image_samples(train_gen)","metadata":{"papermill":{"duration":2.938423,"end_time":"2021-05-27T23:00:14.161979","exception":false,"start_time":"2021-05-27T23:00:11.223556","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:19.120208Z","iopub.execute_input":"2021-05-29T01:33:19.120658Z","iopub.status.idle":"2021-05-29T01:33:23.233909Z","shell.execute_reply.started":"2021-05-29T01:33:19.120624Z","shell.execute_reply":"2021-05-29T01:33:23.233129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### define function to print text in RGB foreground and background colors","metadata":{"papermill":{"duration":0.079787,"end_time":"2021-05-27T23:00:14.314556","exception":false,"start_time":"2021-05-27T23:00:14.234769","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","metadata":{"papermill":{"duration":0.085561,"end_time":"2021-05-27T23:00:14.477607","exception":false,"start_time":"2021-05-27T23:00:14.392046","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:23.235155Z","iopub.execute_input":"2021-05-29T01:33:23.235606Z","iopub.status.idle":"2021-05-29T01:33:23.241917Z","shell.execute_reply.started":"2021-05-29T01:33:23.235567Z","shell.execute_reply":"2021-05-29T01:33:23.241051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create the model","metadata":{"papermill":{"duration":0.07254,"end_time":"2021-05-27T23:00:14.62517","exception":false,"start_time":"2021-05-27T23:00:14.55263","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model_name='EfficientNetB5'\nbase_model=tf.keras.applications.EfficientNetB1(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) ","metadata":{"papermill":{"duration":4.646936,"end_time":"2021-05-27T23:00:19.344261","exception":false,"start_time":"2021-05-27T23:00:14.697325","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:23.243068Z","iopub.execute_input":"2021-05-29T01:33:23.243699Z","iopub.status.idle":"2021-05-29T01:33:28.272627Z","shell.execute_reply.started":"2021-05-29T01:33:23.243659Z","shell.execute_reply":"2021-05-29T01:33:28.27176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create a subclass of callbacks to control learning rate and print training results for each epoch","metadata":{"papermill":{"duration":0.073596,"end_time":"2021-05-27T23:00:19.492198","exception":false,"start_time":"2021-05-27T23:00:19.418602","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class LRA(keras.callbacks.Callback):\n    reset=False\n    count=0\n    stop_count=0\n    tepochs=0\n    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze,batches, initial_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        #self.count=0 # initialize counter that counts epochs with no improvement\n        #self.stop_count=0 # initialize counter that counts how manytimes lr has been adjustd with no improvement  \n        self.initial_epoch=initial_epoch \n        self.batches=batches\n        #self.epochs=epochs\n        best_weights=self.model.get_weights() # set a class vaiable so weights can be loaded after training is completed        \n        msg=' '\n        if freeze==True:\n            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n        else:\n            msgs=f' Starting training using base model { model_name} training all layers '            \n        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n    def on_train_begin(self, logs=None):\n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor', 'Duration', 'Batch')\n        print_in_color(msg, (244,252,3), (55,65,80)) \n        \n    def on_train_batch_begin(self, batch, logs=None):\n        msg='{0:83s}{1:4s}of {2:5s}'.format(' ', str(batch), str(self.batches))\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count\n        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss')\n        #print ( '\\n',v_loss, self.lowest_vloss, acc, self.highest_tracc)\n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor='accuracy'\n            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss\n                color= (0,255,0)\n                self.lr=lr\n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1:\n                    color=(245, 170, 66)\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss                                    \n                else:\n                    self.count=self.count +1 # increment patience counter                    \n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor='val_loss'\n            if v_loss< self.lowest_vloss: # check if the validation loss improved \n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0  \n                color=(0,255,0)\n                self.lr=lr\n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    color=(245, 170, 66)\n                    self.lr=self.lr * self.factor # adjust the learning rate                    \n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter                    \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        msg=f'{str(epoch+1):^3s}/{str(LRA.tepochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{self.lr:^9.5f}{monitor:^11s}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(msg, (0,255,0), (55,65,80))\n            self.model.stop_training = True # stop training","metadata":{"papermill":{"duration":0.100958,"end_time":"2021-05-27T23:00:19.667475","exception":false,"start_time":"2021-05-27T23:00:19.566517","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:28.274069Z","iopub.execute_input":"2021-05-29T01:33:28.27442Z","iopub.status.idle":"2021-05-29T01:33:28.29744Z","shell.execute_reply.started":"2021-05-29T01:33:28.274384Z","shell.execute_reply":"2021-05-29T01:33:28.296458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instantiate the callback and train the model","metadata":{"papermill":{"duration":0.074658,"end_time":"2021-05-27T23:00:19.821685","exception":false,"start_time":"2021-05-27T23:00:19.747027","status":"completed"},"tags":[]}},{"cell_type":"code","source":"epochs =40\npatience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\nstop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\nthreshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\nfactor=.5 # factor to reduce lr by\ndwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\nfreeze=False # if true free weights of  the base model\nbatches=train_steps\ncallbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,dwell=dwell, model_name=model_name, freeze=freeze, batches=batches,initial_epoch=0 )]\nLRA.tepochs=epochs  # used to determine value of last epoch for printing\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0, class_weight=class_weight)","metadata":{"papermill":{"duration":1564.84775,"end_time":"2021-05-27T23:26:24.74422","exception":false,"start_time":"2021-05-27T23:00:19.89647","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-29T01:33:28.298929Z","iopub.execute_input":"2021-05-29T01:33:28.299296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### define function to plot the training data","metadata":{"papermill":{"duration":1.117935,"end_time":"2021-05-27T23:26:29.237001","exception":false,"start_time":"2021-05-27T23:26:28.119066","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","metadata":{"papermill":{"duration":1.138666,"end_time":"2021-05-27T23:26:31.536514","exception":false,"start_time":"2021-05-27T23:26:30.397848","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### define function to generate the confusion matrix and classification report","metadata":{"papermill":{"duration":1.126534,"end_time":"2021-05-27T23:26:33.780761","exception":false,"start_time":"2021-05-27T23:26:32.654227","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def print_info( test_gen, preds, print_code, save_dir, subject ):\n    class_dict=test_gen.class_indices\n    labels= test_gen.labels\n    file_names= test_gen.filenames \n    error_list=[]\n    true_class=[]\n    pred_class=[]\n    prob_list=[]\n    new_dict={}\n    error_indices=[]\n    y_pred=[]\n    for key,value in class_dict.items():\n        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n    # store new_dict as a text fine in the save_dir\n    classes=list(new_dict.values())     # list of string of class names\n    dict_as_text=str(new_dict)\n    dict_name= subject + '-' +str(len(classes)) +'.txt'  \n    dict_path=os.path.join(save_dir,dict_name)    \n    with open(dict_path, 'w') as x_file:\n        x_file.write(dict_as_text)    \n    errors=0      \n    for i, p in enumerate(preds):\n        pred_index=np.argmax(p)        \n        true_index=labels[i]  # labels are integer values\n        if pred_index != true_index: # a misclassification has occurred\n            error_list.append(file_names[i])\n            true_class.append(new_dict[true_index])\n            pred_class.append(new_dict[pred_index])\n            prob_list.append(p[pred_index])\n            error_indices.append(true_index)            \n            errors=errors + 1\n        y_pred.append(pred_index)    \n    if print_code !=0:\n        if errors>0:\n            if print_code>errors:\n                r=errors\n            else:\n                r=print_code           \n            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n            print_in_color(msg, (0,255,0),(55,65,80))\n            for i in range(r):                \n                split1=os.path.split(error_list[i])                \n                split2=os.path.split(split1[0])                \n                fname=split2[1] + '/' + split1[1]\n                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])\n                print_in_color(msg, (255,255,255), (55,65,60))\n                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n        else:\n            msg='With accuracy of 100 % there are no errors to print'\n            print_in_color(msg, (0,255,0),(55,65,80))\n    if errors>0:\n        plot_bar=[]\n        plot_class=[]\n        for  key, value in new_dict.items():        \n            count=error_indices.count(key) \n            if count!=0:\n                plot_bar.append(count) # list containg how many times a class c had an error\n                plot_class.append(value)   # stores the class \n        fig=plt.figure()\n        fig.set_figheight(len(plot_class)/3)\n        fig.set_figwidth(10)\n        plt.style.use('fivethirtyeight')\n        for i in range(0, len(plot_class)):\n            c=plot_class[i]\n            x=plot_bar[i]\n            plt.barh(c, x, )\n            plt.title( ' Errors by Class on Test Set')\n    y_true= np.array(labels)        \n    y_pred=np.array(y_pred)\n    if len(classes)<= 30:\n        # create a confusion matrix \n        cm = confusion_matrix(y_true, y_pred )        \n        length=len(classes)\n        if length<8:\n            fig_width=8\n            fig_height=8\n        else:\n            fig_width= int(length * .5)\n            fig_height= int(length * .5)\n        plt.figure(figsize=(fig_width, fig_height))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n    clr = classification_report(y_true, y_pred, target_names=classes)\n    print(\"Classification Report:\\n----------------------\\n\", clr)","metadata":{"papermill":{"duration":1.144878,"end_time":"2021-05-27T23:26:36.304477","exception":false,"start_time":"2021-05-27T23:26:35.159599","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### evaluate model on the test set then save the model","metadata":{"papermill":{"duration":1.202962,"end_time":"2021-05-27T23:26:39.117227","exception":false,"start_time":"2021-05-27T23:26:37.914265","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tr_plot(history,0)\nacc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps, return_dict=False)[1]*100\nmsg=f'accuracy on the test set is {acc:5.2f} %'\nprint_in_color(msg, (0,255,0),(55,65,80))\nsave_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\nsave_loc=os.path.join(save_dir, save_id)\nmodel.save(save_loc)","metadata":{"papermill":{"duration":5.665391,"end_time":"2021-05-27T23:26:45.919904","exception":false,"start_time":"2021-05-27T23:26:40.254513","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### make predictions on test set and generate confusion matrix and classification report","metadata":{"papermill":{"duration":1.145074,"end_time":"2021-05-27T23:26:48.20201","exception":false,"start_time":"2021-05-27T23:26:47.056936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print_code=0\npreds=model.predict(test_gen) \nprint_info( test_gen, preds, print_code, save_dir, subject )  ","metadata":{"papermill":{"duration":4.245087,"end_time":"2021-05-27T23:26:53.570632","exception":false,"start_time":"2021-05-27T23:26:49.325545","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":1.131599,"end_time":"2021-05-27T23:26:55.833563","exception":false,"start_time":"2021-05-27T23:26:54.701964","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":1.170778,"end_time":"2021-05-27T23:26:58.415029","exception":false,"start_time":"2021-05-27T23:26:57.244251","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":1.204408,"end_time":"2021-05-27T23:27:00.786705","exception":false,"start_time":"2021-05-27T23:26:59.582297","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}