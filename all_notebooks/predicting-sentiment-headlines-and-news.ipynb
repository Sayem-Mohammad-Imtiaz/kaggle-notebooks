{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<b> <u> Multiple Linear Regression </u> </b> <br>\n<b> <u> News Case Study </u> </b> <br>\nProblem Statement:\nEssentially, the company wants â€”\n\nTo identify the variables affecting\n\nTo create a linear model that quantitatively you infer about their effect on the dependent variable\n\n\n\n**So interpretation is important!**","metadata":{}},{"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train = pd.read_csv(\"C:/Users/Administrator/Desktop/Upgrad Case Study/Multiple linear regression - news/train_file.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test = pd.read_csv(\"C:/Users/Administrator/Desktop/Upgrad Case Study/Multiple linear regression - news/test_file.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import preprocessing from sklearn\nfrom sklearn import preprocessing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.IDLink = news_train.IDLink.astype(str)\nnews_train.Title = news_train.Title.astype(str)\nnews_train.Headline = news_train.Headline.astype(str)\nnews_train.Source = news_train.Source.astype(str)\nnews_train.Topic = news_train.Topic.astype(str)\nnews_train.PublishDate = news_train.PublishDate.astype(str)\nnews_train.SentimentTitle = news_train.SentimentTitle.astype(str)\nnews_train.SentimentHeadline = news_train.SentimentHeadline.astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\nnews_train_2 = news_train.apply(le.fit_transform)\nnews_train_2.head(1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = [\"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"Facebook\",\"GooglePlus\",\"LinkedIn\",\"SentimentTitle\",\"SentimentHeadline\"]\n\nnews_train_2[num_vars] = scaler.fit_transform(news_train_2[num_vars])\n\nnews_train_2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train_2.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From , the `Above Dataset`, the max-min scaler is used to put all the values between 0 and 1","metadata":{}},{"cell_type":"markdown","source":"### Dividing into X and Y sets for the model building","metadata":{}},{"cell_type":"markdown","source":"- Here , `SentimentTitle` is the target variable - To be used to predict the demand of the Shared bikes ","metadata":{}},{"cell_type":"markdown","source":"- Here , `SentimentHeadline` is the target variable - To be used to predict the demand of the Shared bikes ","metadata":{}},{"cell_type":"code","source":"y_train = news_train_2.pop('SentimentTitle')\nX_train = news_train_2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building our model\n\nThis time, we will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)","metadata":{}},{"cell_type":"markdown","source":"### RFE\nRecursive feature elimination","metadata":{}},{"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 7)             # running RFE\nrfe = rfe.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns[~rfe.support_]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here , After Using the `RFE` for the Automatic Selection , it has negated `'IDLink', 'Title', 'Source'`","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train.drop([\"IDLink\", \"Title\", \"Source\"], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_new = X_train[col]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_new)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_new).fit()   # Running the linear model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm.params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","metadata":{}},{"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will move to Manual Regression , with respect to the Signifance(P-value) and the VIF Factor","metadata":{}},{"cell_type":"markdown","source":"LinkedIn             -0.0102      0.014     -0.723      `0.469` <br>\nThe P-value of Linked is `0.469` and can be negated","metadata":{}},{"cell_type":"code","source":"X_train_new.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['LinkedIn'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Facebook              0.0101      0.010      0.977      `0.329` <br>\nThe P-value of Linked is `0.0.329` and can be negated","metadata":{}},{"cell_type":"code","source":"X_train_new.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['Facebook'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['PublishDate'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the `VIF` value of the `const` is high : 8.02 and hence can be negated ","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['const'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the above statement, All the VIF values are less than `5` nad P-value are less than `0.05`","metadata":{}},{"cell_type":"markdown","source":"## Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","metadata":{}},{"cell_type":"code","source":"y_train_cnt = lr_2.predict(X_train_lm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Predictions","metadata":{}},{"cell_type":"markdown","source":"#### Applying the scaling on the test sets","metadata":{}},{"cell_type":"code","source":"# import preprocessing from sklearn\nfrom sklearn import preprocessing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.IDLink = news_test.IDLink.astype(str)\nnews_test.Title = news_test.Title.astype(str)\nnews_test.Headline = news_test.Headline.astype(str)\nnews_test.Source = news_test.Source.astype(str)\nnews_test.Topic = news_test.Topic.astype(str)\nnews_test.PublishDate = news_test.PublishDate.astype(str)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test['SentimentTitle'] = 0\nnews_test['SentimentHeadline'] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.SentimentTitle = news_test.SentimentTitle.astype(str)\nnews_test.SentimentHeadline = news_test.SentimentHeadline.astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\nnews_test_2 = news_test.apply(le.fit_transform)\nnews_test_2.head(1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = [\"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"Facebook\",\"GooglePlus\",\"LinkedIn\",\"SentimentTitle\",\"SentimentHeadline\"]\n\nnews_test_2[num_vars] = scaler.fit_transform(news_test_2[num_vars])\n\nnews_test_2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here , The `scaling` is completed on the test data. The values lies in the range between 0 and 1","metadata":{}},{"cell_type":"markdown","source":"#### Applying the scaling on the test sets","metadata":{}},{"cell_type":"code","source":"num_vars = ['IDLink','Title','Headline','Source','Topic', 'PublishDate','Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_2[num_vars] = scaler.transform(news_test_2[num_vars])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dividing into X_test and y_test","metadata":{}},{"cell_type":"code","source":"y_test = news_test_2.pop('SentimentTitle')\nX_test = news_test_2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions\ny_pred = lr_2.predict(X_test_new)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_2.params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The 'SentimentTitle' predicted as per the model is \", y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, The test Dataframe of unknown data on which the `SentimentTitle` is presdicted :","metadata":{}},{"cell_type":"code","source":"news_test.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test2 = y_pred.to_frame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test2.head(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final = news_test.join(news_test2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final = news_test_final.drop(columns=['SentimentTitle'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final = news_test_final.rename(columns={0: 'SentimentTitle'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the Y_Pred is successfully Done on the `test` data. <br>\nThe Test data with `Sentiment Title`","metadata":{}},{"cell_type":"code","source":"news_test_final.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final = news_test_final.drop(columns=['SentimentHeadline'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <u> SentimentHeadline : Predicting on the unseen Test Data </u>","metadata":{}},{"cell_type":"code","source":"news_train.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.IDLink = news_train.IDLink.astype(str)\nnews_train.Title = news_train.Title.astype(str)\nnews_train.Headline = news_train.Headline.astype(str)\nnews_train.Source = news_train.Source.astype(str)\nnews_train.Topic = news_train.Topic.astype(str)\nnews_train.PublishDate = news_train.PublishDate.astype(str)\nnews_train.SentimentTitle = news_train.SentimentTitle.astype(str)\nnews_train.SentimentHeadline = news_train.SentimentHeadline.astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_train.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\nnews_train_2 = news_train.apply(le.fit_transform)\nnews_train_2.head(1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = [\"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"Facebook\",\"GooglePlus\",\"LinkedIn\",\"SentimentTitle\",\"SentimentHeadline\"]\n\nnews_train_2[num_vars] = scaler.fit_transform(news_train_2[num_vars])\n\nnews_train_2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the `scaling` of train data is done between 0 and 1","metadata":{}},{"cell_type":"code","source":"news_train_2.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From , the `Above Dataset`, the max-min scaler is used to put all the values between 0 and 1","metadata":{}},{"cell_type":"code","source":"y_train = news_train_2.pop('SentimentHeadline')\nX_train = news_train_2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building our model - Sentiment Headline\n\nThis time, we will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)","metadata":{}},{"cell_type":"markdown","source":"### RFE\nRecursive feature elimination","metadata":{}},{"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 7)             # running RFE\nrfe = rfe.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns[~rfe.support_]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Negating the Above Value As identified by the `RFE`","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train.drop([\"IDLink\",\"Source\",\"LinkedIn\"], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_new = X_train[col]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_new)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_new).fit()   # Running the linear model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's see the summary of our linear model\nprint(lm.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","metadata":{}},{"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will move to Manual Regression , with respect to the Signifance(P-value) and the VIF Factor","metadata":{}},{"cell_type":"markdown","source":"Title             -0.0036      0.004     -0.852      0.394 <br>\nThe `Significance Value` of the `Title` is : 0.394 . Hence we are dopping this","metadata":{}},{"cell_type":"code","source":"X_train_new.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['Title'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Headline           0.0049      0.004      1.191      0.234 <br>\nThe `Significance Value` of the `Headline` is : 0.234 . Hence we are dopping this","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['Headline'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Facebook          -0.0157      0.012     -1.362      0.173 <br>\nThe `Significance Value` of the `Facebook` is : 0.173 . Hence we are dopping this","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['Facebook'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GooglePlus         0.0230      0.019      1.196      0.232 <br>\nThe `Significance Value` of the `GooglePlus` is : 0.232 . Hence we are dopping this","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['GooglePlus'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PublishDate        0.0059      0.004      1.438      0.150 <br>\nThe `Significance Value` of the `PublishDate` is : 0.150 . Hence we are dopping this","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['PublishDate'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = X_train_new.drop(['const'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build a third fitted model\nX_train_lm = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm).fit()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the summary of the model\nprint(lr_2.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Model is fine , Hen we can go ahead and accept this model ","metadata":{}},{"cell_type":"markdown","source":"## Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","metadata":{}},{"cell_type":"code","source":"y_train_cnt = lr_2.predict(X_train_lm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Predictions","metadata":{}},{"cell_type":"markdown","source":"#### Applying the scaling on the test sets","metadata":{}},{"cell_type":"code","source":"# import preprocessing from sklearn\nfrom sklearn import preprocessing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.IDLink = news_test.IDLink.astype(str)\nnews_test.Title = news_test.Title.astype(str)\nnews_test.Headline = news_test.Headline.astype(str)\nnews_test.Source = news_test.Source.astype(str)\nnews_test.Topic = news_test.Topic.astype(str)\nnews_test.PublishDate = news_test.PublishDate.astype(str)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test['SentimentTitle'] = 0\nnews_test['SentimentHeadline'] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnews_test.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.SentimentTitle = news_test.SentimentTitle.astype(str)\nnews_test.SentimentHeadline = news_test.SentimentHeadline.astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\nnews_test_2 = news_test.apply(le.fit_transform)\nnews_test_2.head(1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = [\"IDLink\",\"Title\",\"Headline\",\"Source\",\"Topic\",\"PublishDate\",\"Facebook\",\"GooglePlus\",\"LinkedIn\",\"SentimentTitle\",\"SentimentHeadline\"]\n\nnews_test_2[num_vars] = scaler.fit_transform(news_test_2[num_vars])\n\nnews_test_2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Applying the scaling on the test sets\n\nnum_vars = ['IDLink','Title','Headline','Source','Topic', 'PublishDate','Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']\n\nnews_test_2[num_vars] = scaler.transform(news_test_2[num_vars])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#### Dividing into X_test and y_test\n\ny_test = news_test_2.pop('SentimentHeadline')\nX_test = news_test_2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions\ny_pred = lr_2.predict(X_test_new)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The 'SentimentHeadline' predicted as per the model is \", y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test2 = y_pred.to_frame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test2.head(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final2 = news_test.join(news_test2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final2.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final2 = news_test_final2.drop(columns=['SentimentHeadline'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final2 = news_test_final2.rename(columns={0: 'SentimentHeadline'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final2.head(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final2 = news_test_final2.drop(columns=['SentimentTitle'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final3 = news_test_final2[[\"IDLink\", \"SentimentHeadline\"]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_test_final3.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.merge(news_test_final, news_test_final3, on='IDLink')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <br><u> The Final Data Set After Predicting 'SentimentTitle'  'SentimentHeadline'</u> </br>\n#### This is done on the unknown/test data","metadata":{}},{"cell_type":"code","source":"result.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}