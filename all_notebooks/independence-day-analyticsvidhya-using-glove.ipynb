{"cells":[{"metadata":{},"cell_type":"markdown","source":"**We have the task of predicting which subject a set of abstract and Title belong to. It is a multi-label classification problem - so each abstract can belong to multiple subjects**\n\n**We explore 2 approaches:**\n1. **Shallow Learning using GloVe embeddings**\n2. **Deep Learning using GloVe embeddings**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import preprocessing, linear_model, ensemble, metrics, model_selection, svm, pipeline, naive_bayes\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nimport spacy\nimport textblob\n#from nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn import decomposition\nfrom nltk.corpus import stopwords \nsw = stopwords.words(\"english\")\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read Data\ntrain = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/train.csv')\ntest = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimensionality of embedding\nn = 200","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt', 'r')\nraw = file.read()\nprint('Read Raw File')\n\n# Get Embeddings in List of Lists format\nemb_list = [val.split(' ') for val in raw.split('\\n')]\nprint('Read as List of Lists')\n\n# Convert Embeddings to dataframe format\nemb = pd.DataFrame(emb_list).T\nemb.columns = emb.iloc[0, :]\nemb = emb.drop(0, axis = 0).astype(float)\nprint('Embeddings converted to DataFrame')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shallow Learning Approach using GloVe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = train['ID']\ntest_id = test['ID']\n\n# Create indices to split train and test on later\ntrain['train_ind'] = np.arange(train.shape[0])\ntest['train_ind'] = np.arange(train.shape[0], train.shape[0]+test.shape[0])\n\n# Merge Train and Test - This approach only works for competitions - not for model deployment in real projects.\ndata = pd.concat([train, test], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optional Step\ndata['ABSTRACT'] = data['TITLE'] + ' ' + data['ABSTRACT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embeddings(word, n = 200):\n    if word.lower() in emb:\n        return emb[word.lower()].values.tolist()\n    else:\n        return np.zeros(n).tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import notebook\ncount = 0\nfinal_emb = []\nfor index, row in notebook.tqdm(data.iterrows()):\n    \n    emb_data = []\n    text = row['ABSTRACT']\n    \n    # Tokenize\n    words = word_tokenize(text)\n    \n    # Remove stopwords and ensure alphanumeric words\n    words = [w for w in words if w not in sw]\n    words = [w for w in words if w.isalpha()]\n    \n    # Sum up embeddings for each word\n    for word in words:\n        emb_data.append(get_embeddings(word, n))\n        \n    sentence_embedding_all = np.array(emb_data)\n\n    sum_vec = sentence_embedding_all.sum(axis = 0)/np.sqrt((sentence_embedding_all.sum(axis = 0) ** 2).sum())\n    final_emb.append(sum_vec)\nfinal_emb = np.array(final_emb)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data back to train and test\nX_train = final_emb[:train.shape[0], :]\ny_train = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[:train.shape[0]]\n\nX_test = final_emb[train.shape[0]:, :]\ny_test = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model - Logistic Regression is a good option for Text classification problems\n#model = linear_model.LogisticRegressionCV(penalty = 'l2', Cs = 10, max_iter = 5000).fit(X_train, y_train)\n#model = linear_model.RidgeClassifierCV().fit(X_train, y_train)\nfrom sklearn import naive_bayes\n\n#model = MultiOutputClassifier(estimator = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)).fit(X_train, y_train)\nmodel = MultiOutputClassifier(estimator = linear_model.LogisticRegressionCV(Cs = 10, cv = 5, n_jobs = -1, max_iter = 5000)).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_multioutput(predictions):\n    return np.array([[val[1] for val in inner] for inner in predictions]).T\n\ndef convert_probs_to_labels(predictions, threshold = .5, labels = None):\n    final = []\n    for prediction in predictions:\n        temp = (prediction > threshold)*1\n        final.append(temp)\n        \n    return final\n\ndef predict_1(predictions, threshold=.5):\n    preds = get_preds_multioutput(predictions)\n    preds = convert_probs_to_labels(preds, threshold = threshold, labels = None)\n    return np.array(preds)\n\n#predict_1(model.predict_proba(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ID'] = test_id\n\n#preds = predict_1(model.predict_proba(X_test))\nsub[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']] = model.predict(X_test).astype(int)\nsub.to_csv('sub.csv', index = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Deep Learning with GloVe Word Embeddings\nCode for creating Embedding matrix for Deep learning purposes taken from:\nhttps://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom  tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['ABSTRACT']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in sw))]\n        corpus.append(words)\n    return corpus\ncorpus=create_corpus(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\npadded=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\nmodel.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences = False))\nmodel.add(Dense(256, activation = 'relu'))\n\nmodel.add(Dense(6, activation='sigmoid'))\n\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data back to train and test\nX_train = padded[:train.shape[0], :]\ny_train = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[:train.shape[0]]\n\nX_test = padded[train.shape[0]:, :]\ny_test = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1,X_test1,y_train1,y_test1=train_test_split(X_train,y_train,test_size=0.15)\nprint('Shape of train',X_train1.shape)\nprint(\"Shape of Validation \",X_test1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train1,y_train1,batch_size=512,epochs=30,validation_data=(X_test1,y_test1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (14, 4))\nplt.plot(history.history['loss'], label = 'train loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ID'] = test_id\n\n#preds = predict_1(model.predict_proba(X_test))\nsub[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']] = (model.predict(X_test)>.6).astype(int)\nsub.to_csv('sub.csv', index = None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}