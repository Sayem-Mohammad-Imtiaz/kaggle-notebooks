{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Heart Disease Prediction","metadata":{}},{"cell_type":"markdown","source":"## Import packages","metadata":{}},{"cell_type":"code","source":"#Import python libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import svm #Import svm model\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import confusion_matrix #Calculate confusion matrix\nfrom xgboost import XGBClassifier\n\n#Visualization packages\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n#Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"**Function to Train and Test Machine Learning Model**","metadata":{}},{"cell_type":"code","source":"#Function to Train and Test Machine Learning Model\ndef train_test_ml_model(X_train,y_train,X_test,Model):\n    model.fit(X_train,y_train) #Train the Model\n    y_pred = model.predict(X_test) #Use the Model for prediction\n\n    # Test the Model\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(y_test,y_pred)\n    accuracy = round(100*np.trace(cm)/np.sum(cm),1)\n\n    #Plot/Display the results\n    cm_plot(cm,Model)\n    print('Accuracy of the Model' ,Model, str(accuracy)+'%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Function to plot Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"#Function to plot Confusion Matrix\ndef cm_plot(cm,Model):\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n    classNames = ['Negative','Positive']\n    plt.title('Comparison of Prediction Result for '+ Model)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=45)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n    plt.show()\n    print(cm[1][0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import data","metadata":{}},{"cell_type":"markdown","source":"**Import the data file 'heart.csv'**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-disease-data/heart.csv\")\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display basic statistics","metadata":{}},{"cell_type":"markdown","source":"**Display Information (info) about the 'data'**","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue'>**COMMENTS :** There is no missing data (all the columns have 303 rows). It must be noted that all the columns are numerical</font>","metadata":{}},{"cell_type":"markdown","source":"**Display basic statistics (description) about the 'data' - numerical**","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Display basic statistics (description) about the 'data' - categorical**","metadata":{}},{"cell_type":"code","source":"#data.select_dtypes(include=['object']).describe(include='all')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue'>**OBSERVATION :** Since there are no categorical columns, this step is not required</font>","metadata":{}},{"cell_type":"markdown","source":"## Convert categorical columns to numerical columns","metadata":{}},{"cell_type":"markdown","source":"**Convert categorical columns of 'data' to numerical columns**","metadata":{}},{"cell_type":"code","source":"#data=pd.get_dummies(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue'>**OBSERVATION :** Since there are no categorical columns, this step is not required</font>","metadata":{}},{"cell_type":"markdown","source":"## Create feature and target set","metadata":{}},{"cell_type":"markdown","source":"**Remove 'Exited' column from Feature set(X) and create Target set(y) with 'target' column**","metadata":{}},{"cell_type":"code","source":"#Create a training set by dropping target column\nX = data.drop('target',axis = 1) \n\n#Create the target set (output)\ny = data.target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling the data values to standardize the range of independent variables","metadata":{}},{"cell_type":"markdown","source":"**Normalize the feature set X**","metadata":{}},{"cell_type":"code","source":"#Feature scaling is a method used to standardize the range of independent variables or features of data.\n#Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX = scale.fit_transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue'>**COMMENTS :** It can be observed that range of values is normalized</font>","metadata":{}},{"cell_type":"markdown","source":"## Split the data into \"train\" and \"test\" set","metadata":{}},{"cell_type":"markdown","source":"**Split the Feature set (X) and Target set (y) into training set (X_train, y_train) and testing set (X_test,y_test)**","metadata":{}},{"cell_type":"code","source":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\nprint(\"X_train size ==>\", X_train.shape)\nprint(\"X_test size ==>\", X_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue'>**COMMENTS :** Total data (303 rows) is divided into Training set(212 rows) and Testing set (91 rows)</font>","metadata":{}},{"cell_type":"markdown","source":"## <font color='blue'>Support Vector Machine (SVM) ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC,NuSVC  #Import packages related to Model\nModel = \"SVC\"\nmodel=SVC() #Create the Model\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>XG Boost Classifer ML model</font>","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier  #Import packages related to Model\nModel = \"XGBClassifier()\"\nmodel=XGBClassifier() #Create the Model\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Gaussian Naive Bayes ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB,MultinomialNB  #Import packages related to Model\nModel = \"GaussianNB\"\nmodel=GaussianNB()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>SGD Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier #Import packages related to Model\nModel = \"SGDClassifier\"\nmodel=SGDClassifier()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Logistic Regression) ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression #Import packages related to Model\nModel = \"LogisticRegression\"\nmodel=LogisticRegression()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Decision Tree Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier #Import packages related to Model\nModel = \"DecisionTreeClassifier\"\nmodel=DecisionTreeClassifier()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Extra Tree Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import ExtraTreeClassifier #Import packages related to Model\nModel = \"ExtraTreeClassifier\"\nmodel=ExtraTreeClassifier()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Quadratic Discriminant Analysis ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #Import packages related to Model\nModel = \"QuadraticDiscriminantAnalysis\"\nmodel = QuadraticDiscriminantAnalysis()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Liner Discriminant Analysis ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis #Import packages related to Model\nModel = \"LinearDiscriminantAnalysis\"\nmodel=LinearDiscriminantAnalysis()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Random Forest Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier #Import packages related to Model\nModel = \"RandomForestClassifier\"\nmodel=RandomForestClassifier()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Ada Boost Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier #Import packages related to Model\nModel = \"AdaBoostClassifier\"\nmodel=AdaBoostClassifier()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>Gradient Boosting Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier #Import packages related to Model\nModel = \"GradientBoostingClassifier\"\nmodel=GradientBoostingClassifier()\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='blue'>KNeighours Classifier Classifier ML model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nModel = \"KNeighborsClassifier\"\nmodel=KNeighborsClassifier(7)\n\ntrain_test_ml_model(X_train,y_train,X_test,Model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}