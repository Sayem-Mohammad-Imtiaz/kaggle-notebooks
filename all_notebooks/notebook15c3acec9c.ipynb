{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        data = pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"note: followed https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html to make this notebook","metadata":{}},{"cell_type":"markdown","source":"split data in input and label, train and validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata = data[[\"class\",\"message\"]]\n#data.head(5)\nX = data[\"message\"]\nY = data[\"class\"]\n#Y = Y.replace({\"spam\":1,\"ham\":0})\nX_train, X_test, y_train, y_test = train_test_split(X, Y,random_state=0)\nX_train.head(5)\ny_train.head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"process input messages","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\n\n\nX_pipe = Pipeline([\n    #go from text to feature vector (includes detection of stopwords,...) with occurences\n    (\"cntvec\",CountVectorizer()),\n    #from count to term frequencies\n    (\"TfidTrans\",TfidfTransformer(use_idf=False)),\n    #use naive bayes classefier\n    #('clf', MultinomialNB())\n])\n\nX_pipe.fit(X_train,y_train)\nX_train_proc = X_pipe.transform(X_train)\nX_test_proc = X_pipe.transform(X_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test different classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn import metrics\n\nclasifiers = [\n    (\"naive bayes\",MultinomialNB()),\n    (\"clf\",SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),\n    (\"decision tree\",DecisionTreeClassifier(max_depth=5)),\n    (\"Linear SVM\",SVC(kernel=\"linear\", C=0.025)),\n    (\"RBF SVM\",SVC(gamma=2, C=1))\n]\n\nfor name,classif in clasifiers:\n    classif.fit(X_train_proc,y_train)\n    y_test_pred = classif.predict(X_test_proc)\n    \n    print(name)\n    print(metrics.classification_report(y_test,y_test_pred))\n    print(\"---------------------------------------------------------------------\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most classifiers correctly label all the \"ham\" messages (note that our dataset has around 6 times more \"ham\" then \"Spam\"), but the clf and the RBF SVM have better precission at labeling \"spam\". \nSince one of my sources recomends clf for text processing we continue with that.\n\nnow find the most obtimal parameters","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n#at clasifier to pipeline\n#X_pipe.steps.append((\"clf\",SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)))\nX_pipe = Pipeline([\n    #go from text to feature vector (includes detection of stopwords,...) with occurences\n    (\"cntvec\",CountVectorizer(ngram_range = (1,2))),\n    #from count to term frequencies\n    (\"TfidTrans\",TfidfTransformer(use_idf=False)),\n    #classifier\n    #(\"clf\",SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None))\n    (\"RBF_SVM\",SVC(gamma=2, C=1))\n])\n\n\n#grid of possible parameters\n#names: stepname in pipeline__parameter name\nparameters = {\n    #'cntvec__ngram_range': [(1, 1), (1, 2)],\n    'TfidTrans__use_idf': (True, False),\n    #'clf__alpha': (1e-2, 1e-3),\n    #'RBF_SVM__gamma': (1e-1,1e+1), #first try\n    'RBF_SVM__gamma': (1,5,1e+1),\n    #'RBF_SVM__C': (1e-2,1e+2) #first try\n    'RBF_SVM__C': (1e-2,1e-1,1)\n    }\n\n#search object\ngs_clf = GridSearchCV(X_pipe, parameters, cv=5, n_jobs=1)\n\ngs_clf = gs_clf.fit(X_train, y_train)\n\n\nfor param_name in sorted(parameters.keys()):\n    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n\n    \ny_test_pred = gs_clf.predict(X_test)\nprint(metrics.classification_report(y_test,y_test_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For clf\nThe parameters:\n* use_idf = False\n* alpha = 0.001\n* and ngram_range = (1,2)\n\ngive the best results.\nIt labels only 2% more spam messages then our initial guess (81%)\n\nfor RBF SVM the following parameters were the best:\n* C = 100\n* gamma = 0.1\n* use_idf = True\n* ngram_range = (1,2)\n\nThe labeling of \"spam\" has improved with 15% (to 93%) outperforming the clf. However it introduces some (altough very few) false spam labels. \n\nA further zoom in the parameters might improve even more.\n\nA second attemp for the SVM (with other parameter grid) gives the following as best:\n* C = 1\n* gamma = 1\n* use_idf = False\n\nThis is much closer to the original parameters used when testing the different clasifiers and the results are also musch closer: an 7% increase in \"spam\" labels and no false \"spam\". Clearly the amount of false spam has a bigger effect on the accuracy the false \"ham\".\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}