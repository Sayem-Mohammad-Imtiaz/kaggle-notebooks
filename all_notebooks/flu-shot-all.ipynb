{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"### Necessary imports ###\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n#import tensorflow as tf\nimport sklearn as sk\n#import matplotlib as mpl\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score\nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.ensemble import RandomForestClassifier as RF\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom catboost import Pool,CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostRegressor\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import svm, model_selection,tree, linear_model, neighbors, naive_bayes, ensemble \nfrom sklearn import discriminant_analysis, gaussian_process\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\n\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IML Funktionen","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\nfrom sklearn.inspection import permutation_importance\n\ndef print_importance(clf):\n    #get classifier per label(j)\n    j = 0\n    for rscv in clf.classifiers_:\n        \n        #get and sort importances from best estimator\n        importances = rscv.best_estimator_.feature_importances_\n        indices = np.argsort(importances)[::-1]\n        \n        #plot_my_tree(rscv)\n        # Print the feature ranking\n        print(\"label \" + str(test_y.columns[j]) + \":\")\n\n        for f in range(train_X.shape[1]-1):\n            print(\"%d. %s (%f)\" % (f +1 , feature_namen[indices[f]], importances[indices[f]]))\n        j+= 1\n        print()        \n\ndef plot_my_tree(rscv):\n    tree1 = rscv.best_estimator_.estimators_[0].tree_\n    fig = plt.figure(figsize=(20,15))\n    _ = tree.plot_tree(rscv.best_estimator_.estimators_[0],\n                  feature_names=feature_namen,\n                  class_names=labels,\n                  filled=True,\n                  max_depth=3,\n                  fontsize=10)        \n        \ndef print_my_trees(clf):\n    j=0\n    for rscv in clf.classifiers_:\n        print(\"label \" + str(test_y.columns[j]) + \":\")\n        plot_my_tree(rscv)\n        j+=1\n        print()    \n\ndef print_permutation(clf):\n    j=0 \n    for rscv in clf.classifiers_:\n        print(\"label \" + str(test_y.columns[j]) + \":\")\n        j=0 \n        r = permutation_importance(rscv.best_estimator_, test_X, test_y[test_y.columns[j]],\n                              n_repeats=30,\n                              random_state=0)\n\n        for i in r.importances_mean.argsort()[::-1]:\n            if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n                print(f\"{feature_namen[i]:<8}   \"\n                    f\"{r.importances_mean[i]:.3f}\"\n                    f\" +/- {r.importances_std[i]:.3f}\")\n        j +=1\n        print()\n  \n                                   \ndef print_coef(clf): \n    for rscv in clf.classifiers_:\n        j=0        \n        for array in rscv.coef_:\n            #safe feature in array\n            coef_features = []\n            for number in array:\n                coef_features.append([number, feature_namen[j]])\n                j += 1\n            coef_features.sort()\n            for pair in coef_features[:8]:\n                print(pair)\n        print()\n        j=1\n                                   \ndef print_scores(clf):\n    j= 0\n    for rscv in clf.classifiers_:\n        print(\"label \" + str(test_y.columns[j]) + \":\")\n        #print(rscv.scores_.info())\n        print(rscv.best_score_)\n        print(rscv.best_params_)\n        j+=1\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform-Funktion","metadata":{}},{"cell_type":"code","source":"def transform_test(d):\n    # Löschen der Werte mit zu vielen Null-Stellen und der ID\n    d.drop(['respondent_id','health_insurance','employment_industry','employment_occupation', 'census_msa', 'hhs_geo_region', 'education', 'income_poverty'],axis=1,inplace = True)\n    \n    # All the NULL values are populated with the mode\n    str_cols = d.select_dtypes(include = 'object').columns\n\n    ### LabelEcoding all categorical types #####\n    for col in d.columns:\n        if d[col].isnull().sum() and d[col].dtypes != 'object':\n            d[col].loc[(d[col].isnull())] = d[col].median()\n    for col in d.columns:\n        if d[col].isnull().sum() and d[col].dtypes == 'object':\n            d[col].loc[(d[col].isnull())] = d[col].mode().max()\n    \n\n    \n    #One hot encoder\n    d = pd.get_dummies(d,drop_first=False)\n    \n    data = d\n    ### Synthesizing two new features cleanliness level of the individual and opinion of vaccine ####    \n    data['old'] = data['age_group_55 - 64 Years'] + data['age_group_65+ Years']\n   \n    data['cleanliness'] =  data['behavioral_antiviral_meds']+ data['behavioral_avoidance']+\\\n                        data['behavioral_face_mask']+data['behavioral_wash_hands']+\\\n                       data['behavioral_large_gatherings'] + data['behavioral_outside_home']+ data['behavioral_touch_face']\n    data['opinion_h1n1'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']-\\\n                      data['opinion_h1n1_sick_from_vacc'] \n    data['opinion_seasonal'] = data['opinion_seas_vacc_effective']+\\\n                      data['opinion_seas_risk'] - data['opinion_seas_sick_from_vacc']\n    data['good_opinion_vacc'] = np.where(data['opinion_seas_vacc_effective'] >= 3,1,0) # 5 before\n    data['good_knowledge'] = np.where(data['h1n1_knowledge'] == 2,1,0)\n    data['risk'] = np.where(data['opinion_h1n1_risk']>=4,1,0)\n    data['old_recc_h1n1'] = data['age_group_55 - 64 Years'] + data['age_group_65+ Years']+\\\n                  data['doctor_recc_h1n1']\n    data['old_recc_seasonal'] = data['age_group_55 - 64 Years'] + data['age_group_65+ Years']+\\\n                  data['doctor_recc_seasonal']\n    \n    data['risk'] = np.where(data['opinion_h1n1_risk']>=4,1,0)\n    data['old'] = np.where(data['old_recc_h1n1']>=2,1,0)\n\n  \n\n    \n    #data['a^2'] = data['age_group']*data['age_group']\n    \n    ###### Dropping other features #########\n    data.drop([\n        # droping features\n        #'h1n1_knowledge', 'h1n1_concern',\n        \n        #'behavioral_antiviral_meds', 'behavioral_face_mask', 'behavioral_avoidance',\n        #'behavioral_wash_hands','behavioral_large_gatherings', 'behavioral_outside_home',\n        ##'behavioral_touch_face',\n        \n        #'education_< 12 Years', 'income_poverty_> $75,000',\n        \n        'opinion_h1n1_vacc_effective', 'opinion_seas_risk', 'opinion_seas_sick_from_vacc',\n        'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_h1n1_risk',\n        #'age_group'\n        #'sex', 'chronic_med_condition', 'education', 'income_poverty','rent_or_own',\n        #'employment_status','race', 'household_children', 'household_adults', 'child_under_6_months',\n        #'marital_status',\n        \n        #'census_msa', 'hhs_geo_region'   \n        \n         ],axis=1,inplace = True)\n    LE = LabelEncoder()\n    for col in d.columns:\n        data[col] = LE.fit_transform(d[col])\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def transform_test(d):\n    # Löschen der Werte mit zu vielen Null-Stellen und der ID\n    d.drop(['respondent_id','health_insurance','employment_industry','employment_occupation', 'census_msa', 'hhs_geo_region', 'education', 'income_poverty' ],axis=1,inplace = True)\n    \n    # All the NULL values are populated with the mode\n    str_cols = d.select_dtypes(include = 'object').columns\n\n    ### LabelEcoding all categorical types #####\n    for col in d.columns:\n        if d[col].isnull().sum() and d[col].dtypes != 'object':\n            d[col].loc[(d[col].isnull())] = d[col].median()\n    for col in d.columns:\n        if d[col].isnull().sum() and d[col].dtypes == 'object':\n            d[col].loc[(d[col].isnull())] = d[col].mode().max()\n    \n    #LE = LabelEncoder()\n    #for col in d.columns:\n    #    d[col] = LE.fit_transform(d[col])\n    \n    #One hot encoder\n    d = pd.get_dummies(d,drop_first=True)\n    \n    data = d\n    ### Synthesizing two new features cleanliness level of the individual and opinion of vaccine ####\n    data['opinion'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']+\\\n                  data['opinion_h1n1_sick_from_vacc'] + data['opinion_seas_vacc_effective']+\\\n                  data['opinion_seas_risk'] + data['opinion_seas_sick_from_vacc']\n    data['cleanliness'] =  data['behavioral_antiviral_meds']+ data['behavioral_avoidance']+\\\n                        data['behavioral_face_mask']+data['behavioral_wash_hands']+\\\n                       data['behavioral_large_gatherings'] + data['behavioral_outside_home']+\\\n                       data['behavioral_touch_face']\n    data['opinion_h1n1'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']-\\\n                      data['opinion_h1n1_sick_from_vacc'] \n    data['opinion_seasonal'] = data['opinion_seas_vacc_effective']+\\\n                      data['opinion_seas_risk'] - data['opinion_seas_sick_from_vacc']\n\n    data['concern'] = np.where(data['h1n1_concern']>=2,1,0)\n    data['good_opinion_vacc'] = np.where(data['opinion_seas_vacc_effective'] == 3,1,0) # 5 before\n    data['good_knowledge'] = np.where(data['h1n1_knowledge'] == 2,1,0)\n    data['risk'] = np.where(data['opinion_h1n1_risk']>=4,1,0)\n    data['concern_knowledge'] = data['h1n1_concern']+data['h1n1_knowledge']\n    \n    #data['a^2'] = data['age_group']*data['age_group']\n    \n    ###### Dropping other features #########\n    data.drop([\n        # droping features\n        'h1n1_knowledge', 'h1n1_concern',\n        \n        'behavioral_antiviral_meds', 'behavioral_face_mask', 'behavioral_avoidance',\n        'behavioral_wash_hands','behavioral_large_gatherings', 'behavioral_outside_home',\n        'behavioral_touch_face',\n        \n        #'education_< 12 Years', 'income_poverty_> $75,000',\n        \n        'opinion_h1n1_vacc_effective', 'opinion_seas_risk', 'opinion_seas_sick_from_vacc',\n        'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_h1n1_risk'\n        \n        #'sex', 'chronic_med_condition','age_group', 'education', 'income_poverty','rent_or_own',\n        #'employment_status','race', 'household_children', 'household_adults', 'child_under_6_months',\n        #'marital_status',\n        \n        #'census_msa', 'hhs_geo_region'   \n        \n         ],axis=1,inplace = True)\n    return data","metadata":{}},{"cell_type":"markdown","source":"## Import und vorbereiten der Daten","metadata":{}},{"cell_type":"code","source":"#X = pd.read_csv(r'../input/changed-feature-names-flus-hot-prediction/training_set_090421.csv')\n#read data from csv\nX = pd.read_csv(r'../input/flu-shot-learning-h1n1-seasonal-flu-vaccines/training_set_features.csv')\nY = pd.read_csv(r'../input/flu-shot-learning-h1n1-seasonal-flu-vaccines/training_set_labels.csv')\n#test_x_data = pd.read_csv(r'../input/flu-shot-learning-h1n1-seasonal-flu-vaccines/test_set_features.csv')\n# \n# = Y\n#Z.drop('respondent_id',axis = 1)\nframes = [X,Y]\ndata = pd.concat(frames,axis=1)\n#print(data.head())\n\ntrain = transform_test(data)\nlabel = train[['h1n1_vaccine','seasonal_vaccine']]\ntrain.drop(['h1n1_vaccine','seasonal_vaccine'],axis=1,inplace = True)\nframes =[train,label]\ntrain = pd.concat(frames,axis=1)\nfeatures = train.columns[:-2]\nlabels = ['h1n1_vaccine', 'seasonal_vaccine']\nfeature_namen = train.columns.array\n\ntrain,test = train_test_split(train,test_size = 0.2,shuffle = True)\ntrain_X,train_y = train[features],train[labels]\ntest_X,test_y = test[features],test[labels]\ntrain_X.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_categorical_features = (train_X.dtypes == 'category').sum()\nn_numerical_features = (train_X.dtypes == 'float').sum()\nprint(f\"Number of samples: {train_X.shape[0]}\")\nprint(f\"Number of features: {train_X.shape[1]}\")\nprint(f\"Number of categorical features: {n_categorical_features}\")\nprint(f\"Number of numerical features: {n_numerical_features}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Utility Function to print the accuracy of the model ###\ndef find_acc(clf,X,truth,s):\n    pred = clf.predict_proba(X)\n    pred = pred.toarray()\n    accuracy = roc_auc_score(truth,pred)\n    print(s+\" Accuracy is : \",accuracy*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Random Forest Classifier with Randomized Search CV for HPO #####\n\nn_estimators = [100,200,500]\nmax_features = ['auto', 'sqrt']\nmax_depth = [15,20,25]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = XGBClassifier(feature_names = train_X,verbose = False)\netc = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20,\\\n                               cv = 3, verbose=2, random_state=42, n_jobs = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Training & Performance ###\nclf = BinaryRelevance(classifier=etc, require_dense=[True,True])\nclf.fit(train_X,train_y)\nfind_acc(clf,train_X,train_y,'Training')\nfind_acc(clf,test_X,test_y,'Test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nn_estimators = [400,500,600]\nmax_features = ['auto', 'sqrt']\nmax_depth = [5,15,20]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [2, 3, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nRF=RandomForestClassifier()\nrf = RF\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20,\\\n                               cv = 3, verbose=2, random_state=42, n_jobs = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Training & Performance ###\nclf = BinaryRelevance(classifier=rf_random, require_dense=[True,True])\nclf.fit(train_X,train_y)\nfind_acc(clf,train_X,train_y,'Training')\nfind_acc(clf,test_X,test_y,'Test')\nprint_scores(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IML Auswertung des Classifiers","metadata":{}},{"cell_type":"code","source":"print_importance(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_my_trees(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_permutation(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test LogisticRegressionCV\nclf = LogisticRegressionCV(cv=10, random_state=0)\nclf = BinaryRelevance(classifier=clf, require_dense=[True,True])\nclf.fit(train_X,train_y)\nfind_acc(clf,train_X,train_y,'Training')\nfind_acc(clf,test_X,test_y,'Test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_coef(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_permutation(clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}