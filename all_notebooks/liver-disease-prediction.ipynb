{"nbformat_minor":1,"nbformat":4,"metadata":{"language_info":{"mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.1","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"execution_count":null,"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#Input data files are available in the \"../input/\" directory.\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"e27c5cbd68d1608ca13b923bfba47ddfe1f01f6b","_cell_guid":"611056ce-4247-438a-9a8b-a8ed218704cf"},"cell_type":"code"},{"source":"Loadup the dataset and explore it...","metadata":{},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"df = pd.read_csv(\"../input/Indian Liver Patient Dataset (ILPD).csv\")\nprint(df.columns) # gives us the names of the features in the dataset that might help predict if patient has a disease.\ndf.describe()","metadata":{},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"df.info()","metadata":{},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"df['alkphos'].fillna(value=0, inplace=True)","metadata":{"collapsed":true},"cell_type":"code"},{"source":"Let us look at the conditional statistics of the data conditioned on whether someone has a liver disease or not for each of the features for to get a feel for how useful they will be in discriminating if someone has a liver disease.","metadata":{},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"data = df.values\nfeat_names = df.columns\ndf_neg = df.loc[df[feat_names[-1]] == 1]\ndf_neg.describe()","metadata":{"scrolled":true},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"df_pos = df.loc[df[feat_names[-1]] == 2]\ndf_pos.describe()","metadata":{},"cell_type":"code"},{"source":"Now let us see the correlations among different features(test measurements) conditioned on someone having a liver disease or not!","metadata":{},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"sns.heatmap(df_neg.corr())\n","metadata":{},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"sns.heatmap(df_pos.corr())","metadata":{},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"sns.heatmap(df.corr())","metadata":{},"cell_type":"code"},{"source":"Looking at the heatmap of the correlations it seems like some tests seem to have a relatively strong positive correlation to someone having a disease. Lets see how accurately we can predict if someone has liver disease just by looking at each of these tests individually by plotting the class conditional histograms of the data..","metadata":{},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"print(np.array(df_neg.values)[:,2].shape)\nprint(np.array(df_pos.values)[:,2].shape)\nfor i in range(2,len(df.columns)):\n    sns.distplot((np.array(df_neg.values)[:,i]),color='b')\n    sns.distplot((np.array(df_pos.values)[:,i]),color='r')\n    plt.figure()","metadata":{"scrolled":true},"cell_type":"code"},{"source":"As seen from the figure above it will be difficult to guess if someone has the disease by looking at the individual test results themselves... since there is a significant overlap in the distributions indicating that the dataset is not linearly separable on any individual axes.....\n\nBut, will  a combination of these tests help make better  predictions? \nTo simplostically answer this, let us try to fit the class condistional distributions and use them to discriminate the data... A glance at the histgrams is hinting that almost all the tests are having unimodal class conditonal distributions.. so let us fit a multivariate gaussian distribution to samples drawn from each of these classes. And use the Mahalanobis distance to tell if someone has a disease or not.. using all the features (except the gender provided..)","metadata":{},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"neg_meas = np.array(df_neg.values)[:,2:-2].astype('float')\nneg_mean = np.mean(neg_meas, axis=0)\nneg_cov  = np.cov(neg_meas, rowvar=0)\nneg_precision = np.linalg.inv(neg_cov)\n\npos_meas = np.array(df_pos.values)[:,2:-2].astype('float')\npos_mean = np.mean(pos_meas, axis=0)\npos_cov  = np.cov(pos_meas, rowvar=0)\npos_precision = np.linalg.inv(pos_cov)\n\n#for i in range(len)\nTP = 1\nTN = 1\nFP = 1\nFN = 1\nNEG = 0\nPOS = 0\nfor i in range(len(df.values)):\n    meas = np.array(df.values[i])[2:-2]\n    neg_diff = meas - neg_mean\n    neg_dist = np.sqrt(np.dot(np.transpose(neg_diff), np.dot(neg_precision, neg_diff)))\n    pos_diff = meas - pos_mean\n    pos_dist = np.sqrt(np.dot(np.transpose(pos_diff), np.dot(pos_precision, pos_diff)))\n    if((pos_dist/neg_dist) < 1):\n        pred = 2\n    else:\n        pred = 1\n    if(df.values[i][-1] == 1):\n        NEG += 1\n        if(pred == df.values[i][-1]):\n            TN += 1\n        else:\n            FP += 1\n    else:\n        POS += 1\n        if(pred == df.values[i][-1]):\n            TP += 1\n        else:\n            FN += 1\n\nconf_matrix = np.array([[TP,FP],[FN,TN]])\nsns.heatmap(conf_matrix)\nprint(conf_matrix)\nprint(TP+FN+TN+FP)\nprecision = (TP*1.0)/(TP+FP)\nrecall    = (TP*1.0)/(TP+FN)\nF_score  = (2.0*precision*recall)/(precision+recall)\nprint(precision)\nprint(recall)\nprint('F-Score : ' + str(F_score))\n        ","metadata":{},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"def get_fscore(cost):\n    TP = 1\n    TN = 1\n    FP = 1\n    FN = 1\n    NEG = 0\n    POS = 0\n    for i in range(len(df.values)):\n        meas = np.array(df.values[i])[2:-2]\n        neg_diff = meas - neg_mean\n        neg_dist = np.sqrt(np.dot(np.transpose(neg_diff), np.dot(neg_precision, neg_diff)))\n        pos_diff = meas - pos_mean\n        pos_dist = np.sqrt(np.dot(np.transpose(pos_diff), np.dot(pos_precision, pos_diff)))\n        if((pos_dist/neg_dist) < cost):\n            pred = 2\n        else:\n            pred = 1\n        if(df.values[i][-1] == 1):\n            NEG += 1\n            if(pred == df.values[i][-1]):\n                TN += 1\n            else:\n                FP += 1\n        else:\n            POS += 1\n            if(pred == df.values[i][-1]):\n                TP += 1\n            else:\n                FN += 1\n    precision = (TP*1.0)/(TP+FP)\n    recall    = (TP*1.0)/(TP+FN)\n    F_score  = (2.0*precision*recall)/(precision+recall)\n    return F_score\ncost_vec = []\nfscore_vec =[]\nfor i in range(50):\n    cost =  i*0.2\n    cost_vec.append(cost)\n    fscore = get_fscore(cost)\n    fscore_vec.append(fscore)\nplt.plot(np.array(cost_vec), np.array(fscore_vec))\nplt.xlabel('relative cost of error')\nplt.ylabel('F-Score')\n","metadata":{},"cell_type":"code"},{"source":"From this it looks like there is not much significant improvement, even when we use all the measurements linearly, for any relative cost... So let us try some non-linear discriminative models now....","metadata":{},"cell_type":"markdown"},{"source":"","metadata":{},"cell_type":"markdown"},{"source":"","metadata":{},"cell_type":"markdown"}]}