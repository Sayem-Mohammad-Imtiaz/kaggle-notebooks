{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Celebrity Faces DCGAN Starter Code\nThis is starter code for a Deep Convolutional Generative Adversarial Network (DCGAN) to generate celeb faces.  I did heavy code reuse/modification of the following tensorflow turtorial:\nhttps://www.tensorflow.org/beta/tutorials/generative/dcgan\n\nThis tensorflow tutorial also gave me some help:\nhttps://www.tensorflow.org/tutorials/load_data/images#build_a_tfdatadataset\n\nI plan to update this Notebook to improve (1) documentation, (2) performance, and (3) visualization.\n\nFeel free to leave comments for improvements!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings # We'll use this to suppress warnings caused by TensorFlow\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # generating plot\n\nimport tensorflow as tf # modeling/training\ntf.enable_eager_execution() # Must execute this at the beginning of the code\n                            # See https://www.tensorflow.org/guide/eager for details\nimport time # Used for epoch timing\n\nimport imageio # GIF generation\nimport glob # GIF generation\nimport PIL # GIF generation\n\n\nimport os\ndata_dir = '/kaggle/input/celeba-dataset/'\nos.listdir(data_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will not actually use any of the data in the CSV files, only the names of the images (for now)."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_eval_partition = pd.read_csv(data_dir + 'list_eval_partition.csv')\nnames_df = list_eval_partition['image_id']\nnames_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a random sample of our images to see what they look like.  I also took a sample of the image shapes to make sure they are all the same. (I believe they are all [218, 178, 3])."},{"metadata":{"trusted":true},"cell_type":"code","source":"img_names = names_df.sample(n=16).values\nshapes = []\nplt.figure(figsize=(10,10))\nfor i, name in enumerate(img_names):\n    plt.subplot(4, 4, i + 1)\n    img = plt.imread(data_dir + 'img_align_celeba/img_align_celeba/' + name)\n    shapes.append(img.shape)\n    plt.imshow(img)\n    plt.title(name)\n    plt.axis('off')\n_=plt.suptitle('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use this function to load the image, resize it to the size necessary for our models, and then preprocess it (currently only centering and rescaling).  Im resizing based on what I need for the models - it'll become more clear why we are resizing when you see the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_and_preprocess_image(name):\n    image = tf.io.read_file(data_dir + 'img_align_celeba/img_align_celeba/' + name)\n    image = tf.image.decode_jpeg(image)\n    image = tf.image.resize_images(image, (216, 176))\n    image = (image - 127.5) / 127.5\n    return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is an important step to ensure all of our images are represented by a Dataset.  I referenced this Tensorflow tutorial as mentioned above: https://www.tensorflow.org/tutorials/load_data/images#build_a_tfdatadataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\nname_ds = tf.data.Dataset.from_tensor_slices(names_df.values)\nimage_ds = name_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nimage_ds = image_ds.shuffle(buffer_size=2000).batch(BATCH_SIZE)\n\nnum_batches = int(np.ceil(len(names_df) / BATCH_SIZE))\nprint('There are {} batches'.format(num_batches))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the models\nNow we need to build our two models - the generator and the discriminator.  First, the generator will take a random input of length 100 and generate it into a face (after training of course).\n\nI used this tutorial as a baseline: https://www.tensorflow.org/beta/tutorials/generative/dcgan\n\nYou can skip the section below if you feel comfortable with how the model upsamples and why we chose to resize our images."},{"metadata":{},"cell_type":"markdown","source":"### Generator model dimensions\nThe tricky part with the generator model is the dimensions of each step of the model. Remember, we are taking an input of shape (BATCH_SIZE, 100) and upsampling it to an output shape of (BATCH_SIZE, x_output, y_output, 3).  In this case, I made the x_output and y_output dimensions as close to the original image shapes as possible.\n\nIn order to upsample we need to essnetialy scale up our pixels by an integer at each step.  To do this we need to find the prime factors of our x_output and y_output dimensions.  Notice that 218 and 178 only have two prime factors each (2 x 109 and 2 x 89, respecively).  I chose output dimensions of 216 and 176, which factor nicely to primes of 2 x 2 x 2 x 3 x 3 x 3 and 2 x 2 x 2 x 2 x 11, respectively.\n\nNow we can use these prime factors to decide the dimensions of the model at each stage.  The stride lengths of each Conv2DTranspose step multiplies each x_output and y_output dimension.  For example: working backwards in the x_output dimension, we take 216 / 2 = 108 for our last stage, 108 / 3 = 36 for the middle stage, and 36 / 3 = 12 for the first stage."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(12*11*128, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((12, 11, 128)))\n    assert model.output_shape == (None, 12, 11, 128)\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(3, 4), padding='same', use_bias=False))\n    assert model.output_shape == (None, 36, 44, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(3, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 108, 88, 32)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 216, 176, 3)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check to see a summary of our generator model and a randomly generated image.  A couple of things here: First, obviously we haven't yet trained our model, so we are not going to get anything that resembles a face. Second, we need to make sure we reverse the preprocessing that we would have done previously to the input."},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = make_generator_model()\nnoise_image = tf.random.normal([1,100,])\ngenerated_image = generator(noise_image, training=False)\nplt.imshow((generated_image[0]*127.5 +127.5) / 255.)\n_=plt.axis('off')\ngenerator.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The discriminator model is much more straight forward.  We are simply going to be doing binary classification (real or fake).  In this case we are just downsampling from (BATCH_SIZE, x_output, x_output, 3) to a single number between 0 (fake) and 1 (real)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[216, 176, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make sure our discriminator model gives us that single number given our randomly generated image above."},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = make_discriminator_model()\ndiscriminator.summary()\nprint(discriminator(generated_image))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss functions\nNow it's time to take a step back and think about what the two models are really trying to accomplish.  Remember, as stated above, the output of our discriminator is a number between zero (fake image) and one (real image).\n\nThe discriminator model is trying to classify all of the fake images that the generator makes as fake and all of the true images as real.  Therefore, it's loss function will look at the cross-entropy between group of ones and the real output AND a group of zeros and the fake output.\n\nThe generator model is trying to make a fake image that looks like a real one.  Therefore, it's loss function will look at the cross-entropy between a group of ones and the fake output."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = loss_obj(tf.ones_like(real_output), real_output)\n    fake_loss = loss_obj(tf.zeros_like(fake_output), fake_output)\n    return real_loss + fake_loss\n\ndef generator_loss(fake_output):\n    return loss_obj(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll simply use a Adam optimizer for each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_optimizer = tf.keras.optimizers.Adam(1e-4)\ndisc_optimizer = tf.keras.optimizers.Adam(1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\nHere's where the rubber meets the road.  We need to define what happens with each batch of images on a training step.  The train_step() function performs the following steps:\n1. Make a vector of shape (BATCH_SIZE, 100) of random numbers\n2. Generate a fake image based on the random vector\n3. Predict whether or not the real images are real and the fake images are fake\n4. Compute the loss for each model (see above)\n5. Compute the gradients of the loss with respect to the trainable variables of each model\n6. Update our trainable variables based on the gradient\n\nWe will return the calculated loss for each model in order to visualize the training process"},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, 100])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        \n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        \n        disc_loss = discriminator_loss(real_output, fake_output)\n        gen_loss = generator_loss(fake_output)\n        \n    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    \n    disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    \n    return gen_loss, disc_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function will help us visualize and save a group of generated images to visualize how our model is doing.  Again, remember we'll have to 'undo' the preprocessing that we did at the beginning."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_and_generate_images(model, epoch, test_output):\n    predictions = model(test_output, training=False)\n    \n    plt.figure(figsize=(10,10))\n    for i in range(len(test_output)):\n        plt.subplot(4,4,i+1)\n        plt.imshow((predictions[i] * 127.5 +127.5) / 255.)\n        plt.axis('off')\n        \n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    _=plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting it all together, for each epoch, we will itterate over the dataset performing a training step for each batch.  The history DataFrame is used for post training analysis of the average loss of each model at each epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(dataset, epochs):\n    print('Begining to train...')\n    \n    history = pd.DataFrame(['gen_loss', 'disc_loss'])\n    for epoch in range(epochs):\n        start = time.time()\n        epoch_gen_loss = tf.keras.metrics.Mean()\n        epoch_disc_loss = tf.keras.metrics.Mean()\n        for i, images in enumerate(dataset):\n            gen_loss, disc_loss = train_step(images)\n            epoch_gen_loss.update_state(gen_loss)\n            epoch_disc_loss.update_state(disc_loss)\n\n        show_and_generate_images(generator, epoch + 1, seed)\n        stats = 'Epoch {0} took {1} seconds. Gen_loss: {2:0.3f}, Disc_loss: {3:0.3f}'\n        print(stats.format(epoch + 1, int(time.time() - start), \n                           epoch_gen_loss.result().numpy(), \n                           epoch_disc_loss.result().numpy()))\n        history = history.append({'gen_loss': epoch_gen_loss.result().numpy(), \n                                  'disc_loss': epoch_disc_loss.result().numpy()}, \n                                  ignore_index=True)\n        \n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the models!\nWe use a 'seed' tensor to show the same images over training time (we'll use it in a nice GIF after training).\nFor our history DataFrame, we increment the index to start at epoch 1.\n\n**WARNING: this code takes ~6 hours to run.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 32\nseed = tf.random.normal([16, 100])\nhistory = train(image_ds, EPOCHS)\nhistory.index = history.index + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Post-training analysis\nTo visualize the training process, we plot the loss of each model.  This should be a prety intersting graphic, because it shows how well the discriminator model is discriminating versus how well the generator model is generating."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.axes(xlabel='epoch', ylabel='loss')\nhistory.plot(ax=ax, figsize=(10,7))\n_=plt.title('Loss History')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we re-use some code from the Tensorflow DCGAN to show the GIF of our random seed over training steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"anim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  last = -1\n  for i,filename in enumerate(filenames):\n    frame = 2*(i**0.5)\n    if round(frame) > round(last):\n      last = frame\n    else:\n      continue\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n  IPython.display.Image(filename=anim_file)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}