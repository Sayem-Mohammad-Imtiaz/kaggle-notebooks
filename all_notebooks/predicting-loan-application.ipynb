{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-21T10:58:10.380629Z","iopub.execute_input":"2021-07-21T10:58:10.381105Z","iopub.status.idle":"2021-07-21T10:58:10.388318Z","shell.execute_reply.started":"2021-07-21T10:58:10.381071Z","shell.execute_reply":"2021-07-21T10:58:10.387556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Loan Application\nThis code below developement version of code that credit to [vipin kumar](https://www.kaggle.com/vipin20/loan-prediction-problem) who gave me idea to improvise the code. Several additional data preprocessing is updated to improve the quality of data in order to reach better quality of prediction, accuracy, recall and the score. \nThe objective of the data analyisis is finding best model that give better prediction for loans status whether will be approved(yes) or rejected(No) based on criteria of applicant(Gender, Status (married, Dependent), Education, Jobs(employee or self employed), Income(Total Income), Loan Amount,Loan Term Credit History, and Property Area. \nThe analysis will be divided to several steps\n1. Data Overview\n2. Data Cleaning & Transform\n   * 2.1 Drop unecessary columns\n   * 2.2 Transform 'Total_Income' data from Object into Float\n   * 2.3 Review the Data Statistik to Understand the Distribution\n   * 2.4  Checking of Missing Data\n   * 2.5 Checking the Skewness and Kurtosis of Data\n3. Build Model Select the suitable Model\n   * 3.1 Dividing Dataframe\n   * 3.2 Splitting the Data of x and y \n   * 3.3 Testing Several Model\n   * 3.4 Chosen Model, Threshold and Scores\n\nWe are going to use data of [Loan Application Data](http://https://www.kaggle.com/vipin20/loan-application-data).\nBefore we start making the code and start data processing, we import necessary module to our script.","metadata":{}},{"cell_type":"code","source":"#import module\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm\nimport warnings as wr\nwr.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:02.750876Z","iopub.execute_input":"2021-07-21T11:13:02.751362Z","iopub.status.idle":"2021-07-21T11:13:02.758251Z","shell.execute_reply.started":"2021-07-21T11:13:02.751329Z","shell.execute_reply":"2021-07-21T11:13:02.757224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Overview\nWe will load dataset and tryhing to review the data. The number or rows and columns, the atributes, the quality of data and make prelimenery judgement what to do for the next steps.","metadata":{}},{"cell_type":"code","source":"# load data and overview the dataset\nloans = pd.read_csv('../input/loan-application-data/df1_loan.csv')\nloans.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:16.952786Z","iopub.execute_input":"2021-07-21T11:13:16.953264Z","iopub.status.idle":"2021-07-21T11:13:16.983875Z","shell.execute_reply.started":"2021-07-21T11:13:16.953229Z","shell.execute_reply":"2021-07-21T11:13:16.982824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loans.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:12.878261Z","iopub.execute_input":"2021-07-21T11:13:12.878595Z","iopub.status.idle":"2021-07-21T11:13:12.883374Z","shell.execute_reply.started":"2021-07-21T11:13:12.878567Z","shell.execute_reply":"2021-07-21T11:13:12.882709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loans.dtypes\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:20.023848Z","iopub.execute_input":"2021-07-21T11:13:20.02437Z","iopub.status.idle":"2021-07-21T11:13:20.032869Z","shell.execute_reply.started":"2021-07-21T11:13:20.024323Z","shell.execute_reply":"2021-07-21T11:13:20.03204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the dataset has 15 columns and 500 row of data. 6 numerical columns (float and integer), and 9 object. We found Total_Income columns identified as \"object(string)\" ( We need to transform this data later to numerical later in the proces.)\n\n\n## 2. DATA CLEANING\nAfter reviewing the data, We have and idea what the data about and the quality of data, to ensure the data integrity, we will started process data cleaning.\n### 2.1 Drop unecessary columns \n'Unnamed:0' and 'Loan_ID' is not important for our data analyisis and we need to drop the data. ","metadata":{}},{"cell_type":"code","source":"#Drop Unnecessary columns\nloans = loans.drop(['Unnamed: 0', 'Loan_ID'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:24.795614Z","iopub.execute_input":"2021-07-21T11:13:24.796123Z","iopub.status.idle":"2021-07-21T11:13:24.80279Z","shell.execute_reply.started":"2021-07-21T11:13:24.796076Z","shell.execute_reply":"2021-07-21T11:13:24.801857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Transform 'Total_Income' data from Object into Float\nTotal_Income is important keys for the analysis. We need to transform 'Total_Income' into numeric data so we can process the data for further process","metadata":{}},{"cell_type":"code","source":"# Remove $ in data 'Total_Income'\nloans['Total_Income'] = loans['Total_Income'].str.replace('$',' ')\nloans.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:30.742512Z","iopub.execute_input":"2021-07-21T11:13:30.74284Z","iopub.status.idle":"2021-07-21T11:13:30.763625Z","shell.execute_reply.started":"2021-07-21T11:13:30.742811Z","shell.execute_reply":"2021-07-21T11:13:30.762249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the 'Total Income' Data type from Objet into Float\nloans['Total_Income'] = loans['Total_Income'].astype(float)\nloans.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:33.633384Z","iopub.execute_input":"2021-07-21T11:13:33.633745Z","iopub.status.idle":"2021-07-21T11:13:33.642736Z","shell.execute_reply.started":"2021-07-21T11:13:33.633711Z","shell.execute_reply":"2021-07-21T11:13:33.641757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loans.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:38.803905Z","iopub.execute_input":"2021-07-21T11:13:38.804241Z","iopub.status.idle":"2021-07-21T11:13:38.80892Z","shell.execute_reply.started":"2021-07-21T11:13:38.80421Z","shell.execute_reply":"2021-07-21T11:13:38.808283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Review the Data Statistik to Understand the Distribution\nKnowing data distribution is important step for data analysis. We need to avoid data that can skew our data set. this is can be reached by view the data statistic describtion and boxplot\n\n#### 2.3.1 Dataset Statistic Description","metadata":{}},{"cell_type":"code","source":"# Average, Percentile,Median, Maximum and Minimum Data\nloans.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:41.373885Z","iopub.execute_input":"2021-07-21T11:13:41.374403Z","iopub.status.idle":"2021-07-21T11:13:41.400385Z","shell.execute_reply.started":"2021-07-21T11:13:41.374359Z","shell.execute_reply":"2021-07-21T11:13:41.399691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.2 Boxplot\n","metadata":{}},{"cell_type":"code","source":"# Boxplot\nboxplot = loans.boxplot(column=['ApplicantIncome', 'CoapplicantIncome', 'Total_Income' ])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:43.703096Z","iopub.execute_input":"2021-07-21T11:13:43.703454Z","iopub.status.idle":"2021-07-21T11:13:43.878553Z","shell.execute_reply.started":"2021-07-21T11:13:43.703408Z","shell.execute_reply":"2021-07-21T11:13:43.877484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From Statistic Describtion and Box plot, we will focus on \"Total_Income\". We can use both data to remove outliers that can skew the dataset and analysis result. We will find out the maximum outside range and will filter out data from outside range.\nFormula:\nThreshold_Max_range = Q75 - 1.5 *(Q75-Q25) = 7495.25 + 1.5*(7495.25-4166) \n","metadata":{}},{"cell_type":"code","source":"Threshold_Max_Total_Income = 7495.25 + 1.5*(7495.25-4166)\nloans1 =loans[loans['Total_Income']< Threshold_Max_Total_Income]\nboxplot = loans1.boxplot(column=['ApplicantIncome', 'CoapplicantIncome', 'Total_Income' ])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:46.323485Z","iopub.execute_input":"2021-07-21T11:13:46.323854Z","iopub.status.idle":"2021-07-21T11:13:46.492669Z","shell.execute_reply.started":"2021-07-21T11:13:46.323819Z","shell.execute_reply":"2021-07-21T11:13:46.491593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Total rows and columns\nloans1.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:50.512859Z","iopub.execute_input":"2021-07-21T11:13:50.513235Z","iopub.status.idle":"2021-07-21T11:13:50.519614Z","shell.execute_reply.started":"2021-07-21T11:13:50.513201Z","shell.execute_reply":"2021-07-21T11:13:50.518651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"New Box plot shown about 41 rows of data that outliers removed from the dataset. We will reflect the data distribution on histogram below.\n\n#### 2.3.3 Histogram Graphic","metadata":{}},{"cell_type":"code","source":"#histogram Graphic\nplt.figure(figsize=(10,7))\nplt.hist(loans1['Total_Income'], bins=20, align='right', color='blue', edgecolor='black')\nplt.ylabel(\"frequency\")\nplt.xlabel(\"Total Income\")\nplt.title(' Histogram of Total Income ')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:53.413306Z","iopub.execute_input":"2021-07-21T11:13:53.4137Z","iopub.status.idle":"2021-07-21T11:13:53.610243Z","shell.execute_reply.started":"2021-07-21T11:13:53.413661Z","shell.execute_reply":"2021-07-21T11:13:53.609047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4  Checking of Missing Data\nIn order to enhance data integrity, we will find missing data and fill the data. We use average value to fill the numerical data and generate string for object data.","metadata":{}},{"cell_type":"code","source":"# Checking the missing data\nloans1.isnull().sum().sort_values(ascending=False) ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:57.162737Z","iopub.execute_input":"2021-07-21T11:13:57.163349Z","iopub.status.idle":"2021-07-21T11:13:57.171815Z","shell.execute_reply.started":"2021-07-21T11:13:57.163311Z","shell.execute_reply":"2021-07-21T11:13:57.171089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking The missing data Percentage From total rows of each atributes\ntotal_null = loans1.isnull().sum().sort_values(ascending=False) \ncount = loans1.isnull().count().sort_values(ascending=False) #total data of rows #500\npercentage = (loans1.isnull().sum()/loans1.isnull().count()).round(2).sort_values(ascending=False)*100 #First sum and order all null values for each variabl\nmissing_data = pd.concat([total_null, percentage], axis=1, keys=['Total', 'Percentage'])\nmissing_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:13:59.43789Z","iopub.execute_input":"2021-07-21T11:13:59.438474Z","iopub.status.idle":"2021-07-21T11:13:59.458988Z","shell.execute_reply.started":"2021-07-21T11:13:59.438436Z","shell.execute_reply":"2021-07-21T11:13:59.458303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The missing data maximum is 8.5 % from 459 observation which mean it is considering low. Generating data with this amount consider safe without skewing the data.\nThe Missing data will be generated using existing data.\n\n#### 2.4.1 Filling numerical missing values\n","metadata":{}},{"cell_type":"code","source":"#Finding numeric column\nnum_column = loans1._get_numeric_data().columns.tolist()\n#finding category column\ncat_column = set(loans1.columns)-set(num_column)\n\n#Filling numerical missing values\nfor col in num_column:\n    loans1[col].fillna(loans1[col].mean(), inplace=True)\n# Filling string missing values\nfor col in cat_column:\n    loans1[col].fillna(loans1[col].mode()[0],inplace=True)  \n# Verification if there null numbers \nloans1.isnull().sum() ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:02.122348Z","iopub.execute_input":"2021-07-21T11:14:02.122722Z","iopub.status.idle":"2021-07-21T11:14:02.143829Z","shell.execute_reply.started":"2021-07-21T11:14:02.122687Z","shell.execute_reply":"2021-07-21T11:14:02.142534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Finding the Duplicate Data\nDuplicate data can skew the analysis. We will check if there any duplicate data exist in Dataset","metadata":{}},{"cell_type":"code","source":"loans1.duplicated().sum() \n# Data shown no duplicate","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:07.623047Z","iopub.execute_input":"2021-07-21T11:14:07.623404Z","iopub.status.idle":"2021-07-21T11:14:07.631799Z","shell.execute_reply.started":"2021-07-21T11:14:07.623368Z","shell.execute_reply":"2021-07-21T11:14:07.6311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Checking the Skewness and Kurtosis of Data\nSkewness is id symetrical distribution and Kurtosis is heaviness distribution\nThe guidance for Skewness:\n 1. Data is fairly symmetrical in range -0.5 and 0.5\n 2. Moderate Skewness in between -1 and -0.5 or 0.5 and 1\n 3. Highly Skewness is under -1 and above 1\nTHe guidance of Kurtosis:\n 1. Leptokurtik, Distribution is tall and thin (K>3)\n 2. Plotikurtik, Distribution is flat and value is spread out (K <3)\n 2. Mesokurtik, Distribution is in between ( K =3)\n","metadata":{}},{"cell_type":"code","source":"#  the \"Total_Income\" Skewness and Kurtosis\nsns.distplot(loans1['Total_Income'])\nprint(\"Skewness coeff. is: %f\" % loans1['Total_Income'].skew().round(2)) # 0.934\nprint(\"Kurtosis coeff. is: %f\" % loans1['Total_Income'].kurt().round(2)) ","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:16.054159Z","iopub.execute_input":"2021-07-21T11:14:16.054838Z","iopub.status.idle":"2021-07-21T11:14:16.4532Z","shell.execute_reply.started":"2021-07-21T11:14:16.054791Z","shell.execute_reply":"2021-07-21T11:14:16.452502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution graphic show the data is moderate skewness and the data moderatly spreadout(Platykurtik). It mean data in in good range distribution\n\nAfter data Cleaning and Transform, we can start build model","metadata":{}},{"cell_type":"markdown","source":"## 3. Build Model \nTo build model, we need to import necessery module for the program\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import roc_curve, auc,roc_auc_score\nfrom sklearn.preprocessing import binarize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:26.674649Z","iopub.execute_input":"2021-07-21T11:14:26.675131Z","iopub.status.idle":"2021-07-21T11:14:26.680023Z","shell.execute_reply.started":"2021-07-21T11:14:26.675097Z","shell.execute_reply":"2021-07-21T11:14:26.679302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Dividing Dataframe\nWe divide dataframe into x and y to build relation model. for x, we use all attributes except for 'Loan_Status'. for y, we use the 'Loan_Status'. String data in dataframe willbe transform into binary format (1 and 0). for Loan_Status/y, which was previously contain string 'Yes' and 'No', will be transform into 1 and 0.","metadata":{}},{"cell_type":"code","source":"#making two variable dataframe x, y\nx=loans1.drop(['Loan_Status'], axis=1)\ny=loans1['Loan_Status']\n# Transform the x data into binary\nx=pd.get_dummies(x)\nx.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:28.95255Z","iopub.execute_input":"2021-07-21T11:14:28.953053Z","iopub.status.idle":"2021-07-21T11:14:28.988207Z","shell.execute_reply.started":"2021-07-21T11:14:28.953005Z","shell.execute_reply":"2021-07-21T11:14:28.987273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Binarizer, transform string to binary value \nlb =LabelBinarizer()\ny=lb.fit_transform(y)\nprint(y[:5])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:31.382953Z","iopub.execute_input":"2021-07-21T11:14:31.383455Z","iopub.status.idle":"2021-07-21T11:14:31.392727Z","shell.execute_reply.started":"2021-07-21T11:14:31.383401Z","shell.execute_reply":"2021-07-21T11:14:31.39154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Splitting the Data of x and y \nWe will split the data into Train and Test. Will take 30% of data set for the test. The data will split into x_train, y_train, x_test, y_test. The Train data are the datasets used to build the model and the test data are data used to testing the model","metadata":{}},{"cell_type":"code","source":"#spliting the data\nx_train,x_test, y_train, y_test = train_test_split(x,y, test_size =0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:35.924265Z","iopub.execute_input":"2021-07-21T11:14:35.924647Z","iopub.status.idle":"2021-07-21T11:14:35.932605Z","shell.execute_reply.started":"2021-07-21T11:14:35.924611Z","shell.execute_reply":"2021-07-21T11:14:35.931267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Testing Several Model\nWe will find suitable model from 5 models that gives more accurate prediction. The Model we use is Decision Tree Classifier, Gaussian NB, K neighbors Classifier, and Random Forest Classifier.","metadata":{}},{"cell_type":"code","source":"# Decision Tree Classifier\ndtf = DecisionTreeClassifier()\ndtf.fit(x_train, y_train)\n\n# Gaussian NB\nn_b = GaussianNB()\nn_b.fit(x_train, y_train)\n\n# K Neighbors Classifier\nknn = KNeighborsClassifier()  \nknn.fit(x_train, y_train)\n\n# Random Forest Classifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:38.43393Z","iopub.execute_input":"2021-07-21T11:14:38.434292Z","iopub.status.idle":"2021-07-21T11:14:38.612708Z","shell.execute_reply.started":"2021-07-21T11:14:38.434258Z","shell.execute_reply":"2021-07-21T11:14:38.61165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We test the accuracy of model using data test in the model.","metadata":{}},{"cell_type":"code","source":"# least model\nprint(\"Decision Tree Classifier Score:\",dtf.score(x_test, y_test).round(2))\nprint(\"Gaussian NB Score:\",n_b.score(x_test, y_test).round(2)) \nprint(\"K Neighbors Classifier Score:\",knn.score(x_test, y_test).round(2))\nprint(\"Random Forest Classifier Score:\",rfc.score(x_test, y_test).round(2))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:40.957704Z","iopub.execute_input":"2021-07-21T11:14:40.958016Z","iopub.status.idle":"2021-07-21T11:14:40.992645Z","shell.execute_reply.started":"2021-07-21T11:14:40.957985Z","shell.execute_reply":"2021-07-21T11:14:40.991622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest Classifier Show promising accuracy Score in 0.82. However, to improve the accuracy of the model, we will analyise model using confusing matrix, the Receiver operating characteristic (ROC) Curve and find the threshold. \n\n#### 3.3.1 Confusing Matrix\n With confusing Matrix, we will find True Positive Value (TP), True Negative Value (TN), False Positif Value (FP), and False Negative Value (FN).","metadata":{}},{"cell_type":"code","source":"# y predict and Confusing Matrix\ny_predict=rfc.predict(x_test)\nCM = confusion_matrix(y_test, y_predict)\nprint(CM) # TP = 89, TN = 20, FP=23, FN = 6\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:43.6731Z","iopub.execute_input":"2021-07-21T11:14:43.673412Z","iopub.status.idle":"2021-07-21T11:14:43.695729Z","shell.execute_reply.started":"2021-07-21T11:14:43.673384Z","shell.execute_reply":"2021-07-21T11:14:43.694559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.2 Receiver Operating Charaacteristing (ROC)\nROC is visual represent how well the classification model work. We will define True Positif Rate (TPR), False Positive Rate (FPR), and Threshold.","metadata":{}},{"cell_type":"code","source":"rfc.predict_proba(x_test[0:10])\n# roc_curve, auc,roc_auc_score \nfpr,tpr,threshold = roc_curve(y_test, rfc.predict_proba(x_test)[:,1] )\nroc_auc=roc_auc_score(y_test,rfc.predict(x_test))\n#graphic plot\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)'%roc_auc )\nplt.plot([0,1],[0,1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.1])\nplt.ylim([0.0,1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:46.203517Z","iopub.execute_input":"2021-07-21T11:14:46.203863Z","iopub.status.idle":"2021-07-21T11:14:46.390368Z","shell.execute_reply.started":"2021-07-21T11:14:46.203834Z","shell.execute_reply":"2021-07-21T11:14:46.389268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3.3 Finding Threshold\nFinding Threshold between 0 to 1 is very qualitative approach. When the Threshold high, The data will be more precision (less false positive) Since we are handling loans approval data, which mean we want to people have chance to get loans. Threshold value is depend on the need. For this case, we will generate threshold dummy and get several output of Accuracy, Precision, Recall, and f1 scores. ","metadata":{}},{"cell_type":"code","source":"y_predict_proba=rfc.predict_proba(x_test)\nthreshold_dummy = [0,0.2,0.4,0.6,0.8,1]\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nfor i in threshold_dummy:\n y_pred_class = binarize(y_predict_proba,i)\n y_pred_class1 = y_pred_class[:,1].astype(int)  \n accuracy_scores.append(accuracy_score(y_test, y_pred_class1))\n precision_scores.append(precision_score(y_test, y_pred_class1))\n recall_scores.append(recall_score(y_test, y_pred_class1))\n f1_scores.append(f1_score(y_test, y_pred_class1))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:50.692673Z","iopub.execute_input":"2021-07-21T11:14:50.692987Z","iopub.status.idle":"2021-07-21T11:14:50.730117Z","shell.execute_reply.started":"2021-07-21T11:14:50.692957Z","shell.execute_reply":"2021-07-21T11:14:50.729236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas import DataFrame\nTL = DataFrame([threshold_dummy, accuracy_scores,precision_scores, recall_scores, f1_scores]).transpose().round(2)\nTL.columns =['threshold', 'accuracy_scores','precision_scores', 'recall_scores', 'f1_scores']\nTL.head(6)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:53.182175Z","iopub.execute_input":"2021-07-21T11:14:53.182687Z","iopub.status.idle":"2021-07-21T11:14:53.202275Z","shell.execute_reply.started":"2021-07-21T11:14:53.182647Z","shell.execute_reply":"2021-07-21T11:14:53.201167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#graphic threshold vs Score\nplt.plot(TL['threshold'],TL['accuracy_scores'] , color='darkorange', lw=lw, label='accuracy ' )\nplt.plot(TL['threshold'],TL['precision_scores'] , color='red', lw=lw, label='precision ')\nplt.plot(TL['threshold'],TL['recall_scores'] , color='blue', lw=lw, label='recall')\nplt.plot(TL['threshold'],TL['f1_scores'] , color='yellow', lw=lw, label='f1_scores ')\nplt.plot([0.4,0.4],[0,1], color='black', lw=lw,label='threshold =0.4', linestyle='--')\nplt.xlim([0.0, 1.1])\nplt.ylim([0.0,1.1])\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.title('Threshold vs Score')\nplt.legend(loc=\"lower left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:55.615242Z","iopub.execute_input":"2021-07-21T11:14:55.615577Z","iopub.status.idle":"2021-07-21T11:14:55.778718Z","shell.execute_reply.started":"2021-07-21T11:14:55.615546Z","shell.execute_reply":"2021-07-21T11:14:55.777562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We found optimal score about 0.4 where the scores show optimal numbers using random forest classifier.\n\n### 3.4 Chosen Model, Threshold and Scores\nto summarise, we use Random forest classifier because it show better accuracy compare than other model. The optimal threshold at 0.4. the model using to make prediction and scores","metadata":{}},{"cell_type":"code","source":"y_predict_proba=rfc.predict_proba(x_test)\ny_pred_class=binarize(y_predict_proba,0.4)\ny_pred_class2=y_pred_class[:,1].astype(int) \n# confusing matrix\nresults = confusion_matrix(y_test, y_pred_class2) \nprint('Confusion Matrix :', results)\nprint('Data Value :')\nprint('True Positif Value (TP):', results[1,1])\nprint('True Negative Value (TN):', results[0,0])\nprint('False Positive Value (FP):', results[0,1])\nprint('False Negative Value (FN):', results[1,0])","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:14:59.082969Z","iopub.execute_input":"2021-07-21T11:14:59.083304Z","iopub.status.idle":"2021-07-21T11:14:59.106876Z","shell.execute_reply.started":"2021-07-21T11:14:59.083274Z","shell.execute_reply":"2021-07-21T11:14:59.105856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"accuracy:\", accuracy_score(y_test, y_pred_class2).round(2))\nprint(\"precision:\", precision_score(y_test, y_pred_class2).round(2))\nprint(\"recall:\", recall_score(y_test, y_pred_class2).round(2))\nprint(\"f1 score:\", f1_score(y_test, y_pred_class2).round(2))","metadata":{"execution":{"iopub.status.busy":"2021-07-21T11:15:02.611823Z","iopub.execute_input":"2021-07-21T11:15:02.612206Z","iopub.status.idle":"2021-07-21T11:15:02.626788Z","shell.execute_reply.started":"2021-07-21T11:15:02.612173Z","shell.execute_reply":"2021-07-21T11:15:02.624661Z"},"trusted":true},"execution_count":null,"outputs":[]}]}