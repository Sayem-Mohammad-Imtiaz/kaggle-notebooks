{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport seaborn as sns\n\nfrom IPython.display import display","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3357331201eecd9419809bdb913b86884bae847"},"cell_type":"markdown","source":"### Principal Component Analysis \n\nWhy do PCA?\n\n - Principal components analysis is an unsupervised learning method where we do not work with labels to study data, i.e., only explanatory features **X** are considered.\n  - Unlike supervised learning, e.g. regression analysis which has a response y to predict, the goal of unsupervised learning is to determine a relationship between features **X** and response y.   \n- Principal components are directions of vectors in which the data set vary the most in terms of the **X** features space.\n- PCA is useful as a method to reduce the dimension of features when there exist many features and we are unsure which features are important.\n  - In this sense, PCA can be viewed as a method for exploratory data analysis because we can compute PCs and visualize the vectors which have the highest variance in feature space. "},{"metadata":{"_uuid":"5b405985df053d20c52dedcbffc71226f2d3bd20"},"cell_type":"markdown","source":"### Import Lahman Baseball Data and Select Features\n  - omit dependent features (e.g. earned runs, earned run average) and common features (e.g. games played)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def lahman_year_team(inputdf,yr='2012',droplist=['yearID','G','Ghome','W','L','Rank']):\n    '''\n    function which obtains numeric-only data from Lahman Teams data set and selects year\n    inputdf    : Lahman Teams.csv, DataFrame\n    yr         : year to select, int\n    droplist   : static features to omit from returned data, list of str\n    returns    : numeric Teams features with team_id as index, DataFrame\n    '''\n    df = inputdf.copy()\n    # select numeric data only for PCA\n    numericdf = df.select_dtypes(exclude=['object'])\n    # assign team_ID as index\n    numericdf.set_index(df['teamID'].values,inplace=True)\n    # filter by year\n    numericdf = numericdf[numericdf.yearID==yr]\n    # drop constant features, where value is dependent or does not vary by player/team performance\n    numericdf.drop(droplist,axis=1,inplace=True)\n    print('Lahman numeric feature team results {}:'.format(yr))\n    return numericdf \n\nteamsdf = pd.read_csv('../input/Teams.csv')\ndroplist=['yearID','G','Ghome','W','L','Rank']\nteams12 = lahman_year_team(teamsdf,yr=2012,droplist=droplist)    \n# teams12.columns\nteams12.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c113e21522d1b27dd00c10740652d04de44c653"},"cell_type":"markdown","source":"### PCA with numpy\n- step 1: center and scale the features matrix\n- step 2: compute covariance matrix of centered features\n- step 3: compute tuple eigenvalues, eigenvectors using np.linalg.eig(X)\n  - eigenvectors are the principal component loading vectors, which are the directions of highest variation within the feature space X\n- step 4: compute the Principal Components by projecting the centered X features onto the PC loading eigenvectors\n  - PC_Matrix = LoadingVector dotproduct centeredFeatures "},{"metadata":{"trusted":true,"_uuid":"f45282a4e10ab2afae574fd200f542b0a9775557"},"cell_type":"code","source":"def center_scale(X):\n    '''\n    returns : X features centered by column mean and scaled by column std, df\n    '''\n    return (X-np.mean(X))/np.std(X)\n\ndef pca(inputdf):\n    '''\n    function which computes largest variance directions (loading vectors) and principal components (score vectors)\n    inputdf    : features to compute variance explained\n    returns    : loading vectors, score vectors as PCs, variance explained as eigenvals    \n    '''\n    df = inputdf.copy()\n    # step 1: center/scale the features\n    C = center_scale(df)\n    print('Shape of centered features matrix = {}'.format(C.shape))\n    # step 2: compute cov of tranpsose of centered features\n    cov = np.cov(C.T)\n    print('shape of covariance matrix = {}'.format(cov.shape))\n    # step 3: compute the PC loading vectors (direction of largest variacne in features space)\n    eigvals,eigvecs = np.linalg.eig(cov)\n    print('shape of eigenvalues, eigenvectors = {}, {}'.format(eigvals.shape,eigvecs.shape))\n    loadingheaders = ['L'+str(i) for i in range(1,len(df.columns)+1)]\n    # eigvecs are loadings \n    loadingdf = pd.DataFrame(eigvecs,columns=loadingheaders,index=df.columns).astype(float)\n    print('shape of loadings df = {}'.format(loadingdf.shape))\n    print('Top 5 PC loading vectors (direction of largest variation in feature-space):')\n    display(loadingdf.loc[:,:'L5'])\n    # step 4: compute score vectors as Principal Components (where scores are features C projected onto loading vectors)\n    scorematrix = loadingdf.values.T.dot(C.T)\n    scoreheaders = ['PC'+str(i) for i in range(1,len(C.columns)+1)]\n    scoredf = pd.DataFrame(scorematrix.T,index=C.index,columns=scoreheaders)\n    display(scoredf.head())\n    return loadingdf,scoredf,eigvals\n\n\nloadingdf,scoredf,eigvals = pca(teams12)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc607feaae4ba6c79f6566c3b232a4b2b03012d8"},"cell_type":"markdown","source":"### Plot Percent Variance Explained\n\n- Each loading vector (eigenvector) and principal component (score vector) has a corresponding eigenvalue, which equals the sample variance of each directional score vector   \n- The cumulative PVE indicates that approximately 90% of the variation of the features is explained by the first 10 principal components "},{"metadata":{"trusted":true,"_uuid":"2380f6954af4003bc931eab6b6e894a246e2bcc0"},"cell_type":"code","source":"def pve(eigvals):\n    '''\n    function which computes percent variance explained (PVE), cumulative PVE of all PCs\n    inputdf     : numeric features X with named indices, DataFrame\n    eigvals     : eigenvalues resulting from principal components analyis, are the corresponding variance explained of ea. PC\n    '''\n    with plt.style.context('seaborn-white'):\n        fig,ax = plt.subplots(figsize=(14,8))\n        var_total = eigvals.sum()\n        # compute proportional variance explained per PC\n        pve = eigvals/var_total\n        # compute cum. variance explained per PC\n        cumpve = np.cumsum(pve)\n        x = [i for i in range(1,len(eigvals)+1)]\n        ax.set_xticks(x)\n        ax.plot(x,pve,label='PVE')\n        ax.plot(x,cumpve,label='PVE_cumulative')\n        ax.set(title='Percent Variance Explained by Principal Components',\n              xlabel='PC',ylabel='Variance Explained')\n        # ref lines\n        hlinecolor='0.74'\n        ax.axhline(y=eigvals[0]/eigvals.sum(),linestyle='dotted',color=hlinecolor)\n        ax.axhline(y=0,linestyle='dotted',color=hlinecolor)\n        ax.axhline(y=1,linestyle='dotted',color=hlinecolor)\n        ax.legend(loc='best')\npve(eigvals)\n\nnp.cumsum(eigvals/eigvals.sum())\n(eigvals/eigvals.sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fab928c0598126d34a6c42ecd03c5df4c220a44c"},"cell_type":"markdown","source":"## Biplot to Visualize First Two PCs Explaining Largest Variation of Data \n - loading vectors reveal directions with largest variation in feature space\n - score vectors (features X projected onto loading direction vectors) reveal features in correspondence with vector directions with highest variance\n "},{"metadata":{"trusted":true,"_uuid":"628200bc41f907c8ea7df0856757a29526328be3"},"cell_type":"code","source":"def lg_ranks(inputdf,year):\n    '''\n    function which displays team end of season results\n    inputdf    : Lahman database Teams.csv, DataFrame\n    year       : year to filter, int\n    '''\n    df = inputdf.copy()\n    algrp = df[(df.yearID==year)&(df.lgID=='AL')].groupby(['teamID','lgID','divID','W','L'],as_index=False).agg({'Rank':'last'}).sort_values(['Rank','lgID','divID'])\n    nlgrp = df[(df.yearID==year)&(df.lgID=='NL')].groupby(['teamID','lgID','divID','W','L'],as_index=False).agg({'Rank':'last'}).sort_values(['Rank','lgID','divID'])\n    print('{} Final MLB Team Standings:'.format(year))\n    return algrp,nlgrp\n\ndef biplot(loadingdf,scoredf,loading_color,score_color,score_axlim=7.5,load_axlim=7.5,load_arrows=4):\n    '''\n    function which computes biplot of PC scores, loadings\n    scoredf    : matrix of PC score vectors, used tp display how indices are projected onto PC loading vectors, DataFrame\n    loadingdf  : matrix of PC loading vectors from centered, std'd features, used to show actual direction of PC1 and PC2 2D vectors, DataFrame\n    _color     : matplotlib line colors for corresponding loading vectors, score projection points, str\n    '''\n    with plt.style.context('seaborn-white'):\n        f = plt.figure(figsize=(14,14))\n        ax0 = plt.subplot(111)\n        # plot the first two score vectors, as annotations, of teamID indices (PC1,PC2 are orhogonal to ea. other)\n        for teamid in scoredf.index:  \n            ax0.annotate(teamid,(scoredf['PC1'][teamid],-scoredf['PC2'][teamid]),ha='center',color=score_color)\n        score_axlim = score_axlim\n        ax0.set(xlim=(-score_axlim,score_axlim),ylim=(-score_axlim,score_axlim),\n               )\n        ax0.set_xlabel('Principal Component 1',color=score_color)\n        ax0.set_ylabel('Principal Component 2',color=score_color)\n        # add reference lines through origin\n        ax0.hlines(y=0,xmin=-score_axlim,xmax=score_axlim,linestyle='dotted',color='grey')\n        ax0.vlines(x=0,ymin=-score_axlim,ymax=score_axlim,linestyle='dotted',color='grey')\n        # plot PC1 and PC2 loadings (two directions in features space with largest variation) as reference vectors\n        ax1 = ax0.twinx().twiny()\n        ax1.set(xlim=(-load_axlim,load_axlim), ylim=(-load_axlim,load_axlim),\n               )\n        ax1.tick_params(axis='y',color='red')\n        ax1.set_xlabel('Principal Component Loading Weights',color=loading_color)\n        # plot first two PC loading vectors (as loadingdf.index annotations)\n        offset_scalar=1.175\n        for feature in loadingdf.index: \n            ax1.annotate(feature,(loadingdf['L1'].loc[feature]*offset_scalar,-loadingdf['L2'].loc[feature]*offset_scalar),color=loading_color)\n        # display first fourPCs as arrows\n        for i in range(0,load_arrows):\n            ax1.arrow(x=0,y=0,dx=loadingdf['L1'][i],dy=-loadingdf['L2'][i],head_width=0.0075,shape='full')\nbiplot(loadingdf,scoredf,loading_color='red',score_color='blue',score_axlim=8.5,load_axlim=.6,load_arrows=len(loadingdf.columns))        \nALrankdf,NLrankdf = lg_ranks(teamsdf,2012)\ndisplay(ALrankdf)\ndisplay(NLrankdf)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1c1625c74190701ecc93bcae583afb24872e3c6"},"cell_type":"markdown","source":"### Biplot Interpretation\n\n- Based on the year-end rankings and the X projections onto the biplot, the teams with better (worse) rankings were projected toward the right (left) sides of the biplot.\n\n- The first principal component (x-axis, blue text) for winning teams places the largest weights on: \n  - SV (relief pitcher game saves) \n  - SOA (strikeouts by pitchers),\n  - SHO (shutouts, i.e. no opponent hits),\n  - IPOuts (Outs pitched, innings pitched x 3) \n    - i.e. winning teams had good pitching statistics.\n- The first loading vector for losing teams places the largest weights on:\n  - ERA (earned run average)\n  - ER (earned runs allowed),\n  - RA (runs allowed),\n     - i.e., losing teams had poor pitching statistics.  \n- The second Principal component vector (y-axis, blue text) with the highest weights are:\n  - Runs (number of points scored),\n  - H (hits)\n  - 2B (doubles),\n   \n- The team indices (blue text) of the scores matrix reveal:\n  - The Cincinnatti Reds (CIN) and Washington Nationals did well SV (saves) and SOA (strikeouts by pitchers)\n  - Instead of PC1, the New York Yankees (NYA) did well with PC2 attributes HR (home runs) and R (runs) and that Steinbrenner did well with stadium attendance.   \n  - The Oakland Athletics' (OAK), San Francisco Giants' (SFN), and Atlanta Braves' (ATL) outderperformance coincided with SHO (pitcher shutouts) and BB (walks by batters).\n\nConclusion:\n  - There is evidence that the majority of first place teams in 2012 focused on good defense (pitching) over strong offense (hitting). \n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}