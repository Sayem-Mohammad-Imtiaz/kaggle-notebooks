{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport collections\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Graph analysis is not a new branch of data science, yet is not the usual “go-to” method data scientists apply today. However there are some crazy things graphs can do. Classic use cases range from fraud detection, to recommendations, or social network analysis. A non-classic use case in NLP deals with topic extraction (graph-of-words)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame({'ID':[1,2,3,4,5,6], \n                   'First Name':['Felix', 'Jean', 'James', 'Daphne', 'James', 'Peter'], \n                   'Family Name': ['Revert', 'Durand', 'Wright', 'Hull', 'Conrad', 'Donovan'],\n                   'Phone number': ['+33 6 12 34 56 78', '+33 7 00 00 00 00', '+33 6 12 34 56 78', '+33 6 99 99 99 99', '+852 0123 4567', '+852 0123 4567'],\n                   'Email': ['felix.revert@gmail.com', 'jean.durand@gmail.com', 'j.custom@gmail.com', pd.np.nan, 'j.custom@gmail.com', pd.np.nan]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Consider a fake profilers use case --> You have a database of clients, and would like to know how they are connected to each other. Especially, you know some clients are involved in complex fraud structure, but visualizing the data at an individual level does not bring out evidence of fraud. The fraudsters look like other usual clients.","execution_count":null},{"metadata":{},"cell_type":"raw","source":"Example: three individuals with the same phone numbers, connected to other individuals with the same email addresses is unusual and potentially risky. The value of the phone number does not give any information in itself (therefore even the best deep learning model would not capture any value out of it), but the fact that individuals are connected through the same values of phone numbers, or email addresses can be synonym of risk.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"column_edge = 'Phone number'\ncolumn_ID = 'ID'\n\ndata_to_merge = df[[column_ID, column_edge]].dropna(subset=[column_edge]).drop_duplicates() # select columns, remove NaN\n\n# To create connections between people who have the same number,\n# join data with itself on the 'ID' column.\ndata_to_merge = data_to_merge.merge(\n    data_to_merge[[column_ID, column_edge]].rename(columns={column_ID:column_ID+\"_2\"}), \n    on=column_edge\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_to_merge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Alright, we have some connections here, but 2 issues:\na) individuals are connected with themselves\nb) When X is connected with Y, then Y is also connected with X, and we have two rows for the same connection\nLet’s clean that up:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# By joining the data with itself, people will have a connection with themselves.\n# Remove self connections, to keep only connected people who are different.\nd = data_to_merge[~(data_to_merge[column_ID]==data_to_merge[column_ID+\"_2\"])] \\\n    .dropna()[[column_ID, column_ID+\"_2\", column_edge]]\n    \n# To avoid counting twice the connections (person 1 connected to person 2 and person 2 connected to person 1)\n# we force the first ID to be \"lower\" then ID_2\nd.drop(d.loc[d[column_ID+\"_2\"]<d[column_ID]].index.tolist(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import networkx as nx\n\nG = nx.from_pandas_edgelist(df=d, source=column_ID, target=column_ID+'_2', edge_attr=column_edge)\n\nG.add_nodes_from(nodes_for_adding=df.ID.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nx.draw(G)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"G.nodes()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"G.edges()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Trying with Email","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\ncolumn_edge = 'Email'\ncolumn_ID = 'ID'\n\ndata_to_merge = df[[column_ID, column_edge]].dropna(subset=[column_edge]).drop_duplicates()\n\ndata_to_merge = data_to_merge.merge(\n    data_to_merge[[column_ID, column_edge]].rename(columns={column_ID:column_ID+\"_2\"}), \n    on=column_edge\n)\n\nd = data_to_merge[~(data_to_merge[column_ID]==data_to_merge[column_ID+\"_2\"])] \\\n    .dropna()[[column_ID, column_ID+\"_2\", column_edge]]\n\nd.drop(d.loc[d[column_ID+\"_2\"]<d[column_ID]].index.tolist(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# Create the connections in the graph\nlinks_attributes = {tuple(row[[column_ID, column_ID+\"_2\"]]): {column_edge: row[column_edge]} for i,row in d.iterrows()}\n\nG.add_edges_from(links_attributes) # create the connection, without attribute. Check it with G.get_edge_data(3,5)\nnx.set_edge_attributes(G=G, values=links_attributes) # adds the attribute. Check it with G.get_edge_data(3,5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Graph analysis","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nnx.draw(G)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"{row[column_ID]:row['First Name'] +' '+ row['Family Name'] for i,row in df.iterrows()}\nnx.set_node_attributes(G, {row[column_ID]:{'Name': row['First Name'] +' '+ row['Family Name']} for i,row in df.iterrows()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nEDGE_SIZE = {\n    'Phone number': 2,\n    'Email': 1,\n}\n\nEDGE_COLOR = {\n    'Phone number': 'purple',\n    'Email': 'red',\n}\n\ndef clean_edge(edge):\n    s.edge[edge[0], edge[1]].values()\n\ndef c_(list_edges): return [a for a in list_edges if a in list(EDGE_COLOR.keys())]\n    \n# For nx.Graph()\ndef edge_sizes(s): return [EDGE_SIZE[c_(list(s.edges[edge[0], edge[1]].keys()))[-1]] for edge in s.edges()] # /!\\ multiple links => one size\ndef edge_colors(s): return [EDGE_COLOR[c_(list(s.edges[edge[0], edge[1]].keys()))[-1]] for edge in s.edges()] # /!\\ multiple links => one color\n\n# For nx.MultiDiGraph()\n# def edge_sizes(s): return [EDGE_SIZE[s.edge[edge[0]][edge[1]][0]['label']] for edge in s.edges()] # /!\\ multiple links => one size\n# def edge_colors(s): return [EDGE_COLOR[s.edge[edge[0]][edge[1]][0]['label']] for edge in s.edges()] # /!\\ multiple links => one color\n\ndef draw(s):\n    pos = nx.spring_layout(s, scale=0.5)\n    node_labels = dict((n,d['Name']) for n,d in s.nodes(data=True))\n#     labels = {**node_labels, **edge_labels}\n    nx.draw(s, pos=pos, width=edge_sizes(s), edge_color=edge_colors(s), alpha=0.8, arrows=False, node_color='lightgrey', node_size=400,\n            labels=node_labels, \n            font_color='black', font_size=8, font_weight='bold',\n           )\n    edge_labels = dict(((u,v),list(d.values())[0]) for u,v,d in G.edges(data=True))\n    nx.draw_networkx_edge_labels(G, pos, edge_labels = edge_labels, font_size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"draw(nx.ego_graph(G=G, n=1, radius=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysing text similarity using spaCy, networkX\n    spaCy - an open source NLP library, \n    word vectors, and\n    networkX - an open source network (graph) analysis and visualisation library. \nThe notebook is partly a reminder for myself on just how (well) these techniques work, but I hope that others find it useful. I'll continue to update it with more techniques over the coming weeks. If you have any suggestions, feel free to make them in the comments, fork the notebook etc. I'm keen to exchange tips and tricks. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport spacy\nimport networkx as nx                        # a really useful network analysis library\nimport matplotlib.pyplot as plt\n# from networkx.algorithms import community   # not used, yet... \nimport datetime                              # access to %%time, for timing individual notebook cells\nimport os\nimport en_core_web_sm\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nlp = en_core_web_sm.load()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"## Twitter data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rowlimit = 500              # this limits the tweets to a manageable number\ndata = pd.read_csv(r'..\\ExtractedTweets.csv', nrows = rowlimit)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tokens = []\nlemma = []\npos = []\nparsed_doc = [] \ncol_to_parse = 'Tweet'\n\nfor doc in nlp.pipe(data[col_to_parse].astype('unicode').values, batch_size=50,n_threads=3):\n    \n    if doc.is_parsed:\n        \n        parsed_doc.append(doc)\n        tokens.append([n.text for n in doc])\n        lemma.append([n.lemma_ for n in doc])\n        pos.append([n.pos_ for n in doc])\n    else:\n        # We want to make sure that the lists of parsed results have the\n        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n        tokens.append(None)\n        lemma.append(None)\n        pos.append(None)\n\n\ndata['parsed_doc'] = parsed_doc\ndata['comment_tokens'] = tokens\ndata['comment_lemma'] = lemma\ndata['pos_pos'] = pos","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Basic checks of the parsed data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.Tweet[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing stopwords","execution_count":null},{"metadata":{},"cell_type":"raw","source":"We could reduce increase the signal:noise ratio in these texts by removing some of the more common words (or stopwords). By removing these from the tweets, we would prevent them from influencing the analysis of whether two tweets are similar. I'm not addressing this is the notebook yet, but I will come back to it later. For now, let's just look at what words are included in spaCy's stopword list.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"stop_words = spacy.lang.en.stop_words.STOP_WORDS\nprint('Number of stopwords: %d' % len(stop_words))\nprint(list(stop_words))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing spaCy's similarity function","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(data['parsed_doc'][0].similarity(data['parsed_doc'][1]))\nprint(data['parsed_doc'][0].similarity(data['parsed_doc'][10]))\nprint(data['parsed_doc'][1].similarity(data['parsed_doc'][10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### If you've limited the rows imported, then you may only have Democrat tweets (which occur first in the list).","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.Party.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"world_data = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# takes 1s for 500 nodes - but of course this won't scale linearly!                              \nraw_G = nx.Graph() # undirected\nn = 0\n\nfor i in world_data['parsed_doc']:        # sure, it's inefficient, but it will do\n    for j in world_data['parsed_doc']:\n        if i != j:\n            if not (raw_G.has_edge(j, i)):\n                sim = i.similarity(j)\n                raw_G.add_edge(i, j, weight = sim)\n                n = n + 1\n\nprint(raw_G.number_of_nodes(), \"nodes, and\", raw_G.number_of_edges(), \"edges created.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"edges_to_kill = []\nmin_wt = 0.94      # this is our cutoff value for a minimum edge-weight \n\nfor n, nbrs in raw_G.adj.items():\n    #print(\"\\nProcessing origin-node:\", n, \"... \")\n    for nbr, eattr in nbrs.items():\n        # remove edges below a certain weight\n        data = eattr['weight']\n        if data < min_wt: \n            # print('(%.3f)' % (data))  \n            # print('(%d, %d, %.3f)' % (n, nbr, data))  \n            #print(\"\\nNode: \", n, \"\\n <-\", data, \"-> \", \"\\nNeighbour: \", nbr)\n            edges_to_kill.append((n, nbr)) \n            \nprint(\"\\n\", len(edges_to_kill) / 2, \"edges to kill (of\", raw_G.number_of_edges(), \"), before de-duplicating\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for u, v in edges_to_kill:\n    if raw_G.has_edge(u, v):   # catches (e.g.) those edges where we've removed them using reverse ... (v, u)\n        raw_G.remove_edge(u, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"strong_G = raw_G\nprint(strong_G.number_of_edges())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"strong_G.remove_nodes_from(list(nx.isolates(strong_G)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from math import sqrt\ncount = strong_G.number_of_nodes()\nequilibrium = 10 / sqrt(count)    # default for this is 1/sqrt(n), but this will 'blow out' the layout for better visibility\npos = nx.fruchterman_reingold_layout(strong_G, k=equilibrium, iterations=300)\nnx.draw(strong_G, pos=pos, node_size=10, edge_color='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Observe in the visualization that if the texts have certain similarities or not.And secondly play with the threshold =.94 to see how it behaves.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [16, 9]  # a better aspect ratio for labelled nodes\n\nnx.draw(strong_G, pos, font_size=3, node_size=50, edge_color='gray', with_labels=False)\nfor p in pos:  # raise positions of the labels, relative to the nodes\n    pos[p][1] -= 0.03\nnx.draw_networkx_labels(strong_G, pos, font_size=8, font_color='k')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster. \n\nNow the data set which we are going to import contains -\n\na) The text of a tweet\nb) A keyword from that tweet (although this may be blank!)\nc) The location the tweet was sent from (may also be blank)\n\nColumns\nid - a unique identifier for each tweet\ntext - the text of the tweet\nlocation - the location the tweet was sent from (may be blank)\nkeyword - a particular keyword from the tweet (may be blank)\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train=pd.read_csv(r\"..\\Twitter Prediction\\train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are two applications you can try using graph:\n\nOne is if you split this data set and just use column \"text\" of tweet can you predict the target just like a classifier by computing similarities\n\nSecondly there are certain missing values in columns :Keyword\" and \"location\" is it possible if you can compute the similarities and the one where \"text\" column shows hhigher similarities can you fill the missing values with same values which have higher similarity and filled entries in columns \"Keyword\" and \"location\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = []\nlemma = []\npos = []\nparsed_doc = [] \ncol_to_parse = 'text'\n\nfor doc in nlp.pipe(train[col_to_parse].astype('unicode').values, batch_size=50,\n                        n_threads=3):\n    if doc.is_parsed:\n        parsed_doc.append(doc)\n        tokens.append([n.text for n in doc])\n        lemma.append([n.lemma_ for n in doc])\n        pos.append([n.pos_ for n in doc])\n    else:\n        # We want to make sure that the lists of parsed results have the\n        # same number of entries of the original trainframe, so add some blanks in case the parse fails\n        tokens.append(None)\n        lemma.append(None)\n        pos.append(None)\n\n\ntrain['parsed_doc'] = parsed_doc\ntrain['comment_tokens'] = tokens\ntrain['comment_lemma'] = lemma\ntrain['pos_pos'] = pos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_train = train[0:500]\nraw_G = nx.Graph() # undirected\nn = 0\n\nfor i in world_train['parsed_doc']:        # sure, it's inefficient, but it will do\n    for j in world_train['parsed_doc']:\n        if i != j:\n            if not (raw_G.has_edge(j, i)):\n                sim = i.similarity(j)\n                raw_G.add_edge(i, j, weight = sim)\n                n = n + 1\n\nprint(raw_G.number_of_nodes(), \"nodes, and\", raw_G.number_of_edges(), \"edges created.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"edges_to_kill = []\nmin_wt = 0.94      # this is our cutoff value for a minimum edge-weight \n\nfor n, nbrs in raw_G.adj.items():\n    #print(\"\\nProcessing origin-node:\", n, \"... \")\n    for nbr, eattr in nbrs.items():\n        # remove edges below a certain weight\n        data = eattr['weight']\n        if data < min_wt: \n            # print('(%.3f)' % (data))  \n            # print('(%d, %d, %.3f)' % (n, nbr, data))  \n            #print(\"\\nNode: \", n, \"\\n <-\", data, \"-> \", \"\\nNeighbour: \", nbr)\n            edges_to_kill.append((n, nbr)) \n            \nprint(\"\\n\", len(edges_to_kill) / 2, \"edges to kill (of\", raw_G.number_of_edges(), \"), before de-duplicating\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for u, v in edges_to_kill:\n    if raw_G.has_edge(u, v):   # catches (e.g.) those edges where we've removed them using reverse ... (v, u)\n        raw_G.remove_edge(u, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"strong_G = raw_G\nprint(strong_G.number_of_edges())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"strong_G.remove_nodes_from(list(nx.isolates(strong_G)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from math import sqrt\ncount = strong_G.number_of_nodes()\nequilibrium = 10 / sqrt(count)    # default for this is 1/sqrt(n), but this will 'blow out' the layout for better visibility\npos = nx.fruchterman_reingold_layout(strong_G, k=equilibrium, iterations=300)\nnx.draw(strong_G, pos=pos, node_size=10, edge_color='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [16, 9]  # a better aspect ratio for labelled nodes\n\nnx.draw(strong_G, pos, font_size=3, node_size=50, edge_color='gray', with_labels=False)\nfor p in pos:  # raise positions of the labels, relative to the nodes\n    pos[p][1] -= 0.03\nnx.draw_networkx_labels(strong_G, pos, font_size=8, font_color='k')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Now try to solve the different applications i mentioned above [it's just for a prespective to understand]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Do give an UPVOTE if you liked it **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}