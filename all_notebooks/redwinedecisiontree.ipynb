{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the info, we can see that there are 1599 non-null values, with data types both float64 and int64.\nLet us check if there are any null  values present in them, (there is no need but just to be sure).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, now there is no need to to put any random value. Now, it's time to visualize the data. For data visualization, we would be using seaborn and matplotlib.pyplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will first using pairplot.\nA pairplot plot a pairwise relationships in a dataset. The pairplot function creates a grid of Axes such that each variable in data will by shared in the y-axis across a single row and in the x-axis across a single column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, a heatmap to find co-relations between the features\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=dataset.corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr,cmap=colormap,xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that the following pair are highly co-related to each other.\n> citric acid and fixed acidity \n<br>\n> density and fixed acidity\n<br>\n> free sulphur dioxide and total slphur dioxide ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We, now convert the quality (i.e, target ) into two categories, as we would be using Decision Tree Classifier here, \nas mentioned in the tips, that if the quality is greater tha 6.5 , it's \"good\"(1) else \"bad\"(0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['quality'] = dataset.quality.apply(lambda x : 1 if x > 6.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data = dataset, x = 'quality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After visualization, the first thing to do is to separate the features from the target variable and then split it into training and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=dataset.drop('quality',1)\ny=dataset['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train,y_test= train_test_split(X,y,test_size=0.30, random_state=37)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After splitting, let's apply our first decision tree model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_base=DecisionTreeClassifier(max_depth=10,random_state=4)\ndt_base.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find the accuracy of our first model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=dt_base.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = metrics.accuracy_score(y_test,y_pred)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our model is 88.5 % accurate. Let's visualize the tree\n<br>\nTrees can be visualized with the help of function plot_tree. So, let's start.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree.plot_tree(dt_base, max_depth=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the size is too small for us to read. I am currently searching for a method and will update it as soon as I'll find","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It's time for HYPERPARAMETER TUNING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_base.tree_.node_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 179 nodes are present, so we can set the range approx 200 to see the result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'max_depth' : range(4,20,4),\n    'min_samples_leaf' : range(20,200,20),\n    'min_samples_split' : range(20,200,20),\n    'criterion' : ['gini','entropy'] \n}\nn_folds = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using gridSearchCV to train models under different hyperparameters and get the result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=34)\ngrid = GridSearchCV(dt, param_grid, cv = n_folds, return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After training, let's find the best parameters that is suited","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_tree = grid.best_estimator_\nbest_tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, finding the accuracy of this best_tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_tree.fit(X_train,y_train)\ny_pred_best = best_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = metrics.accuracy_score(y_test,y_pred_best)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is not much effect in tuning the hyperparameters.  It might be possible that the model may have overfit the data because we can see from the count graph that majority have the target value 0. Thus, to get more accurate result, we could get more data to work on it","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}