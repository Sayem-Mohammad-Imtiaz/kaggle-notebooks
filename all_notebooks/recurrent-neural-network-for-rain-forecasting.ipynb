{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Recurrent Neural Network Model for Rain Forecasting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook builds a model to predict whether or not it will rain tomorrow in Australia using real-world weather data using Recurrent Neural Network with PyTorch. It starts by preprocessing then converting the data to tensors, then building the neural network model with pytorch, then using a loss function and an optimiser to train the model and finally evaluating the model. The dataset contains daily weather observations from numerous Australian weather stations.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First step is to import the necessary libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom torch import nn, optim\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport torch.nn.functional as func\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid')\nsns.set_palette('deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the dataset\ndf = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show first few records\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show dataset dimensions\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show dataset summary\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show the frequency distribution of RainTomorrow\ndf['RainTomorrow'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show percentage\ndf['RainTomorrow'].value_counts()/len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that out of the total number of RainTomorrow values, No RainTomorrow appears 77.58% times and RainTomorrow appears only 22.42% times. Now let's check for missing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have lots of missing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are two ways to deal with missing values, either by deleting incomplete variables if there are too many data missing or by replacing these missing values with estimated value based on the other information available. So as a rule, any column with more than 2,000 missing value will be excluded as they having more missing values that rest of the variables in the dataset. Then before replaceing missing values of other columns with mean, it's wise to first check for outliers as the mean is greatly affected by outliers and works better if the data is normally-distributed while median imputation is preferable for skewed distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = ['Temp9am', 'MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'WindSpeed9am']\ndf[numerical].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show the statistical properties of numerical variables to check for skewed variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On closer inspection, we can see that the Temp9am, MinTemp, MaxTemp and Rainfall columns seem to have a relatively normal distribution, whilst Humidity9am and WindSpeed9am columns have outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing values of normally-distributed columns with mean and skewed distribution with median\ndf['Temp9am'] = df['Temp9am'].fillna(value = df['Temp9am'].mean())\ndf['MinTemp'] = df['MinTemp'].fillna(value = df['MinTemp'].mean())\ndf['MaxTemp'] = df['MaxTemp'].fillna(value = df['MaxTemp'].mean())\ndf['Rainfall'] = df['Rainfall'].fillna(value = df['Rainfall'].mean())\ndf['Humidity9am'] = df['Humidity9am'].fillna(value = df['Humidity9am'].median())\ndf['WindSpeed9am'] = df['WindSpeed9am'].fillna(value = df['WindSpeed9am'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step is to impute missing categorical variables with most frequent value or mode.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['RainToday'] = df['RainToday'].fillna(value = df['RainToday'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is well known that categorical data doesn't work with machine learning and deep learning algorithms, so we gonna encode 'Date', 'Location', 'RainToday' and 'RainTomorrow' columns so we can predict whether or not is going to rain tomorrow?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert data variable into dattime type\ndf['Date'] = df['Date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract year from the date\ndf['Year'] = df['Date'].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract month from the date\ndf['Month'] = df['Date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract day from the date\ndf['Day'] = df['Date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode location\nle = preprocessing.LabelEncoder()\ndf['Location'] = le.fit_transform(df['Location'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode RainToday & RainTomorrow\ndf['RainToday'].replace({'No': 0, 'Yes': 1}, inplace = True)\ndf['RainTomorrow'].replace({'No': 0, 'Yes': 1}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have only 9 columns out of 24 after removing variable with many missing data to predict whether or not is gonna rain tomorrow?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['Temp9am', 'MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'WindSpeed9am', 'RainToday', 'Location', 'Year', 'Month', 'Day']]\ny = df[['RainTomorrow']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final step is to split the data into train and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then convert all of it to Tensors (so we can use it with PyTorch).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = torch.from_numpy(X_train.to_numpy()).float()\ny_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = torch.from_numpy(X_test.to_numpy()).float()\ny_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Neural Network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We gonna create an input layer from our 11 columns: 'Temp9am', 'MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'WindSpeed9am', 'RainToday', 'Location', 'Year', 'Month' and 'Day'. Then the output will be a number between 0 and 1, representing how likely the model thinks it is gonna rain tomorrow. The prediction will be given out by the final layer of the network. We will add 4 hidden layers between the input and output layers. The parameters of those layer will decide the final output. All layers will be fully-connected. One easy way to build the neural network is to create a class that inherits from torch.nn.Module.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nclass Model(nn.Module):\n  def __init__(self, n_features):\n    super(Model, self).__init__()\n    self.fc1 = nn.Linear(n_features, 11)\n    self.fc2 = nn.Linear(11, 8)\n    self.fc3 = nn.Linear(8, 5)\n    self.fc4 = nn.Linear(5, 3)\n    self.fc5 = nn.Linear(3, 1)\n  def forward(self, x):\n    x = func.relu(self.fc1(x))\n    x = func.relu(self.fc2(x))\n    x = func.relu(self.fc3(x))\n    x = func.relu(self.fc4(x))\n    return torch.sigmoid(self.fc5(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(X_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start by creating the layers of our model in the constructor. The forward() method is where the magic happens. It accepts the input x and allows it to flow through each layer. There is a corresponding backward pass (defined by pytorch) that allows the model to learn from the errors that is currently making.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With the model in place, we need to find parameters that predict will it rain tomorrow. First, we need something to tell us how good we are currently doing:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The BCELoss is a loss function that measures the difference between two binary vectors. In our case, the predictions of our model and the real values. It expects the values to be outputed by the sigmoid function. The closer this value gets to 0, the better the model performs.\n\nBut how do we find parameters that minimize the loss function?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Optimisation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Optimisers are used to change the attributes of the neural network such as weights and learning rate in order to reduce the losses. We gonna use Adam optimiser.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimiser = optim.Adam(model.parameters(), lr = 0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naturally, the optimiser requires the parameters. The second argument lr is learning rate. It is a tradeoff between how good parameters we gonna find and how fast we will get there. Finding good values for this can be black magic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Check for GPU","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Doing massively parallel computations on GPUs is one of the enablers for modern deep learning. We will need nVIDIA GPU to transfer all the computation to it. First we will check whether or not a CUDA is available. Then we gonna transfer all training and test sets to whether GPU or CPU. Finally, we move our model and loss function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nX_train = X_train.to(device)\ny_train = y_train.to(device)\n\nX_test = X_test.to(device)\ny_test = y_test.to(device)\n\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the loss function to compare the output with the target\ncriterion = criterion.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rain Forecasting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Having a loss function is great, but tracking the accuracy of our model is something easier to understand, for us mere mortals. Here is the definition for our accuracy:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_accuracy(y_true, y_pred):\n  predicted = y_pred.ge(.5).view(-1)\n  return (y_true == predicted).sum().float() / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We convert every value below 0.5 to 0. Otherwise, we set it to 1. Finally, we calculate the percentage of correct values. With all the pieces of the puzzle in place, we can start training our model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def round_tensor(t, decimal_places = 3):\n  return round(t.item(), decimal_places)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the model\nfor epoch in range(1000):\n    y_pred = model(X_train)\n    y_pred = torch.squeeze(y_pred)\n    train_loss = criterion(y_pred, y_train)\n    if epoch % 100 == 0:\n      train_acc = calculate_accuracy(y_train, y_pred)\n      y_test_pred = model(X_test)\n      y_test_pred = torch.squeeze(y_test_pred)\n      test_loss = criterion(y_test_pred, y_test)\n      test_acc = calculate_accuracy(y_test, y_test_pred)\n      print (str('epoch ') + str(epoch) + str(' Train set: loss: ') + str(round_tensor(train_loss)) + str(', accuracy: ') + str(round_tensor(train_acc)) + str(' Test  set: loss: ') + str(round_tensor(test_loss)) + str(', accuracy: ') + str(round_tensor(test_acc)))\n    optimiser.zero_grad()\n    train_loss.backward()\n    optimiser.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"During the training, we show our model the data for 1,000 times. Each time we measure the loss, propagate the errors trough our model and asking the optimiser to find better parameters.\n\nThe zero_grad() method clears up the accumulated gradients, which the optimiser uses to find better parameters.\n\nWell, using just accuracy wouldn't be a good way to do it. Recall that our data contains mostly no rain examples! Another way to delve a bit deeper into our model performance is to assess the precision and recall for each class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['No rain', 'Raining']\n\ny_pred = model(X_test)\ny_pred = y_pred.ge(.5).view(-1).cpu()\ny_test = y_test.cpu()\nprint(classification_report(y_test, y_pred, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that our model is doing good when it comes to the No rain class. We have so many examples. Unfortunately, we can't really trust predictions of the Raining class. One of the best things about binary classification is that we can have a good look at a simple confusion matrix:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_test, y_pred)\ndf_conf_mat = pd.DataFrame(conf_mat, index = classes, columns = classes)\nheat_map = sns.heatmap(df_conf_mat, annot = True, fmt = 'd')\nheat_map.yaxis.set_ticklabels(heat_map.yaxis.get_ticklabels(), ha = 'right')\nheat_map.xaxis.set_ticklabels(heat_map.xaxis.get_ticklabels(), ha = 'right')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}