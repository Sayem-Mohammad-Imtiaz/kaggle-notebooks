{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Taxi Demand Prediction (NYC Taxi) (Regression Problem)\n\n## Story\n\nIn this notebook we can see that how can we generate a column from a dataset according to our problem need and than create a machine learning models to train our dataset\n\n\n### About Dataset\n\nI took the dataset from New-York Governemnt site i.e (https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and data is of 2015 and 2016 (jan, feb, mar). This dataset will be used for many purposes like predict Total fare of the trip and many more but we use it for the taxi demand prediction. The cool thing is their is no column in the csv for demand of that taxi in specefic area, we will try to do some experiments, to create it and make a machine learning model on that dataset. Hope you guys like the work \n\n\n### Things to learn\n\n1. Feature Engineering\n2. How to handle large Dataset (csv of 1.5GB)\n3. Machine Learning Techniques\n4. Regression\n\n## Cautions\n\nI take less data from csv as kaggle not allow me to use more than 16GB of ram. If you take all of the data than you will get 97% accuracy on validation data and 92% accuracy on test data","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nimport gpxpy.geo # Get the haversine distance\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom prettytable import PrettyTable","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:09.364202Z","iopub.execute_input":"2021-08-18T18:00:09.364589Z","iopub.status.idle":"2021-08-18T18:00:09.37049Z","shell.execute_reply.started":"2021-08-18T18:00:09.364552Z","shell.execute_reply":"2021-08-18T18:00:09.36977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align=\"center\">Loading Data</h1> \n\nLoading Data into pandas DataFrame their are alot of column names into the csv but we will only take the specefic 1\n\n**Colums We will use:**\n1. tpep_pickup_datetime    : Pick up Datetime\n2. tpep_dropoff_datetime   : Drop Off Datetime\n3. trip_distance           : Distance of Trip\n4. pickup_longitude        : PickUp longitude\n5. pickup_latitude         : Pickup Latitude\n6. dropoff_longitude       : Dropoff Longitude\n7. dropoff_latitude        : Dropoff Latitude\n8. total_amount            : Total Fare amount","metadata":{}},{"cell_type":"code","source":"\nbase_path = \"../input/taxidemandfarepredictiondataset/\"\n# we speed the process by decreasing the dimensionality\ncolumns=['tpep_pickup_datetime',\n           'tpep_dropoff_datetime',\n           'trip_distance',\n           'pickup_longitude',\n           'pickup_latitude',\n           'dropoff_longitude',\n           'dropoff_latitude',\n           'total_amount']\n\n\n\n\ndf_2015_1 = pd.read_csv(f'{base_path}yellow_tripdata_2015-01.csv', usecols=columns, nrows=1000000)\ndf_2015_2 = pd.read_csv(f'{base_path}yellow_tripdata_2015-02.csv', usecols=columns, nrows=1000000)\ndf_2015_3 = pd.read_csv(f'{base_path}yellow_tripdata_2015-03.csv', usecols=columns, nrows=1000000)\n\ndf_2016_1 = pd.read_csv(f'{base_path}yellow_tripdata_2016-01.csv', usecols=columns, nrows=1000000)\ndf_2016_2 = pd.read_csv(f'{base_path}yellow_tripdata_2016-02.csv', usecols=columns, nrows=1000000)\ndf_2016_3 = pd.read_csv(f'{base_path}yellow_tripdata_2016-03.csv', usecols=columns, nrows=1000000)\n\ndf_2015 = df_2015_1.append(df_2015_2).append(df_2015_3)\ndf_2016 = df_2016_1.append(df_2016_2).append(df_2016_3)\n\noriginal_2015_len = df_2015.shape[0]\noriginal_2016_len = df_2016.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:14:45.460249Z","iopub.execute_input":"2021-08-18T18:14:45.461227Z","iopub.status.idle":"2021-08-18T18:15:08.161373Z","shell.execute_reply.started":"2021-08-18T18:14:45.461161Z","shell.execute_reply":"2021-08-18T18:15:08.160365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing \n\nPreprocessing Includes some basic tasks like\n\n1. Removing outliers\n2. Build more features which help our model to learn things\n3. Identifying and removing null values","metadata":{}},{"cell_type":"code","source":"\n'''\nIn this fucntion we will drop all the longitude and latitude which are 0s or empty or nan etc\nand we will only take trip which is between 5$ to 45$ removing upper and lower bounds (Outlier removal)\n\nFor that you can make quantiles and take quantiles between your specefic range\n'''\ndef clean_data(df, test=False, predict=False):\n    df = df.dropna(how='any', axis='rows')\n    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n    \n    if \"total_amount\" in list(df):\n        df = df[df.total_amount.between(5, 45)]\n    \n    return df\n\ndf_2015 = clean_data(df_2015)\ndf_2016 = clean_data(df_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:31.025431Z","iopub.execute_input":"2021-08-18T18:00:31.025872Z","iopub.status.idle":"2021-08-18T18:00:33.959669Z","shell.execute_reply.started":"2021-08-18T18:00:31.025841Z","shell.execute_reply":"2021-08-18T18:00:33.958672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align=\"center\">Data Cleaning</h1> ","metadata":{}},{"cell_type":"code","source":"# to decide where to start removing outliers\ndef remove_outliers(data, start=0, end=100):\n    data=np.sort(data)\n    for i in np.linspace(start, end, 10):\n        i=round(i, 6)\n        print(str(i).zfill(5) + \" percentile value is \" + str(round(data[int(len(data)*(float(i)/100))-1], 1)))\n    print(str(float(end)).zfill(3) + \" percentile value is \" + str(data[-1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:33.961446Z","iopub.execute_input":"2021-08-18T18:00:33.961719Z","iopub.status.idle":"2021-08-18T18:00:33.968577Z","shell.execute_reply.started":"2021-08-18T18:00:33.961693Z","shell.execute_reply":"2021-08-18T18:00:33.967334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. <font color='red'>**pickup_latitude**</font> and <font color='red'>**pickup_longitude**</font>  \n[NYC Coordinates Source](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc)","metadata":{}},{"cell_type":"code","source":"# drop rows with coordinates outside NYC \ndef clean_coordinates(df):\n    nrows = df.shape[0]\n    df.drop(df.index[\n        \n            ~((df['pickup_latitude'].between(40.496115395170364, 40.91553277700258)) &\n              (df['pickup_longitude'].between(-74.25559136315209, -73.7000090639354))) \n        \n    ], inplace=True)\n    print(\"Number of rows removed due to wrong coordinates is {}\".format(nrows - df.shape[0]))\n    \nclean_coordinates(df_2015)\nclean_coordinates(df_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:33.970881Z","iopub.execute_input":"2021-08-18T18:00:33.971467Z","iopub.status.idle":"2021-08-18T18:00:36.639201Z","shell.execute_reply.started":"2021-08-18T18:00:33.971413Z","shell.execute_reply":"2021-08-18T18:00:36.638024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. <font color='red'>**trip_duration**</font> - **tpep_pickup_datetime** and **tpep_dropoff_datetime**","metadata":{}},{"cell_type":"code","source":"def clean_trip_duration(df):\n    # convert from object to datetime\n    df['tpep_pickup_datetime']  = pd.to_datetime(df['tpep_pickup_datetime'])\n    df['tpep_dropoff_datetime']  = pd.to_datetime(df['tpep_dropoff_datetime'])\n    \n    # copute the time diffrance between pickup & dropoff\n    # to covert from nanosecondes to minutes we devide by 1000000000 then by 60\n    # store trip_duratin column\n    trip_duration = np.array(df['tpep_dropoff_datetime']-df['tpep_pickup_datetime'])\n    trip_duration = trip_duration/1000000000/60\n    df['trip_duration'] = trip_duration.astype(float)\n    \n    # drop all records that have trip_duration > 2 hours\n    #                            trip_duration <= 0\n    #                            trip_distance <= 0\n    nrows = df.shape[0]\n    df.drop(df[(df['trip_duration'] > 160) | \n               (df['trip_duration'] <= 0)].index, inplace = True)\n    print(\"Number of rows removed due to wrong trip_duration {}\".format(nrows - df.shape[0]))\n    \n    \nclean_trip_duration(df_2015)\nclean_trip_duration(df_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:36.640849Z","iopub.execute_input":"2021-08-18T18:00:36.641301Z","iopub.status.idle":"2021-08-18T18:00:44.012807Z","shell.execute_reply.started":"2021-08-18T18:00:36.641255Z","shell.execute_reply":"2021-08-18T18:00:44.011737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. <font color='red'>**pickup_time**</font>","metadata":{}},{"cell_type":"code","source":"def clean_pickuptime(df):\n    return df.rename(columns={'tpep_pickup_datetime': 'pickup_time'})\n\ndf_2015 = clean_pickuptime(df_2015)\ndf_2016 = clean_pickuptime(df_2016)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:44.014332Z","iopub.execute_input":"2021-08-18T18:00:44.014748Z","iopub.status.idle":"2021-08-18T18:00:44.165559Z","shell.execute_reply.started":"2021-08-18T18:00:44.014706Z","shell.execute_reply":"2021-08-18T18:00:44.164443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. <font color='red'>**trip_distance**</font>","metadata":{}},{"cell_type":"code","source":"def clean_trip_distance(df):\n    nrows = df.shape[0]\n    df.drop(df[(df['trip_distance'] <= 0) | (df['trip_distance'] > 77.5)].index, inplace = True)\n    print(\"Number of rows removed due to speed outliers {}\".format(nrows - df.shape[0]))\n    \nclean_trip_distance(df_2015)\nclean_trip_distance(df_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:44.166934Z","iopub.execute_input":"2021-08-18T18:00:44.16724Z","iopub.status.idle":"2021-08-18T18:00:46.716199Z","shell.execute_reply.started":"2021-08-18T18:00:44.1672Z","shell.execute_reply":"2021-08-18T18:00:46.715085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. <font color='red'>**speed**</font> - trip_distance/trip_duration","metadata":{}},{"cell_type":"code","source":"def compute_speed(df):\n    # computing Taxi speed average (mile/hour)\n    df['speed'] = df['trip_distance']/df['trip_duration']*60\n    \ndef clean_speed(df):\n\n    # Removing speed anomaly/outliers\n    nrows = df.shape[0]\n    df.drop(df[((df['speed'] <= 0) | (df['speed'] > 63.0))].index, inplace = True)\n    print(\"Number of rows removed due to speed outliers {}\".format(nrows - df.shape[0]))\n\n\ncompute_speed(df_2015)\ncompute_speed(df_2016)    \nclean_speed(df_2015)\nclean_speed(df_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:46.719741Z","iopub.execute_input":"2021-08-18T18:00:46.720153Z","iopub.status.idle":"2021-08-18T18:00:49.329743Z","shell.execute_reply.started":"2021-08-18T18:00:46.720108Z","shell.execute_reply":"2021-08-18T18:00:49.328799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n### 6. <font color='red'>**K-Means with respect to longitude and latitude**</font>\n","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, timedelta\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom pandarallel import pandarallel\n\n\n#Clustering pickups\nprint(\"Getting clusters\")\ncoord = df_2015[[\"pickup_latitude\", \"pickup_longitude\"]].values\nregions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\n\nprint(\"Predicting clusters\")\ncluster_column = regions.predict(df_2015[[\"pickup_latitude\", \"pickup_longitude\"]])\ncluster_column_2016 = regions.predict(df_2016[[\"pickup_latitude\", \"pickup_longitude\"]])\ndf_2015[\"pickup_cluster\"] = cluster_column\ndf_2016[\"pickup_cluster\"] = cluster_column_2016\n\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-18T18:00:49.332127Z","iopub.execute_input":"2021-08-18T18:00:49.332565Z","iopub.status.idle":"2021-08-18T18:00:51.110257Z","shell.execute_reply.started":"2021-08-18T18:00:49.332522Z","shell.execute_reply":"2021-08-18T18:00:51.109239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing mins and sec with 0\nprint(\"Removing Hours and seconds\")\npandarallel.initialize()\ndf_2015['pickup_time'] = df_2015.pickup_time.parallel_apply(lambda x : pd.to_datetime(x).replace(minute=0, second=0) + timedelta(hours=1))\ndf_2016['pickup_time'] = df_2016.pickup_time.parallel_apply(lambda x : pd.to_datetime(x).replace(minute=0, second=0) + timedelta(hours=1))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:00:51.115592Z","iopub.execute_input":"2021-08-18T18:00:51.115922Z","iopub.status.idle":"2021-08-18T18:01:36.011951Z","shell.execute_reply.started":"2021-08-18T18:00:51.115889Z","shell.execute_reply":"2021-08-18T18:01:36.010937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"Group by Cluster and time\")\ndf2 = df_2015.groupby(['pickup_time','pickup_cluster']).size().reset_index(name='count')\ndf1 = df_2016.groupby(['pickup_time','pickup_cluster']).size().reset_index(name='count')\n\nprint(\"Converting counts to demand percentage\")\ndf2['count'] = df2['count'].parallel_apply(lambda x :  (x / df2['count'].max()))\ndf1['count'] = df1['count'].parallel_apply(lambda x :  (x / df1['count'].max()))\n\n\nprint(\"Getting month, days, hours, day of week\")\ndf2['month'] = pd.DatetimeIndex(df2['pickup_time']).month\ndf2['day'] = pd.DatetimeIndex(df2['pickup_time']).day\ndf2['dayofweek'] = pd.DatetimeIndex(df2['pickup_time']).dayofweek\ndf2['hour'] = pd.DatetimeIndex(df2['pickup_time']).hour\n\n\ndf1['month'] = pd.DatetimeIndex(df1['pickup_time']).month\ndf1['day'] = pd.DatetimeIndex(df1['pickup_time']).day\ndf1['dayofweek'] = pd.DatetimeIndex(df1['pickup_time']).dayofweek\ndf1['hour'] = pd.DatetimeIndex(df1['pickup_time']).hour\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:01:36.013682Z","iopub.execute_input":"2021-08-18T18:01:36.014231Z","iopub.status.idle":"2021-08-18T18:01:38.990123Z","shell.execute_reply.started":"2021-08-18T18:01:36.014181Z","shell.execute_reply":"2021-08-18T18:01:38.986946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. <font color='red'>**Split data into train and test, X and y**</font>","metadata":{}},{"cell_type":"code","source":"# training X and y\nX_2015_1 = df2[['pickup_cluster', 'month', 'day', 'hour', 'dayofweek']]\ny_2015_1 = df2['count']\n\n\n# training X and y\nX_2016_1 = df1[['pickup_cluster', 'month', 'day', 'hour', 'dayofweek']]\ny_2016_1 = df1['count']\n\nprint(len(X_2015_1))\nprint(len(y_2015_1))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:01:38.992804Z","iopub.execute_input":"2021-08-18T18:01:38.993155Z","iopub.status.idle":"2021-08-18T18:01:39.025789Z","shell.execute_reply.started":"2021-08-18T18:01:38.993118Z","shell.execute_reply":"2021-08-18T18:01:39.024646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. <font color='red'>**Saving Preprocessed DataFrame for Later processing**</font>","metadata":{}},{"cell_type":"code","source":"\n# X_2016.to_csv(\"X_2016_X.csv\")\n# y_2016.to_csv(\"X_2016_Y.csv\")\nfrom sklearn.model_selection import train_test_split\nX_2015, X_2016, y_2015, y_2016 = train_test_split(\n     X_2015_1.values, y_2015_1.values, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:01:39.027638Z","iopub.execute_input":"2021-08-18T18:01:39.028072Z","iopub.status.idle":"2021-08-18T18:01:39.042872Z","shell.execute_reply.started":"2021-08-18T18:01:39.028028Z","shell.execute_reply":"2021-08-18T18:01:39.041699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align=\"center\">Models Training</h1> ","metadata":{}},{"cell_type":"code","source":"print('model training 0/3 (creating model)', end='\\r')\nLReg = LinearRegression()\n\nprint('model training 1/3 (fitting model)', end='\\r')\nLReg.fit(X_2015, y_2015)\n\nprint('model training 2/3 (training model)', end='\\r')\nLReg_y_pred = LReg.predict(X_2016)\n\nprint('model training 3/3 done!           ', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:01:39.044137Z","iopub.execute_input":"2021-08-18T18:01:39.044466Z","iopub.status.idle":"2021-08-18T18:01:39.064422Z","shell.execute_reply.started":"2021-08-18T18:01:39.044437Z","shell.execute_reply":"2021-08-18T18:01:39.063356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('model training 0/3 (creating model)', end='\\r')\nRFRegr = RandomForestRegressor()\n\nprint('model training 1/3 (fitting model)', end='\\r')\nRFRegr.fit(X_2015, y_2015)\n\nprint('model training 2/3 (training model)', end='\\r')\nRFRegr_y_pred = RFRegr.predict(X_2016)\n\nprint('model training 3/3 done!           ', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:01:39.065988Z","iopub.execute_input":"2021-08-18T18:01:39.066616Z","iopub.status.idle":"2021-08-18T18:01:47.925492Z","shell.execute_reply.started":"2021-08-18T18:01:39.066566Z","shell.execute_reply":"2021-08-18T18:01:47.92447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('model training 0/3 (creating model)', end='\\r')\nGBRegr = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n\nprint('model training 1/3 (fitting model)', end='\\r')\nGBRegr.fit(X_2015, y_2015)\n\nprint('model training 2/3 (training model)', end='\\r')\nGBRegr_y_pred = GBRegr.predict(X_2016)\n\nprint('model training 3/3 done!           ', end='\\r')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:01:47.927128Z","iopub.execute_input":"2021-08-18T18:01:47.927574Z","iopub.status.idle":"2021-08-18T18:02:01.213864Z","shell.execute_reply.started":"2021-08-18T18:01:47.927531Z","shell.execute_reply":"2021-08-18T18:02:01.212987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align=\"center\">Models Evaluation</h1> ","metadata":{}},{"cell_type":"code","source":"def model_evaluation(algorithem_name, X_Test, y_pred, y_true):\n    \n    # R2 and Adjasted R2\n    r2 = r2_score(y_true, y_pred)\n    adj_r2 = 1-(1-r2)*((len(X_Test)-1)/(len(X_Test)-X_Test.shape[1]-1))\n    # MSE and RMSE\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = math.sqrt(mse)\n    \n    # print in table\n    x = PrettyTable()\n    x.add_row(['R2', r2])\n    x.add_row(['Adjusted R2', adj_r2])\n    x.add_row(['MSE',mse])\n    x.add_row(['RMSE', rmse])\n    x.title = algorithem_name\n    print(x)\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.218792Z","iopub.execute_input":"2021-08-18T18:02:01.22205Z","iopub.status.idle":"2021-08-18T18:02:01.231083Z","shell.execute_reply.started":"2021-08-18T18:02:01.222004Z","shell.execute_reply":"2021-08-18T18:02:01.230139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. <font color='red'>**y_True**</font> ","metadata":{}},{"cell_type":"code","source":"model_evaluation('y True',X_Test=X_2016, y_pred=y_2016, y_true=y_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.232396Z","iopub.execute_input":"2021-08-18T18:02:01.232692Z","iopub.status.idle":"2021-08-18T18:02:01.24754Z","shell.execute_reply.started":"2021-08-18T18:02:01.232665Z","shell.execute_reply":"2021-08-18T18:02:01.246457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. <font color='red'>**Linear Regression**</font>","metadata":{}},{"cell_type":"code","source":"model_evaluation('Linear Regression',X_Test=X_2016, y_pred=LReg_y_pred, y_true=y_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.2489Z","iopub.execute_input":"2021-08-18T18:02:01.249276Z","iopub.status.idle":"2021-08-18T18:02:01.260417Z","shell.execute_reply.started":"2021-08-18T18:02:01.249236Z","shell.execute_reply":"2021-08-18T18:02:01.259301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. <font color='red'>**Random Forest**</font> ","metadata":{}},{"cell_type":"code","source":"model_evaluation('Random Forest',X_Test=X_2016, y_pred=RFRegr_y_pred, y_true=y_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.261776Z","iopub.execute_input":"2021-08-18T18:02:01.262091Z","iopub.status.idle":"2021-08-18T18:02:01.271429Z","shell.execute_reply.started":"2021-08-18T18:02:01.262064Z","shell.execute_reply":"2021-08-18T18:02:01.270327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. <font color='red'>**Gradient Boosting**</font>","metadata":{}},{"cell_type":"code","source":"model_evaluation('Gradient Boosting',X_Test=X_2016, y_pred=GBRegr_y_pred, y_true=y_2016)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.27288Z","iopub.execute_input":"2021-08-18T18:02:01.273202Z","iopub.status.idle":"2021-08-18T18:02:01.28245Z","shell.execute_reply.started":"2021-08-18T18:02:01.273174Z","shell.execute_reply":"2021-08-18T18:02:01.281383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"LReg_y_pred = LReg.predict(X_2016_1)\nRFRegr_y_pred = RFRegr.predict(X_2016_1)\nGBRegr_y_pred = GBRegr.predict(X_2016_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.284015Z","iopub.execute_input":"2021-08-18T18:02:01.284324Z","iopub.status.idle":"2021-08-18T18:02:01.562842Z","shell.execute_reply.started":"2021-08-18T18:02:01.284295Z","shell.execute_reply":"2021-08-18T18:02:01.561887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Actual","metadata":{}},{"cell_type":"code","source":"model_evaluation('y True',X_Test=X_2016_1, y_pred=y_2016_1, y_true=y_2016_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.570203Z","iopub.execute_input":"2021-08-18T18:02:01.570585Z","iopub.status.idle":"2021-08-18T18:02:01.579029Z","shell.execute_reply.started":"2021-08-18T18:02:01.570552Z","shell.execute_reply":"2021-08-18T18:02:01.577783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression","metadata":{}},{"cell_type":"code","source":"model_evaluation('Linear Regression',X_Test=X_2016_1, y_pred=LReg_y_pred, y_true=y_2016_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.580691Z","iopub.execute_input":"2021-08-18T18:02:01.581161Z","iopub.status.idle":"2021-08-18T18:02:01.588533Z","shell.execute_reply.started":"2021-08-18T18:02:01.581125Z","shell.execute_reply":"2021-08-18T18:02:01.587401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"model_evaluation('Linear Regression',X_Test=X_2016_1, y_pred=RFRegr_y_pred, y_true=y_2016_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.590085Z","iopub.execute_input":"2021-08-18T18:02:01.590468Z","iopub.status.idle":"2021-08-18T18:02:01.601326Z","shell.execute_reply.started":"2021-08-18T18:02:01.590436Z","shell.execute_reply":"2021-08-18T18:02:01.60017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradiant Boasting","metadata":{}},{"cell_type":"code","source":"model_evaluation('Linear Regression',X_Test=X_2016_1, y_pred=GBRegr_y_pred, y_true=y_2016_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T18:02:01.602698Z","iopub.execute_input":"2021-08-18T18:02:01.603087Z","iopub.status.idle":"2021-08-18T18:02:01.613249Z","shell.execute_reply.started":"2021-08-18T18:02:01.603047Z","shell.execute_reply":"2021-08-18T18:02:01.612188Z"},"trusted":true},"execution_count":null,"outputs":[]}]}