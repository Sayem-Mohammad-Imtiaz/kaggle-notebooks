{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **8. Import dataset** <a class=\"anchor\" id=\"8\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = '/kaggle/input/car-evaluation-data-set/car_evaluation.csv'\n\ndf = pd.read_csv(data, header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **9. Exploratory data analysis** <a class=\"anchor\" id=\"9\"></a>\n\n[Table of Contents](#0.1)\n\n\nNow, I will explore the data to gain insights about the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# view dimensions of dataset\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 1728 instances and 7 variables in the data set."},{"metadata":{},"cell_type":"markdown","source":"### View top 5 rows of dataset"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# preview the dataset\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rename column names\n\nWe can see that the dataset does not have proper column names. The columns are merely labelled as 0,1,2.... and so on. We should give proper names to the columns. I will do it as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\n\ndf.columns = col_names\n\ncol_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's again preview the dataset\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the column names are renamed. Now, the columns have meaningful names."},{"metadata":{},"cell_type":"markdown","source":"### View summary of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Frequency distribution of values in variables\n\nNow, I will check the frequency counts of categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\n\nfor col in col_names:\n    \n    print(df[col].value_counts())   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the `doors` and `persons` are categorical in nature. So, I will treat them as categorical variables."},{"metadata":{},"cell_type":"markdown","source":"### Summary of variables\n\n\n- There are 7 variables in the dataset. All the variables are of categorical data type.\n\n\n- These are given by `buying`, `maint`, `doors`, `persons`, `lug_boot`, `safety` and `class`.\n\n\n- `class` is the target variable."},{"metadata":{},"cell_type":"markdown","source":"### Explore `class` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `class` target variable is ordinal in nature."},{"metadata":{},"cell_type":"markdown","source":"### Missing values in variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in variables\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing values in the dataset. I have checked the frequency distribution of values previously. It also confirms that there are no missing values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"# **10. Declare feature vector and target variable** <a class=\"anchor\" id=\"10\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['class'], axis=1)\n\ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **11. Split data into separate training and test set** <a class=\"anchor\" id=\"11\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **12. Feature Engineering** <a class=\"anchor\" id=\"12\"></a>\n\n[Table of Contents](#0.1)\n\n\n**Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.\n\n\nFirst, I will check the data types of variables again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check data types in X_train\n\nX_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical variables\n\n\nNow, I will encode the categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all  the variables are ordinal categorical data type."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import category encoders\n\nimport category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode categorical variables with ordinal encoding\n\nencoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have training and test set ready for model building. "},{"metadata":{},"cell_type":"markdown","source":"# **13. Random Forest Classifier model with default parameters** <a class=\"anchor\" id=\"13\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# instantiate the classifier \n\nrfc = RandomForestClassifier(random_state=0)\n\n\n\n# fit the model\n\nrfc.fit(X_train, y_train)\n\n\n\n# Predict the Test set results\n\ny_pred = rfc.predict(X_test)\n\n\n\n# Check accuracy score \n\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, **y_test** are the true class labels and **y_pred** are the predicted class labels in the test-set."},{"metadata":{},"cell_type":"markdown","source":"Here, I have build the Random Forest Classifier model with default parameter of `n_estimators = 10`. So, I have used 10 decision-trees to build the model. Now, I will increase the number of decision-trees and see its effect on accuracy."},{"metadata":{},"cell_type":"markdown","source":"# **14. Random Forest Classifier model with parameter n_estimators=100** <a class=\"anchor\" id=\"14\"></a>\n\n[Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the classifier with n_estimators = 100\n\nrfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nrfc_100.fit(X_train, y_train)\n\n\n\n# Predict on the test set results\n\ny_pred_100 = rfc_100.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model accuracy score with 10 decision-trees is 0.9247 but the same with 100 decision-trees is 0.9457. So, as expected accuracy increases with number of decision-trees in the model."},{"metadata":{},"cell_type":"markdown","source":"# **15. Find important features with Random Forest model** <a class=\"anchor\" id=\"15\"></a>\n\n[Table of Contents](#0.1)\n\n\nUntil now, I have used all the features given in the model. Now, I will select only the important features, build the model using these features and see its effect on accuracy. \n\n\nFirst, I will create the Random Forest model as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will use the feature importance variable to see feature importance scores."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# view the feature scores\n\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nfeature_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the most important feature is `safety` and least important feature is `doors`."},{"metadata":{},"cell_type":"markdown","source":"# **16. Visualize feature scores of the features** <a class=\"anchor\" id=\"16\"></a>\n\n[Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a seaborn bar plot\n\nsns.barplot(x=feature_scores, y=feature_scores.index)\n\n\n\n# Add labels to the graph\n\nplt.xlabel('Feature Importance Score')\n\nplt.ylabel('Features')\n\n\n\n# Add title to the graph\n\nplt.title(\"Visualizing Important Features\")\n\n\n\n# Visualize the graph\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **17. Build Random Forest model on selected features** <a class=\"anchor\" id=\"17\"></a>\n\n[Table of Contents](#0.1)\n\n\nNow, I will drop the least important feature `doors` from the model, rebuild the model and check its effect on accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# declare feature vector and target variable\n\nX = df.drop(['class', 'doors'], axis=1)\n\ny = df['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will build the random forest model and check accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode categorical variables with ordinal encoding\n\nencoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'persons', 'lug_boot', 'safety'])\n\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)\n\n\n# Predict on the test set results\n\ny_pred = clf.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with doors variable removed : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have removed the `doors` variable from the model, rebuild it and checked its accuracy. The accuracy of the model with `doors` variable removed is 0.9264. The accuracy of the model with all the variables taken into account is 0.9247. So, we can see that the model accuracy has been improved with `doors` variable removed from the model.\n\nFurthermore, the second least important model is `lug_boot`. If I remove it from the model and rebuild the model, then the accuracy was found to be 0.8546. It is a significant drop in the accuracy. So, I will not drop it from the model."},{"metadata":{},"cell_type":"markdown","source":"Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n\n\nWe have another tool called `Confusion matrix` that comes to our rescue."},{"metadata":{},"cell_type":"markdown","source":"# **18. Confusion matrix** <a class=\"anchor\" id=\"18\"></a>\n\n[Table of Contents](#0.1)\n\n\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n\n**True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n\n**True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n\n**False Positives (FP)** – False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n\n\n\n**False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n\n\n\nThese four outcomes are summarized in a confusion matrix given below.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **19. Classification Report** <a class=\"anchor\" id=\"19\"></a>\n\n[Table of Contents](#0.1)\n\n\n**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **20. Results and conclusion** <a class=\"anchor\" id=\"20\"></a>\n\n[Table of Contents](#0.1)\n\n\n1.\tIn this project, I build a Random Forest Classifier to predict the safety of the car. I build two models, one with 10 decision-trees and another one with 100 decision-trees. \n2.\tThe model accuracy score with 10 decision-trees is 0.9247 but the same with 100 decision-trees is 0.9457. So, as expected accuracy increases with number of decision-trees in the model.\n3.\tI have used the Random Forest model to find only the important features, build the model using these features and see its effect on accuracy. The most important feature is `safety` and least important feature is `doors`.\n4.\tI have removed the `doors` variable from the model, rebuild it and checked its accuracy. The accuracy of the model with `doors` variable removed is 0.9264. The accuracy of the model with all the variables taken into account is 0.9247. So, we can see that the model accuracy has been improved with `doors` variable removed from the model.\n5.\tThe second least important model is `lug_boot`. If I remove it from the model and rebuild the model, then the accuracy was found to be 0.8546. It is a significant drop in the accuracy. So, I will not drop it from the model.\n6.\tConfusion matrix and classification report are another tool to visualize the model performance. They yield good performance.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# **21. References** <a class=\"anchor\" id=\"21\"></a>\n\n[Table of Contents](#0.1)\n\n\nThe work done in this project is inspired from following books and websites:-\n\n1. Hands on Machine Learning with Scikit-Learn and Tensorflow by Aurélién Géron\n\n2. Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido\n\n3. https://en.wikipedia.org/wiki/Random_forest\n\n4. https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n\n5. http://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/\n\n6. https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/"},{"metadata":{},"cell_type":"markdown","source":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n"},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}