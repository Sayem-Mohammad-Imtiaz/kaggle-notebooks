{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport nltk\nimport string\nimport re\nimport os\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_TOKENS = 10000\nOUTPUT_LEN = 300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_true = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\ndf_false = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\ndf_true[\"label\"] = 1\ndf_false[\"label\"] = 0\n\ndf = pd.concat((df_true, df_false))\ndf[\"text\"] = df[\"title\"] + \" \" + df[\"text\"]\ndel df[\"title\"]\ndel df[\"subject\"]\ndel df[\"date\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\ndef remove_between_square_brackets(text):\n    return re.sub(\"\\[[^]]*\\]\", \"\", text)\n\ndef remove_links(text):\n    return re.sub(r\"http\\S+\", \"\", text)\n\ndef remove_stopwords(text):\n    return \" \".join(x for x in text.split() if x.lower() not in stop)\n\ndef process_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    \n    text = text.translate(text.maketrans(\"\", \"\", string.punctuation))\n    text = remove_stopwords(text)\n    return text\n\ndf[\"text\"] = df[\"text\"].apply(process_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TextVectorization(max_tokens=MAX_TOKENS, output_sequence_length=OUTPUT_LEN)\nds_train = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\nds_test = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\nvectorizer.adapt(ds_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voc = vectorizer.get_vocabulary()\nword_index = dict(zip(voc, range(2, len(voc))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open(\"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\") as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_tokens = len(voc) + 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word.decode(\"utf-8\"))\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n))\nmodel.add(LSTM(units=128, return_sequences=True, dropout=0.25))\nmodel.add(LSTM(units=64, dropout=0.1))\nmodel.add(Dense(units=32, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(lr=0.01)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = vectorizer(np.array([[s] for s in X_train])).numpy()\nX_test = vectorizer(np.array([[s] for s in X_test])).numpy()\n\nY_train = np.array(Y_train)\nY_test = np.array(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_reduction = ReduceLROnPlateau(monitor=\"val_accuracy\", patience = 2, verbose=1, factor=0.5, min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test), callbacks=[lr_reduction])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, batch_size=128, initial_epoch=10, epochs=20, validation_data=(X_test, Y_test), callbacks=[lr_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, Y_train, batch_size=128, initial_epoch=20, epochs=30, validation_data=(X_test, Y_test), callbacks=[lr_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}