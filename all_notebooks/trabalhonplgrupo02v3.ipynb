{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Atividade Grupo \n* Aluno Wagner luciano Buzzo ra 183274\n* Aluno Julio Cesar Rosa Reis ra 183135\n* Aluno Jader Airton Bezerra Lima ra 183133\n**\n\nUm sistema de classificação de tópicos consiste em um modelo capaz de identificar a qual domínio uma determinada sentença ou texto pertence. Crie um classificador que receba uma frase em inglês e indique se ela faz parte de uma das seguintes categorias: aventura, ficção científica, religião ou governo, utilizando o corpus em anexo.\n\nUse as etapas de pré-processamento que vimos em sala e representações de texto diferentes se desejar (bag-of-words/LSA, Skip-gram, CBOW, GloVe). Utilize os algoritmos de classificação que desejar, e faça seleção de hiperparâmetros. Siga sempre as boas práticas para experimentos de aprendizado de máquina para evitar underfitting e overfitting.\n\nO grupo que obtiver o melhor resultado no corpus de teste (de posse somente do professor) receberá 1 ponto a mais na média de disciplina. Se mais de um grupo tiver o mesmo melhor resultado, nenhum grupo ganhará essa pontuação.\n\nA entrega deve ser feita por meio de script no kaggle compartilhado com o usuário do professor. Basta copiar o link na submissão da solução."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Demais imports necessários para o projeto**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\n\n\n\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import neighbors\n\nfrom sklearn.model_selection import RandomizedSearchCV\nimport scipy\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\n\nfrom sklearn.metrics import *\nfrom sklearn.decomposition import TruncatedSVD\nfrom gensim.models import Word2Vec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/corpus_categorias_treino.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = df['category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = df['category'].value_counts()\nl \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['words'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nstopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):\n    \n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0].lower(), pos))\n\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = []\nfor i in df['words']:\n    t.append(my_tokenizer(i))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#t\nprint(my_tokenizer(df['words'][0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frase_processada.append(' '.join(nova_frase))\nfrase_processada = list()\nfor i in t:\n    frase_processada.append(' '.join(i))\ndf['words_tratada'] = frase_processada\ndf.category.replace(['adventure', 'government','religion','science_fiction'], [0,1,2,3], inplace=True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frase_processada[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Utilizando o processamento de texto conhecido como Bag of words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvetorizar = CountVectorizer(lowercase=False, max_features=50)\nbag_of_words = vetorizar.fit_transform(df[\"words\"])\n\n\ntreino, teste , classe_treino, classe_teste = train_test_split(bag_of_words, df[\"category\"], random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Os classificadores utilizados durante o experimento serão o KNeighbors e o LinearSVN, esse último será utilizado na estratégia oneVsRest, para que ele possa ser utilizado para multiplas classes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def TrocaModelo(treino, classe_treino):#\n    clf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n    clf.fit(treino, classe_treino)\n    previsao_teste  = clf.predict(teste)\n    acuracia_clf = clf.score(teste, classe_teste)\n    print(acuracia_clf)\n    \n    onerestsvn = OneVsRestClassifier(LinearSVC(random_state =  42))\n    onerestsvn.fit(treino, classe_treino)\n    previsao_teste_onerestsvn = onerestsvn.predict(teste)\n    acuracia_onerestsvn = onerestsvn.score(teste, classe_teste)\n    print(acuracia_onerestsvn)\n    \n    \n    if acuracia_clf < acuracia_onerestsvn:\n        my_pipeline = Pipeline([('onerestsvn', onerestsvn)])\n        par = {}\n        hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)\n        print('Best model : LinearSvn in one vc rest')\n    else:\n        my_pipeline = Pipeline([('clf', clf)])\n        par = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n        hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)\n        print('Best model : knn')\n        \n    return hyperpar_selector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector = TrocaModelo(treino, classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)#teste, classe_teste","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nif(best_parameters.get('par')):\n    print(\"Best parameters set:\")\n    for param_name in sorted(par.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Utilizando após o tratamento do texto, TF-IDF (Term Frequency - Inverse Document Frequency) , onde a frequencia das palavras no corpus é considerada, juntamente com LSA para redução de dimensionalidade **\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TrocaModelo(X, Y):\n    \n   \n    tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n\n    tfs = tfidf_vectorizer.fit_transform(X)\n\n    svd_transformer = TruncatedSVD(n_components=1000)\n\n    svd_transformer.fit(tfs)\n    \n    cummulative_variance = 0.0\n    k = 0\n    for var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n        cummulative_variance += var\n        if cummulative_variance >= 0.5:\n            break\n        else:\n            k += 1\n\n    print(k)\n    \n    svd_transformer = TruncatedSVD(n_components=k)\n    svd_transformer.fit(tfs)\n    svd_data = svd_transformer.transform(tfs)\n    #print(sorted(svd_transformer.explained_variance_ratio_)[::-1])\n    \n    \n    treino, teste , classe_treino, classe_teste = train_test_split(svd_data, Y, random_state = 42)\n    \n    clf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n    clf.fit(treino, classe_treino)\n    previsao_teste  = clf.predict(teste)\n    acuracia_clf = clf.score(teste, classe_teste)\n    print(acuracia_clf)\n    \n    onerestsvn = OneVsRestClassifier(LinearSVC(random_state =  42))\n    onerestsvn.fit(treino, classe_treino)\n    previsao_teste_onerestsvn = onerestsvn.predict(teste)\n    acuracia_onerestsvn = onerestsvn.score(teste, classe_teste)\n    print(acuracia_onerestsvn)\n    \n    \n    if acuracia_clf < acuracia_onerestsvn:        \n        my_pipeline = Pipeline([('tfidf', TfidfVectorizer(tokenizer=my_tokenizer)),('svd', SVDDimSelect()),('onerestsvn', onerestsvn)])\n        par = {}\n        hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)\n        print('Best model : LinearSvn in one vc rest')\n    else:\n        my_pipeline = Pipeline([('tfidf', TfidfVectorizer(tokenizer=my_tokenizer)),('svd', SVDDimSelect()),('clf', clf)])\n        par = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n        hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)\n        print('Best model : knn')\n    \n    return hyperpar_selector\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"treino, teste , classe_treino, classe_teste = train_test_split(df[\"words\"], df['category'], random_state = 42)\nhyperpar_selector = TrocaModelo(treino, classe_treino)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)#teste, classe_teste","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nif(best_parameters.get('par')):\n    print(\"Best parameters set:\")\n    for param_name in sorted(par.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\ny_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Arquitetura Skip-Gram**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n\n\nclass Word2VecTransformer(object):\n    \n    ALGO_SKIP_GRAM=1\n    ALGO_CBOW=2    \n    \n    def __init__(self, algo=1):    \n        self.algo = algo\n    \n    def fit(self, X, y=None):     \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        self.word2vec = Word2Vec(X, min_count=2, sg=self.algo)\n        \n        # Pegamos a dimensão da primeira palavra, para saber quantas dimensões estamos trabalhando,\n        # assim podemos ajustar nos casos em que aparecerem palavras que não existirem no vocabulário.\n        first_word = next(iter(self.word2vec.wv.vocab.keys()))\n        self.num_dim = len(self.word2vec[first_word])       \n        \n        return self\n    \n    def transform(self, X, Y=None):        \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.num_dim)], axis=0) \n                         for words in X])\n\n        \n    def get_params(self, deep=True):\n        return {}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino, teste , classe_treino, classe_teste = train_test_split(df[\"words\"], df['category'], random_state = 42)\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\nmy_pipeline = Pipeline([('w2vt', Word2VecTransformer()),('clf', clf)])\npar = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nif(best_parameters.get('par')):\n    print(\"Best parameters set:\")\n    for param_name in sorted(par.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onerestsvn = OneVsRestClassifier(LinearSVC(random_state =  42))\n    \nmy_pipeline = Pipeline([('w2vt', Word2VecTransformer()),('onerestsvn', onerestsvn)])\npar = {}\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nif(best_parameters.get('par')):\n    print(\"Best parameters set:\")\n    for param_name in sorted(par.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Arquitetura CBOW (Continuous Bag of Words)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n\n\nclass Word2VecTransformer(object):\n    \n    ALGO_SKIP_GRAM=1\n    ALGO_CBOW=2    \n    \n    def __init__(self, algo=2):    \n        self.algo = algo\n    \n    def fit(self, X, y=None):     \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        self.word2vec = Word2Vec(X, min_count=2, sg=self.algo)\n        \n        # Pegamos a dimensão da primeira palavra, para saber quantas dimensões estamos trabalhando,\n        # assim podemos ajustar nos casos em que aparecerem palavras que não existirem no vocabulário.\n        first_word = next(iter(self.word2vec.wv.vocab.keys()))\n        self.num_dim = len(self.word2vec[first_word])       \n        \n        return self\n    \n    def transform(self, X, Y=None):        \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.num_dim)], axis=0) \n                         for words in X])\n\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino, teste , classe_treino, classe_teste = train_test_split(df[\"words\"], df['category'], random_state = 42)\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\nmy_pipeline = Pipeline([('w2vt', Word2VecTransformer()),('clf', clf)])\npar = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nif(best_parameters.get('par')):\n    print(\"Best parameters set:\")\n    for param_name in sorted(par.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onerestsvn = OneVsRestClassifier(LinearSVC(random_state =  42))\n    \nmy_pipeline = Pipeline([('w2vt', Word2VecTransformer()),('onerestsvn', onerestsvn)])\npar = {}\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nif(best_parameters.get('par')):\n    print(\"Best parameters set:\")\n    for param_name in sorted(par.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"** O melhor resultado obtido foi utilizando  TF-IDF (Term Frequency - Inverse Document Frequency) , junto com LSA para redução de dimensionalidade, obtendo \n 0.81% utilizando o classificador LinearSvn na estratégio OneVsRest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}