{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotnine as p9\nimport matplotlib.pyplot as plt\nimport sympy as sp\nimport re\n\nfrom random import uniform as rand\nfrom matplotlib import cm\nfrom numpy import array, transpose, linspace, sum, exp, log\nfrom numpy.linalg import inv\nfrom pandas import DataFrame as df\n%matplotlib inline\n\n# Style definitions\nplt.style.use('dark_background')\np9.theme_set(\n    p9.theme_dark() + \n    p9.theme(rect=p9.element_rect(color='black', size=3, fill='black')) +\n    p9.theme(text=p9.element_text(color='lightgray')))\nCOLORS = {'red': '#ff2626',\n          'orange': '#f29524',\n          'green': '#169340',\n          'blue': '#377CB9',\n          'white': '#EEEEEE',\n          'black': '#222222'}\n\n# Misc utils\ndef squaremesh(low,high,res):\n    return np.meshgrid(np.linspace(low,high,res),np.linspace(low,high,res))\ndef randspace(low, high, res):\n    return array([rand(low, high) for _ in range(res)])\ndef field(f, low, high, res):\n    X, Y = squaremesh(low, high, res)\n    return [X, Y, f(X, Y)]\ndef unbox(dictionary, *items):\n    return [dictionary[item] for item in items]\n\n# Plot functions\ndef ax3D():\n    ax = plt.axes(projection='3d', facecolor=(0,0,0,0))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n    return ax  \ndef ax2D():\n    ax = plt.axes(facecolor=(0,0,0,0))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    return ax  \ndef darkgraph():\n    return plt.figure(figsize=(20,10), facecolor=(0.1,0.1,0.1))\ndef surface(X, Y, Z, fig, ax):\n    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0)\n    fig.colorbar(surf, shrink=0.6, aspect=5)\ndef translucent(X, Y, Z, fig, ax, res=300, alpha=0.6):\n    surf = ax.contourf(X, Y, Z, res, cmap=cm.coolwarm, linewidth=0, alpha=alpha)\n    fig.colorbar(surf, shrink=0.5, aspect=5)\ndef fitplot2D(X, Y, f, res=50):\n    if len(X)!=len(Y): raise\n    d1 = df({'x':X,'y':Y,'Source':['Original']*len(X)})\n    Xf = linspace(min(X),max(X),res)\n    d2 = df({'x':Xf,'yf':f(Xf),'Source':['Fitted']*len(Xf)})\n    return (p9.ggplot(pd.concat([d1,d2])) +\n            p9.geom_point(p9.aes('x', 'y', color = 'Source')) +\n            p9.geom_path(p9.aes('x', 'yf', color = 'Source')))\ndef fitplot3D(X, Y, Z, f, fig, ax, res=50, alpha=0.2, pcolor='black'):\n    if not len(X)==len(Y)==len(Z): raise\n    Xf,Yf = squaremesh(min(X),max(X),res)\n    Zf = array([f(x,y) for x,y in zip(np.ravel(Xf), np.ravel(Yf))])\n    Zf = Zf.reshape(Xf.shape)\n    translucent(Xf,Yf,Zf,fig,ax,res=300,alpha=0.2)\n    ax.scatter(X, Y, Z, s=9, color=pcolor)\n\n# Ignore warnings (keep commented during editting)\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Clear this cell's output\nfrom IPython.display import clear_output\nclear_output()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Physics Institute of São Carlos ([IFSC](https://www2.ifsc.usp.br/portal-ifsc/)), University of São Paulo ([USP](https://www5.usp.br/)). Based on Costa Didatic Text #17 ([CDT-17](https://www.researchgate.net/publication/337103890_Linear_Least_Squares_Versatile_Curve_and_Surface_Fitting_CDT-17))."},{"metadata":{},"cell_type":"markdown","source":"# Linear Least Squares\n\nSimple and easy curve fitting for linearizable data. In other words, if we can represent it as a polynomial, it can be linearized with this method."},{"metadata":{},"cell_type":"markdown","source":"## Exercises"},{"metadata":{},"cell_type":"markdown","source":"### 1 - Straight Lines"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def poly2D(X, P): \n    return sum([P[i] * X**i for i in range(len(P))], axis=0)\n\ndef straightfit(X, Y):\n    if len(X)!=len(Y): raise\n    At = np.array([[1]*len(X), X])\n    A = np.transpose(At)\n    return inv(At.dot(A)).dot(At.dot(Y))\n\nX = array([-0.47, -0.26,  0.15,  0.82, -0.60])\nY = array([ 1.14,  1.21,  1.28,  1.47,  0.93])\nP = straightfit(X, Y)\nf = lambda X: poly2D(X, P)\n\nfitplot2D(X, Y, f).draw();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2 - Polynomial curves "},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def polyfit(X, Y, o):\n    if len(X)!=len(Y): raise\n    At = np.array([[x**n for x in X] for n in range(o+1)])\n    A = np.transpose(At)\n    return inv(At.dot(A)).dot(At.dot(Y))\n\nX = array([-0.47, -0.26,  0.15,  0.82, -0.60])\nY = array([ 0.12,  0.25,  0.18,  0.26,  -0.11])\nP = polyfit(X, Y, 3)\nf = lambda X: poly2D(X, P)\n\nfitplot2D(X, Y, f).draw();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3 - Noise"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"nf = lambda X, a, b: 1 - 0.5*X + 2*X**2 + rand(a, b) \n\nX = linspace(-1,1,100)\nY = array([nf(x, -0.3, 0.3) for x in X]) \na,b,c = polyfit(X, Y, 2)\nf = lambda X: a + b*X + c*X**2\n\nfitplot2D(X, Y, f).draw();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4 - Exponential Curves"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"f = lambda X: 0.5 * exp(2.5 * X)\n\nX = randspace(0,1,5)\nY = f(X)\nc, b = polyfit(X, log(Y), 1)\nc = exp(c)\nf = lambda X: c * exp(b * X)\n\nfitplot2D(X, Y, f).draw();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4 - Multivariate polynomials"},{"metadata":{"trusted":true},"cell_type":"code","source":"def opoly(v, o):\n    if len(v) == 1:\n        p = []\n        for n in range(o + 1):\n            p.append(v[0]**n)\n        return p\n    p = []\n    q = opoly(v[1:],o)\n    for n in range(o + 1):\n        for t in q:\n            p.append(v[0]**n * t)\n    return p\n\ndef opolyfit(data, order, vec_out):\n    A = np.array([opoly(v, order) for v in data])\n    At = np.transpose(A)\n    return np.linalg.inv(At.dot(A)).dot(At.dot(vec_out))\n\ndef opolynomial(var, coef):\n    p = opoly(var, int(len(coef)**(1. / len(var)) - 1))\n    return sum([coef[i] * p[i] for i in range(len(coef))])\n\nP = randspace(-1,1,9)\nX = randspace(-1,1,50)\nY = randspace(-1,1,50)\nZ = array([opolynomial(v, P) + rand(-0.1,0.1) for v in np.stack((X, Y), axis=1)])\n\nPf = opolyfit(transpose([X,Y]),2,Z)\nf = lambda X, Y: opolynomial([X,Y],Pf)\n\nfitplot3D(X,Y,Z,f,darkgraph(),ax3D())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Project 1: Real World Data"},{"metadata":{},"cell_type":"markdown","source":"### 1 - Linear case: Modelling my morning coffee watter\n\nI use 250ml of watter every morning for making my coffee. The goal here is to predict how much time I should leave the watter inside the microwave to get it to my desired temperature. Turns out it's fairly linear. Here is how I did it:"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/microwave250gwatter/microwave.csv\")\n\nt, Ti, Tf = unbox(data, 't', 'Ti', 'Tf')\n\ndT = Tf - Ti\n\nX = array(t)\nY = array(dT)\nP = straightfit(X, Y)\nf = lambda X: poly2D(X, P)\n\n(\n    fitplot2D(X, Y, f) +\n    p9.labels.ggtitle(\"$250ml$ of Watter on My Microwave\") +\n    p9.labels.ylab(\"Temperature Change (°C)\") +\n    p9.labels.xlab(\"Time inside Microwave (s)\")\n).draw();\n\nformula = '$t \\\\approx \\\\frac{T_f - T_0 - %g}{%g}$' % (P[0],P[1])\nplt.plot()\nplt.text(83,55,formula,fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2 - Non linear case: Zipf's Law\n\nThere is a strange pattern that permeates texts in all existent languages. If we rank all words from most to least frequently appearing, we find the following relationship:\n\n$$f_{(r)} \\approx  \\frac{f_1}{r} $$\n\nBeing $f_{(r)}$ the number of times a word of ranking $r$ appears, $f_1$ being the number of times the most common word appears. We can also write it like this:\n\n$$f_{(r)} \\approx  f_1 \\times r^{-1} $$\n\nLet us look at some data and verify if it approaches this result. We can see that the data appears linear when seen at log-log scale. So we can aply a simple linear least squares function using the logarithms of the frequencies and rankings and then convert it back to a power law."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.read_csv('../input/english-word-frequency/unigram_freq.csv').head(1000)\nprint(data)\n\nX = array(data.index)+1\nY = array(data['count'])\na, b = polyfit(log(X), log(Y), 1)\nc = exp(a) ; \nf = lambda x: c * x**b\n\n(\n    fitplot2D(X, Y, f, res=50000) +\n    p9.scales.scale_y_log10() +\n    p9.scales.scale_x_log10() +\n    p9.labels.ggtitle(\"Google's English Internet 1000 Most Used Words\") +\n    p9.labels.ylab(\"Number of Occurences\") +\n    p9.labels.xlab(\"Ranking (from most to least common)\")\n).draw();\n\nformula = '$frequency \\\\approx (%.2g) \\\\times ranking^{%.2g}$ \\n' % (c,b)\nplt.plot()\nplt.text(0.05,7.9,formula,fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3 - Multivariable case: Galton's Height Dataset and Regression to the Mean\n\nSir Francis Galton was a renown scientist that created the concept of correlation, the idea of \"Regression Towards Mediocrity\", and much more. In a particular research he put together a dataset of parents and adult children's heights to test if his regression theory worked. Here we will just linearize it to see a correlation between parents and children's heights. We will take a very simplified approach here, as the objective is just showcasing the LLS method."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data = pd.read_table('../input/galton-height-data/galton-stata11.csv')\n\nX = F = data['father']\nY = M = data['mother']\nZ = H = data['height']\n\nPf = opolyfit(transpose([X,Y]),1,Z)\nf = lambda X, Y: opolynomial([X,Y],Pf)\nfig=darkgraph() ; ax=ax3D()\nfitplot3D(X,Y,Z,f,fig,ax)\nax.set_xlabel(\"Father's Height\")\nax.set_ylabel(\"Mother's Height\")\nax.set_zlabel(\"Kid's Height\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, even without treating the data the way Gauton did originally, the regression effect is still visible when we apply linear least squares to the data. Note that mid-parental height is the term Gauton used to describe the mean of the parents  heights. In the fitted data, we can see that kids tend to be shorter if the parents have above average height, and taller if the parents are too short, sugesting that this population, if isolated, might regress to a mean height over many generations."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Mean = np.mean([F, M], axis=0)\nD = H - Mean\nX = Mean ; Y = D\n\nP = straightfit(X, Y)\nf = lambda X: poly2D(X, P)\nmean = pd.DataFrame({'x':[np.mean(Mean)],'y':[0],'Source':['Mean']})\n(\n    fitplot2D(X, Y, f) +\n    p9.geom_point(p9.aes('x', 'y', color='Source'), data=mean, size=3) +\n    p9.labels.ggtitle(\"Height difference between children and their parents\") +\n    p9.labels.ylab(\"Height Difference\") +\n    p9.labels.xlab(\"Mid-Parental Height\") +\n    p9.scale_color_manual(unbox(COLORS,\"red\", \"blue\", \"orange\"))\n).draw();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"It is hard to write about Gauton without remembering his theory of eugenics, that many years latter was used as basis for the nazist eugenics that culminated in the holocaust. A great reminder to ***always take statistics with a grain of salt***, please. \n\nI sugest reading the original paper from 1886 to take a look at Gauton's more detailed analisis, and just to contemplate a piece of science history, right at [this link](http://galton.org/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf). \n\nThanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}