{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BIKE SHARING CASE STUDY\n\n\n## Problem Statement:\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to \nthe ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario.\nSo, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the \nongoing lockdown comes to an end, and the economy restores to a healthy state.\n\nSpecifically, they want to understand the factors affecting the demand for these shared bikes in the American market.\nThe company wants to know:\n\n- Which variables are significant in predicting the demand for shared bikes.\n\n- How well those variables describe the bike demands\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LIBRARIES**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading and Understanding the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing=pd.read_csv(\"/kaggle/input/boombikes/day.csv\")\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising the Data\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.pairplot(bike_sharing)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"here, to create dummy variables,columns that are represnting month,weekday,season,weathersit are in \nnumerical values but by chnaging it into categorical value it will recognize the dummy varibles."},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing['season']=bike_sharing['season'].map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dum_season = pd.get_dummies(bike_sharing['season'],drop_first = True)\ndum_season.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing['weathersit']=bike_sharing['weathersit'].map({1:'clear', 2:'mist', 3:'lightrain', 4:'heavyrain'})\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dum_weathersit = pd.get_dummies(bike_sharing['weathersit'],drop_first = True)\ndum_weathersit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing['mnth']=bike_sharing['mnth'].map({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr',5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dum_mnth = pd.get_dummies(bike_sharing['mnth'],drop_first = True)\ndum_mnth.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing['weekday']=bike_sharing['weekday'].map({0:'Sun',1:'Mon', 2:'Tue', 3:'Wed', 4:'Thu',5:'Fri',6:'Sat'})\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dum_weekday = pd.get_dummies(bike_sharing['weekday'],drop_first = True)\ndum_weekday.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concating the dummy varibles into the dataset.\nbike_sharing = pd.concat([bike_sharing, dum_season,dum_weathersit,dum_mnth,dum_weekday], axis = 1)\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we also drop registered and casual varibles as it is higly correlated with target varible count\n\nbike_sharing.drop(['season','dteday','mnth','weekday','casual','registered','instant','weathersit','atemp'], axis = 1, inplace = True)\nbike_sharing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the Data into Training and Testing Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing_train, bike_sharing_test = train_test_split(bike_sharing, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling the train dataset\nnum_vars = ['temp','hum','windspeed','cnt']\n\nbike_sharing_train[num_vars] = scaler.fit_transform(bike_sharing_train[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 10))\nsns.heatmap(bike_sharing_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference:**\n    year and temperatue variable is highly correlated"},{"metadata":{},"cell_type":"markdown","source":"# Dividing into X and Y sets for the model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = bike_sharing_train.pop('cnt') # target variable\nX_train = bike_sharing_train\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a linear model \n**using RFE(Recursive Feature Elimination)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LinearRegression()\nlr.fit(X_train,y_train)\nrfe=RFE(lr,15)\nrfe=rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning the feature variables having True value to X\nX_train_rfe=X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding constants to X_train\nX_train_lr=sm.add_constant(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a first fitted model\nlr=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping highly correlated variables \nX_train_new=X_train_rfe.drop(['hum'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new)\nlr_1=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping highly correlated variables as temp has high coefficient so we drop next variable with high VIF.\n\nX_train_new1=X_train_new.drop(['workingday'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new1)\nlr_2=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new2=X_train_new1.drop(['Sat'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new2)\nlr_3=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_3.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new3=X_train_new2.drop(['Jan'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new3)\nlr_4=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_4.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new4=X_train_new3.drop(['summer'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new4)\nlr_5=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_5.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new5=X_train_new4.drop(['winter'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new5)\nlr_6=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_6.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new6=X_train_new5.drop(['Sep'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr=sm.add_constant(X_train_new6)\nlr_7=sm.OLS(y_train,X_train_lr).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print a summary of the linear regression model obtained\nprint(lr_7.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nvif = pd.DataFrame()\nX=X_train_new6\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_cnt = lr_7.predict(X_train_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms,\nit's time to go ahead and make predictions using the final, i.e. seventh model."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['temp','hum','windspeed','cnt']\n\nbike_sharing_test[num_vars] = scaler.transform(bike_sharing_test[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_sharing_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dividing into X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = bike_sharing_test.pop('cnt')\nX_test = bike_sharing_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding constant variable to test dataframe\nX_test_lr = sm.add_constant(X_test)\nX_test_lr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_lr=X_test_lr.loc[:,['const','yr','holiday','temp','windspeed','spring','lightrain','mist','Jul']]\nX_test_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions using the seventh model\ny_pred_cnt = lr_7.predict(X_test_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\nLet's now plot the graph for actual versus predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nplt.scatter(y_test, y_pred_cnt)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              \nplt.xlabel('y_test', fontsize = 18)                          \nplt.ylabel('y_pred', fontsize = 16) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the equation of our best fitted line is:\n\n$ price = 0.234  \\times year -0.867  \\times holiday +0.408 \\times temperature -0.159 \\times windspeed -0.146 \\times spring -0.273 \\times lighttrain -0.080 \\times mist + -0.085 \\times Jul  $"},{"metadata":{"trusted":true},"cell_type":"code","source":"r2=r2_score(y_test, y_pred_cnt)\nr2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**r2 of train set is 0.824**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}