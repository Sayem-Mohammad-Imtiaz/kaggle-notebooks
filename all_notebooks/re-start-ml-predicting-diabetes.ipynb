{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"name":"_merged"},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"611b1805-944f-4fb4-89b8-d49b83405bc3","_uuid":"21690b95318e8c753ee9e3555521cda3b9e880cc"},"source":"# Welcome to your Jupyter IPython Notebook\n\nJupyter IPython Notebooks allow you to interactively run commands written in Python and inspect their output.  There are also libraries available that allow you to make graphs directly in the notebook.  This is a very convenient way to explore and analyze data.\n\nIn this tutorial, you will execute each cell (box) separately.  You can do this by clicking on the cell with the mouse and then pressing Shift+Enter.  This will run the code in the cell, and print any output you've requested.\n\nYou can execute cells out-of-order, but it's a good practice to work in order.\n\nYou can edit a cell by clicking in it and then editing the text.\n\nIf you get an error when you execute a cell, you can simply re-execute that cell after fixing the code; you don't need to re-run all the cells in the notebook from the beginning.\n\nIf you want to \"comment\" out a line of text (so that it will not be interpreted as a line of code and executed when you run the cell), you can do that using the # sign.\n\nAll the commands you execute in the notebook will be remembered, in the sense that if you import a library of functions or read some data into a variable in one cell, it will still be available when you execute the next cell.  If you want to start over, you can go to the \"Kernel\" menu at the top of the notebook and click \"Restart\".  This will clear everything that you've executed from the computer's memory, but it will leave all of the code you've written just as it is.\n\nThis cell was written by selecting \"Markdown\" from the drop-down menu above (which by default says \"Code\").  The notebook then understands that a Markdown cell contains comments and not code, and running this cell with Shift+Enter simply formats the text.","cell_type":"markdown"},{"metadata":{"_cell_guid":"d7a9ca13-837d-47c5-aa38-25b30d9f37a6","_uuid":"3ec6ddeed748a4e8af8b117c2f16cec0e781737a","collapsed":true},"source":"## What data will we use?\n\nWe will be using the Pima Indians Diabetes Database, a publicly-available data set, and building a model to predict whether a patient will get diabetes based on several measurements of each patient. The dataset is described here:\n\nhttps://www.kaggle.com/uciml/pima-indians-diabetes-database\n\nEach row represents one patient, and the columns are:\n\n- Pregnancies: Number of times pregnant\n\n- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n- BloodPressure: Diastolic blood pressure (mm Hg)\n\n- SkinThickness: Triceps skin fold thickness (mm)\n\n- Insulin: 2-Hour serum insulin (mu U/ml)\n\n- BMI: Body mass index (weight in kg/(height in m)^2)\n\n- DiabetesPedigreeFunction: Diabetes pedigree function\n\n- Age: Age (years)\n\n- Outcome: Class variable (0 or 1)","cell_type":"markdown"},{"metadata":{"_cell_guid":"62bca6ec-6451-4368-9641-9e375e462a80","_uuid":"ed60186d96269fb714d6ec56cd3fe110b0103613"},"source":"## Read in the data\n\nWe will use the pandas library to work with our data.  pandas defines a format for storing the data called a dataframe, as well as many functions for working with the dataframes.  pandas dataframes can also be used with many other useful Python packages.","cell_type":"markdown"},{"metadata":{"_cell_guid":"7fce0ff8-1860-45fc-9462-058bc3472a02","_uuid":"ab7de5d3b393fe6171e25a60844bf11c413abd75","collapsed":true},"source":"# import the pandas library for working with dataframes\nimport pandas as pd","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7676ceb4-c3c8-4588-b2e9-c609e44dbe97","_uuid":"f8664515f75fdb2a01c622c67fb926a9c3c9e014"},"source":"# read the csv-formatted data file into a pandas dataframe\ndf=pd.read_csv('../input/diabetes.csv')\n# get shape of data frame\nprint('Shape (n_rows,n_columns) of dataframe:',df.shape)\n# print top 5 rows of data frame\ndf.head()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"deab00b7-bed2-42cc-9cf5-f6ef98993826","_uuid":"97efa0018c2526001458ca23e5ef7e74b2e0053f"},"source":"## Inspect the data using some pandas commands","cell_type":"markdown"},{"metadata":{"_cell_guid":"750a7c16-754f-44cb-93c0-859b469cd0ff","_uuid":"a8eb867d916c940c7c42ab5003c3d240a95ed2a3"},"source":"### Sub-select one or more columns as a dataframe and show only the top 5 rows\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"fb406652-7a9f-4620-a069-b7a1a9acf690","_uuid":"bd0f540e866e891c4e16d8d4a532391346444974"},"source":"df[['Outcome','Pregnancies','Insulin']].head()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"source":"","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"df[['Age']].head()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5fc9a3a6-94bf-47df-9faa-32811b041675","_uuid":"7e854771be11f6e9eaa085ebada2e08c4f64c415"},"source":"### Select rows where a condition is true, and find out how many rows are in the resulting dataframe","cell_type":"markdown"},{"metadata":{"_cell_guid":"8a019cad-c2da-4e9b-ae42-f4d343627f86","_uuid":"88bb67cf8ed32c5b87bd3a97b644201fa6e5e158"},"source":"print(df[df.BMI>30].shape) # the first element is the number of rows, the second element is the number of columns\nprint('The number of rows where BMI>30 = ',df[df.BMI>30].shape[0]) # the first element is labeled 0, the second element is labeled 1","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"df[df.BMI<10].head()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"df.BMI>30","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c17cece8-bb4c-4f20-aff3-788d78261caa","_uuid":"ea9d748159e64456b1a46949e65bea43def8a919"},"source":"### Select rows where BMI>30,\n### select the columns Outcome, BMI, and Age, \n### and show only the top 5 rows of the resulting dataframe (using the .head() command)","cell_type":"markdown"},{"metadata":{"_cell_guid":"9b17f63e-e806-4ea0-a87c-d81d95681f0b","_uuid":"bffcf6d351e2f701f93fcd86f77fc9b705a67980"},"source":"df[df.BMI>30][['Outcome','BMI','Age']].head()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"403a958d-9a9f-4508-8e1d-e6008287359b","_uuid":"039262a85dc5980f97b36d9062e94b4c9475d070"},"source":"### Select rows where Outcome is 1 and Preganancies>0,\n### select the columns Glucose and BloodPressure, \n### and show only the top 3 rows of the resulting dataframe (using the .head(nrows) command)","cell_type":"markdown"},{"metadata":{"_cell_guid":"e0f91c00-948a-41f0-93fc-8c99b224d25d","_uuid":"2e014e268bf96ba548516b4b54bce9efbb4a2263"},"source":"df[(df.Outcome==1)&(df.Pregnancies>0)][['Glucose','BloodPressure']].head(5)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72cca19f-d904-4a9e-8234-494097780b5d","_uuid":"70c7586a9a1c1a5f17485814756d73f02d9810ef"},"source":"### EXERCISE: \n\nHow many patients in the study have Outcome is 1 and BloodPressure greater than 70?  \n\nFill in the code below where it says NONE.","cell_type":"markdown"},{"metadata":{"_cell_guid":"013cfdb0-6c45-40d2-9e4e-adca8b708059","_uuid":"27c661ecfd8916e3828e49e8bc6467f5cb467bc8"},"source":"df[(df.Outcome==1)&(df.BloodPressure>70)].shape","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77cc6ac1-6ac0-4888-a726-6d5566cefe21","_uuid":"bbce1a0cf2284344da3822edbad555d95a5cb7d7"},"source":"### Does the data have any missing values?","cell_type":"markdown"},{"metadata":{"_cell_guid":"05090970-2e64-4cef-b9c9-624be8565c1f","_uuid":"661a5978d2e0ae4e569bf27e8c8ba2464825522c"},"source":"df.isnull().sum()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"df.notnull().sum()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6a9ac39-07b2-46ac-8dcc-b2f82570c76b","_uuid":"d63f6b8a3d0e66747242ed21353dd1a8b5717182"},"source":"### Get a list of columns names","cell_type":"markdown"},{"metadata":{"_cell_guid":"cfbb4659-731c-4708-aa71-698612092749","_uuid":"a2e31ea0dbc72ca923bdbd62f07ea0e2c945ec4b"},"source":"df.columns","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fce5963-934d-4e65-8be5-0b89437c6ccd","_uuid":"c081853ff061d52fcc7d433e0d4e1651393b206a"},"source":"### Get the column data types","cell_type":"markdown"},{"metadata":{"_cell_guid":"c33b7896-03e5-465a-a4bf-c608383b2356","_uuid":"ffb21509b5494b35d0099259279b6355cbc2847c"},"source":"df.dtypes","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c1863f1-1f91-4e55-b8e9-d57511aa6baf","_uuid":"f0996bbcfd6c9aec73732db10f46b9f372129ca0"},"source":"### Look at some summary statistics of our data frame","cell_type":"markdown"},{"metadata":{"_cell_guid":"296b4930-4e59-444d-bf0a-fb35a5f2297e","_uuid":"33014b6da3b7f11f134a1131b7471b8e6327a28d"},"source":"df.describe()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cf2e4537-a930-4f81-88d1-a8930a8b4cf1","_uuid":"176e73769a5a9945b84cab3bde24657df5dd0ff0"},"source":"Notice that the \"count\" of values for each column is the same, and the same as the number of rows in the data frame.  That means that there are no missing (NULL) values.","cell_type":"markdown"},{"metadata":{"_cell_guid":"551975a1-14a8-46b0-89c7-82bc7d9b9d9c","_uuid":"193e7f53c1b73c5133f014e11799f75a5372f019"},"source":"### How many of each type of diagnosis are there?","cell_type":"markdown"},{"metadata":{"_cell_guid":"0d41f17e-568f-46b6-93ea-753d0ebdcfb1","_uuid":"0c1a82ecc0106e1097d491dfd3d732210b26c2d6"},"source":"df.Outcome.value_counts()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"df.SkinThickness.value_counts()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45263acf-274f-4206-8447-c0b320501ebc","_uuid":"f7da2a75dbcd7567f515230234c8b2798128c318"},"source":"### What is the mean of \"SkinThickness\" where the Outcome is 1?","cell_type":"markdown"},{"metadata":{"_cell_guid":"1e620f57-d59e-480a-8b33-bddb13c49f67","_uuid":"1590ed1b22aa690db1d857f8067b9f045ac7e525"},"source":"df[df.Outcome==1].SkinThickness.mean()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e55f905e-cf5c-4d6a-95b4-c7c220f44271","_uuid":"a60e9a25c97e972982a8366e1d88f085c3633bb5"},"source":"### EXERCISE: what is the maximum of \"BMI\" where the outcome is 0? ","cell_type":"markdown"},{"metadata":{"_cell_guid":"9e78a81a-b928-4d1f-9946-d1512917f867","_uuid":"a1fd29dd02e5c6abf16da7b961d99432a9287cbb"},"source":"# use the function .max()\ndf[df.Outcome==0].BMI.max()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"max(df[df.Outcome==0].BMI)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1736e25-67de-46fb-acf4-ac90041939ef","_uuid":"25d01df8d668126cdd4d30f6598e75afb0ab0793"},"source":"## Visualize the data: make scatter plots in 2 variables","cell_type":"markdown"},{"metadata":{"_cell_guid":"331cee73-eb41-453f-b388-b8c5e94176bb","_uuid":"a7e787e63fe6ba2f85d2e9a0b193b534ad83f549","collapsed":true},"source":"# get a plotting library\nimport matplotlib.pyplot as plt\n# make it interactive in the notebook\n%matplotlib inline","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21d9d3b2-2a41-42ab-97bd-f252b57f561c","_uuid":"647a3928cae82c66439a93077b7840d94e3a4017"},"source":"# plot Glucose vs BloodPressure and color points according to Outcome\nplt.figure()\nplt.scatter(df[df.Outcome==1].Glucose,df[df.Outcome==1].BloodPressure,label='Diabetes',color='r',s=2)\nplt.scatter(df[df.Outcome==0].Glucose,df[df.Outcome==0].BloodPressure,label='No Diabetes',color='b',s=2)\nplt.legend()\nplt.xlabel('Glucose')\nplt.ylabel('BloodPressure')","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fead5f6b-0e7b-4833-895e-3b7ac6ee32ef","_uuid":"93961d8086e694b267f956aeaa54710f8f64525d"},"source":"Notice first that already you can see a trend that higher glucose is associated with diabetes (Outcome=1, red points), while lower glucose is associated with no diabetes (Outcome=0, blue points).\n\nNotice also that there's a set of points with value 0 for Glucose, and another set with 0 for BloodPressure.  This doesn't make sense physically.  It looks like this data was filled with 0 when the value should have been NULL.  Let's check how many zeros appear in each column.","cell_type":"markdown"},{"metadata":{},"source":"df.columns","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"c='Pregnancies'\ndf[df[c]==0][c].count()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20a93ecf-0d7f-44ff-8efb-ad5147edf80c","_uuid":"519ebca3e20efff0a0849d63fea09f30d28345c5"},"source":"for c in df.columns:\n    print('For column',c,' there are',df[df[c]==0][c].count(),'zero values.')\n","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f2846db-cb07-4bed-b671-e440e0758bba","_uuid":"8132d76d6cd8fcb4c8fc04ab610fece35cde5e80"},"source":"For some of these columns, zero makes sense, like for Pregnancies and Outcome.  But for some of the others, like BloodPressure or BMI, zero definitely doesn't make sense.  Let's have a closer look at the data by making a histogram of the value of the data for each column. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"8f24890d-d9d3-4462-8168-ab49c8af32f7","_uuid":"ffd4b37712db39342a59fbb4d41c28a8ee2f62a2"},"source":"for c in df.columns:\n    plt.figure()\n    plt.hist(df[c],bins=15)\n    plt.xlabel(c)\n    plt.ylabel('frequency')\n    plt.show()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"745faf2f-0716-44d2-9dfe-dceaaae1e01f","_uuid":"e75612ee2a546a04c6205ea89aa08120146ea693"},"source":"From these histograms it seems that many of the zero values are indeed likely missing data which should have been labeled NULL, and will need to be considered before we train a model to classify the data.  \n\nAlso, if we go two cells back to where we printed the number of zeros in each column, we can also see that the column Insulin has 374 values (out of 768 rows total), almost 50% of the values, as zero.\n\nWhen we're ready to build a model, we will first drop (delete) the insulin column since so many of the values are missing.  Then we will impute (fill in) the zeros in the columns where zero doesn't make sense.  We will make the choice to use the mean (average) of the non-zero values in each column to impute the values that are zero. \n","cell_type":"markdown"},{"metadata":{"_cell_guid":"faced6ae-b534-489f-b480-d18bbc08aa32","_uuid":"b95266104aee37803afc7c4cc285c59e98668b2a"},"source":"### EXERCISE: inspect the data yourself by making scatter plots of different columns","cell_type":"markdown"},{"metadata":{"_cell_guid":"af973b17-9114-4ab0-a0d6-35c1adfbdfa4","_uuid":"89876f8e29d81b09b9a15d2601d7d48d643e4b68","collapsed":true},"source":"","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d96a2c27-e1cc-4c82-90ef-d748c2c77aba","_uuid":"305aa95e080599946640c60d91790ca6cff279fb"},"source":"### EXERCISE: find one feature column that does a pretty good job splitting the data on Outcome (the \"target\" or \"label\" column).  At which value would you split that feature column?","cell_type":"markdown"},{"metadata":{"_cell_guid":"22f952c5-c19b-4405-ae1f-56d8e3d939c6","_uuid":"3f1ea92a15b99405db5c9282e43333f0dc6ef9b8","collapsed":true},"source":"# example: plot histograms of Age for Outcome=1 and Outcome=0.\nplt.figure()\nplt.hist(df[df.Outcome==1]['Age'],bins=15,label='Diabetes',color='r',alpha=0.2)\nplt.hist(df[df.Outcome==0]['Age'],bins=15,label='No Diabetes',color='b',alpha=0.2)\nplt.xlabel('Age')\nplt.ylabel('frequency')\nplt.legend()\nplt.show()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a6f5bb1-107a-4236-b065-427b8a894981","_uuid":"304383d9b793b70dd591f77b2015c7ffdeea046a","collapsed":true},"source":"# choose a feature column, plot the histogram, and decide on a split value\n","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84d94146-8f19-4ad8-ad63-1016a1192603","_uuid":"5d71c428fa818ca08340304314dfdb158de88253"},"source":"### EXERCISE: how accurate is your classifier using just 1 split?","cell_type":"markdown"},{"metadata":{"_cell_guid":"04ffb823-f176-4cfa-a5ec-581e63d7e82d","_uuid":"839c2619fe68d7605404e615b367cb4f83d5c7eb","collapsed":true},"source":"# example\n# create a new column in the data frame with the predicted outcome based on your split (here, Age<30 means outcome=0, otherwise outcome=1)\ndf['PredictedOutcome']=np.where(df.Age<30,0,1) # np.where(condition, value if true, value if false)\n# calculate accuracy\nN_correct=df[df.PredictedOutcome==df.Outcome].shape[0]\nN_total=df.shape[0]\naccuracy=N_correct/N_total\nprint('number of correct examples =',N_correct)\nprint('number of examples in total =',N_total)\nprint('accuracy =',accuracy)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0a4e153-f2b7-4dca-91f9-670e62d3c9a8","_uuid":"ab54f8140c8db094e1cf5cbf81c8eedc4aa817fc"},"source":"We will discuss different ways of measuring the quality of a classifier in the next section.","cell_type":"markdown"},{"metadata":{"_cell_guid":"9f0fe2b7-e092-4dea-8c7d-d5ce4ec3745a","_uuid":"cd2b5bdd58a01c5d879ecf5a7952535c047b779b","collapsed":true},"source":"# now check the accuracy of your column and split\n# create a new column in the data frame with the predicted outcome based on your split\n# replace \"NONE\" with your code\ndf['PredictedOutcome']=np.where(df.NONE<NONE,0,1) # np.where(condition, value if true, value if false)\n# calculate accuracy\nN_correct=df[df.PredictedOutcome==df.Outcome].shape[0]\nN_total=df.shape[0]\naccuracy=N_correct/N_total\nprint('number of correct examples =',N_correct)\nprint('number of examples in total =',N_total)\nprint('accuracy =',accuracy)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c6b7690-4ef5-4898-9498-1e6aabc8c9c5","_uuid":"af0cc5d26b6aa49f812ad082458b0564a803cc93","collapsed":true},"source":"    ### Splitting data into TRAIN/TEST","cell_type":"markdown"},{"metadata":{"_cell_guid":"bdcc086f-cd9c-44c7-9ea0-8cd5d48719f9","_uuid":"095675e5326bbfb2dbb93fa72e2e1db79bceedf9"},"source":"import numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.3, random_state = 0)\n\ntrain.describe()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"daaf97ed-5df4-4bd2-be8e-497c238fbf77","_uuid":"d741b4c6655453b07be056765a3e103df7e669e2"},"source":"### CODE FOR DROPPING/IMPUTING DATA AFTER TRAIN/TEST SPLIT","cell_type":"markdown"},{"metadata":{"_cell_guid":"b36dc413-2003-4780-b960-9302be4b30fb","_uuid":"edc361a371c5377ae21cfa1f88def68032815623"},"source":"train.drop('Insulin',axis=1,inplace=True)\ntest.drop('Insulin',axis=1,inplace=True) # axis=1 means drop the column, not the row\n# check that Insulin is no longer in the list of columns\ntrain.columns","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9da0bb69-5298-433c-96d5-9d398084d4ba","_uuid":"9d1cba74a1d82b81068791a8bd73b780d4ebd8dd","collapsed":true},"source":"# numpy provides many useful functions, including allowing us to create new columns in our dataframe based on a condition\nimport numpy as np\n\ndef imputeColumns(dataset):\n    # create a list of columns that we will impute with the average non-zero value in each column\n    columnsToImpute=['Glucose', 'BloodPressure', 'SkinThickness','BMI']\n\n    for c in columnsToImpute:\n        avgOfCol=dataset[dataset[c]>0][[c]].mean()\n        dataset[c+'_imputed']=np.where(dataset[[c]]!=0,dataset[[c]],avgOfCol)\n\nimputeColumns(train)\nimputeColumns(test)\n# check that we've imputed the 0 values  \ntrain[train.Glucose==0][['Glucose','Glucose_imputed']].head()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1c70048-0d9a-4717-a689-7bb6f12ab2b7","_uuid":"c7246d747c679e82bfced449f9bece04584db23b","collapsed":true},"source":"### Extracting input features and output feature","cell_type":"markdown"},{"metadata":{"_cell_guid":"64781656-e62a-4102-b981-8da23e702ca3","_uuid":"836aaecfdd7c88aed5363ad22fd0964a88bdea46","collapsed":true},"source":"X_train = train[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness','BMI', 'DiabetesPedigreeFunction', 'Age']]\nY_train = train[['Outcome']]\nX_test = test[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\nY_test = test[['Outcome']]","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49f3b58b-2693-4491-b306-0c13ec690533","_uuid":"ed223b0092ce696ba89a153492709f45efe16809"},"source":"# Building Decision Trees for Classification","cell_type":"markdown"},{"metadata":{"_cell_guid":"523f74e9-e7df-4e82-a203-633ea5552174","_uuid":"de5c9ae754cb022d07bd9d75554cae40b367e7ce"},"source":"We are going to discuss the process of building a decision tree classifier for the diabetes problem. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n\nWe are building a model that is going to make predictions, so we need to find a way to evaluate the quality of these predictions in order to trust them. Since predictions by definition is for some unseen input, we cannot depend on the data that we used to create the model. We first need to divide the dataset into two non-intersecting parts: training data that is going to be used for building the model and test data for evaluating the model predictions.","cell_type":"markdown"},{"metadata":{"scrolled":true,"_cell_guid":"29397649-abd3-464e-9b91-924d9cd657ad","_uuid":"c61f5bac084dee53b80bfe292420e5924d5217fd"},"source":"Y_train.describe()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52488456-24c6-4391-94b5-efe0bbb450c4","_uuid":"6bf61667a79045f103337652abcff25408cfe632"},"source":"Y_test.describe()","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3de6aaec-ef78-43ce-91ac-201b0727b115","_uuid":"acc8197690c4515e0a374ee794a66dc506dde15f"},"source":"We are ready now to build our first classifier. We use the training data to build our decision tree model. Then we are going to evaluate its score using the test set. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"d403ede8-30db-4755-bfe9-95f4bbd9e1ad","_uuid":"8ce1eb84741e1d078f2f1cb9374ee6b2ee5bc206"},"source":"from sklearn.tree import DecisionTreeClassifier\n\n# Create the classifier\ndecision_tree_classifier = DecisionTreeClassifier(random_state = 0)\n\n# Train the classifier on the training set\ndecision_tree_classifier.fit(X_train, Y_train)\n\n# Evaluate the classifier on the testing set using classification accuracy\ndecision_tree_classifier.score(X_test, Y_test)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48647463-904e-4f8e-ad10-b12956a8ad89","_uuid":"bec80eecf11c5c78f571a50a8f8e8e2fed89d2fc"},"source":"#### Congratualtions! We got around 74% accuracy on our first classifier. Let us first visualize the decision tree built.","cell_type":"markdown"},{"metadata":{"_cell_guid":"bbbdb7c7-83a3-43e5-8d13-001dfcece785","_uuid":"f4722a63759f43bd36d8092d627ab690761a9778","collapsed":true},"source":"from sklearn import tree\n\ndot_file = tree.export_graphviz(decision_tree_classifier, out_file='tree.dot', \n                                feature_names = list(X_train),\n                                class_names = ['healthy', 'ill']) \n","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d534a090-33f6-4922-9cb2-c8de5bce36a8","_uuid":"e0866181be9f4395ae63b7bb8a4e124cca76be8a"},"source":"We noticed that the decision tree built is very deep and too complicated. This indicates that the model \nwill not be able to generalize well. This phenomenon is called overfitting. Mainly, the model memorizes\nthe training data and would have high accuracy on the training data but will perform badly on unseen ones.","cell_type":"markdown"},{"metadata":{"_cell_guid":"5b2d6494-bf6e-4d5d-ae5d-c30ea1cda9b2","_uuid":"7a831dbf9504938d2cc837821110fec48333cd6d"},"source":"print(\"Accuracy on training set: {:.3f}\".format(decision_tree_classifier.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(decision_tree_classifier.score(X_test, Y_test)))","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee148470-6f0e-4a36-b564-25d29bd92394","_uuid":"7cc04d385a2b74121ae290f9090051b4ca1b79eb"},"source":"import graphviz\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90eaef7c-bb49-4e32-b824-8e616ab99630","_uuid":"70fd631728e608036897b1539e6e4e5c7d4be264"},"source":"To avoid overfitting, we can attempt to reduce the complexity of the model. This can be done during building the model\n(pre-pruning) or after building it (post-pruning). Sklearn provide built-in functions to control pre-pruning like\nlimiting the depth of the model. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"38cb48f8-3bba-4341-befa-3bdb40d1bddc","_uuid":"afd2927988a679a649187d494338ce40146def96"},"source":"decision_tree_pruned = DecisionTreeClassifier(random_state = 0, max_depth = 4)\n\ndecision_tree_pruned.fit(X_train, Y_train)\ndecision_tree_pruned.score(X_test, Y_test)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{},"source":"print(\"Accuracy on training set: {:.3f}\".format(decision_tree_pruned.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(decision_tree_pruned.score(X_test, Y_test)))","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1afe3093-6da2-4df8-ae1f-46b2bda90771","_uuid":"2d8647078076022522360e6f7a9c5d65aa34884a"},"source":"pre_pruned_dot_file = tree.export_graphviz(decision_tree_pruned, out_file='pruned_tree.dot', \n                                feature_names = list(X_test),\n                                class_names = ['healthy', 'ill'])\nwith open(\"pruned_tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","cell_type":"code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06bd7b05-c1c0-48ae-b292-3c34cbb8bce6","_uuid":"b926078bfd3a0ef8e1d911925d21984a0bf9852f"},"source":"What other parameters can be used for pre-pruning? Experiment with different parameters and check how the results vary.\n\nhint: consult http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html","cell_type":"markdown"},{"metadata":{"_cell_guid":"c6652bac-37e1-4fe0-b52b-e9fea1b51595","_uuid":"88b886833e4922e423885ff6fa8d16a18adce4ae","collapsed":true},"source":"","cell_type":"code","execution_count":null,"outputs":[]}]}