{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dimension Reduction Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ncancer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.drop('Unnamed: 32', axis=1)\ncancer.diagnosis.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop 'id' column - godd practice to drop columns such as id, name, etc as they bear no fruit in model building.\nX = cancer.loc[:, ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']]\n\ny = cancer.loc[:, 'diagnosis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling of variables\nsc = StandardScaler()\nscaled_X = sc.fit_transform(X.values)\npd.DataFrame(scaled_X, columns=X.columns).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode target variable y\nle = LabelEncoder()\ny = le.fit_transform(y)\npd.DataFrame(y, columns=['diagnosis']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Principal Component Analysis\nThis is a unsupervised technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = decomposition.PCA()\npca.fit_transform(scaled_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see above that the dimensions after PCA has not reduced. We've got entire set of new features also called principal componenets.\nWe have to find which all components are actualy important. We can consider ony those components that give about 80% of information."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information content by all new indep variables\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#do a PCA plot and find the correct number of componenets from Elbo\ndf1 = pd.DataFrame({'Information':pca.explained_variance_ratio_,\n                    'PCs':['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10',\n                          'PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20',\n                          'PC21','PC22','PC23','PC24','PC25','PC26','PC27','PC28','PC29','PC30']})\n\nplt.figure(figsize = (25,6))\nsns.barplot(x = 'PCs',y = 'Information',data = df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"75% - 80% of information  - How many principal components is required for the same? We need to look for the elbow the curve.\nWe can clearly see that almost more than 80% of inforfmation is available with first four components. So, let's choose n_components=5 and apply PCA."},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will do PCA with only 5 components now as they seem to provide 80% of the information.\npca1 = decomposition.PCA(n_components=5)\npca_5var = pca1.fit_transform(scaled_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca1.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(pca1.explained_variance_ratio_)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 82% information is obtained in first 5 components."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X = pd.DataFrame(pca_5var,columns=['PC1','PC2','PC3','PC4','PC5'])\nnew_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(new_X.corr(), annot= True, fmt='.10f', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can see that there is no correlation among the principal components which is very important. We will now train LogisticRegression with these 5 components."},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's compare Logistic Regession with PCA\nnew_X_train, new_X_test, y_train, y_test = train_test_split(new_X, y, test_size = 0.2, random_state = 42)\n\nlogreg = LogisticRegression(solver='lbfgs')\nlogreg.fit(new_X_train, y_train)\n\ny_pred_test = logreg.predict(new_X_test)\nprint(confusion_matrix(y_test,y_pred_test))\nprint(accuracy_score(y_test,y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's compare Logistic Regession without PCA when we have all of the original features\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nlogreg2 = LogisticRegression(solver='lbfgs')\nlogreg2.fit(X_train, y_train)\n\ny_pred_test2 = logreg2.predict(X_test)\nprint(confusion_matrix(y_test, y_pred_test2))\nprint(accuracy_score(y_test, y_pred_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that accuracy is more when PCA is used compared to when it is not used."},{"metadata":{"trusted":true},"cell_type":"code","source":"# factor loading = PC loadings\npca1.components_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LDA ( Linear Discriminant Analysis)\nThis is a supervised technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LinearDiscriminantAnalysis()\nnew_X_train_lda = lda.fit_transform(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_X_train_lda_df = pd.DataFrame(new_X_train_lda,columns=['LDA1'])\nnew_X_train_lda_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg2 = LogisticRegression()\nlg2.fit(new_X_train_lda, y_train)\n\nnew_x_test_lda = lda.transform(X_test)\ny_test_pred_lda = lg2.predict(new_x_test_lda)\n\nprint(confusion_matrix(y_test, y_test_pred_lda))\nprint(accuracy_score(y_test, y_test_pred_lda))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see abobe that accuracy has come down. LDA will do wonder when the target is uniformly distributed. \n\n1 = 50% and 0 = 50% --- lda will do wonder\n\n1 = 90% and 0 = 10% --- lda will poorly perform****"},{"metadata":{},"cell_type":"markdown","source":"### References\n* https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/\n* https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/\n* https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}