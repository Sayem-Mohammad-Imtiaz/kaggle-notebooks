{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Preparation\nFirst step like always, preparing the data before being used. We will do this by using standard libraries *numpy* and *pandas*. ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:44.446216Z","iopub.execute_input":"2021-05-28T01:16:44.44665Z","iopub.status.idle":"2021-05-28T01:16:44.468509Z","shell.execute_reply.started":"2021-05-28T01:16:44.446567Z","shell.execute_reply":"2021-05-28T01:16:44.467301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then after that, we will review a bit about how the data look. By doing this, we can know a little bit more about how we are supposed to do with the data next. ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/emotion-classification-nlp/emotion-labels-train.csv')\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:44.470059Z","iopub.execute_input":"2021-05-28T01:16:44.470604Z","iopub.status.idle":"2021-05-28T01:16:44.521264Z","shell.execute_reply.started":"2021-05-28T01:16:44.470571Z","shell.execute_reply":"2021-05-28T01:16:44.520563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data only has text and label. We can't get any information out of this, except if we wanted to read each rows of the text. But we want to know more about the relation of text column and label column. So, we decided to do sentiment analysis. \n\n**Sentiment analysis** is the analysis to identify how sentimental the texts are. There are two types of sentimental analysis, sentimental analysis based on the *polarity* and sentimental analysis based on the *subjectivity*. Well, you can say that sentiment analysis is something that is very useful to identify how emotional the writer when they wrote the text.   ","metadata":{}},{"cell_type":"code","source":"import spacy\nimport re\nfrom textblob import TextBlob\n\nnlp = spacy.load('en_core_web_sm')\n\npolarity = lambda x: TextBlob(x).sentiment.polarity\nsubjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n\ndef polarity_and_subjectivity(x):\n    a = re.findall(r'[\\w\\d]+', x)\n    b = str()\n    for i in a:\n        b = b + str(' ') + str(i)\n    c = polarity(b)\n    d = subjectivity(b)\n    return c, d","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:44.522539Z","iopub.execute_input":"2021-05-28T01:16:44.522925Z","iopub.status.idle":"2021-05-28T01:16:48.110919Z","shell.execute_reply.started":"2021-05-28T01:16:44.522898Z","shell.execute_reply":"2021-05-28T01:16:48.109834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pols = []\nsubs = []\n\nfor t in train.text:\n    pol, sub = polarity_and_subjectivity(t)\n    pols.append(pol)\n    subs.append(sub)\n    \ntrain['polarity'] = pols\ntrain['subjectivity'] = subs","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:48.112258Z","iopub.execute_input":"2021-05-28T01:16:48.112571Z","iopub.status.idle":"2021-05-28T01:16:50.142213Z","shell.execute_reply.started":"2021-05-28T01:16:48.112543Z","shell.execute_reply":"2021-05-28T01:16:50.141422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:50.143392Z","iopub.execute_input":"2021-05-28T01:16:50.143813Z","iopub.status.idle":"2021-05-28T01:16:50.156704Z","shell.execute_reply.started":"2021-05-28T01:16:50.143771Z","shell.execute_reply":"2021-05-28T01:16:50.155741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right now, we got the data for polarity and subjectivity of each tweets. ","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n## Polarity and Subjectivity of Each Emotions","metadata":{}},{"cell_type":"markdown","source":"After getting the polarity and subjectivity of each tweets, we will analyze all these values based on the emotions they are related to. We will see, how emotions affect the polarity and subjectivity. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(5, 20))\nsns.relplot(data=train, x='polarity', y='subjectivity', row='label')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:50.158071Z","iopub.execute_input":"2021-05-28T01:16:50.158378Z","iopub.status.idle":"2021-05-28T01:16:51.127043Z","shell.execute_reply.started":"2021-05-28T01:16:50.158321Z","shell.execute_reply":"2021-05-28T01:16:51.125969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot above, not so many things that we can take off because of the difference that is not so significant. If we see slightly, it almost like all of these plots don't have difference at all. Only when looked at this closely we finally realize that there are small difference that actually could be meaningful.\n\n**Joy Emotions**\nThe people that are in joy emotion, they tend to be more positive. This positivity can be subjective though, it feels like the joy came from their own character, not necessarily because of what happen in their life. You can see that there are more positive polarity than the negative. When the subjectivity is 1, there are also more positve polarity.\n\n**Fear Emotions**\nIn fear, people tend to more negative. Sometime, they can have positivity (remember, fear down't always relate to negativity), that's when they are less subjective, or I would say more objective.  At the scatter plot, you can see that most of the data are located on the center bottom, but there are more negative data than the positive data, signify that fear mostly comes from negativity.\n\n**Anger Emotions**\nIn anger emotions, there are no particular pattern. You can be postive and angry, but you also can be negative and angry. The reason why someone became angry can also be sujective and negative. \n\n**Sad Emotions**\nIn sad, we can be more negative. It showed at the plot, that there are slightly more data at the left side of the plot. ","metadata":{}},{"cell_type":"markdown","source":"# Data Processing\n\nNext thing we need to do is processing the data, so then we can feed them to the machine learning model that we will build. Because this is a text data, we will be using **regular expression** and **spacy**, a library in python that is specialized to analyze text. ","metadata":{}},{"cell_type":"code","source":"def words_reduction(x):\n    a = re.findall(r'[\\w\\d]+', x)\n    b = str()\n    for i in a:\n        b = b + str(' ') + str(i)\n    return b\n\ndef searching_vector(x):\n    nlp_x = nlp(x)\n    vec_x = nlp_x.vector\n    if len(vec_x) == 0:\n        vec_x = np.zeros([1, 96])\n    return vec_x\n\ndef get_words_vector(x):\n    text_vector = searching_vector(words_reduction(x[0]))\n    for t in x[1:]:\n        new_vector = searching_vector(words_reduction(t))\n        text_vector = np.vstack((text_vector, new_vector))\n    return text_vector\n\ntrain_text_vector = get_words_vector(train.text)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:16:51.129267Z","iopub.execute_input":"2021-05-28T01:16:51.129608Z","iopub.status.idle":"2021-05-28T01:17:26.891588Z","shell.execute_reply.started":"2021-05-28T01:16:51.129577Z","shell.execute_reply":"2021-05-28T01:17:26.890613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our objective in this project is to understand the emotions more and to build a model that can predict the emotion of a text or tweet specifically. If you wanna build a machine learning model that can predict something, yoiu can't use text as input. The text has to be preprocessed first. Turning it into numbers is the most common option. \n\nWe can change this tweet from text to collection of numbers or vector. This vector is basically the center of every words vector on the tweet (each word has their own vector). After all of this process, the text data tha tight now in a form of vector can be useful for a machine learning model.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ntarget_transformer = LabelEncoder()\ntrain_label_transformed = target_transformer.fit_transform(np.array(train.label).reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:26.893304Z","iopub.execute_input":"2021-05-28T01:17:26.893608Z","iopub.status.idle":"2021-05-28T01:17:26.90337Z","shell.execute_reply.started":"2021-05-28T01:17:26.893581Z","shell.execute_reply":"2021-05-28T01:17:26.901862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid = pd.read_csv('/kaggle/input/emotion-classification-nlp/emotion-labels-val.csv')\nvalid.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:26.904839Z","iopub.execute_input":"2021-05-28T01:17:26.905259Z","iopub.status.idle":"2021-05-28T01:17:26.934726Z","shell.execute_reply.started":"2021-05-28T01:17:26.905216Z","shell.execute_reply":"2021-05-28T01:17:26.933895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First Try","metadata":{}},{"cell_type":"code","source":"valid_text_vector = get_words_vector(valid.text)\nvalid_label_transformed = target_transformer.transform(np.array(valid.label).reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:26.935707Z","iopub.execute_input":"2021-05-28T01:17:26.936111Z","iopub.status.idle":"2021-05-28T01:17:30.305855Z","shell.execute_reply.started":"2021-05-28T01:17:26.936075Z","shell.execute_reply":"2021-05-28T01:17:30.30492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:30.306974Z","iopub.execute_input":"2021-05-28T01:17:30.307261Z","iopub.status.idle":"2021-05-28T01:17:30.448809Z","shell.execute_reply.started":"2021-05-28T01:17:30.307233Z","shell.execute_reply":"2021-05-28T01:17:30.447872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [LogisticRegression(), GaussianNB(), SVC(), KNeighborsClassifier(),\n          DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier()]\nmodel_names = ['Logistic Regression', 'Gaussian Naive-Bayes', 'SVC', 'K-Neighbors Classifier',\n               'Decision Tree Classifier', 'Random Forest Classifier', 'Ada Boost Classifier']\nscores = np.zeros([len(models), 1])\n\nfor i, m in enumerate(models):\n    clf =  m.fit(train_text_vector, train_label_transformed)\n    scores[i] = clf.score(valid_text_vector, valid_label_transformed)\n    \nscores = pd.DataFrame(scores, columns=['score'], index=model_names)\nscores","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:30.450356Z","iopub.execute_input":"2021-05-28T01:17:30.450795Z","iopub.status.idle":"2021-05-28T01:17:38.999395Z","shell.execute_reply.started":"2021-05-28T01:17:30.450753Z","shell.execute_reply":"2021-05-28T01:17:38.998379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are using several machine learning models here to find out which model that has the best performance. The best one that we can get will be used for further use. \n\nFrom the table above, you can see that there is no model that has a satisfying performance. The highest score that we can get is 34%, which means when this model predict they tend to have more mistakes than the right predictions. From this take, we know that we need to make some adjustments. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\noh_transformer = OneHotEncoder()\ntrain_oh_label = oh_transformer.fit_transform(np.array(train.label).reshape(-1, 1))\ntrain_oh_label = train_oh_label.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:39.000938Z","iopub.execute_input":"2021-05-28T01:17:39.001338Z","iopub.status.idle":"2021-05-28T01:17:39.009486Z","shell.execute_reply.started":"2021-05-28T01:17:39.001295Z","shell.execute_reply":"2021-05-28T01:17:39.008373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4, activation='relu'))\nmodel.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:39.011142Z","iopub.execute_input":"2021-05-28T01:17:39.011511Z","iopub.status.idle":"2021-05-28T01:17:45.320774Z","shell.execute_reply.started":"2021-05-28T01:17:39.011477Z","shell.execute_reply":"2021-05-28T01:17:45.319792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_text_vector, train_oh_label, epochs=30, batch_size=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:45.321901Z","iopub.execute_input":"2021-05-28T01:17:45.322342Z","iopub.status.idle":"2021-05-28T01:17:50.083271Z","shell.execute_reply.started":"2021-05-28T01:17:45.3223Z","shell.execute_reply":"2021-05-28T01:17:50.082283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also tried to use deep learning which gives another bad results. ","metadata":{}},{"cell_type":"markdown","source":"## Second Try\n\nThis time, we would try to eliminate some of the words from the tweets. Our assumption is, there are so many words that are not important in the tweets, so it moved the vector to certain position that is not really represent the meaning of the tweet. We would only choose:\n1. Adjective\n2. Verb\n3. Noun\n\nWe believe that these are part of the sentences or in this case tweet, that give meaning. By understanding these three parts, we could easily understand the whole meaning of a sentence. Especially if we fed this to a machine, which has no intuition about grammar.  ","metadata":{}},{"cell_type":"code","source":"def get_the_parts(txt):\n    long = len(txt)\n    important = ['ADJ', 'VERB', 'NOUN']\n    the_parts = []\n    for t in txt:\n        if t.pos_ in important:\n            the_parts.append(str(t))\n    parts_text = str()\n    for t in the_parts:\n        parts_text = parts_text + ' ' + t\n    return parts_text\n\nnew_texts = [get_the_parts(nlp(words_reduction(x))) for x in train.text]\nnew_vec = get_words_vector(new_texts)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:17:50.084603Z","iopub.execute_input":"2021-05-28T01:17:50.084891Z","iopub.status.idle":"2021-05-28T01:18:53.735942Z","shell.execute_reply.started":"2021-05-28T01:17:50.084864Z","shell.execute_reply":"2021-05-28T01:18:53.734891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_valids = [get_the_parts(nlp(words_reduction(x))) for x in valid.text]\nnew_valids_vec = get_words_vector(new_valids)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:18:53.737368Z","iopub.execute_input":"2021-05-28T01:18:53.737813Z","iopub.status.idle":"2021-05-28T01:18:59.777737Z","shell.execute_reply.started":"2021-05-28T01:18:53.73777Z","shell.execute_reply":"2021-05-28T01:18:59.777004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_2 = np.zeros([len(models), 1])\n\nfor i, m in enumerate(models):\n    clf =  m.fit(new_vec, train_label_transformed)\n    scores_2[i] = clf.score(new_valids_vec, valid_label_transformed)\n    \nscores_2 = pd.DataFrame(scores_2, columns=['score'], index=model_names)\nscores_2","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:18:59.780807Z","iopub.execute_input":"2021-05-28T01:18:59.781431Z","iopub.status.idle":"2021-05-28T01:19:08.283416Z","shell.execute_reply.started":"2021-05-28T01:18:59.781369Z","shell.execute_reply":"2021-05-28T01:19:08.282661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Third Try\n\nEven after being selective with the words to proceed, we still have the same struggle. We don't think that we can make a good model by using word embeddings for this case. So, we just think about a more simple solution which is using **count vectorizer**. Well word embedding itself is already a vectorizer, but this time the vectorizer is specified only for this use, while word embedding is a word with its meaning but in a form of vector.  \n","metadata":{}},{"cell_type":"code","source":"sketch = new_texts\nno_space = lambda x: x.lower()\nnew_texts = [no_space(x) for x in sketch]\n\nsketch_valid = [words_reduction(x) for x in valid.text]\nnew_valid = [no_space(x) for x in sketch_valid]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:19:08.28467Z","iopub.execute_input":"2021-05-28T01:19:08.285105Z","iopub.status.idle":"2021-05-28T01:19:08.297221Z","shell.execute_reply.started":"2021-05-28T01:19:08.285061Z","shell.execute_reply":"2021-05-28T01:19:08.296484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\ntrain_text_cv = cv.fit_transform(new_texts)\nvalid_text_cv = cv.transform(new_valid)\n\ntrain_text_cv = train_text_cv.toarray() \nvalid_text_cv = valid_text_cv.toarray() ","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:19:08.298521Z","iopub.execute_input":"2021-05-28T01:19:08.298827Z","iopub.status.idle":"2021-05-28T01:19:08.425712Z","shell.execute_reply.started":"2021-05-28T01:19:08.298798Z","shell.execute_reply":"2021-05-28T01:19:08.4247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_3 = np.zeros([len(models), 1])\n\nfor i, m in enumerate(models):\n    clf =  m.fit(train_text_cv, train_label_transformed)\n    scores_3[i] = clf.score(valid_text_cv, valid_label_transformed)\n    \nscores_3 = pd.DataFrame(scores_3, columns=['score'], index=model_names)\nscores_3","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:19:08.427201Z","iopub.execute_input":"2021-05-28T01:19:08.427549Z","iopub.status.idle":"2021-05-28T01:21:49.395057Z","shell.execute_reply.started":"2021-05-28T01:19:08.427519Z","shell.execute_reply":"2021-05-28T01:21:49.393744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this, we can get the accuracy of 82% which came from Logistic Regression Performance. ","metadata":{}},{"cell_type":"markdown","source":"# Summary\n\n1. Not so many distinctions can be seen by sentiment analysis from many different emotions, whether it is anger, fear, sad, or joy. \n2. Building a model that can classify emotions of tweets, from this project, we know that **word embeddings** is not a good idea. It is better to just use **count vectorizer**.\n","metadata":{}}]}