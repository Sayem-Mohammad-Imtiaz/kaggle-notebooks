{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spotify recommandation","metadata":{}},{"cell_type":"markdown","source":"From statistics about liked and disliked songs , we're going to create a model to predict wether I like a song or not\n\nSummary :\n\n1. Data Collection\n2. Data Features\n3. Data Cleaning\n4. Exploratory Data analysis\n5. Modelling\n6. Testing on new data","metadata":{}},{"cell_type":"markdown","source":"## 1. Data Collection\n\n### 1.1 Playlist creation\nI collected 100 liked songs and 95 disliked songs\n\nFor those I like , I made a [playlist](https://open.spotify.com/playlist/2WONKi3eZaR29QaQCRSiAE?si=a2463f1d382f4399) of my favorite 100 songs. It is mainly French Rap , sometimes American rap , rock or electro music.\n\nFor those I dislike , I collected songs from various kind of music so the model will have a broader view of what I don't like\n\nThere is :\n- [25 metal songs ( Cannibal Corps )](https://open.spotify.com/playlist/37i9dQZF1DZ06evO0grpKg?si=3c829a46465d4367)\n- [20 \" I don't like \" rap songs ( PNL )](https://open.spotify.com/playlist/37i9dQZF1DX2fxPY4lXxv8?si=c69f40a2a2014a25)\n- [25 classical songs](https://open.spotify.com/playlist/1h0CEZCm6IbFTbxThn6Xcs?si=933db0752a684db0)\n- [25 Disco songs](https://open.spotify.com/playlist/2rkU3Aop33atDJoF8LCCjh?si=5e1247ee29284f0a)\n\nI didn't include any Pop song because I'm kinda neutral about it\n\n### 1.2 Getting the ID's\n\n1. From the [Spotify's API \"Get a playlist's Items\"](https://developer.spotify.com/console/get-playlist-tracks/) , I turned the playlists into json formatted data which cointains the ID and the name of each track ( ids/yes.py and ids/no.py ). NB : on the website , specify \"items(track(id,name))\" in the fields format , to avoid being overwhelmed by useless data.\n\n2. With a script ( ids/ids_to_data.py ) , I turned the json data into a long string with each ID separated with a comma.\n\n### 1.3 Getting the statistics\n\nNow I just had to enter the strings into the [Spotify API \"Get Audio Features from several tracks\"](https://developer.spotify.com/console/get-audio-features-several-tracks/) and get my data files ( data/good.json and data/dislike.json )","metadata":{}},{"cell_type":"markdown","source":"## 2. Data features\n\nFrom [Spotify's API documentation](https://developer.spotify.com/documentation/web-api/reference/#object-audiofeaturesobject) :\n\n* **acousticness** : A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n* **danceability** : Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n* **duration_ms** : The duration of the track in milliseconds.\n* **energy** : Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n* **instrumentalness** : Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n* **key** : The key the track is in. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on.\n* **liveness** : Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n* **loudness** : The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n* **mode** : Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n* **speechiness** : Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n* **tempo** : The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n* **time_signature** : An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).\n* **valence** : A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\nAnd the variable that has to be predicted :\n\n* **liked** : 1 for liked songs , 0 for disliked songs","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Cleaning\n\nWe're going to :\n* Take each json files \n* Turn them into a dataframe\n* Add a \"Liked\" column\n* Drop useless columns\n* Shuffle them ( it's somewhat better for learning )\n* Save it as a csv file","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport json","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:40.968858Z","iopub.execute_input":"2021-07-27T20:23:40.969809Z","iopub.status.idle":"2021-07-27T20:23:40.974763Z","shell.execute_reply.started":"2021-07-27T20:23:40.969756Z","shell.execute_reply":"2021-07-27T20:23:40.973868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/spotify-recommendation/good.json\",\"r\") as f:\n    liked = json.load(f)\nliked = pd.DataFrame(liked[\"audio_features\"])\nliked","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:40.976509Z","iopub.execute_input":"2021-07-27T20:23:40.977229Z","iopub.status.idle":"2021-07-27T20:23:41.03444Z","shell.execute_reply.started":"2021-07-27T20:23:40.977163Z","shell.execute_reply":"2021-07-27T20:23:41.033369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/spotify-recommendation/dislike.json\",\"r\") as f:\n    disliked = json.load(f)\ndisliked = pd.DataFrame(disliked[\"audio_features\"])\ndisliked","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.036118Z","iopub.execute_input":"2021-07-27T20:23:41.036455Z","iopub.status.idle":"2021-07-27T20:23:41.079627Z","shell.execute_reply.started":"2021-07-27T20:23:41.036423Z","shell.execute_reply":"2021-07-27T20:23:41.07858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add the \"Liked\" column","metadata":{}},{"cell_type":"code","source":"liked[\"liked\"] = [1] * 100\ndisliked[\"liked\"] = [0] * 95","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.081176Z","iopub.execute_input":"2021-07-27T20:23:41.081517Z","iopub.status.idle":"2021-07-27T20:23:41.087693Z","shell.execute_reply.started":"2021-07-27T20:23:41.081488Z","shell.execute_reply":"2021-07-27T20:23:41.086517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"liked","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.089166Z","iopub.execute_input":"2021-07-27T20:23:41.089768Z","iopub.status.idle":"2021-07-27T20:23:41.139835Z","shell.execute_reply.started":"2021-07-27T20:23:41.089721Z","shell.execute_reply":"2021-07-27T20:23:41.138813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disliked","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.141326Z","iopub.execute_input":"2021-07-27T20:23:41.141925Z","iopub.status.idle":"2021-07-27T20:23:41.182263Z","shell.execute_reply.started":"2021-07-27T20:23:41.14188Z","shell.execute_reply":"2021-07-27T20:23:41.181513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([liked,disliked])\ndata","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.183454Z","iopub.execute_input":"2021-07-27T20:23:41.183752Z","iopub.status.idle":"2021-07-27T20:23:41.227376Z","shell.execute_reply.started":"2021-07-27T20:23:41.183724Z","shell.execute_reply":"2021-07-27T20:23:41.226461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop useless columns\n\nWe're going to drop things like id's , url's ...","metadata":{}},{"cell_type":"code","source":"data.drop([\"type\",\"id\",\"uri\",\"track_href\",\"analysis_url\"],axis=1,inplace=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.229847Z","iopub.execute_input":"2021-07-27T20:23:41.230161Z","iopub.status.idle":"2021-07-27T20:23:41.262041Z","shell.execute_reply.started":"2021-07-27T20:23:41.230132Z","shell.execute_reply":"2021-07-27T20:23:41.260833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shuffle rows\n\nIf you don't do it , the model will somewhat think they only have to learn what is a liked song because they'll only see them at the beginning \n\nSuffling prevents this from happening","metadata":{}},{"cell_type":"code","source":"data = data.sample(frac=1)\ndata","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.263842Z","iopub.execute_input":"2021-07-27T20:23:41.264171Z","iopub.status.idle":"2021-07-27T20:23:41.294996Z","shell.execute_reply.started":"2021-07-27T20:23:41.26413Z","shell.execute_reply":"2021-07-27T20:23:41.294014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the dataframe as a csv file","metadata":{}},{"cell_type":"code","source":"try :\n    data = pd.read_csv(\"../input/spotify-recommendation/data.csv\")\n    print(\"Loading file...\")\nexcept :\n    data.to_csv(\"../input/spotify-recommendation/data.csv\",index=False)\n    print(\"Saving file...\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.296337Z","iopub.execute_input":"2021-07-27T20:23:41.296838Z","iopub.status.idle":"2021-07-27T20:23:41.306652Z","shell.execute_reply.started":"2021-07-27T20:23:41.296805Z","shell.execute_reply":"2021-07-27T20:23:41.305633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Exploratory Data Analysis\n\nAs all figures are integers or digits , we're just going to see the correlation between them and the liked column","metadata":{}},{"cell_type":"code","source":"# import the library we're going to use\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.308049Z","iopub.execute_input":"2021-07-27T20:23:41.3086Z","iopub.status.idle":"2021-07-27T20:23:41.318567Z","shell.execute_reply.started":"2021-07-27T20:23:41.308553Z","shell.execute_reply":"2021-07-27T20:23:41.317382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = data.corr()[[\"liked\"]]\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(\n    corr, \n    annot=True,\n    ax=ax\n);","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.319867Z","iopub.execute_input":"2021-07-27T20:23:41.320162Z","iopub.status.idle":"2021-07-27T20:23:41.732379Z","shell.execute_reply.started":"2021-07-27T20:23:41.320126Z","shell.execute_reply":"2021-07-27T20:23:41.731668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the figures , I'm very likely...\n* To like ... songs :\n    * danceable\n    * high energy\n    * loud\n    * with many words\n    * fast\n    * with high amount of beats\n    * slightly positive \n* To dislike ... songs :\n    * not very accoustic\n    * with low instrumentalness\n    * short ","metadata":{}},{"cell_type":"markdown","source":"## 5. Modelling","metadata":{}},{"cell_type":"markdown","source":"For this , we're going to try several models:\n* SVC with RBF kernel\n* Random Forest Classifier\n* KNN Classifier","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Initial modelling","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split , GridSearchCV\nfrom sklearn.metrics import accuracy_score , log_loss , roc_auc_score ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.733614Z","iopub.execute_input":"2021-07-27T20:23:41.733881Z","iopub.status.idle":"2021-07-27T20:23:41.738728Z","shell.execute_reply.started":"2021-07-27T20:23:41.733855Z","shell.execute_reply":"2021-07-27T20:23:41.737745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(y_true,y_pred):\n    return accuracy_score(y_true , y_pred) , log_loss(y_true , y_pred ) , roc_auc_score(y_true , y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.740175Z","iopub.execute_input":"2021-07-27T20:23:41.740613Z","iopub.status.idle":"2021-07-27T20:23:41.751826Z","shell.execute_reply.started":"2021-07-27T20:23:41.740571Z","shell.execute_reply":"2021-07-27T20:23:41.750825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X , y = data.drop(\"liked\",axis=1) , data.liked\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.753151Z","iopub.execute_input":"2021-07-27T20:23:41.753514Z","iopub.status.idle":"2021-07-27T20:23:41.766333Z","shell.execute_reply.started":"2021-07-27T20:23:41.753482Z","shell.execute_reply":"2021-07-27T20:23:41.765293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\n\nsvc = SVC(kernel=\"rbf\")\nsvc.fit(X_train,y_train)\ny_svc_pred = svc.predict(X_test)\n\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_rf_pred = rf.predict(X_test)\n\nknn = KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_knn_pred = knn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.767804Z","iopub.execute_input":"2021-07-27T20:23:41.768254Z","iopub.status.idle":"2021-07-27T20:23:41.994375Z","shell.execute_reply.started":"2021-07-27T20:23:41.768211Z","shell.execute_reply":"2021-07-27T20:23:41.993345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_svc , loss_svc , auc_svc = evaluation(y_test , y_svc_pred)\nclass_rf , loss_rf , auc_rf = evaluation(y_test ,y_rf_pred)\nclass_knn , loss_knn , auc_knn = evaluation(y_test , y_knn_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:41.995715Z","iopub.execute_input":"2021-07-27T20:23:41.996036Z","iopub.status.idle":"2021-07-27T20:23:42.009212Z","shell.execute_reply.started":"2021-07-27T20:23:41.996004Z","shell.execute_reply":"2021-07-27T20:23:42.008086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = {\n    \"SVC\":{\n        \"Accuracy\":class_svc,\n        \"Loss\":loss_svc,\n        \"AUC\":auc_svc\n    },\n    \"Random Forest\":{\n        \"Accuracy\":class_rf,\n        \"Loss\":loss_rf,\n        \"AUC\":auc_rf\n    },\n    \"KNN\":{\n        \"Accuracy\":class_knn,\n        \"Loss\":loss_knn,\n        \"AUC\":auc_knn\n    }\n}\nscores = pd.DataFrame(scores)\nscores","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:42.010794Z","iopub.execute_input":"2021-07-27T20:23:42.011286Z","iopub.status.idle":"2021-07-27T20:23:42.027878Z","shell.execute_reply.started":"2021-07-27T20:23:42.011236Z","shell.execute_reply":"2021-07-27T20:23:42.026828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores.drop(\"Loss\").plot.bar();","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:42.02931Z","iopub.execute_input":"2021-07-27T20:23:42.029937Z","iopub.status.idle":"2021-07-27T20:23:42.221342Z","shell.execute_reply.started":"2021-07-27T20:23:42.029887Z","shell.execute_reply":"2021-07-27T20:23:42.220062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores.drop([\"Accuracy\",\"AUC\"]).plot.bar();","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:42.222898Z","iopub.execute_input":"2021-07-27T20:23:42.223305Z","iopub.status.idle":"2021-07-27T20:23:42.391036Z","shell.execute_reply.started":"2021-07-27T20:23:42.22327Z","shell.execute_reply":"2021-07-27T20:23:42.390278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With such results , I'm not even going to try to hyperparameter tune the other models","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# as i've already run it locally , I didn't include all my attempts to limit running time\nnp.random.seed(42)\n\nrf = RandomForestClassifier(n_jobs=-1)\nrf_grid = {\n 'max_depth': [10,15],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1,3],\n 'min_samples_split': [2,4],\n 'n_estimators': [10,42,100]\n}\nrf_cv = GridSearchCV(rf,rf_grid,verbose=2,cv=3)\nrf_cv.fit(X_train , y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:23:42.392356Z","iopub.execute_input":"2021-07-27T20:23:42.392943Z","iopub.status.idle":"2021-07-27T20:24:15.212181Z","shell.execute_reply.started":"2021-07-27T20:23:42.392895Z","shell.execute_reply":"2021-07-27T20:24:15.210981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cv.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:24:15.2141Z","iopub.execute_input":"2021-07-27T20:24:15.214575Z","iopub.status.idle":"2021-07-27T20:24:15.221484Z","shell.execute_reply.started":"2021-07-27T20:24:15.214525Z","shell.execute_reply":"2021-07-27T20:24:15.220653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_pred = rf_cv.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:24:15.224706Z","iopub.execute_input":"2021-07-27T20:24:15.22502Z","iopub.status.idle":"2021-07-27T20:24:15.34111Z","shell.execute_reply.started":"2021-07-27T20:24:15.224991Z","shell.execute_reply":"2021-07-27T20:24:15.340113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_acc , cv_loss , cv_auc = evaluation(y_test,cv_pred)\nprint(cv_acc , cv_loss , cv_auc)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:24:15.344119Z","iopub.execute_input":"2021-07-27T20:24:15.344512Z","iopub.status.idle":"2021-07-27T20:24:15.353141Z","shell.execute_reply.started":"2021-07-27T20:24:15.344478Z","shell.execute_reply":"2021-07-27T20:24:15.352282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp = {\n    \"Old\":{\n        \"Accuracy\":class_rf,\n        \"Loss\":loss_rf,\n        \"AUC\":auc_rf\n    },\n    \"CV\":{\n        \"Accuracy\":cv_acc,\n        \"Loss\":cv_loss,\n        \"AUC\":cv_auc\n    }\n}\ncomp = pd.DataFrame(comp)\ncomp","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:24:15.354568Z","iopub.execute_input":"2021-07-27T20:24:15.354895Z","iopub.status.idle":"2021-07-27T20:24:15.373759Z","shell.execute_reply.started":"2021-07-27T20:24:15.354865Z","shell.execute_reply":"2021-07-27T20:24:15.37274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp.drop(\"Loss\").plot.bar();","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:24:15.375218Z","iopub.execute_input":"2021-07-27T20:24:15.375609Z","iopub.status.idle":"2021-07-27T20:24:15.545586Z","shell.execute_reply.started":"2021-07-27T20:24:15.375564Z","shell.execute_reply":"2021-07-27T20:24:15.544572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp.drop([\"Accuracy\",\"AUC\"]).plot.bar();","metadata":{"execution":{"iopub.status.busy":"2021-07-27T20:24:15.546759Z","iopub.execute_input":"2021-07-27T20:24:15.54705Z","iopub.status.idle":"2021-07-27T20:24:15.713788Z","shell.execute_reply.started":"2021-07-27T20:24:15.547021Z","shell.execute_reply":"2021-07-27T20:24:15.71283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I don't really understand why it didn't improve , it did on my side with the same code**","metadata":{}},{"cell_type":"code","source":"# Reimporting libraries in case I just want to run this cell\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport os\nimport json\nimport urllib.parse\n\ndata = pd.read_csv(\"../input/spotify-recommendation/data.csv\")\nX , y = data.drop(\"liked\",axis=1) , data.liked\n\ntry :\n    model = pickle.load(open(\"./model.sav\", 'rb'))\nexcept:\n    model = RandomForestClassifier(n_jobs=-1,\n                                  max_depth=15,\n                                  min_samples_leaf=1,\n                                  min_samples_split=4,\n                                  n_estimators=42)\n\n    model.fit(X,y)\n\n    pickle.dump(model, open(\"./model.sav\", 'wb'))\n\ntoken = input(\"\"\" Spotify token :\n\nTo create one , visit this page : https://developer.spotify.com/console/get-several-tracks/\n\nLog in to your spotify Account , and then copy what's in \"OAuth Token\" field \"\"\")\nquery = input(\"\\n\\n\\nName of the track and artist ( be careful the database is somewhat capricious ) : \")\n\nquery = urllib.parse.quote(query)\nstream = os.popen(f'curl -X \"GET\" \"https://api.spotify.com/v1/search?q={query}&type=track\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer {token}\"')\ndata = stream.read()\ntry :\n    data = json.loads(data)[\"tracks\"][\"items\"][0]\n    song_id = data[\"id\"]\n    artist = data[\"artists\"][0][\"name\"]\n    title = data[\"name\"]\n    stream = os.popen(f'curl -X \"GET\" \"https://api.spotify.com/v1/audio-features/{song_id}\" -H \"Accept: application/json\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer {token}\"')\n    data = stream.read()\n    data = json.loads(data)\n    data = pd.DataFrame(data,index=[0])\n    data.drop([\"type\",\"id\",\"uri\",\"track_href\",\"analysis_url\"],axis=1,inplace=True)\n    print(f\"\\n\\n\\n\\nThere is {list(model.predict_proba(data)[0])[1]*100:.2f}% chance that Brice likes \\\"{title}\\\" by {artist}\\n\\n\\n\")\nexcept KeyError:\n    print(\"\\n\\n\\nYour token has expired , create a new one : https://developer.spotify.com/console/get-several-tracks/\\n\\n\\n\")\nexcept IndexError:\n    print(\"\\n\\n\\nWe didn't find the song you were looking for\\n\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:59:29.205066Z","iopub.execute_input":"2021-07-28T07:59:29.205622Z","iopub.status.idle":"2021-07-28T07:59:38.861107Z","shell.execute_reply.started":"2021-07-28T07:59:29.205506Z","shell.execute_reply":"2021-07-28T07:59:38.859942Z"},"trusted":true},"execution_count":null,"outputs":[]}]}