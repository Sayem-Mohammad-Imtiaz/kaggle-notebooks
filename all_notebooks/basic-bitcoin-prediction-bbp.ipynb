{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"fc9eaf08-5c20-4943-9e14-5c364be1b6cb","_cell_guid":"07e948fd-f4bc-4a40-9116-ca1f0af9675f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:03:49.227636Z","iopub.execute_input":"2021-08-13T01:03:49.228016Z","iopub.status.idle":"2021-08-13T01:03:49.250516Z","shell.execute_reply.started":"2021-08-13T01:03:49.227933Z","shell.execute_reply":"2021-08-13T01:03:49.249648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Make Predict Close price per ten minutes**\n1. **Load Data from bitcoin-historical-data**\n2. **Using Open price and Volumes to predict Close price**\n3. **Re-build our dataframe to fit our purpose(num-2)**\n4. **Making dataset by using WindowGenerator to predict time-sequences**\n5. **Build a model with TensorFlow(Conv1D and LSTM)**\n6. **Lets Predict!**","metadata":{"_uuid":"027b990d-e5e5-43ea-9d05-4cc589ce52c6","_cell_guid":"34ca54f8-adcc-47c1-a6e7-55a3e85be006","trusted":true}},{"cell_type":"code","source":"!pip install -q tensorflow-gpu\nimport tensorflow as tf\nprint(\"GPU\", \"Positive\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"Negative\" )\nprint(\"version : \",tf.__version__)","metadata":{"_uuid":"3c97b738-0a8d-4c59-ae83-3973fa3d1299","_cell_guid":"d3416a0f-06fe-412e-98fe-845583ec9c1e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:03:49.254616Z","iopub.execute_input":"2021-08-13T01:03:49.254913Z","iopub.status.idle":"2021-08-13T01:04:02.418522Z","shell.execute_reply.started":"2021-08-13T01:03:49.254885Z","shell.execute_reply":"2021-08-13T01:04:02.416646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SET to show all columns\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n# SET data path\npath = os.path.join(dirname, filename)\n\n# Load Data\ndf = pd.read_csv(path)\n# Making dataframe clear\ndf = df.dropna()\nprint(df.isna().sum())\n\n# I want to use \"Open\",\"Volume_(BTC)\" and \"Volume_(Currency)\" as input data(aka features)\n# and Set \"Close\" as y_true\n\n# Drop columns arent used as input data\ndf.drop(columns=[\"Timestamp\",\"High\",\"Low\", \"Weighted_Price\"], inplace=True)\nprint(df)\n# Set 'Close' as y_ture\ny_true = df.pop('Close')\ndf['y_true'] = y_true\nprint(df)\n\n# Split train set and test set\nn = len(df)\ntrain_df = df[:int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\n# Normalize\nnorm_train_df = (train_df-train_df.mean())/train_df.std()\nnorm_test_df = (test_df-test_df.mean())/test_df.std()\nprint(norm_train_df.describe().transpose())","metadata":{"_uuid":"1e4539d5-3f8f-422d-bbde-b7583ce3f151","_cell_guid":"2650cb58-8527-40f6-9e0d-197a77c6d241","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:04:02.420932Z","iopub.execute_input":"2021-08-13T01:04:02.421363Z","iopub.status.idle":"2021-08-13T01:04:11.080118Z","shell.execute_reply.started":"2021-08-13T01:04:02.421302Z","shell.execute_reply":"2021-08-13T01:04:11.079093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build Window generator\nclass WindowGenerator():\n    # Set Parameters\n    def __init__(self,input_width, label_width, offset, \n                 train_df = norm_train_df, test_df = norm_test_df,\n                label_columns=None):\n        # store data\n        self.train_df, self.test_df = train_df, test_df\n        # Make label dictionary\n        if label_columns is not None:\n            self.label_dict = {name: i for i, name in enumerate(label_columns)}\n        self.label_columns = label_columns\n        \n        # Set winodw parameters\n        self.input_width, self.label_width = input_width, label_width\n        self.total_window_size = input_width + offset\n        \n        self.input_slice = slice(0,input_width)\n        self.input_example = np.arange(self.total_window_size)[self.input_slice]\n        \n        self.label_start = self.total_window_size - label_width\n        self.label_slice = slice(self.label_start,None)\n        self.label_example = np.arange(self.total_window_size)[self.label_slice]\n        \n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input Example: {self.input_example}',\n            f'Label Example: {self.label_example}',\n            f'Label Name: {self.label_columns}'])\n    \n    def split_datasets(self, datasets):\n        # inputs.shape = (batch, index(time), features)\n        # features are in datasets : Open, Volume_(BTC), Volume_(Currency), Close\n        # So except Close, the others are used as inputs\n        inputs = datasets[:, self.input_slice, :-len(self.label_columns) ]\n        labels = datasets[:, self.label_slice, -len(self.label_columns):]\n#         if self.label_columns is not None:\n#             labels = tf.stack(\n#                 [labels[:, :, self.label_dict[name]] for name in self.label_columns],\n#             axis=-1)\n        # Set Shape\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        \n        \n        return inputs, labels\n    \n    def datasets_maker(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n                data, targets=None, sequence_length=self.total_window_size, \n                sequence_stride=1,shuffle=False, batch_size=60)\n        ds = ds.map(self.split_datasets)\n        \n        return ds\n    \n    @property\n    def train(self):\n        return self.datasets_maker(self.train_df)\n    \n    @property\n    def test(self):\n        return self.datasets_maker(self.test_df)\n    \nten = WindowGenerator(input_width=10, label_width=1, offset=10, \n                     label_columns=['Close'])\nprint(\"Train : {}\\nTest : {}\".format(ten.train,ten.test))","metadata":{"_uuid":"298a9004-796a-4413-8c1d-b97d29dbc5e7","_cell_guid":"a05f9a99-89d0-4f15-8ec1-09f93b4e1f57","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:04:11.082111Z","iopub.execute_input":"2021-08-13T01:04:11.082565Z","iopub.status.idle":"2021-08-13T01:04:13.061023Z","shell.execute_reply.started":"2021-08-13T01:04:11.08252Z","shell.execute_reply":"2021-08-13T01:04:13.060067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build Model\nclass Build_Model(tf.keras.Model):\n    def __init__(self):\n        super(Build_Model, self).__init__()\n        self.lstm = tf.keras.layers.LSTM(32, return_sequences=False, \n                                        kernel_regularizer='l2')\n        self.dense = tf.keras.layers.Dense(32, activation='relu')\n        self.last = tf.keras.layers.Dense(1)\n        \n    def call(self, x):\n        x = self.lstm(x)\n        x = self.dense(x)\n        return self.last(x)\n\nmodel = Build_Model()","metadata":{"_uuid":"a8c3fbe0-34dd-4b9f-be91-521fb11dc529","_cell_guid":"4fb89557-0a0f-4702-a22c-f25d561e84e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:04:13.062639Z","iopub.execute_input":"2021-08-13T01:04:13.063023Z","iopub.status.idle":"2021-08-13T01:04:13.394119Z","shell.execute_reply.started":"2021-08-13T01:04:13.062983Z","shell.execute_reply":"2021-08-13T01:04:13.39318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Loss and Optimizer\nloss_object = tf.keras.losses.Huber()\noptimizer = tf.keras.optimizers.Adam()\n\n@tf.function\ndef train_step(x,y):\n    with tf.GradientTape() as tape:\n        predictions = model(x)\n        loss = loss_object(y, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n@tf.function\ndef test_step(x,y):\n    predictions = model(x)\n    loss = loss_object(y, predictions)","metadata":{"_uuid":"5650e5a1-e83f-4a4e-afb5-0d6df43608f9","_cell_guid":"1f3e912f-619c-499c-9604-7ac1fbe91d1c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:04:13.396314Z","iopub.execute_input":"2021-08-13T01:04:13.396935Z","iopub.status.idle":"2021-08-13T01:04:13.405337Z","shell.execute_reply.started":"2021-08-13T01:04:13.396891Z","shell.execute_reply":"2021-08-13T01:04:13.404257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Train and Test datasets\ntrain_dataset = ten.train\ntest_dataset = ten.test\n# Train the model\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    for train_x, train_y in train_dataset:\n        train_step(train_x, train_y)\n        \n    for test_x, test_y in test_dataset:\n        test_step(test_x, test_y)\n        \n    print(\"EPOCHS : {} \".format(epoch+1))\n\ntf.keras.backend.clear_session()","metadata":{"_uuid":"af76e981-252c-44a1-a5b2-636b532b90e0","_cell_guid":"09af3a8d-24c1-44bd-a4fa-0f623adbe945","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:04:13.409546Z","iopub.execute_input":"2021-08-13T01:04:13.409947Z","iopub.status.idle":"2021-08-13T01:52:05.122914Z","shell.execute_reply.started":"2021-08-13T01:04:13.409899Z","shell.execute_reply":"2021-08-13T01:52:05.122063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets take graphs\nimport matplotlib.pyplot as plt\n\norigin = norm_test_df['y_true']\nforecast = model.predict(test_dataset).ravel(order='C')\nprint(\"origin : {}\\nforecast : {}\".format(origin.shape, forecast.shape))\n\nplt.figure()\nplt.plot(range(len(origin)), origin, label=\"Origin\", color='r')\nplt.plot(range(len(forecast)), forecast, label=\"Forecast\", color='b')\nplt.legend()\nplt.show()\n\ntf.keras.backend.clear_session()","metadata":{"_uuid":"4f140073-1e39-47f2-8dbc-ad785752e85b","_cell_guid":"1e4a5993-ef00-4de5-8f66-5d6398a7452b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-13T01:52:05.125118Z","iopub.execute_input":"2021-08-13T01:52:05.125494Z","iopub.status.idle":"2021-08-13T01:52:32.815844Z","shell.execute_reply.started":"2021-08-13T01:52:05.125455Z","shell.execute_reply":"2021-08-13T01:52:32.814873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Finally! We Got Extra Simple prediction model!**","metadata":{"_uuid":"8a72ff5b-abc7-4941-acec-5f0e4e01de2b","_cell_guid":"044ea654-dcb8-4f57-8ee8-7bdea580e5a9","trusted":true}}]}