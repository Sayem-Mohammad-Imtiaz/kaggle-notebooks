{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contents\n## 1. Data collection and storage\n- Crawler script key code\n- How to use\n\n## 2. Data preparation\n- Import Module\n- Load the data and fetch the data set\n- word segmentation\n\n## 3. Exploration and visualization\n- The distribution of views\n- The words in the training set\n\n## 4. Experimentation and prediction\n- Neural Networks: CNN (train and prediction)\n- Machine learning: Bayes","metadata":{}},{"cell_type":"markdown","source":"# Data collection and storage","metadata":{}},{"cell_type":"markdown","source":"- key code\n- you can run this cell but it will cost a lot of time, and you can also run spider.py directly","metadata":{}},{"cell_type":"code","source":"def run_spider():\n    import requests\n    from lxml import etree\n    import re\n    import time #有必要时用来睡眠\n    import pandas as pd \n    # version1：存在两个问题1是link有时候不是cqu而微信等来源2是最后一次再写文件不安全（每隔几轮就保存）\n    # version2：也有两个问题1是速度太慢可以用多进程和异步io解决2是读写文件每次重写csv可以改成追加且list变frame再保存太麻烦直接写或者csv库\n    # version3：加入了多进程没;对比了csv模块pd更快尤其存pkl更快；修复了nexturl会死循环；修复浏览数为零bug\n    # 使用追加而非内存保存全部的数据；但避免中断运行后追加重复命名加上随机\n    start = time.time()\n    url=\"http://news.cqu.edu.cn/newsv2/news-126.html\"\n    url2 = \"http://news.cqu.edu.cn/newsv2/news-126.html?&page=590\" #测试url\n    url3 = \"https://news.cqu.edu.cn/newsv2/news-132.html\"\n\n    header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\"}\n    results=[]\n    flag = True\n    while flag:\n        html = requests.get(url, headers=header).text\n        # print(html)\n        listpage = etree.HTML(html)\n        # lis = listpage.xpath('/html/body/div[3]/div/div[2]') \n        lists = listpage.xpath('//div[@class=\"lists\"]/div[@class=\"item\"]') #这里需要注意选择的是item\n\n        # 一个page内的抓取\n        for oneSelector in lists: # 收集的细节\n            # testSelector = oneSelector.xpath(\"@class\")[0]\n            # print(testSelector) #item\n\n            # 不同板块的div顺序还是不同要选class\n            link = oneSelector.xpath(\"div[@class='content']/div[@class='title']/a/@href\")[0]\n            if \"http://news.cqu.edu.cn\" not in link:\n                link = \"http://news.cqu.edu.cn\" + link\n            title = oneSelector.xpath(\"div[@class='content']/div[@class='title']/a/text()\")[0]\n            abstract = oneSelector.xpath(\"div[@class='content']/div[@class='abstract']/text()\")[0]\n            try:\n                html_sub = requests.get(link, headers=header).text\n                newspage = etree.HTML(html_sub)\n                datetime = (newspage.xpath(\"//div[@class='ibox']/span[2]/text()\"))[0]\n                author = newspage.xpath(\"//div[@class='dinfoa']/p/a/text()\") #a个数不定的但可以这样加入到list\n                # TODO: 这个count不能直接抓得找api发现在html里有这个api\n                view_num = newspage.xpath(\"//span[@id='hits']/text()\")[0]\n\n                api = newspage.xpath('//script[@language=\"JavaScript\"]/@src')[0] #$('#todaydowns').html('227');$('#weekdowns').html('227');$('#monthdowns').html('227');$('#hits').html('227');\n                apiout = (requests.get(api,headers=header).text) \n                view_num = apiout.split(\"#hits').html('\")[-1].replace(\"');\", \"\") #更好的对上面字符串提取是正则\n                print(view_num)\n                tags = newspage.xpath(\"//div[@class='tags']/a/text()\")\n                # 有些正文的p不一样还有含img的 newsbody = newspage.xpath(\"//p[@style='text-align: justify;']/text()\")\n                newsbody = newspage.xpath(\"//div[@class='acontent']/p/text()\")\n                results.append([datetime,title,author,abstract,link,tags,newsbody,view_num])\n\n            except:\n                print(\"Error:\",link)\n        next_url = listpage.xpath('//div[@class=\"page\"]/a[last()]/@href')[0]\n        cur_page = listpage.xpath('//div[@class=\"page\"]/span/text()')[0]\n\n        if next_url and cur_page==next_url.split(\"=\")[-1]:\n            # url = \"http://news.cqu.edu.cn/newsv2/\"+ next_url\n            print(cur_page,next_url.split(\"=\")[-1])\n            flag = False\n        url = next_url\n        print(\"crawl:\"+url)\n    filename = \"data_set\"+str(time.time())+\".pkl\"\n    df = pd.DataFrame(results,columns=['datetime','title','author','abstract','link','tags','newsbody','view_num']) #,columns=['link','title','abstract']\n    # df.to_csv(filename, mode='a',encoding='utf_8_sig')\n    df.to_pickle(filename)\n# run_spider()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-06T14:04:07.121433Z","iopub.execute_input":"2021-06-06T14:04:07.121918Z","iopub.status.idle":"2021-06-06T14:04:07.140044Z","shell.execute_reply.started":"2021-06-06T14:04:07.121814Z","shell.execute_reply":"2021-06-06T14:04:07.138976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\n- The data is saved in the PKL format because it is fast to read and write and is small in volume.I have also saved a copy of the CSV for easy viewing, so we can load the file directly here.","metadata":{}},{"cell_type":"code","source":"# import\nimport os\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport jieba\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore') \n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:04:07.141462Z","iopub.execute_input":"2021-06-06T14:04:07.142052Z","iopub.status.idle":"2021-06-06T14:04:08.007111Z","shell.execute_reply.started":"2021-06-06T14:04:07.142016Z","shell.execute_reply":"2021-06-06T14:04:08.006306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- load data and stopwords","metadata":{}},{"cell_type":"code","source":"def get_stopwords(stopwords_dir=\"../input/cqunews/stopwords\"):\n    \"\"\"\n       读取停用词库,这里只是用了一个停用词库   如果要预测结果更高点的话，可以将多个语料库结合起来使用\n       :return: set 包含的停用词\n       \"\"\"\n    files = os.listdir(stopwords_dir)#返回路径下的所有文件\n    stopwords = []\n    for f in files:\n        filepath = os.path.join(stopwords_dir, f)#把文件拼接成文件路径\n        with open(filepath, 'r', encoding='utf-8') as r:\n            w = r.read().split()\n        stopwords += w\n    return set(stopwords) #b把所有停用词文件里的都取出来，然后放到集合里","metadata":{"execution":{"iopub.status.busy":"2021-06-06T15:34:28.753113Z","iopub.execute_input":"2021-06-06T15:34:28.753493Z","iopub.status.idle":"2021-06-06T15:34:28.759182Z","shell.execute_reply.started":"2021-06-06T15:34:28.753464Z","shell.execute_reply":"2021-06-06T15:34:28.758122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = [r\"/kaggle/input/cqunews/zhaosheng.pkl\",\\\n            r\"/kaggle/input/cqunews/zonghe.pkl\"]\nstopwords = get_stopwords(\"/kaggle/input/cqunews/stopwords\")\ndata = (pd.concat([pd.read_pickle(file) for file in filepath]))\nX = data.drop(['view_num'],axis=1)\ny = data['view_num'].astype(int) #转换数字\ndata.head() #一万八条 ","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:04:08.016312Z","iopub.execute_input":"2021-06-06T14:04:08.016822Z","iopub.status.idle":"2021-06-06T14:04:10.12009Z","shell.execute_reply.started":"2021-06-06T14:04:08.016779Z","shell.execute_reply":"2021-06-06T14:04:10.119188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- View the pageview distribution","metadata":{}},{"cell_type":"code","source":"# max(y) 54514\n# min(y) 9\n# (y.mean()) 798\n# y.median() 489 中位数\n# y.plot() 这个图x是序号没什么意义\n# ct = Counter(y)\n# plt.bar(list(ct.keys()),list(ct.values()),label='xl1')#label为设置图例标签，需要配合legend（）函数才能显示出\n# (sorted(ct.items(), key=lambda data: data[1])) 不是直方图也不是折线图\n# y.plot(kind='bar') 太慢了\nplt.hist(y,range=(0,y.mean()*2),bins=10) # 原本氛围是0到最大看到基本分布在一侧于是确定范围","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:04:10.121291Z","iopub.execute_input":"2021-06-06T14:04:10.121626Z","iopub.status.idle":"2021-06-06T14:04:10.283004Z","shell.execute_reply.started":"2021-06-06T14:04:10.121598Z","shell.execute_reply":"2021-06-06T14:04:10.282088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Processing the label\n<!-- - 离散问题/二分类/多分类\n- 保留离散数据当作回归预测浏览数\n- 标签二值化（标准？均值/中位数）\n- 标签区间化（多少个？按照最小最大均分？指数型区间？均值为中心分？） -->","metadata":{}},{"cell_type":"code","source":"label = y.apply(lambda x:x>y.mean()) # 看图就知道用均值为负较多\nfrom sklearn.preprocessing import StandardScaler\nstandardscaler = StandardScaler()\nystd = standardscaler.fit_transform(np.array(y).reshape(-1,1))\nCounter(label)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:04:10.284424Z","iopub.execute_input":"2021-06-06T14:04:10.284779Z","iopub.status.idle":"2021-06-06T14:04:11.942013Z","shell.execute_reply.started":"2021-06-06T14:04:10.284742Z","shell.execute_reply":"2021-06-06T14:04:11.941181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Use Jieba for Chinese word segmentation and take out the X (We chose to focus only on the abstract column, and we also tried to merge the tags columns without much effect)\n>特征选择  取一列？取多列合并一列？取多列？","metadata":{}},{"cell_type":"code","source":"# 使用title+tags合并，且后者不需要分词\nimport jieba.posseg as pseg\ndef content_to_word(line):\n    gen = pseg.cut(line)\n    words = []\n    for i in gen:\n        if i.flag != 'x' and i.word not in stopwords  and not i.word.isdigit(): #停用词\n            words.append(i.word) #只加非标点符号\n    return ' '.join(words) #空格分割的字符串\n# data['tags']+data['title'] 需要先tags处理\n# temp = data['tags'].map(lambda x: \" \".join(x)) #多运行几次会被分词单字因为tags会反复使用所有改用temp存数据\n# data['title'] = data['title'].map(content_to_word)\n# X = data['title'] #+ temp\nX = data['abstract'].map(content_to_word)\nX","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:04:11.943288Z","iopub.execute_input":"2021-06-06T14:04:11.94362Z","iopub.status.idle":"2021-06-06T14:05:22.862091Z","shell.execute_reply.started":"2021-06-06T14:04:11.943585Z","shell.execute_reply":"2021-06-06T14:05:22.861147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploration and visualization\n- This section is going to visualize X and manipulate X to make it a better representation","metadata":{}},{"cell_type":"code","source":"import wordcloud\ndef word_cloud_analyse(datas,stopwords):\n    X=datas['abstract']\n    words=[]\n    for i in range(len(X)):\n        l=[]\n        l=jieba.lcut(X.iloc[i])\n        l=[t for t in l  if t not in stopwords and t!=' ']\n        words+=l\n    words=' '.join(words)\n#     ,font_path=\"C:\\\\Windows\\\\Fonts\\\\HGWYS_CNKI.TTF\"\n    wd=wordcloud.WordCloud(background_color=\"white\")\n    wd.generate(words)\n    return wd\n        \n\nstopwords=get_stopwords()\nwd=word_cloud_analyse(data,stopwords)\n%pylab inline\nimport matplotlib.pyplot as plt\nplt.imshow(wd, interpolation='bilinear')\nplt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T15:35:16.906123Z","iopub.execute_input":"2021-06-06T15:35:16.906468Z","iopub.status.idle":"2021-06-06T15:35:32.619851Z","shell.execute_reply.started":"2021-06-06T15:35:16.906437Z","shell.execute_reply":"2021-06-06T15:35:32.61909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimentation and prediction\n- This section tries neural network CNN and machine learning Bayes and makes predictions","metadata":{}},{"cell_type":"code","source":"# This function returns the test set training set after word vectorization\ndef tokenizer_data(X, y, MAX_SEQUENCE_LENGTH):\n    \n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(X)\n    joblib.dump(tokenizer, './models/Tokenizer')\n\n    sequences = tokenizer.texts_to_sequences(X)\n    word_index = tokenizer.word_index\n    print('数据集含 %s 个不同的词.' % len(word_index))\n    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    labels = to_categorical(np.asarray(y)) # onehot \n    labels = np.asarray(y)\n    print('Shape of data tensor:', data.shape,\"[0]:\", data[0])\n    print('Shape of label tensor:', labels.shape,\"[0]:\", labels[0])\n\n\n    # 第一部分划分应该放这儿/模型fit报错就是因为直接用的原始数据\n    X, X_test, y, y_test = train_test_split(data, labels, test_size=.20, random_state=2048)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.10, random_state=2048)\n    return word_index,X_train,y_train,X_val,y_val,X_test,y_test\nword_index, X_train, y_train, X_val, y_val, X_test, y_test = tokenizer_data(X,label,MAX_SEQUENCE_LENGTH) #用ystd是做回归的","metadata":{"execution":{"iopub.status.busy":"2021-06-06T15:26:50.965014Z","iopub.execute_input":"2021-06-06T15:26:50.96537Z","iopub.status.idle":"2021-06-06T15:26:53.733739Z","shell.execute_reply.started":"2021-06-06T15:26:50.965341Z","shell.execute_reply":"2021-06-06T15:26:53.732928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This cell is about training CNN. If there is no GPU, it may be slow, so you can skip this part and use the trained files directly","metadata":{}},{"cell_type":"code","source":"import joblib\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Input, Flatten, Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\nfrom tensorflow.keras.models import Sequential\n# train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import initializers\n\nMAX_SEQUENCE_LENGTH = 100\nEMBEDDING_DIM = 200\ndef train():\n    print('3. #############载入word2vec...##########')\n    embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)\n    print('4. #############模型及训练...#############')\n\n    model = Sequential()\n    # 使用上面构造的layer\n    model.add(embedding_layer)\n    model.add(Dropout(0.3))\n    model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1)) \n    model.add(MaxPooling1D(3))\n#     model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(EMBEDDING_DIM, activation='relu'))\n#     model.add(Dense(EMBEDDING_DIM, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n\n    # model.add(Dense(y_train.shape[1], activation='softmax')) #多分类softmax\n    model.add(Dense(1, activation='sigmoid')) #二分类\n#     model.add(Dense(1)) #回归\n\n    model.summary()  \n    from tensorflow.python.keras.utils.vis_utils import plot_model\n    plot_model(model, to_file='cnn_model.png',show_shapes=True)\n    cce = tf.keras.losses.CategoricalCrossentropy()\n    bce = tf.keras.losses.BinaryCrossentropy()\n    # mse = tf.keras.losses.MSE()\n    opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n    model.compile(loss=bce,\n                  optimizer='rmsprop', #rmsprop\n                  metrics=['acc']) #'mae', 'mse','acc'\n    print(model.metrics_names)\n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=128) #loss为零\n    model.save('./models/cnn_word.h5')\n\n\n    print('4. #############评估...#############')\n    print(model.evaluate(X_test, y_test))\n    return model\nmodel = train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-06T15:00:47.859758Z","iopub.execute_input":"2021-06-06T15:00:47.86009Z","iopub.status.idle":"2021-06-06T15:01:06.92518Z","shell.execute_reply.started":"2021-06-06T15:00:47.860059Z","shell.execute_reply":"2021-06-06T15:01:06.924354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](cnn_model.png)","metadata":{"_kg_hide-input":false}},{"cell_type":"markdown","source":"#### A summary of the model\n- We see that the loss of verification set increases first and then decreases in training, indicating that CNN overfits this model, which may be caused by label binarization and data volume\n- 用错损失函数会导致训练不能进行\n- 初始使用cnn会过拟合即loss下降val loss先降后升\n- 将模型输出改为多分类（onehot编码如1变成0 1）改动没用\n- 改成回归loss非常大\n- 使用正则化项反而训练没有效果 [keras文档](https://keras-cn.readthedocs.io/en/latest/other/regularizers/)\n- 优化器方面：SGD是优化不了，adam虽然更快但也是先下降后升\n- 想到不管标签如何划分二类只是影响acc而loss变化是一样的所以划分没多大意义，另一方面变成回归只有loss指标且也是先降低再震荡\n- 换成abstract列代替title会好些另外用新闻主题代替呢","metadata":{}},{"cell_type":"markdown","source":"- Here's how to make predictions","metadata":{}},{"cell_type":"code","source":"def word_to_vector(text, MAX_SEQUENCE_LENGTH=100):\n    tokenizer = joblib.load(\"./models/Tokenizer\")\n    word_index = tokenizer.word_index\n    sequences = tokenizer.texts_to_sequences([text])\n    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n#     data = pad_sequences(sequences)\n\n    return data\ndef predict(sentence):\n    model = tf.keras.models.load_model('./models/cnn_word.h5')\n\n    gen = pseg.cut(sentence)\n    words = []\n    for i in gen:\n        if i.flag != 'x' and i.word not in stopwords and not i.word.isdigit() :\n            words.append(i.word)  \n    data = ' '.join(words) \n    data = word_to_vector(data)\n    print(((model.predict(data)))) #直接model调用\npredict(\"为庆代，2021年5月，学校举办了“中国梦•劳动’美个学院踊跃参加。\")\npredict(\"6月3日下午，联谊赛决赛在重大花园乒乓球馆拉开战幕。从学院教职工和硕士研究生选拔出的16名选手参加了决赛\")\n# Since we take the average above as a positive sample, we need the page views greater than 800 to be greater than 0.5","metadata":{"execution":{"iopub.status.busy":"2021-06-06T15:29:22.91993Z","iopub.execute_input":"2021-06-06T15:29:22.92026Z","iopub.status.idle":"2021-06-06T15:29:24.899794Z","shell.execute_reply.started":"2021-06-06T15:29:22.920213Z","shell.execute_reply":"2021-06-06T15:29:24.89893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['abstract'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T15:28:51.199972Z","iopub.execute_input":"2021-06-06T15:28:51.200306Z","iopub.status.idle":"2021-06-06T15:28:51.205681Z","shell.execute_reply.started":"2021-06-06T15:28:51.200276Z","shell.execute_reply":"2021-06-06T15:28:51.204655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attempt decision tree \n# from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier,AdaBoostClassifier #,DecisionTreeClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import cross_val_score\n# from sklearn.feature_selection import RFE\n# from sklearn.decomposition import PCA, TruncatedSVD, PCA\n# from mlxtend.classifier import EnsembleVoteClassifier \n# DTC_Classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n# ETC_Classifier = ExtraTreesClassifier()\n# RTC_Classifier = RandomForestClassifier(criterion='entropy')\n# #  Gaussian Naive Baye Model\n# BNB_Classifier = GaussianNB()\n\n# # GBM\n# import xgboost as xgb\n# XGB_Classifier = xgb.XGBClassifier()\n# # ADA\n# ADA_Classifier = AdaBoostClassifier(\n#     DTC_Classifier,\n#     n_estimators=100,\n#     learning_rate=1.5)\n# import lightgbm as lgb\n# params = {\n#     \"objective\": 'binary',\n#     \"num_leaves\": 24,\n#     \"learning_rate\": 0.2,\n#     \"bagging_fraction\": 0.9440403047411987,\n#     \"bagging_freq\": 8\n# }\n# LGB_Classifier = lgb.LGBMClassifier(objective='binary',num_leaves=31,learning_rate=0.2,bagging_fraction=0.8963,bagging_freq=4)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:48:09.743625Z","iopub.execute_input":"2021-06-06T14:48:09.743945Z","iopub.status.idle":"2021-06-06T14:48:09.748124Z","shell.execute_reply.started":"2021-06-06T14:48:09.743916Z","shell.execute_reply":"2021-06-06T14:48:09.747161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Evaluate Models\n# from sklearn.model_selection import cross_val_score\n# from sklearn import metrics\n\n# models = []\n# # models.append(('Decision Tree Classifier', DTC_Classifier))  #简直过拟合了训练集上近1的acc且绝大多数的预测概率直接1\n# # models.append(('Random Forest Classifier', RTC_Classifier))\n# # models.append(('Extract Tree Classifier', ETC_Classifier))\n# models.append(('BNB Classifier', BNB_Classifier))\n\n\n# # models.append(('XGB Classifier', XGB_Classifier)) #xgb的fit耗时很长但效果比lgb好点，经测predict高点\n# # models.append(('LGB Classifier', LGB_Classifier))\n# # models.append(('LogisticRegression', LGR_Classifier))\n# def train_evaluate(x,y,istest=False):\n#     for i, v in models:\n#         if istest==False:\n#             v.fit(x,y)\n#         t1 = time.time()\n#         vpred = v.predict(x)\n# #         scores = cross_val_score(v, x, y, cv=10) #单就这一句就用了cv次fit函数太费时间\n#         accuracy = metrics.accuracy_score(y, vpred) #和model.score一样作用\n#         cm = metrics.confusion_matrix(y, vpred)\n#         classification = metrics.classification_report(y, vpred)\n#         print('Cost',time.time()-t1)\n#         print('============================== {} Model Evaluation =============================='.format(i))\n#         print()\n# #         print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n#         print (\"Model Accuracy:\" \"\\n\", accuracy)\n#         print(f\"TNR:{cm[1][1]/(cm[0][1]+cm[1][1])}\\nTPR:{cm[0][0]/(cm[0][0]+cm[1][0])}\" \"\\n\")\n#         print(\"Classification report:\", classification)\n# #         show_feature_importance(v)\n#         fpr,tpr,thr = metrics.roc_curve(y,v.predict_proba(x)[:,1],1)\n# #         display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=metrics.auc(fpr,tpr),estimator_name=i)\n#         ac = metrics.auc(fpr,tpr)\n#         plt.plot(fpr,tpr,label=f\"{i}:AUC={ac:1.5f}\") #scatter画点可见是离散\n#         print('FPR:\\t',fpr.shape)#是一每个\n# #         metrics.plot_roc_curve(v,x,y) 为了画在一个图里用RocCurveDisplay(也不能) 但这个是散点显示折线（默认的thr取值是按照score而不是0-1之间依次取）\n#         plt.legend() # 设置位置，没有这个label不生效(还必须在每此plot后)\n#     plt.ylabel('TPR(真正类率：实际正中预测正)',fontsize='15')\n#     plt.xlabel('FPR(假正类率：实际负中预测正)',fontsize='15')\n#     plt.title('ROC(接收者操作特征，每个阈值确定个点)',fontsize='20')\n#     plt.show()\n#     plt.savefig('roc.png')\n#     from IPython.display import FileLink\n#     FileLink('roc.png')\n# import time\n# word_index, X_train, y_train, X_val, y_val, X_test, y_test = tokenizer_data(X,y,MAX_SEQUENCE_LENGTH=80) #用y是做回归的\n# X_test,y_test\n# Counter(label)\n# # train_evaluate(X_train,y_train) \n# # train_evaluate(X_test,y_test,True) ","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:06:14.721474Z","iopub.execute_input":"2021-06-06T14:06:14.721793Z","iopub.status.idle":"2021-06-06T14:06:14.734689Z","shell.execute_reply.started":"2021-06-06T14:06:14.721758Z","shell.execute_reply":"2021-06-06T14:06:14.73383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:11:48.552717Z","iopub.execute_input":"2021-06-06T14:11:48.553073Z","iopub.status.idle":"2021-06-06T14:11:48.557447Z","shell.execute_reply.started":"2021-06-06T14:11:48.55304Z","shell.execute_reply":"2021-06-06T14:11:48.556273Z"},"trusted":true},"execution_count":null,"outputs":[]}]}