{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Python Packages**","metadata":{"id":"MGijtb7eXPXz"}},{"cell_type":"code","source":"# Import python packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","metadata":{"id":"RyCjnv_HXPX5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Dataset**","metadata":{"id":"qv4bU1BZXPX6"}},{"cell_type":"code","source":"# Load the dataset\ndata = pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')","metadata":{"id":"Hpv7pjTpXPX6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA)**","metadata":{"id":"fwthZ0EvXPX6"}},{"cell_type":"markdown","source":"## **Basic Statistical and EDA Operations**","metadata":{}},{"cell_type":"code","source":"# Check info of data\ndata.info()","metadata":{"id":"WfINPR8CXPX7","outputId":"cdb2a3ac-54fe-4adf-a855-444f82b48d24","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n1) There are no null values\n2) There are 3 features with object data type","metadata":{"id":"sg9IfO2aXPX8"}},{"cell_type":"code","source":"# Check Descriptive information of dataset features\ndata.describe(include='all').T","metadata":{"id":"VoYkEzS8XPX8","outputId":"4643245e-72f2-42ec-eebd-9c8f3f963e70","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) There are 2932 unqiue surnames.\n\n2) There are around 25% of observations in above dataset have zero Balance. Need futher investigation.","metadata":{"id":"DogfBLgdXPX8"}},{"cell_type":"code","source":"#Print head of dataset\ndata.head()","metadata":{"id":"F7Ka8_YaXPX9","outputId":"9160d3de-b927-4107-ef53-4489ef0bb0e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print tail of headset\ndata.tail()","metadata":{"id":"Rihn3V0OXPX9","outputId":"d7c265f5-42ce-498c-e41f-4d5650344486","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print 10 random observation of dataset\ndata.sample(10)","metadata":{"id":"u9vK42u-XPX9","outputId":"4684c2c6-8a56-4443-8b66-9c01c9ce1d9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Based on number of 3 unique values in Geopgraphy feature and above 10 random sample of dataset, we can infer values as Spain, France and Germany.\n\n2) By looking at the features, we can say that CustomerID and Surname doesn't contribute to Bank Churn Prediction dependent variable 'Exited'.","metadata":{"id":"vwUw3mPZXPX-"}},{"cell_type":"code","source":"#Check for duplicate records\ndata.duplicated().sum()","metadata":{"id":"hX_hvmI2XPX-","outputId":"f2610607-3d77-4f82-af2c-e0318d5af752","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n1) There are no duplicate records","metadata":{"id":"CaU1YPKjXPX-"}},{"cell_type":"code","source":"# Check for null values\ndata.isnull().sum()","metadata":{"id":"N_7ITVsiXPX-","outputId":"acd3d33d-d570-4e2f-bd81-13f9b137693e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) There are no null values.","metadata":{"id":"l5yw8dpbXPX_"}},{"cell_type":"code","source":"# Check for number of unique values in features\ndata.nunique()","metadata":{"id":"q1qSFaxhXPX_","outputId":"bc3fe8fe-7e04-4246-fb0c-66d149e8e452","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Geography. Gender,NumOfProducts, HasCrCard, IsActiveMember and Existed features are categorical.","metadata":{"id":"0rs49iUqXPYA"}},{"cell_type":"code","source":"# Check value count for dataset features\nfor feature in data.columns:\n  print(data[feature].value_counts(normalize=True))","metadata":{"id":"o2aHHsAHyKpO","outputId":"8c6ff320-aee7-437f-c8c9-3158fa474aa9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Based on value count of **Geography feature**, around 50% of observation are from France.\n\n2) Based on **Gender** value count, number of observations are distributed equally among Male and Female.\n\n3) Based on **Balance** value count, 36% of observation is having 0 Balance.\n\n4) Based on **NumOfProducts** value count, Majority of accounts, bank account affiliated products owned by Customers are 1 and 2.\n\n5) Based on **HasCrCard** value count, around 71% of observations has credit card.\n\n6) Based on **IsActiveMember** value count, observations are equally distributed.\n\n7) Based on **Exited** value count, 80% of observation have not exited the bank services.","metadata":{"id":"UkhKt-wFyg5X"}},{"cell_type":"markdown","source":"## **Univariate and Bivariate Analysis**","metadata":{"id":"ERTU9M-iXPYA"}},{"cell_type":"code","source":"data.columns","metadata":{"id":"SkaqQbas1Spr","outputId":"334892ba-8ab3-4f2d-8ea2-de7758188ce3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw box plot to check for outliers in continous feature\nfor feature in ['CreditScore', 'Tenure', 'Age', 'Balance', 'EstimatedSalary']:\n  plt.figure(figsize=(5, 5))\n  sns.boxplot(x = feature, data = data)","metadata":{"id":"90_vMF4vXPYA","outputId":"a511c3b1-c305-468a-c17a-3c9bdd1d6908","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nThere are outliers in CreditScore feature.","metadata":{"id":"Lh5jFHTV_tkS"}},{"cell_type":"code","source":"# Check for outlier in Credit Score feature\nquantile25 = data.CreditScore.quantile(0.25)\nquantile75 = data.CreditScore.quantile(0.75)\n\niqr = (quantile75 - quantile25)\n\nlowWhisker = quantile25 - (1.5 * iqr)\nupperWhisker = quantile75 + (1.5 * iqr)\n\nlowOutliersCount = len(data[data.CreditScore < lowWhisker])\nupperOutliersCount = len(data[data.CreditScore > upperWhisker])\n\npercentageOfOutliers = ((lowOutliersCount + upperOutliersCount) / len(data)) * 100\n\nprint ( \"Percentage of outliers in CreditScore feature : {0}% \" .format(percentageOfOutliers))","metadata":{"id":"pPd-LLdY2Fmo","outputId":"30ed6155-ffdc-4cc9-bac0-ef4e540baee2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nPercentage of outliers in Credit Score feature is quite low. Let's treat these outliers by replace with boundary values instead of removing these observations because all these observations have Exited value as 1 and observation with Exited value as 1 are only 20%.","metadata":{"id":"MnRqXDcn4uN3"}},{"cell_type":"code","source":"# Treat Credit Score outliers by replacing with boundary values \ndata.CreditScore.clip(lower = lowWhisker, upper = upperWhisker, inplace=True)","metadata":{"id":"l1E9AtQP4p4g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Add new features by classifying Age, Balance and Credit Scores**","metadata":{"id":"EsynPioS7Eky"}},{"cell_type":"code","source":"# Check descriptive information of Age feature\ndata.Age.describe()","metadata":{"id":"rfiYoclDy5Ru","outputId":"24be45bf-5870-41a2-f290-42dd6ad6e5c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Organize Age in 4 groups based on range\ndata['AgeGroup'] = pd.cut(data.Age,bins=[17, 65, 93],labels=['Adult','Elderly'])","metadata":{"id":"0ZOKCy-mz8HM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print head of dataframe\ndata.head()","metadata":{"id":"JjqPMJD-0RhW","outputId":"49d661de-b64b-493b-d008-0a3c0cf9423e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar plot of Different Age Group Vs Exited feature\nsns.countplot(x = 'AgeGroup', data = data[data.Exited == 1])","metadata":{"id":"qHbYPekQtZtv","outputId":"59a2943b-7380-48f5-9dc2-986b9870f4ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nMajority of the customers exited are more likely is in Adult Age group (17 - 65)","metadata":{"id":"q-DuZtiXuXlT"}},{"cell_type":"code","source":"# Check descriptive information of CreditScore feature\ndata.CreditScore.describe()","metadata":{"id":"0JKiIWm30so8","outputId":"ffce87b4-0900-4542-c122-c8ece8d5d278","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classify Credit Score based on FICO range\ndata['CreditScoreGroup'] = pd.cut(data.CreditScore,bins=[300, 579, 669, 739, 799, 900],labels=[0, 1, 2, 3, 4])\ndata['CreditScoreGroup'] = data.CreditScoreGroup.astype(int)","metadata":{"id":"62SFFw5T1BCY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar plot of Different Credit Score Group Vs Exited feature\nsns.countplot(x = 'CreditScoreGroup', data = data[data.Exited == 1])","metadata":{"id":"zLrWjxtB1o5c","outputId":"0f20cc1b-36bf-4071-c801-23bb4635c9eb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nMajority of the customers exited the bank is in low Credit age (Very poor and Fair)","metadata":{"id":"oHVkcJdtv18H"}},{"cell_type":"code","source":"# Bar plot of Different Credit Score Group Vs Exited feature\nplt.figure(figsize=(10, 10))\nsns.countplot( x = 'CreditScoreGroup', data = data[data.Exited == 1], hue = 'AgeGroup')","metadata":{"id":"0Gz_Yh8p1xn-","outputId":"658dda1b-fa76-412d-c181-caf10ba973e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nAbove plot shows that Adult Age group is in majority when it comes to leaving the bank services across all credit score groups.","metadata":{"id":"u1HUaS_1w7PA"}},{"cell_type":"code","source":"# Organize Balance in 2 groups based on zero and non-zero\ndata['BalanceGroup'] = data.Balance.apply(lambda x : 1 if x > 0 else 0)\ndata.BalanceGroup.value_counts(normalize=True).plot(kind = 'bar')","metadata":{"id":"16XEvIzZ1ke8","outputId":"cf27f3a9-f222-4bfa-e2e5-338f7662906c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nAround 36% of observations are having balance value as 0.","metadata":{"id":"pNCRcyj0I07P"}},{"cell_type":"markdown","source":"## **Plotting feature with continous values vs Exited feature**","metadata":{"id":"_9fMMURu-VVg"}},{"cell_type":"code","source":"# Draw box plot to check relation between Exited and continous features in dataset\nfor feature in ['CreditScore', 'Tenure', 'Age', 'Balance', 'EstimatedSalary']:\n  plt.figure(figsize=(5, 5))\n  sns.boxplot(x = 'Exited', y = feature, data = data)\n\n  quantile25 = data[data.Exited == 1][feature].quantile(0.25)\n  quantile75 = data[data.Exited == 1][feature].quantile(0.75)\n  print (\"50% of observation lies in feature {0} between {1} and {2} based on 25th and 75th percentile when Exited value is 1\" .format(feature, quantile25, quantile75)) \n  \n  quantile25 = data[data.Exited == 0][feature].quantile(0.25)\n  quantile75 = data[data.Exited == 0][feature].quantile(0.75)\n  print (\"50% of observation lies in feature {0} between {1} and {2} based on 25th and 75th percentile when Exited value is 0\" .format(feature, quantile25, quantile75)) \n","metadata":{"id":"uhSq5RLU-QEU","outputId":"21b54101-82b9-4398-e75f-aeb31ee16d43","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) 50% of observation lies in feature CreditScore between 578.0 and 716.0 based on 25th and 75th percentile when Exited value is 1\n\n2) 50% of observation lies in feature CreditScore between 585.0 and 718.0 based on 25th and 75th percentile when Exited value is 0\n\n3) 50% of observation lies in feature Tenure between 2.0 and 8.0 based on 25th and 75th percentile when Exited value is 1\n\n4) 50% of observation lies in feature Tenure between 3.0 and 7.0 based on 25th and 75th percentile when Exited value is 0\n\n5) 50% of observation lies in feature Age between 38.0 and 51.0 based on 25th and 75th percentile when Exited value is 1\n\n6) 50% of observation lies in feature Age between 31.0 and 41.0 based on 25th and 75th percentile when Exited value is 0\n\n7) 50% of observation lies in feature Balance between 38340.02 and 131433.33 based on 25th and 75th percentile when Exited value is 1\n\n8) 50% of observation lies in feature Balance between 0.0 and 126410.28 based on 25th and 75th percentile when Exited value is 0\n\n9) 50% of observation lies in feature EstimatedSalary between 51907.72 and 152422.91 based on 25th and 75th percentile when Exited value is 1\n\n10)50% of observation lies in feature EstimatedSalary between 50783.49 and 148609.95500000002 based on 25th and 75th percentile when Exited value is 0","metadata":{"id":"yBDCBb48_-2k"}},{"cell_type":"markdown","source":"## **Plotting feature with categorical values vs Exited feature**","metadata":{"id":"7jWZOg0O-5U4"}},{"cell_type":"code","source":"# Draw count plot to check relation between Exited and continous features in dataset\nfor feature in ['Geography', 'Gender', 'NumOfProducts', 'HasCrCard', \n                'IsActiveMember', 'AgeGroup', 'CreditScoreGroup']:\n  plt.figure(figsize=(5, 5))\n  sns.countplot(x = feature, data = data, hue = 'Exited')\n\n  # Percentage of observation by Geography for each Exited Category ( 0 and 1)\n  print(\"================================================================\")  \n  print(\"(Exited == 0) =======>>>>\\n{0}\"  .format(data[data.Exited == 0][feature].value_counts(normalize = True)))\n  print(\"(Exited == 1) =======>>>>\\n{0}\"  .format(data[data.Exited == 1][feature].value_counts(normalize = True)))","metadata":{"id":"V1rEtx-G-09g","outputId":"7dd1ce4a-e5b5-4759-c94b-f0f077e1f179","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Around 80% of customer observations(who leaves bank i.e. Exited == 1) from France and Germany are more likely to leave Bank services\n\n2) Around 69% of Customer(who leaves bank i.e. Exited == 1) who owns single number of bank products are more likely to leave Bank services\n\n3) Around 70% of Customer(who leaves bank i.e. Exited == 1) who owns credit card are more likely to leave Bank services\n\n4) Around 64% of Customer(who leaves bank i.e. Exited == 1) who are not active member are more likely to leave Bank services\n\n5) Around 98% of Customer(who leaves bank i.e. Exited == 1) lies in Adult Age group are more likely to leave Bank services\n\n6) Around 82% of Customer(who leaves bank i.e. Exited == 1) lies in Fair, Good and Very poor credit age group are more likely to leave Bank services","metadata":{"id":"EW1TmGNgF2r5"}},{"cell_type":"markdown","source":"## **Multivariate Analysis**","metadata":{}},{"cell_type":"code","source":"# Check Exited feature based on different category of BalanceGroup, Gender and CreditScoreGroup feature\ng = sns.FacetGrid(data, row=\"BalanceGroup\", col = 'Gender', aspect=2, row_order = [0, 1], col_order = ['Male', 'Female'], hue = 'Exited', hue_order = [0, 1])\ng.map(sns.countplot, 'CreditScoreGroup', order = [0, 1, 2, 3, 4]).add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Customers with lower creditScore( between very poor - Good) are more likely to leave bank services.\n2) Customers are more likely to leave bank services when Balance is non zero in comparison to zero balance account\n3) Percentage of Female customers leaving bank services is higher than percentage of male customers.","metadata":{}},{"cell_type":"code","source":"# Check Exited feature based on different category of BalanceGroup, Gender and Geography feature\ng = sns.FacetGrid(data, row=\"BalanceGroup\", col = 'Gender', aspect=2, row_order = [0, 1], col_order = ['Male', 'Female'], hue = 'Exited', hue_order = [0, 1])\ng.map(sns.countplot, 'Geography', order = ['France', 'Spain', 'Germany']).add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) There are higher percentage of Female customers leaving bank service for Germany when balance in account in non zero.\n2) Percentage of customers leaving bank services from France is lesser in comparison to other than Spain and Germany Geographical location. ","metadata":{}},{"cell_type":"code","source":"# Check Exited feature based on different category of BalanceGroup, Gender and AgeGroup feature\ng = sns.FacetGrid(data, row=\"BalanceGroup\", col = 'Gender', aspect=2, row_order = [0, 1], col_order = ['Male', 'Female'], hue = 'Exited', hue_order = [0, 1])\ng.map(sns.countplot, 'AgeGroup', order = ['Adult', 'Elderly']).add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Percentage of Adult customers leaving bank services is higher in comparison to Elderly customers.","metadata":{}},{"cell_type":"code","source":"# Check Exited feature based on different category of BalanceGroup, Gender and IsActiveMember feature\ng = sns.FacetGrid(data, row=\"BalanceGroup\", col = 'Gender', aspect=2, row_order = [0, 1], col_order = ['Male', 'Female'], hue = 'Exited', hue_order = [0, 1])\ng.map(sns.countplot, 'IsActiveMember', order = [0, 1]).add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Percentage of Non Active customers leaving bank services is higher in comparison to active customers.","metadata":{}},{"cell_type":"code","source":"# Check Exited feature based on different category of NumOfProducts, Gender and HasCrCard feature\ng = sns.FacetGrid(data, row=\"Gender\", col = 'NumOfProducts', aspect=1, col_order = [1, 2, 3, 4], row_order = ['Male', 'Female'], hue = 'Exited', hue_order = [0, 1])\ng.map(sns.countplot, 'HasCrCard', order = [0, 1]).add_legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check value count of Exited feature for customer owns 3 or 4 number of products\ndata[data.NumOfProducts.isin([3, 4])].Exited.value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) Customers having NumOfProducts as 3 OR 4 are more likely to leave bank services.","metadata":{}},{"cell_type":"markdown","source":"## **Draw Pairplot**","metadata":{}},{"cell_type":"code","source":"# Univariate and Bivariate analysis using Pairplot\nsns.pairplot(data = data, corner=True, diag_kind='kde')","metadata":{"id":"8jGNOSnoXPYB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) In Diagonal plots, there are no outliers ","metadata":{"id":"kqYJM8RiXPYB"}},{"cell_type":"markdown","source":"## **Draw Heatmap to check for multicollinearity and relationship between two variables**","metadata":{}},{"cell_type":"code","source":"# Check Heatmap to check for collinearity\nplt.figure(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, cmap='YlGnBu')","metadata":{"id":"EcoaK3vMXPYB","outputId":"71dec58e-70c3-4c84-d76d-2a58e15bd254","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n1) CreditScoreGroup Vs CreditScore and BalanceGroup Vs Balance are highly correlation. Drop one of the feature.","metadata":{"id":"i7Suz8fyXPYB"}},{"cell_type":"markdown","source":"## **Eliminating features based on collinearity and large unique values in some of the features**","metadata":{}},{"cell_type":"markdown","source":"**1) Drop CustomerId, Surname and RowNumber feature becuase of large value of unique values and does not impacts target variable (Exited)**\n\n**2) Drop CreditScore and Balance feature to address multicollinearity**","metadata":{"id":"Z6gon98FXPYC"}},{"cell_type":"code","source":"# Remove CustomerId, Surname and RowNumber feature\ndata.drop(columns=['CustomerId', 'Surname', 'RowNumber'], inplace = True)\n\n# Drop one feature from set of two highly correlated features as per Heatmap from both train and test datasets\ndata.drop(columns = ['CreditScore', 'Balance'], inplace = True)","metadata":{"id":"RssDGI79XPYC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print 10 random sample from dataset\ndata.sample(10)","metadata":{"id":"-yHYRzkEXPYC","outputId":"ee6f22a7-29c1-4bee-c3fd-e08e66d1ecef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check shape of dataset\ndata.shape","metadata":{"id":"AhpDQw5dXPYC","outputId":"48031bf3-8db6-4d31-92da-248199770508","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check datatype of features in dataset\ndata.info()","metadata":{"id":"6nHSlmZzXPYD","outputId":"5404ee3d-63d3-42dc-cbcc-dc2a45a03b23","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Converting object data type to Categorical data type**","metadata":{}},{"cell_type":"code","source":"# Convert feature with object data type (Geography, Surname and Gender) to categorical data type \ndata.Geography =  pd.Categorical(data.Geography)\ndata.Gender =  pd.Categorical(data.Gender)","metadata":{"id":"eiais3BDJWyI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check datatype of features in dataset\ndata.info()","metadata":{"id":"JQ7IcwzSKCuv","outputId":"b15c520a-6450-48d3-f51b-ba6ed58a84ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preparing data for model**","metadata":{"id":"VRR2uJc-yVYt"}},{"cell_type":"markdown","source":"## **Splitting dataset into dependent and independent features (X and Y)**","metadata":{}},{"cell_type":"code","source":"# Split dataset feature into Dependent and Independent variables\nX = data.drop(columns=['Exited'])\ny = data['Exited']","metadata":{"id":"JW_X8YFbXPYD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **One Hot Encoding on Categorical features**","metadata":{"id":"VA_w4vNEX6m9"}},{"cell_type":"code","source":"# Apply One Hot Encoding to Geography, Gender and AgeGroup categorical variables\ncategorical_cols = ['Geography', 'Gender', 'AgeGroup']\nX = pd.get_dummies(X, columns = categorical_cols, drop_first = True)","metadata":{"id":"zglQWda5XPYD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Splitting dataset into training and testing dataset**","metadata":{"id":"c6yWYmqFYHGU"}},{"cell_type":"code","source":"# Import python package to import train_test_split\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"URePps69XPYE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into train and test dataset in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True, random_state = 42)","metadata":{"id":"vt_-gJDGXPYE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check shape of training dataset (indepenedent feature)\nX_train.shape","metadata":{"id":"8iDdvKxaXPYE","outputId":"f6413217-6bec-447f-b1e7-db4c41478c3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check shape of test dataset (indepenedent feature)\nX_test.shape","metadata":{"id":"SqTvjDYOXPYE","outputId":"c15170fc-c998-4275-87f4-2f3889bb1a2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check value count of dependent feature and distribution of training dataset based on target variable (Exited)\ny_train.value_counts(normalize=True)","metadata":{"id":"7I5Z3yCfKori","outputId":"0e93c3f4-e354-4c2a-afb8-bd08d6c6f5d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check value count of dependent feature and distribution of testing dataset based on target variable (Exited)\ny_test.value_counts(normalize=True)","metadata":{"id":"vRcQiBHlKtiQ","outputId":"119305ba-ed41-46e8-e4f7-f00634a11ac9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n80% of observation are for the customers who left the bank services. We can train the model with existing imbalance data first and then we will try to over and under sample dataset and train the model again to see if recall can be improved without underfitting/overfitting model.","metadata":{}},{"cell_type":"markdown","source":"## **Normalizing training and testing dataset using StandardScaler**","metadata":{"id":"fEuF4DetYaHL"}},{"cell_type":"code","source":"# Check head of independent training dataset \nX_train.head()","metadata":{"id":"Nh1eKR30OXfD","outputId":"9606e8f6-98c2-4be2-da8b-f8e6cb17a880","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print feature name of independent training dataset\nX_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import required package to normalize training and testing dataset separately\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nX_train = pd.DataFrame(MinMaxScaler().fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(MinMaxScaler().fit_transform(X_test), columns=X_test.columns)","metadata":{"id":"-Rd0R0ayXPYF","outputId":"4bb82401-7ccd-4844-f318-e4e83c120eb4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check head of training dataset to make sure features are normalized\nX_train.head()","metadata":{"id":"fCN1PBWxXPYF","outputId":"a9da3f5d-cd72-4f20-f01f-eb3dfde5124d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check head of testing dataset to make sure features are normalized\nX_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print columns of independent training dataset\nX_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print shape of independent training dataset\nX_train.shape","metadata":{"id":"QsJCVIUC3iq-","outputId":"320f7b12-b249-4342-943c-f67d637a1198","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Utility class**","metadata":{}},{"cell_type":"code","source":"# Import tensorflow and metrics package\nimport tensorflow as tf\nfrom sklearn.metrics import recall_score, precision_score\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create list of metrics\nrecallList = []\nprecisionList = []\nTestAccuracyList = []\nTrainingAccuracyList = []\nepochsList = []\nTechniqueList = []\nLearningRateList = []\nbatchSizeList = []\n\n# Defining ANN Model class\nclass ANNModel:\n    '''\n    This class implements ANN model using Keras.Sequential and calculate classification metrics\n    '''\n    def __init__(self, techniqueName, annModel, X_train, y_train, X_test, y_test, learning_rate, epochs, batchSize = 32):\n        '''\n        Initializing class member variables with different parameters like learning rate and epochs\n        '''\n        self.modelMetrics = None\n        self.y_pred = None\n        self.y_prediction = []\n\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.model = annModel\n        self.batchSize = batchSize\n\n        LearningRateList.append(self.learning_rate)\n        epochsList.append(self.epochs)\n        TechniqueList.append(techniqueName)\n        batchSizeList.append(self.batchSize)\n\n    def trainANNModel(self, classWeight = None):\n        '''\n        This member function compiles and train the keras sequential model\n        '''\n        self.model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), metrics=['accuracy'])\n        if (classWeight is None):\n            self.modelMetrics = self.model.fit(self.X_train, self.y_train, epochs=self.epochs, validation_split = 0.2 , verbose=0, batch_size = self.batchSize, class_weight = classWeight)\n        else:\n            self.modelMetrics = self.model.fit(self.X_train, self.y_train, epochs=self.epochs, validation_split = 0.2 , verbose=0, batch_size = self.batchSize, class_weight = classWeight)\n\n    def plotLossFunction(self):\n        '''\n        This member function  plot loss function over different epochs\n        '''\n        hist = pd.DataFrame(self.modelMetrics.history)\n        hist['epoch'] = self.modelMetrics.epoch\n\n        plt.plot(hist['loss'])\n        plt.plot(hist['val_loss'])\n        plt.legend((\"train\" , \"valid\") , loc =0)\n        plt.xlabel('Epoch---->')\n        plt.ylabel('Loss---->')\n\n    def evaluateANNModel(self):\n        '''\n        This member function evaluates model performance on testing dataset\n        '''\n        loss, acc = self.model.evaluate(self.X_train, self.y_train, verbose=0)\n        print('Training Accuracy: %.3f'  % acc)\n        print('Training Loss: %.3f' % loss)\n        TrainingAccuracyList.append(acc)\n\n        loss, acc = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n        print('Test Accuracy: %.3f'  % acc)\n        print('Test Loss: %.3f' % loss)\n        TestAccuracyList.append(acc)\n\n        self.y_pred = np.round(self.model.predict(self.X_test), 2)\n\n        for pred in self.y_pred:\n            if pred > 0.5:\n                self.y_prediction.append(1)\n            else:\n                self.y_prediction.append(0)\n\n    def printConfusionMatrix(self):\n        '''\n        This member function prints confusion matrix\n        '''\n        recallScore = np.round(recall_score(self.y_test.values, self.y_prediction, zero_division = 0), 2)\n        recallList.append(recallScore)\n\n        precisionScore = np.round(precision_score(self.y_test.values, self.y_prediction, zero_division = 0), 2)\n        precisionList.append(precisionScore)\n\n        print('Recall Score : {0}' .format(recallScore))\n        print('Precision Score : {0}' .format(precisionScore))\n\n        print(classification_report(self.y_test, self.y_prediction))\n\n        cm = tf.math.confusion_matrix(labels=self.y_test,predictions=self.y_prediction)\n\n        plt.figure(figsize = (5,3))\n        sns.heatmap(cm, annot=True, fmt='d')\n        plt.xlabel('Predicted')\n        plt.ylabel('Truth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train ANN Model on Unbalance training dataset and Predict the result**","metadata":{"id":"L6kI3dSEYoeT"}},{"cell_type":"code","source":"# Create keras Sequential model\nmodel = tf.keras.models.Sequential ([tf.keras.layers.Dense(256, input_dim=X_train.shape[1],activation='relu'),\n                                     tf.keras.layers.Dropout(0.2),   # Avoiding Overfitting and underfitting\n                                     tf.keras.layers.Dense(32, activation='tanh'),\n                                     tf.keras.layers.Dropout(0.2), # Avoiding Overfitting and underfitting\n                                     tf.keras.layers.Dense(16, activation='relu'),\n                                     tf.keras.layers.Dense(1, activation='sigmoid')])\n\n# Creating ANNModel object\ntrainANNModel_1 = ANNModel('ANN Unbalanced', model, X_train, y_train, X_test, y_test, learning_rate = 0.0001, epochs = 90, batchSize=30)\n\n# Train ANN Model on training dataset\ntrainANNModel_1.trainANNModel()\n\n# Plot loss value over different epochs\ntrainANNModel_1.plotLossFunction()\n\n# Evaluate model on testing dataset and predict the results using 0.5 as threshold\ntrainANNModel_1.evaluateANNModel()\n\n# Print Confusion Matrix\ntrainANNModel_1.printConfusionMatrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Improvements/Insights:**\n\nThe recall of the minority class is very less. It proves that the model is more biased towards majority class. So, it proves that this is not the best model.\nNow, let's try class weight to balance data and see their accuracy and recall results as part of improvement ","metadata":{}},{"cell_type":"markdown","source":"# **Improvement in Models by balancing dataset using Cost sensitive neural network**","metadata":{}},{"cell_type":"markdown","source":"## **Train ANN Model on by applying class weight to balance training dataset and Predict the result**","metadata":{}},{"cell_type":"code","source":"# Create keras Sequential model\nmodel = tf.keras.models.Sequential ([tf.keras.layers.Dense(256, input_dim=X_train.shape[1],activation='relu'),\n                                     tf.keras.layers.Dropout(0.2),   # Avoiding Overfitting and underfitting\n                                     tf.keras.layers.Dense(32, activation='tanh'),\n                                     tf.keras.layers.Dropout(0.2), # Avoiding Overfitting and underfitting\n                                     tf.keras.layers.Dense(16, activation='relu'),\n                                     tf.keras.layers.Dense(1, activation='sigmoid')])\n\n# Creating ANNModel object\ntrainANNModel_2 = ANNModel('ANN Balanced', model, X_train, y_train, X_test, y_test, learning_rate = 0.0001, epochs = 50, batchSize=30)\n\n# Define class weight...Since minority Exited feature dataset is 20% in comparison to 80% of majority of observation with Exited value 0.\n# Let's increase the weight by 5 times for minority (with Exited value as 1)\nclassWeight = {0 : 1, 1: 3}\n\n# Train ANN Model on training dataset\ntrainANNModel_2.trainANNModel(classWeight = classWeight)\n\n# Plot loss value over different epochs\ntrainANNModel_2.plotLossFunction()\n\n# Evaluate model on testing dataset and predict the results using 0.5 as threshold\ntrainANNModel_2.evaluateANNModel()\n\n# Print Confusion Matrix\ntrainANNModel_2.printConfusionMatrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Metrics**","metadata":{}},{"cell_type":"code","source":"# Create dataframe to capture model metrics for different type of dataset\nmetrics = pd.DataFrame()\nmetrics['Model Technique'] = TechniqueList\nmetrics['Learning Rate'] = LearningRateList\nmetrics['Epochs'] = epochsList\nmetrics['Training Accuracy'] = TrainingAccuracyList\nmetrics['Testing Accuracy'] = TestAccuracyList\nmetrics['Batch Size'] = batchSizeList\nmetrics['Recall'] = recallList\nmetrics['Precision'] = precisionList\nmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Analysis based on Metrics:**\n1) Model should be selected based on recall along with precision and Accuracy. We are performing analysis to predict whether customer will leave the bank services in future or not.\nBased on above metrics, we should use cost sensitive ANN model where recall is better along with precision and accuracy.","metadata":{}},{"cell_type":"markdown","source":"# **Suggestion to Bank to reduce number of customers leaving bank services:**","metadata":{}},{"cell_type":"code","source":"# Detemine the parameters Bank need to work on reducing customers leaving bank services\ndata[(data.Gender == 'Female') & (data.AgeGroup == 'Adult') & ( (data.NumOfProducts == 3) | (data.NumOfProducts == 4) )].Exited.value_counts(normalize = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\nBank should work on bank services focussing above parameters i.e. Female Adult Age group (17 - 65) having NumOfProducts 3 OR 4 to reduce customers leaving bank services","metadata":{}}]}