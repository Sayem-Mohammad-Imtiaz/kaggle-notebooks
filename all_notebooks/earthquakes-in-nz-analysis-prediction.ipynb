{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Earthquakes in NZ - Analysis & Prediction\n\nIn this notebook we will try to analysis and create a model to predict (hopefully!) earthquakes. Following will be the course of action\n\n* Libraries\n* Data Load, Exploration & Preparation\n* Data Visualisation\n* Feature Engineering\n* Data Split (Training & Test)\n* Build, Train & Test Model (Simple)\n* Build Neural Network (Complex)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generic Libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualisation Libraries\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport warnings\nfrom matplotlib import cm\nfrom mpl_toolkits.basemap import Basemap\nimport folium\nfrom folium.plugins import CirclePattern, FastMarkerCluster\n\n#Data Formatting\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\npd.set_option('display.max_columns', 500)\nwarnings.filterwarnings(\"ignore\")\n#pd.options.display.float_format = '{:.2f}'.format\n\n#Garbage Collector\nimport gc\n\n#Date-Time Libraries\nimport datetime\nimport time\n\n#SK Learn Libraries\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n#Tabulate Library\nfrom prettytable import PrettyTable\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Load, Exploration & Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Data\nurl = '../input/earthquakes-data-nz/earthquakes_NZ.csv'\ndata = pd.read_csv(url, header='infer',parse_dates=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe, there are around 20k+ records","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for missing values\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good that we don't have any missing values.\n\nNow, re-naming the column names & then formatting the data in 'depth' column to show upto 2 decimal points & the data in 'magnitude' column to show upto 1 decimal point.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the data-types for each column\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming Columns\ndata = data.rename(columns={\"origintime\": \"Time\", \"longitude\": \"Long\", \" latitude\": \"Lat\", \" depth\": \"Depth\", \" magnitude\": \"Magnitude\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Formating Column Data\ndata[['Depth']] = data[['Depth']].applymap(\"{0:.2f}\".format)\ndata[['Magnitude']] = data[['Magnitude']].applymap(\"{0:.1f}\".format)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting Depth & Magnitude columns to Float\nfor col in ['Depth', 'Magnitude']:\n    data[col] = data[col].astype('float')\n\n#Converting Time column to datetime\ndata['Time'] =  pd.to_datetime(data['Time'], format='%Y-%m-%d%H:%M:%S.%f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding a new column 'Desc' based on the value in the Magnitude column. Following is the categorization based on the information [here](https://www.gns.cri.nz/Home/Learning/Science-Topics/Earthquakes/Monitoring-Earthquakes/Other-earthquake-questions/What-is-the-difference-between-Magnitude-and-Intensity/The-Richter-Magnitude-Scale)\n\n0 - 2 = Micro\n2 - 3.9 = Minor\n4 - 4.9 = Light\n5 - 5.9 = Moderate\n6 - 6.9 = Strong\n7 - 7.9 = Major\n8 - 9.9 = Great\n10+ = Epic\n\nLet's create a function based on this new information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to categorize Magnitude\ndef desc(mag):\n    if 0 <= mag <= 2.0:\n        return 'Micro'\n    elif 2.0 <= mag <= 3.9:\n        return 'Minor'\n    elif 4.0 <= mag <= 4.9:\n        return 'Light'\n    elif 5.0 <= mag <= 5.9:\n        return 'Moderate'\n    elif 6.0 <= mag <= 6.9:\n        return 'Strong'\n    elif 7.0 <= mag <= 7.9:\n        return 'Major'\n    elif 8.0 <= mag <= 9.9:\n        return 'Great'\n    else:\n        return 'Epic'\n    \n#Applying the function to the Magnitude Column\ndata['Desc'] = data['Magnitude'].apply(lambda mag: desc(mag))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the UTC date-time data in the Time column into UNIX time for easy ingestion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to convert UTC time to Unix Time\ndef ConvertTime(UTCtime):\n    dt = datetime.datetime.strptime(UTCtime, '%Y-%m-%d %H:%M:%S.%f')\n    ut = time.mktime(dt.timetuple())\n    return ut\n\n#Converting to string type as the 'time' only accepts str\ndata['Time'] = data['Time'].astype('str')  \n\n#Applying the function to the Magnitude Column\ndata['Time'] = data['Time'].apply(ConvertTime)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this stage our data is all prep'd and ready for use. Before doing that we shall take a backup of this dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_backup = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#freeing Memory\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Count Plot\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(figsize=(8,8))\nax = sns.countplot(x=\"Desc\", data=data, palette=\"Blues_d\")\n\nplt.title('Earthquake Count')\nplt.ylabel('Count')\nplt.xlabel('Earthquake Magnitude')\n\ntotals = []\n\nfor i in ax.patches:\n    totals.append(i.get_height())\n\ntotal = sum(totals)\n\nfor i in ax.patches:\n   # ax.text(i.get_x()+0.25, i.get_height()+100,str(round((i.get_height()/total)*100, 1))+'%', fontsize=9,color='black')\n   ax.text(i.get_x()+0.25, i.get_height()+100,str(i.get_height()), fontsize=9,color='black')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** The majority of the earthquake were between 0 & 3.9 magnitude.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a plot of Magnitude vs Depth\nplt.figure(figsize=(10,8))\nsns.boxenplot(x='Desc', y='Depth', data=data, scale=\"linear\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**  \n\n* Almost all of the earthquakes occured at the depth of 0 - 300 Kms. \n* Some of the Light magnitude (4.0 - 4.9) earthquakes occured at a depth of beyond 300 Kms. \n* All the Strong magnitude (6.0 - 6.9) earthquakes occured at a depth of less than 100 Kms.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nFeature Engineering is process where the features are extracted from the given dataset. This is done by observing the correlation in the dataset.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a seperate dataset for observing correlation\ndf = pd.DataFrame(data, columns=['Time','Long','Lat', 'Depth', 'Magnitude'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the Correlation\ncorr = df.corr(method='pearson')\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(df.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(df.columns)\nax.set_yticklabels(df.columns)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** As observed from the above plot, there is absolutely no correlation between the columns.\n\nHowever, for the sake of this example we will define the following:\n\n* Features = Time, Latitude, Longitude\n* Target = Magnitude, Depth\n\n**Note**: Here we are going to use the dataframe created for observing correlations. This is because the original dataset contains categorical data and we will have to encode it before training the model. At this stage, I do not want to perform 'Encoding' and that is why using the dataframe with only numerical data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the feature & target\nfeature_col = ['Time','Long','Lat']\ntarget_col = ['Depth','Magnitude']\n\n#Applying to the current dataset\nX = df[feature_col]\ny = df[target_col]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Split (Traing & Test)\n\nSplitting the data into sizeable chunks to train and then test the model. The training dataset will contain 90% of the records & the testing dataset will contain the remaining 10%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 0.1\nstate = 0\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=size, random_state=state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tabulate the Dataset & its size\nt = PrettyTable(['Dataset','Size'])\nt.add_row(['X_train', X_train.size])\nt.add_row(['X_test', X_test.size])\nt.add_row(['y_train', y_train.size])\nt.add_row(['y_test', y_test.size])\nprint(t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build, Train & Test Model (Simple)\n\nIn the simple model, we are going to implement *Random Forest Regressor* model to train, test and predict the output. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build the model\nrfr = RandomForestRegressor(n_estimators=500, criterion='mse', min_samples_split = 4, verbose=0, random_state=0)\n\n#Train the model\nrfr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making a Prediction using the testing-data\ny_pred = rfr.predict(X_test)\n\n#Finding the accuracy & precision of the model\nprint(\"Simple Model Accuracy : \",'{:.1%}'.format(rfr.score(X_test, y_test)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has achieved an accuracy of 63.3%. This is strictly OK , however it can be increased by increasing the \"n_estimators\" in the model.\n\n**Note:** Increasing the \"n_estimators\" will mean that training the model will increase the CPU usage so it is recommended to use GPU.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#garbage collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Neural Network (Complex)\n\nThe above model was a simple model that used a linear regression approach for prediction. This is probably why the model achieved an accuracy of 63.3%. So now, we will try to build a Neural Network using TensorFlow & Keras and then we shall train this model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TensorFlow & Keras Libraries\nimport tensorflow as tf    \nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.metrics import categorical_accuracy\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import *\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building the neural model\ndef build_model():\n    model = Sequential()\n    model.add(Dense(16, activation='relu', input_shape=(3,)))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    \n    model.compile(loss='squared_hinge', metrics=['accuracy'],optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Instantiate the Model\nmodel = build_model()\n\n#Model Architecture Summary\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Train the Model\nmodel.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating the Model\n[test_loss, test_acc] = model.evaluate(X_test, y_test)\nprint(\"Model Evaluation Results on Test Data : Loss = {:.1%}, Accuracy = {:.1%}\".format(test_loss, test_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Wonder Why](https://waterfordwhispersnews.com/wp-content/uploads/2014/11/jpg)\n\n\n**Conclusion**: Hmm, wonder why the accuracy hasn't improved !!, probably because of the small test-data and/or no-correlation between the data.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}