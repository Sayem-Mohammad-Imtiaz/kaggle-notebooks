{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\nimport re\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm_notebook as tqdm\nfrom prettytable import PrettyTable\nimport os\n\nfrom plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"## $1.1$ Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data taken from Kaggle: https://www.kaggle.com/manasvee1/donorschooseorg-application-screening\n\nproject_data = pd.read_csv('../input/train.csv')\nresource_data = pd.read_csv('../input/resources.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how to replace elements in list python: https://stackoverflow.com/a/2582163/4084039\ncols = ['Date' if x=='project_submitted_datetime' else x for x in list(project_data.columns)]\n\n#sort dataframe based on time pandas python: https://stackoverflow.com/a/49702492/4084039\nproject_data['Date'] = pd.to_datetime(project_data['project_submitted_datetime'])\nproject_data.drop('project_submitted_datetime', axis=1, inplace=True)\nproject_data.sort_values(by=['Date'], inplace=True)\n\n# how to reorder columns pandas python: https://stackoverflow.com/a/13148611/4084039\nproject_data = project_data[cols]\n\nproject_data.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.2a$ preprocessing of `project_subject_categories`"},{"metadata":{"trusted":true},"cell_type":"code","source":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.2b$ preprocessing of `project_subject_subcategories`\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https://stackoverflow.com/a/47301924/4084039\n# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n# https://stackoverflow.com/questions/23669024/how-to-strip-a-specific-word-from-a-string\n# https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.2c$ preprocessing of `project_grade_category`"},{"metadata":{"trusted":true},"cell_type":"code","source":"proj_grade_cat = []\n\nfor i in range(len(project_data)):\n    pgc = project_data[\"project_grade_category\"][i].replace(\" \", \"_\")\n    proj_grade_cat.append(pgc)\n    \nproject_data.drop(['project_grade_category'], axis=1, inplace=True)\nproject_data[\"project_grade_category\"] = proj_grade_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.3$ Text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef getProcessedData(txt_type, working_data):\n    preprocessed_data = []\n    # tqdm is for printing the status bar\n    \n    for sentance in tqdm(working_data[txt_type].values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https://gist.github.com/sebleier/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_data.append(sent.lower().strip())\n        \n    return preprocessed_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2><font color='red'> $1.4$ Preprocessing of `project_title`</font></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Covered Above ...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## $1.5$ Preparing data for models\nwe are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical"},{"metadata":{},"cell_type":"markdown","source":"### $1.5.1$ Vectorizing Categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getCountDict(cat_type):\n    count_dict = {}\n    info_list = project_data[cat_type]\n    project_data.loc[project_data[cat_type].isnull(), cat_type] = 'nan'\n    \n    for phrase in info_list:\n        for data in phrase.split():\n            if data not in count_dict: count_dict[data] = 0\n            #elif data not in ['nan', np.nan]:\n            else:\n                count_dict[data] += 1\n            \n    return dict(sorted(count_dict.items(), key=lambda x: x[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef getFitCAT_Vectorizer(working_data, cat_type, hstack_features):\n    '''\n    Fit on only train data.\n    '''\n    working_data.loc[working_data[cat_type].isnull(), cat_type] = 'nan'\n    #print (working_data.keys())\n    \n    if 1:\n        sorted_cat_dict = getCountDict(cat_type)\n        print ('Keys...', sorted_cat_dict.keys())\n        hstack_features += sorted_cat_dict.keys()\n        vectorizer = CountVectorizer(vocabulary=sorted_cat_dict.keys(), lowercase=False, binary=True)\n    \n    vectorizer.fit(working_data[cat_type].values)\n    return vectorizer\n    \ndef getVectorizeCategData(working_data, cat_type, data_type):\n    working_data.loc[working_data[cat_type].isnull(), cat_type] = 'nan'\n    \n    categories_one_hot = vectorizer.transform(working_data[cat_type].values)\n    #print(vectorizer.get_feature_names())\n    print(\"Shape of matrix after one hot encodig \",categories_one_hot.shape)\n    \n    return categories_one_hot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $1.5.2$ Vectorizing Text data"},{"metadata":{},"cell_type":"markdown","source":"#### $1.5.2.1$ Bag of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFitBOW_Vectorizer(preprocessed_data):\n    vectorizer = CountVectorizer(min_df=10)\n    vectorizer.fit(preprocessed_data)\n    \n    return vectorizer\n\ndef getBOWVectorizeTxtData(preprocessed_data, vectorizer):\n    text_bow = vectorizer.transform(preprocessed_data)\n    print(\"Shape of matrix after one hot encodig \",text_bow.shape)\n    \n    return text_bow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.5.2.2 TFIDF vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef getFitTFIDF_Vectorizer(preprocessed_data):\n    vectorizer = TfidfVectorizer(min_df=10)\n    vectorizer.fit(preprocessed_data)\n    return vectorizer\n\ndef getTFIDFVectorizeTxtData(preprocessed_data, vectorizer):\n    text_tfidf = vectorizer.transform(preprocessed_data)\n    print(\"Shape of matrix after one hot encodig \",text_tfidf.shape)\n    return text_tfidf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $1.5.3$ Vectorizing Numerical features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nproject_data = pd.merge(project_data, price_data, on='id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\nimport warnings \nwarnings.filterwarnings(\"ignore\") \n\ndef getFitNUM_Vectorizer(working_data, num_type):\n    '''\n    Fit on only train data.\n    '''\n    \n    num_scalar = Normalizer()\n    num_scalar.fit(working_data[num_type].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n    return num_scalar\n\ndef getNUM_Vectors(working_data, num_type, num_scalar):\n    # Now standardize the data with above maen and variance.\n    num_standardized = num_scalar.transform(working_data[num_type].values.reshape(-1, 1))\n    #print(f\"Mean : {num_scalar.mean_[0]}, Standard deviation : {np.sqrt(num_scalar.var_[0])}\")\n    return num_standardized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $1.5.4$ Merging all the above features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\ndef getMergedFeatures(working_data, merge_on):\n    valid_cols = []\n    for key, value in working_data.items():\n        if key in merge_on:\n            valid_cols.append(value)\n   \n    return hstack(tuple(valid_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assignment $10$: Clustering"},{"metadata":{},"cell_type":"markdown","source":"- <font color='red'>step 1</font>: Choose any vectorizer (data matrix) that you have worked in any of the assignments, and got the best AUC value.\n- <font color='red'>step 2</font>: Choose any of the <a href='https://scikit-learn.org/stable/modules/feature_selection.html'>feature selection</a>/<a href='https://scikit-learn.org/stable/modules/decomposition.html'>reduction algorithms</a> ex: selectkbest features, pretrained word vectors, model based feature selection etc and reduce the number of features to 5k features\n- <font color='red'>step 3</font>: Apply all three kmeans, Agglomerative clustering, DBSCAN\n    - <strong>K-Means Clustering:</strong> <br>\n        ● Find the best ‘k’ using the elbow-knee method (plot k vs inertia_)<br>\n    - <strong>Agglomerative Clustering: </strong><br>\n        ● Apply <a href='https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/'>agglomerative algorithm</a> and try a different number of clusters like 2,5 etc. <br>\n        ● You can take less data points (as this is very computationally expensive one) to perform hierarchical clustering because they do take a considerable amount of time to run. <br>\n    - <strong>DBSCAN Clustering: </strong><br>\n        ● Find the best ‘eps’ using the <a href='https://stackoverflow.com/a/48558030/4084039'>elbow-knee method</a>.<br>\n        ● You can take a smaller sample size for this as well.\n- <font color='red'>step 4</font>: Summarize each cluster by manually observing few points from each cluster.\n- <font color='red'>step 5</font>: You need to plot the word cloud with essay text for each cluster for each of algorithms mentioned in <font color='red'>step 3</font>."},{"metadata":{},"cell_type":"markdown","source":"<h1>$2.$ Clustering </h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.1$ Choose the best data matrix on which you got the best AUC</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classes of X & project_data have almost same proportion.\nX = project_data[:10000]\n\ny = X['project_is_approved']\n\n# BOW matrix gave the best AUC value in NB Machine Learning model.\nset1_cols = ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix',\n             'price', 'teacher_number_of_previously_posted_projects', \n             'essay_text_bow', 'project_title_text_bow']\n\nplt_title1 = 'BOW'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.2$ Make Data Model Ready: encoding numerical, categorical features</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ordered dict will be used to ensure one to one correspondence between datapoints features and hstack_features.\n\nfrom collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict = OrderedDict({})\ncols_dict = OrderedDict({'cat_cols': ['school_state','clean_categories', 'clean_subcategories', 'project_grade_category', 'teacher_prefix'],\n                 'num_cols': ['price', 'teacher_number_of_previously_posted_projects']\n            })\nhstack_features = []\n\nfor col_type, cols_name in cols_dict.items():\n    if col_type == 'cat_cols':\n        for cat_type in cols_name:\n            print (cat_type)\n            vectorizer = getFitCAT_Vectorizer(X, cat_type, hstack_features)\n            hot_encode = getVectorizeCategData(X, cat_type, vectorizer)\n            data_dict[cat_type] = hot_encode\n    else:\n        for num_type in cols_name:\n            vectorizer = getFitNUM_Vectorizer(X, num_type)\n            hstack_features.append(num_type)\n            num_vectors = getNUM_Vectors(X, num_type, vectorizer)\n            data_dict[num_type] = num_vectors\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.3$ Make Data Model Ready: encoding eassay, and project_title</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_type in ['essay','project_title']:\n    preprocessed_data = getProcessedData(col_type, X)\n    vectorizer_bog = getFitBOW_Vectorizer(preprocessed_data)\n    text_bow = getBOWVectorizeTxtData(preprocessed_data, vectorizer_bog)\n    data_dict['%s_text_bow'%col_type] = text_bow\n    \n    if col_type == \"essay\":\n        essay_hot_info = (vectorizer_bog.get_feature_names(), text_bow.toarray())\n           \n    hstack_features += vectorizer_bog.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.4$ Dimensionality Reduction on the selected features & defining Wordcloud functions </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n# Reference: chi2 isn't working due to negative values; used https://stackoverflow.com/questions/25792012/feature-selection-using-scikit-learn\n\nfrom sklearn.feature_selection import SelectKBest, f_classif \n\ndata_matrix = getMergedFeatures(data_dict, set1_cols)\nprint (data_matrix.shape, len(hstack_features))\n\nselector = SelectKBest(f_classif, k=5000)\nX_new = selector.fit_transform(data_matrix, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\ndef getCorupusDict(essay_hot_info, y_pred):\n    one_hot_featr, one_hot_enc = essay_hot_info\n    one_hot_enc_cols = one_hot_enc.shape[1]\n    corpus_dict = {}\n    i = 0\n    for each_x in tqdm(y_pred):\n        if each_x not in corpus_dict: corpus_dict[each_x] = ''\n        for j in range(one_hot_enc_cols):\n            if one_hot_enc[i][j] >= 0.5:\n                corpus_dict[each_x] = \"%s %s\"%(corpus_dict[each_x], one_hot_featr[j].strip())\n        i += 1\n    \n    return corpus_dict\n\ndef plotWordCloud(word_corpus, i, algo_title):\n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white', \n                    stopwords = stopwords,\n                    collocations = False,\n                    min_font_size = 10).generate(word_corpus) \n    \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.title('The word cloud with essay text for cluster no. %s for algorithm- %s'%(i, algo_title))\n    #plt.tight_layout(pad = 0) \n\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.5$ Apply Kmeans</h2>"},{"metadata":{},"cell_type":"markdown","source":"$\\rightarrow$ Working on $10k$ data points."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ndef getLosses(hypers, data_matrix):\n    obj_loss = []\n    \n    for k in hypers:\n        kmeans = KMeans(n_clusters=k, random_state=0, n_jobs=-1).fit(data_matrix)\n        obj_loss.append(kmeans.inertia_)\n        \n    return obj_loss\n\ndef plotGraph(hypers, obj_loss):\n    plt.plot(hypers, obj_loss, label='knee')\n    \n    plt.title(\"Plot to find best K using elbow-knee method\")\n    plt.xlabel('number of clusters (K)')\n    plt.ylabel('loss value')\n    plt.legend()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo_title = 'Kmeans' \nhypers = [4, 9, 16, 25, 36]\nlosses = getLosses(hypers, X_new)\nplotGraph(hypers, losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting total clusters $=15$ after analysing the plot. **Once selected, plotting wordcloud for each cluser.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=15, random_state=0, n_jobs=-1).fit(X_new)\ncorpus_dict = getCorupusDict(essay_hot_info, kmeans.labels_)\ncorpus_dict = dict(sorted(list(corpus_dict.items()), key=lambda x: x[0]))\nfor key, val in corpus_dict.items():\n    plotWordCloud(val, key, algo_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations** - "},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\nfrom collections import Counter\n\ntable = PrettyTable()\ntable.field_names = [\"Cluster No\", \"No. of words in cluster\", \"Most frequent words\"]\n\nfor key, val in corpus_dict.items():\n    freq_dict = dict(sorted(list(Counter(val.split()).items()), key=lambda x: x[1], reverse=True))\n    table.add_row([key, len(val.split()), \",\".join(list(freq_dict.keys())[:10])])\n\nprint (table) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.6$ Apply AgglomerativeClustering</h2>"},{"metadata":{},"cell_type":"markdown","source":"$\\rightarrow$ Working on $5k$ data points."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as shc\n\nX_new_agg = X_new.todense()[:5000]\n\nalgo_title = 'Agglomerative Clustering'\nplt.figure(figsize=(10, 7))  \nplt.title(\"Dendogram\")  \ndend = shc.dendrogram(shc.linkage(X_new_agg, method='ward'))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting total clusters $=3$ after analysing the dendogram. **Once selected, plotting wordcloud for each cluser.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(X_new_agg)  \n\ncorpus_dict = getCorupusDict(essay_hot_info, cluster.labels_)\ncorpus_dict = dict(sorted(list(corpus_dict.items()), key=lambda x: x[0]))\nfor key, val in corpus_dict.items():\n    plotWordCloud(val, key, algo_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"table = PrettyTable()\ntable.field_names = [\"Cluster No\", \"No. of words in cluster\", \"Most frequent words\"]\n\nfor key, val in corpus_dict.items():\n    freq_dict = dict(sorted(list(Counter(val.split()).items()), key=lambda x: x[1], reverse=True))\n    table.add_row([key, len(val.split()), \",\".join(list(freq_dict.keys())[:10])])\n\nprint (table) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>$2.7$ Apply DBSCAN</h2>"},{"metadata":{},"cell_type":"markdown","source":"$\\rightarrow$ Working on $5k$ data points."},{"metadata":{},"cell_type":"markdown","source":"Considering minPts $= log_e{\\|size\\ of\\ data set\\|} = \\lfloor log_e(5000) \\rfloor = 8$ (Reference [here](https://stackoverflow.com/questions/12893492/choosing-eps-and-minpts-for-dbscan-r/48558030#48558030))"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KDTree\n\nalgo_title = 'DBSCAN Clustering'\nminPts = 8\ntree = KDTree(X_new_agg)\n\nidx = 0\nepss = []\nfor x_i in tqdm(X_new_agg):\n    epss.append(tree.query(X_new_agg[idx], return_distance=True, k=minPts)[0][0][-1])\n    idx += 1\nepss.sort()\n\nplt.plot(range(0,5000), epss[:5000])\nplt.title(\"Plot to find best eps using elbow-knee method\")\nplt.xlabel('Integers')\nplt.ylabel('eps-values')\nplt.legend('kneee')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecting optimal radius $\\epsilon=19$ after analysing the plot. **Once selected, plotting wordcloud for each cluser.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\ncluster = DBSCAN(eps=19, min_samples=minPts).fit(X_new_agg)\ncorpus_dict = getCorupusDict(essay_hot_info, cluster.labels_)\nprint (\"number of clusters gotten:\", len(corpus_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_dict = dict(sorted(list(corpus_dict.items()), key=lambda x: x[0]))\nfor key, val in corpus_dict.items():\n    plotWordCloud(val, key, algo_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"table = PrettyTable()\ntable.field_names = [\"Cluster No\", \"No. of words in cluster\", \"Most frequent words\"]\n\nfor key, val in corpus_dict.items():\n    freq_dict = dict(sorted(list(Counter(val.split()).items()), key=lambda x: x[1], reverse=True))\n    table.add_row([key, len(val.split()), \",\".join(list(freq_dict.keys())[:10])])\n\nprint (table) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}