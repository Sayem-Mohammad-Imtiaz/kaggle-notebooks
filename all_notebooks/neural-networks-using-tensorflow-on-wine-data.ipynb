{"cells":[{"metadata":{"_uuid":"7b0d2966fb9cbd86d5c90b1355b4bbc2739b2ea6"},"cell_type":"markdown","source":"# Neural Network on Winedata using Tensorflow"},{"metadata":{"_uuid":"f8f88a50ec010524c9dbe4293d2409c31671773d"},"cell_type":"markdown","source":"This a simple neural network classification on the winedata, I have kept it simple, not much of the data preprocessing and visualization. "},{"metadata":{"_uuid":"aa739b87cdd9e0664d286c1dfc85715b46eecbb9"},"cell_type":"markdown","source":"Frankly speaking, this dataset is not a worth of Neural Networks, but I chose this dataset, since this is my first attempt on NN's and i want to keep it simple."},{"metadata":{"_uuid":"b1a9bf225c1ba382c89d8cd0050af18c8d33d6db"},"cell_type":"markdown","source":"The dataset is available at 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'"},{"metadata":{"trusted":true,"_uuid":"4584f175e08628f22196430292044cdc66956035","collapsed":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8d11b97dff1f253278d33cb6d513d1fb7524c6d"},"cell_type":"markdown","source":" The Column names and other details are at 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names'"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0cd301c6a2fd9cc7297fa31d044af8bd724dff47"},"cell_type":"code","source":"colnames = ['Class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','dilute','Proline']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d013770d01f73d52409b6f5b1291a6ae2a5a254b","collapsed":true},"cell_type":"code","source":"df = pd.read_csv('../input/wine.data.txt',names = colnames,index_col = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ef5938c7a49f33539de591f68337de411dd3e96"},"cell_type":"markdown","source":"Snapshot of the data"},{"metadata":{"trusted":true,"_uuid":"96c472cbd9b516079a0d42609004be0978f53474","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"436eec5677ca31fe3e8c69266d379f7f38f2570a"},"cell_type":"markdown","source":"Check if it has any null values"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9ca501d7a854eaaabed40bbb3568c451c3bda9c7"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"e26d4117dd593aaa2aceb99ad8201aba88a9956c"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03c0b3975ce654c11db8e218ffdea206f8396c32"},"cell_type":"markdown","source":"To know about the class distribution."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5b16161b164aedb4f9061d48a3840f04e97a323f"},"cell_type":"code","source":"df.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b437422071236f38b3cc517020f7cea1dac2cbc8"},"cell_type":"markdown","source":"Check which columns are correlated "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b66f5e3d584eb41159e219e075616076cac72eba"},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5363b634cc94b75ea2eac0bcea956208c7751dc0"},"cell_type":"markdown","source":"As shown above Ash has very very low correlation in detecting the class, so it can be removed to improve our model. I'm dropping the ash columns in the following code."},{"metadata":{"_uuid":"45f0b86c108967aef039681ce250926370ccf8cd"},"cell_type":"markdown","source":"# We convert the Class labels into the Onehot format."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7f244830d1f691e57003649d40e2bd8d3e871c75"},"cell_type":"code","source":"df = pd.get_dummies(df, columns=['Class'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8005c4aee5105f1459172fe665ceb911accd82c5"},"cell_type":"markdown","source":"Now only take the labels into to new dataframe"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7274d0ffcc981119a21f039d30852334ecd371a9"},"cell_type":"code","source":"labels = df.loc[:,['Class_1','Class_2','Class_3']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e11556e62dfcf51ddea7c49c1efe105cae239cc"},"cell_type":"markdown","source":"For Neural Nets the data should be in numpy arrays, so convert them,"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"24dd30b0e361e2e1b4be06d1b1c20630e5d731d7"},"cell_type":"code","source":"labels = labels.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64dc586a494787d416af97c23257323feaf8039e"},"cell_type":"markdown","source":"Now collect the features dataframe"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ff7809dcd912f76e16b674eacecf2d3cb4abea7c"},"cell_type":"code","source":"features = df.drop(['Class_1','Class_2','Class_3','Ash'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9dd29efdf49bcf49622c4b9ba75c34c35e04316"},"cell_type":"markdown","source":"Convert the feature dataframe to numpy arrays"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6baa8c2e4eb1218357ac757022f357eac5dd0f79"},"cell_type":"code","source":"features = features.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bc378fbb50b7b4321826720f164c3d9fd851f318"},"cell_type":"code","source":"print(type(labels))\nprint(type(features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f21b08c90f04dab50f31fcbd9e3f44669f75795"},"cell_type":"markdown","source":"Check the shape of the arrays"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1573e1df72d847de663f1f5341a76d9f37f85c1b"},"cell_type":"code","source":"print(labels.shape)\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35e3d2bb11aad09d82b9e3fe3a0076cb2253db44"},"cell_type":"markdown","source":"Split into the training and testing sets, We have just 178 columns, which is a very very very small data for NN's"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2a5a8ee9417e53c376b759e2703a49a3001944f7"},"cell_type":"code","source":"train_x,test_x,train_y,test_y = train_test_split(features,labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d56a341ce2c372d5a66b410f1ecc0d734cb7b88"},"cell_type":"markdown","source":"Print the shapes of the split datasets."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e946007cd8e365b91cdaf6e66e0d41423bcdaa0c"},"cell_type":"code","source":"print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c51fb593960868864b2c6f79ebccedef654adb51"},"cell_type":"markdown","source":"Everything looks good, so lets go further."},{"metadata":{"_uuid":"3c18e6b561b93ace905d9685674d97c1981ed7dc"},"cell_type":"markdown","source":"Neural Networks perform better if the data is scaled between (0,1). So, lets do it."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"895e710336a842a3c1ebe642fc77e293a8a7e726"},"cell_type":"code","source":"scale = MinMaxScaler(feature_range = (0,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cd0ec098e4a0d9183d7a2388e5420b0e56e679e0"},"cell_type":"code","source":"train_x = scale.fit_transform(train_x)\ntest_x = scale.fit_transform(test_x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d7aa92a7f2a50a987368766758ef9da76138584"},"cell_type":"markdown","source":"Snapshot of the features and labels"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3974ea3b69f816fdaf88821d711fd831f48ed13a"},"cell_type":"code","source":"print(train_x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"526fee9385d03581cfaa78ccf7536eb73ae14b9d"},"cell_type":"code","source":"print(train_y[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b73e6f52d8c687efc2acfb2915e50fce8c7bff"},"cell_type":"markdown","source":"# The Neural Network part begins here."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"511cd3b0ca2e776c31bcacc04d1fcf31ec03c5b3"},"cell_type":"code","source":"X = tf.placeholder(tf.float32,[None,12]) # Since we have 12 features as input\ny = tf.placeholder(tf.float32,[None,3])  # Since we have 3 outut labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cb8a26ea2ac4e36e620bb207ae7222ae7a70928"},"cell_type":"markdown","source":"Lets create our model with 2 hidden layers with 80 and 50 nodes respectively.\nIt was suggested(by online tutor) to use xavier_initializer for weights and zeros initializer for biases, but not mandatory.( I have been used to it, so i continued with this)\nI have also ran the model with random weights, they eventually optimize."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f88795f018754ca3269c44dcdbf98b19a81cbdc2"},"cell_type":"code","source":"weights1 = tf.get_variable(\"weights1\",shape=[12,80],initializer = tf.contrib.layers.xavier_initializer())\nbiases1 = tf.get_variable(\"biases1\",shape = [80],initializer = tf.zeros_initializer)\nlayer1out = tf.nn.relu(tf.matmul(X,weights1)+biases1)\n\nweights2 = tf.get_variable(\"weights2\",shape=[80,50],initializer = tf.contrib.layers.xavier_initializer())\nbiases2 = tf.get_variable(\"biases2\",shape = [50],initializer = tf.zeros_initializer)\nlayer2out = tf.nn.relu(tf.matmul(layer1out,weights2)+biases2)\n\nweights3 = tf.get_variable(\"weights3\",shape=[50,3],initializer = tf.contrib.layers.xavier_initializer())\nbiases3 = tf.get_variable(\"biases3\",shape = [3],initializer = tf.zeros_initializer)\nprediction =tf.matmul(layer2out,weights3)+biases3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c49b7fb31a11f0ff4e422bc8e0629afc1194327d"},"cell_type":"markdown","source":"Define the loss function, softmax_cross_entropy_with_logits_v2 is suggested over softmax_cross_entropy_with_logits because of label backpropagation, and then optimize the loss function. I have choose the learning rate as 0.001\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"626a6a7f693016ad266b21ce81af4f26894e52aa"},"cell_type":"code","source":"cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ab08d82e31ad135186cf55927a4d2b991d6890d"},"cell_type":"markdown","source":"I'm expecting that everyone has an idea about the below process so I'm not going to elaborate much on this."},{"metadata":{"_uuid":"07bed46cdd796d150a2f0bbf596507bd97c88dfc"},"cell_type":"markdown","source":"Matches is a list(tensor) which takes 1, if the index of largest element in prediction and y are equal and 0 it the indices are not equal.\nAccuracy is calculated by taking the mean of those matches."},{"metadata":{"trusted":true,"_uuid":"b3143ac9e0a1ae5372ecf8dd64e4766af04cf99c","collapsed":true},"cell_type":"code","source":"acc = []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(201):\n        opt,costval = sess.run([optimizer,cost],feed_dict = {X:train_x,y:train_y})\n        matches = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n        accuracy = tf.reduce_mean(tf.cast(matches, 'float'))\n        acc.append(accuracy.eval({X:test_x,y:test_y}))\n        if(epoch % 100 == 0):\n            print(\"Epoch\", epoch, \"--\" , \"Cost\",costval)\n            print(\"Accuracy on the test set ->\",accuracy.eval({X:test_x,y:test_y}))\n    print(\"FINISHED !!!\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ddc87a87b9745c378a91dc85f460f47572d0ede"},"cell_type":"markdown","source":"We can see that they cost(loss) is reducing and the Accuracy is increasing, which shows that our model is training."},{"metadata":{"_uuid":"4280bf8f9c83d3aa05e69bbf0df0fb1353df090e"},"cell_type":"markdown","source":"Lets plot the Accuracy over epochs"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"dd42e4f12a2c71330f3a7a442617ac29bf0d0948"},"cell_type":"code","source":"plt.plot(acc)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fa8cd14a91a5662d2240ea8af1a7459c5052082"},"cell_type":"markdown","source":"The graph of the accuracy over training steps(Epochs)"},{"metadata":{"_uuid":"9eb5c8a9366521926680cae11910cee503651d66"},"cell_type":"markdown","source":"# Last words.\nThis is not the best dataset for the neural networks. I would suggest something huge, like really huge.\nThe accuracy we got might not the best we can get.We can tune the model more by changing the epochs, learning rate.\nThere is no much significane of this kernel, better accuracies might be achieved by using sklearn's logisticregression etc.This is just for my understanding of the neural networks.\nThankyou!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}