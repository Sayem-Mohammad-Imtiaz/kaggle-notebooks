{"cells":[{"metadata":{"_uuid":"47aa60c4-f966-4f9e-87ac-e34f0cd17fde","_cell_guid":"af476837-b1c2-4c7e-be09-fc43ecc133c9","trusted":true},"cell_type":"markdown","source":"**(1) Introduction**\n# \n# Here is my Image Input/Digit Recognizer colab kernel."},{"metadata":{"_uuid":"227baf1e-fcef-43ed-9244-455c0cc77164","_cell_guid":"b1884d79-3cac-49b6-9516-9a935134d69b","trusted":true},"cell_type":"markdown","source":"**(2) Package Imports**"},{"metadata":{"_uuid":"d5e1689d-1958-4f67-896a-4fe89358adb2","_cell_guid":"86fa23df-fa96-489a-bbdf-d3dcb3b1a23b","trusted":true},"cell_type":"code","source":"!pip install image_slicer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90165fff-2bf8-4dbe-824e-e7c42e0ea184","_cell_guid":"ea2c5b1d-a1f9-454d-80c5-05e2fd7cb8c1","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport seaborn as sns\nfrom PIL import Image, ImageFilter\nimport keras\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport image_slicer\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4ddd3a0-7fab-4923-be19-2d3c724132a3","_cell_guid":"a1e26dd2-7080-4265-98d3-e88e3405acb1","trusted":true},"cell_type":"markdown","source":"**(2) Data Import**"},{"metadata":{"_uuid":"008ecd03-12bd-4815-aa0d-b6a61f7aa29a","_cell_guid":"53e8304d-0a9f-405e-9790-fa5371e83448","trusted":true},"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/digit-recognizer/train.csv')\ntest = pd.read_csv(r'/kaggle/input/digit-recognizer/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"128cbb1a-df24-42ff-8576-04cbc1465bc4","_cell_guid":"08ccde72-a75a-42b6-b1fd-207a0703f401","trusted":true},"cell_type":"code","source":"data_check = train.head(10)\nprint(data_check)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f67ba83-1cac-42c7-a0c6-e0f7968015b0","_cell_guid":"3ff56cee-7ad6-42a6-8c5a-12682c1ca77e","trusted":true},"cell_type":"markdown","source":"Check the datasets for nulls using the following code."},{"metadata":{"_uuid":"027b8bef-042d-4b22-8f44-07ab10b69659","_cell_guid":"36b6a6c6-b5d9-4db1-a062-e9891ef522ee","trusted":true},"cell_type":"code","source":"train.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"434bbff2-4117-4d9c-9c44-736a9029c932","_cell_guid":"50d0f07b-f78f-4fb5-81fd-b3f78c4bca05","trusted":true},"cell_type":"code","source":"test.isnull().any().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"325cc3a7-acfd-4e8d-8751-73f69f0b4476","_cell_guid":"56eaef62-3bbc-41bc-b979-94885c604559","trusted":true},"cell_type":"markdown","source":"Checking the numbers."},{"metadata":{"_uuid":"e9cac0c0-d1c0-46df-b29f-6b1a09fa3ea9","_cell_guid":"4943337c-040e-42eb-b657-4b6a802bb568","trusted":true},"cell_type":"code","source":"print((train['label'].value_counts()))\nsns.countplot(train['label'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91ca85f1-c425-49ec-abd6-d575285caff2","_cell_guid":"54c7750d-1c9e-411e-b410-4b4e066c64ba","trusted":true},"cell_type":"code","source":"# Split the dataset into features (X_) and target variable (Y_)\n# Change the data types to suit.\nX_train = (train.iloc[:, 1:].values).astype('float32')\nY_train = train.iloc[:, 0].values.astype('int32')\n\nX_test = test.values.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94530240-2d51-4e93-878e-a994191a4d1f","_cell_guid":"dda2b944-d30c-4e82-8bca-0c41c4be1bcb","trusted":true},"cell_type":"code","source":"# View some example 28x28pixel images of digits.\nplt.figure(figsize=(20, 8))\nx, y = 10, 4\nfor i in range(40):\n    plt.subplot(y, x, i + 1)\n    plt.imshow(X_train[i].reshape((28, 28)), interpolation='nearest')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ee82703-603c-47d3-b938-9a6719a62f6e","_cell_guid":"1423d385-3d3d-4a22-82a0-8b2052daa24b","trusted":true},"cell_type":"code","source":"# Function to visualise an example as a grey-scale image.\ndef visualize_input(img, ax):\n    ax.imshow(img, cmap='gray')\n    wdth, hgt = img.shape\n    thresh = img.max()/2.5\n    for x in range(wdth):\n        for y in range(hgt):\n            ax.annotate(str(round(img[x][y], 2)), xy=(y, x),\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                        color='white' if img[x][y] < thresh else 'black')\n\n\n# convert the training set into integer values for the gray-scale comparison.\nX_train = (train.iloc[:, 1:].values).astype('int32')\n# define the size of the figure\nfig = plt.figure(figsize=(12, 12))\n# define subplots\nax = fig.add_subplot(111)\n# execute visualize_input function with first element in training set\nvisualize_input(X_train[1].reshape(28, 28), ax)\n\n# convert training set back into float values\nX_train = (train.iloc[:, 1:].values).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9677e96e-04b7-4bba-bb73-24942826ba89","_cell_guid":"db5de447-c246-45cf-b26f-95afc169ad53","trusted":true},"cell_type":"code","source":"# Convolutional Neural Networks (CNNs) converge quicker on [0->1] data rather than [0->255].\n# Code to convert is below.\nX_train = X_train/255.0\nX_test = X_test/255.0\n\n# The data now needs to be re-shaped to convert the data from 1x784 column row to a 28pixel by 28pixel 'image'.\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n\n# Encode labels to one hot vectors (e.g. 2 -> [0,0,1,0,0,0,0,0,0,0])\n# Dummy transforming the labels. A 2 in the label column would give:\n# label = 0: 0, label = 1: 0, label = 2: 1, label = 3: 0, etc...\nY_train = to_categorical(Y_train, num_classes=10)\n\n# Splitting the training dataset into train and validation (test) sets using the sklearn split package.\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=56)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80d93ec2-d018-46c5-9cc1-7f1dd7527dd1","_cell_guid":"cb09d532-5213-459a-b27b-efa9f2bc4f80","trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d0c9ed-65c1-4fdf-9b4a-93569917aa4b","_cell_guid":"e6dcd501-1523-4e75-a4bb-629fb26b38e9","trusted":true},"cell_type":"markdown","source":"An Epoch represents one iteration over the entire dataset.\n# \n# Because we cannot pass the entire dataset into a neural network at once.\n# \n# We need to divide the dataset into batches to feed into the NN.\n# \n# We have 37800 'images' as data and a batch size of 64, then an epoch should contain 37800/64 = ~590 iterations."},{"metadata":{"_uuid":"b6f4b154-eb8b-40da-a875-61b5bfd3fb0e","_cell_guid":"4ada9cde-b36d-452c-856a-267574337390","trusted":true},"cell_type":"code","source":"# Setting the CNN model.\nbatch_size = 64\nepochs = 3\ninput_shape = (28, 28, 1)\n\n# Sequential API which adds one model at a time.\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n                 kernel_initializer='he_normal', input_shape=input_shape))\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n                 kernel_initializer='he_normal'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(Dropout(0.20))\nmodel.add(Conv2D(64, (3, 3), activation='relu',\n                 padding='same', kernel_initializer='he_normal'))\nmodel.add(Conv2D(64, (3, 3), activation='relu',\n                 padding='same', kernel_initializer='he_normal'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='relu',\n                 padding='same', kernel_initializer='he_normal'))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10, activation='softmax'))\n\n# Define the optimizer and compile the model\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n                                            patience=3,\n                                            verbose=1,\n                                            factor=0.5,\n                                            min_lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d38440-b6dc-4c06-bee8-64227c2eb4ed","_cell_guid":"069c9508-5d71-497c-8a83-99292a9ddf67","trusted":true},"cell_type":"markdown","source":"Data augmentation to alter the images to imporve the model and to hinder any overfitting."},{"metadata":{"_uuid":"aa59d104-2c16-44ed-b6f1-f5565f3026d9","_cell_guid":"cac2cd99-d296-4aac-95a4-64abe6673644","trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(featurewise_center=False, samplewise_center=False,\n                             featurewise_std_normalization=False,\n                             samplewise_std_normalization=False, zca_whitening=False,\n                             rotation_range=15, zoom_range=0.1, width_shift_range=0.1,\n                             height_shift_range=0.1, horizontal_flip=False, vertical_flip=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4961a858-8e7a-49d3-a917-34766eaa34ce","_cell_guid":"28789bd1-09b5-4e79-baea-293f434097ea","trusted":true},"cell_type":"code","source":"# View a summary of the model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1840c792-2730-4892-8acf-ccddc5d5aa1f","_cell_guid":"698e9531-b25f-47e4-9dc1-22832fb52e51","trusted":true},"cell_type":"code","source":"datagen.fit(X_train)\nh = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n                        epochs=epochs, validation_data=(X_val, Y_val), verbose=1,\n                        steps_per_epoch=X_train.shape[0] // batch_size,\n                        callbacks=[learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e8c2ec4-6e50-45d3-9b73-d17b9c614ea0","_cell_guid":"4cbb5c92-67cd-4155-b07c-bb032f994f5e","trusted":true},"cell_type":"code","source":"final_loss, final_acc = model.evaluate(X_val, Y_val, verbose=0)\nprint(\"validation loss: {0:.6f}, validation accuracy: {1:.6f}\".format(final_loss, final_acc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69709409-ca31-43cf-9ebd-a00dbeea2272","_cell_guid":"40c8ccb3-7cfb-488f-824c-757bbb1259aa","trusted":true},"cell_type":"code","source":"# Visualise the errors using a confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Digit Confusion matrix',cmap=plt.cm.Greens):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors\nY_pred_classes = np.argmax(Y_pred, axis=1)\n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val, axis=1)\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes=range(11))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e54f2716-976d-4997-9e81-1de5d98e9266","_cell_guid":"a8f11729-55f3-44bf-aeee-263ea2edd18d","trusted":true},"cell_type":"code","source":"errors = (Y_pred_classes - Y_true != 0)\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\n\ndef display_errors(errors_index, img_errors, pred_errors, obs_errors):\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows, ncols)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row, col].imshow((img_errors[error]).reshape((28, 28)))\n            ax[row, col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error], obs_errors[error]))\n            n += 1\n\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors, axis=1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors\nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2068cd9a-5c57-40a2-929f-cfe4bd601dc4","_cell_guid":"a94da4d7-3485-4f85-8740-15c7ca73c288","trusted":true},"cell_type":"code","source":"# Function to convert an image into 28x28pixel data entry.\n\ndef imageprepare(argv):\n    '''\n    This function returns the pixel values.\n    The input is a png file location.\n    '''\n    im = Image.open(argv).convert('L')\n    width = float(im.size[0])\n    height = float(im.size[1])\n    # create white canvas of 28x28pixels.\n    newImage = Image.new('L', (28, 28), (255))\n    \n    # Check which dimension is bigger:\n    if width > height:  \n        # Width is bigger. Width becomes 20 pixels.\n        # Re-size height according to ratio.\n        nheight = int(round((20.0 / width * height), 0))\n        if (nheight == 0):\n            nheight = 1\n        img = im.resize((20, nheight), Image.ANTIALIAS) \\\n            .filter(ImageFilter.SHARPEN)\n        wtop = int(round(((28 - nheight) / 2), 0))\n        newImage.paste(img, (4, wtop))\n    else:\n        # Height is bigger. Height becomes 20 pixels.\n        # Re-size width according to ratio.\n        nwidth = int(round((20.0 / height * width), 0))\n        if (nwidth == 0):\n            nwidth = 1\n        img = im.resize((nwidth, 20), Image.ANTIALIAS) \\\n            .filter(ImageFilter.SHARPEN)\n        wleft = int(round(((28 - nwidth) / 2), 0))\n        newImage.paste(img, (wleft, 4))\n\n    tv = list(newImage.getdata())\n\n    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n    return tva","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50a266f9-656c-49c6-94b1-20d8687ebecc","_cell_guid":"89920c6d-afdb-4e8a-8ff4-7f654e381d32","trusted":true},"cell_type":"code","source":"# Use the above function to import a 5x5 image of digits and print prediction results.\n\ndef prediction_printer(image_name, grid_size):\n    splits = grid_size**2\n    data = np.array(image_slicer.slice(file + image_name + '.png', splits))\n    i_df = pd.DataFrame(data=data)\n    i_df[0] = i_df[0].astype(str).str.strip('.png>').str.split(' - ').str[1]\n    image_list = i_df[0].tolist()\n    df = pd.DataFrame([])\n    for img in image_list:\n        df = df.append([imageprepare(file + img + \".png\")])\n    df = (df.iloc[:, 0:].values).astype('float32')\n    df = df.reshape(df.shape[0], 28, 28, 1)\n    predictions = model.predict_classes(df)\n    new_df = pd.DataFrame(np.array_split(predictions.tolist(), grid_size))\n    print(new_df.to_string(index=False, header=None))\n\nfile = r'/kaggle/input/images/'\nprediction_printer(image_name='5x5split', grid_size=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}