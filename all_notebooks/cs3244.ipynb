{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier as NNetC\nfrom sklearn.neural_network import MLPRegressor as NNetR\nfrom sklearn.linear_model import LogisticRegression as LogReg\nfrom sklearn.ensemble import RandomForestClassifier as RanFor\nfrom sklearn.ensemble import ExtraTreesClassifier as ExtTre\nfrom sklearn.ensemble import VotingClassifier as Voting\nfrom sklearn.tree import DecisionTreeClassifier as DecTree\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n# from statsmodels.stats.outliers_influence import variance_inflation_factor\n# from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n# from scipy import stats\n\n# Feature selection + K-fold cross validation?","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_data = pd.read_csv(\"/kaggle/input/credit-card-approval-prediction/application_record.csv\")\ndf_result = pd.read_csv(\"/kaggle/input/credit-card-approval-prediction/credit_record.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete rows with no loans in that month\nindexNames = df_result[df_result['STATUS'] == 'X' ].index # Get row indices with STATUS == 'X'\ndf_result.drop(indexNames, inplace = True) # Delete these row indices from dataFrame\n\n# Change 'STATUS' column to numeric\ndf_result.loc[df_result['STATUS'] == 'C', 'STATUS'] = '-1'\ndf_result['STATUS'] = pd.to_numeric(df_result['STATUS'])\ndf_result['STATUS'] = df_result['STATUS'] + 1\n\n# Group by ID with the 'MONTH' and 'STATUS' being averaged\ndf_grouped = df_result.groupby(['ID']).mean() # Group by ID\ndf_grouped.index.name = 'ID'\ndf_grouped.reset_index(inplace = True)\n\n# Merge df_data and df_result by ID\ndf = pd.merge(df_data, df_grouped, on = 'ID')\n\n# Drop useless ID column\ndf.drop(['ID'], axis = 1, inplace = True)\n\n# Convert categorical columns to dummy columns\ndf = pd.get_dummies(df)\n\n# Shift 'STATUS' column to the back\nstatus = df['STATUS']\ndf.drop(labels = ['STATUS'], axis = 1,inplace = True)\ndf.insert(55, 'STATUS', status)\n\n# Write cleaned data to csv file\ndf.to_csv(path_or_buf = \"/kaggle/working/combined_record.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getScaledTrainTestSets(df):\n    ncol = df.shape[1]\n    \n    # Get train-test splits\n    x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, 0:(ncol - 2)], df['STATUS'], test_size = 0.20, random_state = 0)\n\n    # Transform to mean 0 and unit variance\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_test = scaler.transform(x_test) # scale with the same parameters\n\n    return x_train, x_test, y_train, y_test\n  \n\ndef doPCA(x_train, x_test):\n    pca = PCA(n_components = 0.99)\n    pca.fit(x_train)\n\n    originalCol = x_train.shape[1]\n\n    x_train = pca.transform(x_train)\n    x_test = pca.transform(x_test)\n\n    newCol = x_train.shape[1]\n\n    print(\"PCA Transformation: Number of predictors reduced from \" + str(originalCol) + \" to \" + str(newCol) + \"\\n\\n\")\n    \n    return x_train, x_test\n\ndef runModelC(df, model):\n    df_this = df\n    df_this['STATUS'] = round(df_this['STATUS'])\n        \n    x_train, x_test, y_train, y_test = getScaledTrainTestSets(df_this)\n    x_train, x_test = doPCA(x_train, x_test)\n    \n    # Feature selection: actually not needed here, since we did PCA\n    FS = VarianceThreshold(threshold = 0.10)\n    x_train = FS.fit_transform(x_train, y_train)\n    x_test = FS.transform(x_test)\n    \n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    \n    accuracy = model.score(x_test, y_test)\n    f1 = f1_score(y_test, y_pred, average = 'weighted')\n    \n    print(\"Accuracy = \" + str(accuracy))\n    print(\"F1 Score = \" + str(f1))\n    \n    return accuracy, f1\n\ndef runModelR(df, model):\n    df_this = df\n        \n    x_train, x_test, y_train, y_test = getScaledTrainTestSets(df_this)\n    x_train, x_test = doPCA(x_train, x_test)\n\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    \n    y_test = np.around(y_test)\n    y_pred = np.around(y_pred)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average = 'weighted')\n    \n    print(\"Accuracy = \" + str(accuracy))\n    print(\"F1 Score = \" + str(f1))\n    \n    return accuracy, f1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLPRegressor: since the 'STATUS' is actually mathematically related (we took the average)\nNNR = NNetR()\nscore = runModelR(df, NNR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MLPClassifier\nNNC = NNetC(learning_rate_init = 0.01, max_iter = 1000)\nscore = runModelC(df, NNC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LogisticRegressionClassifier\nLR = LogReg(solver= 'lbfgs', max_iter = 1000, multi_class = 'multinomial', random_state = 0)\nscore = runModelC(df, LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestClassifier\nRF = RanFor(n_estimators = 10, bootstrap = False, random_state = 0)\nscore = runModelC(df, RF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DecisionTreeClassifier\nDT = DecTree()\nscore = runModelC(df, DT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ExtraTreesClassifier\nET = ExtTre()\nscore = runModelC(df, ET)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VotingClassifier\nVT = Voting(estimators=[('et', ET), ('rf', RF), ('nnc', NNC)], voting = 'soft') # Similar results to 'hard'\nscore = runModelC(df, VT)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}