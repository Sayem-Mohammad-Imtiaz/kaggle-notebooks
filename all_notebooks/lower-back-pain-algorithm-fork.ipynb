{"nbformat":4,"cells":[{"execution_count":null,"cell_type":"markdown","outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"9604b895dec981fb867b5690a02ca4d05a2e7dfc","_cell_guid":"8284d05e-ae19-4b52-84af-3fd845332bcd"},"source":"Consider this solved\n---\nand congrats a pharmacist\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"trusted":false,"_execution_state":"idle","_uuid":"c9216d6c22612eca89a726cc57a7863c58f37198","_cell_guid":"404d4f99-505b-c6ba-0700-d42db0d9ef36"},"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# read data into dataset variable\ntrain = pd.read_csv(\"../input/Dataset_spine.csv\")\n\n\n# Drop the unnamed column in place (not a copy of the original)#\ntrain.drop('Unnamed: 13', axis=1, inplace=True)\ntrain.columns = ['Pelvic Incidence','Pelvic Tilt','Lumbar Lordosis Angle','Sacral Slope','Pelvic Radius', 'Spondylolisthesis Degree', 'Pelvic Slope', 'Direct Tilt', 'Thoracic Slope', 'Cervical Tilt','Sacrum Angle', 'Scoliosis Slope','Outcome']\n# Concatenate the original df with the dummy variables\n#data = pd.concat([data, pd.get_dummies(data['Class_att'])], axis=1)\n\n# Drop unnecessary label column in place. \n#data.drop(['Class_att','Normal'], axis=1, inplace=True)\nprint(train)\ntrain.describe().T"},{"execution_count":null,"cell_type":"markdown","outputs":[],"metadata":{"_uuid":"b80ae877f82a9e578ac4852a1674a858d5c2edf4","_cell_guid":"779a9779-dd74-18ae-2ca8-e1d135bcfda8"},"source":"<h1>Exploratory Data Analysis </h1>"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"e83a43c5b863e463c4ed1aa6f01714d246a64694","_cell_guid":"5169942c-245a-4bfb-b458-736b2ed4cf6e"},"source":"# Categorical features\ncat_cols = []\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        cat_cols.append(c)\nprint('Categorical columns:', cat_cols)\n\n# Dublicate features\nd = {}; done = []\ncols = train.columns.values\nfor c in cols: d[c]=[]\nfor i in range(len(cols)):\n    if i not in done:\n        for j in range(i+1, len(cols)):\n            if all(train[cols[i]] == train[cols[j]]):\n                done.append(j)\n                d[cols[i]].append(cols[j])\ndub_cols = []\nfor k in d.keys():\n    if len(d[k]) > 0: \n        # print k, d[k]\n        dub_cols += d[k]        \nprint('Dublicates:', dub_cols)\n\n# Constant columns\nconst_cols = []\nfor c in cols:\n    if len(train[c].unique()) == 1:\n        const_cols.append(c)\nprint('Constant cols:', const_cols)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"e20d210fb9e5ad288348662fa397268110c97b13","_cell_guid":"97a35da9-f138-467f-b849-9607b78af2ce"},"source":"def add_new_col(x):\n    if x not in new_col.keys(): \n        # set n/2 x if is contained in test, but not in train \n        # (n is the number of unique labels in train)\n        # or an alternative could be -100 (something out of range [0; n-1]\n        return int(len(new_col.keys())/2)\n    return new_col[x] # rank of the label\n\ndef clust(x):\n    kl=0\n    if x<0.75:\n        kl=1\n    if x>0.75 and x<4:\n        kl=2\n    if x>4:\n        kl=4\n    return kl\n\nnew_col= train[['Pelvic Tilt','Outcome']].groupby('Outcome').describe().fillna(method='bfill')\nnew_col.columns=['count','mean','std','min','p25','p50','p75','max']\nnew_col['eff']=new_col['std']/new_col['mean']\nnew_col['eff2']=new_col['eff']*new_col['std']\nnew_col['clust']=new_col['eff2'].map(clust)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"9cb35ef7bb976fcf97b163d9bc892ab8bb332358","_cell_guid":"029f3ec9-7e6b-468b-aa72-4070a6c7efc3"},"source":"print(new_col)\nprint(train)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"410cfa0f1523f7179f5e261a3ade3d110d12ffc0","_cell_guid":"52c00933-943d-44eb-b439-0cd60bdadbb2"},"source":"train=pd.merge(train,new_col, how='inner', left_on='Outcome', right_index=True)\nsns.pairplot(train[['Pelvic Tilt','std','eff2','Outcome']],hue='Outcome')\nplt.show()"},{"execution_count":null,"cell_type":"markdown","outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"fc9dbf4967d44fccef6c0c016b14da6e7b5a2927","_cell_guid":"433af771-abd4-4ae4-a66f-f5730467f2b1"},"source":"What cluster is separating the data ?\n-----"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"de1ee6de54f38375e31f9b3830bd2b67762db97b","_cell_guid":"441d1c1e-4c12-4858-b4d9-15db24f78127"},"source":"from sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.cluster import KMeans\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n# INPUT df  (dataframe en welke kolommen je gebruikt om te klusteren)\n# define 'clust' groep\n# define drop colomns\n\n#-------------------------------------\nlabels= train['clust']\nX = train.drop('Outcome',axis=1)\nn_comp = 5  #define number of clusters\n#-------------------------------------\n\nprint('-------Principal Component Analysis---------')\n# PCA\npca = PCA(n_components=n_comp, random_state=4)\nresults = pca.fit_transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\nsns.set(style=\"ticks\")\nsns.pairplot(results,hue='clust')\nplt.show()\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(12, 12))\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(results[0], results[1], results[2], c=labels, cmap=plt.cm.Paired)\nax.set_title(\"First three Singular Value\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nprint('-------Singular Value Decomposition---------')\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\nresults = tsvd.fit_transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\nsns.set(style=\"ticks\")\nsns.pairplot(results,hue='clust')\nplt.show()\n\nprint('-------Fast I  Component Analysis---------')\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nresults = ica.fit_transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\nsns.set(style=\"ticks\")\nsns.pairplot(results,hue='clust')\nplt.show()\n\nprint('-------Gaussian Random Projection---------')\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\nresults = grp.fit_transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\nsns.set(style=\"ticks\")\nsns.pairplot(results,hue='clust')\nplt.show()\n\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(12, 12))\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(results[0], results[1], results[2], c=labels, cmap=plt.cm.Paired)\nax.set_title(\"First three Gaussian\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nprint('-------Sparse Random Projection---------')\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nresults = srp.fit_transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\nsns.set(style=\"ticks\")\nsns.pairplot(results,hue='clust')\nplt.show()\n\n\nplt.show()\n\n\nprint('-------KMeans Classification---------')\n#Kmeans\nkmeans = KMeans(n_clusters=n_comp, random_state=0).fit(X)\nresults=kmeans.transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\nsns.set(style=\"ticks\")\nsns.pairplot(results,hue='clust')\nplt.show()\n\n\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(12, 12))\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(results[0], results[1], results[2], c=labels, cmap=plt.cm.Paired)\nax.set_title(\"First three Kmeanss\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\n"},{"execution_count":null,"cell_type":"markdown","outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"87ba381ee8984f4d2178afb141bdf1d3e513899b","_cell_guid":"5cad00ad-5d11-4645-aa47-bcb64cfc556c"},"source":"There are three cluster methods separating the data... so consider it solved\n---"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"3b3f0a1553caec1bf920cb1e41fc32adfbcb068a","_cell_guid":"41afa75b-1514-4fe0-8a98-26bd52c659a0"},"source":"#adding the cluster that best separates the variability\n# input X=df[kolom] from previous\n# input df dataframe\nprint('-------PCA---------')\n# PCA\n# PCA\npca = PCA(n_components=n_comp, random_state=4)\nresults = pca.fit_transform(X)\nresults=pd.DataFrame(results)\nresults['clust']=labels\n\n\n\n#print(results)\nfrom mpl_toolkits.mplot3d import Axes3D\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(12, 12))\nax = Axes3D(fig, elev=-20, azim=85)\nax.scatter(results[0], results[1], results[2], c=labels, cmap=plt.cm.Paired)\nax.set_title(\"First three Sparse Random Projections\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\n\n#Append decomposition components to datasets  # to do in next part\nfor i in range(1, n_comp + 1):\n    train['pca_' + str(i)] = results[i - 1]\n"},{"execution_count":null,"cell_type":"markdown","outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"08cb8c1ef6aa108e11104dff3d8a3e47d0783c02","_cell_guid":"3a71e73f-d464-404e-ba19-2284a595850f"},"source":"Lets make it overly difficult ;-)\n----\nlets use a Tpot , but just for fun of using it"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"collapsed":false,"trusted":false,"_execution_state":"idle","_uuid":"5d70e00facc82bc5fdcb31043b8c5d0a303058f6","_cell_guid":"0df701fe-bd8e-4336-a91f-24d23dfa8429"},"source":"from tpot import TPOTClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop('Outcome',axis=1).astype(np.float64),\n    train.clust.astype(np.float64), train_size=0.75, test_size=0.25)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_iris_pipeline.py')"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"trusted":false,"_execution_state":"idle","_uuid":"7ba25fbcc87afdb0803893f28cceba90b476a8c5","_cell_guid":"522d3325-68d5-91cf-eed7-745e8a0d4681"},"source":"ypred = pl.predict(X_test)\nypred = ypred.reshape(-1,1)\n\npl.score(X_test, y_test)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"trusted":false,"_execution_state":"idle","_uuid":"e91816a2a4c54c13f9b081ffaa47deb83ee65c1c","_cell_guid":"9c07b6ad-40d2-0f84-1f1e-89dac46a7cea"},"source":""}],"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"}},"_is_fork":false,"_change_revision":0,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":0}