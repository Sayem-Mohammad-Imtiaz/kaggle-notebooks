{"cells":[{"metadata":{"_uuid":"d5b4cded07f6e3795f506e5bc7b046f7e4d02b30"},"cell_type":"markdown","source":"Hello there! In this kernel, I'll be building a simple but effective recommender system using matrix factorization. Let's get to it. \n\nThe chapter names will describe what happens in each chapter."},{"metadata":{"_uuid":"2a2ea6c0e2be90b8e3c44e8c324b847c3c1f5960"},"cell_type":"markdown","source":"Contents:\n    1. A breif introduction to:\n        - Anime\n        - Recommender Systems\n    2. Workflow\n    3. Matrix Factorization: An introduction\n    4. Building the recommender system\n    5. Making recommendations"},{"metadata":{"_uuid":"3884b7c7a8d915ec9f6d15850856c31fc04d4b93"},"cell_type":"markdown","source":"MyAnimeList is a website where anime fans gather and share their views on anime/manga they've watched. The site currently has information on about 17K anime (it did when I last checked). For those of you who don't know what anime are, they are Japanese animated serials / movies which are generally created from Japanese manga. Manga are Japanese comics. But, some anime are different. They are created from what are called light novels. Light novels are pretty awesome. You should check 'em out. Sword Art Online or Violet Evergarden would be a great place to start. \nThen there are other anime like Pokemon that are created from games.\n\nAnime are pretty popular. If you were born before 2005 and watched Cartoon Network, chances are you've watched anime (remember Pokemon, Dragon Ball-Z, BeyBlade, etc. ? They're all anime). "},{"metadata":{"_uuid":"4b78a6d47b645f2a70a9d4cd174ff341c8851ff4"},"cell_type":"markdown","source":"A recommender system or a recommendation system (sometimes replacing \"system\" with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. That's from [wikipedia](https://en.wikipedia.org/wiki/Recommender_system)."},{"metadata":{"_uuid":"3cec828d09eddbc368e910ed28f24b1049ec68a5"},"cell_type":"markdown","source":"There are many algorithms to build recommender systems. In this kernel, I'll be building a recommender system that makes use of a particular set of algorithms called [Matrix Factorization](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)"},{"metadata":{"_uuid":"92b3fee0123eec885700269f8ff422b9aa7354ee"},"cell_type":"markdown","source":"I'll be first writing a simple algorithm and in later versions use the Surprise Python library to factorize the matrix and make recommendations.\n\nLet's begin!"},{"metadata":{"_uuid":"b435373cf6a1692dce42a458d9dd4ba84ec22134"},"cell_type":"markdown","source":"# Workflow\nI'll be using a simple approach in this kernel. First, I'll load the data and wrangle the data to get rid of missing values and stuff. Next, we'll generate the rating matrix using which the recommendations will be made. Third, is the matrix factorization step and fourth will be making the actuall recommendations."},{"metadata":{"_uuid":"f266208337d02f91233c152957b71ecb9ea38061"},"cell_type":"markdown","source":"# Matrix Factorization: A Brief Introduction\nIn 2006, Netflix launched the Netflix prize competition. The goal of the competition was to improve the accuracy of their recommendation algorithm by 10%. The team which had registered this improvement after a year would receive a million dollars. It was during this competition that the matrix factorization approach was popularized. Out of all the algorithms that were used, the set of matrix factorization techniques stood out for their power and accuracy."},{"metadata":{"_uuid":"013d8aea337a9ca1352a36a63dc8b5de4e9d3558"},"cell_type":"markdown","source":"If you don't know what matrix factorization is, then I suggest that you read about it. [Nicolas Hug](http://nicolas-hug.com/blog/matrix_facto_1)'s series of blog posts on the topic does an amazing job of acquainting the reader with the concept of matrix factorization. It gives you a good intuition about the process while keeping the math as easy as possible. I'll only give you a really short introduction here and dive into the actual process in the kernel. "},{"metadata":{"_uuid":"8ab71e6979ccca926593d4f6f95c771687c37c31"},"cell_type":"markdown","source":"Matrix factorization is a singular vector decomposition technique (or is it the other way round? I always get confused), which reduces the dimension of your feature space. What SVD does is, it enables you to express a really big matrix as the product of two (technically three, but for the sake of simplicity, let's keep it to two), smaller matrices. Here's a neat mathematical formula. I'm including it for no other reason than coolness. It just looks cool.\n$$R = P Q^{T}$$\n\nwhere:\n    $R$ is the original matrix of order $nxn$; $P$ is an orthogonal matrix of order $nxk$; $Q$ is an orthogonal matrix of  order $nxk$."},{"metadata":{"_uuid":"e481e48d12cc67fa99e883737630e254494918b9"},"cell_type":"markdown","source":"For more information on singular vector decomposition, check a linear algebra textbook. For more information on recommender systems and matrix factorization, check this [paper](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf)."},{"metadata":{"_uuid":"06319ceaa2652df6322bf4efcdb34cd8086c7923"},"cell_type":"markdown","source":"I'm limiting the amount of information I include here about matrix factorization because I don't want this kernel to be about MF. It's about making anime recommendations and I'll stick to that. "},{"metadata":{"_uuid":"a59e4cc0406033d2b6b08de14ac526d52b8e1b9a"},"cell_type":"markdown","source":"# Building the Recommender System\nI'll start with the implementation detail for the recommender system here. First we'll start by loading the data and doing a little exploratory analysis. Maybe plot a few things. Next, we'll prepare the rating matrix and write our [Stochastic Gradient Descent](https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochastic-gradient-descent) algorithm, which we'll use to train our model. Lastly, I'll use the model to make recommendations given an anime. "},{"metadata":{"trusted":true,"_uuid":"c22bd6093595aa1a8bc69a4513ce1e4c5f1d3f27"},"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f0b5943539c1e291f522de74e6628db83979b59"},"cell_type":"code","source":"# load in the data\n# the anime dataset\nanime = pd.read_csv(\"../input/anime-recommendations-database/anime.csv\")\n\n# the users rating dataset\nuser_ratings = pd.read_csv(\"../input/anime-recommendations-database/rating.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab78937ba5b1caab300352b5d78e3ea496bbceb1"},"cell_type":"code","source":"print(anime.shape); anime.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ecfbe95914b6c8be1658e79c7552bda350ecf67"},"cell_type":"markdown","source":"The dataset on anime has about 12.3K anime and 7 columns. One of the columns is the name of the anime. We'll be using this later. "},{"metadata":{"trusted":true,"_uuid":"23e694bb55dd5eddb8b9cfaaeda027c5ef94b5e2"},"cell_type":"code","source":"print(user_ratings.shape); user_ratings.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ff0819990afe430fc0c8dd79a81a22de7acd41d"},"cell_type":"markdown","source":"The rating database has about 8 million rows and three columns: The user id, the anime id and the rating given by the user.\n\nThe rating column contains a -1 for those anime that a particular user watched but didn't rate. We'll start off our data prep by replacing these values with NaN's."},{"metadata":{"trusted":true,"_uuid":"cef596d41aede62ffd494929c00088ad345c3796"},"cell_type":"code","source":"# replaing the -1's in user_rating.rating with np.nan\nuser_ratings.loc[user_ratings.rating == -1, \"rating\"] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b2c812be051cbf89265de399db235b0d759a8e6"},"cell_type":"code","source":"# number of nulls in user_rating\nuser_ratings.isnull().mean() # about 19% of the values in user_rating.rating are missing.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56b471de894eda8b0e0a649b32627359c1b205b0"},"cell_type":"markdown","source":"Next up, we'll look at the number of missing values in anime to get a good idea about what's happening over there. "},{"metadata":{"trusted":true,"_uuid":"58afcf8beafb19a90427e060be67cb497dbf44f5"},"cell_type":"code","source":"anime.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8a26b8ac7cf02e43429befc72164c8970bbf7fb"},"cell_type":"markdown","source":"There are not a lot of missing values. But, these are not really of our concern in this kernel. I'll need only the anime ids and the anime names. So, I'll merge the two datasets and drop off all the columns that are not required."},{"metadata":{"trusted":true,"_uuid":"5b9599a690e1e06ed330c4552905d3a63fabe671"},"cell_type":"code","source":"# merging anime and user_ratings\nuser_ratings = pd.merge(user_ratings, anime, on = \"anime_id\")\n\n# dropping the unnecessary columns\nuser_ratings.drop([\"genre\", \"type\", \"episodes\", \"rating_y\", \"members\"], axis = 1, inplace = True)\n\n# renaming rating_x to rating\nuser_ratings.rename(columns = {\"rating_x\": \"rating\"}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b8db3220b1cd5a8bb804f4bd7b38351373417ad"},"cell_type":"markdown","source":"Now that we have the required data loaded, I can start with building the actual recommender system. The first step is to get the matrix of ratings from the data. We can use the pivot_table method of pd.DataFrame to get that. "},{"metadata":{"_uuid":"360d61cfa13f3cb4ed0c98d16fed4e670320caac"},"cell_type":"markdown","source":"But, before that, I'll filter out about 60000 users to make the computation process easy. SGD is an iterative process that works well when there's more data. But, this also means that it's going to take a long time to run. So for computation reasons, I'm going to limit the dataset to about 1000 users. (More on this at the end)"},{"metadata":{"trusted":true,"_uuid":"ceca4b8a31cb1c75e17c7cb9327a276b657560cc"},"cell_type":"code","source":"# filtering out the first 5000 users\nuser_ratings = user_ratings[user_ratings.user_id <= 1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecbac076f6c3660cf1d830edbdd9f7a8f6b21e8f"},"cell_type":"code","source":"user_ratings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef766dfe34ef6b93318f2a5bdd881ede306e688d"},"cell_type":"code","source":"# getting the rating matrix\nrating_matrix = user_ratings.pivot_table(values = \"rating\", index = \"user_id\", columns = \"anime_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28059302a0bce5d1b4bdde1a083d28f1159c39e8"},"cell_type":"code","source":"rating_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73b2e3a64286561644a06a8721fdcc293a1c4f6b"},"cell_type":"markdown","source":"The rating matrix we have here has a lot of missing values and only a few non missing values. This is what we call a sparese matrix. Our job is to predict those missing values. "},{"metadata":{"trusted":true,"_uuid":"d904455a39dcf646109e89bc9cdbad6d481cf5b9"},"cell_type":"markdown","source":"We're going to finish up with our data preparation by filling out those missing values with 0's."},{"metadata":{"trusted":true,"_uuid":"ff7aa8d7086e8487cb8fa3ddd32abe4159257896"},"cell_type":"code","source":"rating_matrix.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca4b46277dedd72f47548b86a59c6130d7ea8761"},"cell_type":"markdown","source":"Now that we have everything we need to make predictions, the final missing ingredient is the algorithm that will give us our recommendations. For the implementation detail, I suggest you to go through Andrew Ng's lecture on [Stochastic Gradient Descent]() and this amazing [post]() on Analytics Vidhya which gives you the code for implementing SGD. I've borrowed the code here. "},{"metadata":{"trusted":true,"_uuid":"6b09e452fc8a6088757e83cf180a30707b5ae344"},"cell_type":"code","source":"# setting to raise exceptions\nnp.seterr(all = \"raise\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"527f29264d4e09eb79bf2da5003fa8235d4787b9"},"cell_type":"code","source":"class MF():\n    \n    def __init__(self, rating_matrix, learning_rate = 0.01, reg_coef = 0.02, n_factors = 10,\n                 n_epochs = 5):\n        self.R = rating_matrix\n        self.alpha = learning_rate\n        self.reg_coef = reg_coef\n        self.k = n_factors\n        self.n_epochs = n_epochs\n        self.n_users, self.n_items = self.R.shape\n        \n    def getTrainset(self):\n        trainset = [(u, i, self.R[u, i]) for u in range(self.n_users) for i in range(self.n_items) if self.R[u, i] > 0]\n        self.trainset =  trainset\n        \n    def fit(self):\n        self.getTrainset()\n        \n        self.p = np.random.normal(0, 0.1, size = (self.n_users, self.k))\n        self.q = np.random.normal(0, 0.1, size = (self.n_items, self.k))\n        \n        training_errors = []\n        \n        for epoch in range(self.n_epochs):\n            np.random.shuffle(self.trainset)\n            \n            self.sgd()\n            \n            mse = self.mse()\n            \n            training_errors.append((epoch + 1, mse))\n            \n        self.training_errors = training_errors\n        \n    def mse(self):\n        xs, ys = self.R.nonzero()\n            \n        predicted = self.getPredictedMatrix()\n            \n        err = 0\n            \n        for x, y in zip(xs, ys):\n            err += ((self.R[x, y] - predicted[x, y]) ** 2)\n                \n        return np.sqrt(err)\n        \n    def sgd(self):\n        for u, i, r_ui in self.trainset:\n                \n            prediction = np.dot(self.p[u, :], self.q[i, :].T)\n            err = r_ui - prediction\n                \n                # updates\n            self.p[u, :] += self.alpha * (err * self.q[i, :] - self.reg_coef * self.p[u, :])\n            self.q[i, :] += self.alpha * (err * self.p[u, :] - self.reg_coef * self.q[i, :])\n                \n    def getPrediction(self, u, i):\n        return np.dot(self.p[u, :], self.q[i, :].T)\n        \n    def getPredictedMatrix(self):\n        return np.dot(self.p, self.q.T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"120d28fe7e0018ed0a207be6841de64d7d12f66d"},"cell_type":"markdown","source":"For computational efficiency, it's better to pass the rating matrix as a numpy array. So, I'll convert it into a numpy array and then run the training process."},{"metadata":{"trusted":true,"_uuid":"c3340e17433124f9bed269ab57cb9c3976d749cc"},"cell_type":"code","source":"# converting the rating matrix into a numpy array\nR = np.array(rating_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3441943ccfa31586cd6c57a7c6226ed350d8cbb2"},"cell_type":"code","source":"mf = MF(rating_matrix = R, n_epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8156ba7f63210e4d8db59a5c4597df89905c2031"},"cell_type":"code","source":"mf.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48b61a14177be003849873f1924a128cc62f62df"},"cell_type":"code","source":"mf.training_errors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"613d7c601d11f236ae3146d757b2652470e24d29"},"cell_type":"markdown","source":"The Stochastic Gradiet Descent algorithm seems to work. The reason I'm considering only 1000 users is because I faced some problems with np.random.normal with 5000 users. The function started generating values that caused floating point errors. So, I settled on testing the algorithm with 1000 users first. "},{"metadata":{"_uuid":"b2887180615771ec47c41f11d17fb0408c0bdd62"},"cell_type":"markdown","source":"The model needs to be tuned a lot more. For the next version, here's what I have planned:\n    1. Write another class that makes recommendations but uses the Surprise library and it's advanced algorithms.\n    2. Create a function to recommend anime."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}