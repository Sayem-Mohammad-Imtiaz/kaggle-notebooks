{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Python ML final _ Fuel efficiency prediction with \"means of categories\"\n \n By 國立陽明交通大學 醫學三 陳冠元 10701054\n\n \n Inspired by https://www.freecodecamp.org/news/how-to-build-your-first-neural-network-to-predict-house-prices-with-keras-f8db83049159/","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T20:37:06.851702Z","iopub.execute_input":"2021-06-21T20:37:06.852101Z","iopub.status.idle":"2021-06-21T20:37:06.86344Z","shell.execute_reply.started":"2021-06-21T20:37:06.852065Z","shell.execute_reply":"2021-06-21T20:37:06.862223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Data\nImport the csv file to have a look about the datasets.","metadata":{}},{"cell_type":"code","source":"#Just import the dataset so we can take a look...\nimport pandas as pd\ndata = pd.read_csv('../input/co2-emission-by-vehicles/CO2 Emissions_Canada.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:06.865542Z","iopub.execute_input":"2021-06-21T20:37:06.865982Z","iopub.status.idle":"2021-06-21T20:37:06.918071Z","shell.execute_reply.started":"2021-06-21T20:37:06.865939Z","shell.execute_reply":"2021-06-21T20:37:06.917026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\nMake and model of the vehicle is removed since it could overwhelm the neural network.\n\nOther fuel consumption data is removed due to the algebraic relationship between fuel consuption and CO2 emission.","metadata":{}},{"cell_type":"code","source":"# Cannot quantify Make and Model of an automobile.\ndata.pop('Make')\ndata.pop('Model')\n\n#These four values have tight algebraic relationship with the predicted values.\ndata.pop('Fuel Consumption City (L/100 km)')\ndata.pop('Fuel Consumption Hwy (L/100 km)')\ndata.pop('Fuel Consumption Comb (mpg)')\ndata.pop('CO2 Emissions(g/km)')\n\n#I thought taking these information would increase model accuracy but it did not.\n#data.pop('Cylinders')\n#data.pop('Transmission')\n#data.pop('Fuel Type')\n\n#Save the modified csv file.\ndata.to_csv('preprocessed_fuel_efficiency.csv', index=False)\npd.read_csv('preprocessed_fuel_efficiency.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:06.920437Z","iopub.execute_input":"2021-06-21T20:37:06.920842Z","iopub.status.idle":"2021-06-21T20:37:06.987653Z","shell.execute_reply.started":"2021-06-21T20:37:06.920801Z","shell.execute_reply":"2021-06-21T20:37:06.986823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Microsoft Excel function for averaging a column according to the condition of another column:\n\nhttps://www.excelforum.com/excel-general/541631-calculate-average-in-a-column-based-on-criteria-in-another-column.html","metadata":{}},{"cell_type":"markdown","source":"Dealing with vehicle class, transmission, fuel type information with means of groups(?)\n\nThe means stand for the CO2 emission ot the group in grams per kilometer.","metadata":{}},{"cell_type":"code","source":"#Read in the csv file that contain the information about the means of different groups.\nmeans_of_groups = pd.read_csv('../input/means-of-groups/means of groups.csv')\n\n#Convert it into an array\nMOG = means_of_groups.values\nmeans_of_groups\n\n#What I really did is use Microsoft Excel to calculate the average CO2 emission according to the attributions below:","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:06.989834Z","iopub.execute_input":"2021-06-21T20:37:06.990159Z","iopub.status.idle":"2021-06-21T20:37:07.008283Z","shell.execute_reply.started":"2021-06-21T20:37:06.990129Z","shell.execute_reply":"2021-06-21T20:37:07.007288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to Environmental Protection Agency of US, \nAverage fuel efficiency of typical passenger vehicle is 10.69L/100km\n\nhttps://www.epa.gov/greenvehicles/greenhouse-gas-emissions-typical-passenger-vehicle","metadata":{}},{"cell_type":"markdown","source":"My old method of replacing attribution information with integers, but it someimes cause problems.","metadata":{}},{"cell_type":"code","source":"#new_data = pd.read_csv(\"preprocessed_fuel_efficiency.csv\")\n\n#x == 0\n#for x in range(len(MOG)):\n   # new_data['Vehicle Class'] = new_data['Vehicle Class'].replace({str(MOG[x][0]): MOG[x][1]})\n   # new_data['Transmission'] = new_data['Transmission'].replace({str(MOG[x][0]): MOG[x][1]})\n    #new_data['Fuel Type'] = new_data['Fuel Type'].replace({str(MOG[x][0]): MOG[x][1]})\n   # x=x+1    \n\n#new_data.to_csv(\"preprocessed_fuel_efficiency.csv\", index=False)\n\n#new_data","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:07.009485Z","iopub.execute_input":"2021-06-21T20:37:07.009749Z","iopub.status.idle":"2021-06-21T20:37:07.013312Z","shell.execute_reply.started":"2021-06-21T20:37:07.009724Z","shell.execute_reply":"2021-06-21T20:37:07.012417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nnew_data = pd.read_csv(\"preprocessed_fuel_efficiency.csv\")\ndataset = new_data.values\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:07.014576Z","iopub.execute_input":"2021-06-21T20:37:07.014905Z","iopub.status.idle":"2021-06-21T20:37:07.038022Z","shell.execute_reply.started":"2021-06-21T20:37:07.014877Z","shell.execute_reply":"2021-06-21T20:37:07.037136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://stackoverflow.com/questions/18716564/python-cant-assign-to-literal\n\nhttps://www.guru99.com/variables-in-python.html","metadata":{}},{"cell_type":"code","source":"# This loop replaces the attribution information with integers.\n# Somehow if I go a=0, b=0 python will give me error messages...\na,b,c = 0,0,0\n\nfor a in range(len(dataset)):\n    for b in range(len(MOG)):\n        for c in range(len(dataset[a])):\n            if dataset[a][c] == MOG[b][0]:\n                dataset[a][c] = MOG[b][1]\n            c=c+1\n        b=b+1\n    a=a+1    \n\n#This loop simplifies the fuel efficiency to 0 = below average; 1 = above average. \nx,y=0,0\nfor x in range(len(dataset)):\n    if dataset[x][5] < 10.69:\n        dataset[x][5] = 1\n        y=y+1\n    else:\n        dataset[x][5] = 0\n    x=x+1\nz=y/x\n\n#Just to examine the percentage of car models that perform above average in fuel efficiency.\n#If the results turn out to be near 0.5, it means the dataset coresponds to EPA data.\nprint(z)\n#See how the attribution information and fuel efficiency data is modified.\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:07.041553Z","iopub.execute_input":"2021-06-21T20:37:07.04185Z","iopub.status.idle":"2021-06-21T20:37:10.167746Z","shell.execute_reply.started":"2021-06-21T20:37:07.041823Z","shell.execute_reply":"2021-06-21T20:37:10.166634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The reason why have to save the array back to csv file is that the type of the array is \"object\"\n#This will cause the data for validation not being accepted by the deep learning model.\n\n#Somehow the problem could be resolved by saving the array to csv and converting it back to array.\n#Heading will be lost therefore it had to be added back manually.\ndataset_csv = np.asarray(dataset)\nnp.savetxt(\"preprocessed_fuel_efficiency.csv\", dataset_csv, delimiter=\",\", \n           header='Mean emission of Vehicle Class, Engine Size(L), Cylinders, Mean emission of Transmission, Mean emission of Fuel Type, Above average fuel efficiency')\nnew_data = pd.read_csv(\"preprocessed_fuel_efficiency.csv\")\n\n#This is the dataset AFTER PREPROCESSING.\nnew_data","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.169311Z","iopub.execute_input":"2021-06-21T20:37:10.1697Z","iopub.status.idle":"2021-06-21T20:37:10.255688Z","shell.execute_reply.started":"2021-06-21T20:37:10.169658Z","shell.execute_reply":"2021-06-21T20:37:10.254717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the fuel efficiency results into a separated list.","metadata":{}},{"cell_type":"code","source":"#Converting the preprocessed csv file to array...again.\ndataset = new_data.values\n\n#Splitting the results\nX = dataset[:,0:5]\nY = dataset[:,5]\n\nprint(X)\nprint(Y)\n\n#this way there is a dot after the every element.","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.256986Z","iopub.execute_input":"2021-06-21T20:37:10.257266Z","iopub.status.idle":"2021-06-21T20:37:10.264334Z","shell.execute_reply.started":"2021-06-21T20:37:10.257236Z","shell.execute_reply":"2021-06-21T20:37:10.263491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make the values of every column between 0 and 1 so the deep learning model can process it.\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)\nX_scale","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.26555Z","iopub.execute_input":"2021-06-21T20:37:10.265857Z","iopub.status.idle":"2021-06-21T20:37:10.278197Z","shell.execute_reply.started":"2021-06-21T20:37:10.265829Z","shell.execute_reply":"2021-06-21T20:37:10.277037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since N=7385, 10% for testing and validation is quite enough.\nfrom sklearn.model_selection import train_test_split\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.1)\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\nprint(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.279603Z","iopub.execute_input":"2021-06-21T20:37:10.279959Z","iopub.status.idle":"2021-06-21T20:37:10.294409Z","shell.execute_reply.started":"2021-06-21T20:37:10.279893Z","shell.execute_reply":"2021-06-21T20:37:10.292796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Model\n\nhttps://keras.io/api/layers/activations/","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.297774Z","iopub.execute_input":"2021-06-21T20:37:10.298137Z","iopub.status.idle":"2021-06-21T20:37:10.302683Z","shell.execute_reply.started":"2021-06-21T20:37:10.298084Z","shell.execute_reply":"2021-06-21T20:37:10.301543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#5 inputs with 4 hidden neuron layers and 1 output.\n#5 inputs with 4 hidden neuron layers and 1 output.\nmodel = Sequential([\n    Dense(5, activation='relu', input_shape=(5,)),\n    Dense(64, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(10, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.304396Z","iopub.execute_input":"2021-06-21T20:37:10.304795Z","iopub.status.idle":"2021-06-21T20:37:10.35063Z","shell.execute_reply.started":"2021-06-21T20:37:10.304744Z","shell.execute_reply":"2021-06-21T20:37:10.349635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adam optimizer works somehow works better than sgd.\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['AUC','accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.352187Z","iopub.execute_input":"2021-06-21T20:37:10.352583Z","iopub.status.idle":"2021-06-21T20:37:10.365969Z","shell.execute_reply.started":"2021-06-21T20:37:10.352544Z","shell.execute_reply":"2021-06-21T20:37:10.365029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I find 32 per batch is optimal and at least 100 epochs.\nhist = model.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:10.367473Z","iopub.execute_input":"2021-06-21T20:37:10.367823Z","iopub.status.idle":"2021-06-21T20:37:47.202032Z","shell.execute_reply.started":"2021-06-21T20:37:10.367789Z","shell.execute_reply":"2021-06-21T20:37:47.2012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"model.evaluate(X_test, Y_test)[1]\nprint(X_test)\nprint(Y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:47.203293Z","iopub.execute_input":"2021-06-21T20:37:47.203556Z","iopub.status.idle":"2021-06-21T20:37:47.273469Z","shell.execute_reply.started":"2021-06-21T20:37:47.20353Z","shell.execute_reply":"2021-06-21T20:37:47.272691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"#just plot the stuff above out.\nimport matplotlib.pyplot as plt\nplt.plot(hist.history['val_loss'])\nplt.plot(hist.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Val', 'Train'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:47.274836Z","iopub.execute_input":"2021-06-21T20:37:47.27528Z","iopub.status.idle":"2021-06-21T20:37:47.472658Z","shell.execute_reply.started":"2021-06-21T20:37:47.275224Z","shell.execute_reply":"2021-06-21T20:37:47.47185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist.history['val_auc'])\nplt.plot(hist.history['auc'])\nplt.plot(hist.history['val_accuracy'])\nplt.plot(hist.history['accuracy'])\nplt.title('Model AUC and Accuracy')\nplt.ylabel('AUC and Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Val AUC', 'Train AUC','Val Accuracy', 'Train Accuracy'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T20:37:47.473824Z","iopub.execute_input":"2021-06-21T20:37:47.474153Z","iopub.status.idle":"2021-06-21T20:37:47.670284Z","shell.execute_reply.started":"2021-06-21T20:37:47.474118Z","shell.execute_reply":"2021-06-21T20:37:47.669263Z"},"trusted":true},"execution_count":null,"outputs":[]}]}