{"cells":[{"metadata":{"id":"F98f1GoqcGHP"},"cell_type":"markdown","source":"DATA EXPLORATION"},{"metadata":{"id":"AV6RbONGcGHQ","executionInfo":{"status":"ok","timestamp":1606723350777,"user_tz":-330,"elapsed":2163,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\nimport os\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"id":"V8-XMOpAcGHT","executionInfo":{"status":"ok","timestamp":1606723372864,"user_tz":-330,"elapsed":2248,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1', usecols=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{"id":"4HqrOhzjcGHV","executionInfo":{"status":"ok","timestamp":1606723378302,"user_tz":-330,"elapsed":2945,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"bee95328-7d3e-42da-89ae-6509052f3aac","trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"0gXVbtrbcGHX","executionInfo":{"status":"ok","timestamp":1606723379172,"user_tz":-330,"elapsed":2638,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"78ce9495-1646-410f-db10-7aa08523569f","trusted":true},"cell_type":"code","source":"# Rename the columns with more explicit names\ndata.rename(columns={'v1' : 'label', 'v2' : 'message'}, inplace=True)\n\n# Five first rows of the dataset\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"VijO1iiOcGHa","executionInfo":{"status":"ok","timestamp":1606723381354,"user_tz":-330,"elapsed":1375,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"6fb82c7f-9c7f-44e7-a20c-b5fc1c4c24cd","trusted":true},"cell_type":"code","source":"# Give a brief description of the dataset\ndata.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"paB_jfjXcGHc","executionInfo":{"status":"ok","timestamp":1606723383385,"user_tz":-330,"elapsed":1565,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"1136d69f-1761-403c-a52a-6ed999d25d56","trusted":true},"cell_type":"code","source":"data[\"label\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], \n                                      figsize = (6, 6), autopct = '%1.1f%%', \n                                      shadow = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"SWyqsCoYcGHe","executionInfo":{"status":"ok","timestamp":1606723385445,"user_tz":-330,"elapsed":1955,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"900bb7c3-beaf-4c6a-8db0-124e2e27d109","trusted":true},"cell_type":"code","source":"data['message'].apply(lambda x: len(x.split(' '))).mean() # average words per sms","execution_count":null,"outputs":[]},{"metadata":{"id":"P5DbG1RwcGHg","executionInfo":{"status":"ok","timestamp":1606723385912,"user_tz":-330,"elapsed":1266,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"7bd76578-f28f-4da2-e73d-c443d6363a61","trusted":true},"cell_type":"code","source":"data['length'] = data['message'].map(lambda text: len(text))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"FSXt6bJxcGHj","executionInfo":{"status":"ok","timestamp":1606723387585,"user_tz":-330,"elapsed":1448,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"16d57f9d-5f94-4b89-ab36-725f8dc14d84","trusted":true},"cell_type":"code","source":"data.length.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"vg5PLZ1ycGHl"},"cell_type":"markdown","source":"Is there any difference in message length between spam and ham?"},{"metadata":{"id":"ZPkLfjCLcGHl","executionInfo":{"status":"ok","timestamp":1606723390536,"user_tz":-330,"elapsed":2843,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"2e752745-0679-466a-dcf6-c19239825c83","trusted":true},"cell_type":"code","source":"data.hist(column='length', by='label', bins=50)","execution_count":null,"outputs":[]},{"metadata":{"id":"xGjbl8CycGHn","executionInfo":{"status":"ok","timestamp":1606723391410,"user_tz":-330,"elapsed":1940,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"9013e525-9915-4704-a842-7ebec432b6f6","trusted":true},"cell_type":"code","source":"import seaborn as sns\nham =data[data['label'] == 'ham']['message'].str.len()\nsns.distplot(ham, label='Ham')\nspam = data[data['label'] == 'spam']['message'].str.len()\nsns.distplot(spam, label='Spam')\nplt.title('Distribution by Length')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"VzoTdGnIcGHp","executionInfo":{"status":"ok","timestamp":1606723392804,"user_tz":-330,"elapsed":1570,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"2de6fd21-3038-4282-c94d-e773923a7abb","trusted":true},"cell_type":"code","source":"# Visualization of the most frequent words of the dataset\ncount1 = Counter(\" \".join(data[\"message\"]).split()).most_common(30)\ndf1 = pd.DataFrame.from_dict(count1)\ndf1 = df1.rename(columns={0: \"words\", 1 : \"count\"})\nfig = plt.figure()\nax = fig.add_subplot(111)\ndf1.plot.bar(ax=ax, legend = False)\nxticks = np.arange(len(df1[\"words\"]))\nax.set_xticks(xticks)\nax.set_xticklabels(df1[\"words\"])\nax.set_ylabel('Number of occurences')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"OJPY_juDcGHr","executionInfo":{"status":"ok","timestamp":1606723394128,"user_tz":-330,"elapsed":1215,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"trusted":true},"cell_type":"code","source":"# Wordcloud paramters\nwc_height, wc_width = (512, 1024)\nwc_bckp_color = 'white'\nwc_max_words = 400\nwc_max_font_size = 60\nrandom_state = 42\nwc_figsize = (12, 10)","execution_count":null,"outputs":[]},{"metadata":{"id":"Qvw3ilQYcGHt","executionInfo":{"status":"ok","timestamp":1606723397443,"user_tz":-330,"elapsed":3355,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"ead0cec8-714a-465e-889f-6720fa3eca90","trusted":true},"cell_type":"code","source":"# Extracting spam messages from DataFrame\nspam_df = data.loc[data['label'] == 'spam']\n\n# Creating wordcloud for spam\nspam_wc = WordCloud(\n    height=wc_height, width=wc_width, background_color=wc_bckp_color,\n    max_words=wc_max_words, max_font_size=wc_max_font_size,\n    random_state=random_state\n).generate(str(spam_df['message']))\n\n# Display the wordcloud\nfig = plt.figure(figsize=wc_figsize)\nplt.imshow(spam_wc)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"5KiQ3kLLcGHv","executionInfo":{"status":"ok","timestamp":1606723399480,"user_tz":-330,"elapsed":3094,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"1d1aa30e-fcf9-409b-9101-54d1f42abb86","trusted":true},"cell_type":"code","source":"# Extracting ham messages from DataFrame\nham_df = data.loc[data['label'] == 'ham']\n\n# Creating wordcloud for ham\nham_wc = WordCloud(\n    height=wc_height, width=wc_width, background_color=wc_bckp_color,\n    max_words=wc_max_words, max_font_size=wc_max_font_size,\n    random_state=random_state\n).generate(str(ham_df['message']))\n\n# Display the wordcloud\nfig = plt.figure(figsize=wc_figsize)\nplt.imshow(ham_wc)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YFRyh3g6cGHy","executionInfo":{"status":"ok","timestamp":1606723399887,"user_tz":-330,"elapsed":1275,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"trusted":true},"cell_type":"code","source":"#read data in CSV format according to your PC's address\ndata = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",encoding='latin')","execution_count":null,"outputs":[]},{"metadata":{"id":"S9J17b8XcGH0"},"cell_type":"markdown","source":"Data Preprocessing:\n\nNow that we have a basic understanding of what our dataset looks like, lets convert our labels to binary variables, 0 to represent 'ham'(i.e. not spam) and 1 to represent 'spam' for ease of computation.\n\nYou might be wondering why do we need to do this step? The answer to this lies in how scikit-learn handles inputs. Scikit-learn only deals with numerical values and hence if we were to leave our label values as strings, scikit-learn would do the conversion internally(more specifically, the string labels will be cast to unknown float values).\n\nOur model would still be able to make predictions if we left our labels as strings but we could have issues later when calculating performance metrics, for example when calculating our precision and recall scores. Hence, to avoid unexpected 'gotchas' later, it is good practice to have our categorical values be fed into our model as integers.\n\n\n    Convert the values in the 'label' column to numerical values using map method as follows: {'ham':0, 'spam':1} This maps the 'ham' value to 0 and the\n        ' spam' value to 1.\n    Also, to get an idea of the size of the dataset we are dealing with, print out number of rows and columns using 'shape'.\n"},{"metadata":{"id":"qdIL8k1_cGH1","executionInfo":{"status":"ok","timestamp":1606723405965,"user_tz":-330,"elapsed":4754,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"a97af0df-ae2c-4f8e-a8dc-95acc772a42b","trusted":true},"cell_type":"code","source":"data.rename(columns={'v1':'label','v2':'Text'},inplace=True)\ndata['numClass'] = data['label'].map({'ham':0, 'spam':1})\ndata['Count']=0\nfor i in np.arange(0,len(data.Text)):\n    data.loc[i,'Count'] = len(data.loc[i,'Text'])\n\n# Unique values in target set\nprint(\"Unique values in the label set: \", data.label.unique())\nprint('Number of texts in the total set: {}'.format(data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"id":"1DwBdNHlcGH3","executionInfo":{"status":"ok","timestamp":1606723405968,"user_tz":-330,"elapsed":3012,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"51f1af0c-3c41-45c1-84fa-0ea3068f82f9","trusted":true},"cell_type":"code","source":"ham  = data[data.numClass == 0]\nham_count  = pd.DataFrame(pd.value_counts(ham['Count'],sort=True).sort_index())\nprint(\"Number of ham messages in data set:\", ham['label'].count())\nprint(\"Ham Count value\", ham_count['Count'].count())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RxakVQv6cGH6","executionInfo":{"status":"ok","timestamp":1606723405969,"user_tz":-330,"elapsed":1592,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"47dde9ad-ca56-47bf-84da-07d53cf26d41","trusted":true},"cell_type":"code","source":"spam = data[data.numClass == 1]\nspam_count = pd.DataFrame(pd.value_counts(spam['Count'],sort=True).sort_index())\nprint(\"Number of spam messages in data set:\", spam['label'].count())\nprint(\"Spam Count value:\", spam_count['Count'].count())","execution_count":null,"outputs":[]},{"metadata":{"id":"yaDQKYdLcGH8"},"cell_type":"markdown","source":"fig, ax = plt.subplots(figsize=(17,5))\nspam_count['Count'].value_counts().sort_index().plot(ax=ax, kind='bar',facecolor='red');\nham_count['Count'].value_counts().sort_index().plot(ax=ax, kind='bar',facecolor='green');"},{"metadata":{"id":"a8eK0KjUcGH8"},"cell_type":"markdown","source":"Preparing the Data"},{"metadata":{"id":"Oq7lcsaNcGH9","executionInfo":{"status":"ok","timestamp":1606723409663,"user_tz":-330,"elapsed":3228,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import f1_score,accuracy_score\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\nfrom scipy.stats import sem\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"id":"a8H-xhR2cGH_","executionInfo":{"status":"ok","timestamp":1606723410215,"user_tz":-330,"elapsed":1872,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"abe7890e-c2b0-447a-920a-b7e3950d2a47","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"naryVAXmcGIA"},"cell_type":"markdown","source":"Bag of words:\n    \nWhat we have here in our data set is a large collection of text data (5,572 rows of data). Most ML algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy.\n\nHere we'd like to introduce the Bag of Words(BoW) concept which is a term used to specify the problems that have a 'bag of words' or a collection of text data that needs to be worked with. The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter.\n\nUsing a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.\n\nNote:\n\n    The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he'\n        differently. It does this using the lowercase parameter which is by default set to True.\n    It also ignores all punctuation so that words followed by a punctuation mark (for example: 'hello!') are not treated differently than the same words not\n        prefixed or suffixed by a punctuation mark (for example: 'hello'). It does this using the token_pattern parameter which has a default regular \n        expression which selects tokens of 2 or more alphanumeric characters.\n    The third parameter to take note of is the stop_words parameter. Stop words refer to the most commonly used words in a language. They include words like \n        'am', 'an', 'and', 'the' etc. By setting this parameter value to english, CountVectorizer will automatically ignore all words(from our input text)\n        that are found in the built in list of english stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we\n        are trying to find certain key words that are indicative of spam.\n"},{"metadata":{"id":"0BTr4ks-cGIB","executionInfo":{"status":"ok","timestamp":1606723412639,"user_tz":-330,"elapsed":1922,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"b9af356e-1e81-46e0-8cb6-7293e1e3c2af","trusted":true},"cell_type":"code","source":"#Removing stopwords of English\nstopset = set(stopwords.words(\"english\"))\nstopset","execution_count":null,"outputs":[]},{"metadata":{"id":"E1Yd3nZicGIF","executionInfo":{"status":"ok","timestamp":1606723414831,"user_tz":-330,"elapsed":1045,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"trusted":true},"cell_type":"code","source":"#Initialising Count Vectorizer\nvectorizer = CountVectorizer(stop_words=stopset,binary=True)\nvectorizer = CountVectorizer()\n    \nX = vectorizer.fit_transform(data.Text)\n# Extract target column 'label'\ny = data.numClass","execution_count":null,"outputs":[]},{"metadata":{"id":"fIqCR-tecGII","executionInfo":{"status":"ok","timestamp":1606723416820,"user_tz":-330,"elapsed":1464,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"b5c27170-dc17-4c38-da6b-1ae7ab5add06","trusted":true},"cell_type":"code","source":"vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"id":"WTjgmZjIcGIK","executionInfo":{"status":"ok","timestamp":1606723420013,"user_tz":-330,"elapsed":1147,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"cd578bd8-48ed-4b78-8fa4-55978f927c9f","trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"id":"mq06TEWOcGIM","executionInfo":{"status":"ok","timestamp":1606723513719,"user_tz":-330,"elapsed":2083,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"5d58625e-7753-4f16-869e-77b66d00df2e","trusted":true},"cell_type":"code","source":"doc_array = vectorizer.transform(data.Text).toarray()\ndoc_array","execution_count":null,"outputs":[]},{"metadata":{"id":"KgtSnGaecGIO"},"cell_type":"markdown","source":"frequency_matrix = pd.DataFrame(doc_array, columns = vectorizer.get_feature_names())\nfrequency_matrix.head()"},{"metadata":{"id":"DHN5Xd5NcGIO","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"Xobj6MBicGIQ"},"cell_type":"markdown","source":"Training and testing sets:\n\nNow that we have understood how to deal with the Bag of Words problem we can get back to our dataset and proceed with our analysis. Our first step in this regard would be to split our dataset into a training and testing set so we can test our model later.\n\n    Instructions: Split the dataset into a training and testing set by using the train_test_split method in sklearn. Split the data using the following variables:\n\n        X_train is our training data for the 'text' column.\n        y_train is our training data for the 'label' column\n        X_test is our testing data for the 'text' column.\n        y_test is our testing data for the 'label' column Print out the number of rows we have in each our training and testing data.\n\n"},{"metadata":{"id":"rMyr85MzcGIQ","executionInfo":{"elapsed":13604,"status":"ok","timestamp":1606486806509,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"},"user_tz":-330},"outputId":"2a9a70c1-c851-4c25-8d0c-6203f953eeed","trusted":true},"cell_type":"code","source":"#Performing test train Split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, train_size=0.70, random_state=None)\n\n# Show the results of the split\nprint(\"\\n\")\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"x97mpxk_cGIS"},"cell_type":"markdown","source":"Training and Evaluating Models"},{"metadata":{"id":"XFHZ3qsrcGIS","trusted":true},"cell_type":"code","source":"objects = ('Multi-NB','SVM','KNN', 'RF')","execution_count":null,"outputs":[]},{"metadata":{"id":"fwQ6fa9VcGIU","trusted":true},"cell_type":"code","source":"def train_classifier(clf, X_train, y_train):    \n    clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"As7RSQt3cGIW","trusted":true},"cell_type":"code","source":"# function to predict features \ndef predict_labels(clf, features):\n    return(clf.predict(features))","execution_count":null,"outputs":[]},{"metadata":{"id":"xrQdq9tlcGIY","trusted":true},"cell_type":"code","source":"pip install catboost","execution_count":null,"outputs":[]},{"metadata":{"id":"KVdQxYRWcGIZ","trusted":true},"cell_type":"code","source":"# Initialize the four models\nA = MultinomialNB(alpha=1.0,fit_prior=True)\nB = SVC(kernel='linear',probability=True)\nC = KNeighborsClassifier(n_neighbors=1)\nD = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=None)","execution_count":null,"outputs":[]},{"metadata":{"id":"17PrzcLicGIb","trusted":true},"cell_type":"code","source":"clf = [A,B,C,D]\nacc_score = [0,0,0,0]\naccuracy = [0,0,0,0]","execution_count":null,"outputs":[]},{"metadata":{"id":"r0YSS2IicGIc","executionInfo":{"elapsed":23278,"status":"ok","timestamp":1606486816210,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"},"user_tz":-330},"outputId":"04e8667c-7a4f-431d-b517-ccd2e8b59b59","trusted":true},"cell_type":"code","source":"for a in range(0,4):\n    print(objects[a])\n    train_classifier(clf[a], X_train, y_train)\n    y_pred = predict_labels(clf[a],X_test)\n    pred_val = f1_score(y_test, y_pred)\n    acc_score[a]=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n    print(\"Accuracy in %:\")\n    print(acc_score[a]*100)\n    print(\"F1 Score\")\n    print(pred_val)\n    print(\"\\n\")\n    cm = confusion_matrix(y_test, y_pred)\n    class_label = [\"ham\", \"spam\"]\n    df_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n    sns.heatmap(df_cm, annot=True, fmt='d')\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.show()\n    print(\"Train accuracy: \", clf[a].fit(X_train, y_train).score(X_train, y_train))\n    print(\"mse test data: \",mean_squared_error(y_test, y_pred))\n    y_train_pred = predict_labels(clf[a],X_train)\n    print(\"mse train data: \",mean_squared_error(y_train, y_train_pred))\n    print(\"\\n\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7AdkDPPZ3z4C","executionInfo":{"elapsed":42823,"status":"ok","timestamp":1606486835762,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"},"user_tz":-330},"outputId":"4e2a8433-4155-4801-f7d2-8845e43b4977","trusted":true},"cell_type":"code","source":"eclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[1])], voting='soft', weights=[1,2])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 0,1 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[2])], voting='soft', weights=[2,1])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 0,2 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[3])], voting='soft', weights=[2,1])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 0,3 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[1]), ('m2', clf[2])], voting='soft', weights=[2,1])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 1,2 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[1]), ('m2', clf[3])], voting='soft', weights=[2,1])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 1,3 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[2]), ('m2', clf[3])], voting='soft', weights=[1,2])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 2,3 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[1]), ('m3', clf[2])], voting='soft', weights=[2,3,1])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 0,1,2%:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")   \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[2]), ('m3', clf[3])], voting='soft', weights=[3,1,2])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 0,2,3 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[1]), ('m2', clf[2]), ('m3', clf[3])], voting='soft', weights=[3,1,2])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in 1,2,3 %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n\neclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[1]), ('m3', clf[2]), ('m4', clf[3])], voting='soft', weights=[2,2,1,1])      \neclf = eclf.fit(X_train, y_train)                 \ny_pred = eclf.predict(X_test)\npred_val = f1_score(y_test, y_pred)\nacc=accuracy_score(y_test, y_pred)\nprint(\"Accuracy in all %:\")\nprint(acc*100)\nprint(\"F1 Score\")\nprint(pred_val)\nprint(\"\\n\")  \ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\nprint(\"\\n\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"AmhoVhvVcGIe","trusted":true},"cell_type":"code","source":"y_pos = np.arange(len(objects))\ny_val = [ x for x in acc_score]","execution_count":null,"outputs":[]},{"metadata":{"id":"HhmV1MUHcGIf","executionInfo":{"elapsed":42816,"status":"ok","timestamp":1606486835766,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"},"user_tz":-330},"outputId":"a1017b55-fa17-4bd0-8eb9-655a1705ccfb","trusted":true},"cell_type":"code","source":"plt.bar(y_pos,y_val, align='center', alpha=0.7)\nplt.xticks(y_pos, objects)\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy of Models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"f5Kl9Y1Ui9WW","executionInfo":{"elapsed":79061,"status":"ok","timestamp":1606486872016,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"},"user_tz":-330},"outputId":"bdc27e80-a157-46f0-fa02-fc59caa1434b","trusted":true},"cell_type":"code","source":"for a in range(0,4):\n    print(objects[a])\n    kfolds = KFold(n_splits=10)\n    score = cross_val_score(clf[a], X, y, cv=kfolds)\n    print(\"Accuracy:\",(score.mean()*100), \"standard_error:\", sem(score))\n    accuracy[a]=score.mean()*100\n\ny_pos = np.arange(len(objects))\ny_val = [ x for x in accuracy]\nplt.bar(y_pos,y_val, align='center', alpha=0.7)\nplt.xticks(y_pos, objects)\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy of Models')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"cmsOnKrH3_QC","executionInfo":{"status":"ok","timestamp":1606487328916,"user_tz":-330,"elapsed":34382,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"eded92a9-6061-4c4d-a768-b005956eb527","trusted":true},"cell_type":"code","source":"kfolds = KFold(n_splits=10)\neclf = VotingClassifier(estimators=[ ('m1', clf[0]), ('m2', clf[1])], voting='soft', weights=[1,2])      \nscore = cross_val_score(eclf, X, y, cv=kfolds)\nprint(\"Accuracy:\",(score.mean()*100), \"standard_error:\", sem(score))","execution_count":null,"outputs":[]},{"metadata":{"id":"vOevNkje-dE6"},"cell_type":"markdown","source":"CHANCE LEVEL"},{"metadata":{"id":"sP1SFT1I58ue","executionInfo":{"elapsed":83470,"status":"ok","timestamp":1606486876436,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"},"user_tz":-330},"outputId":"dcea6bec-e6bb-43a3-c9a9-991c6e5be914","trusted":true},"cell_type":"code","source":"print(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"szTLpaVd666S","executionInfo":{"status":"ok","timestamp":1606701121670,"user_tz":-330,"elapsed":1279,"user":{"displayName":"GANDRA JOSHITHA","photoUrl":"","userId":"14729451453534601632"}},"outputId":"b226021d-495c-4382-9f89-76b198ba3dec","trusted":true},"cell_type":"code","source":"nums = np.random.choice([0, 1], size=5572)\naccuracy=accuracy_score(y, nums)\nprint(\"accuracy: \",accuracy*100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}