{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfull_data = pd.read_csv('../input/fake-news/fake_train.csv')\nfull_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,5))\nfull_data.label.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('Fake (1) and Real (0) in the Imbalanced Dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\nno = full_data[full_data.label == 0]\nyes = full_data[full_data.label == 1]\nyes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123)\noversampled = pd.concat([no, yes_oversampled])\n\nfig = plt.figure(figsize = (8,5))\noversampled.label.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('Fake (1) and Real (0) after Oversampling (Balanced Dataset)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Data Pattern in Training Data\nimport seaborn as sns\nsns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = oversampled.isnull().sum().sort_values(ascending=False)\npercent = (oversampled.isnull().sum()/oversampled.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oversampled.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute categorical var with Mode\noversampled['title'] = oversampled['title'].fillna(oversampled['title'].mode()[0])\noversampled['author'] = oversampled['author'].fillna(oversampled['author'].mode()[0])\noversampled['text'] = oversampled['text'].fillna(oversampled['text'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in oversampled.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    oversampled[col] = lencoders[col].fit_transform(oversampled[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nMiceImputed = oversampled.copy(deep=True) \nmice_imputer = IterativeImputer()\nMiceImputed.iloc[:, :] = mice_imputer.fit_transform(oversampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MiceImputed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MiceImputed.isna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detecting outliers with IQR\nQ1 = MiceImputed.quantile(0.25)\nQ3 = MiceImputed.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing outliers from dataset\nMiceImputed = MiceImputed[~((MiceImputed < (Q1 - 1.5 * IQR)) |(MiceImputed > (Q3 + 1.5 * IQR))).any(axis=1)]\nMiceImputed.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heatmap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = MiceImputed.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(250, 25, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .9})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot( data=MiceImputed, vars=( 'id', 'title','author','text'), hue='label' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardizing data\nfrom sklearn import preprocessing\nr_scaler = preprocessing.MinMaxScaler()\nr_scaler.fit(MiceImputed)\nmodified_data = pd.DataFrame(r_scaler.transform(MiceImputed), index=MiceImputed.index, columns=MiceImputed.columns)\nmodified_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance using Filter Method (Chi-Square)\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = modified_data.loc[:,modified_data.columns!='label']\ny = modified_data[['label']]\nselector = SelectKBest(chi2, k=4)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\nX = MiceImputed.drop('label', axis=1)\ny = MiceImputed['label']\nselector = SelectFromModel(rf(n_estimators=100, random_state=0))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(rf(n_estimators=100, random_state=0).fit(X,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf(n_estimators=100, random_state=0).fit(X,y),random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = MiceImputed[['id', 'title', 'author', 'text']]\ntarget = MiceImputed['label']\n\n# Split into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=12345)\n\n# Normalize Features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_cur(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score, plot_confusion_matrix, roc_curve, classification_report\ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model.fit(X_train,y_train, verbose=0)\n    else:\n        model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred) \n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"ROC Area under Curve = {}\".format(roc_auc))\n    print(\"Cohen's Kappa = {}\".format(coh_kap))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    \n    probs = model.predict_proba(X_test)  \n    probs = probs[:, 1]  \n    fper, tper, thresholds = roc_curve(y_test, probs) \n    plot_roc_cur(fper, tper)\n    \n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, accuracy, roc_auc, coh_kap, time_taken","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'l1', 'solver':'liblinear'}\n\nmodel_lr = LogisticRegression(**params_lr)\nmodel_lr, accuracy_lr, roc_auc_lr, coh_kap_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nparams_nn = {'hidden_layer_sizes': (30,30,30),\n             'activation': 'logistic',\n             'solver': 'lbfgs',\n             'max_iter': 500}\n\nmodel_nn = MLPClassifier(**params_nn)\nmodel_nn, accuracy_nn, roc_auc_nn, coh_kap_nn, tt_nn = run_model(model_nn, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nparams_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 100,\n             'random_state': 12345}\n\nmodel_rf = RandomForestClassifier(**params_rf)\nmodel_rf, accuracy_rf, roc_auc_rf, coh_kap_rf, tt_rf = run_model(model_rf, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.80\nwidth = 0.90\n\nclf1 = LogisticRegression(random_state=12345)\nclf2 = MLPClassifier(random_state=12345, verbose = 0)\nclf3 = RandomForestClassifier(random_state=12345)\nX_list = MiceImputed[[\"title\", \"author\", \"text\"]] #took only really important features\nX = np.asarray(X_list, dtype=np.float32)\ny_list = MiceImputed[\"label\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(3,3)\nfig = plt.figure(figsize=(18, 14))\n\nlabels = ['Logistic Regression',\n          'Neural Network',\n          'Random Forest',]\n\nfor clf, lab, grd in zip([clf1, clf2, clf3],\n                         labels,\n                         itertools.product([0, 1, 2],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value}, \n                                filler_feature_ranges={2: width}, \n                                legend=2)\n    plt.title(lab)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_scores = [accuracy_lr,  accuracy_nn, accuracy_rf]\nroc_auc_scores = [roc_auc_lr, roc_auc_nn, roc_auc_rf]\ncoh_kap_scores = [coh_kap_lr,coh_kap_nn, coh_kap_rf]\ntt = [tt_lr, tt_nn, tt_rf]\n\nmodel_data = {'Model': ['Logistic Regression','Neural Network','Random Forest'],\n              'Accuracy': accuracy_scores,\n              'ROC_AUC': roc_auc_scores,\n              'Cohen_Kappa': coh_kap_scores,\n              'Time taken': tt}\ndata = pd.DataFrame(model_data)\n\nfig, ax1 = plt.subplots(figsize=(12,10))\nax1.set_title('Model Comparison: Accuracy and Time taken for execution', fontsize=13)\ncolor = 'tab:green'\nax1.set_xlabel('Model', fontsize=13)\nax1.set_ylabel('Time taken', fontsize=13, color=color)\nax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='summer')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Accuracy', fontsize=13, color=color)\nax2 = sns.lineplot(x='Model', y='Accuracy', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax3 = plt.subplots(figsize=(12,10))\nax3.set_title('Model Comparison: Area under ROC and Cohens Kappa', fontsize=13)\ncolor = 'tab:blue'\nax3.set_xlabel('Model', fontsize=13)\nax3.set_ylabel('ROC_AUC', fontsize=13, color=color)\nax4 = sns.barplot(x='Model', y='ROC_AUC', data = data, palette='winter')\nax3.tick_params(axis='y')\nax4 = ax3.twinx()\ncolor = 'tab:red'\nax4.set_ylabel('Cohen_Kappa', fontsize=13, color=color)\nax4 = sns.lineplot(x='Model', y='Cohen_Kappa', data = data, sort=False, color=color)\nax4.tick_params(axis='y', color=color)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nWe can observe that **Random Forest** have performed better compared to other models. However, if speed is an important thing to consider, we can stick to Random Forest instead of Logistic Regration or Neural Network model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}