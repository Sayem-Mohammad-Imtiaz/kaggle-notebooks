{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intermediate Natural Language Processing (NLP)\n## Real World Applications of Word Embeddings"},{"metadata":{},"cell_type":"markdown","source":"### Notebook Organization:\n- Loading and comparing pretrained word embeddings\n- Applying word embeddings to a problem\n- Training your own embeddings\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/'\nimport os\nfor path, dirs, files in os.walk(f'{path}'):\n    print(path)\n    for f in files:\n        print(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading and comparing pretrained word embeddings\nWe load a few different models to compare how they evaluate similar queries"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/gensim-word-vectors/'\nfrom gensim.models import KeyedVectors\n\nGLOVE_TWITTER = f'{path}glove-twitter-100/glove-twitter-100'\ntwitter_model = KeyedVectors.load_word2vec_format(GLOVE_TWITTER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GLOVE_WIKI = f'{path}glove-wiki-gigaword-300/glove-wiki-gigaword-300'\nwiki_model = KeyedVectors.load_word2vec_format(GLOVE_WIKI)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Corpuses have different emphases"},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"arms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(\"arms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(\"cloud\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"cloud\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(\"occupy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"occupy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(\"cluod\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"cluod\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"foriegn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(\"foriegn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This blogpost describes a strategy for [correcting spelling using word embeddings](https://blog.usejournal.com/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26)"},{"metadata":{},"cell_type":"markdown","source":"#### Analogies\nThe classical example: **man::king as woman::?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"POSITIVE_LIST = ['woman', 'king']\nNEGATIVE_LIST = ['man']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ubiqutous example of bias:\n**man::programmer as woman::?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"POSITIVE_LIST = ['woman', 'programmer']\nNEGATIVE_LIST = ['man']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**man::doctor as woman::?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"POSITIVE_LIST = ['woman', 'doctor']\nNEGATIVE_LIST = ['man']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now the reverse:\n**woman::doctor as man::?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"POSITIVE_LIST = ['man', 'doctor']\nNEGATIVE_LIST = ['woman']\ntwitter_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_model.most_similar(positive=POSITIVE_LIST, negative=NEGATIVE_LIST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dimensionality: a curse or not?\nNow we compare two GloVE models trained on the same (Twitter) data, one which is represented by vectors of 100 dimensions and another with 25 m=dimensions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"GLOVE_TWITTER_S = f'{path}glove-twitter-25/glove-twitter-25'\ntwitter_model_s = KeyedVectors.load_word2vec_format(GLOVE_TWITTER_S)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"arms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model_s.most_similar(\"arms\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(\"cloud\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model_s.most_similar(\"cloud\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model.most_similar(positive=['woman', 'king'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_model_s.most_similar(positive=['woman', 'king'], negative=['man'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load spaCy and the English language model\nimport spacy\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase = \"NLP is so fun!\"\ndoc = nlp(phrase)\nprint(f'spaCy vectors are {len(doc[3].vector)} dimensions long')\n# Get the vector for 'fun':\nprint(f'First 20 values of vector for \"{doc[3]}\"\\n', doc[3].vector[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean vector for the entire sentence\nprint(f'First 20 values of vector for phrase \"{phrase}\"\\n', doc.vector[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing 'NLP' to 'Good'"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc[0].similarity(doc[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing 'NLP' to 'bad'"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase = \"NLP is so bad!\"\ndoc2 = nlp(phrase)\ndoc2[0].similarity(doc2[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing 'good' to 'bad'"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc[3].similarity(doc2[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing 'NLP is good' to 'NLP is bad'"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc.similarity(doc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_obama = 'Obama speaks to the media in Illinois'\nsentence_president = 'The President greets the press in Chicago'\nobama = nlp(sentence_obama)\npresident = nlp(sentence_president)\nobama.similarity(president)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_obama2 = 'Obama speaks in Illinois'\nobama2 = nlp(sentence_obama2)\nobama2.similarity(president)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_nlp = 'NLP is so fun!'\nnlp_fun = nlp(sentence_nlp)\nobama2.similarity(nlp_fun)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"president.similarity(nlp_fun)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying word embeddings to a ML pipeline\nDataset: [IMDB Data set for NLP analysis](https://www.kaggle.com/rajathmc/bag-of-words-meets-bags-of-popcorn-#labeledTrainData.tsv)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport spacy\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/bag-of-words-meets-bags-of-popcorn-/'\ntrain = pd.read_csv(f'{path}labeledTrainData.tsv', header = 0, delimiter = '\\t', quoting = 3)\ntest = pd.read_csv(f'{path}testData.tsv', header = 0, delimiter = '\\t', quoting = 3)\nprint(f'train dim:{train.shape}, test dim:{test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = []\nfor index, row in train.iterrows():\n   doc = nlp(row[\"review\"])\n   features.append(doc.vector)\n    \nfeatures_test = []\nfor index, row in test.iterrows():\n   doc = nlp(row[\"review\"])\n   features_test.append(doc.vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train[\"sentiment\"]\nfeatures = train_vectors\n\nlabels_test = test[\"sentiment\"]\nfeatures_test = test_vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\nclf = LinearSVC() # Whatever model name FOR CLASSIFICATION\nclf.fit(features, labels)\npreds_test = clf.predict(features_test)\n\nprint(classification_report(labels_test,\n                           preds_test,\n                           target_names=train[\"sentiment\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training your own embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation as sp\nimport re\nfrom spacy.lang.en import English\nparser = English()\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\nfrom gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we start transforming and processing text, we want to look at what are standard features in the libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nlp.Defaults.stop_words)\nprint(len(nlp.Defaults.stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.Defaults.stop_words.add(\"my_new_stopword\")\nprint(len(nlp.Defaults.stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.Defaults.stop_words.remove(\"my_new_stopword\")\nprint(len(nlp.Defaults.stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPLIST = nlp.Defaults.stop_words\nSYMBOLS = \" \".join(sp).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\nfrom bs4 import BeautifulSoup\n\ndef lemmatizeText(document):\n    '''\n    Removes html tags\n    Replaces newlines, carriage returns and multiple spaces with a single space\n    Uncases text\n    Parses text into lemmas excluding stopwords, symbols and pronouns\n    '''\n    soup = BeautifulSoup(document)\n    text = soup.get_text(\" \")\n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\s\\s+\", \" \")\n    text = text.lower()\n    tokens = nlp(text)\n    lemmas = []\n    for tok in tokens:\n        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n    tokens = lemmas\n    tokens = [tok for tok in tokens if tok not in STOPLIST]\n    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizeText(train[\"review\"][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ncorpus = []\nstart = time.time()\nfor index, row in train.iterrows():\n   lemmatized_rev = lemmatizeText(row['review'])\n   corpus.append(lemmatized_rev)\n   if((index % 500)==0):\n        end = time.time()\n        print('{} rows processed in {} seconds'.format(index,end-start))\n        start = time.time() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count = 0\nfor doc in corpus:\n   word_count += len(doc)\nprint(f'The corpus has {len(corpus)} documents and {word_count} words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nimport time\n\nphrases = Phrases(sentences=corpus,min_count=25,threshold=50)\nbigram = Phraser(phrases)\nstart = time.time() \nfor index,sentence in enumerate(corpus):\n    corpus[index] = bigram[sentence]\n    if((index % 5000)==0):\n        end = time.time()\n        print('{} rows processed in {} seconds'.format(index,end-start))\n        start = time.time() \n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start =  time.time()\nmodel = word2vec.Word2Vec(corpus, workers = 4, size = 100, min_count = 40, window = 10, sample = 0.0001)\nend = time.time()\nprint(end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.init_sims(replace = True)\nmodel.save(fname_or_handle = \"w2v_imdb_100d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class MySentences(object):\n#     def __init__(self, dirname):\n#         self.dirname = dirname\n \n#     def __iter__(self):\n#         for fname in os.listdir(self.dirname):\n#             for line in open(os.path.join(self.dirname, fname)):\n#                 yield line.split()\n \n# #sentences = MySentences('/some/directory') # a memory-friendly iterator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport bokeh\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n\nimdb_model =  gensim.models.Word2Vec.load(fname_or_handle='w2v_imdb_100d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_closestwords_tsnescatterplot(model, word):\n    arr = np.empty((0,100), dtype='f')\n    word_labels = [word]\n    # get close words\n    close_words = model.similar_by_word(word)\n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_closestwords_tsnescatterplot(model, 'chef')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"References:\n- Training word2vec embeddings: https://rare-technologies.com/word2vec-tutorial/\n- Sentiment analysis using word2vec: https://www.kaggle.com/kyen89/2-sentiment-analysis-word2vec\n- Data streaming using generators: https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/\n- Using GloVE + Keras: https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}