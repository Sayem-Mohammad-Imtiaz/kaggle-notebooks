{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Objective"},{"metadata":{},"cell_type":"markdown","source":"To collect data from an API/non static source, store and preprocess it and make preliminary analysis."},{"metadata":{},"cell_type":"markdown","source":"### Goal"},{"metadata":{},"cell_type":"markdown","source":"Conduct social media analysis for Delhi Riots to conduct sentiment analysis and user profiling."},{"metadata":{},"cell_type":"markdown","source":"### Background"},{"metadata":{},"cell_type":"markdown","source":"Following complete [lockdown of Indian-Administered-Kashmir](https://www.aljazeera.com/indepth/inpictures/pictures-100-days-crippling-lockdown-kashmir-191110141155667.html) on August 5, 2019 through abrogation of Article 370 of Indian constitution which gave autonomy to the reagion, government of India, on December 11, 2019, passed a controversial bill called [\"Citizenship Amendment Bill\"](https://www.bbc.com/news/world-asia-india-50670393), which aimed to provide citizenship to non-muslim minorities through naturalization. These two events were were opposed [nationally](https://edition.cnn.com/2019/12/31/opinions/india-citizenship-law-crosses-line-singh/index.html) and [internationally](https://www.indiatoday.in/india/story/caa-protest-world-students-international-foreign-modi-india-1637241-2020-01-16). Fueling religious radicalization, these events led [riots](https://en.wikipedia.org/wiki/2020_Delhi_riots) in major cities in the country, more specifically Delhi, capital of India. These riots are reffered to as **Delhi Riots**."},{"metadata":{},"cell_type":"markdown","source":"[EU DisinfoLab](https://www.disinfo.eu/), a Brussels based NGO, focused on tackling disinformation campaigns targeting EU, on November 26, 2019 realeased a report titled, [\"Uncovered: 265 coordinated fake local media outlets serving Indian interests\"](https://www.disinfo.eu/publications/uncovered-265-coordinated-fake-local-media-outlets-serving-indian-interests). This report further raised questions on authenticity of online content and this extends to content on social media. Governments and lobbyists have been using [social media](https://www.nytimes.com/2020/03/29/technology/facebook-google-twitter-november-election.html) to stir public preception."},{"metadata":{},"cell_type":"markdown","source":"### Data Identification"},{"metadata":{},"cell_type":"markdown","source":"#### Platform selection"},{"metadata":{},"cell_type":"markdown","source":"Criterion for platform selection are as following:  \n- textual rich data\n- api/tool for data collection availability  \n- amount of disucussion  \n\n\nIn order to conduct this analysis, following social media analysis are considered:  \n- facebook\n- twitter\n- reddit  \n\nFor this task **twitter** is chosen and data is acquired using [twint](https://github.com/twintproject/twint) an opensource library to fetch twitter public data without any limit. Figure below shows rationale behind the decision for platform and tool selection."},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/hamzaafridi/delhi-riots/blob/master/platform_selection_mindmap.jpeg?raw=true)"},{"metadata":{},"cell_type":"markdown","source":"#### Tool selection|"},{"metadata":{"trusted":false},"cell_type":"code","source":"#suppress warnings generated by ipython for cleaner working\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inorder to collect data [twint](https://github.com/twintproject/twint) is used. Twint is an advanced twitter scrapping tool which has no limits and no authentication required. The project is 2 years old, however, there is [active participation](https://github.com/twintproject/twint/graphs/code-frequency) by contributors.  "},{"metadata":{"trusted":false},"cell_type":"code","source":"import twint #twitter scrapping tool\nimport nest_asyncio #twint has dependency on this\nimport os #to measure file size\nimport time #to process time data\nimport pandas as pd\nimport seaborn as sns #plotting tool\nimport matplotlib.pyplot as plt #plotting tool","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data collection"},{"metadata":{},"cell_type":"markdown","source":"There are two steps to data collection:  \n1. collect and store tweets\n2. extract contributing twitter handles and collect profile data for them"},{"metadata":{},"cell_type":"markdown","source":"#### Collect and store tweets \nDue to some network issue system would break after acquiring 14000 (approx.) tweets. Thus implemented a safe aquring 10k tweets at a time with delay 30sec after each request is implemented so that it doesn't appear as a ddos attack. "},{"metadata":{"trusted":false},"cell_type":"code","source":"nest_asyncio.apply() #so that multiple requests can be made at the same time\ntweet_client = twint.Config() # configure a client\n\n#search configuration\ntweet_client.Search = \"delhiriots\" #serach querry\ntweet_client.Limit = 10000 #max number of results\ntweet_client.Store_csv = True #store data to a csv file by appending it\ntweet_client.Output = \"tweets.csv\" #file name\ntweet_client.Hide_output = True #do not print output in the notebook\ntweet_client.Resume = 'last_qerry.txt'#last checkpoint to continue search incase of error","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"old_size = os.path.getsize('./tweets.csv') #track file size\ntwint.run.Search(tweet_client) #run search\nnew_size = os.path.getsize('./tweets.csv') #track file size","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"while(old_size<new_size):\n    twint.run.Search(tweet_client) #run search\n    old_size = new_size\n    new_size = os.path.getsize('./tweets.csv') #track file size\n    time.sleep(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Collect and store participating user data "},{"metadata":{},"cell_type":"markdown","source":"This can be accomplished using preliminary data extraction and processing from raw data aquired in previous section. This will be accomplished by: \n- selecting only twitter handle from tweets.csv \n- removing duplicates if any\n- acquire user data for each user"},{"metadata":{"trusted":false},"cell_type":"code","source":"# reading twitter handles from tweets.csv\nparticipant_handles_df = pd.read_csv('tweets.csv', usecols=['username'])\n\n# remove duplicates\nparticipant_handles_df.drop_duplicates(inplace=True)\nparticipant_handles_df.to_csv(\"unique_users.csv\") # store users for future use","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"number of unique users: \",participant_handles_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to aquire these many users data simultanously we will employ technique of multiprocessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"import threading #multiprocessing\nimport asyncio\n\ndef search_query(username):\n    asyncio.set_event_loop(asyncio.new_event_loop())\n    \n    #client configuration\n    user_client = twint.Config()\n    user_client.Store_csv = True\n    user_client.Hide_output = True\n    user_client.Output = \"users_parallel.csv\"\n    for user in username:\n        user_client.Username = user\n        twint.run.Lookup(user_client)\n\nmax_queries=100 #number of simultanous querries\nuser_thread=[]\n\n#generating and starting threads\nfor i in range(max_queries):\n    user_thread.append(threading.Thread(target=search_query, args=(participant_handles_df.username[580*i:580*i+579],)))\n    user_thread[i].start()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preperation and analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Data loading"},{"metadata":{},"cell_type":"markdown","source":"In this step both tweets and users data are stored in seperate data frames."},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df = pd.read_csv(\"tweets.csv\") # read tweets data and store in dataframe\nusers_df = pd.read_csv(\"users.csv\") # read users data and store in dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen from the priliminary evaluation of above data there are many columns that are irrelavent to our analysis and can be safely dropped."},{"metadata":{"trusted":false},"cell_type":"code","source":"users_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to previous case we can drop multiple features from here as they are not relavent to our goals or are empty anyways."},{"metadata":{},"cell_type":"markdown","source":"#### Feature analysis"},{"metadata":{},"cell_type":"markdown","source":"In this step we will coduct an inspection of columns/features of raw data for both data sets. We will make conclusions to which columns to drop in the preprocessing step."},{"metadata":{},"cell_type":"markdown","source":"Let's look at columns/features of tweets data."},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the feature list above following are the relavent features:\n- created_at\n- date\n- time\n- timezone\n- username\n- tweet\n- mentions\n- replies_count\n- retweet_count\n- likes_count\n- hashtag"},{"metadata":{"trusted":false},"cell_type":"code","source":"users_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the features above following are relavent features:\n- name\n- username\n- bio\n- location\n- join_date\n- join_time\n- tweets\n- following\n- followers\n- likes\n- private\n- verified"},{"metadata":{},"cell_type":"markdown","source":"#### Preprecessing"},{"metadata":{},"cell_type":"markdown","source":"In this step I will have the data go through the following steps:\n- drop irrelevant columns (features) from both datasets\n- extract tweets that are before the event and keep a seperate copy\n- convert date column to datetime format\n- drop tweets that are before the event (February 2020)\n- join the the two data sets appropriately"},{"metadata":{},"cell_type":"markdown","source":"##### droping irrelavent features"},{"metadata":{"trusted":false},"cell_type":"code","source":"#drop columns that are not required in tweets dataframe\ntweets_df.drop(columns=['id', 'conversation_id', 'user_id', 'name', 'place', 'urls', 'photos',\n       'cashtags', 'link', 'retweet', 'quote_url', 'video', 'near', 'geo',\n       'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to',\n       'retweet_date', 'translate', 'trans_src', 'trans_dest'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#verify if drop of columns successful in tweets dataframe\ntweets_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#drop columns that are not required in users dataframe\nusers_df.drop(columns=['id', 'url', 'media', 'profile_image_url', 'background_image'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#verify if drop of columns successfil in users dataframe\nusers_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### covert date column to datetime datetype"},{"metadata":{},"cell_type":"markdown","source":"This conversion is easily accomplished using to_datetime function in pandas."},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df['date'] = pd.to_datetime(tweets_df.date)\nusers_df['join_date'] = pd.to_datetime(users_df.join_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#verification of successful conversion\ntweets_df['date'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before going anyfurther lets looks at when the oldest tweet was when we got this data"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"oldest tweet date:\",min(tweets_df.date))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like we have older data, we will have to drop anything older than our time of interest"},{"metadata":{},"cell_type":"markdown","source":"##### extrating tweets older than 1st February 2020"},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_bft = tweets_df[(tweets_df.date<\"2020-02-22\")] #filter tweets older than 22nd February 2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#verify successful filtering\nmax(tweets_bft.date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### drop tweets before the event"},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df = tweets_df[(tweets_df.date>\"2020-02-21\")] #filter tweets latest than 22nd February 2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#verify successful filtering\nmin(tweets_df.date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### join users and tweets data frames"},{"metadata":{},"cell_type":"markdown","source":"However, joining the two dataframes might not be an efficient solution. This step is just being done to demonstrate understading of integrating data together."},{"metadata":{"trusted":false},"cell_type":"code","source":"tweet_user_df = tweets_df.set_index('username').join(users_df.set_index('username')) #join with username as key","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tweet_user_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# the below filter has been set as participating users were too large (58k+) even with multiprocessing it is taking more time. So data is scrapped using a seperate script\ntweet_user_df[~(tweet_user_df.join_date.isna())].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analysis"},{"metadata":{},"cell_type":"markdown","source":"\n##### from tweets data"},{"metadata":{},"cell_type":"markdown","source":"##### tweet frequency analysis"},{"metadata":{},"cell_type":"markdown","source":"As we have have already filtered tweets data, let's first analyze the frequecy of tweets overtime."},{"metadata":{"trusted":false},"cell_type":"code","source":"date_df=tweets_df.date.groupby(tweets_df[\"date\"]).count() #group by using date to generate histogram\ndate_df.plot(kind=\"bar\", title=\"histogram of tweets\", figsize=(16,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above histogram shows that the during the riots there were maximum number of tweets, thus highlighting the importance of this issue. Maximum number of tweets are dated for 25 February, 2020."},{"metadata":{},"cell_type":"markdown","source":"##### engagement analysis"},{"metadata":{},"cell_type":"markdown","source":"Another important analysis would be to visualize engagements on tweets. For this we will filter out any tweet that has zero interaction and visualize the spread of the data using boxplot."},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df[['retweets_count','likes_count','replies_count']].boxplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As clear from the above diagram most engagement is closer to zero. If we filter that out we can have better understanding of spread. Also there are clearly few outliers, with significant likes and/or retweets."},{"metadata":{},"cell_type":"markdown","source":"We will now generate box plot again by ignoring tweets greater than 0 engagments."},{"metadata":{"trusted":false},"cell_type":"code","source":"tweets_df[((tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count)>0)][['retweets_count','likes_count','replies_count']].boxplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No signigicant different is observed. Let's visualize distribution of these engagements. Note: engagement is sum of retweets, likes and replies count."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nsns.distplot(tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count, hist=False, bins=1).set_title(\"distribution of engagement\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The clear spike not close to zero suggests that this was a highly engaging topic. There was contribution to the discussion by the participants."},{"metadata":{},"cell_type":"markdown","source":"##### top tweets"},{"metadata":{},"cell_type":"markdown","source":"We will now analyze tweet of the with highest engagement feature both individual and commulative."},{"metadata":{"trusted":false},"cell_type":"code","source":"#tweet with most likes\nbest_tweet_likes = tweets_df[(tweets_df.likes_count==max(tweets_df.likes_count))]\nprint(\"Best tweet based on likes: \\\"%s\\\" by %s\"%(best_tweet_likes.tweet.values[0], best_tweet_likes.username.values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#tweet with most likes\nbest_tweet_retweets = tweets_df[(tweets_df.retweets_count==max(tweets_df.retweets_count))]\nprint(\"Best tweet based on retweets: \\\"%s\\\" by %s\"%(best_tweet_retweets.tweet.values[0], best_tweet_retweets.username.values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#tweet with most replies\nbest_tweet_replies = tweets_df[(tweets_df.replies_count==max(tweets_df.replies_count))]\nprint(\"Best tweet based on replies: \\\"%s\\\" by %s\"%(best_tweet_replies.tweet.values[0], best_tweet_replies.username.values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#tweet with most engagements\nbest_tweet_engagements = tweets_df[((tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count)==max(tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count))]\nprint(\"Best tweet based on engagement: \\\"%s\\\" by %s\"%(best_tweet_engagements.tweet.values[0], best_tweet_engagements.username.values[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that abdulhamidahmad who is Editor in Cheif of Gulf News got most likes, retweets and likes. On the other shekhargupta who is an Indian journalist got most replies.  "},{"metadata":{},"cell_type":"markdown","source":"##### from participants data"},{"metadata":{},"cell_type":"markdown","source":"Now we will analyze participant data."},{"metadata":{},"cell_type":"markdown","source":"##### verified users"},{"metadata":{},"cell_type":"markdown","source":"It is expected to be very small but the engagement of verified vs un-verified is priliminary information we can get from the participating users data."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nplt.title(\"verified vs unverified participants\")\nplt.pie(users_df.username.groupby(users_df.verified).count(), labels=[\"unverified\",\"verified\"],autopct='%1.1f%%', explode=[0.2,0],startangle=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from the above pie 2% participants are verified users and 98% are unverified users. Not alot of information is available on what percentage of users are verified. The closest numbers I could get were in this [article](https://www.brandwatch.com/blog/twitter-stats-and-statistics/). However, given strict rules of twitter on verification of twitter handles I believe it's a significant number, which suggests the seriousness of the issue."},{"metadata":{},"cell_type":"markdown","source":"##### twitter age analysis"},{"metadata":{},"cell_type":"markdown","source":"In order to track down fake army accounts twitter age is an important paramter."},{"metadata":{"trusted":false},"cell_type":"code","source":"users_df[\"twitter_age\"]=max(tweets_df['date'])-users_df['join_date'] #max is the latest tweet that is on 30th March 2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(users_df[\"twitter_age\"].dt.days, hist=False)\nplt.title(\"distribution of twitter age of participants\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting how distribution plots show two peaks. The first peak near zero might be an indicator of creation of twitter army."},{"metadata":{},"cell_type":"markdown","source":"##### followers, following, tweets and likes analysis"},{"metadata":{},"cell_type":"markdown","source":"In order to analyse this I will divide the set into two groups based on age:\n1. age < 50 days\n2. age > 50 days"},{"metadata":{"trusted":false},"cell_type":"code","source":"#statisitcal description of users with less than 50 days age\nusers_df[(users_df.twitter_age.dt.days<50)][['tweets',\"followers\",\"following\",\"likes\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For participants with are significantly new on twitter, number of average tweets and likes are suspicious. However, this needs to be further investigated to conclude fake users."},{"metadata":{"trusted":false},"cell_type":"code","source":"#statisitcal description of users with less than 50 days age\nusers_df[(users_df.twitter_age.dt.days>50)][['tweets',\"followers\",\"following\",\"likes\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a rought estimation, however, this includes celebrities with verified accounts and huge fan following. So this table will have skewed data."},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Just with priliminary analysis, it has been observed that twint is quite reliable source for data acquisition from twitter. It does however come with it's limitation. Analysis of delhiriots indicate usage of bots to influence public sentiments. However, further investigation has to be performed, to make viable conclusion."},{"metadata":{},"cell_type":"markdown","source":"### Future work"},{"metadata":{},"cell_type":"markdown","source":"With involvement of NLP and other statistical analysis tool this investigation can be taken further and more insights can be taken out from this dataset. If successful these techniques can be employed to track and uncover such global operations."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}