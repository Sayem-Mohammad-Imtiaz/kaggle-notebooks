{"cells":[{"metadata":{},"cell_type":"markdown","source":" # Linear Discriminant Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Objective:** To study classification using Linear Discriminant Analysis."},{"metadata":{},"cell_type":"markdown","source":"Logistic regression is a classification algorithm traditionally limited to only two-class classification problems.\nIf you have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique. In classification we consider K classes {1, 2, 3 , …, K} and an input vector X.\n\n**LDA makes some simplifying assumptions about your data:**\n\n* That your data is Gaussian, that each variable is is shaped like a bell curve when plotted.\n* That each attribute has the same variance, that values of each variable vary around the mean by the same amount on average.\n\n**LDA classification relies on Bayes theorem which states:**\n![![image.png](attachment:image.png)](https://miro.medium.com/max/376/1*xbJD9vVvqp6a4rYoxpSePg.png)\n\nThis formula says we can relate the probability of X belonging to each class to the probability of input taking on the value of X in each class. For simplicity we shall assume we only have 1 input variable.We can view P(X|Y) as a probability density function.\n\n![![image.png](attachment:image.png)](https://miro.medium.com/max/263/1*kmji15KeRCbDUWfRMjd7Nw.png)\n\nThe technique will work better if the input variables come from an approximately normal distribution and the less this approximation holds, the poorer the performance will be.\n\n**Plugging the normal distribution into Bayes theorem we get:**\n![![image.png](attachment:image.png)](https://miro.medium.com/max/693/1*pbJIDzqnjbevdWwpmVhOJA.png)\n\nFor simplicity assume there are only 2 classes, so i=0 or 1, and the classes share the same variance for X.\nWe want to find the class that maximizes this value.So, we will take log and simplify this equation as shown:\n\n![![![image.png](attachment:image.png)](https://miro.medium.com/max/875/1*PZBarEJMelBeZGZZu713yA.png)\n\nMany of these terms remain constant so don’t contribute to determining which class has the largest probability. We can remove them leaving us with the discriminant function.\n\n![![image.png](attachment:image.png)](https://miro.medium.com/max/566/1*oziY74CouiYMmLSNr2sZqA.png)\n\nWe can classify X belonging to the class that yields the largest discriminant function value.The formula is linear in X which is where the name LDA comes from."},{"metadata":{},"cell_type":"markdown","source":"Here, I am going to use titanic dataset to classify the survived/not-survived and check the accuracy of the model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\ndf = pd.read_csv('../input/titanicdataset-traincsv/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I am dropping the unnecessary data features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I am removing the NaN values to acquire accuracy in the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I filled the empty values using the mean value and preprocessed the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Pclass = df.fillna(df.Pclass.mean())\nnew = {'male':0, 'female':1}\ndf.Sex = df.Sex.map(new)\ndf.Age = df.Age.fillna(df.Age.mean())\ndf.SibSp = df.fillna(df.SibSp.mean())\ndf.Parch = df.fillna(df.Parch.mean())\ndf.Fare = df.fillna(df.Fare.mean())\nnew1 = {'S':1,'C':2, 'Q':3}\ndf = df.replace({'Embarked':new1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the input and output features are defined."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = df[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\ny = df['Survived']\nprint(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building LDA Model using scikitlearn:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel = LinearDiscriminantAnalysis()\nres = model.fit(X, y).transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.decision_function(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.scatter(res,y_pred)\nplt.xlabel('Input')\nplt.ylabel('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** Thus, the accuracy of the model is 78% using Linear Discriminant Analysis."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}