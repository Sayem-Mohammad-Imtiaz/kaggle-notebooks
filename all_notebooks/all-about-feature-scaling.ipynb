{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='begin'></a>\n <p style=\"background-color:silver; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">All about Feature Scaling:ðŸ“ˆðŸ“‰</p>","metadata":{}},{"cell_type":"markdown","source":"<img src= \"https://miro.medium.com/max/2400/1*_783tuRRVcTUwyFWB8VG0g.png\" width=\"100%\"  align=\"center\"  hspace=\"5%\" vspace=\"5%\"/>","metadata":{}},{"cell_type":"markdown","source":"<ul><li><p style=\"font-family: Arials, sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Real-world datasets often contain features that are varying in degrees of magnitude, range and units. Therefore, in order for machine learning models to interpret these features on the same scale, we need to perform feature scaling.<br>\n<li><p style=\"font-family: Arials, sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman; font-size:200%; text-align:center;\"><b>In this kernal we will see what are the Common techniques of Feature Scaling and how to implement them</b></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman; font-size:200%;color:skyblue; text-align:center;\"><b>IF YOU LIKE THIS NOTE BOOK PLEASE UPVOTEðŸ˜Š</b></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman; font-size:200%; text-align:center;\"><b>Also check my Notebook on <a href=\"https://www.kaggle.com/mysarahmadbhat/types-of-transformations-for-better-distribution\">Types Of Transformation For Better Distribution:</a></b></p>","metadata":{}},{"cell_type":"markdown","source":" \n<p style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #808080; line-height:1.0\">TABLE OF CONTENT</p>\n\n\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"><a href=\"#part1\" style=\"color:#808080\">0 OVERVIEW</a></p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"><a href=\"#part2\" style=\"color:#808080\">0 IMPORTING LIBRARIES</a></p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"><a href=\"#part3\" style=\"color:#808080\">2 IMPORTING Data-Sets</a></p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"><a href=\"#part4\" style=\"color:#808080\">3 Most Common Technique:</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\">\n<a href=\"#part5\" style=\"color:#808080\">3.1 Min Max Scaler</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\">\n<a href=\"#part6\" style=\"color:#808080\">3.2  Standard Scaler</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\">\n<a href=\"#part7\" style=\"color:#808080\">3.3  Max Abs Scaler</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"> \n<a href=\"#part8\" style=\"color:#808080\">3.4 Robust Scaler</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"> \n<a href=\"#part9\" style=\"color:#808080\">3.5 Quantile Transform Scaler</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"> \n<a href=\"#part10\" style=\"color:#808080\">3.6 Power Transformer Scaler</a></p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"> \n<a href=\"#part11\" style=\"color:#808080\">3.7 Unit Vector Scaler</a></p>\n\n\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1.3\"><a href=\"#part12\" style=\"color:#808080\">4 Multiple Scaler</a></p>\n\n","metadata":{}},{"cell_type":"markdown","source":"<p id=\"part1\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; text-align:center; letter-spacing: 3px; color: #5B2C6F \">0 OVERVIEW:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman; font-size:200%; text-align:center;\"><b>First of all Why we care About Feature Scaling?</b></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Machine learning algorithm just sees number â€” if there is a vast difference in the range say few ranging in thousands and few ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.<br>\nThe machine learning algorithm works on numbers and does not know what that number represents. A weight of 10 grams and a price of 10 dollars represents completely two different things â€” which is a no brainer for humans, but for a model as a feature, it treats both as same.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Some examples of algorithms where feature scaling matters are:<br> K-nearest neighbors (KNN) with a Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally. <br> K-Means uses the Euclidean distance measure here feature scaling matters. <br> Scaling is critical while performing Principal Component Analysis(PCA). PCA tries to get the features with maximum variance, and the variance is high for high magnitude features and skews the PCA towards high magnitude features.<br> We can speed up gradient descent by scaling because Î¸ descends quickly on small ranges and slowly on large ranges, and oscillates inefficiently down to the optimum when the variables are very uneven.<br>  Algorithms that do not require normalization/scaling are the ones that rely on rules.<br> They would not be affected by any monotonic transformations of the variables. Scaling is a monotonic transformation.<br> Examples of algorithms in this category are all the tree-based algorithms â€” CART, Random Forests, Gradient Boosted Decision Trees. These algorithms utilize rules (series of inequalities) and do not require normalization.</p>","metadata":{}},{"cell_type":"markdown","source":"<p id=\"part2\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; text-align:center; letter-spacing: 3px; color: #5B2C6F \">1. IMPORTING LIBRARIES:</p>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:40:34.717525Z","iopub.execute_input":"2021-07-02T17:40:34.717863Z","iopub.status.idle":"2021-07-02T17:40:34.722035Z","shell.execute_reply.started":"2021-07-02T17:40:34.717834Z","shell.execute_reply":"2021-07-02T17:40:34.721017Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part3\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; text-align:center; letter-spacing: 3px; color: #5B2C6F \">2. IMPORTING DATA SETS:</p>","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/iris/Iris.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:37:16.438596Z","iopub.execute_input":"2021-07-02T17:37:16.438956Z","iopub.status.idle":"2021-07-02T17:37:16.450039Z","shell.execute_reply.started":"2021-07-02T17:37:16.438929Z","shell.execute_reply":"2021-07-02T17:37:16.449251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part4\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; text-align:center; letter-spacing: 3px; color: #5B2C6F \">3. Most Common Techniques:</p>","metadata":{}},{"cell_type":"markdown","source":"<p id=\"part5\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.1 Min-Max Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<img src= \"https://miro.medium.com/max/229/0*Gy668nQfirqf6W4c\" width=\"50%\"  align=\"center\"  hspace=\"5%\" vspace=\"5%\"  />","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g., between zero and one. This Scaler shrinks the data within the range of -1 to 1 if there are negative values. We can set the range like [0,1] or [0,5] or [-1,1].\nThis Scaler responds well if the standard deviation is small and when a distribution is not Gaussian. This Scaler is sensitive to outliers.</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n                   'PRICE': [1,3,2,5]},\n                   index = ['Orange','Apple','Banana','Grape'])\ndf1 = pd.DataFrame(scaler.fit_transform(df),\n                   columns=['WEIGHT','PRICE'],\n                   index = ['Orange','Apple','Banana','Grape'])\n\nax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n                     marker = '*',s=80, label='BREFORE SCALING');\ndf1.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n                 marker = 'o',s=60,label='AFTER SCALING', ax = ax);\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:41:20.817917Z","iopub.execute_input":"2021-07-02T17:41:20.818482Z","iopub.status.idle":"2021-07-02T17:41:21.001811Z","shell.execute_reply.started":"2021-07-02T17:41:20.818434Z","shell.execute_reply":"2021-07-02T17:41:21.000743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Let's Try on Iris Data Set:</p>","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\ndf.drop(\"Species\",axis=1,inplace=True)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ndf1=pd.DataFrame(scaler.fit_transform(df),columns=['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])\nax=df.plot.scatter(x=\"SepalLengthCm\",y=\"SepalWidthCm\",marker=\"*\",label=\"Before-Scaling\",color=\"red\")\ndf1.plot.scatter(x=\"SepalLengthCm\",y=\"SepalWidthCm\",marker=\"+\",label=\"After-Scaling\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:41:47.673455Z","iopub.execute_input":"2021-07-02T17:41:47.673921Z","iopub.status.idle":"2021-07-02T17:41:47.928005Z","shell.execute_reply.started":"2021-07-02T17:41:47.673889Z","shell.execute_reply":"2021-07-02T17:41:47.92725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part6\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.2 Standard Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">The Standard Scaler assumes data is normally distributed within each feature and scales them such that the distribution centered around 0, with a standard deviation of 1.\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. If data is not normally distributed, this is not the best Scaler to use.</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandardscaler=StandardScaler()\ndf = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n                   'PRICE': [1,3,2,5]},\n                   index = ['Orange','Apple','Banana','Grape'])\ndf1=pd.DataFrame(scaler.fit_transform(df),columns=['WEIGHT', 'PRICE'],index = ['Orange','Apple','Banana','Grape'])\n\nax=df.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",marker=\"*\",s=60,label=\"Before Scaling\",color=\"red\")\ndf1.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",marker=\"*\",s=60,label=\"After Scaling\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:41:36.992179Z","iopub.execute_input":"2021-07-02T17:41:36.992627Z","iopub.status.idle":"2021-07-02T17:41:37.220823Z","shell.execute_reply.started":"2021-07-02T17:41:36.992594Z","shell.execute_reply":"2021-07-02T17:41:37.220087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Let's Try on Iris Data Set:</p>","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\ndf.drop(\"Species\",axis=1,inplace=True)\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ndf1=pd.DataFrame(scaler.fit_transform(df),columns=['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])\nax=df.plot.scatter(x=\"SepalLengthCm\",y=\"SepalWidthCm\",marker=\"*\",label=\"Before-Scaling\",color=\"red\")\ndf1.plot.scatter(x=\"SepalLengthCm\",y=\"SepalWidthCm\",marker=\"+\",label=\"After-Scaling\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:41:43.253741Z","iopub.execute_input":"2021-07-02T17:41:43.254344Z","iopub.status.idle":"2021-07-02T17:41:43.492798Z","shell.execute_reply.started":"2021-07-02T17:41:43.25431Z","shell.execute_reply":"2021-07-02T17:41:43.492014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part7\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.3 Max Abs Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Scale each feature by its maximum absolute value. This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set is 1.0. It does not shift/center the data and thus does not destroy any sparsity.\nOn positive-only data, this Scaler behaves similarly to Min Max Scaler and, therefore, also suffers from the presence of significant outliers.</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler\nmaxabsscaler=MaxAbsScaler()\ndf = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n                   'PRICE': [1,3,2,5]},\n                   index = ['Orange','Apple','Banana','Grape'])\ndf1=pd.DataFrame(maxabsscaler.fit_transform(df),columns=['WEIGHT', 'PRICE'],index = ['Orange','Apple','Banana','Grape'])\n\nax=df.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",marker=\"+\",s=60,label=\"Before Scaling\",color=\"red\")\ndf1.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",marker=\"*\",s=60,label=\"After Scaling\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:42:43.25291Z","iopub.execute_input":"2021-07-02T17:42:43.253385Z","iopub.status.idle":"2021-07-02T17:42:43.482801Z","shell.execute_reply.started":"2021-07-02T17:42:43.253353Z","shell.execute_reply":"2021-07-02T17:42:43.482064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Let's Try on Iris Data Set:</p>","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\ndf.drop(\"Species\",axis=1,inplace=True)\nfrom sklearn.preprocessing import MaxAbsScaler\nmaxabsscaler=MaxAbsScaler()\ndf1=pd.DataFrame(maxabsscaler.fit_transform(df),columns=['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])\nax=df.plot.scatter(x=\"SepalLengthCm\",y=\"SepalWidthCm\",marker=\"*\",label=\"Before-Scaling\",color=\"red\")\ndf1.plot.scatter(x=\"SepalLengthCm\",y=\"SepalWidthCm\",marker=\"+\",label=\"After-Scaling\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:42:49.152427Z","iopub.execute_input":"2021-07-02T17:42:49.152893Z","iopub.status.idle":"2021-07-02T17:42:49.743228Z","shell.execute_reply.started":"2021-07-02T17:42:49.152861Z","shell.execute_reply":"2021-07-02T17:42:49.742059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part8\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.4 Robust Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">As the name suggests, this Scaler is robust to outliers. If our data contains many outliers, scaling using the mean and standard deviation of the data wonâ€™t work well.\nThis Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). The centering and scaling statistics of this Scaler are based on percentiles and are therefore not influenced by a few numbers of huge marginal outliers. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required.</p>","metadata":{}},{"cell_type":"code","source":"dfr = pd.DataFrame({'WEIGHT': [15, 18, 12,15,100],\n                   'PRICE': [1,3,2,5,20]},\n                   index = ['Orange','Apple','Banana','Grape','Jackfruit'])\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf21 = pd.DataFrame(scaler.fit_transform(dfr),\n                   columns=['WEIGHT','PRICE'],\n                   index = ['Orange','Apple','Banana','Grape','Jackfruit'])\nax = dfr.plot.scatter(x='WEIGHT', y='PRICE',color=\"red\", \n                     marker = '*',s=80, label='BREFORE SCALING');\ndf21.plot.scatter(x='WEIGHT', y='PRICE', color=\"green\",\n                 marker = 'o',s=60,label='STANDARD', ax = ax,figsize=(12,6))\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ndf31 = pd.DataFrame(scaler.fit_transform(dfr),\n                   columns=['WEIGHT','PRICE'],\n                   index = ['Orange','Apple','Banana','Grape','Jackfruit'])\ndf31.plot.scatter(x='WEIGHT', y='PRICE', color=\"black\",\n                 marker = 'v',s=60,label='ROBUST', ax = ax,figsize=(12,6))\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:43:07.773249Z","iopub.execute_input":"2021-07-02T17:43:07.77368Z","iopub.status.idle":"2021-07-02T17:43:08.04289Z","shell.execute_reply.started":"2021-07-02T17:43:07.773648Z","shell.execute_reply":"2021-07-02T17:43:08.041761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part9\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.5 Quantile Transformer  Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Transform features using quantiles information.<br>\nThis method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is, therefore, a robust pre-processing scheme.<br>\nThe cumulative distribution function of a feature is used to project the original values. Note that this transform is non-linear and may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable. This is also sometimes called as Rank scaler.</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\nscaler = QuantileTransformer()\ndf = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n                   'PRICE': [1,3,2,5]},\n                   index = ['Orange','Apple','Banana','Grape'])\ndf1 = pd.DataFrame(scaler.fit_transform(df),\n                   columns=['WEIGHT','PRICE'],\n                   index = ['Orange','Apple','Banana','Grape'])\nax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n                     marker = '+',s=80, label='BREFORE SCALING');\ndf1.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n                 marker = 'o',s=60,label='AFTER SCALING', ax = ax,figsize=(6,4))\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:43:40.093206Z","iopub.execute_input":"2021-07-02T17:43:40.093553Z","iopub.status.idle":"2021-07-02T17:43:40.314031Z","shell.execute_reply.started":"2021-07-02T17:43:40.093525Z","shell.execute_reply":"2021-07-02T17:43:40.312568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part10\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.6 Power Transformer  Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">The power transformer is a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to the variability of a variable that is unequal across the range (heteroscedasticity) or situations where normality is desired.\nThe power transform finds the optimal scaling factor in stabilizing variance and minimizing skewness through maximum likelihood estimation. Currently, Sklearn implementation of PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='yeo-johnson')\ndf = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n                   'PRICE': [1,3,2,5]},\n                   index = ['Orange','Apple','Banana','Grape'])\ndf1 = pd.DataFrame(scaler.fit_transform(df),\n                   columns=['WEIGHT','PRICE'],\n                   index = ['Orange','Apple','Banana','Grape'])\nax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n                     marker = '*',s=80, label='BREFORE SCALING');\ndf1.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n                 marker = 'o',s=60,label='AFTER SCALING', ax = ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:43:59.373543Z","iopub.execute_input":"2021-07-02T17:43:59.373871Z","iopub.status.idle":"2021-07-02T17:43:59.614434Z","shell.execute_reply.started":"2021-07-02T17:43:59.373844Z","shell.execute_reply":"2021-07-02T17:43:59.613536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part11\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">3.7 Unit Vector  Scaler:</p>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/110/0*BNgQzjr02S0lRolV.png\"></img>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\">Scaling is done considering the whole feature vector to be of unit length. This usually means dividing each component by the Euclidean length of the vector (L2 Norm). In some applications (e.g., histogram features), it can be more practical to use the L1 norm of the feature vector.<br>\nLike Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.</p>","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame({'WEIGHT': [15, 18, 12,10],\n                   'PRICE': [1,3,2,5]},\n                   index = ['Orange','Apple','Banana','Grape'])\ndf1=df.apply(lambda x:x/np.linalg.norm(x,1))\ndf1\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T16:41:52.943583Z","iopub.execute_input":"2021-07-02T16:41:52.944141Z","iopub.status.idle":"2021-07-02T16:41:52.957638Z","shell.execute_reply.started":"2021-07-02T16:41:52.944108Z","shell.execute_reply":"2021-07-02T16:41:52.956804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=df.apply(lambda x:x/np.linalg.norm(x,2))\ndf2","metadata":{"execution":{"iopub.status.busy":"2021-07-02T16:42:40.83442Z","iopub.execute_input":"2021-07-02T16:42:40.835113Z","iopub.status.idle":"2021-07-02T16:42:40.846764Z","shell.execute_reply.started":"2021-07-02T16:42:40.835078Z","shell.execute_reply":"2021-07-02T16:42:40.846068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=df.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",label=\"Before Scaling\",color=\"red\")\ndf1.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",label=\"After Scaling\",color=\"green\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T16:48:32.463214Z","iopub.execute_input":"2021-07-02T16:48:32.463553Z","iopub.status.idle":"2021-07-02T16:48:32.659053Z","shell.execute_reply.started":"2021-07-02T16:48:32.463525Z","shell.execute_reply":"2021-07-02T16:48:32.658223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=df.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",label=\"Before Scaling\",color=\"red\")\ndf2.plot.scatter(x=\"WEIGHT\",y=\"PRICE\",label=\"After Scaling\",color=\"green\",ax=ax)\nplt.axhline(0, color='red',alpha=0.2)\nplt.axvline(0, color='red',alpha=0.2);","metadata":{"execution":{"iopub.status.busy":"2021-07-02T16:48:45.724093Z","iopub.execute_input":"2021-07-02T16:48:45.724681Z","iopub.status.idle":"2021-07-02T16:48:45.908122Z","shell.execute_reply.started":"2021-07-02T16:48:45.724626Z","shell.execute_reply":"2021-07-02T16:48:45.907519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p id=\"part12\"></p>\n<p style=\"font-family: Arials; font-size: 26px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #5B2C6F \">4. Try Multiple Scalers:</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:  sans-serif; font-size: 20px; text-align:center; color: rgba(0,0,0,.7)\"> In this block i will try Multiple Scaling Techniques on a single data frame:</p>","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:22:50.048763Z","iopub.execute_input":"2021-07-02T17:22:50.049213Z","iopub.status.idle":"2021-07-02T17:22:50.05554Z","shell.execute_reply.started":"2021-07-02T17:22:50.049171Z","shell.execute_reply":"2021-07-02T17:22:50.054537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame({\"height\":[1,2,3,4],\"weight\":[10,20,30,40]})\ndf\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ndf1=pd.DataFrame(scaler.fit_transform(df),columns=['height', 'weight'])\nax=df.plot.scatter(x=\"height\",y=\"weight\",label=\"Before Transforming\",marker=\"*\",color=\"red\",figsize=(20,10))\ndf1.plot.scatter(x=\"height\",y=\"weight\",label=\"After Transforming\",marker=\"+\",color=\"green\",ax=ax)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ndf2=pd.DataFrame(scaler.fit_transform(df),columns=['height', 'weight'])\ndf2.plot.scatter(x=\"height\",y=\"weight\",label=\"Standard Scaler\",marker=\"o\",color=\"black\",ax=ax)\n\nfrom sklearn.preprocessing import MaxAbsScaler\nscaler=MaxAbsScaler()\ndf3=pd.DataFrame(scaler.fit_transform(df),columns=['height', 'weight'])\ndf3.plot.scatter(x=\"height\",y=\"weight\",label=\"Max Abs Scaler\",marker=\"v\",color=\"orange\",ax=ax)\n\nfrom sklearn.preprocessing import RobustScaler\nscaler=RobustScaler()\ndf4=pd.DataFrame(scaler.fit_transform(df),columns=['height', 'weight'])\ndf4.plot.scatter(x=\"height\",y=\"weight\",label=\"Robust Scaler\",marker=\"s\",color=\"yellow\",ax=ax)\n\nfrom sklearn.preprocessing import QuantileTransformer\nscaler=QuantileTransformer()\ndf5=pd.DataFrame(scaler.fit_transform(df),columns=['height', 'weight'])\ndf5.plot.scatter(x=\"height\",y=\"weight\",label=\"Quantile Transformer\",marker=\"p\",color=\"blue\",ax=ax)\n\nfrom sklearn.preprocessing import PowerTransformer\nscaler=PowerTransformer()\ndf5=pd.DataFrame(scaler.fit_transform(df),columns=['height', 'weight'])\ndf5.plot.scatter(x=\"height\",y=\"weight\",label=\"Power Transformer\",marker=\"x\",color=\"pink\",ax=ax)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T17:44:20.328202Z","iopub.execute_input":"2021-07-02T17:44:20.328543Z","iopub.status.idle":"2021-07-02T17:44:20.656593Z","shell.execute_reply.started":"2021-07-02T17:44:20.328514Z","shell.execute_reply":"2021-07-02T17:44:20.65569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman; font-size:200%;color:skyblue; text-align:center;\"><b>IF YOU FOUND THIS NOTE BOOK USEFUL PLEASE UPVOTEðŸ˜Š</b></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:newtimeroman; font-size:200%; text-align:center;\"><b>Also check my Notebook on <a href=\"https://www.kaggle.com/mysarahmadbhat/types-of-transformations-for-better-distribution\">Types Of Transformation For Better Distribution:</a></b></p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}