{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 线性回归"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"生成需要拟合的线性数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array([1, 2, 3])\ny = 2 * x + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"需要学习的参数是 w 和 b，初始化都为 0\n\n定义一个一次函数(即只有一个参数的线性函数)作为预测函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"w = 0\nb = 0\ndef predict(x):\n    return w * x + b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"求损失函数，这里使用均方误差除以2，即 $\\frac{1}{2N}\\sum_{i=1}^N{(y\\_predict-y)^2}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def MSE(y_predict, y):\n    return 0.5 * np.mean((y_predict - y) ** 2) # np.mean的作用就是把所有的平方差加起来求平均","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MSE(predict(x), y)  #输出一下当前w和b下误差是多少","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"loss = $\\frac{1}{2N}(\\boldsymbol{y}-\\boldsymbol{y\\_{predict})^\\top} (\\boldsymbol{y}-\\boldsymbol{y\\_{predict}})=\\frac{1}{2N}(\\boldsymbol{y}-(w\\boldsymbol{x}+b))^\\top (\\boldsymbol{y}-(w\\boldsymbol{x}+b))\\quad\\quad$ //向量形式\n\nloss =$\\frac{1}{2N}\\sum_{i=1}^N{(y-y\\_predict)^2}=\\frac{1}{2N}\\sum_{i=1}^N{(y-(wx+b))^2}\\quad\\quad $    //标量形式\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"w = 0\nb = 0\nlearning_rate = .01\nloss_list = []                    #保存训练过程中每次更新后的loss\nfor i in range(50):\n    d_w = (y - predict(x)) @ -x   #w的梯度\n    d_b = np.sum(predict(x) - y)  #b的梯度\n    w -= learning_rate * d_w      #更新w(为了降低loss则减去梯度，增加loss则加上梯度)\n    b -= learning_rate * d_b      #更新b\n    loss = MSE(predict(x), y)     #求出新的loss\n    loss_list.append(loss)        #记录loss，以便后面可视化\n    print(w, b, loss)             #输出更新后的w,b以及loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_list)                #可视化loss每一步变化","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 梯度下降"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(x):                    #目标(损失)函数\n    return x ** 2\n\ndef d(x):                    #梯度\n    return 2 * x\n\nx = np.linspace(-5, 5, 100)  #画出损失函数\ny = f(x)\nplt.plot(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, clear_output\nx_start = -3              #初始化参数\nlearning_rate = 0.1       #学习率\nstep = 10                 #迭代步数\nfor i in range(step):\n    x_start = x_start - learning_rate * d(x_start)    #用梯度更新参数\n    plt.title(\"x: %.4f, y: %.4f\" % (x_start, f(x_start)))  #标题\n    plt.plot(x, y)                                         #画出损失函数\n    plt.plot(x_start, f(x_start), 'ro')                    #画出当前的参数值以及对应的损失函数 \n    plt.show()\n    clear_output(wait=True)                                #等到下一张图来了再删除上一张\n    plt.pause(10)                                          #每张图停留0.5s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keras MLP(多层感知机/神经网络)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras.models import Sequential                               #序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1]\n])\ny = np.array([0, 0, 0, 1])\n\nmodel = Sequential()                  #创建一个序列模型的对象          \nmodel.add(Dense(2))                   #添加一个全连接层，2->2\nmodel.add(Activation('sigmoid'))      #添加一个sigmoid的激活函数\nmodel.add(Dense(1))                   #继续添加一个全连接层, 2->1\nmodel.add(Activation('sigmoid'))      #再添加一个sigmoid的激活函数\n\n#编译模型\nmodel.compile(optimizer=SGD(lr=1),    #选用sgd的优化器，学习率设为1\n              loss='binary_crossentropy', #损失(目标)函数采用二分类交叉熵损失函数\n              metrics=['accuracy'])       #用精度作为性能评价指标","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x, y, epochs=500)    #模型训练，history记录了训练过程中的一些中间信息","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练过程可视化"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history[\"loss\"])       #画出训练过程中每一步loss的变化\nplt.plot(history.history[\"acc\"])        #画出训练过程中每一步精度的变化","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:        #遍历模型的所有层\n    print(layer.get_weights())    #输出每层的权重(即需要训练的参数)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# MLP MNIST"},{"metadata":{},"cell_type":"markdown","source":"数据读取及预处理\n\n*这里数据读取时，需要注意路径名是否正确，当添加多个数据集时，input会为每一个数据集建一个文件夹*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nmnist_train = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")   #数据读取，当你的input只有mnist数据集时，目录应改为‘../input/mnist_train.csv’\nmnist_test = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")     #具体路径最好通过 os.listdir('../input/') 看下\nx_train = np.array(mnist_train.iloc[:, 1:]).reshape(-1, 28, 28)      #去除训练数据第一列的标签数据，并将数据reshape成 N×h×w\ny_train = np.array(mnist_train.iloc[:, 0])                           #提取训练数据标签\nx_test = np.array(mnist_test.iloc[:, 1:]).reshape(-1, 28, 28)        #去除测试数据第一列的标签数据，并将数据reshape成 N×h×w\ny_test = np.array(mnist_test.iloc[:, 0])                             #提取测试数据标签\n\nnum_classes = 10    #这个问题中一共有10类","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nx_train = x_train / 255        #数据归一化\nx_test = x_test / 255          \ny_train = to_categorical(y_train, num_classes)  #将标签变成独热编码，方便后面的交叉熵计算\ny_test = to_categorical(y_test, num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()                         #创建一个序列模型的对象\nmodel.add(Flatten(input_shape=(28, 28)))     #对于每个数据(这里指图片)全连接输入必输为一个向量，因此使用Flatten层起到了reshape的作用\nmodel.add(Dense(512, activation='relu'))     #加上全连接层 784->512\nmodel.add(Dropout(0.2))                      #dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.2的概率)，可以起到解耦合，防止过拟合等一系列作用\nmodel.add(Dense(512, activation='relu'))     #加上全连接层 512->512\nmodel.add(Dropout(0.2))                      #添加dropout层\nmodel.add(Dense(num_classes, activation='softmax')) #添加全连接层，然后加上softmax的激活函数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()  #输出模型各层详细信息，可以看到各层参数状况","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型编译"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',   #损失函数使用多类交叉熵损失函数\n              optimizer=\"rmsprop\",               #优化器采用rmsprop\n              metrics=['accuracy'])              #用精度作为性能评估指标","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练、评估及预测"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32        #每次输入32张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 5             #保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train, #训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,   #越大，训练过程中显示的信息越详细\n                    validation_data=(x_test, y_test))  #验证集\nscore = model.evaluate(x_test, y_test, verbose=0)  #模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])  #测试集上模型损失\nprint('Test accuracy:', score[1]) #测试集上模型精度","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(x_test[0:1]) #模型预测，输出预测的标签信息","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MLP CIFAR-10"},{"metadata":{},"cell_type":"markdown","source":"加载CIFAR-10数据集"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.keras'))  #join：将两个字符串合成一个路径；expanduser：把path中包含的\"~\"和\"~user\"转换成用户目录，这里~为/tmp/\nif not exists(cache_dir):   #检测路径是否存在\n    makedirs(cache_dir)     #不存在的情况下创建路径\ndatasets_dir = join(cache_dir, 'datasets') \nif not exists(datasets_dir):\n    makedirs(datasets_dir)           #最终创建路径为/tmp/.keras/datasets/\n    \n# if not exists(join(cache_dir,'datasets')):\n#     makedirs(join(cache_dir,'datasets'))\n\n\n# kaggle kernel可以使用linux shell命令，只需要在命令前加上感叹号\n!cp ../input/cifar10-python/cifar-10-python.tar.gz ~/.keras/datasets/    #将/kaggle/input/cifar10-python/下的cifar-10-python.tar.gz文件拷贝到刚创建的/tmp/.keras/datasets目录下\n!ln -s  ~/.keras/datasets/cifar-10-python.tar.gz ~/.keras/datasets/cifar-10-batches-py.tar.gz #为cifar-10-batches-py.tar.gz创建名字为cifar-10-python.tar.gz的符号链接\n!tar xzvf ~/.keras/datasets/cifar-10-python.tar.gz -C ~/.keras/datasets/ #将cifar-10-python.tar.gz解压","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /tmp/.keras/datasets/cifar-10-batches-py/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"数据预处理"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import cifar10\nfrom keras.utils import to_categorical\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()   #用keras提供的api读取数据\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\nx_train = x_train / 255  #数据归一化\nx_test = x_test / 255\nnum_classes = 10         #数据一共有10类\ny_train = to_categorical(y_train, num_classes) #将训练数据的标签独热编码\ny_test = to_categorical(y_test, num_classes)   #将测试数据的标签独热编码","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Dropout\n\nmodel = Sequential()                          #创建序列模型的对象\nmodel.add(Flatten(input_shape=(32, 32, 3)))   #用Flatten层将数据reshape成batchsize×（32*32*3）\nmodel.add(Dense(512, activation='relu'))      #添加全连接层，使用relu作为激活函数3072->512\nmodel.add(Dropout(0.2))                       #添加dropout层\nmodel.add(Dense(512, activation='relu'))      #添加全连接层，使用relu作为激活函数512->512\nmodel.add(Dropout(0.2))                       #添加dropout层\nmodel.add(Dense(num_classes, activation='softmax'))#添加全连接层，激活函数为softmax 512->10\n\nmodel.compile(loss='categorical_crossentropy',  #多类交叉熵损失函数\n              optimizer=\"rmsprop\",              #优化器使用rmsprop\n              metrics=['accuracy'])             #评估指标：精度","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练、评估"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 5\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN on CIFAR-10"},{"metadata":{},"cell_type":"markdown","source":"模型构建"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',   #添加卷积层；32：卷积核的个数;（3，3）:卷积核大小；padding=’same‘：图片卷积后大小不变\n                 input_shape=x_train.shape[1:]))#第一个卷基层需要告诉它输入图片大小，以方便网络推导后面所需参数\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))    #最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"模型训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 5\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}