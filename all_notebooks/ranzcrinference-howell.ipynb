{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nsys.path.append('../input/pytorchimagemodels')\n\nimport os\nimport typing as tp\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensorV2\n\n\nimport timm\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 640\nBATCH_SIZE = 32\nTEST_PATH = '../input/ranzcr-clip-catheter-line-classification/test'\nDEBUG = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\nif DEBUG:\n    test = test.sample(frac=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['StudyInstanceUID'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{TEST_PATH}/{file_name}.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image\n    \ndef get_transforms():\n        return Compose([\n            Resize(IMAGE_SIZE, IMAGE_SIZE),\n            Normalize(\n            ),\n            ToTensorV2(),\n        ])\n    \ndef get_activation(activ_name: str = \"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(inplace=True),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    else:\n        raise NotImplementedError\n\n\nclass Conv2dBNActiv(nn.Module):\n    \"\"\"Conv2d -> (BN ->) -> Activation\"\"\"\n\n    def __init__(\n            self, in_channels: int, out_channels: int,\n            kernel_size: int, stride: int = 1, padding: int = 0,\n            bias: bool = False, use_bn: bool = True, activ: str = \"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(Conv2dBNActiv, self).__init__()\n        layers = []\n        layers.append(nn.Conv2d(\n            in_channels, out_channels,\n            kernel_size, stride, padding, bias=bias))\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_channels))\n\n        layers.append(get_activation(activ))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        return self.layers(x)\n\n\nclass SSEBlock(nn.Module):\n    \"\"\"channel `S`queeze and `s`patial `E`xcitation Block.\"\"\"\n\n    def __init__(self, in_channels: int):\n        \"\"\"Initialize.\"\"\"\n        super(SSEBlock, self).__init__()\n        self.channel_squeeze = nn.Conv2d(\n            in_channels=in_channels, out_channels=1,\n            kernel_size=1, stride=1, padding=0, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"Forward.\"\"\"\n        # # x: (bs, ch, h, w) => h: (bs, 1, h, w)\n        h = self.sigmoid(self.channel_squeeze(x))\n        # # x, h => return: (bs, ch, h, w)\n        return x * h\n\n\nclass SpatialAttentionBlock(nn.Module):\n    \"\"\"Spatial Attention for (C, H, W) feature maps\"\"\"\n\n    def __init__(\n            self, in_channels: int,\n            out_channels_list: tp.List[int],\n    ):\n        \"\"\"Initialize\"\"\"\n        super(SpatialAttentionBlock, self).__init__()\n        self.n_layers = len(out_channels_list)\n        channels_list = [in_channels] + out_channels_list\n        assert self.n_layers > 0\n        assert channels_list[-1] == 1\n\n        for i in range(self.n_layers - 1):\n            in_chs, out_chs = channels_list[i: i + 2]\n            layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"relu\")\n            setattr(self, f\"conv{i + 1}\", layer)\n\n        in_chs, out_chs = channels_list[-2:]\n        layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"sigmoid\")\n        setattr(self, f\"conv{self.n_layers}\", layer)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = x\n        for i in range(self.n_layers):\n            h = getattr(self, f\"conv{i + 1}\")(h)\n\n        h = h * x\n        return h\n    \nclass MultiHeadModel(nn.Module):\n\n    def __init__(\n            self, base_name: str = 'resnext50_32x4d',\n            out_dims_head: tp.List[int] = [3, 4, 3, 1], pretrained=False):\n        \"\"\"\"\"\"\n        self.base_name = base_name\n        self.n_heads = len(out_dims_head)\n        super(MultiHeadModel, self).__init__()\n\n        # # load base model\n        base_model = timm.create_model(base_name, pretrained=pretrained)\n        in_features = base_model.num_features\n\n        # # remove global pooling and head classifier\n        base_model.reset_classifier(0, '')\n\n        # # Shared CNN Bacbone\n        self.backbone = base_model\n\n        # # Multi Heads.\n        for i, out_dim in enumerate(out_dims_head):\n            layer_name = f\"head_{i}\"\n            layer = nn.Sequential(\n                SpatialAttentionBlock(in_features, [64, 32, 16, 1]),\n                nn.AdaptiveAvgPool2d(output_size=1),\n                nn.Flatten(start_dim=1),\n                nn.Linear(in_features, in_features),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.5),\n                nn.Linear(in_features, out_dim))\n            setattr(self, layer_name, layer)\n\n    def forward(self, x):\n        \"\"\"\"\"\"\n        h = self.backbone(x)\n        hs = [\n            getattr(self, f\"head_{i}\")(h) for i in range(self.n_heads)]\n        y = torch.cat(hs, axis=1)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(models, test_loader, device):\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for model in models:\n            model.cuda()\n            model.eval()\n            with torch.no_grad():\n                y_preds1 = model(images)\n                y_preds2 = model(images.flip(-1))\n            y_preds = (y_preds1.sigmoid().to('cpu').numpy() + y_preds2.sigmoid().to('cpu').numpy()) / 2\n            avg_preds.append(y_preds)  #---暂时取消---0.05\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_multi_head_model(model_name, checkpoint):\n    state_dict = torch.load(checkpoint)  # 模型可以保存为pth文件，也可以为pt文件。\n\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = k[7:] # remove `module.`，表面从第7个key值字符取到最后一个字符，正好去掉了module.\n        new_state_dict[name] = v #新字典的key值对应的value为一一对应的值。\n        \n    model = MultiHeadModel(model_name, [3, 4, 3, 1], False)\n    model.load_state_dict(new_state_dict)\n    \n    return model\n\ntest_dataset = TestDataset(test, transform=get_transforms())\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                         num_workers=4 , pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = test.iloc[:, 1:12].columns.tolist()\n\nresnet200d_public = test.copy()\nregnety_080 = test.copy()\nresnest101e = test.copy()\ndm_nfnet_f0 = test.copy()\nseresnet152d = test.copy()\nresnet200d = test.copy()\ntf_b5 = test.copy()\n\nres = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(load_multi_head_model('resnet200d', '../input/razerck/resnet200d_b32_lr0.0004_cx_mh/resnet200d_b32_lr0.0004_fold0_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('resnet200d', '../input/razerck/resnet200d_b32_lr0.0004_cx_mh/resnet200d_b32_lr0.0004_fold1_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('resnet200d', '../input/razerck/resnet200d_b32_lr0.0004_cx_mh/resnet200d_b32_lr0.0004_fold2_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('resnet200d', '../input/razerck/resnet200d_b32_lr0.0004_cx_mh/resnet200d_b32_lr0.0004_fold3_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('resnet200d', '../input/razerck/resnet200d_b32_lr0.0004_cx_mh/resnet200d_b32_lr0.0004_fold4_mh_best_AUC.pth'))\npredictions = inference(models, test_loader, device)\nresnet200d[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet200D(nn.Module):\n    def __init__(self, model_name='resnet200d'):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, 11)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n\ndef LoadResNet200D(path):\n    model = ResNet200D()\n    model.load_state_dict(torch.load(path, map_location='cuda:0'))\n    return model\n\nmodels = []\nmodels.append(LoadResNet200D('../input/resnet200d-baseline-benchmark-public/resnet200d_fold0_cv953.pth'))\nmodels.append(LoadResNet200D('../input/resnet200d-baseline-benchmark-public/resnet200d_fold1_cv955.pth'))\nmodels.append(LoadResNet200D('../input/resnet200d-baseline-benchmark-public/resnet200d_fold2_cv955.pth'))\nmodels.append(LoadResNet200D('../input/resnet200d-baseline-benchmark-public/resnet200d_fold3_cv957.pth'))\nmodels.append(LoadResNet200D('../input/resnet200d-baseline-benchmark-public/resnet200d_fold4_cv954.pth'))\n\npredictions = inference(models, test_loader, device)\nresnet200d_public[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(load_multi_head_model('seresnet152d', '../input/razerck/seresnet152d_b32_lr0.0004_cx_mh/seresnet152d_b32_lr0.0004_fold0_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('seresnet152d', '../input/razerck/seresnet152d_b32_lr0.0004_cx_mh/seresnet152d_b32_lr0.0004_fold1_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('seresnet152d', '../input/razerck/seresnet152d_b32_lr0.0004_cx_mh/seresnet152d_b32_lr0.0004_fold2_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('seresnet152d', '../input/razerck/seresnet152d_b32_lr0.0004_cx_mh/seresnet152d_b32_lr0.0004_fold3_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('seresnet152d', '../input/razerck/seresnet152d_b32_lr0.0004_cx_mh/seresnet152d_b32_lr0.0004_fold4_mh_best_AUC.pth'))\n\npredictions = inference(models, test_loader, device)\nseresnet152d[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(load_multi_head_model('dm_nfnet_f0', '../input/razerck/dm_nfnet_f0_b32_lr0.0003/dm_nfnet_f0_b32_lr0.0003_fold0_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('dm_nfnet_f0', '../input/razerck/dm_nfnet_f0_b32_lr0.0003/dm_nfnet_f0_b32_lr0.0003_fold1_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('dm_nfnet_f0', '../input/razerck/dm_nfnet_f0_b32_lr0.0003/dm_nfnet_f0_b32_lr0.0003_fold2_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('dm_nfnet_f0', '../input/razerck/dm_nfnet_f0_b32_lr0.0003/dm_nfnet_f0_b32_lr0.0003_fold3_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('dm_nfnet_f0', '../input/razerck/dm_nfnet_f0_b32_lr0.0003/dm_nfnet_f0_b32_lr0.0003_fold4_mh_best_AUC.pth'))\n\npredictions = inference(models, test_loader, device)\ndm_nfnet_f0[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(load_multi_head_model('tf_efficientnet_b5_ns', '../input/razerck/tf_efficientnet_b5_ns_b32_lr0.0004_cx_mh/tf_efficientnet_b5_ns_b32_lr0.0004_fold0_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('tf_efficientnet_b5_ns', '../input/razerck/tf_efficientnet_b5_ns_b32_lr0.0004_cx_mh/tf_efficientnet_b5_ns_b32_lr0.0004_fold1_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('tf_efficientnet_b5_ns', '../input/razerck/tf_efficientnet_b5_ns_b32_lr0.0004_cx_mh/tf_efficientnet_b5_ns_b32_lr0.0004_fold2_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('tf_efficientnet_b5_ns', '../input/razerck/tf_efficientnet_b5_ns_b32_lr0.0004_cx_mh/tf_efficientnet_b5_ns_b32_lr0.0004_fold3_mh_best_AUC.pth'))\nmodels.append(load_multi_head_model('tf_efficientnet_b5_ns', '../input/razerck/tf_efficientnet_b5_ns_b32_lr0.0004_cx_mh/tf_efficientnet_b5_ns_b32_lr0.0004_fold4_mh_best_AUC.pth'))\n\npredictions = inference(models, test_loader, device)\ntf_b5[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models = []\n# models.append(load_multi_head_model('regnety_080', '../input/razerck/regnety_080_b32_lr0.0003/regnety_080_b32_lr0.0003_fold0_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('regnety_080', '../input/razerck/regnety_080_b32_lr0.0003/regnety_080_b32_lr0.0003_fold1_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('regnety_080', '../input/razerck/regnety_080_b32_lr0.0003/regnety_080_b32_lr0.0003_fold2_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('regnety_080', '../input/razerck/regnety_080_b32_lr0.0003/regnety_080_b32_lr0.0003_fold3_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('regnety_080', '../input/razerck/regnety_080_b32_lr0.0003/regnety_080_b32_lr0.0003_fold4_mh_best_AUC.pth'))\n\n# predictions = inference(models, test_loader, device)\n# regnety_080[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models = []\n# models.append(load_multi_head_model('resnest101e', '../input/razerck/resnest101e_b32_lr0.0003/resnest101e_b32_lr0.0003_fold0_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('resnest101e', '../input/razerck/resnest101e_b32_lr0.0003/resnest101e_b32_lr0.0003_fold1_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('resnest101e', '../input/razerck/resnest101e_b32_lr0.0003/resnest101e_b32_lr0.0003_fold2_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('resnest101e', '../input/razerck/resnest101e_b32_lr0.0003/resnest101e_b32_lr0.0003_fold3_mh_best_AUC.pth'))\n# models.append(load_multi_head_model('resnest101e', '../input/razerck/resnest101e_b32_lr0.0003/resnest101e_b32_lr0.0003_fold4_mh_best_AUC.pth'))\n\n# predictions = inference(models, test_loader, device)\n# resnest101e[target_cols] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 0.25\nw = [0.25, 0.2, 0.2, 0.2, 0.15]\nprint(sum(w))\nres[target_cols] = w[0]*resnet200d[target_cols]**a + w[1]*resnet200d_public[target_cols]**a + w[2]*seresnet152d[target_cols]**a + w[3]*dm_nfnet_f0[target_cols]**a + w[4]*tf_b5[target_cols]**a\nres.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res[['StudyInstanceUID'] + target_cols].to_csv('submission.csv', index=False)\nres.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}