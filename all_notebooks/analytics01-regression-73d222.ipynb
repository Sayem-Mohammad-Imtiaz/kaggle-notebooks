{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Regression\nRegression is about taking a set of input a set of features and creating a model that predicts numerical values. _Examples of regressor models include the types of analysis done in financial markets to predict pricing, looking at how the features of an automobile might affect its gas mileage, or anything else that might be measured on a (usually broad) range._\n\nIn this example we will examine BitCoin prices and see if we can predict the value.  The data comes from a set of Coinbase trades from December of 2014 to January of 2018 and is available from Kaggle. _See the references section of this chapter for links._\n\nThroughout this activity, we will:\n\n* Transform and prepare the data so that we can utilize it in a regression analysis. This will include resampling data into a set of points summarized by the day.\n* Create a summary of the data and look for distinguishing features.\n* Create a regressor for the data which can be used to predict the cost of BitCoin.\n* Asses the accuracy and perforance of the model.\n\n### Linear Regression\nLinear regression is the simplest and most widely used algorithm for building regression models. The algorithm plots the dataset as a set of points with the target variable on the y axis. It then attempts to fit a straight line (or plane) to the points using a variant of the equation $y=m*x+b$.\n\n\n### Evaluating the Accuracy of a Regressor\n\n* means squared error: standard measurement of evaluation for regression. Average square difference between the true value of the target variable and the model value.\n* [R2 score (coefficient of determination)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html). Used for testing the accuracy of the model. Best possible score is 1 and the score can be negative, as the model can be worse than random chance.\n\n\n### Import Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import ensemble, linear_model, model_selection, preprocessing, svm\n\n# Import tools that we can use to evalue the accuracy of the model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation\nInitial data preparation:\n\n* load data from CSV file\n* convert the unix time stamp to a datetime so that it is easier to resample\n* reset the index\n* rename columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Resampling data from minute interval to day\nbit_df = pd.read_csv('../input/coinbase/coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv',\n  low_memory=False, error_bad_lines=True)\nbit_df['Timestamp'] = bit_df.Timestamp.astype('int', errors='ignore')\n\n# Convert unix time to datetime so that it is easier to resample\nbit_df['date'] = pd.to_datetime(bit_df.Timestamp, unit='s', errors='coerce')\n\n# Reset index\nbit_df = bit_df.set_index('date')\n\n# Rename columns so easier to code\nbit_df = bit_df.rename(columns={'Open':'open', 'High': 'hi', 'Low': 'lo',\n   'Close': 'close', 'Volume_(BTC)': 'vol_btc',\n   'Volume_(Currency)': 'vol_cur',\n   'Weighted_Price': 'wp', 'Timestamp': 'ts'})\n\n# Coerce to numeric data types (safeguard against corrupt data)\nbit_df['hi'] = pd.to_numeric(bit_df.hi, errors='coerce')\nbit_df['lo'] = pd.to_numeric(bit_df.lo, errors='coerce')\nbit_df['close'] = pd.to_numeric(bit_df.close, errors='coerce')\nbit_df['open'] = pd.to_numeric(bit_df.open, errors='coerce')\nbit_df['ts'] = pd.to_numeric(bit_df.ts, errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Resample the initial data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resample and only use recent samples that aren't missing\nbit_df = bit_df.resample('d').agg({'open': 'mean', 'hi': 'mean',\n    'lo': 'mean', 'close': 'mean', 'vol_btc': 'sum',\n    'vol_cur': 'sum', 'wp': 'mean', 'ts': 'min'}).iloc[-1000:]\n\n# Drop last row as it is not complete\nbit_df = bit_df.iloc[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the data to view values\nbit_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transpose the header values\nbit_df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a description of the data\nbit_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the distribution\nbit_df.plot(figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bit_df.close.plot(figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Load Data\nExercises associated with this example look at predicting the size of forest fires on meteorological data.\n\n* Load data from a CSV file\n* Inspect, summarize, and plot the dataset\n\n\n### Can we predict tomorrow's close based on today's info?\nWe will use a row of data for input. We will call the input X and the prediction y. This is called \"supervised learning\" as we will feed in both X and y to train the model.\n\nLet's use a model called Linear Regression. This performs better if we *standardize* the data (0 mean, 1 std).\n\nFor 2 dimensions this takes the form of:\n\n$y = m*x + b$\n\nM is the slope (or coefficient) and b is the intercept.\n\nLet's see if we can predict the open price from the ts component."},{"metadata":{"trusted":true},"cell_type":"code","source":"bit_df.plot(kind='scatter', x='ts', y='open', figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create our input (X) and our labelled data (y) to train our model\nX = bit_df[['ts']].iloc[:-1]  # drop last row because it represents the value trying to be predicted\ny = bit_df.close.shift(-1).iloc[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train a model and predict output if it were given X\nlr_model = linear_model.LinearRegression()\nlr_model.fit(X, y)\npred = lr_model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the real data, our prediction (blue), and the model from the coeffictient (green shifted)\nax = bit_df.plot(kind='scatter', x='ts', y='open', color='black', figsize=(14,10))\nax.plot(X, pred, color='blue')  # matplotlib plot\nax.plot(X, X*lr_model.coef_ + lr_model.intercept_+ 100, linestyle='--', color='green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vertical distance between line and point is the error. *Ordinary Least Squares*\n# regression tries to minimize the square of the distance.\nmean_squared_error(y, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R2 score is a measure from 0-100\n# 0 - the model explains none of the variation\n# 100 - 100% of the variation is explained by the model\nprint(r2_score(y, pred))\n\n# Note that the .score method gives the same value\nprint(lr_model.score(X, y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Regression\n\n* Use linear regression to predict `area` from the other columns.\n* Calculate the predictive model's score.\n\n\n### Visualize Performance of the Model: Actual and Predicted Values\nYou can plot the actuals and the predicted values. It looks like the model does a pretty poor job of describing the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction error plot from Yellowbrick\n# plot of actual (blue) vs predicted (black dash)\n# ideally would be around 45 degree line\nfig, ax = plt.subplots(figsize=(10, 8))\nerr_viz = PredictionError(lr_model)\n\n# Model is already fit\n#err_viz.fit(X, y)\nerr_viz.score(X, y)\nerr_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot result\ny_df = pd.DataFrame(y)\ny_df['pred'] = pred\ny_df['err'] = y_df.pred - y_df.close\n(y_df\n #.iloc[-50:]\n .plot(figsize=(14,10))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Visualize the Errors\n\n* Plot the actual y and predicted y against one another to compare the accuracy of the model\n\n\n### Improve the Accuracy of the Model: Try More Features\nIn an attempt to get a better model we are going to use more features to make a prediction. Many machine language estimators require \"standardization\" of the data and will perform badly if the individual features do not more or less look like normally distributed data: Gaussian distributions with a zero mean and unit variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop last row because we don't know what future is\nX = (bit_df.drop(['close'], axis=1).iloc[:-1])\ny = bit_df.close.shift(-1).iloc[:-1]\ncols = X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The describe method on a dataframe gives a statistical summary of the columns\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to scale the data so that volume and ts don't get more\n# weight that other values\nss = preprocessing.StandardScaler()\nss.fit(X)\nX = ss.transform(X)\nX = pd.DataFrame(X, columns=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can now see that the data has a mean close to 0\n# and a std of 1\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a linear regression model using the normalized data\nlr_model2 = linear_model.LinearRegression()\nlr_model2.fit(X, y)\npred = lr_model2.predict(X)\nlr_model2.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot result\ny_df = pd.DataFrame(y)\ny_df['pred'] = pred\ny_df['err'] = y_df.pred - y_df.close\ny_df.plot(figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot result\ny_df = pd.DataFrame(y)\ny_df['pred'] = pred\ny_df['err'] = y_df.pred - y_df.close\ny_df.iloc[-50:].plot(figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# our scores get worse with recent data\nlr_model2.score(X[-50:], y[-50:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model2.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X.columns, lr_model2.coef_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These coefficients correspond to the columns in X\npd.DataFrame(list(zip(X.columns, lr_model2.coef_)), columns=['Feature', 'Coeff'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bit_df.plot(kind='scatter', x='wp', y='close', figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bit_df.plot(kind='scatter', x='vol_cur', y='close', figsize=(14,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Regression\n* Try scaling the input and using the log of the area and see if you get a better score.\n* Examine the coefficients\n\n\n### Training/Test Split\nIn fact we were cheating, predicting things that we already saw serves little purpose. The model could just memorize the data and get a perfect score. But it wouldn't *generalize* to unseen data.\n\nTo see how it will perform in the real world we will train on a portion of the data and test on a portion that it hasn't seen."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into a set for training and testing: X = Feature, Y = Target\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model on the training on the training data, evalue on the testing data\nlr_model2 = linear_model.LinearRegression()\nlr_model2.fit(X_train, y_train)\nlr_model2.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df2 = pd.DataFrame(y_test)\ny_df2['pred'] = lr_model2.predict(X_test)\ny_df2['err'] = y_df2.pred - y_df2.close\n(\ny_df2\n #   .iloc[-50:]\n    .plot(figsize=(14,10))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yellow brick version\nfig, ax = plt.subplots(figsize=(10, 10))\nerr_viz2 = PredictionError(lr_model2)           # Attempt to show the\nerr_viz2.score(X_test, y_test)                  \nerr_viz2.poof()                                 # Draw/show/poof the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercise: Regression with Train/Test Split\nSplit the data into test and training data. What is the score on the test data?\n\n### Visualize Errors with Residual Plots\n**A residual is the difference between the prediction and the actual.** If we plot predicted value against residuals, we should get a random distribution. If not, a different model would be better given the data. _A pattern in the residuals implies that there is a non-parametric relationship at play._"},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_plot(model, X_train, y_train, X_test, y_test):\n    fig = plt.figure(figsize=(14,10))\n    ax = plt.subplot(111)\n    plt.scatter(model.predict(X_train),\n                model.predict(X_train) - y_train,\n                c='b', alpha=.3,\n                label='train')\n    plt.scatter(model.predict(X_test),\n                model.predict(X_test) - y_test,\n                color='green', alpha=.3,\n                label='test')\n    plt.title('Residual Plot - Train (blue), Test (green)')\n    plt.ylabel('Residual')\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual_plot(lr_model2, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Yellowbrick version\nfig, ax = plt.subplots(figsize=(10, 10))\nres_viz = ResidualsPlot(lr_model2)\nres_viz.fit(X_train, y_train)\nres_viz.score(X_test, y_test)\nres_viz.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercises: Residual Plot\nMake a residual plot of your test and train data\n\n\n### Other Models: SVM, Random Forest, and Huber\nLogistic Regression is not the only model that can be used for classification:\n\n* **SVM**: Support vector machines include both linear and non-linear variations. Like logistic regression, the main idea is to find the line (or plane or dividing shape) that separates the targets/classes optimally. Instead of measuring the distance to all points, VCMs try to find the largest margin between only the points on either side of the decision line. But rather than worry about points that are far away to the boundary of a decision (e.g., the obvious ones), the algorithm focuses on the points that the closest to the line. It then seeks to place the line in such a way so that the distance of those points is as great as possible."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"SVMS use a trick to map points that are non-linear in nature to a coordinate plane that is non-linear. The algorithm then tries to find a linear boundary in the warped space."},{"metadata":{},"cell_type":"markdown","source":"* **Random Forest**: Random forests rely upon the use of a decision tree. Decision trees are based on a series of branch points that help to make a decision. When using a decision tree algorithm, you allow the computer to figure out (based on the training data) which variables are the most imortant. It then puts these at the top of the tree and gradually uses less important variables in subsequent branches until a path to target outcomes has been plotted."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In decision trees, the top most level branches have an enormous influence on the quality of the tree. If new data doesn't follow the same distribution as the training set, then the model doesn't generalize quite as well.\n\nRandom forests build a collection of decision trees and apply these to new observations. It then uses a set of \"votes\" to weight the outputs of several trees and apply them to the new observation. It provides the majority vote in the case of classification or the mean value when performing regression.\n\n- Random forests have a degree of immunity to unimportant features\n- They are also able to cope with noisy datasets or those with missing values"},{"metadata":{},"cell_type":"markdown","source":"* **Huber**: A regression algorithm that is useful with datasets with outliers. It does this by scoring the outliers and weighting their scores appropriately."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop last row because we don't know what future is\n\nX = (bit_df\n         .drop(['close'], axis=1)\n         .iloc[:-1])\ny = bit_df.close.shift(-1).iloc[:-1]\ncols = X.columns\n\nss = preprocessing.StandardScaler()\nss.fit(X)\nX = ss.transform(X)\nX = pd.DataFrame(X, columns=cols)\n\nX_train, X_test, y_train, y_test = model_selection.\\\n    train_test_split(X, y, test_size=.3, random_state=42)\n\n# Create an SVM model using the Epsilon-Support Vector Regression    \nsvm_model = svm.SVR(kernel='linear')\nsvm_model.fit(X_train, y_train)\nsvm_model.score(X_test, y_test)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_reg_model(model, df):\n    # drop last row because we don't know what future is\n\n    X = (df\n             .drop(['close'], axis=1)\n             .iloc[:-1])\n    y = df.close.shift(-1).iloc[:-1]\n    cols = X.columns\n\n    ss = preprocessing.StandardScaler()\n    ss.fit(X)\n    X = ss.transform(X)\n    X = pd.DataFrame(X, columns=cols)\n\n    X_train, X_test, y_train, y_test = model_selection.\\\n        train_test_split(X, y, test_size=.3, random_state=42)\n\n    #svm_model = svm.SVR(kernel='linear')\n    model.fit(X_train, y_train)\n    return model.score(X_test, y_test), X_test, y_test, X_train, y_train    \n\n# Generate a random forest model\nrf_reg = ensemble.RandomForestRegressor()\nscore, X_test, y_test, X_train, y_train = train_reg_model(rf_reg, bit_df)\nprint(score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def error_plot(X_test, y_test, model):\n    y_df3 = pd.DataFrame(y_test)\n    y_df3['pred'] = model.predict(X_test)\n    y_df3['err'] = y_df3.pred - y_df3.close\n    (\n    y_df3\n     #   .iloc[-50:]\n        .plot(figsize=(14,10))\n    )\nerror_plot(X_test, y_test, rf_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yellow brick version\nfig, ax = plt.subplots(figsize=(10, 10))\nerr_viz3 = PredictionError(rf_reg)\nerr_viz3.score(X_test, y_test)\nerr_viz3.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual_plot(rf_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a Huber Regressor\nhuber_reg = linear_model.HuberRegressor()\nhuber_reg.fit(X_train, y_train)\nhuber_reg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_plot(X_test, y_test, huber_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yellow brick version\nfig, ax = plt.subplots(figsize=(10, 10))\nerr_viz4 = PredictionError(huber_reg)\nerr_viz4.score(X_test, y_test)\nerr_viz4.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual_plot(huber_reg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"huber_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model.HuberRegressor(\n    alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,\n    tol=1e-05, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exercises: Other Models\nTry using another model (`RandomForestRegressor` or `SVR`) and assess the accuracy of the new model"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}