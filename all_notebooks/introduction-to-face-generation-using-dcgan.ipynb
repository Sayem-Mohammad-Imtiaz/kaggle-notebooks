{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction:\nGAN i.e. generative adversarial network was first introduced into the world by Ian Goodfellow in 2014. It is the type of neural network used to generate new images. In this notebook, we will explore GAN networks, and train a GAN to generate few artificial images ourselves.<br/>\nThis notebook is forked from [mehmet lauda tekman's code](https://www.kaggle.com/mehmetlaudatekman/dcgans-face-generation).<br/>\nThere are a quite a number of different GAN architectures like [vanilla GAN](https://www.kaggle.com/zorroblue/vanilla-gan-on-fashion-mnist), DCGAN i.e. deep convolutional GAN etc. In this notebook, we are implementing DCGAN.<br/>\nHere are a few resources which helped develop this notebook as well as to understand GAN training.<br/>\n(1)[celeb dataset original link](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)<br/>\n\n(3)[code inspiration](https://towardsdatascience.com/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b)<br/>\n(4)[random sampling from numpy array](https://www.kite.com/python/answers/how-to-select-random-rows-from-a-numpy-array-in-python)<br/>\n(5)[How to identify gan training failures](https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/)<br/>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport PIL\nimport os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport cv2\nimport random\n\nMAIN_PATH = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* First, we are going to create a list that contains paths of images. We'll get random paths from that list."},{"metadata":{"trusted":true},"cell_type":"code","source":"image_paths = glob(MAIN_PATH+\"/*\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(image_paths)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have 202K images in CelebA dataset, so we won't read all images, we'll read them as batches.\n\nLet's define a function. This function will take path argument read image from that path."},{"metadata":{"trusted":true},"cell_type":"code","source":"def readImage(path,image_size=(256,256)):\n    img = np.asarray(PIL.Image.open(path).resize(image_size))\n    # img - 127.5 / 127.5 ==> compress between [-1,1]\n    img = ((img - 127.5) / 127.5).astype(\"float32\")\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's test our function."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_img = readImage(\"../input/celeba-dataset/img_align_celeba/img_align_celeba/000007.jpg\")\nprint(test_img.shape)\n\nplt.imshow(test_img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I said we'll read images as batches so now we'll define a generator that yield one batch data."},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\n#STEPS_PER_EPOCH = len(image_paths) // BATCH_SIZE\n#1582 steps are too many steps. Let's just train for 500 steps.\nSTEPS_PER_EPOCH = 500\nprint(\"Steps per epochh are\",STEPS_PER_EPOCH)\ndef dataGenerator(batch_size):\n    while True:\n        paths = random.choices(image_paths,k=batch_size)\n        batch = []\n        for p in paths:\n            batch.append(readImage(p))\n        \n        yield np.asarray(batch)\n\ndataGen = dataGenerator(BATCH_SIZE)\nprint(next(dataGen).shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our data is ready, let's start to build our GANs\n\nFirst, we'll create our Generator. Generator will upsample our seed (I explained it in introduction) using convolutional transpose layers (upsampling layers)"},{"metadata":{"trusted":true},"cell_type":"code","source":"WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.2)\ndef make_generator():\n    model = tf.keras.Sequential()\n    \n    # Random noise to 16x16x256 image\n    model.add(layers.Dense(16*16*256,use_bias=False,input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Reshape((16,16,256)))\n    \n    assert model.output_shape == (None,16,16,256)\n    \n    model.add(layers.Conv2DTranspose(128,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    assert model.output_shape == (None,32,32,128)\n    \n    model.add(layers.Conv2DTranspose(128,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    assert model.output_shape == (None,64,64,128)\n    \n    model.add(layers.Conv2DTranspose(64,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    assert model.output_shape == (None,128,128,64)\n    \n    model.add(layers.Conv2DTranspose(3,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT,\n                                     activation=\"tanh\"\n                                    ))\n              # Tanh activation function compress values between -1 and 1. \n              # This is why we compressed our images between -1 and 1 in readImage function.\n    assert model.output_shape == (None,256,256,3)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = make_generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we'll use cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(fake_output):\n    # First argument of loss is real labels\n    # We've labeled our images as 1 (real) because\n    # we're trying to fool discriminator\n    return cross_entropy(tf.ones_like(fake_output),fake_output)\n\ngen_optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we'll build our discriminator network.\n* Discriminator network is a CNN based image classifier that classify images real or fake."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_discriminator():\n    model = tf.keras.Sequential()\n    \n    model.add(layers.Conv2D(64,(5,5),strides=(2,2),padding=\"same\",input_shape=(256,256,3)))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(128,(5,5),strides=(2,2),padding=\"same\"))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(265,(5,5),strides=(2,2),padding=\"same\"))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = make_discriminator()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_loss(real_images,fake_images):\n    real_loss = cross_entropy(tf.ones_like(real_images),real_images)\n    fake_loss = cross_entropy(tf.zeros_like(fake_images),fake_images)\n    total_loss = real_loss + fake_loss\n    return total_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator_optimizer = tf.keras.optimizers.Adam(lr=1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we'll train model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\nNOISE_DIM = 100\n\n@tf.function\ndef train_step(images):\n    # We've created random seeds\n    noise = tf.random.normal([BATCH_SIZE,NOISE_DIM])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generator generated images\n        generated_images = generator(noise,training=True)\n        \n        # We've sent our real and fake images to the discriminator\n        # and taken the decisions of it.\n        real_output = discriminator(images,training=True)\n        fake_output = discriminator(generated_images,training=True)\n        \n        # We've computed losses of generator and discriminator\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output,fake_output)\n    \n    # We've computed gradients of networks and updated variables using those gradients.\n    gradients_of_generator = gen_tape.gradient(gen_loss,generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n    \n    gen_optimizer.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport sys\ndef train(epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        for step in range(STEPS_PER_EPOCH):\n            train_step(next(dataGen))\n    \n            sys.stdout.write(f\"\\rSTEP: {step}/{STEPS_PER_EPOCH}\")\n            sys.stdout.flush()\n            \n        finish_time = round(time.time() - start,2)\n        print(f\"Epoch {epoch}/{epochs} Process Time : {finish_time}\")\n        print(\"-\"*15)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's generate and display some synthetic images."},{"metadata":{"trusted":true},"cell_type":"code","source":"noise = tf.random.normal([16,100])\ngenerated_images = np.asarray(generator(noise,training=False))\n\nfig = plt.figure(figsize=(6,6))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow((generated_images[i,:,:,:]*127.5+127.5).astype(\"int\"))\n    plt.axis(\"off\")\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## save the model for future use\n[Related resource](https://www.tensorflow.org/tutorials/keras/save_and_load)"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.save('/kaggle/working/shyam_face_generator')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion:\nAs you can see, some amount of human like faces are coming up. This is the basic DCGAN implementation. Obviously, increasing the epoch numbers can improve the quality of the images. Hope you liked the content. Please comment and share your views about the project.<br/> For limited kernel time(9hrs), we have only been able to create low class images. You can try and train for longer time, and get better images. i.e. Increasing the epoch will improve the quality.\nSo, that's it folks! hope you learnt some and liked it!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}