{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import preprocessing, linear_model, ensemble, metrics, model_selection, svm, pipeline, naive_bayes\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nimport spacy\nimport textblob\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.multioutput import MultiOutputClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read Data\ntrain = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/train.csv')\ntest = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = train['ID']\ntest_id = test['ID']\n\n# Create indices to split train and test on later\ntrain['train_ind'] = np.arange(train.shape[0])\ntest['train_ind'] = np.arange(train.shape[0], train.shape[0]+test.shape[0])\n\n# Merge Train and Test - This approach only works for competitions - not for model deployment in real projects.\ndata = pd.concat([train, test], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create class which performs Label Encoding - if required\nclass categorical_encoder:\n    def __init__(self, columns, kind = 'label', fill = True):\n        self.kind = kind\n        self.columns = columns\n        self.fill = fill\n        \n    def fit(self, X):\n        self.dict = {}\n        self.fill_value = {}\n        \n        for col in self.columns:\n            label = preprocessing.LabelEncoder().fit(X[col])\n            self.dict[col] = label\n            \n            # To fill\n            if self.fill:\n                self.fill_value[col] = X[col].mode()[0]\n                X[col] = X[col].fillna(self.fill_value[col])\n                \n        print('Label Encoding Done for {} columns'.format(len(self.columns)))\n        return self\n    def transform(self, X):\n        for col in self.columns:\n            if self.fill:\n                X[col] = X[col].fillna(self.fill_value[col])\n                \n            X.loc[:, col] = self.dict[col].transform(X[col])\n        print('Transformation Done')\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Lemmatizer - if required\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, articles):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to Create CountEncoded and tf-idf features\ndef add_text_features(text_column_name, data_file, max_features = 2000, txn = 'tf-idf', min_df = 1, max_df = 1.0,\n                     ngram_range = (1, 1), lowercase = True, sparse = False, tokenizer = None):\n    if txn == 'count':\n        # Use Count Vectorizer\n        counts = CountVectorizer(max_features = max_features, min_df = min_df, \n        max_df = max_df, ngram_range = ngram_range, lowercase = lowercase, tokenizer=tokenizer).fit(data_file[text_column_name])\n    if txn == 'tf-idf':\n        counts = pipeline.make_pipeline(CountVectorizer(max_features = max_features, min_df = min_df, \n        max_df = max_df, ngram_range = ngram_range, lowercase = lowercase, tokenizer=tokenizer),\n                                        TfidfTransformer()).fit(data_file[text_column_name])\n    text_features = counts.transform(data_file[text_column_name])\n    \n    # Return for sparse output\n    if sparse: return text_features, None\n    \n    # Create Mapping\n    if txn == 'count':\n        mapping = {val: key for key, val in counts.vocabulary_.items()}\n    if txn == 'tf-idf':\n        mapping = {val: key for key, val in counts['countvectorizer'].vocabulary_.items()}\n    \n    # Create DataFrame\n    text_features_data = pd.DataFrame(text_features.toarray())\n    text_features_data = text_features_data.rename(mapping, axis = 1)\n    text_cols = text_features_data.columns.tolist()\n    \n    # Append to dataframe\n    data_copy = pd.concat([data_file.reset_index(drop = True), text_features_data.reset_index(drop = True)], axis = 1)\n    return data_copy, text_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy, text_cols = add_text_features(text_column_name = 'ABSTRACT', \n                                     data_file = data, max_features = 150000, min_df = 5, max_df = .5,\n                                    ngram_range = (1, 3), lowercase = True, sparse = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data back to train and test\nX_train = data_copy[:train.shape[0], :]\ny_train = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[:train.shape[0]]\n\nX_test = data_copy[train.shape[0]:, :]\ny_test = data[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].iloc[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model - Logistic Regression is a good option for Text classification problems\n#model = linear_model.LogisticRegressionCV(penalty = 'l2', Cs = 10, max_iter = 5000).fit(X_train, y_train)\n#model = linear_model.RidgeClassifierCV().fit(X_train, y_train)\nfrom sklearn import naive_bayes\n\n#model = MultiOutputClassifier(estimator = naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)).fit(X_train, y_train)\nmodel = MultiOutputClassifier(estimator = linear_model.LogisticRegressionCV(Cs = 10, cv = 5, n_jobs = -1, max_iter = 5000)).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_multioutput(predictions):\n    return np.array([[val[1] for val in inner] for inner in predictions]).T\n\ndef convert_probs_to_labels(predictions, threshold = .5, labels = None):\n    final = []\n    for prediction in predictions:\n        temp = (prediction > threshold)*1\n        final.append(temp)\n        \n    return final\n\ndef predict_1(predictions, threshold=.5):\n    preds = get_preds_multioutput(predictions)\n    preds = convert_probs_to_labels(preds, threshold = threshold, labels = None)\n    return np.array(preds)\n\n#predict_1(model.predict_proba(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ID'] = test_id\n\npreds = predict_1(model.predict_proba(X_test))\nsub[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']] = model.predict(X_test).astype(int)\nsub.to_csv('sub.csv', index = None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}