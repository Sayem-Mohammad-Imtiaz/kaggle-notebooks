{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Reading a text-based dataset into pandas","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objects as go\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove the Unnamed: 2, Unnamed: 3, Unnamed: 4 columns due to all the entries were null.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# see the null data here\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# search the most relevant message \ndf['v2'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count of ham and spam\ndf['v1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert categorical v1 to numerical with new column\ndf['v1_nm'] = df.v1.map({'ham':0, 'spam':1})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# interactive plotly hist plot for numerical vi_nm columns(i,e ham and spam)\ndf['v1_nm'].iplot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a new column with message length using v2 column\ndf['v2_le'] = df.v2.apply(len)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram plot for spam and ham labels with respeect to message length\nplt.figure(figsize=(12,8))\ndf[df['v1']=='ham'].v2_le.plot(bins = 50, kind= 'hist', color='blue', label='ham', alpha=0.75)\ndf[df['v1']=='spam'].v2_le.plot(bins=50, kind= 'hist', color='red', label = 'spam', alpha=0.75)\nplt.legend()\nplt.xlabel('Message length')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the ham for some numerical insights\ndf[df['v1']=='ham'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the spam some numerical insights\ndf[df['v1']=='spam'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the both numerical columns\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see in describe we have 910 word message, let's look at it\ndf[df['v2_le']==910].v2.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Pre-processing\n\nOur main issue with our data is that it is all in text format (strings). The classification algorithms that we usally use need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the bag-of-words approach, where each unique word in a text will be represented by one number.\n\nIn this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n\nAs a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here.\n\nLet's create a function that will process the string in the message column, then we can just use apply() in pandas do process all the text in the DataFrame.\n\nFirst removing punctuation. We can just take advantage of Python's built-in string library to get a quick list of all the possible punctuation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string \nfrom nltk.corpus import stopwords\n\ndef text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    \n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n    \n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's \"tokenize\" these messages. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens (words that we actually want).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_msg'] = df.v2.apply(text_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nwords = df[df['v1']=='ham'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\nham_words = Counter()\n\nfor msg in words:\n    ham_words.update(msg)\n    \nprint(ham_words.most_common(50))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = df[df.v1=='spam'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])\nspam_words = Counter()\n\nfor msg in words:\n    spam_words.update(msg)\n    \nprint(spam_words.most_common(50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization\n\nCurrently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.\n\nNow we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n\nWe'll do that in three steps using the bag-of-words model:\n\n*    Count how many times does a word occur in each message (Known as term frequency)\n*    Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n*    Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\nLet's begin the first step:\n\nEach vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's CountVectorizer. This model will convert a collection of text documents to a matrix of token counts.\n\nWe can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\nX = df.clean_msg\ny = df.v1_nm\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split X and y into training and testing sets \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of arguments and parameters that can be passed to the CountVectorizer. In this case we will just specify the analyzer to be our own previously defined function:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)\n\n\n\n# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)\n\n# examine the document-term matrix\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building and evaluating a model\n\nWe will use multinomial Naive Bayes:\n\n*    The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import and instantiate a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time nb.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for false positives (ham incorrectly classifier)\n# X_test[(y_pred_class==1) & (y_test==0)]\nX_test[y_pred_class > y_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for false negatives (spam incorrectly classifier)\nX_test[y_pred_class < y_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of false negative \nX_test[4949]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm (poorly calibrated)\ny_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('bow', CountVectorizer()), \n                 ('tfid', TfidfTransformer()),  \n                 ('model', MultinomialNB())])\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing models\n\nWe will compare multinomial Naive Bayes with logistic regression:\n\n*    Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import an instantiate a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model using X_train_dtm\n%time logreg.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm (well calibrated)\ny_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning the vectorizer\n\nThus far, we have been using the default parameters of CountVectorizer:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# show default parameters for CountVectorizer\nvect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n\n*    stop_words: string {'english'}, list, or None (default)\n*        If 'english', a built-in stop word list for English is used.\n*        If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n*        If None, no stop words will be used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove English stop words\nvect = CountVectorizer(stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*    ngram_range: tuple (min_n, max_n), default=(1, 1)\n*        The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n*        All values of n such that min_n <= n <= max_n will be used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# include 1-grams and 2-grams\nvect = CountVectorizer(ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*    max_df: float in range [0.0, 1.0] or int, default=1.0\n*        When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n*        If float, the parameter represents a proportion of documents.\n*        If integer, the parameter represents an absolute count.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignore terms that appear in more than 50% of the documents\nvect = CountVectorizer(max_df=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*    min_df: float in range [0.0, 1.0] or int, default=1\n*        When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n*        If float, the parameter represents a proportion of documents.\n*        If integer, the parameter represents an absolute count.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# only keep terms that appear in at least 2 documents\nvect = CountVectorizer(min_df=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*    Guidelines for tuning CountVectorizer:\n*        Use your knowledge of the problem and the text, and your understanding of the tuning parameters, to help you decide what parameters to tune and how to tune them.\n*        Experiment, and let the data tell you the best approach!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}