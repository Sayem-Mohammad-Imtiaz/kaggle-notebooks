{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"The US border crossing dataset contains information of the inbound crossings at the U.S.-Canada and the U.S.-Mexico borders, thus reflecting the number of vehicles, containers, passengers or pedestrians entering the United States. Not data for outbound crossing is provided. \n\nThis is a fairly easy dataset, very nice for exercising some plotting and pandas skills for beginers. Also, I tried some modelling and forecasting using an ARIMA method.\n\n<b>NOTE:</b> This is an ongoing project. <b> I very much appreciate your feedback </b> :)"},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"Let's first load the data and take a look at it..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib                  # 2D Plotting Library\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as mgrid\nimport seaborn as sns              # Python Data Visualization Library based on matplotlib\n\nimport calendar \n\n### Plotly for interactive plots\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/border-crossing-entry-data/Border_Crossing_Entry_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.info())\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it seems that each row consists essentially in a counting (column \"Value\") for the \"crossing method\" (column \"Measure\") such as trucks, trains, etc; together with some geographical information. "},{"metadata":{},"cell_type":"markdown","source":"Looking for missing/null values..."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert dates from strings to date format:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dates from strings to date format\ndata['Date'] = pd.to_datetime(data['Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The US has two terrestrial borders, namely with Canada and Mexico. So we expect 2 possible values for 'Border'"},{"metadata":{"trusted":true},"cell_type":"code","source":"borders = data['Border'].unique()\nprint(borders)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What years are included in the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"years = data['Date'].map(lambda x : x.year).unique()\nyears","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My next guess is that the number of unique elements in \"Port Code\", \"Port Name\" and \"Location\" should coincide. Let's check it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} port names.\".format(len(data['Port Name'].unique())))\nprint(\"There are {} port codes.\".format(len(data['Port Code'].unique())))\nprint(\"There are {} different locations.\".format(len(data['Location'].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They do not! What's going on? "},{"metadata":{"trusted":true},"cell_type":"code","source":"ports = data[['Port Name','Port Code']].drop_duplicates()\nports[ports['Port Name'].duplicated(keep = False)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that Eastport has two different port codes, and that's the source of the discrepancy between the number of codes and the names. Indeed, one corresponds to Eastpot, Idaho, and the other one to Eastport, Maine."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[[29,217]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change it to avoid further confusion,"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data['Port Name'] == 'Eastport') & (data['State'] == 'Idaho'), 'Port Name'] = 'Eastport, ID'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, concerning the locations, I wonder how many locations a port can have... there are almost twice more locations than ports according to the unique elements counting. Also, is it possible that the same location is shared by 2 or more ports?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take the port's unique code and location\nlocs = data[['Port Code','Location']]\n\n# take only unique values\nlocs = locs.drop_duplicates()\nprint(\"There are {} different pairs of port code's and locations\".format(len(locs)))\n# how many locations has each port?\nls = locs['Port Code'].value_counts()\npts = locs['Location'].value_counts()\n\n# Another way of doing the same...\n# ls = locs.groupby(['Port Code']).count()\nprint('')\nprint('Port codes and # of locations:')\nprint(ls.head(10))\nprint('')\nprint('Locations and # of ports:')\nprint(pts.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 229 pairs of ports and locations but only 224 unique locations. So it seems that there are some ports (with different codes and names) sharing the same location."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\nsns.set(style = \"darkgrid\")\n\n# Pie plots\n# This function generates autopct, for displaying both the percent value and the original value.\ndef make_autopct(values):\n    def my_autopct(pct):\n        total = sum(values)\n        val = int(round(pct*total/100.0))\n        return '{p:.2f}%  ({v:d})'.format(p=pct,v=val)\n    return my_autopct\n\nls.value_counts().plot.pie(explode = [0.15,0,0],\n                           autopct = make_autopct(ls.value_counts().values),\n                           ax = ax[0,0])\nax[0,0].set(title = '# of locations per port', ylabel = '')\npts.value_counts().plot.pie(explode = [0.15,0], \n                            autopct = make_autopct(pts.value_counts().values), \n                            ax = ax[1,0])\nax[1,0].set(title = '# of ports per location', ylabel = '')\n\n# Countplots using seaborn\nax[0,1] = sns.countplot(ls, ax=ax[0,1])\nax[0,1].set(title = '# of locations per port', xlabel = '# of locations')\nax[1,1] = sns.countplot(pts, ax=ax[1][1])\nax[1,1].set(title = '# of ports per location', xlabel = '# of ports')\n\nplt.subplots_adjust(\n    wspace =  0.25,     # the amount of width reserved for blank space between subplots\n    hspace = 0.3 # the amount of height reserved for white space between subplots\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the ports have two locations, and most of the locations are unique to a port, excepting 5 of them."},{"metadata":{},"cell_type":"markdown","source":"Which ports are sharing which locations?"},{"metadata":{"trusted":true},"cell_type":"code","source":"rpt = pts[pts.values > 1].index\nrpt_locs = locs[locs['Location'].isin(rpt)]\nrpt_locs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following pairs of ports (by codes) are sharing locations. We see that 3020 (Frontier, Washington) and 3015 (Boundary, Washington) actually share 2 locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"l = rpt_locs.set_index('Location')\npairs = [l.loc[x].values.flatten().tolist() for x in rpt]\nprint(pairs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's one of those shared locations\ndata.iloc[[19267,22492]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature measure indicates the type of counting. Among the 'Measure' types, 'Personal Vehicle Passengers', 'Bus Passengers','Pedestrians' and 'Train Passengers' count people, whereas the others count vehicles."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Measure'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"people = data[data['Measure'].isin(['Personal Vehicle Passengers', 'Bus Passengers','Pedestrians', 'Train Passengers'])]\nvehicles = data[data['Measure'].isin(['Trucks', 'Rail Containers Full','Truck Containers Empty', 'Rail Containers Empty',\n       'Personal Vehicles', 'Buses', 'Truck Containers Full'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### People"},{"metadata":{"trusted":true},"cell_type":"code","source":"people_borders = people[['Border','Value']].groupby('Border').sum()\npeople_borders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = people_borders.values.flatten()\nlabels = people_borders.index\nfig = go.Figure(data=[go.Pie(labels = labels, values=values)])\nfig.update(layout_title_text='Total inbound persons, since 1996')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the values and set the date as index\np = people[['Date','Border','Value']].set_index('Date')\n\n# Group by years and border\np = p.groupby([p.index.year, 'Border']).sum()\np.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_MEX = p.loc(axis=0)[:,'US-Mexico Border'].values.flatten().tolist()\nval_CAN = p.loc(axis=0)[:,'US-Canada Border'].values.flatten().tolist()\nyrs = p.unstack(level=1).index.values\n\n# Bar chart \nfig = go.Figure(go.Bar(x = yrs, y = val_MEX, name='US-Mexico Border'))\nfig.add_trace(go.Bar(x = yrs, y = val_CAN, name='US-Canada Border'))\n\nfig.update_layout(title = 'Total inbounds (people), by border and years', barmode='stack', xaxis={'categoryorder':'category ascending'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Studying the annual growth, by borders:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unstack the Data Frame\nvals = p.unstack().Value\nval_MEX = vals['US-Mexico Border']\nval_CAN = vals['US-Canada Border']\nval_TOT = val_MEX + val_CAN\ngrowth_MEX = val_MEX.diff().dropna()/val_MEX.values[:-1]*100\ngrowth_CAN = val_CAN.diff().dropna()/val_CAN.values[:-1]*100\ngrowth_TOT = val_TOT.diff().dropna()/val_TOT.values[:-1]*100\n\nyrs = vals.index.values\n\n# Bar chart \n# We drop the values for 2019 as there are data only until april\nfig = go.Figure(go.Bar(x = yrs, y = growth_MEX.values[:-1], name='US-Mexico Border'))\nfig.add_trace(go.Bar(x = yrs, y = growth_CAN.values[:-1], name='US-Canada Border'))\nfig.add_trace(go.Line(x = yrs, y = growth_TOT.values[:-1], name='Total'))\n\nfig.update_layout(title = 'Border transit annual growth (people), by border and years', \n                  barmode='group', \n                  xaxis={'categoryorder':'category ascending'},\n                  yaxis=go.layout.YAxis(\n                      title=go.layout.yaxis.Title(\n                      text=\"Annual growth (%)\",\n                      font=dict(                      \n                      size=18,\n                      color=\"#7f7f7f\")\n            \n        )\n    )\n                 \n                 )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How do people cross the borders?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the values and set the date as index\nm = people[['Date','Measure','Value']].set_index('Date')\n\n# Group by years and border\nm = m.groupby([m.index.year,'Measure']).sum()\nm.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar chart \nmeasures = ['Personal Vehicle Passengers', 'Bus Passengers','Pedestrians', 'Train Passengers']\nyrs = m.unstack().index.values\n\nfig = go.Figure(data = [go.Bar(x = yrs, y = m.loc(axis=0)[:, mes].values.flatten().tolist(), name = mes) for mes in measures ])\n    \nfig.update_layout(title = 'Total inbounds (people), by measure and years', barmode='stack', xaxis={'categoryorder':'category ascending'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total # of people crossing the border since 1996, splited by Measure"},{"metadata":{"trusted":true},"cell_type":"code","source":"people_measure = people[['Measure','Value']].groupby('Measure').sum()\nvalues = people_measure.values.flatten()\nlabels = people_measure.index\nfig = go.Figure(data=[go.Pie(labels = labels, values=values)])\nfig.update(layout_title_text='Total inbound persons, since 1996')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do people entering from Mexico and Canada have the same prefered means of transportation for crossing the border?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the values and set the date as index\nmb = people[['Date','Border','Measure','Value']].set_index('Date')\n\n# Group by years and border\nmb = mb.groupby([mb.index.year,'Border','Measure']).sum()\n\n# Bar chart, US-Canada Border\n\nfig = go.Figure(data = [go.Bar(x = yrs, y = mb.loc(axis=0)[:,'US-Canada Border', mes].values.flatten().tolist(), name = mes) for mes in measures ])\nfig.update_layout(title = 'US-Canada inbounds (people), by measure and years', barmode='stack', xaxis={'categoryorder':'category ascending'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar chart, US-Canada Border\n\nfig = go.Figure(data = [go.Bar(x = yrs, y = mb.loc(axis=0)[:,'US-Mexico Border', mes].values.flatten().tolist(), name = mes) for mes in measures ])\nfig.update_layout(title = 'US-Mexico inbounds (people), by measure and years', barmode='stack', xaxis={'categoryorder':'category ascending'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the number of pedestrians crossing the US-Mexico Border seems to be almost constant in time, compared to the number of Personal Vehicle Passengers, which learly sets the trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15, 8)})\nfig,ax = plt.subplots()\nmb.loc(axis=0)[:,'US-Mexico Border', :].unstack().Value.plot(title='US-Mexico Border inbound crossings',ax=ax)\nfig.tight_layout()\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Does it also happen in Canada?"},{"metadata":{"trusted":true},"cell_type":"code","source":"mb.loc(axis=0)[:,'US-Canada Border', :].unstack().Value.plot(title='US-Canada Border inbound crossings')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avarage for each measure for the last 5 years, and  its contribution to the total:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the values and set the date as index\n\nstart_year = 2014\nend_year = 2018\n\nm = people[['Date','Border','Measure','Value']].set_index('Date')\n\n# Group by years and measure\nm = m.groupby([m.index.year,'Border', 'Measure']).sum()\n\nm_can = m.loc(axis=0)[start_year:end_year,'US-Canada Border'].groupby('Measure').mean()\nm_mex = m.loc(axis=0)[start_year:end_year,'US-Mexico Border'].groupby('Measure').mean()\n\n# plotting, pie charts\nf,ax = plt.subplots(ncols=2, nrows=1)\n\nm_can['Value'].plot.pie( ax = ax[0], autopct = '%1.1f%%')\nm_mex['Value'].plot.pie( ax = ax[1], autopct = '%1.1f%%')\n\nax[0].set(title = 'Canadian border, average from {} to {}'.format(start_year,end_year), ylabel = '')\nax[1].set(title = 'Mexican border, average from {} to {}'.format(start_year,end_year), ylabel = '')\nf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's study the correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inbounds by years and Measure, since 1996\nd = data[['Date','Measure','Value']].set_index('Date')\n\nyear_measure_df = d.pivot_table('Value', index = d.index.year, columns = 'Measure', aggfunc = 'sum')\nyear_measure_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_measure_df.corr().style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Crossings by states:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Incoming people by state and type of vehicle, since 1996\n\nPStateVehicle_df = people.pivot_table('Value', index = 'State', columns = 'Measure', aggfunc = 'sum')\nPStateVehicle_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualazing this data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rest = PStateVehicle_df[PStateVehicle_df.sum(axis=1)  < PStateVehicle_df.sum().sum()*0.04].sum().rename('Rest')\n\nt = PStateVehicle_df[PStateVehicle_df.sum(axis=1)  > PStateVehicle_df.sum().sum()*0.04]\nt = t.append(rest)\n# Sort them by total flux\nt = t.iloc[np.argsort(t.sum(axis=1)).values]\n# Combine Train and bus passagners in \"others\"\nt['Other']=t['Bus Passengers'] + t['Train Passengers']\nt = t.drop(['Bus Passengers', 'Train Passengers'], axis=1)\n\n# Plot\nfig, ax = plt.subplots()\n\nsize = 0.4\n\na= t.sum(axis=1).plot.pie(radius = 1,\n       wedgeprops=dict(width=size+0.23, edgecolor='w'), ax = ax, autopct = '%1.1f%%', pctdistance= 0.8)\n\nb=pd.Series(t.values.flatten()).plot.pie(radius = 1- size,colors = ['#DF867E','#8DC0FB','#A9EE84'],\n       wedgeprops=dict(width=size-0.2, edgecolor='w'), ax=ax, labels = None)\n\nax.set(ylabel=None)\nred_patch = matplotlib.patches.Patch(color='#DF867E', label='Pedestrians')\nblue_patch = matplotlib.patches.Patch(color='#8DC0FB', label='Personal vehicle passengers')\ngreen_patch = matplotlib.patches.Patch(color='#A9EE84', label='Others')\nplt.legend(handles=[blue_patch,red_patch, green_patch], loc='best', bbox_to_anchor=(0.75, 0.5, 0.5, 0.5))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have the shares of states changed with time?"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_year = 2015\nend_year = 2018\n\n# Group by years and states\np_states = people[['Date','State','Value']].set_index('Date')\np_states = p_states.groupby([p_states.index.year, 'State']).sum()\n# Select date range and compute mean\np_states = p_states.loc(axis=0)[start_year:end_year,:].groupby('State').mean()\n# Sort, for nice visualization\np_states = p_states['Value'].sort_values()\n# Take only states with more than 4% of share \nrest = p_states[p_states < p_states.sum()*.04].sum()\np_states = p_states[p_states > p_states.sum()*.04].append(pd.Series({'Rest' : rest}))\n\n# Same for all years:\np_states_tot = people[['State','Value']].groupby('State').sum()\np_states_tot = p_states_tot['Value'].sort_values()\nrest_tot = p_states_tot[p_states_tot < p_states_tot.sum()*.04].sum()\np_states_tot = p_states_tot[p_states_tot > p_states_tot.sum()*.04].append(pd.Series({'Rest' : rest_tot}))\n\n\n# plotting, pie charts\nf,ax = plt.subplots(ncols=2, nrows=1)\n\np_states_tot.plot.pie( ax = ax[0], autopct = '%1.1f%%')\np_states.plot.pie( ax = ax[1], autopct = '%1.1f%%')\n\nax[0].set(title = 'States share (inbound people), since 1996', ylabel = '')\nax[1].set(title = 'States share (inbound people), average from {} to {}'.format(start_year,end_year), ylabel = '')\nf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shares have remained fairly constant, with California gaining some popularity."},{"metadata":{},"cell_type":"markdown","source":"### Geographical Visualization"},{"metadata":{},"cell_type":"markdown","source":"How are the crossings distributed among ports?"},{"metadata":{"trusted":true},"cell_type":"code","source":"p_ports = people[['Port Name','Value']].groupby('Port Name').sum().Value.sort_values(ascending = False)\np_ports.hist()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vast majority of ports have had less than 100M crossings, whereas a very few of them have a lot. Border crossings are concentrated in few ports among the 114 of them. Which are the most transited?"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_p = 10\n\npctg = p_ports.head(num_p).sum()/p_ports.sum()*100\n\nprint('The {} most transited ports are:'.format(num_p))\nprint(p_ports.head(num_p))\nprint('')\nprint ('The {} most transited ports (out of 117) take {:.2f} % of all persons crossings into the US.'.format(num_p, pctg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the total number of persons crossing the border towards the US, including a rolling mean (by years) to see the trend:"},{"metadata":{"trusted":true},"cell_type":"code","source":"p_locs = people[['Location','Value']].groupby('Location').sum().reset_index()\n\n# functions to get coordinates,\ndef get_lon(point) :\n    par = point[7:-1].partition(' ')\n    return float(par[0])\n\ndef get_lat(point) :\n    par = point[7:-1].partition(' ')\n    return float(par[2])\n\n# adding a column with the coordinates to the dataframe\np_locs['lat'] = p_locs.Location.apply(lambda x: get_lat(x))\np_locs['lon'] = p_locs.Location.apply(lambda x: get_lon(x))\n\n# As some locations have 2 ports, we have to take are of it in the labelling,\nps = data[['Port Name','Location']].drop_duplicates().set_index('Location')\np_locs['Ports'] = p_locs.Location.apply(lambda x : ', '.join(ps.loc[x].values.flatten()))\np_locs['text'] = p_locs['Ports'] + '<br>Crossings: ' + (p_locs['Value']/1e6).astype(str)+' million'\n\n\ncolor = \"crimson\"\nscale = 500000\n\nfig = go.Figure()\nfig.add_trace(go.Scattergeo(\n    locationmode = 'USA-states',\n    lon = p_locs['lon'],\n    lat = p_locs['lat'],\n    text = p_locs['text'],\n    marker = dict(\n        size = p_locs['Value']/scale,\n        color = color,\n        line_color='rgb(40,40,40)',\n        line_width=0.5,\n        sizemode = 'area')))\n\nfig.update_layout(\n        title_text = 'US Borders, total inbound persons since 1996<br>(Click legend to toggle traces)',\n        showlegend = False,\n        geo = dict(\n            scope = 'usa',\n            landcolor = 'rgb(217, 217, 217)',\n        )\n    )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling and Forecasting"},{"metadata":{},"cell_type":"markdown","source":"Analyzing the time series:"},{"metadata":{"trusted":true},"cell_type":"code","source":"people_crossing_series = people[['Date','Value']].groupby('Date').sum()\npeople_crossing_series_CAN = people[people['Border'] == 'US-Canada Border'][['Date','Value']].groupby('Date').sum()\npeople_crossing_series_MEX = people[people['Border'] == 'US-Mexico Border'][['Date','Value']].groupby('Date').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15, 8)})\nfig, ax = plt.subplots()\n\n#Define a rolling mean, by years\nrmean = people_crossing_series.rolling(12, center=True).mean()\nrmean_MEX = people_crossing_series_MEX.rolling(12, center=True).mean()\nrmean_CAN = people_crossing_series_CAN.rolling(12, center=True).mean()\n\nax.plot(people_crossing_series,\n       marker='.', linestyle='-', linewidth=1, alpha = 1, label='Total')\nax.plot(rmean,\n       marker=None, linestyle='-', linewidth=1.5, alpha = 0.5, label='Total, rolling mean (years)', color = 'b')\n\nax.plot(people_crossing_series_MEX,\n       marker='.', linestyle='-', linewidth=1, alpha = 1, label='Mexico', color = 'r')\nax.plot(rmean_MEX,\n       marker=None, linestyle='-', linewidth=1.5, alpha = 0.5, label='Mexico, rolling mean (years)', color = 'r')\n\nax.plot(people_crossing_series_CAN,\n       marker='.', linestyle='-', linewidth=1, alpha = 1, label='Canada', color = 'g')\nax.plot(rmean_CAN,\n       marker=None, linestyle='-', linewidth=1.5, alpha = 0.5, label='Canada, rolling mean (years)', color = 'g')\n\nax.set(title = 'Total monthly persons entering in the US, from 1996', xlabel = 'year')\nax.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like something happened in 2002 in the US-Mexican border..."},{"metadata":{},"cell_type":"markdown","source":"Let's analyse a shorter period "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nstart = '2015'\nend = '2018'\n\n\nax.plot(people_crossing_series.loc[start:end],\n       marker='o', linestyle='-', linewidth=0.8, alpha = 1, label='Total', color = 'b')\nax.plot(rmean.loc[start:end],\n       marker=None, linestyle='-', linewidth=1.5, alpha = 0.5, label='Total, rolling mean (years)', color = 'b')\n\nax.plot(people_crossing_series_MEX.loc[start:end],\n       marker='.', linestyle='-', linewidth=0.8, alpha = 0.9, label='Mexico', color = 'r')\nax.plot(rmean_MEX.loc[start:end],\n       marker=None, linestyle='-', linewidth=1.5, alpha = 0.5, label='Mexico, rolling mean (years)', color = 'r')\n\nax.plot(people_crossing_series_CAN.loc[start:end],\n       marker='.', linestyle='-', linewidth=0.8, alpha = 0.9, label='Canada',color = 'g')\nax.plot(rmean_CAN.loc[start:end],\n       marker=None, linestyle='-', linewidth=1.5, alpha = 0.5, label='Canada, rolling mean (years)', color = 'g')\n\nax.set(title = 'Total persons entering in the US, from {} to {}'.format(start, end))\nax.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see the seasonal component, with a period of one year. Minimums take place during the winter, notaly in february, whereas the maximums are in summer, during August and Juy. Is this behaviour the same in both borders?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\n\ngrid = mgrid.GridSpec(nrows=2, ncols=1, height_ratios=[2, 1])\n\nseas = fig.add_subplot(grid[0])\ntrend = fig.add_subplot(grid[1], sharex = seas)\n\nstart = '2015'\nend = '2018'\n\nseas.plot(people_crossing_series.loc[start:end]/people_crossing_series.loc[start:end].sum(),\n       marker='o', linestyle='-', linewidth=0.8, alpha = 1, label='Total', color = 'b')\n\nseas.plot(people_crossing_series_MEX.loc[start:end]/people_crossing_series_MEX.loc[start:end].sum(),\n       marker='.', linestyle='-', linewidth=0.8, alpha = 0.9, label='Mexico', color = 'r')\n\nseas.plot(people_crossing_series_CAN.loc[start:end]/people_crossing_series_CAN.loc[start:end].sum(),\n       marker='.', linestyle='-', linewidth=0.8, alpha = 0.9, label='Canada', color = 'g')\n\nseas.set(title = 'Persons entering in the US, from {} to {}, normalised'.format(start, end),\n      ylabel = 'arbitrary units')\nseas.legend()\n\ntrend.plot(rmean.loc[start:end]/rmean.loc[start:end].sum(),\n       marker='', linestyle='-', linewidth=2, alpha = 1, label='Total', color = 'b')\n\ntrend.plot(rmean_MEX.loc[start:end]/rmean_MEX.loc[start:end].sum(),\n       marker='', linestyle='-', linewidth=2, alpha = 1, label='Mexico', color = 'r')\n\ntrend.plot(rmean_CAN.loc[start:end]/rmean_CAN.loc[start:end].sum(),\n       marker='', linestyle='-', linewidth=2, alpha = 1, label='Canada', color = 'g')\n\ntrend.set(ylabel = ' Trend (arbitrary units)')\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = '2011'\nend = '2018'\npcsm = people_crossing_series.loc[start:end]\n\nfig, ax = plt.subplots(2,figsize = (18,13))\n\nfor i in range(11) :\n    mm = pcsm[pcsm.index.month == i] \n    ax[0].plot(mm, label = calendar.month_abbr[i])\n    ax[1].plot(mm/mm.sum(), label = calendar.month_abbr[i])\n    \nax[0].set(title = 'persons entering the US between {} and {}, total by months'.format(start, end),\n         ylabel = '# people')\nax[1].set(title = 'persons entering the US between {} and {}, trend by months'.format(start, end),\n         ylabel = 'arbitrary units')\nax[0].legend()\nax[1].legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trends look fairly regular and similar for all months"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = '2011'\nend = '2018'\npcsm = people_crossing_series.loc[start:end]\nmonths = [calendar.month_abbr[m] for m in range(1,13)]\nfig, ax = plt.subplots(2,figsize = (18,13))\n\nstart = int(start)\nend = int(end)\n\nfor i in range(start, end) :\n    yy = pcsm[pcsm.index.year == i];\n    yy = yy.set_index(yy.index.month);\n    ax[0].plot(yy\n               , label = i)\n    ax[1].plot(yy/yy.sum()\n               , label = i)\n    \nax[0].set(title = 'persons entering the US between {} and {}, total by years'.format(start, end),\n         ylabel = '# people')\n\nax[1].set(title = 'persons entering the US between {} and {}, seasonal (normalised)'.format(start, end),\n         ylabel = 'arbitrary units')\n\nplt.setp(ax, xticks = range(1,13), xticklabels = months)\nax[0].legend()\nplt.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The seasonality is also fairly regular."},{"metadata":{},"cell_type":"markdown","source":"### Decomposition"},{"metadata":{},"cell_type":"markdown","source":"Seasonal decomposition: We will decompose the time series into its seasonal component, a trend component, and noise (error), as this structure is evident from the plots above. I will use data for the total number of persons entering the US from 2011 onwards, to avoid overfitting in the linear models.\n\nWe can make an additive or a multiplicative decomposition. Let's do both and see which one works better"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\npcsm = people_crossing_series.loc['2011':]\n\n# Multiplicative Decomposition \nres_mul = seasonal_decompose(pcsm, model='multiplicative', extrapolate_trend='freq')\n\n# Additive Decomposition\nres_add = seasonal_decompose(pcsm, model='additive', extrapolate_trend='freq')\n\n# extrapolate_trend='freq' gets rid of NaN values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot\nfig, axes = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(15,8))\n\nres_mul.observed.plot(ax=axes[0,0], legend=False)\naxes[0,0].set_ylabel('Observed')\n\nres_mul.trend.plot(ax=axes[1,0], legend=False)\naxes[1,0].set_ylabel('Trend')\n\nres_mul.seasonal.plot(ax=axes[2,0], legend=False)\naxes[2,0].set_ylabel('Seasonal')\n\nres_mul.resid.plot(ax=axes[3,0], legend=False)\naxes[3,0].set_ylabel('Residual')\n\nres_add.observed.plot(ax=axes[0,1], legend=False)\naxes[0,1].set_ylabel('Observed')\n\nres_add.trend.plot(ax=axes[1,1], legend=False)\naxes[1,1].set_ylabel('Trend')\n\nres_add.seasonal.plot(ax=axes[2,1], legend=False)\naxes[2,1].set_ylabel('Seasonal')\n\nres_add.resid.plot(ax=axes[3,1], legend=False)\naxes[3,1].set_ylabel('Residual')\n\naxes[0,0].set_title('Multiplicative')\naxes[0,1].set_title('Additive')\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both look nice, with the residuals resembling white noise. Let's take the multiplicative one."},{"metadata":{},"cell_type":"markdown","source":"### ARIMA seasonally adjusted modelling"},{"metadata":{},"cell_type":"markdown","source":"Here I do an ARIMA modelling and forecasting for the time series once the seasonal component is substracted."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deseasonalized data\n\ndes = res_mul.trend * res_mul.resid\ndes.plot(figsize = (15,10))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I'm planning to use a linear regression model for forecasting, we should guarantee that the series is stationary, i.e. the statistical properties (like mean, variance, autocorrelations...) do not vary with time. If there were some autocorrelations, that would mean that the independent variables in our linear regression model are not that independent (bad thing!). By looking at the plot above, clearly our series show non-stationarity.\n\nHere we will check the stationarity of the time series using an ADF test (Augmented Dickey Fuller test). This is the most commonly used test, where the null hypothesis is \"the time series possesses a unit root and is non-stationary\". If the P-Value in the ADH test is less than a given significance level (0.05), we reject the null hypothesis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the stationarity through a ADF test statistics\n\nfrom statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(des.Value.dropna())\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The series is not stationary. Thus, we differenciate until we kill the autocorrelations;"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\n#plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n\n# Original Series\nfig, axes = plt.subplots(3, 2, figsize=(16,10))\n\naxes[0, 0].plot(des.Value)\naxes[0, 0].set_title('Original Series')\nplot_acf(des, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(des.Value.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(des.diff().dropna(), ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(des.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\nplot_acf(des.diff().diff().dropna(), ax=axes[2, 1])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With one differenciation, the autocorrelations are almost away. If we take two differenciations, we see that the first term is strongly anticorrelated, meaning that the series has been over-differenciated. Hence, we can take only one differentiation. Indeed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_diff = adfuller(des.diff().Value.dropna())\nprint('ADF Statistic: %f' % result_diff[0])\nprint('p-value: %f' % result_diff[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking d=1, the number of MA terms (the value of q) can be estimated by looking at the autocorrelation function after one differenciation. There's only 1 term away from the significance line. I will take q=1."},{"metadata":{},"cell_type":"markdown","source":"How many Auto Regressive (AR) terms do we need (what's the vaue of the term p?). Let's inspect the partial autocorrelation functions (PACF). This functions essentially tells as the correlation between the series and its lag, after excluding the contributions from the intermediate lags:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(16,10))\n\naxes[0, 0].plot(des.Value)\naxes[0, 0].set_title('Original Series')\nplot_pacf(des, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(des.Value.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_pacf(des.diff().dropna(), ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(des.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\nplot_pacf(des.diff().diff().dropna(), ax=axes[2, 1])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Difficult to tell... There are three terms that barely cross the significance threshold in the firs differenciation... lets take p=1. Also, we see that the PACF d=1 show a sinusoidal beheavour, indicating that the arima model might be of the form ARIMA(0,1,1)"},{"metadata":{},"cell_type":"markdown","source":"Now I'm going to build the ARIMA model using the statsmodel package."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\n\n# ARIMA(p,d,q) Model \nmodel = ARIMA(des, order=(0,1,1))\nmodel_fit = model.fit(disp=0)\n\n# Print the information of the model\nprint(model_fit.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The p-values of the constant term and the MA term are very small. The residuals should resemble white noise: normally distributed with zero mean and constant variance, and should be uncorrelated. This would mean that we are no leaving information in them, which otherwise would mean that the model is improvable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot residual errors\n\nresiduals = pd.DataFrame(model_fit.resid)\n\nfig, ax = plt.subplots(2,2, figsize=(15,8))\nresiduals.plot(title=\"Residuals\", ax=ax[0,0])\nresiduals.plot(kind='kde', title='Density', ax=ax[0,1])\nplot_acf(model_fit.resid.dropna(), ax=ax[1,0])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks good. Let's plot the actual values versus the fitted ones:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual vs Fitted\nmodel_fit.plot_predict()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to validate the the model, we make a Out-of-Time cross-validation."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(len(des))\nprint(len(des)*.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import acf\n\n# Create Training and Test\ntrain = des[:74]\ntest = des[74:]\n\n# Build Model\nmodel_train = ARIMA(train, order=(0,1,1))  \n# and fit- using MLE method\nfitted_train = model_train.fit(disp=-1)  \n\n# Forecast\nfc, se, conf = fitted_train.forecast(25, alpha=0.05)  # 95% conf\n\n# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(15,8))\nplt.plot(train, label='training')\nplt.plot(test, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now try the automated tool auto_arima() from the pmdarima pakage for automatically searching the best (p,d,q) combination such that minimizes the AIC:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nimport pmdarima as pm\n\nmodel_auto = pm.auto_arima(des, start_p=0, start_q=0,\n                      test='adf',       # use adftest to find optimal 'd'\n                      max_p=4, max_q=4, # maximum p and q\n                      m=12,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=False)\n\nprint(model_auto.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So after a bit of brute force we see that (2, 1, 3) seems to be the best choice, with the lowest AIC and also with p-values below the 0.05 threshold, which is good. However, the AIC is very similar for both (2, 1, 3) and (0,1,1), so I wil use the latter for simplicity. Recall that with (p,d,q) =  (0,1,1) we have a Moving Avarage of order q=1 on the differenciated series y', \n\n$ y_{t}' = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1}$.\n\nAnyway we can also check the residuals again using a nice plot_diagnostics method from the pmdarima package:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_auto.plot_diagnostics(figsize=(15,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Forecast\n\nLastly, we can include now the seasonality to make the final forecast. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# What's the last date in our data?\npeople_crossing_series.tail(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Intervals for forecasting\n\ndate_start = people_crossing_series.tail(1).index[0]\ndate_end = '2020-12-01'\ndate_rng = pd.date_range(start=date_start, end=date_end, freq='MS', closed = 'right') # range for forecasting\nn_forecast = len(date_rng) # number of steps to forecast\n\nseasonal = res_mul.seasonal.loc['2018-01-01':'2018-12-01'].values # seasonal component, we take the 2018 ones, but they are all the same.\ntms = pd.Series(np.tile(seasonal.flatten(),11), index = pd.date_range(start='2019-01-01', end = '2029-12-01', freq='MS'))  # This is just a very long series with the seasonality.\n\ndef make_seasonal(ser) :\n    seasonal_series = ser * tms # Include the seasonality\n    seasonal_series = seasonal_series[~seasonal_series.isnull()] # trim extra values\n    return seasonal_series\n    \n# Forecast\n\nmodel = ARIMA(des, order=(0,1,1))\nmodel_fit = model.fit(disp=0)\n\nfc1, se1, conf1 = model_fit.forecast(n_forecast, alpha = 0.0455)  # 2 sigma Confidence Level (95,55% conf)\nfc2, se2, conf2 = model_fit.forecast(n_forecast, alpha = 0.3173)  # 1 sigma Confidence Level (68,27% conf)\n\n# Make as pandas series \nfc1_series = pd.Series(fc1, index = date_rng)\nlower_series1 = pd.Series(conf1[:, 0], index = date_rng)\nupper_series1 = pd.Series(conf1[:, 1], index = date_rng)\n\n# Include seasonality\nfc1_series, lower_series1, upper_series1 = [make_seasonal(fc1_series), make_seasonal(lower_series1), make_seasonal(upper_series1)]\n\nplt.figure(figsize=(12,5), dpi=100)\n\n#plt.plot(des, label='actual')\n#plt.plot(people_crossing_series, label='actual')\nplt.plot(des * res_mul.seasonal, label='data')\nplt.plot(fc1_series , label='forecast')\n\n# Confidence level intervals\nplt.fill_between(lower_series1.index,lower_series1, upper_series1, \n                 color='k', alpha=.15, label='2$\\sigma$ Confidence level (95%)')\nplt.title('Forecast 2019/20')\nplt.legend(loc='upper left', fontsize=8)\n#plt.ylim(10000000, 30000000)\nplt.xlim('2016', '2021')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the seasonally adjusted forecast with the total:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\n\ngrid = mgrid.GridSpec(nrows=2, ncols=1, height_ratios=[2, 1])\nsns.set(rc={'figure.figsize':(15, 8)})\n\nseas = fig.add_subplot(grid[0])\nseas_adj = fig.add_subplot(grid[1], sharex = seas)\n\nseas.plot(des * res_mul.seasonal, label='data')\nseas.plot(fc1_series , label='forecast')\nseas.fill_between(lower_series1.index,lower_series1, upper_series1, \n                 color='k', alpha=.15, label = '2$\\sigma$ Confidence level (95%)')\n\n# seasonal adjusted:\nseas_adj.plot(des, label='data')\n\nseas_adj.plot(pd.Series(fc1, index = date_rng) , label='forecast')\nseas_adj.fill_between(date_rng,\n                  pd.Series(conf1[:, 0], index = date_rng), \n                  pd.Series(conf1[:, 1], index = date_rng), \n                 color='k', alpha=.15, label = '2$\\sigma$ Confidence level (95%)')\nplt.xlim('2016', '2021')\nseas.set_title('Forecast 2019/20')\nseas.legend()\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}