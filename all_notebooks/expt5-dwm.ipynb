{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\n\n### Handling Missing Values in Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint('There are {} categorical variables\\n'.format(len(categorical)))\nprint('The categorical variables are :', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat1 = [var for var in categorical if df[var].isnull().sum()!=0]\nprint(df[cat1].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical:\n    print(var + ' conatins '+str(len(df[var].unique()))+ \" labels \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Date column into respective 'Year','Month' & 'Day'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n\ndf.drop('Date',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint(\"There are {} categorical variables : \".format(len(categorical)))\nprint(categorical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing the missing categorical values by the most frequent value in respective columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical:\n    df[var].fillna(df[var].mode()[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [var for var in df.columns if df[var].dtype!='O']\nprint(numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num1 = df[numerical].isnull().sum()\nnum1 = num1[num1!=0]\nnum1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing the missing numercial values by the mean of their respective columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in num1.index:\n    col_mean = df[col].mean()\n    df[col].fillna(col_mean,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nnew_df = df\nfor col in categorical:\n    new_df[col] = le.fit_transform(df[col])\ncol_names = new_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling using MinMaxScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nss = MinMaxScaler()\nnew_df = ss.fit_transform(new_df)\nnew_df = pd.DataFrame(new_df,columns = col_names )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new_df.to_csv(\"weatherCleaned.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization\n\n### Heatmap of correlation among the columns of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = new_df.corr()\nplt.figure(figsize=(16,12))\nplt.title('Correlation Heatmap of Rain in Australia Dataset')\nax = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white',cmap='viridis')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = new_df.RainTomorrow\nX = new_df.drop('RainTomorrow',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting into training and testing sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying various classifying algorithms on the training set and predicting the RainTomorrow using training set.\n\n### 1.1 Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\ngnb.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(cross_val_score(gnb,X_train,y_train,cv=3))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations :\n>> ### GaussianNB implements gaussian naive bayes algorithm for classification.\n>> ### It assumes the maximum likelihood of the features to be Gaussian and classifies the dataset accordingly.\n>> ### The confusion matrix depicts that 2861 are False Positives and 6 are False Negatives.\n>> ### Thus, Gaussian Naive Bayes algorithm is able to predict rain tommorrow with accuracy of 94.95%. "},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(max_depth=10, min_samples_split=2,random_state=42)\ndtc.fit(X_train,y_train)\ny_pred = dtc.predict(X_test)\ndtc.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(cross_val_score(dtc,X_train,y_train,cv=3))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations :\n\n>> ### Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. \n>> ### The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n>> ### DecisionTreeClassifier is capable of both binary classification and multiclass classification.\n>> ### The confusion matrix shows that there are 0 FP or FN.\n>> ### Hence, the DecisionTreeClassifier is able to predict rain tomorrow with an impressive accuracy of 100%."},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Support Vector Machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVC(random_state=42)\nsvc.fit(X_train,y_train)\ny_pred = svc.predict(X_test)\nsvc.score(X_test,y_test)\nprint(cross_val_score(svc,X_train,y_train,cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations :\n>> ### Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n>> ###  LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel.\n>> ### This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.\n>> ### From the confusion matrix it is evident that 18 are FP and 2963 are FN.\n>> ### Therby, the LinearSVC is able to predict rain tomorrow with 94.75% accuracy.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=200,max_depth=10, random_state=42)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\nrfc.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(cross_val_score(rfc,X_train,y_train,cv=3))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations :\n>> ### In random forests (RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement.\n>> ### A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n>> ### The confusion matrix depicts that there are only 4 FN.\n>> ### Hence, the RandomForestClassifier is able to predict rain tomorrow with 99.99% accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Comaprison of Various Classifying algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"Naive Bayes\",\"Decision Tree\",\"Linear SVM\",\"Random Forest\",]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(names,results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion :\n\n>> ### The Decison Tree Algorithm outperforms other algorithms in terms of precison,accuracy and recall.\n>> ### Also,LinearSVM is the lowest in terms of accuracy.\n>> ### Gaussian Naive Bayes performs well in case of binary classification.\n>> ### Thus, Random Forest and Decision Trees are best suited for binary classification problems."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}