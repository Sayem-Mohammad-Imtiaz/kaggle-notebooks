{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\n\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nlabels = pd.read_csv('../input/clothing-dataset-full/images.csv')\nlabels.head()\n\nlabels['image'] = labels['image'] + '.jpg'\nlabel_df = labels[['image', 'label']]\nclasses=list(label_df['label'].unique())\nrepl={}\nfor i in range(len(classes)):\n    repl[classes[i]]=i\nfor i in range(len(label_df)):\n    label_df['label'][i]=repl[label_df['label'][i]]\n\nfrom sklearn.model_selection import train_test_split\ntrain_label_df, test_label_df = train_test_split(label_df, test_size=0.10)\ntrain_label_df.to_csv ('./train_csv.csv', index = False, header=True)\ntest_label_df.to_csv ('./test_csv.csv', index = False, header=True)\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport os\nfrom PIL import Image\nimport torch\n\nclass DressDataset(Dataset):\n    def __init__(self, root_dir, annotation_file, transform=None):\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(annotation_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_id = self.annotations.iloc[index, 0]\n        img = Image.open(self.root_dir+ img_id).convert(\"RGB\")\n        y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return (img, y_label)\ntransform = transforms.Compose(\n    [transforms.Resize((1000,1000)),transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 1\n\ntrainset = DressDataset(root_dir='../input/clothing-dataset-full/images_original/', annotation_file='./train_csv.csv', transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = DressDataset(root_dir='../input/clothing-dataset-full/images_original/', annotation_file='./test_csv.csv', transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:31:27.197585Z","iopub.execute_input":"2021-06-22T09:31:27.198242Z","iopub.status.idle":"2021-06-22T09:31:30.210381Z","shell.execute_reply.started":"2021-06-22T09:31:27.19815Z","shell.execute_reply":"2021-06-22T09:31:30.209424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1(torch.nn.Module):\n    def __init__(self):\n        \n        super().__init__()\n        self.weight1 = nn.Parameter(torch.randn(3, 20, 20))\n        self.weight2 = nn.Parameter(torch.randn(3, 20, 20))\n\n    def forward(self, x):\n        \n        batch_size = 1\n        channels = 3\n        image = x # input image\n\n        kh, kw = 20, 20 # kernel size\n        dh, dw = 20, 20 # stride\n\n        # Create conv\n        \n        inputs=x\n        patches = inputs.unfold(2, kh, dh).unfold(3, kw, dw)\n        #print(patches.shape)\n        patches = patches.contiguous().view(batch_size, channels, -1, kh, kw)\n        #print(patches.shape)\n        nb_windows = patches.size(2)\n\n       \n        patches = patches.permute(0, 2, 1, 3, 4)\n        #print(patches.shape)\n        patches = patches.view(-1, channels, kh, kw)\n        #print(patches.shape)\n\n        conv = nn.Conv2d(channels, batch_size, (kh, kw), stride=(dh, dw), bias=False)\n        patches = patches * torch.mul(self.weight1, self.weight2)\n        patches = patches.sum(1)\n        patches=patches.unsqueeze(0)\n        return patches","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:31:35.90436Z","iopub.execute_input":"2021-06-22T09:31:35.904704Z","iopub.status.idle":"2021-06-22T09:31:35.914968Z","shell.execute_reply.started":"2021-06-22T09:31:35.904676Z","shell.execute_reply":"2021-06-22T09:31:35.914149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 =Conv1()\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(250000, 20)\n        \n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = self.fc1(x)\n        return x\n\n\nnet = Net()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:31:43.740867Z","iopub.execute_input":"2021-06-22T09:31:43.741381Z","iopub.status.idle":"2021-06-22T09:31:43.817739Z","shell.execute_reply.started":"2021-06-22T09:31:43.741348Z","shell.execute_reply":"2021-06-22T09:31:43.816869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:31:47.505167Z","iopub.execute_input":"2021-06-22T09:31:47.505713Z","iopub.status.idle":"2021-06-22T09:31:47.511774Z","shell.execute_reply.started":"2021-06-22T09:31:47.505663Z","shell.execute_reply":"2021-06-22T09:31:47.510808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses=[]\nfor epoch in range(6):  \n\n    running_loss = 0.0\n    i=0\n    for inputs, labels in trainloader:\n        \n        optimizer.zero_grad()\n        outputs = net(inputs.float())\n        loss = criterion(outputs,labels.long())\n        loss.backward()\n        optimizer.step()        \n        running_loss += loss.item()\n        i+=1\n        if i % 200 == 0:    \n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 200))\n            running_loss = 0.0\n    with torch.no_grad():\n        val_loss=0\n        for val_inputs,val_labels in testloader:\n            val_outputs = net(val_inputs.float())\n            loss = criterion(val_outputs,val_labels.long())     \n            val_loss += loss.item()\n        losses.append(val_loss)\n        print(\"Validation loss:\",val_loss)\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:31:50.708777Z","iopub.execute_input":"2021-06-22T09:31:50.709153Z","iopub.status.idle":"2021-06-22T11:01:24.792178Z","shell.execute_reply.started":"2021-06-22T09:31:50.709121Z","shell.execute_reply":"2021-06-22T11:01:24.78903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:01:38.68733Z","iopub.execute_input":"2021-06-22T11:01:38.68793Z","iopub.status.idle":"2021-06-22T11:01:38.913817Z","shell.execute_reply.started":"2021-06-22T11:01:38.687863Z","shell.execute_reply":"2021-06-22T11:01:38.912828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(net, \"./demo1.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:01:44.446955Z","iopub.execute_input":"2021-06-22T11:01:44.447341Z","iopub.status.idle":"2021-06-22T11:01:44.49794Z","shell.execute_reply.started":"2021-06-22T11:01:44.447311Z","shell.execute_reply":"2021-06-22T11:01:44.497073Z"},"trusted":true},"execution_count":null,"outputs":[]}]}