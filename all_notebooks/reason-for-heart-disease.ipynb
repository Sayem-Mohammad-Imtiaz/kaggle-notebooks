{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Disease Prediction"},{"metadata":{},"cell_type":"markdown","source":"\n**Import libraries**\n\nLet's first import all the necessary libraries. I'll use numpy and pandas to start with. For visualization, I will use pyplot subpackage of matplotlib, use rcParams to add styling to the plots and rainbow for colors. For implementing Machine Learning models and processing of data, I will use the sklearn library.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For processing the data, I'll import a few libraries. To split the available dataset for testing and training, I'll use the train_test_split method. To scale the features, I am using StandardScaler."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nNext, I'll import all the Machine Learning algorithms I will be using.\n\n   - K Neighbors Classifier\n   - Support Vector Classifier\n   - Decision Tree Classifier\n   - Random Forest Classifier\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Import dataset**\n\nNow that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file dataset.csv. I'll use the pandas read_csv method to read the dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is now loaded into the variable dataset. I'll just take a glimpse of the data using the desribe() and info() methods before I actually start processing and visualizing it."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scale of each feature column is different and quite varied as well. While the maximum for age reaches 77, the maximum of chol (serum cholestoral) is 564."},{"metadata":{},"cell_type":"markdown","source":"\n**Understanding the data**\n\nNow, we can use visualizations to better understand our data and then look at any processing we might want to do.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndef plot_correlation_map( df ):\n    corr = heart.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\nplot_correlation_map( heart )   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive. Next, I'll take a look at the histograms for each variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" heart['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total no 1s are 165 and 0s are 138, which is not 50% of 1s ."},{"metadata":{"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 8,6\nplt.bar(heart['target'].unique(), heart['target'].value_counts(), color = ['red', 'green'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Preprocessing **\n- Need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I'll use the get_dummies method to create dummy columns for categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.get_dummies(heart, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will use the StandardScaler from sklearn to scale my dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"standardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Machine Learning**\n\nI'll now import train_test_split to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**K Neighbors Classifier**\n\nThe classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    knn_classifier.fit(X_train, y_train)\n    knn_scores.append(knn_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have the scores for different neighbor values in the array knn_scores. I'll now plot it and see for which value of K did I get the best scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The score for K Neighbors Classifier is {}% with {} nieghbors.\".format(knn_scores[7]*100, 8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score for K Neighbors Classifier is 87.0% with 8 nieghbors."},{"metadata":{},"cell_type":"markdown","source":"\n**Support Vector Classifier**\n\nThere are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The linear kernel performed the best, being slightly better than rbf kernel.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The score for Support Vector Classifier is {}% with {} kernel.\".format(svc_scores[0]*100, 'linear'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The score for Support Vector Classifier is 83.0% with linear kernel.*\n\n\n\n**Decision Tree Classifier**\n\nHere, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of max_features and see which returns the best accuracy.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    dt_classifier.fit(X_train, y_train)\n    dt_scores.append(dt_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'blue')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model achieved the best accuracy at three values of maximum features, 2, 4 and 18."},{"metadata":{},"cell_type":"markdown","source":"\n**Random Forest Classifier**\n\nNow, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rf_classifier.fit(X_train, y_train)\n    rf_scores.append(rf_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], rf_scores[i])\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for variable no of Estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The maximum score is achieved when the total estimators are 100 or 500."},{"metadata":{},"cell_type":"markdown","source":"\n# Conclusion\n\nIn this Notebook, we have used Machine Learning for  prediction of patient who has  suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. I then applied four Machine Learning algorithms, K Neighbors Classifier, Support Vector Classifier, Decision Tree Classifier and Random Forest Classifier. I varied parameters across each model to improve their scores. In the end, K Neighbors Classifier achieved the highest score of 87% with 8 nearest neighbors.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}