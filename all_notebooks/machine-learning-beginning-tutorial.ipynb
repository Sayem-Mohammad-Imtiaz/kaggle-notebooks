{"cells":[{"metadata":{"_cell_guid":"eccf1404-a4eb-4b95-b17a-52a05a89064a","_uuid":"e9f224c6ef2d0ea11b2e259fdd77fd4980f3cccf"},"cell_type":"markdown","source":"# Machine Learning Crash Course"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed for any analysis needed in the notebook.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns # for pretty visualizations\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Day 1\n\n## What is ML, DS, and AI?\n>  \"When you’ve written the same code 3 times, write a function. When you’ve given the same in-person advice 3 times, write a blog post\" -@drob\n\nAt a high level the difference between data science (DS), machine learning (ML), and artificial intelligence (AI) can be explained in the following 3 points:\n* DS produces **insights**\n* ML produces **predictions**\n* AI produces **actions**\n\nOf course the fields have large overlays, and each successive step relies on the previous in many ways, but these are the large differentiators when comparing and relating the three fields.\n\n\n"},{"metadata":{"_cell_guid":"250c14d5-d410-48f4-8faa-04105b2e8e9d","_uuid":"3cba85c7bfbabf5ae3dc9983e7ad8e3ac832c933"},"cell_type":"markdown","source":"## ML General Process (High Level)\n1. Exploratory Analysis\n    * First, \"get to know\" the data. This step should be quick, efficient, and decisive.\n\n2. Data Cleaning\n    * Then, clean your data to avoid many common pitfalls. Better data beats fancier algorithms. This can include transforming your data for easier use and enhanced performance in algorithms.\n\n3. Feature Engineering\n    * Next, help your algorithms \"focus\" on what's important by creating new features. Incredibly important in differentiating your findings from others and gaining insights via combining interesting new features that can aid with a better solution/confounding variables.\n\n4. Algorithm Selection\n    * Choose the best, most appropriate algorithms without wasting your time. This step can also include ensembling methods and bagging.\n\n5. Model Training\n    * Finally, train your models. This step is pretty formulaic once you've done the first 4."},{"metadata":{"_cell_guid":"01976c44-f667-4d7a-87d3-c8b5146aff15","_uuid":"6f328a66e5e3473b96dbb230646ea7187b65cebf"},"cell_type":"markdown","source":"# Day 2\n\n## Exploratory Analysis\nYou are a commander with scarce resources.\nExploratory analysis is sending your scouts and spies ahead to learn where best to deploy your forces.\n\nExploratory analysis is the essential step of looking at what data we have and really understanding it.\n* You’ll gain valuable hints for Data Cleaning (which can make or break your models).\n* You’ll think of ideas for Feature Engineering (which can take your models from good to great).\n* You’ll get a \"feel\" for the dataset, which will help you communicate results and deliver greater impact.\n\n## Basic Information\n1. How many observations (rows) do you have?\n2. How many features (columns) do you have?\n3. What are the data types? Numerical, categorical, date times?\n4. Do you have a target variable (what you are trying to predict)?\n\n## Let's Display Some Observations\nWe can display some actual observations (rows) in our dataset just to get a feel for what the data looks like.\nThe common methods used to do so are:\n\n\n"},{"metadata":{"_cell_guid":"863983d0-3213-4194-a9d3-900ded177192","_uuid":"17c7b154e42b9a52539ae9a391b2ba9c1a407984","collapsed":true,"trusted":true},"cell_type":"code","source":"# First,\n# Let us import our dataset into a pandas dataframe (like an Excel table with rows and columns)\ndf = pd.read_csv('../input/beers.csv')\n\n# Use the variable df that holds the object containing your data\ndf.head(5) # will print out the first 5 rows\ndf.tail(5) # will print out the 5 last rows","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45edcda1-a7b7-4dab-b874-949ab3f530bc","_uuid":"2cf184fcd1bac085d322e0803c1308fd1c71c0e3"},"cell_type":"markdown","source":"* Notice the tail(5) command is what was outputted from the previous cell (block of code in this notebook), and overrides the head() method.\n\nWe can also use Pandas **slicing** functionality:"},{"metadata":{"_cell_guid":"55e4d202-1429-4b2f-848b-e66ea64c604a","_uuid":"c7233630603645c22a33862d42ae43a242870785","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"df[10:20] # uses pandas slicing to get a specified number of observations","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a32e69d-f2b8-425b-be2b-9106ebbfebb1","_uuid":"fb1169659aadc53d4d2db1accad46a1dc136bab4"},"cell_type":"markdown","source":"## Obtaining subsets of data in Pandas\nLabel based\t= loc\n\nPosition based = iloc"},{"metadata":{"_cell_guid":"9ae79448-1c29-4ac9-87ab-097fae2ef99a","_uuid":"d7a2e6f2b64dff3503ca1e459eb539ce3d8aafa8","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"df.info() # allows us to see information about our variables and their data types","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81d8d6cb-0049-4489-824e-157adabb088b","_uuid":"455051070ea265f0ee2b0fc925f216a1be7c3c62"},"cell_type":"markdown","source":"This is not the stage where we are doing intense analysis. That is for later.\nWe are just getting a feel for the data right now.\n* Do the columns make sense?\n* What types of values are we seeing in the various columns?\n* Are there a lot of missing values we will need to address?\n* Are the values on the right scale, meaning will we need to normalize the data to be on a similiar scale?\n"},{"metadata":{"_cell_guid":"a902c326-879e-4403-90b9-b3bc53978e56","_uuid":"ade2d382cfb93aef84595f4317ce18679ab955eb"},"cell_type":"markdown","source":"# Distributions\nNext we can look at the distributions of our different features.\nTypically we can look at the histograms to see what is going on such as:\n* Unexpected distibutions\n* Outliers that may affect our analysis (some may not make sense and could be candidates of data entry error for example)\n* Features that are binary\n* Boundaries that are not clear or are illogical\n* Measurement errors\n\nStart making notes that stick out and dig deeper into those potential problems. This will come in handy in the Data Cleansing phase of the project."},{"metadata":{"_cell_guid":"32a06cbc-0294-40a9-b8b2-e0a07deb7f2a","_uuid":"24178faff48d3300b6357d6a56f4fa4f098ca1b3","collapsed":true,"trusted":true},"cell_type":"code","source":"sns.set(color_codes=True) # setup seaborn for visualizations\nsns.distplot(df.abv.dropna()) #histogram of 'abv' feature in our dataset\nplt.show() # used to show the histogram in the output of the notebook","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5a81bc9-2661-4b9f-8fbc-baa2978acfe1","_uuid":"804a808a326dca331f78f85ebef5dc2001edb3b8","collapsed":true,"trusted":true},"cell_type":"code","source":"#scatterplot\nsns.set()\ndf.columns\ncols = ['abv', 'ibu']\nsns.pairplot(df[cols].dropna(), size = 3.5) # must use dropna() or fillna() with avg or median depending on outliers\n                                            # in order for sns to plot without errors\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da636221-ab12-44af-a95f-ea913c04824b","_uuid":"12319be570c63310ff2a845c3734fd19b693f749"},"cell_type":"markdown","source":"## Distribution of Categorical Features\nA **class** is a unique value for a feature.\nTo see the dsitribution of categorical features, we can create a barchart of the unique classes within a feature.\nFor instance, we can do a count of each 'style' of beer in this example data set (Cider, Belgian IPA, etc).\n\nWe want to look for sparse classes (very low counts compared to the other classes). Sparse classes can create a major issue of *overfitness*, or they do not affect the model much at all.\n\nWe should take note of sparse classes in this step, which will could lend us a hand in the feature engineering step later on."},{"metadata":{"_cell_guid":"61a340ed-000c-4cc4-856a-2bf6458e601a","_uuid":"ce0756773a3b7d1334e2bd0576aa7ea66ea370b2"},"cell_type":"markdown","source":"## Segmentations\nBox plots allow us to see the interaction between categorical values and numerical values.\n"},{"metadata":{"_cell_guid":"9c0f4cc9-4d78-46eb-8ea8-4f425438bdd8","_uuid":"7a9e2c6f34b11285b48141c38e372b3c0612bd0c","collapsed":true,"trusted":true},"cell_type":"code","source":"segment_ex = sns.boxplot(x=\"style\", y=\"abv\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1529e1e7-861c-47c0-bfd4-f0040fb37bed","_uuid":"5f15c338754cb8f4dd23b886b1bed2fba0b97051"},"cell_type":"markdown","source":"Woah, that is a lot of different beer styles, this does not lend itself to a great segmentation box plot because it is too noisy. We could filter or slice on the styles with the highest Abv values to make it cleaner and if you are looking to purchase beer that will make the party more fun.\n\nLet us just look at 'Abv' though for experimenation."},{"metadata":{"_cell_guid":"f153775e-9d6e-438b-9f51-2039e32354c1","_uuid":"b6f46296738ccb4a77616e0d3293a741627c5b25","collapsed":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(x = df['abv'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"840a1d58-bea1-4313-9cbd-016daeb2c09b","_uuid":"7d3a88726e59a3d32022a04d3f4a90f23e3525a1"},"cell_type":"markdown","source":"We can see the quartiles and median bar for the Abv percentages for each beer in our dataset. It looks like the median Abv% is around 5.7% or so (visually)."},{"metadata":{"_cell_guid":"bb97e1c6-4e43-4b37-bebf-680e29b3ae2c","_uuid":"d4dd4ca6dcb294ed5e64b711689aae597aea81b1"},"cell_type":"markdown","source":"## Correlations\nFirst thing is first. You have probably heard this many times before, but it is good to always reiterate this when dealing with data:\n> Correlation does not equal causation.\n> Correlation != causation.\n\nCorrelation ranges from -1 to 1, with values closer to 0 being less correlated.\n\nNote, from statistics r^2 is our correlation variable that will give us these values.\n\nIf the r^2 is 0.98, this means the two variables are very positively correlated.\nSay we have a dataset of different age groups of children and their heights, the age and height of the individual will be positively correlated, intuitively.\n\nWe can use correlation heatmaps to see correlation relationships between the different variables in our dataset."},{"metadata":{"_cell_guid":"f86e985c-d604-4615-852a-5757c640decc","_uuid":"68d306b10e28363cf29434e83c097076b23cfafc","collapsed":true,"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a17ea23-c056-466c-a8e7-a7a86225ecec","_uuid":"d262444fefe01995c69fec58f8af7b0ab0044113"},"cell_type":"markdown","source":"Wow, there isn't much in this dataset to work with. Next time, we will need to use a better dataset that can offer more insights.\nBut, we will work with what we have for now.\n\nMaybe we want to see what correlates with the alcohol content in the beer. As expected the IBU (International Bitterness Units scale), which is a gauge of beer's bitterness correlates the most with abv. This makes sense since alcohol is inherently bitter in its chemical properties.\n\nQuestions to ask ourselves when looking at correlations between variables:\n* Which features are strongly correlated with the target variable?\n* Are there interesting or unexpected strong correlations between other features?\n\nAgain, our aim is to gain intuition about the data, which will help us throughout the rest of the workflow."},{"metadata":{"_cell_guid":"e7b7b295-be1a-4827-9f74-6e62fe1abd31","_uuid":"ebc3d854dcf15ff44e49f64ff6dff923797fb203"},"cell_type":"markdown","source":"## What about missing data?\nWe can create a table of total missing values and the percent of the missing values within each feature.\n"},{"metadata":{"_cell_guid":"80f18bfa-4d6e-4d84-8cf5-4867b65e0937","_uuid":"923146d2f7bce55f55c18abb8d988d97bcb563b1","collapsed":true,"trusted":true},"cell_type":"code","source":"#missing data\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"764035d1-5c75-4bc8-833f-0d23a4f30e67","_uuid":"287fac4bbafecc247ac8684ad0f15948b1a0272e","collapsed":true,"trusted":true},"cell_type":"code","source":"#dealing with missing data\ndf = df.drop((missing_data[missing_data['Total'] > 1]).index, 1, inplace=True)\ndf = df.drop(df.loc[df['style'].isnull()].index, inplace=True)\ndf.isnull().sum().max() #just checking that there's no missing data missing...","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f1b5917-93a8-4b2b-ba69-01b4868a0515","_uuid":"ce701a43c5dea34e5f57670b4ff3ebbb11ad8ed0","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cad2d969-0575-4f80-80e5-ba7cdf60643e","_uuid":"7eac82e2439f84c52b5a9d4d2a880b2d43d7d060"},"cell_type":"markdown","source":"## Ending the Exploratory Phase\nWe should now have a decent understanding of the dataset, some notes for data cleaning, and possibly some ideas for feature engineering.\nAs we become more advanced in the DS method, further steps can be added for more in depth analysis in this phase."},{"metadata":{"_cell_guid":"7b62e44f-2953-4003-a7ab-6a7daaed4209","_uuid":"e7f204a179aa42a2a347b8d0ee517d756eb52410"},"cell_type":"markdown","source":"# Day 3\n\n## Data Cleansing\n> Garbage in = garbage out\n\nA clean data set with simple algorithms is better than a messy dataset with advanced algorithms. \n\n## Unwanted Observations\nFirst, we need to remove any **duplicates** from our dataset.\nNext, we need to delete any irrelevent observations or even features, if needed. This is when our notes about the sparse classes and other observations from the exploratory phase will come in the clutch. Doing these steps before feature engineering is not only logical, but will save us copious amounts of time and headache.\n\n## Structural Errors\nCheck for typos or inconsistent capitalization (i.e. you have two classes 'Style' and 'style' that should be joined into one class)\nFinally, check for mislabeled classes, i.e. separate classes that should really be the same.\ne.g. If ’N/A’ and ’Not Applicable’ appear as two separate classes, you should combine them.\ne.g. ’IT’ and ’information_technology’ should be a single class.\n\n## Unwanted Outliers\nOutliers are innocent until proven guilty. We should never remove an outlier just because it’s a \"big number.\" That big number could be very informative for your model.\nWe can’t stress this enough: you must have a good reason for removing an outlier, such as suspicious measurements that are unlikely to be real data.\n\n## Missing Data\nMissing data is like missing a puzzle piece. If you drop it, that’s like pretending the puzzle slot isn’t there. If you impute it, that’s like trying to squeeze in a piece from somewhere else in the puzzle.\n\n\"Missingness\" is almost always informative in itself, and you should tell your algorithm if a value was missing.\n\nThe best way to handle missing data for categorical features is to simply label them as ’Missing’!\n* We are essentially adding a new class for the feature.\n* This tells the algorithm that the value was missing.\n* This also gets around the technical requirement for no missing values.\n\nFor missing numeric data, you should flag and fill the values.\n\n* Flag the observation with an indicator variable of missingness.\n* Then, fill the original missing value with 0 just to meet the technical requirement of no missing values.\n\nBy using this technique of flagging and filling, we are essentially allowing the algorithm to estimate the optimal constant for missingness, instead of just filling it in with the mean.\n\n## Conclusion\nAfter properly completing the Data Cleansing step, we should have a robust dataset that avoids many of the most common pitfalls.\n\nThis can really save us from a ton of headaches down the road, so please don't rush this step."},{"metadata":{"_cell_guid":"03b88a7b-4b38-40f4-a840-d65d270d50fe","_uuid":"0f12ad691a944f283c0e3957b0f03d3ee490f573"},"cell_type":"markdown","source":"# Day 4\n\n## Feature Engineering\nTo start, feature engineering is very open-ended. There are literally infinite options for new features to create.\nPlus, you’ll need domain knowledge to add informative features instead of just more noise.\n\nThis is a skill that you’ll develop naturally with time and practice, but you can give yourself a big head-start if you have a framework in place.\nA feature engineering framework simply consists of \"heuristics\" that you can rely on to spark ideas.\n\nIf you're a beginner, heuristics can help you know where to start looking... and if you're experienced, heuristics can help you get unstuck.\n\n## Interaction features\nLet's set aside the beer dataset for a second to illustrate this step.\n\n#### Example (real-estate)\n* Let's say we already had a feature called 'num_schools', i.e. the number of schools within 5 miles of a property.\n* Let's say we also had the feature 'median_school', i.e. the median quality score of those schools.\n* However, we might suspect that what's really important is having many school options, but only if they are good.\n* Well, to capture that interaction, we could simple create a new feature 'school_score' = 'num_schools' x 'median_school'\n\n## Sparse classes\nThere's no formal rule of how many each class needs.\nIt also depends on the size of your dataset and the number of other features you have.\nAs a rule of thumb, we recommend combining classes until each one has at least ~50 observations. As with any \"rule\" of thumb, use this as a guideline (not actually as a rule).\nLet's take a look at the real-estate example:\n\nBefore grouping sparse classes\nTo begin, we can group similar classes. In the chart above, the 'exterior_walls' feature has several classes that are quite similar.\n* We might want to group 'Wood Siding', 'Wood Shingle', and 'Wood' into a single class. In fact, let's just label all of them as 'Wood'.\n\nNext, we can group the remaining sparse classes into a single 'Other' class, even if there's already an 'Other' class.\n* We'd group 'Concrete Block', 'Stucco', 'Masonry', 'Other', and 'Asbestos shingle' into just 'Other'.\n\nAfter combining sparse classes, we have fewer unique classes, but each one has more observations.\nOften, an eyeball test is enough to decide if you want to group certain classes together.\n\n## Dummy Variables\nDummy variables are a set of binary (0 or 1) variables that each represent a single class from a categorical feature.\n\nThe information you represent is exactly the same, but this numeric representation allows you to pass the technical requirements for algorithms.\n\n## Remove unused\nFinally, remove unused or redundant features from the dataset.\n\nUnused features are those that don’t make sense to pass into our machine learning algorithms. Examples include:\n* ID columns\n* Features that wouldn't be available at the time of prediction\n* Other text descriptions\n* Redundant features would typically be those that have been replaced by other features that you’ve added during feature engineering."},{"metadata":{"_cell_guid":"dce8c3c8-9471-4d7c-888d-4a19c5483a20","_uuid":"1feddd557934a5ee594c29d70565c787ea043687"},"cell_type":"markdown","source":"# Day 5\n\n## Algorithm Selection\nOften you can break your problem into regresson, classification, or clustering predictors.\n\n## Flaws of linear regression\nTo introduce the reasoning for some of the advanced algorithms, let's start by discussing basic linear regression. Linear regression models are very common, yet deeply flawed. Although you can sometimes get away with some fast insights using a quick and dirty linear regression model, it is usually not the best model.\n\nSimple linear regression models fit a \"straight line\" (technically a hyperplane depending on the number of features, but it's the same idea). In practice, they rarely perform well. We actually recommend skipping them for most machine learning problems.\n\nTheir main advantage is that they are easy to interpret and understand. However, our goal is not to study the data and write a research report. Our goal is to build a model that can make accurate predictions.\n\nIn this regard, simple linear regression suffers from two major flaws:\n\nIt's prone to overfit with many input features.\nIt cannot easily express non-linear relationships.\nLet's take a look at how we can address the first flaw.\n\n## Regularization\nThis is the first \"advanced\" tactic for improving model performance. It’s considered pretty \"advanced\" in many ML courses, but it’s really pretty easy to understand and implement.\n\nThe first flaw of linear models is that they are prone to be overfit with many input features.\nLet's take an extreme example to illustrate why this happens:\n* Let's say you have 100 observations in your training dataset.\n* Let's say you also have 100 features.\n* If you fit a linear regression model with all of those 100 features, you can perfectly \"memorize\" the training set.\n* Each coefficient would simply memorize one observation. This model would have perfect accuracy on the training data, but perform poorly on unseen data.\n* It hasn’t learned the true underlying patterns; it has only memorized the noise in the training data.\n* Regularization is a technique used to prevent overfitting by artificially penalizing model coefficients.\n* It can discourage large coefficients (by dampening them).\n* It can also remove features entirely (by setting their coefficients to 0).\n* The \"strength\" of the penalty is tunable. (More on this tomorrow...)\n\n## Regularized Regression\nThere are 3 common types of regularized linear regression algorithms:\n\n**LASSO Regression**\nLasso, or LASSO, stands for Least Absolute Shrinkage and Selection Operator.\n* Lasso regression penalizes the absolute size of coefficients.\n* Practically, this leads to coefficients that can be exactly 0.\n* Thus, Lasso offers automatic feature selection because it can completely remove some features.\n* Remember, the \"strength\" of the penalty should be tuned.\n* A stronger penalty leads to more coefficients pushed to zero.\n\n**Ridge Regression**\n* Ridge regression penalizes the squared size of coefficients.\n* Practically, this leads to smaller coefficients, but it doesn't force them to 0.\n* In other words, Ridge offers feature shrinkage.\n* Again, the \"strength\" of the penalty should be tuned.\n* A stronger penalty leads to coefficients pushed closer to zero.\n\n**Elastic Net Regression**\nElastic-Net is a compromise between Lasso and Ridge.\n* Elastic-Net penalizes a mix of both absolute and squared size.\n* The ratio of the two penalty types should be tuned.\n* The overall strength should also be tuned.\n\nOh and in case you’re wondering, there’s no \"best\" type of penalty. It really depends on the dataset and the problem. We recommend trying different algorithms that use a range of penalty strengths as part of the tuning process, which we'll cover in detail tomorrow.\n\n## Decision Trees\nAwesome, we’ve just seen 3 algorithms that can protect linear regression from overfitting. But if you remember, linear regression suffers from two main flaws:\n* It's prone to overfit with many input features.\n* It cannot easily express non-linear relationships.\n\nHow can we address the second flaw?\n\nWell, we need to move away from linear models to do so.... we need to bring in a new category of algorithms.\n\nDecision trees model data as a \"tree\" of hierarchical branches. They make branches until they reach \"leaves\" that represent predictions.\n\nDue to their branching structure, decision trees can easily model nonlinear relationships.\n\n* For example, let's say for Single Family homes, larger lots command higher prices.\n* However, let's say for Apartments, smaller lots command higher prices (i.e. it's a proxy for urban / rural).\n* This reversal of correlation is difficult for linear models to capture unless you explicitly add an interaction term (i.e. you can anticipate it ahead of time).\n* On the other hand, decision trees can capture this relationship naturally.\n\nUnfortunately, decision trees suffer from a major flaw as well. If you allow them to grow limitlessly, they can completely \"memorize\" the training data, just from creating more and more and more branches.\n\n**As a result, individual unconstrained decision trees are very prone to being overfit.**\n\nSo, how can we take advantage of the flexibility of decision trees while preventing them from overfitting the training data?\n\n## Ensembles\nEnsembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are bagging and boosting.\n\n**Bagging**\nBagging attempts to reduce the chance overfitting complex models.\n* It trains a large number of \"strong\" learners in parallel.\n* A strong learner is a model that's relatively unconstrained.\n* Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.\n\n**Boosting**\nBoosting attempts to improve the predictive flexibility of simple models.\n* It trains a large number of \"weak\" learners in sequence.\n* A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n* Each one in the sequence focuses on learning from the mistakes of the one before it.\n* Boosting then combines all the weak learners into a single strong learner.\n\nWhile bagging and boosting are both ensemble methods, they approach the problem from opposite directions. Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n\nEnsembling is a general term, but when the base models are decision trees, they have special names: random forests and boosted trees!\n\n**Random Forests**\nRandom forests train a large number of \"strong\" decision trees and combine their predictions through bagging.\n\nIn addition, there are two sources of \"randomness\" for random forests:\n\n1. Each tree is only allowed to choose from a random subset of features to split on (leading to feature selection).\n2. Each tree is only trained on a random subset of observations (a process called resampling).\nIn practice, random forests tend to perform very well right out of the box.\n\n* They often beat many other models that take up to weeks to develop.\n* They are the perfect \"swiss-army-knife\" algorithm that almost always gets good results.\n* They don’t have many complicated parameters to tune.\n\n**Boosted Trees**\nBoosted trees train a sequence of \"weak\", constrained decision trees and combine their predictions through boosting.\n\n* Each tree is allowed a maximum depth, which should be tuned.\n* Each tree in the sequence tries to correct the prediction errors of the one before it.\nIn practice, boosted trees tend to have the highest performance ceilings.\n\n* They often beat many other types of models after proper tuning.\n* They are more complicated to tune than random forests.\n\n## Conclusion\nWhew, that was a lot! If you need to, feel free to let it sink in a bit and then re-read the lesson.\n\nKey takeaway: The most effective algorithms typically offer a combination of regularization, automatic feature selection, ability to express nonlinear relationships, and/or ensembling. Those algorithms include:\n* Lasso regression\n* Ridge regression\n* Elastic-Net\n* Random forest\n* Boosted tree"},{"metadata":{"_cell_guid":"38e031af-d0eb-4b8f-b970-0e6520013a3a","_uuid":"74d628d6d36f8ddc04b820bf3101a8728151756a"},"cell_type":"markdown","source":"# Day 6\n\n## Building Our Models\nMajority of our time in data science and machine learning is spent on:\n1. Exploring the data.\n2. Cleaning the data.\n3. Engineering new features.\n\nNow, it is time to fasten up our seatbelts and build our models. This is the *fun* part.\n\n## Split dataset\nLet’s start with a crucial but sometimes overlooked step: Spending your data.\n\nThink of your data as a limited resource.\n\n* You can spend some of it to train your model (i.e. feed it to the algorithm).\n* You can spend some of it to evaluate (test) your model.\n* But you can’t reuse the same data for both!\nIf you evaluate your model on the same data you used to train it, your model could be very overfit and you wouldn’t even know! A model should be judged on its ability to predict new, unseen data.\n\nTherefore, you should have separate **training** and **test** subsets of your dataset.\nTraining sets are used to fit and tune your models. Test sets are put aside as \"unseen\" data to evaluate your models.\n* You should always split your data before doing anything else.\n* This is the best way to get reliable estimates of your models’ performance.\n* After splitting your data, don’t touch your test set until you’re ready to choose your final model!\nComparing test vs. training performance allows us to avoid overfitting... If the model performs very well on the training data but poorly on the test data, then it’s overfit.\n\n## Hyperparameters\nSo far, we’ve been casually talking about \"tuning\" models, but now it’s time to treat the topic more formally.\n\nWhen we talk of tuning models, we specifically mean tuning hyperparameters.\n\nThere are two types of parameters in machine learning algorithms.\n\nThe key distinction is that model parameters can be learned directly from the training data while hyperparameters cannot.\n\n**Model parameters**\nModel parameters are learned attributes that define individual models.\n* e.g. regression coefficients\n* e.g. decision tree split locations\nThey can be learned directly from the training data\n\n**Hyperparameters**\nHyperparameters express \"higher-level\" structural settings for algorithms.\n* e.g. strength of the penalty used in regularized regression\n* e.g. the number of trees to include in a random forest\nThey are decided before fitting the model because they can't be learned from the data\n\n## Cross-validation\nNext, it’s time to introduce a concept that will help us tune our models: cross-validation.\n\nCross-validation is a method for getting a reliable estimate of model performance using only your training data.\n\nThere are several ways to cross-validate. The most common one, 10-fold cross-validation, breaks your training data into 10 equal parts (a.k.a. folds), essentially creating 10 miniature train/test splits.\n\nThese are the steps for 10-fold cross-validation:\n1. Split your data into 10 equal parts, or \"folds\".\n2. Train your model on 9 folds (e.g. the first 9 folds).\n3. Evaluate it on the 1 remaining \"hold-out\" fold.\n4. Perform steps (2) and (3) 10 times, each time holding out a different fold.\n5. Average the performance across all 10 hold-out folds.\n\nThe average performance across the 10 hold-out folds is your final performance estimate, also called your cross-validated score. Because you created 10 mini train/test splits, this score is usually pretty reliable.\n\n## Fit and tune models\nNow that we've split our dataset into training and test sets, and we've learned about hyperparameters and cross-validation, we're ready fit and tune our models.\n\nBasically, all we need to do is perform the entire cross-validation loop detailed above on each set of hyperparameter values we'd like to try.\n\nThe high-level pseudo-code looks like this:"},{"metadata":{"_cell_guid":"698abf1a-ce9a-415e-a03b-078057a5130f","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"3e9f2ebc3de9660d32d3f8274bc5ed2fc5f5e7c9","collapsed":true,"trusted":true},"cell_type":"code","source":"# pseudocode for hyperparameter loop using cross validation; IGNORE ERROR\nFor each algorithm (i.e. regularized regression, random forest, etc.):\n  For each set of hyperparameter values to try:\n    Perform cross-validation using the training set.\n    Calculate cross-validated score.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d9ecac8-a58c-4fbf-ac20-4e0d4d3171d0","_uuid":"2c0b876bdd7027b976f062f2a12e08a716d4efe8"},"cell_type":"markdown","source":"At the end of this process, you will have a cross-validated score for each set of hyperparameter values... for each algorithm.\n\nThen, we'll pick the best set of hyperparameters within each algorithm:"},{"metadata":{"_cell_guid":"d17d907b-90ab-420c-a5a2-e7c02656174b","_kg_hide-output":true,"_uuid":"681e9ad917a4abd94fd4def1049d8a165c4037e7","collapsed":true,"trusted":true},"cell_type":"code","source":"# pseudocode; IGNORE ERROR\nFor each algorithm:\n  Keep the set of hyperparameter values with best cross-validated score.\n  Re-train the algorithm on the entire training set (without cross-validation).","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"01dbdacd-d4d4-4685-b489-ae12fe41087d","_uuid":"f0df6bc8adf229bb11ff04d8fbdce3b4159f293b"},"cell_type":"markdown","source":"It's kind of like the Hunger Games... each algorithm sends its own \"representatives\" (i.e. model trained on the best set of hyperparameter values) to the final selection.\n\n## Select winner\nBy now, you'll have 1 \"best\" model for each algorithm that has been tuned through cross-validation. Most importantly, you've only used the training data so far.\n\nNow it’s time to evaluate each model and pick the best one, a la Hunger Games style.\n\nBecause you've saved your test set as a truly unseen dataset, you can now use it get a reliable estimate of each models' performance.\n\nThere are a variety of performance metrics you could choose from. We won't spend too much time on them here, but in general:\n* For regression tasks, we recommend Mean Squared Error (MSE) or Mean Absolute Error (MAE). (Lower values are better)\n* For classification tasks, we recommend Area Under ROC Curve (AUROC). (Higher values are better)\n\nThe process is very straightforward:\n* For each of your models, make predictions on your test set.\n* Calculate performance metrics using those predictions and the \"ground truth\" target variable from the test set.\n\nFinally, use these questions to help you pick the winning model:\n* Which model had the best performance on the test set? (performance)\n* Does it perform well across various performance metrics? (robustness)\n* Did it also have (one of) the best cross-validated scores from the training set? (consistency)\n* Does it solve the original business problem? (win condition)"},{"metadata":{"_cell_guid":"04d3d49f-f649-4c46-8b92-b9af4eb43dcf","_uuid":"f7a52f270ae973700b984c57b3f47c6310769cf2"},"cell_type":"markdown","source":"# Day 7\n\n**Recap**\n* On day 1, you saw a bird's-eye view of the entire machine learning workflow.\n* Then, on day 2, you learned our framework for fast, efficient, and decisive exploratory analysis.\n* Day 3 was all about data cleaning, which is perhaps the most important step of all!\n* Next, on day 4, we shared our favorite heuristics for feature engineering.\n* On day 5, we discussed regularization and ensembles, and you learned about 5 algorithms that leverage those mechanisms.\n* And yesterday on day 6, we walked through a proven formula for training excellent models after the other steps have been completed correctly.\n\nNow, we will talk about next steps in our DS/ML education.\n\nStrike while the iron is hot! Go on and tackle a problem with a different dataset while its fresh in our head.\nMaybe split it up in 7 days like this tutorial, one section per day. We will do the same methods, but since it is a different, unique dataset and problem, we will most definitely learn something different in approach and in the details.\n\nIt is my recommendation (although everyone learns differently) to skip the textbooks and jump into projects ASAP because it's much faster to learn in context, i.e. \"learning by doing.\"\n\nPlus, it will be easier to stay motivated and continue progressing.\n\nOnwards!\n\n"},{"metadata":{"_cell_guid":"8037d0db-007b-4f38-ba69-6422f578a379","_uuid":"c081c05ca7103effadf8308c611bbd7d41939567"},"cell_type":"markdown","source":"Sources:\n* [1] http://varianceexplained.org/r/ds-ml-ai/\n* [2] https://tryolabs.com/blog/2017/03/16/pandas-seaborn-a-guide-to-handle-visualize-data-elegantly/\n* [3] https://elitedatascience.com/data-cleaning"},{"metadata":{"_cell_guid":"6e967390-a045-4ced-aa50-dc58e981a9e3","_uuid":"c69b583244a6437726576461bac3d145e23eb3a4","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}