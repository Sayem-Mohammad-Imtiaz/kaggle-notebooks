{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BOOM BIKES\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Author : Diprodeep Ghosh","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Background:**\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Problem Statement:**\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demand\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Business Goal:**\nWe are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import preprocessing\nfrom IPython.core.interactiveshell import InteractiveShell\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n# from collections import defaultdicta\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# READ DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# READ DATA\n\nbikedata = pd.read_csv(\"../input/boombikes/day.csv\",parse_dates=['dteday']) \nprint(bikedata.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape check \nprint(bikedata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  descriptive information check\n\nprint(bikedata.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#descriptive  statistical information check\n\nprint(bikedata.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA QUALITY CHECK","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## NULL/MISSING values checking :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of missing values in each column\n\nround(100*(bikedata.isnull().sum()/len(bikedata.index)), 2).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" we can see all the percentage value is zero so there is no missing or NULL value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Duplicate data Checking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbike_duplicate = bikedata\n\n# Checking for duplicates and dropping the entire duplicate row if any\nbike_duplicate.drop_duplicates(subset=None, inplace=True)\nbike_duplicate.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After running the drop duplicate command the shape of the dataframe is same as that of the original.So we can say that there is no duplicate value in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing Unwanted Columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From a high level analysis of the data dictionary we can conclude that the columns namely instant, dteday, casual & registered can be droped from the dataset.Following are the reasons why:\n\n1.instant : its only an index value \n\n2.dteday: we already have a seperate coloumn for month and year so we dont need date seperately.\n\n3.casual & registered : cotains count of bike with different category.since our count will not be specific to any category so we dont need it.\n\nI will be creating a new dataframe named bikedata_new which will have the dataframe with the droped coloumns,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new=bikedata[['season', 'yr', 'mnth', 'holiday', 'weekday',\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n       'cnt']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Dummy Variable","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Will create dummy variable for the following coloumns:\n\n1.'mnth'\n\n2.'weekday'\n\n3.'season' \n\n4.'weathersit'       ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the datatype to category\nbikedata_new['season']=bikedata_new['season'].astype('category')\nbikedata_new['weathersit']=bikedata_new['weathersit'].astype('category')\nbikedata_new['mnth']=bikedata_new['mnth'].astype('category')\nbikedata_new['weekday']=bikedata_new['weekday'].astype('category')\nbikedata_new.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#creating the dummy variables\n#using drop_first to drop the first variable for each set of dummies created\n\nbikedata_new = pd.get_dummies(bikedata_new, drop_first=True)\n\nbikedata_new.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Splitting\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will split the entire data set into training set and testing set in 70:30 ration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(bikedata_new, train_size = 0.70, test_size = 0.30, random_state = 333)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking out training set info\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking out training set size\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking out testing set info\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking out testing set size\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so the data has been sucessfully split into 70% train data and 30% test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA on Training Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###  Numeric Variables  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbikedata_num=df_train[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']] #taking only numerical variable\n\nsns.pairplot(bikedata_num, diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the above pair plot we can see a LINEAR relationship between temp,atemp and cnt","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Categorical Variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will create boxplot for each of the categorical\nvariable to see how it stacks up with the target variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking categorical variables before creating dummy variables\n\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bikedata)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bikedata)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bikedata)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bikedata)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bikedata)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bikedata)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following are the analysis from the above boxplots - \n\nseason: Season 3 has the highest nbr of booking with a median close to 5000 closely followed by season 2 and season 3. This indicates, season can be a good predictor for the dependent variable. - \n\nmnth: months 5,6,7,8 & 9 have majority bike booking with a median of hovering around 4000 booking per month. This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable. \n\nweathersit: Majority of the booking is happening during ‘weathersit1 with a median of close to 5000 booking (for the period of 2 years). This was followed by weathersit2 .This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable. \n\nholiday: Almost majority of the bike booking were happening when it is not a holiday which means this data is clearly biased so it will not be a good predictor for the dependent variable.\n\nweekday: weekday variable shows very close trend having their independent medians between 4000 to 5000 bookings. \n\nworkingday: greater no.of bike booking were happening in ‘workingday’ with a median of close to 5000 booking ","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25,20))\nax=sns.heatmap(bikedata_new.corr(), annot = True, cmap=\"YlGnBu\")\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RESCALING ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply scaler()\n\nnumerical_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_train[numerical_vars] = scaler.fit_transform(df_train[numerical_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking values after scaling\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LINEAR MODEL","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dividing the training dataset into X and Y sets for the model building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('cnt')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive feature elimination:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#VIF CHECK \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n# Add a constant\nX_train_lm1 = sm.add_constant(X_train_rfe)\n\n# Create a first fitted model\nlr1 = sm.OLS(y_train, X_train_lm1).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameter check\n\nlr1.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model Summary\nprint(lr1.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the variable 'atemp' based on its High p-value & High VIF -","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_rfe.drop([\"atemp\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr2 = sm.OLS(y_train, X_train_lm2).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the parameters obtained\n\nlr2.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the variable 'hum' based on its Very High 'VIF' value. - choosing 'hum' over 'temp' because based on general knowledge we can say temp can have effects on businessess like bike rental","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"hum\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr3 = sm.OLS(y_train, X_train_lm3).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr3.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr3.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the variable 'season3' based on its Very High 'VIF' value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# MODEL 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"season_3\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm4 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr4 = sm.OLS(y_train, X_train_lm4).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr4.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr4.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the variable 'mnth_10' based on its Very High p-value campared to others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# MODEL 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"mnth_10\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm5 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr5 = sm.OLS(y_train, X_train_lm5).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr5.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr5.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the variable 'mnth_3' based on its High 'p-value' caomparing with others","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# MODEL 6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"mnth_3\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm6 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr6 = sm.OLS(y_train, X_train_lm6).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr6.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr6.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a model in which we have all the P-values as zero and also all the VIFs are less than 5 depicting very low multicollinearity between the predictors.So we can consider this as our final model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Interpretation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Final Model Interpretation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Hypothesis Testing:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"H0 : B1 = B2 = ......... = Bn = 0\n\nH1 : at least one Bi != 0    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr6.params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the list above it can be seen that none of the coefficient are equal to zero which means we can reject the null hypothesis.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Significance of the final model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The overall Significance of the final model is determined by the F-statistics value (higher the value greater the significance of the model).From the above summary of lr6 model we can see that it has :\n\nF-statistic:          233.8\n\nThe F-Statistics value of 233 (which is greater than 1) states that the overall model is significant\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Assumptions Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Residual Analysis Of Training Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_predict = lr6.predict(X_train_lm6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual = y_train-y_train_predict\n\n\nfig = plt.figure()\nsns.distplot((residual), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)  \nplt.xlabel('Errors', fontsize = 18)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### From the normally distributed residuals we can assume that the linear regression is valid","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear relationship between X and Y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new=bikedata_new[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bikedata_num, diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the pair plot, we could see there is a linear relation between temp and atemp variable with the predictor ‘cnt’.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Multicollinearity between the predictor variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### As no VIF value is above 5 so we can assume that there is no multicollinearity between the pradictor variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Applying the Final Model(lr6) to make Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying scaling\n\nnumerical_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_test[numerical_vars] = scaler.transform(df_test[numerical_vars])\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dividing into X_test and y_test\n\ny_test = df_test.pop('cnt')\nX_test = df_test\n\nX_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the variables that are part of final model.\ncol1=X_train_new.columns\n\nX_test=X_test[col1]\n\n# Adding constant variable to test dataframe\nX_test_lm6 = sm.add_constant(X_test)\n\nX_test_lm6.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions using the final model (lr6)\n\ny_predict = lr6.predict(X_test_lm6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL EVALUATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n\nfig = plt.figure()\nplt.scatter(y_test, y_predict, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R^2 Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#r2 = 1-(RSS/TSS)\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adjusted R^2  TEST","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r2=0.8203092200749708","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n is number of rows in X\n\nn = X_test.shape[0]\n\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\nadjusted_r2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FINAL RESULT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### FINAL RESULT COMPARISON: \n\n##### Train R^2 :0.824  \n\n##### Train Adjusted R^2 :0.821  \n\n##### Test R^2 :0.820  \n\n##### Test Adjusted R^2 :0.812","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# The equation of best fitted surface based on model lr6","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"cnt=0.084143+(yr×0.230846)+(workingday×0.043203)+(temp×0.563615)−(windspeed×0.155191)+(season2×0.082706)+(season4×0.128744)+(mnth9×0.094743)+(weekday6×0.056909)−(weathersit2×0.074807)−(weathersit3×0.306992)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}