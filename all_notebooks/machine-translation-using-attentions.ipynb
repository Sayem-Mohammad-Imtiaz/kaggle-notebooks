{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Machine Translation Using Attention:**"},{"metadata":{},"cell_type":"markdown","source":"# English to Hindi Translation: "},{"metadata":{},"cell_type":"markdown","source":"## **Data Preparation:**"},{"metadata":{},"cell_type":"markdown","source":"## **Import all libraries:**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport tensorflow as tf\n\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/english-to-hindi-parallel-dataset/newdata.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('Unnamed: 0',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.dropna()\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=int(input())\n\nen=data['english_sentence'].values[n]\n\nhi=data['hindi_sentence'].values[n]\n\nprint(en)\n\nprint(hi)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning The Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\nsc = list(set(string.punctuation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing special charcaters\n\ndata['english_sentence']=data['english_sentence'].apply(lambda x: x.lower())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['english_sentence']=data['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in sc))\ndata['hindi_sentence']=data['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in sc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['english_sentence']=data['english_sentence'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\ndata['hindi_sentence']=data['hindi_sentence'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['english_sentence']=data['english_sentence'].apply(lambda x: '<start> '+x+' <end>')\ndata['hindi_sentence']=data['hindi_sentence'].apply(lambda x: '<start> '+x+' <end>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['length_eng_sentence']=data['english_sentence'].apply(lambda x:len(x.split(\" \")))\ndata['length_hin_sentence']=data['hindi_sentence'].apply(lambda x:len(x.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fliter the values based upon length of sentences:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data[data['length_eng_sentence']<=20]\ndata=data[data['length_hin_sentence']<=20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n=int(input())\n\nen=data['english_sentence'].values[n]\n\nhi=data['hindi_sentence'].values[n]\n\nprint(en)\n\nprint(hi)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Data"},{"metadata":{},"cell_type":"markdown","source":"* combine all words\n* sort the words based upon frequency \n* assign the ranks of the words based upon frequency\n* convert the text sentence into list of tokens\n* padding the token's list"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter \ndef tokenize(lang):\n    words=[]\n    for i in lang:\n        words.extend(i.split())\n    s=Counter(words)\n    a=list(s.keys())\n    b=list(s.values())\n    ind=np.argsort(np.array(b))\n    word_to_ind={}\n    for i in range(len(ind)):\n        word_to_ind[a[ind[-(i+1)]]]=i+1\n    sequences=[]\n    for i in lang:\n        sen=[]\n        for j in i.split():\n            sen.append(word_to_ind[j])\n        sequences.append(sen)\n    pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences,padding='post')\n    \n    return word_to_ind,pad_sequences\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_word_to_ind,en_sequences=tokenize(data['english_sentence'].values)\nhin_word_to_ind,hin_sequences=tokenize(data['hindi_sentence'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(en_word_to_ind),len(hin_word_to_ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_sequences.shape,hin_sequences.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_sequences[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split The data into train and validation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(en_sequences,hin_sequences, test_size=0.2)\n\n\nprint(len(x_train), len(y_train), len(x_val), len(y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# shuffle data and use Data Generators:"},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = len(x_train)\nBATCH_SIZE = 128\nsteps_per_epoch = len(x_train)//BATCH_SIZE\nembedding_dim = 256\nunits = 512\nvocab_inp_size = len(en_word_to_ind)+1\nvocab_tar_size = len(hin_word_to_ind)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Attention MOdel:"},{"metadata":{},"cell_type":"markdown","source":"# Enoder of Attention Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Attention Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n\n        query_with_time_axis = tf.expand_dims(query, 1)\n\n        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n\n\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decoder of Attention Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n        self.attention = BahdanauAttention(self.dec_units)\n\n    def call(self, x, hidden, enc_output):\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n\n        x = self.embedding(x)\n\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n        output, state = self.gru(x)\n\n        \n        output = tf.reshape(output, (-1, output.shape[2]))\n\n        x = self.fc(output)\n\n        return x, state, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Optimizer and Loss Function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define checkpoint to store the Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = '/kaggle/working/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n        dec_hidden = enc_hidden\n\n        dec_input = tf.expand_dims([hin_word_to_ind['<start>']] * BATCH_SIZE, 1)\n\n        for t in range(1, targ.shape[1]):\n           \n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n            loss += loss_function(targ[:, t], predictions)\n\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss / int(targ.shape[1]))\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n\n    if (epoch + 1) % 10 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Of the Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hin_ind_to_word={}\n\nfor i in hin_word_to_ind:\n    hin_ind_to_word[hin_word_to_ind[i]]=i\n    \nen_ind_to_word={}\n\nfor i in en_word_to_ind:\n    en_ind_to_word[en_word_to_ind[i]]=i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_sentence(sentence):\n    x=sentence.lower()\n    x=''.join(ch for ch in x if ch not in sc)\n    x=''.join([i for i in x if not i.isdigit()])\n    x='<start> '+x+' <end>'\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(sentence):\n    attention_plot = np.zeros((20, 20))\n\n    sentence = preprocess_sentence(sentence)\n   \n    inputs = [en_word_to_ind[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=20,padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, 512))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([hin_word_to_ind['<start>']], 0)\n\n    for t in range(20):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        \n\n        if hin_ind_to_word[predicted_id] == '<end>':\n            return result, sentence, attention_plot\n        result += hin_ind_to_word[predicted_id] + ' '\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    k=int(input())\n    sentence=''\n    for j in range(1,len(x_val[k])-1):\n        if  x_val[k][j+1]==0:\n            continue\n        sentence+=en_ind_to_word[x_val[k][j]]+' '\n    \n    pred,x,atten_plot=evaluate(sentence.strip())\n    actual=''\n    for j in range(1,len(y_val[k])-1):\n        if  x_val[k][j+1]==0:\n            continue\n        \n        actual+=' '+hin_ind_to_word[y_val[k][j]]\n    x=' '.join([j for j in x.split()[1:-1]])       \n    print(\"english sentence---> \"+x)\n    print('\\n')\n    print('predicted sentence--->'+pred)\n    print('\\n')\n    print('actual sentence-->'+actual)\n    print('\\n')\n    print('--------------------------------------')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}