{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Aprendizado Supervisionado\n\nJaime Hikaru Mishima\n\n## Small Business Administration (SBA) Historical Loans (7a and 504)\nOver 1.6M loans guaranteed by the SBA to small businesses as part of their 7(a) and 504 Loan programs. Dataset includes including the name, address, and industry of the recipient as well as the loan amount, term current status and issuing bank details.\n\n7(a) Loans: SBA 7(a) loans are for a maximum of 2 million dollars, with SBA loan guarantee of no more than 1.5 million dollars (75%).\n\n504 Loans: The 504 Loan Program provides approved small businesses with long-term, fixed-rate financing used to acquire fixed assets for expansion or modernization. 504 loans are made available through Certified Development Companies (CDCs), SBA's community based partners for providing 504 Loans.\n\n[Base Location](https://aws.amazon.com/marketplace/pp/prodview-27syih2hg4av2?qid=1587175655831&sr=0-25&ref_=srh_res_product_title#overview)\n<br>\n[Source](https://www.sba.gov/about-sba/open-government/foia#section-header-32)\n"},{"metadata":{},"cell_type":"markdown","source":"## Previsão de Default em Empréstimos\n**Contexto**: A Small Business Administration (SBA) é uma agência norte-americana que apoia as pequenas empresas. A SBA conecta empreendedores com credores e financiamento para ajudá-los a planejar, iniciar e expandir seus negócios. Com uma base histórica de empréstimos feitos desde 2009, a ideia é prever o não pagamento de acordo com variáveis de negócio."},{"metadata":{},"cell_type":"markdown","source":"## 0. Importar Bibliotecas"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom matplotlib import pyplot as plt\nimport fklearn, matplotlib\nfrom tqdm import tqdm\n\nfrom sklearn.feature_selection import SelectKBest, chi2 \nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom fklearn.preprocessing.splitting import time_split_dataset\nfrom fklearn.preprocessing.splitting import space_time_split_dataset\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom itertools import groupby\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from HelperMethods import HelperClass\n# https://github.com/jaimemishima/Data-Science-Projects/blob/master/Credit%20Card%20Fraud%20Detection.ipynb\nclass HelperClass(object):\n        \n    # helper method\n    @staticmethod\n    def stars():\n        print (\"***********************\")\n    \n    # print metrics as dataframe\n    @staticmethod\n    def print_dataframe(values):\n    \n        metrics_print = ['True Positive', 'True Negative', 'False Negative', 'False Positive',\n    'Accuracy', 'Precision', 'Recall', 'Specificity', 'F1 Score', 'Roc Auc Score']\n\n        df_metrics = pd.DataFrame(\n            {'Metrics': metrics_print,\n             'Values': values\n            })\n\n        print (df_metrics.to_string(header=False, index=False))\n        HelperClass.stars()\n        \n                \n        \n    # Disclaimer: metodo obtido em:\n    # http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    # Metodo para plotar a matrix de confusao\n    @staticmethod\n    def plot_confusion_matrix(cm, classes,\n                              normalize=False,\n                              title='Confusion matrix',\n                              cmap=plt.cm.Blues):\n        \"\"\"\n        This function prints and plots the confusion matrix.\n        Normalization can be applied by setting `normalize=True`.\n        \"\"\"\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix:\")\n        else:\n            print('Confusion matrix, without normalization:')\n\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() / 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, format(cm[i, j], fmt),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n        plt.tight_layout()\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        \n        # adjust plot\n        \n        bottom, top = plt.gca().get_ylim()\n        plt.gca().set_ylim(bottom + 0.5, top - 0.5)\n        plt.show()\n\n\n\n\n    # ROC curve\n    @staticmethod\n    def plot_roc_curve(y_true, y_scores):\n\n        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n\n        HelperClass.stars()\n        print (\"Roc Curve:\")\n        HelperClass.stars()\n\n        plt.plot(fpr, tpr, label = 'ROC Curve', linewidth = 2)\n        plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n        plt.title('ROC Curve')\n        plt.xlim([0.0, 0.001])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.show()\n\n\n\n    # Precision Recall Curve\n    @staticmethod\n    def plot_precision_recall_curve(y_true, y_scores):\n\n        HelperClass.stars()\n        print (\"Precision Recall Curve:\")\n        HelperClass.stars()\n\n        precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n\n        plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n\n        plt.plot(recall, precision, linewidth=2)\n        plt.xlim([0.0,1.0])\n        plt.ylim([0.0,1.05])\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision Recall Curve')\n        plt.show()\n\n\n\n    # Show classification report\n    @staticmethod\n    def show_full_classification_report(y_true, y_pred, y_scores, classes):\n\n        HelperClass.stars()\n        print (\"Metrics Report:\")\n        HelperClass.stars()\n\n        cm = confusion_matrix(y_true, y_pred)\n\n        true_positive = cm[1,1]\n        true_negative = cm[0,0]\n        false_negative = cm[1,0]\n        false_positive = cm[0,1]\n\n        accuracy = ((true_positive + true_negative)/(true_positive + true_negative + false_negative + false_positive))\n        precision = (true_positive/(true_positive + false_positive))\n        recall = (true_positive/(true_positive + false_negative))  \n        sensitivity = (true_positive/(true_positive + false_negative))  \n        specificity = (true_negative/(true_negative + false_positive))  \n        f1_score = ((2 * precision * recall)/(precision + recall))\n        \n        roc_auc = roc_auc_score(y_true, y_scores)\n\n        print (\"Classification Report:\")\n        HelperClass.stars()\n\n        values = []\n\n        values.append(true_positive)\n        values.append(true_negative)\n        values.append(false_negative)\n        values.append(false_positive)\n        values.append('{:.4f}'.format(accuracy))\n        values.append('{:.4f}'.format(precision))\n        values.append('{:.4f}'.format(recall))\n        values.append('{:.4f}'.format(specificity))\n        values.append('{:.4f}'.format(f1_score))\n        values.append('{:.4f}'.format(roc_auc))\n\n        HelperClass.print_dataframe(values)\n\n        HelperClass.plot_confusion_matrix(cm, classes)\n\n        HelperClass.plot_roc_curve(y_true, y_scores)\n\n        HelperClass.plot_precision_recall_curve(y_true, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None) # para mostrar todas as colunas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Tratamento da Base"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw = pd.read_excel(\"FOIA - 7(a)(FY2010-Present).xlsx\", sheet_name=\"7A_FY2010_Present\")\n# raw.to_csv(\"foia_7a.csv\",index = False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_base = pd.read_csv(\"foia_7a.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = raw_base.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AsOfDate'] = df['AsOfDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\ndf['ApprovalDate'] = df['ApprovalDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\ndf['FirstDisbursementDate'] = df['FirstDisbursementDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\ndf['PaidInFullDate'] = df['PaidInFullDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\ndf['Target'] = np.where(df['PaidInFullDate'].isnull(), 1, 0)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['LoanStatus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vamos manter somente os emprestimos que foram totalmente pagos (PIF = Paid in Full) e cobrados (CHGOFF = Charged Off)\n# COMMIT = não desembolsado, CANCLD = cancelado, EXEMPT = foram desembolsados mas não foram cancelados, cobrados ou pagos.\ndf = df[df['LoanStatus'].isin(['PIF', 'CHGOFF'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"foia_7a_tratado.csv\",index = False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"foia_7a_tratado.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df['AsOfDate'] = df['AsOfDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\ndf['ApprovalDate'] = df['ApprovalDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\ndf['FirstDisbursementDate'] = df['FirstDisbursementDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))\n#df['PaidInFullDate'] = df['PaidInFullDate'].apply(lambda x: pd.to_datetime(str(x), format='%Y-%m-%d'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing unnused columns\ndf = df.drop(columns=['AsOfDate', 'Program', 'BorrName', 'BorrStreet', 'BankStreet', 'LoanStatus'])\n\n# removing variables related with target\ndf = df.drop(columns=['PaidInFullDate', 'ChargeOffDate', 'GrossChargeOffAmount'])\n\n# converting categorical columns\ndf['NaicsCode'] = df['NaicsCode'].astype(str)\ndf['CongressionalDistrict'] = df['CongressionalDistrict'].astype(str)\ndf['BorrZip'] = df['BorrZip'].astype(str)\ndf['BankZip'] = df['BankZip'].astype(str)\n\ndf['Id'] = range(1, 1+len(df))\ndados = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dados.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Análise Exploratória"},{"metadata":{},"cell_type":"markdown","source":"Análise exploratória serve para desvendar um pouco do processo gerador de dados. Quanto mais souber sobre como os dados são gerados, melhor será as chances de usar a técnica de modelagem mais adequada. Algumas análises são padrões em todos os processos de análise exploratória:\n\n1. Análise de distribuição. Ex: qual a média, mínimo e máximo de cada variável\n2. Análise de dados faltantes. Ex: Quais variáveis têm dados faltantes? Qual a proporção de dados faltantes por variável? Porque esses dados estão faltando? É um bug ou uma característica do processo gerador de dados?\n3. Variáveis categóricas. Existem variáveis categóricas? Qual a cardinalidade das variáveis categóricas? As categorias são ordenadas ou sem ordem?\n4. Correlação. Qual a correlação entre variáveis?\n5. Censura: Quais as limitações do target? Há algum processo de espera temporal entre a observação das variáveis e a observação do target (em problemas de previsão, a resposta geralmente é sim!). Como é esse processo. Há alguma outra variável que limite o target?"},{"metadata":{"trusted":true},"cell_type":"code","source":"dados.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Análise de distribuição\nObservamos alguns valores estranhos, por exemplo `TermInMonths` com máximo de `360`."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndados.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Valores Nulos\nPodemos ver que `FirstDisbursementDate`, `FranchiseCode`, `FranchiseName`,  tem valores nulos. Para a franquia, somente 7% da base tem o campo preenchido.\n\nPodemos ver que ~8% da base não pagou o empréstimo (`PaidInFullDate` está nulo ou `ChargeOffDate` está preenchido)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dados.isna().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Cardinalidade\nSexo, PerfilEconomico, RegiaodoPais, PerfilCompra tem baixa cardinalidade"},{"metadata":{"trusted":true},"cell_type":"code","source":"dados.apply(pd.Series.nunique).sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5 Correlação\nQueremos ver quais as variáveis que estão mais relacionadas com o target de alguma forma.\n\n*TermsInMonths* parece ser uma variável bem forte, seguido de *InitialInterestRate*."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'Target'\ndados.corr(method='pearson')[target].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"É importante olhar como as features estão relacionadas entre si. \n\nPor exemplo, não queremos usar duas features muito correlacionadas no modelo ao mesmo tempo."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nax = sns.heatmap(dados.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=0.5, linecolor='black')\n\n# rotate xlabel\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n\nax.axes.set_title(\"Matriz de Correlação\",fontsize=20)\n#ax.set_xlabel(\"X Label\",fontsize=30)\n#ax.set_ylabel(\"Y Label\",fontsize=20)\nax.tick_params(labelsize=10)\n\n# adjust plot\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Tratamento dos Dados\nImputação, dummies. Geracao da ABT de modelagem"},{"metadata":{"trusted":true},"cell_type":"code","source":"dados.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_volume = dados['ApprovalDate'].groupby(dados[\"ApprovalDate\"].dt.year).count().to_frame()#.plot(kind=\"bar\")\nyear_volume = year_volume.rename(columns={\"ApprovalDate\": \"Count Loans\"})\nyear_volume['Share Loans'] = year_volume['Count Loans'].div(year_volume['Count Loans'].sum(), axis=0).multiply(100)\nyear_volume","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nax = sns.barplot(x=year_volume.index, y='Count Loans',data=year_volume, dodge=False)\nax.axes.set_title(\"Volume de Loans anual\",fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dados['ApprovalDate'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dados['ApprovalDate'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_fn = space_time_split_dataset(train_start_date=\"2009-10-01\",\n                                    train_end_date=\"2015-12-31\",\n                                    holdout_end_date=\"2019-09-30\",\n                                    split_seed=42,\n                                    space_holdout_percentage=0.2,\n                                    space_column=\"Id\",\n                                    time_column=\"ApprovalDate\")\n\ndados_amostral = dados.sample(n = 10000)\ntrain_set, out_of_space_ho, out_of_time_ho, out_of_space_time_ho =  split_fn(dados_amostral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_set.shape)\nprint(out_of_space_ho.shape)\nprint(out_of_time_ho.shape)\nprint(out_of_space_time_ho.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Imputação"},{"metadata":{},"cell_type":"markdown","source":"Imputação de mediana nas variáveis numéricas (no caso serve apenas para ValorCompraAnual).\n\nImputação de `unk` (para unknown) para as variáveis categóricas (não se aplica para essa base)."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_variables = dados.describe().transpose().index.tolist()\ncategorical_variables = list(set(dados.columns.tolist()) - set(numerical_variables))\nnumerical_variables.remove('Target')\nnumerical_variables.remove('Id')\nprint('Variveis numericas: ',numerical_variables)\nprint('Variveis categoricas: ',categorical_variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fklearn.training.imputation import imputer, placeholder_imputer\nfrom toolz import compose\n\nnum_impute_learner = imputer(columns_to_impute=numerical_variables,\n                             impute_strategy=\"median\")\n\ncat_impute_learner = placeholder_imputer(columns_to_impute=categorical_variables,\n                                          placeholder_value=\"unk\")\n\n#tupla que retorna: [0] a funcao, [1] dataset e [2] log\nnum_impute_fn, _, num_impute_log = num_impute_learner(train_set)\ncat_impute_fn, _, cat_impute_log = cat_impute_learner(train_set)\n\ncompose(num_impute_fn, cat_impute_fn)(train_set).isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Tratamento Outliers, Dummies e Label encoder\nAplica-se um **capper** para valores acima de um certo valor e o **foorer** para aplicar um threshold inferior. Não se aplica na base.\n\n**One hot encoder** para variáveis categóricas com baixa cardinalidade.\n\n**Label encoder** para variáveis categóricas com alta cardinalidade."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndados.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fklearn.training.transformation import capper, floorer\nfrom fklearn.training.transformation import label_categorizer\nfrom fklearn.training.transformation import onehot_categorizer\n\n# Capping altos\ncapper_fn = capper(columns_to_cap=['GrossApproval', 'SBAGuaranteedApproval'], \n                   precomputed_caps={'GrossApproval': 5000000,\n                                     'SBAGuaranteedApproval': 5250000\n                                    })\n\n# One hot encoding (cria dummies)\ncategorical_features_onehot = ['BusinessType', 'DeliveryMethod', 'subpgmdesc']\noh_encode_learner = onehot_categorizer(columns_to_categorize=categorical_features_onehot,\n                                       hardcode_nans=False, # hardcodes an extra column with 1 if nan or unseen else 0\n                                       drop_first_column=True)\n\n# Label encoding\ncategorical_features_label_encoding = list(set(categorical_variables) - set(categorical_features_onehot) \n                                           - set(['ApprovalDate']))\nle_encode_learner = label_categorizer(\n                                columns_to_categorize=categorical_features_label_encoding,\n                                store_mapping=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fklearn.training.pipeline import build_pipeline\n\npipeline_learner = build_pipeline(\n    capper_fn,\n    num_impute_learner,\n    cat_impute_learner,\n    oh_encode_learner,\n    le_encode_learner\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Using the created pipeline I transform my data\n_, pre_processed_data, _ = pipeline_learner(dados)\n#_, pre_processed_data, _ = pipeline_learner(dados_amostral)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, out_of_space_ho, out_of_time_ho, out_of_space_time_ho =  split_fn(pre_processed_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Métodos de Seleção\nFiltro, wrapper, embedded"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols = ['Target', 'fklearn_feat__PerfilEconomico==3', 'fklearn_feat__Sexo==mulher', 'fklearn_feat__PerfilCompra==8','fklearn_feat__RegiaodoPais==Região Sul']\n# train_df_copy = train_df\n# test_df_copy = test_df\n# for col in cols:\n#     if col in train_df_copy.columns:\n#         train_df_copy = train_df_copy.drop(columns=col, axis=1)\n#     if col in test_df_copy.columns:\n#         test_df_copy = test_df_copy.drop(columns=col, axis=1)\n\nexplicativas_train = train_set[list(set(train_set) - set(['ApprovalFiscalYear', 'ApprovalFiscalYear', 'ApprovalDate', 'FirstDisbursementDate', 'Target']))]\ntarget_train = train_set['Target']\n\nexplicativas_test = out_of_time_ho[list(set(out_of_time_ho) - set(['ApprovalFiscalYear', 'ApprovalFiscalYear', 'ApprovalDate', 'FirstDisbursementDate', 'Target']))]\ntarget_test = out_of_time_ho['Target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Baseado em Filtro"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\nx_norm = MinMaxScaler().fit_transform(explicativas_train)\n\n# chamada do objeto\nchi2_selector = SelectKBest(chi2)\n\nchi2_selector.fit(x_norm, target_train)\n\nchi_s = chi2_selector.get_support()\n\nchi_feature = explicativas_train.loc[:,chi_s].columns.tolist()\nprint(str(len(chi_feature)), 'variaveis selecionadas')\nprint(chi_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Wrapper"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# chamada do objeto\nrfe_selector = RFE(estimator=LogisticRegression(), step=10) #default is half, n_features_to_select=10)\n\nrfe_selector.fit(explicativas_train, target_train)\nrfe_support = rfe_selector.get_support()\nrfe_feature = explicativas_train.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'variaveis selecionadas:')\nprint(rfe_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nem_selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\nem_selector.fit(explicativas_train, target_train)\n\nem_sup = em_selector.get_support()\nem_feature = explicativas_train.loc[:,em_sup].columns.tolist()\nprint(str(len(em_feature)), 'variaveis selecionadas:')\nprint(em_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Comparando Filtro, Wrapper e Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_selection_df = pd.DataFrame({'Variaveis':explicativas_train.columns,\n                                     'chi2':chi_s,\n                                     'RFE': rfe_support,\n                                     'Random forest': em_sup})\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\nfeature_selection_df = feature_selection_df.sort_values(['Total', 'Variaveis'], ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_selection_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Selecionando features que apareceram em pelo menos dois métodos de seleção de variáveis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"var_select = feature_selection_df[feature_selection_df['Total'] > 1]['Variaveis'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_treino = explicativas_train[var_select]\ny_treino = target_train\n\nx_teste = explicativas_test[var_select]\ny_teste = target_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Modelagem usando Gridsearch e Cross Validation Kfold: Regressao Logística, Random Forest e Gradient Boosting)"},{"metadata":{},"cell_type":"markdown","source":"### 6.1.1 Regressão Logística"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nRL = LogisticRegression(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dicionário de hyperparâmetros para o **GridSearch** da Regressão Logística"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters = {\"C\":np.logspace(-3,3,7), \n#                    \"penalty\":[\"l1\",\"l2\"]} # l1 lasso l2 ridge\n\nhyperparameters = {'penalty' : ['l1','l2'], \n                   'class_weight' : ['balanced', None], \n                   'C' : [0.001, 0.01, 0.1, 1, 10]\n                  }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngrid_RL = GridSearchCV(RL, \n                  hyperparameters, \n                  cv=10,\n                  verbose=0)\ngrid_RL.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_RL.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passo os melhores parametros do Gridsearch como argumentos na LogisticRegression():"},{"metadata":{"trusted":true},"cell_type":"code","source":"RL = LogisticRegression(**grid_RL.best_params_, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation** com o melhor modelo dado pelo GridSearch acima:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nacuracias_RL_treino = cross_val_score(estimator=RL,\n                            X = x_treino,\n                            y = y_treino,\n                            cv=10)\n\nacuracias_RL_teste = cross_val_score(estimator=RL,\n                            X = x_teste,\n                            y = y_teste,\n                            cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('REGRESSAO LOGISTICA - Acuracia de Treino:',round(acuracias_RL_treino.mean()*100,2))\nprint('REGRESSAO LOGISTICA - Acuracia de Teste:',round(acuracias_RL_teste.mean()*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['Pago','Default']\ny_pred = grid_RL.predict(x_teste)\ny_scores_RL = grid_RL.predict_proba(x_teste)[:,1]\n\nHelperClass.show_full_classification_report(y_teste, y_pred, y_scores_RL, classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.2 K-nearest neighbors (KNN)"},{"metadata":{},"cell_type":"markdown","source":"Dicionário de hyperparâmetros para o **GridSearch** do KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"parametros_grid = {'n_neighbors': [3, 5, 7],\n                   'weights': ['uniform', 'distance']\n                  }\n\nKNN = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_KNN = GridSearchCV(estimator=KNN,\n                    param_grid=parametros_grid,\n                    scoring='recall',\n                    cv=10)\ngrid_KNN.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_KNN.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passo os melhores parâmetros do Gridsearch como argumentos na KNeighborsClassifier():"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nKNN = KNeighborsClassifier(**grid_KNN.best_params_)\nKNN.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation** com o melhor modelo dado pelo GridSearch acima:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import cross_val_score\nacuracias_KNN_treino = cross_val_score(estimator=KNN,\n                            X = x_treino,\n                            y = y_treino,\n                            cv=10)\n\nacuracias_KNN_teste = cross_val_score(estimator=KNN,\n                            X = x_teste,\n                            y = y_teste,\n                            cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RANDOM FOREST - Acuracia de Treino:',round(acuracias_KNN_treino.mean()*100,2))\nprint('RANDOM FOREST - Acuracia de Teste:',round(acuracias_KNN_teste.mean()*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['Pago','Default']\ny_pred = grid_KNN.predict(x_teste)\ny_scores_KNN = grid_KNN.predict_proba(x_teste)[:,1]\n\nHelperClass.show_full_classification_report(y_teste, y_pred, y_scores_KNN, classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.3 Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Dicionário de hyperparâmetros para o **GridSearch** da Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nparametros_grid = {\n    'n_estimators':[5,10,100],\n    'criterion':['gini', 'entropy'],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_features': [2],\n    'bootstrap':[True, False],\n    'min_samples_leaf': [2,3],\n    'max_depth':[5]\n}\n\nRF = RandomForestClassifier(random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_RF = GridSearchCV(estimator=RF,\n                    param_grid=parametros_grid,\n                    scoring='accuracy',\n                    cv=5)\ngrid_RF.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_RF.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_RF.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passo os melhores parâmetros do Gridsearch como argumentos na RandomForestClassifier():"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nRF = RandomForestClassifier(**grid_RF.best_params_, random_state=42)\nRF.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation** com o melhor modelo dado pelo GridSearch acima:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import cross_val_score\nacuracias_RF_treino = cross_val_score(estimator=RF,\n                            X = x_treino,\n                            y = y_treino,\n                            cv=10)\n\nacuracias_RF_teste = cross_val_score(estimator=RF,\n                            X = x_teste,\n                            y = y_teste,\n                            cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RANDOM FOREST - Acuracia de Treino:',round(acuracias_RF_treino.mean()*100,2))\nprint('RANDOM FOREST - Acuracia de Teste:',round(acuracias_RF_teste.mean()*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['Pago','Default']\ny_pred = grid_RF.predict(x_teste)\ny_scores_RF = grid_RF.predict_proba(x_teste)[:,1]\n\nHelperClass.show_full_classification_report(y_teste, y_pred, y_scores_RF, classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.4 Soft Voting Classifier: Combina 3 classificadores em 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclasses = ['Pago','Default']\nvoting_clf = VotingClassifier (\n        estimators = [('lg', grid_RL), ('knn', grid_KNN), ('rf', grid_RF)], voting='soft')\n    \nvoting_clf.fit(x_treino, y_treino)\n\ny_pred = voting_clf.predict(x_teste)\ny_scores_VotingClassifier = voting_clf.predict_proba(x_teste)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HelperClass.show_full_classification_report(y_teste, y_pred, y_scores_VotingClassifier, classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.5 Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nGB = GradientBoostingClassifier(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dicionário de hyperparâmetros para o **GridSearch** do Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"parametros_gb = {'min_samples_split': [3, 5],\n                 'min_samples_leaf': [3, 5],\n                 'max_depth': [3,5],\n                 'n_estimators':[2,5],\n                 'loss':['deviance'],\n                 'learning_rate': [0.05, 0.2],\n                 'max_features':[\"log2\",\"sqrt\"],\n                 #'criterion': [\"friedman_mse\",  \"mae\"],\n                 #'subsample':[0.5,0.8, 1.0]\n                }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\n# assinatura do objeto\ngrid_GB = GridSearchCV(estimator=GB,\n                       param_grid=parametros_gb,\n                       cv=5,\n                       n_jobs=-1) # tenta rodar em paralelo, se possível\ngrid_GB.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GB.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GB.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passo os melhores parâmetros do Gridsearch como argumentos na GradientBoostingClassifier():"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nGB = GradientBoostingClassifier(**grid_GB.best_params_, random_state=42)\nGB.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation** com o melhor modelo dado pelo GridSearch acima:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import cross_val_score\nacuracias_GB_treino = cross_val_score(estimator=GB,\n                            X = x_treino,\n                            y = y_treino,\n                            cv=5)\n\nacuracias_GB_teste = cross_val_score(estimator=GB,\n                            X = x_teste,\n                            y = y_teste,\n                            cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GRADIENT BOOSTING - Acuracia de Treino:',round(acuracias_GB_treino.mean()*100,2))\nprint('GRADIENT BOOSTING - Acuracia de Teste:',round(acuracias_GB_teste.mean()*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['Pago','Default']\ny_pred = grid_GB.predict(x_teste)\ny_scores_GB = grid_GB.predict_proba(x_teste)[:,1]\n\nHelperClass.show_full_classification_report(y_teste, y_pred, y_scores_GB, classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.6 XGBoosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# from sklearn.metrics import roc_auc_score, log_loss\n# # An experiment to understand why weak learners work better:\n# max_depth = np.unique(np.random.randint(3, 15, size=7))\n# num_estimators = np.unique(np.concatenate((np.random.randint(3, 15, size=7), np.random.randint(15, 100, size=5)),axis=0))\n\n# auc = pd.DataFrame()\n\n# for depth in max_depth:\n#     for tot_trees in num_estimators:\n        \n#         xgb_ = XGBClassifier(max_depth=depth, num_estimators=tot_trees)\n#         xgb_.fit(x_treino, y_treino)\n        \n#         test_xgb  = xgb_.predict_proba(x_teste)[:,1]\n#         auc_test  = roc_auc_score(y_teste.values, test_xgb)\n        \n\n#         auc = pd.concat([auc, pd.DataFrame(data={'AUC': [auc_test], 'max_depth':[depth] , 'num_estimators':[tot_trees]})], axis=0)\n#         print(auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nXGB = XGBClassifier(random_state=42)\n\n#brute force scan for all parameters, here are the tricks\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have \n#much fun of fighting against overfit \n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\nparameters_xgb = {#'nthread':[4], #when use hyperthread, xgboost may become slower\n                  'objective':['binary:logistic'],\n                  'learning_rate': [0.05,0.1], #so called `eta` value\n                  'max_depth': [2],\n                  'min_child_weight': [11],\n                  #'silent': [1],\n                  #'subsample': [0.8],\n                  #'colsample_bytree': [0.7],\n                  'n_estimators': [1000], #number of trees, change it to 1000 for better results\n                  #'missing':[-999],\n                  #'seed': [1337],\n                  #'lambda':[1.2, 1.3],\n                  #'alpha':[1.2, 1.3],\n\n                 }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\n# assinatura do objeto\ngrid_XGB = GridSearchCV(estimator=XGB,\n                       param_grid=parameters_xgb,\n                       cv=5,\n                       n_jobs=-1) # tenta rodar em paralelo, se possível\ngrid_XGB.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_XGB.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passo os melhores parâmetros do Gridsearch como argumentos na XGBClassifier():"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nXGB = XGBClassifier(**grid_XGB.best_params_, random_state=42)\nXGB.fit(x_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation** com o melhor modelo dado pelo GridSearch acima:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import cross_val_score\nacuracias_XGB_treino = cross_val_score(estimator=XGB,\n                            X = x_treino,\n                            y = y_treino,\n                            cv=5)\n\nacuracias_XGB_teste = cross_val_score(estimator=XGB,\n                            X = x_teste,\n                            y = y_teste,\n                            cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GRADIENT BOOSTING - Acuracia de Treino:',round(acuracias_XGB_treino.mean()*100,2))\nprint('GRADIENT BOOSTING - Acuracia de Teste:',round(acuracias_XGB_teste.mean()*100,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['Pago','Default']\ny_pred = grid_XGB.predict(x_teste)\ny_scores_XGB = grid_XGB.predict_proba(x_teste)[:,1]\n\nHelperClass.show_full_classification_report(y_teste, y_pred, y_scores_XGB, classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1.4 Comparando Modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelos = pd.DataFrame({'Modelo': ['Regressao Logistica',\n                                   'Random Forest',\n                                   'Gradient Boosting',\n                                   'XGboosting'\n                                  ],\n                        'Acuracia_treino':[round(acuracias_RL_treino.mean()*100,2), \n                                           round(acuracias_RF_treino.mean()*100,2),\n                                           round(acuracias_GB_treino.mean()*100,2),\n                                           round(acuracias_XGB_treino.mean()*100,2)],\n                        'Acuracia_teste':[round(acuracias_RL_teste.mean()*100,2), \n                                          round(acuracias_RF_teste.mean()*100,2),\n                                          round(acuracias_GB_teste.mean()*100,2),\n                                          round(acuracias_XGB_teste.mean()*100,2)]\n                       })\n\ncomparacao = modelos.sort_values(by='Acuracia_teste', ascending=False)\ncomparacao = comparacao[['Modelo', 'Acuracia_treino', 'Acuracia_teste']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparacao","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Salvando Modelo"},{"metadata":{},"cell_type":"markdown","source":"Melhores estimadores do melhor modelo (com maior acurácia foi o Gradient Boosting):"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_GB.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.externals import joblib\n\n# salva modelo Gradient Boosting\njoblib.dump(grid_GB.best_estimator_, 'modelo_loan_gb_total.pkl', compress = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# salva modelo Random Forest\njoblib.dump(grid_RF.best_estimator_, 'modelo_loan_rf_total.pkl', compress = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# salva modelo Regressao Logistica\njoblib.dump(grid_RL.best_estimator_, 'modelo_loan_rl_total.pkl', compress = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Recuperando Modelo Salvo"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo_treinado = open('modelo_loan_gb.pkl', 'rb')\nmodel = joblib.load(modelo_treinado)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(x_teste)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict_proba(x_teste)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}