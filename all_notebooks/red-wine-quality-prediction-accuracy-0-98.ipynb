{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U geometric-smote","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.combine import SMOTETomek\nfrom gsmote import GeometricSMOTE\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nimport warnings\nfrom pprint import pprint\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explanatory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features are numerical and the target is of categorical type with 3, 4, 5, 6, 7 and 8 as its categories. Hence its a milti-class classification problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There aren't any null values in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Skewness in the data will affect signficantly the performance of the model. Hence we have to apply some kind of transforms to get the symmetry in the distribution of the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Skewness in the data\n1. log transformation. (Used)\n2. Square root transformation.\n3. Box-Cox transformation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Residual Sugar, Chlorides, Free Sulphur Dioxide, Total Sulphur Dioxide, and Sulphates are highly right skewed. Apply logarithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_transform(col):\n    return np.log(col[0])\n\ndata['residual sugar'] = data[['residual sugar']].apply(log_transform, axis=1)\ndata['chlorides'] = data[['chlorides']].apply(log_transform, axis=1)\ndata['free sulfur dioxide'] = data[['free sulfur dioxide']].apply(log_transform, axis=1)\ndata['total sulfur dioxide'] = data[['total sulfur dioxide']].apply(log_transform, axis=1)\ndata['sulphates'] = data[['sulphates']].apply(log_transform, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\ncolumns = list(data.columns)\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.boxplot(data['quality'], data[columns[k]], ax = ax1[i][j])\n            k += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_outputs(col):\n    return col[0] - 3\n\ndata['quality'] = data[['quality']].apply(scale_outputs, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.DataFrame(data.quality, columns=['quality'])\ntrain_data = data.drop('quality', axis=1)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Imbalance\n1. Under Sampling.\n2. Over Sampling.\n3. SMOTETomek. (SMOTE(first) + Tomek(second))\n4. SMOTE.\n5. Geometric SMOTE.\n\nAnd for some other handling methods refer this website: https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/\n\n#### SMOTETomek worked the best for me.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_labels.quality)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampler = SMOTETomek()\ntrain_res, labels_res = sampler.fit_sample(train_data, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(labels_res.quality)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SMOTETomek performs SMOTE first and then Tomek. This is the reason for the majority classes to have less samples than the minority class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"geometric_smote = GeometricSMOTE()\ntrain_resG, labels_resG = geometric_smote.fit_resample(train_data, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(labels_resG.quality)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_data = pd.concat([train_res, labels_res], axis=1)\ncorr = total_data.corr()\nplt.figure(figsize=(10, 10))\nplt.title('Correlation Matrix')\nsns.heatmap(corr, cmap='YlGnBu', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What can we tell about the matrix?\n1. Volatile acidity and alchohol have high correlations with the target variable.\n2. The correlation of each feature with itself is 1.\n3. There are many independent vriables that are actually highly dependent on the other independent feature. In such cases the importance of each independent feature calculated will not be close to accurate.\n\n### Some reasons for high correlation between features:\n1. Free Sulphur and Total Sulphur : Total Sulfur Dioxide (TSO2) is the portion of SO2 that is free in the wine plus the portion that is bound to other chemicals in the wine such as aldehydes, pigments, or sugars.\n2. PH with acidic features : We know that PH increase with decrease in acidic property of the chemical. Hence Inverse relationship can be seen here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Training and Parameter Tuning\n\nSource: https://jamesrledoux.com/code/randomized_parameter_search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n#cross validation.\ndef cross_validate(model, train, labels):\n    scores = []\n    best_accuracy = 0\n    print(train.shape)\n\n    for train_ind, val_ind in kfold.split(train, labels):\n\n        t_data = train.loc[train_ind]\n        t_labels = labels.loc[train_ind]\n        v_data = train.loc[val_ind]\n        v_labels = labels.loc[val_ind]\n    \n        model.fit(t_data, t_labels)\n        preds = model.predict(v_data)\n        score = accuracy_score(v_labels, preds)\n        scores.append(score)\n        \n        if score > best_accuracy:\n            best_accuracy = score\n            best_model = model\n    \n    print('Accuracy : ' + str(round(sum(scores)/len(scores), 2)))\n    return best_model, best_accuracy\n    \n#parameter tuning.\ndef tune(model, params, train, labels):\n    search = RandomizedSearchCV(model, params, n_iter=20, cv=6, random_state=21)\n    best_model = search.fit(train, labels)\n    pprint(best_model.best_estimator_.get_params())\n    return best_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators' : [125, 150, 175, 200],\n    'max_depth' : [6, 7, 8],\n    'max_features' : [4, 5, 6, 7],\n    'bootstrap' : [True],\n    'min_samples_leaf' : [2, 3, 4]\n}\n\nmodel = RandomForestClassifier()\n\nbest_model = tune(model, params, train_res, labels_res)\nrfc = RandomForestClassifier(**best_model.best_estimator_.get_params())\nfinal_model, best_accuracy = cross_validate(rfc, train_res, labels_res)\nprint('Accuracy of the best model : ' + str(round(best_accuracy, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'eta' : [0.1, 0.2, 0.3, 0.4],\n    'max_depth' : [4, 5, 6, 7],\n    'verbosity' : [1],\n    'subsample' : [0.5, 0.75, 1],\n    'n_estimators' : [75, 100, 125, 150],\n    'min_child_weight' : [2, 3, 4, 5],\n    'objective' : ['multi:softmax'],\n    'num_class' : [6]\n}\n\nmodel = XGBClassifier()\n\nbest_model = tune(model, params, train_res, labels_res)\nxgb = XGBClassifier(**best_model.best_estimator_.get_params())\nfinal_model, best_accuracy = cross_validate(xgb, train_res, labels_res)\nprint('Accuracy of the best model : ' + str(round(best_accuracy, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(final_model.feature_importances_,train_res.columns)), columns=['Value','Feature'])\n\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('XGB Features (avg over folds)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators' : [75, 100, 125],\n    'num_iterations' : [225, 250, 275, 300],\n    'learning_rate' : [0.05, 0.075],\n    'max_depth' : [6],\n    'num_leaves' : [30, 35, 40],\n    'min_data_in_leaf' : [15],\n    'bagging_fraction' : [0.4, 0.5, 0.6],\n    'feature_fraction' : [0.5, 0.6, 0.7],\n    'lambda_l2' : [0.5, 0.75, 1],\n    'subsample' : [0.5, 0.75, 1]\n}\n\nmodel = lgb.LGBMClassifier()\n\nbest_model = tune(model, params, train_res, labels_res)\nmodel_lgb = lgb.LGBMClassifier(**best_model.best_estimator_.get_params())\nfinal_model, best_accuracy = cross_validate(model_lgb, train_res, labels_res)\nprint('Accuracy of the best model : ' + str(round(best_accuracy, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(final_model.feature_importances_,train_res.columns)), columns=['Value','Feature'])\n\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do like the notebook if you have learnt something from this notebook. If you are an expert and here to review do like so that I get encouraged, learn more. Thank you for completing the notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}