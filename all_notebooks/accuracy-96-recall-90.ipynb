{"cells":[{"metadata":{"_uuid":"67dfe16e-135c-4274-ab29-74ba4035ebcb","_cell_guid":"38984a93-29ac-43fd-9d48-4763cc66114c","trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"Predict Customer Churn_Credit Card Dataset_Kaggle.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/gist/phamhuyen286/164eaf94b6b37586111295f2467aa865/predict-customer-churn_credit-card-dataset_kaggle.ipynb\n\nA manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction\n\nI got this dataset from a website with the URL as https://leaps.analyttica.com/home. I have been using this for a while to get datasets and accordingly work on them to produce fruitful results. The site explains how to solve a particular business problem.\n\nNow, this dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features.\n\nWe have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npath = '/kaggle/input/credit-card-customers/BankChurners.csv'\n\ndata = pd.read_csv(r'/kaggle/input/credit-card-customers/BankChurners.csv')\n\ndata.head(5)\n\ndata.dtypes\n\ndata.shape\n\n# as the data suggested to delete two last columns so that it's easy to not making any confuse\n\ndata = data.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1)\n\ndata.shape\n\n\"\"\"Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as Non-churning will do. So recall (TP/TP+FN) need to be higher.\"\"\"\n\ndata.head(5)\n\n# check for the missing value\n\ndata.isnull().sum(axis=0) # we dont have missing value in itit\n\n\"\"\"As part of project requirement, we would like to study all features which most influential factors to customer churn. To do that we will perform EDA\"\"\"\n\n\n\n\"\"\"## Univariate Analysis are using for categorical data\"\"\"\n\ndata.Attrition_Flag.unique() # we will use label encoder for this\n\ndata['Attrition_Flag'].value_counts(normalize =True)\n\n\"\"\"Attrition is set of customer who is churn. we got 16% of customer under Attrition & 84% of them are existing\"\"\"\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndata.groupby(['Education_Level','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)\nplt.show()\n\ndata.groupby(['Marital_Status','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)\nplt.show()\n\ndata.groupby(['Card_Category','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)\nplt.show()\n\ndata.groupby(['Income_Category','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)\nplt.show()\n\nfrom sklearn.preprocessing import LabelEncoder\naf_le = LabelEncoder()\ndata['Attrition_Flag'] = af_le.fit_transform(data['Attrition_Flag'])\n\ndata.Education_Level.unique() #Unknown can be a factor to look into other in the future. however we can check if we have Unknown of education level in the data\n\ndata['Education_Level'].value_counts()\n\ndata['Education_Level'].value_counts(normalize=True)\n\n\"\"\"We understand that 6 defined categories for Education Level while most contribution come from Graduate for 31% and then Highschool. However there are number of Unknown also there. Let us check how many unknown in Education level has been associate with the Attrition and also other\"\"\"\n\ndata['Marital_Status'].value_counts(normalize=True)\n\ndata['Card_Category'].value_counts(normalize=True)\n\ndata['Gender'].value_counts(normalize=True)\n\n#Encoding the categorical data\n\nfrom sklearn.preprocessing import LabelEncoder\n\ngender_le = LabelEncoder()\ndata['Gender'] = gender_le.fit_transform(data['Gender'])\n\n\n\nEdu_le = LabelEncoder()\ndata['Education_Level'] = Edu_le.fit_transform(data['Education_Level'])\n\ncard_le = LabelEncoder()\ndata['Card_Category'] = card_le.fit_transform(data['Card_Category'])\n\nmarital_le = LabelEncoder()\ndata['Marital_Status'] = marital_le.fit_transform(data['Marital_Status'])\n\ndata.head(5)\n\ndata['Income_Category'].value_counts(normalize=True)\n\n# the same we can also encode the data\nincome_le = LabelEncoder()\ndata['Income_Category'] = income_le.fit_transform(data['Income_Category'])\n\ndata.head(5)\n\ndata = data.drop(columns=['CLIENTNUM'])\n\ndata.dtypes\n\ndata.describe()\n\n\"\"\"# Features Important\"\"\"\n\n#Using Pearson Correlation\nplt.figure(figsize=(22,20))\ncor = data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()\n\n# Split input & output data\n\ny = data['Attrition_Flag']\ny = np.array(y)\ny = y.reshape(-1,1)\ny.shape\n\nX = data.drop(columns=['Attrition_Flag'])\nX.shape\n\n# Split train & test  data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n#Import libraries for modelling\n\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom time import time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier\nlr=LogisticRegression()\ndt=DecisionTreeClassifier()\nknn=KNeighborsClassifier()\nrf=RandomForestClassifier()\nada=AdaBoostClassifier()\nbag=BaggingClassifier()\nxtree=ExtraTreesClassifier()\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount=CountVectorizer()\nfrom sklearn.decomposition import PCA\n\nX=X.apply(zscore)\n\nstart_time=time()\nmodel_list=[lr,dt,knn,rf,ada,bag,xtree]\nScore=[]\nfor i in model_list:\n    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)\n    i.fit(x_train,y_train)\n    y_pred=i.predict(x_test)\n    score=accuracy_score(y_test,y_pred)\n    Score.append(score)\nprint(pd.DataFrame(zip(model_list,Score),columns=['Model Used','R2-Score']))\nend_time=time()\nprint(round(end_time-start_time,2),'sec')\n\n\"\"\"Find out F1, accuracy and Recall for RandomForest\"\"\"\n\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)\nrf=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=100)\nrf.fit(x_train,y_train)\ny_pred=rf.predict(x_test)\naccuracy_score(y_test,y_pred)\n\ntrain_pred = rf.predict(x_train)\n\ntest_pred = rf.predict(x_test)\n\n# Use score method to get accuracy of model\nscore = rf.score(x_train, y_train)\nprint('Train F1 Score = {} %'.format(round(score,4)*100))\n\n# Use score method to get accuracy of model\nscore = rf.score(x_test, y_test)\nprint('Test F1 Score = {} %'.format(round(score,4)*100))\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nconf_matrix = confusion_matrix(y_test, test_pred)\nprint(conf_matrix)\n\nplt.figure(figsize=(11,9))\nsns.heatmap(conf_matrix, annot=True, fmt=\".3f\", linewidths=.5)\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nall_sample_title = 'Test F1 Score:{}%'.format(round(score, 4)*100)\nplt.title(all_sample_title, size = 15)\n\nfrom sklearn.metrics import classification_report\n\nprint('*********************  Training Data Report  **********')\nprint(classification_report(y_train, train_pred))\n\nprint('***********************  Test Data Report  **********')\nprint(classification_report(y_test, test_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}