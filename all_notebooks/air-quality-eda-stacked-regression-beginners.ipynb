{"cells":[{"metadata":{},"cell_type":"markdown","source":"The aim is to build a machine learning model which can predict AQI based on the 7 features and the dataset consisting of 7844 records.\n\nWith inspiration from:\n* https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n* https://www.kaggle.com/virajkadam/notebookc835013f04\n* https://www.kaggle.com/tzachymorad/cancer-cost-beginner-s-guide-prep-and-stacking"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport re\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint,uniform\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\n#import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pune-air-quality-index/PNQ_AQI.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\ndata.info()\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change Date strings to numbers and sort by date."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Date'] = pd.to_datetime(data['Date'])\n#data['Date'] = data['Date'].apply(lambda x: int(x.timestamp()))\ndata.sort_values(by=['Date'], inplace=True, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After doing some Googling, I found that BDL probably means \"below detection limit\".\n\nSo I did the following:\n* Create a new feature of BDL for each column with such values\n* Extract the last 3 characters of each row (giving NA or a number)\n* Replace NA with 0 and convert the numbers to integers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for _, col in enumerate(list(data.columns[1:3])):\n    data[f'{col} BDL'] = data[f'{col}'].map(lambda x: 1 if 'BDL' in x else 0)\n    data[f'{col}'] = data[f'{col}'].apply(lambda x: x[-3:])\n    data[f'{col}'] = data[f'{col}'].apply(lambda x: 0 if 'NA' in x else int((re.findall(r'\\d+',x))[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pick and show outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_features = list(data.columns[1:5])\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    \n    for col in features:\n        q1 = np.nanpercentile(df[col], 25)\n        q3 = np.nanpercentile(df[col], 75)\n        iqr = q3 - q1\n        outlier_step = 1.5 * iqr\n        outlier_list_col = df[(df[col] < q1 - outlier_step) | (df[col] > q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n)\n    return multiple_outliers\n\nOutliers_to_drop = detect_outliers(data,1,outlier_features)\ndata.loc[Outliers_to_drop]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(Outliers_to_drop, axis = 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Delete multiple names for Locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"rep={'MPCB-KR':'Karve Road','MPCB-SWGT':'Swargate','MPCB-BSRI':'Bhosari',\\\n     'MPCB-NS':'Nal Stop','MPCB-PMPR':'Pimpri','Pimpri Chinchwad':'Chinchwad'}\ndata['Location'].replace(rep,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Drop rows without a label (i.e., where AQI is NaN).\n* Copy target into a new series.\n* Drop the copied target, and a column without relevant data.\n* Fill NaNs with nearest values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(axis=0, subset=['AQI'], inplace=True)\ndata.drop(['CO2 µg/m3'], axis=1, inplace=True)\ndata.fillna(method='bfill', axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Move AQI to beginning and summarize data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['AQI'] + [c for c in data if c not in ['AQI']]]\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show correlation between numerical features and the label."},{"metadata":{"trusted":true},"cell_type":"code","source":"g1 = sns.heatmap(data.iloc[:,:5].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All are positively correlated with the AQI; the strongest correlation is with Respirable Suspended Particulate Matter"},{"metadata":{},"cell_type":"markdown","source":"Find correlations between non-numeric features."},{"metadata":{"trusted":true},"cell_type":"code","source":"date_sampler = data.set_index('Date').groupby('Location').resample('W').bfill().droplevel(0).reset_index()\ng2 = sns.FacetGrid(date_sampler, row='Location', height=2, aspect=6)\ng2.map(sns.pointplot, 'Date', 'AQI', 'SO2 µg/m3 BDL', palette='deep')\ng2.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It seems that more S02 measurements that were BDL generally happened earlier in the timeframe of the dataset.\n* It also seems like AQI got generally worse later in the timeframe.\n* Both of these points should be explored further, as together they indicate increasing levels of harmful substances in the air."},{"metadata":{"trusted":true},"cell_type":"code","source":"g3 = sns.factorplot(y=\"AQI\",x=\"Location\", data=data,kind=\"violin\")\ng4 = sns.factorplot(y=\"Nox µg/m3\",x=\"Location\", data=data,kind=\"violin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we see that Chinchwad had the greatest variation in both AQI and Nox.\n* We also see that the distribution of AQI across all locations is similar to Nox (more centered for Karve Road and Pimpri, slightly skewed down for Swargate, and more skewed for Chinchwad).\n* Finally, Karve Road had some of the worst days in terms of AQI, but this didn't draastically change its median AQI compared to the other locations,\n* These points indicate a slight correlation between location and AQI. "},{"metadata":{},"cell_type":"markdown","source":"Turn the Locations into categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"Location = pd.get_dummies(data.Location, prefix='Location')\nframes = [data, Location]\ndata = pd.concat(frames, axis=1)\ndata.drop(columns=['Location'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate data from target, create train and test sets (without dates) and scale the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = data.AQI\ndata.drop(['AQI'], axis=1, inplace=True)\nX_train, X_test, y_train, y_test\\\n    = train_test_split(data.iloc[:,1:], target, test_size=0.25, random_state=42)\n\nXscaler = preprocessing.RobustScaler().fit(X_train)\nX_train_transformed = Xscaler.transform(X_train)\nX_test_transformed = Xscaler.transform(X_test)\nyscaler = preprocessing.RobustScaler().fit(y_train.to_frame())\ny_train = np.log1p(y_train)\ny_test = np.log1p(y_test)\ny_train_transformed = yscaler.transform(y_train.to_frame())\ny_test_transformed = yscaler.transform(y_test.to_frame())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build a simple model, perform cross-validation on the training set, and predict and calculate the mean square error and absolute square error on the test."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression().fit(X_train_transformed, y_train_transformed)\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ncross_val_score(reg, X_train_transformed, y_train_transformed, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg.predict(X_test_transformed)\nmean_absolute_error(y_test_transformed, y_pred)\nmean_squared_error(y_test_transformed, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a regression model by stacking a couple of models, perform cross-validation and calculate MSE/MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [('lr', RidgeCV()),('svr', LinearSVR(random_state=42))]\nstacking_reg = \\\n    StackingRegressor(estimators=estimators,\\\n                      final_estimator=RandomForestRegressor(n_estimators=10,random_state=42))\nstacked_reg = stacking_reg.fit(X_train_transformed, y_train_transformed)\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ncross_val_score(stacked_reg, X_train_transformed, y_train_transformed, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_y_pred = stacked_reg.predict(X_test_transformed)\nmean_absolute_error(y_test_transformed, stacked_y_pred)\nmean_squared_error(y_test_transformed, stacked_y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The cross-validation score for the stacked regressors was slightly higher, but the error in the test was also higher. Maybe fine-tuning the hyperparameters will help.\n* Check for best hyperparameters using GridSearchCV, to improve the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_distributions = {'n_estimators': randint(1, 5),'max_depth': randint(5, 10)}\nsearch = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n                            n_iter=5,\n                            param_distributions=param_distributions,\n                            random_state=0)\nsearch.fit(X_train_transformed, y_train_transformed)\ncross_val_score(search, X_train_transformed, y_train_transformed, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = search.predict(X_test_transformed)\nmean_absolute_error(y_test_transformed, y_pred)\nmean_squared_error(y_test_transformed, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This single model was better than all others because we found the optimal hyperparameters.\n* Create a new ensemble regressor with the optimized Ramdom Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_param_distributions = {'n_estimators': [50, 100],\n        'learning_rate': [0.01, 0.05, 0.1, 0.3, 1],\n        'loss': ['linear', 'square', 'exponential']}\nada_search = GridSearchCV(AdaBoostRegressor(random_state=0),ada_param_distributions)\nada_search.fit(X_train_transformed, y_train_transformed)\ncross_val_score(ada_search, X_train_transformed, y_train_transformed, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_param_distributions = {\n        \"max_depth\": [3, 5, 8],\n        \"max_features\": [\"log2\", \"sqrt\"],\n        \"criterion\": [\"friedman_mse\", \"lad\"],\n        \"subsample\": [0.5, 0.75, 1.0]}\ngbr_search = GridSearchCV(GradientBoostingRegressor(random_state=0),gbr_param_distributions)\ngbr_search.fit(X_train_transformed, y_train_transformed)\ncross_val_score(gbr_search, X_train_transformed, y_train_transformed, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [('abr', ada_search),('gbr', GradientBoostingRegressor(random_state=0))]\nstacking_reg = \\\n    StackingRegressor(estimators=estimators,\\\n                      final_estimator=search)\nstacked_reg = stacking_reg.fit(X_train_transformed, y_train_transformed)\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ncross_val_score(stacked_reg, X_train_transformed, y_train_transformed, cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_y_pred = stacked_reg.predict(X_test_transformed)\nmean_absolute_error(y_test_transformed, stacked_y_pred)\nmean_squared_error(y_test_transformed, stacked_y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"After several attempts at stacking random forest, adaboost and gradient boosting using different parameters with the help of gridsearch, I still didn't get a MSE as good as I did with random forset alone, so that's the one I'll stay with"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'\\nFinal MAE: {mean_absolute_error(y_test_transformed, y_pred)}')\nprint(f'\\nFinal MSE: {mean_squared_error(y_test_transformed, y_pred)}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}