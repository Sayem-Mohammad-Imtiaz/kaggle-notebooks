{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this Problem dataset, for finding the factors contributing to Car Prices, I will use Linear Regression with RFE and form different model depending on the p-value and VIF (for feature significance).\n\nWill be updating the notebook with additional ML techniques (like adding Regularization while Model building / building Polynomial transformed features with degree 2,3) soon."},{"metadata":{},"cell_type":"markdown","source":"Author: Akhil Shukla\n\nIf you use parts of this notebook in your own scripts, please give some sort of credit (for example link back to this).\n\nThanks!"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\n\nA Chinese automobile company <b>Geely Auto</b> aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. "},{"metadata":{},"cell_type":"markdown","source":"They want to specifically understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\n1. Which variables are significant in predicting the price of a car\n2. How well those variables describe the price of a car\n\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<b>We will follow folowing steps for building our model with the variables that are significant, and to be considered by Geely Autos in understanding the car pricing in the American Automobile market:\n\n1. `Reading and understanding the data`\n\n2. `Visualizing the data`\n\n3. `Data Preparation`\n\n4. `Training the model`\n\n5. `Residual analysis`\n\n6. `Predicting and evaluation on the test set`</b>"},{"metadata":{},"cell_type":"markdown","source":"### At the end we will make suggestions and recommendations to Geely Autos to help them enter the US market, utilizing our analysis on factors leading to Car price behavior."},{"metadata":{},"cell_type":"markdown","source":"____________________________________________________________________________________________________________________________________________"},{"metadata":{},"cell_type":"markdown","source":"### IMPORTING LIBRARIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Importing statsmodel library for statistical summary\nimport statsmodels.api as sm \n\n#Importing sklearn methods\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Importing sklearn RFE and LinearRegression methods\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\npd.set_option('display.max_columns', 600)\npd.set_option('display.max_rows', 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STEP 1: Reading and Understanding the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcar = pd.read_csv('../input/car-data/CarPrice_Assignment.csv')\ndfcar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Dataset shape          ---- The data contains 205 rows and 26 columns.\n\n    Null value status      ---- None of the values in the dataset are null."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcar.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Visualising the Data\n\nLets visualise the given dataset to find out any visible clues, and check:\n- which features show strong linear relationship with our target variable 'price'.\n- the correlation among the features."},{"metadata":{},"cell_type":"markdown","source":"#### Visualising All the Variables\n\nLet's make a pairplot of all the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=2)\nsns.pairplot(dfcar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations** from above graphs:\n\n1. When plotted against 'price': most of the independent variables are showing linear behaviour with 'price'. Hence a linear regression model should be a good choice for the given data.\n\n2. Also we see that there are many features that show multicollinearity. Lets check that now."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))  \nsns.set(font_scale=1.5)\nsns.heatmap(dfcar.corr(),annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the above correlation table, the features:\n- highwaympg, citympg are very highly correlated. We can drop one of them. Lets drop highwaympg.\n\n- wheelbase, carlength, carwidth, curbweight, enginesize are highly correlated with each other - **lets plot corr map of just these var. for better readability**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop the highwaympg for reasons mentioned above.\n\ndfcar = dfcar.drop(['highwaympg'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,10))\ncorr1 = dfcar[['wheelbase','carlength','carwidth','curbweight','carheight','enginesize','price']].corr()\ncorr1.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations:\n\ncurbweight, carwidth, wheelbase and carlength are highly correlated. Lets just keep one out of these.\n\n- drop `curbweight` -> it is highly correlated with other features.\n- drop `carwidth`   -> it is highly correlated with other features.\n- drop `wheelbase`  -> it is highly correlated with other features, but lets keep it for now.\n- `carheight` is less correlated with others, so we'll keep it for now.\n- `enginesize` has a very high corr with 'price', so we'll keep it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop the curbweight, carwidth and wheelbase column for reasons mentioned above.\n\ndfcar = dfcar.drop(['curbweight', 'carwidth', 'wheelbase'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising Categorical Variables\n\nAs you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1)\nplt.figure(figsize=(20, 12))\nplt.subplot(3,3,1)\nsns.boxplot(y = 'fueltype', x = 'price', data = dfcar)\nplt.subplot(3,3,2)\nsns.boxplot(y = 'aspiration',x = 'price', data = dfcar)\nplt.subplot(3,3,3)\nsns.boxplot(y = 'doornumber', x = 'price', data = dfcar)\nplt.subplot(3,3,4)\nsns.boxplot(y = 'carbody', x = 'price', data = dfcar)\nplt.subplot(3,3,5)\nsns.boxplot(y = 'drivewheel', x = 'price', data = dfcar)\nplt.subplot(3,3,6)\nsns.boxplot(y = 'enginelocation', x = 'price', data = dfcar)\nplt.subplot(3,3,7)\nsns.boxplot(y = 'enginetype', x = 'price', data = dfcar)\nplt.subplot(3,3,8)\nsns.boxplot(y = 'cylindernumber', x = 'price', data = dfcar)\nplt.subplot(3,3,9)\nsns.boxplot(y = 'fuelsystem', x = 'price', data = dfcar)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations:\n\nLets make some visible observations from the boxplots. This might come handy for intuitive modelling later.\n\n1.Fueltype: not much of a difference on price.\n\n2.Aspiration: turbo is on an average costlier than std.\n\n3.DoorNumber: No visible effect on price.\n\n4.Carbody: Hatchback and Wagon are usually cheaper. Hardtop is available across wide range of price.\n\n5.Drivewheel: rwd is usually costlier.\n\n6.Enginelocation: Rear is clearly expensive.\n\n7.Enginetype: very general.. will check later.\n\n8.Cylindernumber: eight is clearly expensive. three and four are comparatively cheaper.\n\n9.Fuelsystem: very general.. will check later."},{"metadata":{},"cell_type":"markdown","source":"Lets create a derived variable containing just the car company name. We do not need the car model information."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcar['CarComp'] = dfcar['CarName'].str.replace('-',' ')\ndfcar['CarComp'] = dfcar['CarComp'].str.split(' ', n=1, expand=True)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notice** that there are some car companies \n\n- some car companies are duplicates with upper lower case distinction. We need to fix this - we will make all car company names as lowercase.\n- some car company names are misplelled, because of which they are being treated as different companies - we need to fix this.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert all car company names to lowercase\ndfcar['CarComp'] = dfcar['CarComp'].str.lower()\n\n#Note some car company name are entered with typos/ or lowercase. Lets fix that.\ncorrect_name = {'maxda' : 'mazda', 'porcshce' : 'porsche', 'toyouta' : 'toyota', 'vokswagen' : 'volkswagen', 'vw' : 'volkswagen'}\ndfcar['CarComp'] = dfcar['CarComp'].replace(correct_name, regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop the CarName and car_ID column as we dont need it anymore\ndfcar = dfcar.drop(['CarName', 'car_ID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets also visualize the derived feature CarComp\n\nplt.figure(figsize=(15,8))\nsns.boxplot(x = 'price', y = 'CarComp', data = dfcar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We observe that the car company 'mercury' has just a single entry in the data. Hence wont be of any use. We'll discard it later, when we create dummy var for car company names."},{"metadata":{},"cell_type":"markdown","source":"Now lets apply some encoding on the categorical features."},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Data Preparation\n\nCategorical feature encoding\n- Let us map values like 1,2,3.. for `ordinal feature values`, and create dummy var for the `nominal features values`, so that they can be used in a regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ordinal Features:\n\n# dfcar.fueltype.unique()           # 'gas', 'diesel'\n\ndfcar.fueltype = dfcar.fueltype.map({'gas':0, 'diesel':1})\n\n# dfcar.aspiration.unique()         # 'std', 'turbo'\n\ndfcar.aspiration = dfcar.aspiration.map({'std':0, 'turbo':1})\n\n# dfcar.doornumber.unique()         # 'two', 'four'\n\ndfcar.doornumber = dfcar.doornumber.map({'two':0, 'four':1})\n\n# dfcar.enginelocation.unique()     # 'front', 'rear'\n\ndfcar.enginelocation = dfcar.enginelocation.map({'front':0, 'rear':1})\n\ndfcar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Nominal Features:\n\ncar = pd.get_dummies(dfcar)\n\n# From now on we will use `car` dataset only.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`One unfinished task`: As stated above while plotting the boxplot for CarComp names, since mercury company has just one entry in the data, we don't need it, and hence would drop this `CarComp_mercury` column"},{"metadata":{"trusted":true},"cell_type":"code","source":"car = car.drop(['CarComp_mercury'], axis=1)\ncar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Training the model\n\nLet us start towards building our LR model. Lets first split data into train-test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us split the dataset 'car' into train and test data (70:30 ratio) using sklearn train_test_split() method\n\ndf_train, df_test = train_test_split(car, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Features \n\nWe will use Normalization scaling, and scale all the numeric features. We dont need to do anything with the created dummy variables as they are already 0 or 1, and hence in the range [0,1] "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit on train data\n\n# Create list num_vars(contains the list of numeric predictor variables):\n\nnum_vars = ['symboling','carlength','carheight','enginesize','boreratio','stroke','compressionratio','horsepower','peakrpm','citympg','price']\n\n# Lets normalize both df_train and df_test here.\n\ndf_train[num_vars] = df_train[num_vars].apply(lambda x: (x- np.mean(x))/(x.max() - x.min()))\ndf_test[num_vars] = df_test[num_vars].apply(lambda x: (x- np.mean(x))/(x.max() - x.min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated. \n\ncorr = df_train[df_train.columns].corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Price is showing strong relationship with:\n\n>Positive Corr\n   - 'enginesize' (0.867), 'horsepower' (0.806), 'carwidth' (0.799), 'drivewheel_rwd' (0.677), 'wheelbase' (0.622) \n   \n>Negative Corr\n   - 'citympg' (-0.674), 'drivewheel_fwd' (-0.635), "},{"metadata":{},"cell_type":"markdown","source":"Before beginning to build a model, let us again check the scatterplot of features vs price to get a better intuitive understanding."},{"metadata":{},"cell_type":"markdown","source":"Car Dimension numeric features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df_train, x_vars=['carlength', 'carheight'], y_vars='price',height=4, aspect=1, kind='scatter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Car engine related numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df_train, x_vars=['enginesize', 'horsepower', 'stroke'], y_vars='price',height=4, aspect=1, kind='scatter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other numeric features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(df_train, x_vars=['boreratio', 'compressionratio','peakrpm', 'citympg'], y_vars='price',height=4, aspect=1, kind='scatter')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OBSERVATIONS:**\n\n`carweight`, `enginesize` show strong positive linear relationship with car `price`. \n\nAlso they are important features to have in model for making business decisions for`Geely Automobiles`."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('price')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building a linear model\n\nFit a regression line through the training data using `statsmodels`.\n\nSince there are a lot of variables, we will make use of RFE to determine the top good features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_in = X_train.columns[rfe.support_]\ncol_in","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_out = X_train.columns[~rfe.support_]\ncol_out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model using statsmodel, for the detailed statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col_in]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Define a function that displays the Statistics summary and also return the X_train_lm dataset, and the modeling object"},{"metadata":{"trusted":true},"cell_type":"code","source":"def LRM_Summ(df):\n    # Display the linear model summary\n    \n    #Adding a constant\n    X_train_lm = sm.add_constant(df)\n    \n    #Fit\n    lm = sm.OLS(y_train,X_train_lm).fit()\n    \n    #Printing the statistics summary\n    print(lm.summary())\n    \n    #Returning the X_train_lm dataset and the VIF table for all the features present in the model\n    return X_train_lm, lm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Defining a function that display VIF of the features present in the model and their correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def LRM_VIF_Corr(df):    \n    \n    # Calculate the VIFs for the current model\n    vif = pd.DataFrame()\n    X = df\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    print('\\n')\n    print(vif)\n    \n    \n    #Printing the correlation matrix of features in the current model along with price\n    col_var = list(vif.Features)\n    col_var = col_var + ['price']\n    corr = car[col_var].corr()\n    return(corr.style.background_gradient(cmap='coolwarm'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL 1:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lmod1, lmod1 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model 1: \n - The model contains 15 features.\n - All the features look significant (as per p-value) but notice the extremely high VIF (infinite)"},{"metadata":{},"cell_type":"markdown","source":"**MODEL 2:**\n\n`engine_rotor` and `cylindernumber_two` have infinite VIF, indicating that they have a correlarion of 1.\n\nWe need to drop one of them. Lets drop `engine_rotor`."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['enginetype_rotor'], axis=1)\ncar = car.drop(['enginetype_rotor'], axis=1)\n\nX_train_lmod2, lmod2 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL 3:**\n\nSince the p-value of `enginetype_dohcv` is 0.064 (> 0.05), hence we'll drop `enginetype_dohcv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['enginetype_dohcv'], axis=1)\ncar = car.drop(['enginetype_dohcv'], axis=1)\n\nX_train_lmod3, lmod3 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL 4:**\n\n\nSince the p-value of `cylindernumber_eight` is 0.068 (> 0.05), hence we'll drop `cylindernumber_eight`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['cylindernumber_eight'], axis=1)\n\ncar = car.drop(['cylindernumber_eight'], axis=1)\n\nX_train_lmod4, lmod4 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features seem to be farily significant. Lets look above at the correlation table to find any visible multicollinearity."},{"metadata":{},"cell_type":"markdown","source":"**MODEL 5:**\n\nNotice that `cylindernumber_four` is highly (negatively) correlated with `enginesize`, hence we'll drop `cylindernumber_four`."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['cylindernumber_four'], axis=1)\n\ncar = car.drop(['cylindernumber_four'], axis=1)\n\nX_train_lmod5, lmod5 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL 6:**"},{"metadata":{},"cell_type":"markdown","source":"The `cylindernumber_twelve` is having a high p-value of 0.149. Lets drop this feature and check."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['cylindernumber_twelve'], axis=1)\n\ncar = car.drop(['cylindernumber_twelve'], axis=1)\n\nX_train_lmod6, lmod6 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL 7:**"},{"metadata":{},"cell_type":"markdown","source":"Notice `stroke` has a high p-value, and also `stroke` has considerable correlation with `enginesize` and `carlength`. Hence dropping it would be a good idea."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['stroke'], axis=1)\n\ncar = car.drop(['stroke'], axis=1)\n\nX_train_lmod7, lmod7 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODEL 8:**"},{"metadata":{},"cell_type":"markdown","source":"The feature `boreratio` has a high p-value. Lets drop it and check if there is significant change"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['boreratio'], axis=1)\n\ncar = car.drop(['boreratio'], axis=1)\n\nX_train_lmod8, lmod8 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping `boreratio` had almost no drop in Adjusted R-square which is a good thing."},{"metadata":{},"cell_type":"markdown","source":"**MODEL 9:**"},{"metadata":{},"cell_type":"markdown","source":"Now lets drop `cylindernumber_three` which is having a high significant value of 0.07."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['cylindernumber_three'], axis=1)\n\ncar = car.drop(['cylindernumber_three'], axis=1)\n\nX_train_lmod9, lmod9 = LRM_Summ(X_train_rfe)\nLRM_VIF_Corr(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our model looks fairly well at this point. The features in our model are all significant and also the VIF associated with the features are all below 2.6. We can perform some more feature trimming, but it might backfire as it will bring further more cutting, hence reducing our feature size.\n\n#### Let us now do some Residual Analysis, followed by our Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Residual Analysis on the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = lmod9.predict(X_train_lmod9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = y_train - y_train_pred\nsns.distplot(res, color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Predication and Evaluation on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('price')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding  constant variable to test dataframe\nX_test_lmod9 = sm.add_constant(X_test)\nX_test_lmod9.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test_lm9 dataframe by dropping variables from X_test_lm9\nX_test_lmod9 = X_test_lmod9.drop(col_out, axis=1)\nX_test_lmod9 = X_test_lmod9.drop(['enginetype_dohcv', 'enginetype_rotor', 'cylindernumber_eight',\n       'cylindernumber_four', 'cylindernumber_three', 'stroke', 'boreratio', 'cylindernumber_twelve'], axis=1)\nX_test_lmod9.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict\ny_test_pred = lmod9.predict(X_test_lmod9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate the model\nr2_score(y_true = y_test, y_pred = y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual vs Predicted\nindex = [i for i in range(1,63,1)]\nfig = plt.figure()\nplt.plot(index,y_test, color=\"blue\", linewidth=3.5, linestyle=\"-\")     #Plotting Actual\nplt.plot(index,y_test_pred, color=\"purple\",  linewidth=3.5, linestyle=\"-\")  #Plotting predicted\nfig.suptitle('Actual and Predicted')              # Plot heading \nplt.xlabel('Index')                               # X-label\nplt.ylabel('Car Price')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting y_test against y_pred to see the relationship.\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('y_test vs y_test_pred')              # Plot heading \nplt.xlabel('y_test')                          # X-label\nplt.ylabel('y_test_pred')     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The y_test_pred and y_test show a linear relation for most of the values. The other isolated points as seen above in the plot are the outliers, and not of much significance. The important point to note here is that the y_test and y_test_pred are following a somewhat linear relation, which in other words mean that the y_test_pred is able to consistently predict the values similar to the actual values carried by the y_test data. "},{"metadata":{},"cell_type":"markdown","source":"### Assessing the Model:\nLets scatter plot the error and see if the error is random or shows some pattern. The desired outcome is that there should be no visible pattern, and the distribution should be random."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error terms\nfig = plt.figure(figsize = (16,6))\n\nindex = [i for i in range(1,63)]\n\n#To see the randomness with dots\nplt.subplot(1,2,1)\nplt.scatter(index, y_test-y_test_pred, color = 'red')\n\nfig.suptitle('Error Terms')              # Plot heading \nplt.xlabel('Index')                      # X-label\nplt.ylabel('y_test - y_test_pred')                # Y-label\n\n\n# To join the randomness with dots to see if it has any pattern\nplt.subplot(1,2,2)\nplt.plot(index, y_test-y_test_pred, color=\"red\")\n\nfig.suptitle('Error Terms')              # Plot heading \nplt.xlabel('Index')                      # X-label\nplt.ylabel('y_test - y_test_pred')                # Y-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> In the above scatterplot, the error is randomly distributed and it does not follow any pattern. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us plot the histogram plot of the error terms to see if they follow the Normal Distribution.\n\nfig = plt.figure(figsize = (4,4))\n\nsns.distplot((y_test - y_test_pred),bins=10)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test-y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)             ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Also the (y_test - y_test_pred) is following a Normal distribution with mean close to zero (~0.00241)"},{"metadata":{},"cell_type":"markdown","source":"> The above model shows an R-square and adjusted R-square value of `0.898` and `0.892`.\n> Also the predicted values from the test data shows an R-square of `0.856`, which is pretty close to the one observed on the training data, and hence displays a good model.\n\n(I have tried many other models as well, and they are all comparable. The accuracy of this model is pretty good. And also the error distribution is much more random in this model compared to the others that I had tried. There can of-course be other models, that show similar or slightly better results.)"},{"metadata":{},"cell_type":"markdown","source":"## Inference\n  \n1. The R2 score of the predicted test data value is 86.1% which is pretty close to Adj R-square of the train data. This indicates that our model is able to predict the target var. values pretty well.\n\n2. The R square and Adjusted R square value in the training data are almost similar @ 86.8% and 86.2% respectively, which indicates that there is almost no redundany in the variables that we have chosen, and they all hold significance.\n\n3. The scatter plot of the Error Terms (y_test-y-pred) shows that the error terms are randomly distributed, and does not follow any visible pattern, which indicates that it is just the white noise. \n\n4. The R2 score of the predicted test data value is 86.1% which is pretty close to Adj R-square of the train data. This indicates that our model is able to predict the target var. values pretty well.\n\n5. From the histogram plot of the error terms in the test data, we can see that the error terms are following a Normal Distribution centered around a mean of close to `zero`. \n\n6. The predictor variables that can affect the car price are:\n\n   1.`carlength`, \n   \n   2.`enginesize`, \n   \n   3.`cylindernumber_two`,\n   \n   4.`CarComp_audi`,\n   \n   5.`CarComp_bmw`, \n   \n   6.`CarComp_buick`,\n   \n   7.`CarComp_porsche`"},{"metadata":{},"cell_type":"markdown","source":"# Recommendations to Geely Autos:\n>As per the model, the car price American automobile market is largely driven by the `carlength` and `enginesize`. The number of `cylinders` used as a gas fuel is also relevent to some extent in determining the prices.\n\n>The car brands, particularly `Audi`, `BMW`, `Buick`, `Porsche` also drive the car price in the American Automobile Market. So Geely Autos can invest time in studying these car brands particularly, which will help them in making decisions regarding the consumer auto choice in the American Market, while they set up their manufacturing unit in US market."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}