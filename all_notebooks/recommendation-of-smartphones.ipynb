{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Case Study : Recommendation on Smart Phones"},{"metadata":{},"cell_type":"markdown","source":"CONTEXT: \nIndia is the second largest market globally for smartphones after China. About 134 million smartphones were sold across India in the year 2017 and is estimated to increase to about 442 million in 2022. India ranked second in the average time spent on mobile web by smartphone users across Asia Pacific. The combination of very high sales volumes and the average smartphone consumer behaviour has made India a very attractive market for foreign vendors. As per Consumer behaviour, 97% of consumers turn to a search engine when they are buying a product vs. 15% who turn to social media. If a seller succeeds to publish smartphones based on user’s behaviour/choice at the right place, there are 90% chances that user will enquire for the same. This Case Study is targeted to build a recommendation system based on individual consumer’s behaviour or choice. \n• DATA DESCRIPTION: \n• author : name of the person who gave the rating \n• country : country the person who gave the rating belongs to \n• data : date of the rating \n• domain: website from which the rating was taken from \n• extract: rating content \n• language: language in which the rating was given \n• product: name of the product/mobile phone for which the rating was given \n• score: average rating for the phone \n• score_max: highest rating given for the phone \n• source: source from where the rating was taken  \n\n• PROJECT OBJECTIVE: We will build a recommendation system using popularity based and collaborative filtering methods to recommend mobile phones to a user which are most popular and personalised respectively.. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom collections import defaultdict\nfrom surprise import SVD\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Data files\nrev1 = pd.read_csv('../input/recommendation-system/phone_user_review_file_1.csv', encoding='iso-8859-1')\nrev2 = pd.read_csv('../input/recommendation-system/phone_user_review_file_2.csv', encoding='iso-8859-1')\nrev3 = pd.read_csv('../input/recommendation-system/phone_user_review_file_3.csv', encoding='iso-8859-1')\nrev4 = pd.read_csv('../input/recommendation-system/phone_user_review_file_4.csv', encoding='iso-8859-1')\nrev5 = pd.read_csv('../input/recommendation-system/phone_user_review_file_5.csv', encoding='iso-8859-1')\nrev6 = pd.read_csv('../input/recommendation-system/phone_user_review_file_6.csv', encoding='iso-8859-1')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev1.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev2.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev3.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev4.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev5.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev6.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev5.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev6.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1a. Merge the provided CSVs into one data-frame. \nrev_f = pd.concat([rev1,rev2,rev3,rev4,rev5,rev6],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_copy = rev_f.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking training dataset attributes datatypes \nrev_f.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All columns are objects except score and score_max which are floating point."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1b. Check a few observations and shape of the data-frame.\nrev_f.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_f.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standard deviation from the mean score of 8 is 2.616121e+00"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values\nrev_f.isnull().values.any() # If there are any null values in data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_counts = rev_f.isnull().sum()  # This prints the columns with the number of null values they have\nprint (null_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1d. Check for missing values. Impute the missing values if there is any. \n# filling the null values in column 'score' and 'score_max' \nrev_f = rev_f.fillna(rev_f.median())\n\n# dropping the null values in columns 'extract' ,'author' and 'product'\nrev_f = rev_f.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1c. Round oﬀ scores to the nearest integers. \nrev_f['score'] = rev_f['score'].astype(int) \nrev_f['score_max'] = rev_f['score_max'].astype(int) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_f.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1e. Check for duplicate values and remove them if there is any. \nrev_d = rev_f.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1g. Drop irrelevant features. Keep features like Author, Product, and Score. \n# we can drop phone_url,date,lang,country,source,domain and extract since they do not contribute in deciding popularity.  \nrev_d.drop(['phone_url','date','lang','country','source','domain','score_max','extract'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_vs = rev_d.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rev_d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1f. Keep only 1000000 data samples. Use random state=612\ndf = rev_d.sample(n=1000000, random_state=612)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2a. Identify the most rated features.\n#sorting on products that got highest mean score\ndf.groupby('product')['score'].mean().sort_values(ascending=False).head()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 b. Identify the users with most number of reviews. \n(df['author'].value_counts()).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The product that got most number of reviews.\ndf['product'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting authors who gave greater than 50 ratings\ndf1 = pd.DataFrame(columns=['author', 'a_count'])\ndf1['author']=df['author'].value_counts().index.tolist() \ndf1['a_count'] = list(df['author'].value_counts() > 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get names of indexes for which count column value is False\nindex_names = df1[ df1['a_count'] == False ].index \n# drop these row indexes from dataFrame \ndf1.drop(index_names, inplace = True) \ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting product that got more than 50 ratings\ndf2 = pd.DataFrame(columns=['product', 'p_count'])\ndf2['product']=df['product'].value_counts().index.tolist() \ndf2['p_count'] = list(df['product'].value_counts() > 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get names of indexes for which count column value is False\nindex_names = df2[ df2['p_count'] == False ].index \n# drop these row indexes from dataFrame \ndf2.drop(index_names, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting data rows where product is having more than 50 ratings.  \ndf3 = df[df['product'].isin(df2['product'])] \ndf3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting data rows from df3 where author has given more than 50 ratings.\n# 2c. so that we get the data with products having more than 50 ratings and users who have given more than 50 ratings\ndf4 = df3[df3['author'].isin(df1['author'])]\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2c. Report the shape of the final dataset.\ndf4.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a popularity based model and recommend top 5 mobile phones. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the mean score for a product by grouping it.\nratings_mean_count = pd.DataFrame(df.groupby('product')['score'].mean()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the number of ratings a product got\nratings_mean_count['rating_counts'] = pd.DataFrame(df.groupby('product')['score'].count())  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Recommending the 5 mobile phones based in highest mean score and highest number of ratings the product got. \nratings_mean_count.sort_values(by=['score','rating_counts'], ascending=[False,False]).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pb = df\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a collaborative filtering model using SVD. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# arranging columns in the order of user id,item id and rating to be fed in the svd\ncolumns_titles = ['author','product','score']\nvs_rev = rev_vs.reindex(columns=columns_titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep only 5000 data samples. Use random state=612\nvs_data = vs_rev.sample(n=5000, random_state=612)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Build a collaborative filtering model using SVD. \nreader = Reader(rating_scale=(1, 10))\ndata = Dataset.load_from_df(vs_data,reader = reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = data.build_full_trainset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset.ur","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo = SVD()\nalgo.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Than predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset = trainset.build_anti_testset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the  predicted items and their estimated ratings for test user."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n(predictions, n=5):\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8. Try and recommend top 5 products for test users\ntop_n = get_top_n(predictions, n=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the top 5 predicted items and their ratings for test users."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, _) in user_ratings])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Evaluate the collaborative model. Print RMSE value for SVD\nprint(\"SVD Model : Test Set\")\naccuracy.rmse(predictions, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE of SVD model is lower than for cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_Iu(uid):\n    \"\"\" return the number of items rated by given user\n    args: \n      uid: the id of the user\n    returns: \n      the number of items rated by the user\n    \"\"\"\n    try:\n        return len(trainset.ur[trainset.to_inner_uid(uid)])\n    except ValueError: # user was not part of the trainset\n        return 0\n    \ndef get_Ui(iid):\n    \"\"\" return number of users that have rated given item\n    args:\n      iid: the raw id of the item\n    returns:\n      the number of users that have rated the item.\n    \"\"\"\n    try: \n        return len(trainset.ir[trainset.to_inner_iid(iid)])\n    except ValueError:\n        return 0\n    \nbf = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\nbf['Iu'] = bf.uid.apply(get_Iu)\nbf['Ui'] = bf.iid.apply(get_Ui)\nbf['err'] = abs(bf.est - bf.rui)\nbest_predictions = bf.sort_values(by='err')[:10]\nworst_predictions = bf.sort_values(by='err')[-10:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a collaborative filtering model using kNNWithMeans from surprise using Item based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#data_II = vs_rev.sample(n=5000, random_state=612)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read dataset.\nreader = Reader(rating_scale=(1, 10))\ndata_I = Dataset.load_from_df(vs_data,reader = reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset_I, testset_I = train_test_split(data_I, test_size=.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use user_based true/false to switch between user-based or item-based collaborative filtering\nalgo = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': False})\nalgo.fit(trainset_I)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the  model against the testset\ntest_pred_I = algo.test(testset_I)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_I","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get RMSE\nprint(\"Item-based Model : Test Set\")\naccuracy.rmse(test_pred_I, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a collaborative filtering model using kNNWithMeans from surprise using User based model"},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = Reader(rating_scale=(1, 10))\ndata_U = Dataset.load_from_df(vs_data,reader = reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset_U, testset_U = train_test_split(data_U, test_size=.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use user_based true/false to switch between user-based or item-based collaborative filtering\nalgo = KNNWithMeans(k=50, sim_options={'name': 'pearson_baseline', 'user_based': True})\nalgo.fit(trainset_U)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can now query for specific predicions\nuid = 'Frances DeSimone'  # raw user id\niid = 'Samsung Galaxy Star Pro DUOS S7262 Unlocked Ce.'  # raw item id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a prediction for specific users and items.\npred = algo.predict(uid, iid, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"when, author = Frances DeSimone ,\nitem: Samsung Galaxy Star Pro DUOS S7262 Unlocked Ce.\nestimated rating is 8.03"},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the trained model against the testset\ntest_pred_U = algo.test(testset_U)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#6. Predict score (average rating) for test users\ntest_pred_U","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the prediction of user item combinations and the estimated ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Evaluate the collaborative model. Print RMSE value for User Based CF\nprint(\"User-based Model : Test Set\")\naccuracy.rmse(test_pred_U, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_df = df\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9. Check for outliers and impute them as required. \n# only score is the column which is numeric so we check it for outliers.\n#Checking for outliers in the sample of 1000000\nsns.boxplot(x= d_df['score'], color='cyan')\nplt.show()\nprint('Boxplot of score')\n#calculating the outiers in attribute \nQ1 = d_df['score'].quantile(0.25)\nQ2 = d_df['score'].quantile(0.50)\nQ3 = d_df['score'].quantile(0.75) \nIQR = Q3 - Q1\nL_W = (Q1 - 1.5 *IQR)\nU_W = (Q3 + 1.5 *IQR)    \nprint('Q1 is : ',Q1)\nprint('Q2 is : ',Q2)\nprint('Q3 is : ',Q3)\nprint('IQR is:',IQR)\nprint('Lower Whisker, Upper Whisker : ',L_W,',',U_W)\nbools = (d_df['score'] < (Q1 - 1.5 *IQR)) |(d_df['score'] > (Q3 + 1.5 * IQR))\nprint('number of outliers are:',bools.sum())   #calculating the number of outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 147884 outliers in the column score"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  function to treat outliers\n#Removing outliers by removing data below lower whisker and above upper whisker\nQ1 = d_df['score'].quantile(0.25)\nQ3 = d_df['score'].quantile(0.75)\nIQR = Q3 - Q1\nd_df = d_df[(d_df['score'] > (Q1 - 1.5 *IQR)) & (d_df['score'] < (Q3 + 1.5 *IQR))]\nbools = (d_df['score'] < (Q1 - 1.5 *IQR)) |(d_df['score'] > (Q3 + 1.5 * IQR))\nprint('number of outliers are:',bools.sum())   #calculating the number of outliers\nd_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10. Try cross validation techniques to get better results.\ncross_validate(algo,data_U, measures=['RMSE'], cv=3, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" 7. Report your findings and inferences.\nSamsung Galaxy Note5 is the most popular product \nAmazon Customer is the most active author who writes reviews.\nLenovo Vibe K4 Note (White,16GB) was rated by most of the authors\nCV rmse was 2.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"11. In what business scenario you should use popularity based Recommendation Systems ? \nAns. Popularity based recommendation system relies on the popularity,trends and frequency counts of which items were most purchased.It is used buy the travel companies selling holiday packages in a season, by Google News and other news websites to show Top Stories with images.\n"},{"metadata":{},"cell_type":"markdown","source":"12.  In what business scenario you should use CF based Recommendation Systems ? \nAns. Collaborative Filtering is used to building intelligent recommender systems that can learn to give better recommendations as more information about users is collected. It isa personalised recommender system , recommendations are made based on the past behaviour of the user. Most websites like Amazon, YouTube, and Netflix use collaborative filtering as a part of their sophisticated recommendation system."},{"metadata":{},"cell_type":"markdown","source":"13.  What other possible methods can you think of which can further improve the recommendation for diﬀerent users ?\nAns. Apart from Popularity and Collaborative Filtering , Content-based, Demographic, Utility based, Knowledge based and Hybrid recommendation system can be used as per the user needs."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}