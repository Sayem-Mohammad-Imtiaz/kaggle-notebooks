{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Context\n\nConcrete is one of the most important materials in civil engineering. Concrete's compressive strength is a highly nonlinear function of age and ingredients.\n\n# Business Problem\n\nThis is a supervised regression problem. The objective is to accurately predict the strength of steel using various ingredients used in the production of concrete.\n\n\n# Data\n\nThe dataset contains 1030 observations accross 8 input variables and an output variable. The variable name, variable type, the measurement unit and a brief description is provided.\n\n  **Name --              Data Type --      Measurement --       Description**\n\nCement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\n\nBlast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\n\nFly Ash (component 3) -- quantitative -- kg in a m3 mixture -- Input Variable\n\nWater (component 4) -- quantitative -- kg in a m3 mixture -- Input Variable\n\nSuperplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\n\nCoarse Aggregate (component 6) -- quantitative -- kg in a m3 mixture -- Input \n\nFine Aggregate (component 7) -- quantitative -- kg in a m3 mixture -- Input Variable\n\nAge -- quantitative -- Day (1~365) -- Input Variable\n\nConcrete compressive strength -- quantitative -- MPa -- Output Variable|\n\n\n# Acknowledgements\n\nOriginal Owner and Donor\n\nProf. I-Cheng Yeh\n\nDepartment of Information Management\n\nChung-Hua University\n\nCitation Request:\n\nI-Cheng Yeh, \"Modeling of strength of high performance concrete using artificial neural networks,\" Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998).\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries & Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor ","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:23.313806Z","iopub.execute_input":"2021-07-28T03:22:23.314162Z","iopub.status.idle":"2021-07-28T03:22:23.927607Z","shell.execute_reply.started":"2021-07-28T03:22:23.314086Z","shell.execute_reply":"2021-07-28T03:22:23.926418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Dataset\ndf = pd.read_csv(r\"../input/regression-with-neural-networking/concrete_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:23.929934Z","iopub.execute_input":"2021-07-28T03:22:23.930283Z","iopub.status.idle":"2021-07-28T03:22:23.944627Z","shell.execute_reply.started":"2021-07-28T03:22:23.930248Z","shell.execute_reply":"2021-07-28T03:22:23.943295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"# 1.1 Univariate Analysis","metadata":{}},{"cell_type":"code","source":"# Any missing values ?\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:23.946732Z","iopub.execute_input":"2021-07-28T03:22:23.947068Z","iopub.status.idle":"2021-07-28T03:22:23.962354Z","shell.execute_reply.started":"2021-07-28T03:22:23.947038Z","shell.execute_reply":"2021-07-28T03:22:23.960944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are the datatypes of our variables ?\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:23.964164Z","iopub.execute_input":"2021-07-28T03:22:23.964653Z","iopub.status.idle":"2021-07-28T03:22:23.982339Z","shell.execute_reply.started":"2021-07-28T03:22:23.964602Z","shell.execute_reply":"2021-07-28T03:22:23.980928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary statistics\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:23.984113Z","iopub.execute_input":"2021-07-28T03:22:23.984574Z","iopub.status.idle":"2021-07-28T03:22:24.033187Z","shell.execute_reply.started":"2021-07-28T03:22:23.984502Z","shell.execute_reply":"2021-07-28T03:22:24.031925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All predictors in the dataset are quantitative datatypes. Moreover, there are no missing values in the data. Lets analyse the probability distribution of the variables through a series of univariate density plots. ","metadata":{}},{"cell_type":"code","source":"# Independent and Dependent variables\nX = df[1:]\ny = df['Strength']","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:24.035034Z","iopub.execute_input":"2021-07-28T03:22:24.035486Z","iopub.status.idle":"2021-07-28T03:22:24.042459Z","shell.execute_reply.started":"2021-07-28T03:22:24.035436Z","shell.execute_reply":"2021-07-28T03:22:24.039947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kernal Density plots to see the distribution of variables\nsns.set(style=\"white\") \nplt.figure(figsize = (20 , 20))\nfor variable in range(9):\n    plt.subplot(4, 3, variable + 1)\n    sns.kdeplot(df[list(X)[variable]], shade = True, color=\"green\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:24.044195Z","iopub.execute_input":"2021-07-28T03:22:24.044546Z","iopub.status.idle":"2021-07-28T03:22:26.02831Z","shell.execute_reply.started":"2021-07-28T03:22:24.044489Z","shell.execute_reply":"2021-07-28T03:22:26.027051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most variables do not follow normal distribution. It would be interesting to convert the response variable into a category of 'optimal' and 'suboptimal' in order to detect any interesting patterns between the predictors and the target variable. I'll conisder the observations at the 75th percentile or greater to be good.","metadata":{}},{"cell_type":"markdown","source":"# 1.2 Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"# Observations at the 75th percentile (46) are considered optimal. Anything lower than 46 is suboptimal.\ndf['target_binary'] = np.where( df.Strength > 46, 'optimal', 'suboptimal')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:26.032215Z","iopub.execute_input":"2021-07-28T03:22:26.032755Z","iopub.status.idle":"2021-07-28T03:22:26.040893Z","shell.execute_reply.started":"2021-07-28T03:22:26.032701Z","shell.execute_reply":"2021-07-28T03:22:26.039495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boxplots of good and suboptimal compressive strength against ingredients.\nsns.set(style=\"white\") \nplt.figure(figsize = (20 , 60))\nfor variable in range(8):\n    plt.subplot(15,3 , variable + 1)\n    sns.boxplot(x = df['target_binary'], y =df[list(X)[variable]],  palette='Set1' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:26.043302Z","iopub.execute_input":"2021-07-28T03:22:26.043836Z","iopub.status.idle":"2021-07-28T03:22:27.167915Z","shell.execute_reply.started":"2021-07-28T03:22:26.043785Z","shell.execute_reply":"2021-07-28T03:22:27.166662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some intersting patterns between cement, water, superplasticizer and strength. Lets further split the target variable into 4 categories good, better, great, perfect to further refine this visualization.","metadata":{}},{"cell_type":"code","source":"# Creating 3 bins out of the response variable.\ndf['target_cat'] = pd.cut(df.Strength,\n                     bins=[0, 23, 34, 46, 82],\n                     labels=[\"Good\", \"Better\", \"Great\", \"Perfect\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:27.170437Z","iopub.execute_input":"2021-07-28T03:22:27.170803Z","iopub.status.idle":"2021-07-28T03:22:27.180317Z","shell.execute_reply.started":"2021-07-28T03:22:27.170769Z","shell.execute_reply":"2021-07-28T03:22:27.179105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boxplots of good and bad compressive strength against ingredients.\nsns.set(style=\"white\") \nplt.figure(figsize = (20 , 60))\nfor variable in range(9):\n    plt.subplot(15,3 , variable + 1)\n    sns.boxplot(x = df['target_cat'], y = df[list(X)[variable]],  palette='Set1' )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:27.18196Z","iopub.execute_input":"2021-07-28T03:22:27.182433Z","iopub.status.idle":"2021-07-28T03:22:29.084825Z","shell.execute_reply.started":"2021-07-28T03:22:27.182392Z","shell.execute_reply":"2021-07-28T03:22:29.083631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seem to be some interesting patterns between the ingredients and the strength of the product. Greater quantities of cement and superplasticizer and lower amounts of water seems to correlate with a stronger product. The boxplot also indicates the presence of outliers. Lets create a pearsons correlation correlogram to quantitatively detect these associations.  ","metadata":{}},{"cell_type":"code","source":"# Lets make subsets of the independent and dependent variables\nX =  df.drop(columns=['Strength', 'target_cat', 'target_binary'])\ny =  df['Strength']","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:29.086302Z","iopub.execute_input":"2021-07-28T03:22:29.086656Z","iopub.status.idle":"2021-07-28T03:22:29.093518Z","shell.execute_reply.started":"2021-07-28T03:22:29.08662Z","shell.execute_reply":"2021-07-28T03:22:29.092171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets deploy a correlogram for colrrelation\ncolormap = plt.cm.Blues\nplt.figure(figsize=(10,8))\nsns.heatmap(df.corr(), cmap=colormap, annot=True, linewidths=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:29.095045Z","iopub.execute_input":"2021-07-28T03:22:29.095449Z","iopub.status.idle":"2021-07-28T03:22:29.940368Z","shell.execute_reply.started":"2021-07-28T03:22:29.095324Z","shell.execute_reply":"2021-07-28T03:22:29.939169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pearsons correlation assumes normality, linearity, homoscedasticity and no outliers. These assumptions are violated for most variables. This may be a potential reason as to why pearsons correlation won't detect nuances between predictors and the independent variables properly. Additionally, the relationship between inputs does seem to be largely non-linear. Nonetheless, cement, water, and superplastisizer do seem to be moderately correlated with strength.","metadata":{}},{"cell_type":"markdown","source":"# 2) Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Some machine learning algorithms require features to be scaled through normalization or standardization. I will now split the dataset into training and testing and then perform standardization. This will tansform the features such that their mean and standard deviation will become 0 and 1.","metadata":{}},{"cell_type":"code","source":"# Split the dataset into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 43)\n\n# Feature Scaling \nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test =sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:22:29.941994Z","iopub.execute_input":"2021-07-28T03:22:29.942412Z","iopub.status.idle":"2021-07-28T03:22:29.962094Z","shell.execute_reply.started":"2021-07-28T03:22:29.942373Z","shell.execute_reply":"2021-07-28T03:22:29.960724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Predictive Modelling","metadata":{}},{"cell_type":"markdown","source":"Lets apply a number of linear and non linear regression models and retrieve their predictive accuracy.","metadata":{}},{"cell_type":"code","source":"# Store models in a dictionary\nalgorithms = {\"KNN\": KNeighborsRegressor(),\n          \"Linear Regression\": LinearRegression(), \n          \"Ridge Regression\": Ridge(),\n          \"Random Forest\": RandomForestRegressor(),\n          \"SVR RBF\": SVR(),\n          \"Linear SVR\": LinearSVR(),\n          \"Decision Tree\":DecisionTreeRegressor(),\n          \"Adaboost\" :AdaBoostRegressor(),\n          \"Gradient Boosting\":GradientBoostingRegressor(),\n          \"Neural Network\": MLPRegressor(max_iter=10000) ,\n          \"XGBRegressor\" : XGBRegressor()}\n\n\n# Create function to train and test the model\ndef train_and_test(algorithms, X_train,y_train,X_test,y_test):\n    model_scores = {}\n    for name, model in algorithms.items():\n        model.fit(X_train, y_train)\n        print(name + \" R2: {:.2f}\".format(model.score(X_test, y_test)))\n\n# Training and testing\nmodel_scores = train_and_test(algorithms, X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:38:11.612627Z","iopub.execute_input":"2021-07-28T03:38:11.613022Z","iopub.status.idle":"2021-07-28T03:38:22.165649Z","shell.execute_reply.started":"2021-07-28T03:38:11.612989Z","shell.execute_reply":"2021-07-28T03:38:22.164669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The non-linear algorithms perform far better than their linear counterparts. Amongst the non-linear models XGBoost seems to outperform all other models. I will now set a grid search and perform hyperparametric tuning to try and further enhance its performance.","metadata":{}},{"cell_type":"code","source":"# XGboost on the default settings seems to outperform all other models.\nestimator = XGBRegressor()\nestimator.fit(X_train, y_train)\nprint(\"R2: {:.2f}\".format(estimator.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:37:40.07883Z","iopub.execute_input":"2021-07-28T03:37:40.081125Z","iopub.status.idle":"2021-07-28T03:37:40.2357Z","shell.execute_reply.started":"2021-07-28T03:37:40.081057Z","shell.execute_reply":"2021-07-28T03:37:40.234657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets set up a gridsearch CV and optimize some hyperparameters\nparam_grid       =      {\"learning_rate\": (0.05, 0.10, 0.15, 0.2),\n                         \"max_depth\": [5, 6, 8],\n                         \"min_child_weight\": [ 5, 7, 9, 11],\n                         \"gamma\":[ 0.0, 0.1, 0.2, 0.25],\n                         \"colsample_bytree\":[ 0.3, 0.4, 0.5, 0.7],\n                          \"n_estimators\": [1000]}\n\n# GridSearchCV\noptimized_estimator =  GridSearchCV(estimator, param_grid)\noptimized_estimator.fit(X_train, y_train)\n\n# Retrieve Best Parameters\noptimized_estimator.best_params_\nfor i, j in optimized_estimator.best_params_.items():\n    print(\"\\nBest \" + str(i) + \" parameter: \" +  str(j))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:54:24.75047Z","iopub.execute_input":"2021-07-28T03:54:24.751127Z","iopub.status.idle":"2021-07-28T04:43:40.899605Z","shell.execute_reply.started":"2021-07-28T03:54:24.751085Z","shell.execute_reply":"2021-07-28T04:43:40.898155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"XGBoost R2 after hyperparametric Tuning: {:.2f}\".format(optimized_estimator.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T04:50:29.996402Z","iopub.execute_input":"2021-07-28T04:50:29.997105Z","iopub.status.idle":"2021-07-28T04:50:30.0135Z","shell.execute_reply.started":"2021-07-28T04:50:29.997045Z","shell.execute_reply":"2021-07-28T04:50:30.012602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model seems to have improved very slightly. ","metadata":{}},{"cell_type":"markdown","source":"# 4) Conclusion\n\nThis notebook performed supervised learning to predict the compressive strength of concrete using various ingredients used to produce concrete. The tasks carried out in this report inlclude exploratory data analysis through visualizations and statistics, followed by preprocessing, modelling and hyperparametric tuning. There is still room for imporvement but I'll stop here for now. ","metadata":{}},{"cell_type":"markdown","source":"# **Thank you for reading ! :D**","metadata":{}}]}