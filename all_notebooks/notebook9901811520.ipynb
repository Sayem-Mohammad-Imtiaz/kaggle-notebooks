{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport re\nimport time\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('source',axis=1)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unwanted_idx=[]\nx=re.compile(r'\\d+')\nfor idx,cols in data.iterrows():\n    try:\n        if x.match(cols['english_sentence']):\n            unwanted_idx.append(idx)\n    except:\n        print(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(unwanted_idx + [37554,59804],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_d={}\nfor text in data.english_sentence:\n    l=len(text.split(' '))\n    eng_d.setdefault(l,0)\n    eng_d[l]+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_dic={k:v for k,v in sorted(eng_d.items(),key=lambda x: x[1],reverse=True)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_dic[10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_d={}\nfor text in data.hindi_sentence:\n    l=len(text.split(' '))\n    hindi_d.setdefault(l,0)\n    hindi_d[l]+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_dic={k:v for k,v in sorted(hindi_d.items(),key=lambda x: x[1],reverse=True)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_dic[10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unwanted_len_id=[]\nfor idx,cols in data.iterrows():\n    if len(cols.english_sentence.split(' ')) > 10 or len(cols.hindi_sentence.split(' ')) > 10:\n        unwanted_len_id.append(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(unwanted_len_id,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=data.iloc[:25000]\nval=data.iloc[25000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.to_numpy()\nval=val.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english=train[:,0]\nhindi=train[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_and_tokenize(language):\n    pattern=r'[!\"#$%&()*,+-./:;\\[\\]<=>?@\\\\^_`{|}~\\t\\n\\d“”]'\n    language=tf.strings.lower(language)\n    language=tf.strings.regex_replace(language,pattern,'')\n    language=tf.strings.strip(language)\n    \n    lang=[]\n    for text in language:\n        lang.append('<sos> '+ text.numpy().decode('utf-8') + ' <eos>')\n            \n    lang=np.array(lang)\n    tokenizer=keras.preprocessing.text.Tokenizer(filters='',split=' ')\n    tokenizer.fit_on_texts(lang)\n    tensor=tokenizer.texts_to_sequences(lang)\n    \n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post',value=0)\n\n    return tensor,tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_tokenized,eng_tokenizer=preprocess_and_tokenize(english)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_tokenized,hindi_tokenizer=preprocess_and_tokenize(hindi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=tf.data.Dataset.from_tensor_slices((tf.Variable(eng_tokenized),tf.Variable(hindi_tokenized))).shuffle(10000).batch(128).prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(keras.Model):\n    def __init__(self,vocab_size=10000,emb_dim=128,units=256,batch_size=64):\n        super(Encoder,self).__init__()\n        self.units = units\n        self.batch = batch_size\n        self.emb_layer = keras.layers.Embedding(vocab_size,emb_dim)\n        self.lstm = keras.layers.LSTM(self.units,return_sequences=True,return_state=True)\n        \n    def call(self,x,states):\n        emb=self.emb_layer(x)\n        output,hidden,carry=self.lstm(emb,initial_state=states)\n        return output,hidden,carry\n    \n    def init_hidden_state(self):\n        return tf.zeros((self.batch,self.units)),tf.zeros((self.batch,self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(keras.Model):\n    def __init__(self,vocab_size=10000,emb_dim=128,units=256,batch_size=64):\n        super(Decoder,self).__init__()\n        self.units = units\n        self.batch = batch_size\n        self.emb_layer = keras.layers.Embedding(vocab_size,emb_dim)\n        self.lstm = keras.layers.LSTM(self.units,return_sequences=True,return_state=True)\n        self.fc=keras.layers.Dense(vocab_size)\n        \n    def call(self,x,states):\n        emb=self.emb_layer(x)\n        output,hidden,carry=self.lstm(emb,initial_state=states)\n        output=self.fc(output)\n        return output,hidden,carry","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer=keras.optimizers.Adam()\nloss=keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\naccuracy=keras.metrics.SparseCategoricalAccuracy()\ndef loss_fn(true,pred):\n    mask = tf.math.logical_not(tf.math.equal(true, 0))\n    loss_=loss(true,pred)\n    mask=tf.cast(mask,dtype=loss_.dtype)\n    loss_*=mask\n    return tf.reduce_mean(loss_)\n\ndef update_accuracy(true,pred):\n    accuracy.update_state(true,pred)\n    \ndef get_accuracy():\n    accuracy_=accuracy.result().numpy()\n    return accuracy_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_vocab_size=len(eng_tokenizer.index_word) + 1\ndec_vocab_size=len(hindi_tokenizer.index_word) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder=Encoder(enc_vocab_size,256,512,128)\ndecoder=Decoder(dec_vocab_size,256,512,128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_tokenizer.index_word[0]='<pad>'\neng_tokenizer.word_index['<pad>']=0\nhindi_tokenizer.index_word[0]='<pad>'\nhindi_tokenizer.word_index['<pad>']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train(input,target,enc_hidden):\n    loss__=0.0\n    with tf.GradientTape() as tape:\n        enc_output,enc_h,enc_c=encoder(input,enc_hidden)\n        enc_states=[enc_h,enc_c]\n        dec_input=tf.expand_dims(target[:,0],1)\n        \n        for t in range(1,target.shape[1]):\n            dec_output,_,_=decoder(dec_input,enc_states)\n            loss__+=loss_fn(target[:,t],dec_output)\n            #update_accuracy(target[:,t],dec_output)\n            dec_input = tf.expand_dims(target[:, t], 1)\n        \n    batch_loss=loss__/int(target.shape[1])\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients=tape.gradient(loss__,variables)\n        \n    optimizer.apply_gradients(zip(gradients,variables))\n\n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=50\nsteps_per_epoch=25000//128 #batches\nfor epoch in range(epochs):\n    start=time.time()\n    enc_hidden=encoder.init_hidden_state()\n    total_loss=0.0\n    for (batch,(inp,tar)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss=train(inp,tar,enc_hidden)\n        total_loss+=batch_loss\n    #print('Epoch {}      Accuray {:.2f}   Avg. Loss {:.4f}'.format(epoch + 1,float(accuracy.result()),total_loss/steps_per_epoch),end='  ')\n    print('Epoch {}      Avg. Loss {:.4f}'.format(epoch + 1,total_loss/steps_per_epoch),end='  ')\n    print('Time Taken: {:.1f} sec'.format(time.time() - start))\n    accuracy.reset_states()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(input):\n    hidden=[tf.zeros((1,512)),tf.zeros((1,512))]\n    _,enc_h,enc_c=encoder(input,hidden)\n    enc_states=[enc_h,enc_c]\n    result=[]\n    dec_input = tf.expand_dims(input[:,0], 0)\n    for t in range(input.shape[1]):\n        dec_output,_,_=decoder(dec_input,enc_states)\n        output_id=tf.math.argmax(dec_output[0],-1)\n        output_id=output_id[0].numpy()\n        if output_id == hindi_tokenizer.word_index['<eos>']:\n            return ' '.join(result)\n        dec_input = tf.expand_dims([output_id], 0)\n        result.append(hindi_tokenizer.index_word[output_id])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(text):\n    pattern=r'[!\"#$%&()*,+-./:;\\[\\]<=>?@\\\\^_`{|}~\\t\\n\\d“”]'\n    text=tf.strings.lower(text)\n    text=tf.strings.regex_replace(text,pattern,'')\n    text=tf.strings.strip(text)\n    return text.numpy().decode(\"utf-8\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_translation(input_text):\n    length=12\n    eng_processed=['<sos>',]\n    for i in preprocess_input(input_text).split(' '):\n        if i != '':\n            eng_processed.append(i)\n    eng_processed.append('<eos>')\n    eng_tokenized=[eng_tokenizer.word_index[i] for i in eng_processed]\n    eng_tokenized=tf.keras.preprocessing.sequence.pad_sequences([eng_tokenized],maxlen=length,padding='post',value=0)\n    hindi_pred=predict(eng_tokenized)\n    print('Pred. Hindi: ',hindi_pred,end='\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [1,3,10,15,18,19,21,23,24,26,28,33,34,36,38,45 ]:\n    print('No. ',i)\n    print('English:     ',english[i])\n    print('Hindi:       ',hindi[i])\n    get_translation(str(english[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint.save(file_prefix = checkpoint_prefix)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}