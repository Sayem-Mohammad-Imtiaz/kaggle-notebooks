{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8ff0a4c0-5147-fdec-6744-ae00af908ebf"},"source":"Hello everyone, in this kernel I want to share how I did to predict the  final grade of math thanks to features.\n\n----------\n\n\n**Please if you have remark or you see mistake tell me in order to correct them.** "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4f67615-042a-cce8-abb1-b0c61885b6bf"},"outputs":[],"source":"#\n### Packages importation\n#\n\n# Packages to manipulate our data\nimport pandas as pd\nimport numpy as np\n\n# Packages to plot our data \nimport matplotlib\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\n# Packages to use de cross-validation in order to see the precision of our models\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Packages to use the gridsearch function in order to parametrize your model\nfrom sklearn.model_selection import GridSearchCV\n\n# Packages which contain models to predict our final grade\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\n\n#\n### Dataset importation\n#\n\ndf = pd.read_csv(\"../input/student-mat.csv\",sep=\",\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"11bcbff2-d45c-651d-1c2a-1e9b5ec18dda"},"source":"**Data Importation**\n--------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16ac3b77-4b0e-2db9-3852-91c6b963fd0a"},"outputs":[],"source":"df = pd.read_csv(\"../input/student-mat.csv\",sep=\",\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f5ea339-d71a-b3e6-6a28-809459ecfe89"},"source":"**First insight of the dataset**\n--------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5297bab-9065-fa4d-c855-b15d53f8a91e"},"outputs":[],"source":"df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"edb9a5aa-50bd-fe35-da38-fa3a53813aac"},"source":"**Distribution of the target variable**\n---------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fab9b89-c2f2-754b-102e-305e9c22482a"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"grade\":df[\"G3\"]})\nprices.hist()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f99a6fd8-5773-03de-4466-dc4244bc3eb2"},"source":"**Change our qualitative variables into binairie variables**\n------------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"513594a2-0f2a-cd29-d206-1a582a4f858d"},"outputs":[],"source":"df = pd.get_dummies(df)\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ff274d1-8e52-3c6d-a605-9e7ccfda5540"},"source":"## Separate our target from the other features ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"146cb2ca-3abf-7fb8-286c-ed321b98e298"},"outputs":[],"source":"df_X = df.drop(['G1','G2','G3'],1)\ndf_Y = df[['G3']]"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2165751-777b-9e45-cf96-54fba1bab414"},"source":"## First model ##\n\nThe first step in any machine learning problem is to start with a simple model. Like that you will have a base and see you improvement at each step."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e4f7e78-23be-a805-0549-82227367f80c"},"outputs":[],"source":"# In this we will initialize all the parameter we want to test.\n# We know that alpha and l1_ratio will not have a great value\n\nparam_grid = [\n  {'alpha': [0.1,0.2,0.3,0.4,0.5,1,10], 'l1_ratio': [0.0001,0.001,0.01,0.1,1,10]  }\n ]\n\n# This function will test all the parameter initialize above and check the best parameter by a cross-validation.\nbest_param = GridSearchCV(ElasticNet(), param_grid , cv=5).fit(df_X,df_Y).best_params_\n\n# We initialize our model with our best parameters \nlr_1 = ElasticNet( alpha = best_param['alpha'] , l1_ratio = best_param['l1_ratio'] )\n\n# We will see the performance of our model with a cross-validation of 5.\nscores = cross_val_score(lr_1, df_X, df_Y, cv=10)\n\n\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8298064-ffd0-3061-948c-a908de2f238a"},"source":"##Features selection ##\nHere we will try to keep only features usefull to predict the final grade\n\n\n----------\nWe will try to delete features with lot of correlation with each other. \nFor exemple we have sex_men and sex_women maybe it is not usefull to keep both. Because if sex_men = 0 we know that sex_women = 1."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f833fcae-aedf-7990-7af5-d10439239904"},"outputs":[],"source":"dict_corr = dict()\n\ndf_rmv = df.drop(['G1','G2','G3'],1)\n\n# I write this function to put in dict_corr features with more than 0.9 correlation with each other\nfor i in df_rmv.columns :\n    if (df_rmv.corr()[i].abs().sort_values(ascending=False).drop(i)[0] > 0.9) :\n        dict_corr[i] = df_rmv.corr()[i].abs().sort_values(ascending=False).drop(i).index[0]\n        \nsuppr = list()\n\n# When two features have more than 0.9 corr this function delete just one of them\nfor i in dict_corr :\n    if not(i in suppr) and not(dict_corr[i] in suppr) :\n        suppr.append(i)\n        \ndf_test = df.drop(suppr,1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8a51775-550a-c1bd-e9d0-daefe9ea3073"},"outputs":[],"source":"# Now we will see if this improve our R square\n\nscores = cross_val_score(ElasticNet( alpha = best_param['alpha'] , l1_ratio = best_param['l1_ratio'] ),df_test.drop(['G1','G2','G3'],1), df[['G3']], cv=10)\n\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e0070e60-df0a-49be-3767-d3a953f4c016"},"source":"We see that this way to reduce the number of features is not relevant.\n\n\n----------\n\nNow we will calculate the contribution of every features to the target. And we will create model with the features which gave us the best contribution and after we will create another model with the first two contributions.  And we will continue with all the features. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f08253b-3f45-1b8c-5aef-4bd351092a2e"},"outputs":[],"source":"# We calcul the std in advance for the target\nstd_target_1 = df['G3'].std()\n\n# We calcul the cov and take only for 'G3'\ncontrib_1 = df.cov()['G3'].drop(['G1','G2','G3'])\n\n# We will compute the contribution for all features\nfor i in contrib_1.index :\n    std_i = df[i].std()\n    if std_i != 0 :\n        contrib_1[i] = (contrib_1[i]/(std_target_1*std_i ))\n    else :\n        contrib_1[i] = 0\n        \n# Now we take the absolute value and sort or vector\ncontrib_1 = contrib_1.abs().sort_values(ascending=False)\n\n# We will create a dictionnary for all the number of variables and the performance associated.\nperformance = {}\n\nfor i in range(1,len(contrib_1)) :\n    df_X = df[contrib_1.index[list(range(0,i))]]\n    performance[i] = cross_val_score(ElasticNet( alpha = best_param['alpha'] , l1_ratio = best_param['l1_ratio'] ), df_X, df_Y, cv=5).mean()\n    \n# Here we will take the model with the best performance \nvar = contrib_1.index[list(range(0,max(performance, key=performance.get)))]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da121f41-61be-7590-45f0-684ffcc823ac"},"outputs":[],"source":"# We select our new set of data\ndf_test = df[var]\n\n# Now we will see if this improve our R square\n\nscores = cross_val_score(ElasticNet( alpha = best_param['alpha'] , l1_ratio = best_param['l1_ratio'] ),df_test, df_Y , cv=5)\n\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"047c206d-1c2d-3fdf-f652-31f59f7ee8f2"},"source":"This way to select features is relevant."},{"cell_type":"markdown","metadata":{"_cell_guid":"80c71b9a-6e74-2374-00b8-c171c8bfac89"},"source":"## MidTerm grade ##\n\nWe can see before that with that kind of features we just have 0.12 R square. We can predict only 12% of the variance of the data is very low. In front of that we can see that without midterm grade we can not predict the final grade. \nIn the following "},{"cell_type":"markdown","metadata":{"_cell_guid":"def0802c-aa19-e3cf-8f37-fd7eb07230d2"},"source":"## Seperate our target from features ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c2bd734-f6b2-1070-72e8-79723ea4bd7d"},"outputs":[],"source":"df_X = df.drop(['G3'],1)\ndf_Y = df[['G3']]"},{"cell_type":"markdown","metadata":{"_cell_guid":"9b51e09a-df3c-a173-baf8-52da9d08eabf"},"source":"## Best parameters ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe425b1d-bf49-7b4e-3eed-fbc596abaf07"},"outputs":[],"source":"param_grid = [\n  {'alpha': [0.01,0.05,0.1,0.15,0.2,1,10], 'l1_ratio': [0.0001,0.001,0.01,0.1,0.5,1,3,5,10]  }\n ]\n\n# This function will test all the parameter initialize above and check the best parameter by a cross-validation.\nbest_param = GridSearchCV(ElasticNet(), param_grid , cv=5).fit(df_X,df_Y).best_params_"},{"cell_type":"markdown","metadata":{"_cell_guid":"444a587c-f10c-8737-a379-964dd31ffa1f"},"source":"## Features selection ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36251951-ec06-35a9-2b1e-bacc2beb077c"},"outputs":[],"source":"# We calcul the std in advance for the target\nstd_target_1 = df_Y.std()\n\n# We calcul the cov and take only for 'G3'\ncontrib_1 = df.cov()['G3'].drop(['G3'])\n\n# We will compute the contribution for all features\nfor i in contrib_1.index :\n    std_i = df[i].std()\n    if std_i != 0 :\n        contrib_1[i] = (contrib_1[i]/(std_target_1*std_i ))\n    else :\n        contrib_1[i] = 0\n        \n# Now we take the absolute value and sort or vector\ncontrib_1 = contrib_1.abs().sort_values(ascending=False)\n\n# We will create a dictionnary for all the number of variables and the performance associated.\nperformance = {}\n\nfor i in range(1,len(contrib_1)) :\n    df_X = df[contrib_1.index[list(range(0,i))]]\n    performance[i] = cross_val_score(ElasticNet( alpha = best_param['alpha'] , l1_ratio = best_param['l1_ratio'] ), df_X, df_Y, cv=5).mean()\n    \n# Here we will take the model with the best performance \nvar = contrib_1.index[list(range(0,max(performance, key=performance.get)))]\n\n# We select our new set of data\ndf_X = df[var]\n\n# Now we will see if this improve our R square\n\nscores = cross_val_score(ElasticNet( alpha = best_param['alpha'] , l1_ratio = best_param['l1_ratio'] ),df_X, df_Y , cv=5)\n\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"83a8122c-6a40-ce31-d11a-9dacce3a87d1"},"source":"## Support Vector Machine ##\n\nNow we had select our variable and we have a base score of 0.814, we can test other model to see if we can increase or R square."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de107d4f-585b-7f4a-0545-b2c286a88c0c"},"outputs":[],"source":"param_grid = [\n  {'C': [29,29.5,30,30.5,31,31.5,32], 'gamma' : [0.005,0.007,0.008,0.009,0.01,0.015] }\n]\n\nbest_param = GridSearchCV(svm.SVR(kernel='rbf',epsilon=0.0001), param_grid , cv=5).fit(df_X,np.ravel(df_Y)).best_params_\n\n\nscores = cross_val_score(svm.SVR(C=best_param['C'],epsilon=0.002,gamma=best_param['gamma'],kernel='rbf'), df_X, np.ravel(df_Y), cv=5)\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4fb0f4a2-260d-f2b4-bc12-010a2d48d954"},"source":"## Neural network ##\nThis kind of model is based on linear function, so we can use our previous seleciton of features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83c1b048-5334-e6e9-1797-62fa449c1be3"},"outputs":[],"source":"NN_1 = MLPRegressor(alpha=41,hidden_layer_sizes=(1000,1000,1000))\n\nscores = cross_val_score(NN_1, df_X, np.ravel(df_Y), cv=5)\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc43aa40-930b-d102-482a-da4efd4c3249"},"source":"## Random forest ##\n\nNow we will test the random forest model. This model is not linear so we have to find a different way to select features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed48c69d-5a2c-4800-77bf-46c1a5bea1f4"},"outputs":[],"source":"df_X = df.drop(['G3'],1)\ndf_Y =  df[['G3']]\n\nparam_grid = [ {'n_estimators': [30,35,40,45,50,100], 'max_depth' : [4,5,6,7,8,9,10] }]\nbest_param = GridSearchCV(RandomForestRegressor(), param_grid , cv=5).fit(df_X,np.ravel(df_Y)).best_params_\n\nscores = cross_val_score(RandomForestRegressor(max_depth =best_param['max_depth'],n_estimators=best_param['n_estimators']), df_X, np.ravel(df_Y), cv=5)\n\nprint('Average of our R square : {0}\\nVariance of our R square : {1} '.format(scores.mean(),scores.var()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c57809d1-6d50-bb44-9a3c-7726f614cd4a"},"outputs":[],"source":"nombre = 1\nparams = {\"objective\": \"reg:linear\", \"booster\":\"gblinear\"}\ndf = pd.get_dummies(df)\n    \n    \nfor i in range(0,nombre) :\n    cut = df[int((len(df)/nombre)*i):int((len(df)/nombre)*(1+i))]\n    train, test = train_test_split(cut,test_size=0.1)\n                \n    train_X = train.drop(['G3'],1)\n    train_Y = train[['G3']]\n    test_X = test.drop(['G3'],1)\n    test_Y = test[['G3']]              \n\n    T_train_xgb = xgb.DMatrix(train_X, train_Y)\n    T_test_xgb = xgb.DMatrix(test_X, test_Y)\n    \n    gbm = xgb.train(dtrain=T_train_xgb,params=params)\n\n    Y_pred = gbm.predict(T_test_xgb)\n    print(r2_score(test_Y,Y_pred))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}