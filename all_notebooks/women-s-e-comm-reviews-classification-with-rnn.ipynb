{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.utils import shuffle\nimport string\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.__version__ # newest version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the data, Womens Clothing E-Commerce Reviews.csv, into memory.","metadata":{}},{"cell_type":"code","source":"data_path = '../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv'\ndf = pd.read_csv(data_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Concatenate the Title, Review Text, Division Name, Department Name and Class Name as a new feature of Reviews.","metadata":{}},{"cell_type":"code","source":"df.drop(df.columns[0],inplace=True, axis=1)\ndf = df[['Title', 'Review Text', 'Division Name', 'Department Name', 'Class Name', 'Recommended IND']]\n\n# see if label has any null values, and it doesn't\ndf['Recommended IND'].isnull().values.any()\n\n# fill the nan in features with ''(empty string)\ndf = df.fillna('')\n\n# concatenate\ndf['Reviews'] = df['Title'] + ' ' + df['Review Text'] + ' ' + df['Division Name'] + ' ' + df['Department Name'] + ' ' + df['Class Name']\n\n# remove the title review text, division name, department name and class name columns\ndf = df[['Reviews', 'Recommended IND']]\n\n# shuffle the data frame\ndf = shuffle(df, random_state=2021)\n\n# remove punctuation\ndf[\"Reviews\"] = df['Reviews'].str.replace('[{}]'.format(string.punctuation), '')\n\n# lower-case everything\ndf['Reviews'] = df['Reviews'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the entire dataset's unique words and its frequency\ntotal_words = df['Reviews'].str.split()\n\ntotal_words.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_word_set = set()\ntotal_words.apply(total_word_set.update)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_word_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word frequency distribution\nfrom collections import Counter\n\ncount_dict = Counter(total_word_set)\nVOCAB_SIZE = len(count_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorize text\n# sequence_length = 100\n\nencoder = TextVectorization(max_tokens = VOCAB_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert the df(data frame) to a tf dataset","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices(\n           ( tf.cast(df['Reviews'].values, tf.string),\n            tf.cast(df['Recommended IND'].values, tf.int32)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.element_spec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print out an instance in the dataset\nfor example, label in dataset.take(1):\n  print('text: ', example.numpy())\n  print('label: ', label.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = int(len(dataset)*0.7)\n\ntrain_dataset = dataset.take(TRAIN_SIZE)\ntest_dataset = dataset.skip(TRAIN_SIZE) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print out a training example and a test example\n\nprint('========== TRAINING EXAMPLE','='*50)\nfor sentence, label in train_dataset.take(1):\n    print('text: ', sentence.numpy())\n    print('label: ', label.numpy())\nprint()    \nprint('========== TEST EXAMPLE', '='*54)\nfor sentence, label in test_dataset.take(1):\n    print('text: ', sentence.numpy())\n    print('label: ', label.numpy())\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning the tring and test dataset\n\n    # previous buffer_size hyperparameter BUFFER_SIZE = 10000\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16\n\ntrain_dataset = train_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#since we set the batch_size=16, when we take out 1, which means 1 batch (16 obs/rows)\n# features, labels\nfor example, label in train_dataset.take(1):\n  print('texts: ', example.numpy()[:3])\n  print()\n  print('labels: ', label.numpy()[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a baseline model of Word embeddings to forecast the Recommended IND based on Reviews using deep learning.","metadata":{}},{"cell_type":"markdown","source":"## Create the text encoder\n\nThe raw text needs to be processed before it can be used in the model. I use the ***experimental.preprocessing.TextVectorization*** layer. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nVOCAB_SIZE = 1000\nencoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n# We only need to convert features (NOT label) ot int\nencoder.adapt(train_dataset.map(lambda text, label: text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the first 20 tokens\nnp.array(encoder.get_vocabulary())[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create embedding layer","metadata":{}},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),\n                                            output_dim=64,\n                                            mask_zero=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create, train and complie the base model","metadata":{}},{"cell_type":"code","source":"embedding_dim=16\n\nmodel1 = tf.keras.Sequential([\n    encoder,\n    embedding_layer,\n    GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(\n    train_dataset,\n    validation_data=test_dataset, \n    epochs=15,\n    callbacks=[tensorboard_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = model1.evaluate(test_dataset)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))\n\n# The accuracy of this model is only 18%, which is not great at all. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a second model of RNN using a bidirectional LSTM to forecast the Recommended IND based on Reviews","metadata":{}},{"cell_type":"code","source":"model2 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='accuracy', mode='max', patience=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history2 = model2.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset, \n                    validation_steps=30,\n                    callbacks = [early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss2, test_acc2 = model2.evaluate(test_dataset)\n\nprint(f'Test Loss: {round(test_loss2,2)}')\nprint(f'Test Accuracy: {round(test_acc2,4)*100}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_graphs(history2, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history2, 'loss')\nplt.ylim(0,None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run a test with some new reviews on model2\n\n- if the prediction is >= 0.0, it is positive reveiw, otherwise, negative","metadata":{}},{"cell_type":"code","source":"def make_prediction(text):\n    prediction = model2.predict(np.array([text]))\n    if prediction >= 0.0:\n        return 'a positive review.'\n    else:\n        return 'a negative review.'\n\n\n# run a few validation predictions:\nreview_1 = ('The shirt is cool. The print on the shirt '\n               'is so much fun. I would recommend this product.') #true label = 1, positive review\n\nreview_2 = ('The pattern is hedious, and the fit is weird. '\n                 'I would not recommend this to anyone.') #true label = 0, negative review\n \nreview_3 = ('So happy! I order size M because I have my belly and it works perfect, '\n                 'the waist is wide helping to control.') # true label = 1, positive review\n\nreview_4 = ('True to size and comfy! The inner lining is soft and dry-fit while the outside is a bit more like a windbreaker material. '\n                 'I like that they are super lightweight and not so thin that you can see your underwear') # true label = 1, positive review\n\ntext_list = [review_1, review_2, review_3, review_4]\n\n\n\ncounter = 0\nfor review in text_list:\n    counter += 1\n    print(f'Review {counter}: {make_prediction(review)}')\n    print()\n    \n# model2 got every review correct. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}