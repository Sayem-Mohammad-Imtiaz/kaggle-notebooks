{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# <font color=\"#703bdb\">Part 2. Data Integration and Processing : Automation Pipeline</font> <hr>\n\n<a href=\"http://policingequity.org/\">Center of Policing Equity</a> is a research and action think tank that works collaboratively with law enforcement, communities, and political stakeholders to identify ways to strengthen relationships with the communities they serve. CPE is also the home of the nationâ€™s first and largest <a href=\"http://policingequity.org/national-justice-database/\">database</a> tracking national statistics on police behavior. \n\nThe main aim of CPE is to bridge the divide created by communication problems, suffering and generational mistrust, and forge a path towards public safety, community trust, and racial equity. This kernel series is my contribution to the <a href=\"https://www.kaggle.com/center-for-policing-equity/data-science-for-good\">Data Science for Good: Center for Policing Equity</a>. The contribution is focused on providing a generic, robust, and automated approach to integrate, standardize the data and further diagnose disparities in policing, shed light on police behavior, and provide actionable recommendations. \n\nFollowing are parts of Kernels Submissions in order:  \n\n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-solution-workflow-science-of-policing-equity/\">Part 1: Solution Workflow - The Science of Policing Equity </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-automation-pipeline-integration-processing\">Part 2: Data Integration and Processing : Automation Pipeline</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-example-runs-of-automation-pipeline\">Part 3: Example Runs of Automation Pipeline </a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-1-analysis-report-minneapolis-24-00013\">Part 4.1: Analysis Report - Minneapolis Police Department (24-00013) </a>   </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/4-2-analysis-report-lapd-49-00033\">Part 4.2: Analysis Report - Los Angles Police Department (49-00033) </a>   </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/4-3-analysis-report-officer-level-analysis\">Part 4.3: Analysis Report - Indianapolis Officer Level Analysis (23-00089) </a>   </li></ul>\n\nThe complete overview of the solution is shared in the *first kernel*. It explains the process and flow of automation, standardization, processing, and analysis of data. In the *second kernel*, the first component of the solution pipeline : data integration and processing is implemented. It processes both core level data as well as department level data. In the *third kernel*, this pipeline is executed and run for several departments. After all the standardized and clean data is produced, it is analysed with different formats of the Analysis Framework in 4.1, 4.2 and 4.3 kernels. In *kernel 4.1*, core analysis is done along with link with crime rate and poverty data. In *kernel 4.2*, core analysis is done along with statistical analysis. In *kernel 4.3*, officer level analysis is done. \n\n<hr>\n\nThis kernel, is the second of the series. In this kernel, the implementation of first two components of the entire pipeline is done. \n\n## <font color=\"#703bdb\">Kernel Conents </font> \n\n### <a href=\"#a\">Component A - Core Data Integration and Processing  </a>\n\n<ul>\n    <li><a href=\"#a1\">Step1 : Seting up the global config parameters - general  </a>  </li>\n    <li><a href=\"#a2\">Step2: Structured repository creation    </a>  </li>\n    <li><a href=\"#a3\">Step3: Standardization of police shape files  </a>  </li>\n    <li><a href=\"#a4\">Step4: Standardization of ACS / Census data   </a>  </li>\n    <li><a href=\"#a5\">Step5: Trigger function for this component   </a>  </li>\n</ul>\n\n### <a href=\"#b\">Component B - Department Level Processing Pipeline  </a>\n\n<ul>\n    <li><a href=\"#b1\">Step1 : Set Global Config Parameters - ShapeFiles  </a>  </li>\n    <li><a href=\"#b2\">Step2 : Find Overlapping Census Tracts with Department Districts    </a>  </li>\n    <li><a href=\"#b3\">Step3 : Setting up global config parameters - ACS  </a>  </li>\n    <li><a href=\"#b4\">Step 4 : Enrich ACS Information in Overlapped Districts   </a>  </li>\n    <li><a href=\"#b5\">Step 5: Standardize Police Incidents Data </a>  </li>\n    <li><a href=\"#b6\">Step 6: Extending Police Data using External Datasets </a>  </li>\n    <li><a href=\"#b7\">Step 7: Save Final Cleaned Datasets </a>  </li>\n    <li><a href=\"#b8\">Step 8: Component B Trigger Function</a>  </li>\n</ul>\n\nFirst, load the important libraries to be used in the overall implementation"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e6d0558cc157ad3777c7bb8ab8ab534d5cb799ec"},"cell_type":"code","source":"import shutil, os, folium, warnings\nfrom shapely.geometry import Point\nimport pandas as pd, numpy as np \nfrom collections import Counter\nfrom statistics import median\nimport geopandas as gpd\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aac2ee1106aee3bb664280a55dc839861392e0ba"},"cell_type":"markdown","source":"<a id=\"a\"></a>\n## <font color=\"#703bdb\">Component A : Core Data Integration and Processing </font> <hr>\n\nThis is the first component of the overall pipeline. In this component, the major focus is on the integrate and process two main datasets to be used in the complete solution. These datasets are : ACS data for different regions and the Shape Files corresponding to different police departments. Following are the key tasks which are executed in this component. \n\n1. Integration of data from multiple data sources     \n2. Creation of Structured Repository     \n3. Processing of Police Department Shape Files    \n&nbsp;&nbsp;&nbsp;&nbsp; 3.1 File naming conventions   \n&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Error handelling   \n&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Consistent parameters     \n4. Processing of ACS Data    \n&nbsp;&nbsp;&nbsp;&nbsp; 4.1 File naming conventions  \n&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Cleanup of metric and meta files   \n\nHere is the overview of this component:  \n\n<br>\n\n![](https://i.imgur.com/2d4b7BG.png)\n\n\n<br>\n\n<a id=\"a1\"></a>\n## <font color=\"#703bdb\">Step 1. Seting up global config parameters</font><hr>\n\nIn the first step of the pipeline, we define the important global configurations to be used in the pipeline. This step serves as a configuration defining step, which can be changed again and again by the user with different types or sources of data to be used. This config acts like a controller to the user through which they can control how they want to execute the pipeline. In this config file, following parameters need to be defined: \n\n> **_base_dir** : The base directory path containing all the raw data  \n> **_root_dir** : The new directory path which will contain all the cleaned and structured data  \n> **ct_base_path** : The base path containing the census-tracts shape files   \n> **external_datasets_path** : The base path of any external data to be used "},{"metadata":{"trusted":true,"_uuid":"75e789ec97238d9fb69eec491ca155b73d3fca6f"},"cell_type":"code","source":"ct_base_path = \"../input/census-tracts/cb_2017_<NUM>_tract_500k/cb_2017_<NUM>_tract_500k.shp\"\nexternal_datasets_path = \"../input/external-datasets-cpe/\"\n_base_dir = \"../input/data-science-for-good/cpe-data/\"\n_root_dir = \"CPE_ROOT/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f26bec22deb5be29a9d39a833f391c55cc405b9e"},"cell_type":"markdown","source":"The given data may contains many unnecessary shape files which may not be of user, so we define the mandatory shape file extensions which are required. Rest can be ignored. Additionally, we also define the names of the new directories to be created."},{"metadata":{"trusted":true,"_uuid":"5fc66b230d5a348dbce500aeacda05b0ef4fb219"},"cell_type":"code","source":"## define the new directory names and mandatory shape files \nmandatory_shapefiles = [\"shp\", \"shx\", \"dbf\", \"prj\"]\nnew_dirs = [\"shapefiles\", \"events\", \"metrics\", \"metrics_meta\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29ec9a5064accefb4d5a0cf8256422ef87f2b92e"},"cell_type":"markdown","source":"<a id=\"a2\"></a>\n## <font color=\"#703bdb\">Step 2. Structured Repository Creation </font><hr>\n\nThe next step of the pipeline creates a new well defined repository structure containing repositories with proper naming conventions. After the execution of this pipeline component, for every department, well-defined structured repositories are created. <br><br>\n\n![](https://i.imgur.com/XSTXkaF.png)\n\n<br>\n\nTheir structure is as follows: \n\n> **shapefiles** : Contains the four mandatory shapefile types with filenames standardized, (example : \"department\".shp)  \n> **events** : Contains police level data : use-of-force / arrests / vehicle stops etc  \n> **metrics** : Contains ACS / census level metrics data with filenames standardized (example : \"education.csv\" )  \n> **metrics-meta** : Contains the corresponding meta data for every metrics  \n\nWe define two main functions for this step. \n\n> **_cleanup_environment() :** This function is used to cleanup the environment, removes any unnecessary repository in the path         \n> **_create_repository_structure():** This function is used to create the new repositories that will save the cleaned up data.  "},{"metadata":{"trusted":true,"_uuid":"e745a654b84a53f26853a5a3cbd5d11e2cf653a3"},"cell_type":"code","source":"## Utility function to cleanup the environment\ndef _cleanup_environment():\n    if os.path.exists(_root_dir):\n        !rm -r CPE_ROOT\n        pass\n    return None\n\n## Function to create a new repository structure \ndef _create_repository_structure():            \n    ## refresh environment \n    _cleanup_environment()\n    \n    ## list of all departments whose raw data is available\n    depts = [_ for _ in os.listdir(_base_dir) if \"Dept\" in _]\n    \n    ## master folder\n    os.mkdir(_root_dir) \n    for dept in depts:\n\n        ## every department folder \n        os.mkdir(_root_dir + \"/\" + dept)         \n        for _dir in new_dirs:\n        \n            ## sub directories for - shapefiles, acsdata, metrics, metrics-meta\n            os.mkdir(_root_dir + \"/\" + dept + \"/\" + _dir + \"/\")            \n    print (\"Status : Directory Structured Created\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08df86f42a6c08b541f6e7507ac0c5b053474eff"},"cell_type":"markdown","source":"<a id=\"a3\"></a>\n## <font color=\"#703bdb\">Step 3. Standardization of Police Shape Files </font><hr>\n\nIn this step, standardization of police shape files is performed. Following tasks are executed in this step: \n\n**1. File Naming Conventions:** The given shape files for every department comprises of different names, so it is important to standardize them and maintain a consistent naming. In this step, The shapefiles corresponding to every deparment are standardized. In this process the only the mandatory files are picked, their names are changed to following: \n\n![](https://multimedia.journalism.berkeley.edu/media/upload/tutorials/qgis-basics/shp.jpg)\n\n> - department.shp\n> - department.shx \n> - department.prj  \n> - department.dbf  \n\n<br>\n**2. Missing Files Error Handelling :** Addionally, some files may contain errors, so this step also performes error handelling. It uses a config, in which user can define what error handling needs to be performed. A global variable (config) is defined missing_shape_meta which cotains a list of key-value pairs for every department which needs to be fixed. For example, in the given data, the \"prj\" data is missing for Department : 37-00027. So its content will be manually supplied in this config. \n\n> **missing_shape_meta** = { \"Dept_37-00027\" : {\"prj\" : dept_37_27_prj} }\n\nAnd, we define a function that will perform the corresponding error handling among the shape files. To fix the corresonding issues, it performs two steps\n- Step 1 : Add the missing prj content   \n- Step 2 : Fix the CRS of the shape file  \n\n<br>\n**3. Consistent Coordinate System (CRS): **\n\nAnother important part of this challenge is to automatically identify the coordinate systems of the shape files and make them consistent for further analysis. The PRJ files contains data about the projected coordinate system. The provides the information about : name for the projected coordinate system, the geographic coordinate system, the projection and all the parameters needed for the projection. Tools such as arcgis and mapinfo can definately help in solving this issue, but doing this programatically is bit challenging. In my current Implementation, I convert the given shapefile into one standard CRS : \"epsg=4326\". Later this part can be changed and made dynamic. \n\n<hr>\n\n### Implementation \n\nSo for the implementation part, we define the function which perform the standardization and cleanup of shapefiles. Additionally, the cleaned files are moved to new location. Following are the main steps which are performed in this function: \n\nStep 1 : Configure the old and New Paths   \nStep 2 : Standardize the file names and move to new path  \nStep 3: Fix Errorenous shape files  \n\nCurrent limitation of this function is that it does not handles the directories in which more than one shapefiles are present. To handle this, the improtant and most relevant police shape files should be kept in the base raw data level. "},{"metadata":{"trusted":true,"_uuid":"044703f9a7ccaa60c37e6f7d158cd4d3b23ffab3"},"cell_type":"code","source":"## Function to standardize the shape files\ndef _standardize_shapefiles():\n    depts = [_ for _ in os.listdir(_base_dir) if \"Dept\" in _]\n    for dept in depts:    \n        ## Step1: Configure the old and new path\n        shp_dir = dept.replace(\"Dept_\",\"\") + \"_Shapefiles/\"\n        old_pth = _base_dir + dept + \"/\" + shp_dir\n        new_pth = _root_dir + dept + \"/\" + \"shapefiles/\"\n\n        ## Step2: Standardize the file names and move to new path \n        _files = os.listdir(old_pth)\n        for _file in _files:\n            if _file[-3:].lower() not in mandatory_shapefiles:\n                continue\n            ext = \".\".join(_file.split(\".\")[1:]).lower()\n            new_name = \"department.\" + ext\n            shutil.copy(old_pth+_file, new_pth+new_name)\n\n        ## Step3: Fix Erroroneus shapefiles\n        fix_flag = _fix_errors_shapefiles(new_pth, dept)\n        \n    print (\"Status : Shapefile Standardization Complete\")\n    return None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30eb93d46f8be123659e896c8b32be87de43dc3a"},"cell_type":"markdown","source":"Next, we define the function for error handelling."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"68d00a386e80ef0a77e0ddd4e78a537a684afdfe"},"cell_type":"code","source":"dept_37_27_prj = 'PROJCS[\"NAD_1983_StatePlane_Texas_Central_FIPS_4203_Feet\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS_1980\",6378137,298.257222101]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"False_Easting\",2296583.333333333],PARAMETER[\"False_Northing\",9842499.999999998],PARAMETER[\"Central_Meridian\",-100.3333333333333],PARAMETER[\"Standard_Parallel_1\",30.11666666666667],PARAMETER[\"Standard_Parallel_2\",31.88333333333333],PARAMETER[\"Latitude_Of_Origin\",29.66666666666667],UNIT[\"Foot_US\",0.30480060960121924],AUTHORITY[\"EPSG\",\"102739\"]]'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"377c28284095be00a4b7f51c3b68d4649f5a1b4b","_kg_hide-input":false},"cell_type":"code","source":"## create a config to handle the errors in raw shape files\nmissing_shape_meta = { \"Dept_37-00027\" : {\"prj\" : dept_37_27_prj} }\n\n## Function to fix / cleanup the errors in shapefile types\ndef _fix_errors_shapefiles(_path, dept):\n    \"\"\"\n    :params:\n    _path : root path containig the shape files \n    dept : selected dept if it is called only for a particular department\n    \"\"\"\n    \n    if dept not in missing_shape_meta:\n        return False\n    \n    ## Fix the errors in raw corresponding shape files\n    for extension, content in missing_shape_meta[dept].items():\n        if extension == \"prj\": \n            # Step1: Add missing prj file\n            with open(_path + \"department.prj\", 'w') as outfile:\n                outfile.write(content)\n            \n            # Step2: Fix CRS of shape file\n            df = gpd.read_file(_path + 'department.shp')\n            df.to_file(filename = _path + 'department.shp', \n                       driver='ESRI Shapefile', crs_wkt = content)\n\n        elif extension == \"shx\":\n            ## This function can be extended for other shape filetypes\n            ## the corresponding logic can be added in these blocks \n            pass\n    return True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80730536600fcff1fe8ffa9665ca0da31224ab80"},"cell_type":"markdown","source":"<a id=\"a4\"></a>\n## <font color=\"#703bdb\">Step 4. Standardization of ACS / Census Data </font><hr>\n\nIn the next step, ACS / Census data is cleaned up and moved to new path. The names of the ACS files are standardized and cleaned up. Additionally, their corresponding meta files are also stored with same naming convention. Different Metrics (education, housing, income etc) data are shared but this part can be extended with more data. Following steps are performed in this step. \n\n1 : Configure the old and new paths  \n2 : Move all the ACS datafiles  \n3 : Standardize / Cleanup the names  \n4.1 : Move the data files - Metric Data  \n4.2 : Move the meta files - Metric Meta  "},{"metadata":{"trusted":true,"_uuid":"8723fbd7b9a32ff21276be14e33aa64b01c11491"},"cell_type":"code","source":"## cleaned names corresponding to given raw metric names\nacs_metrics_dic = { 'owner-occupied-housing' : 'housing', 'education-attainment' : 'education', 'employment' : 'employment', 'education-attainment-over-25' : 'education25', 'race-sex-age' : 'race-sex-age', 'poverty' : 'poverty', 'income' : 'income' }\nmetrics_names = list(acs_metrics_dic.values())\n\n## function to cleanup and move the ACS data\ndef _standardize_acs():\n    depts = [_ for _ in os.listdir(_base_dir) if \"Dept\" in _]\n    for dept in depts:  \n        ## Step1: Configure the old and new path\n        acs_dir = dept.replace(\"Dept_\",\"\") + \"_ACS_data\"\n        old_dirs = os.listdir(_base_dir + dept +\"/\"+ acs_dir)\n        new_dirs = [f.replace(dept.replace(\"Dept_\",\"\"),\"\") for f in old_dirs]\n        new_dirs = [f.replace(\"_ACS_\",\"\") for f in new_dirs]\n        \n        ## Step2: Move all ACS datafiles\n        for j, metric in enumerate(old_dirs):\n            metric_files = os.listdir(_base_dir + dept +\"/\"+ acs_dir +\"/\"+ metric)\n            _file = [f for f in metric_files if \"metadata\" not in f][0]\n            _meta = [f for f in metric_files if \"metadata\" in f][0]\n\n            ## Step3: Standardize / Cleanup the name \n            for name, clean_name in acs_metrics_dic.items():\n                if \"25\" in metric:\n                    cname = \"education25\"\n                if name in metric:\n                    cname = clean_name     \n\n            ## Step4.1 : Move Metric File\n            old_path = _base_dir + dept +\"/\"+ acs_dir +\"/\"+ metric +\"/\"+ _file\n            new_path = _root_dir + dept +\"/metrics/\" + cname + \".csv\"\n            shutil.copy(old_path, new_path)\n\n            ## Step4.2 : Move Metrics meta files\n            old_path = _base_dir + dept +\"/\"+ acs_dir +\"/\"+ metric +\"/\"+ _meta\n            new_path = _root_dir + dept +\"/metrics_meta/\" + cname + \".csv\"\n            shutil.copy(old_path, new_path)\n\n    print (\"Status : Standardization of Metrics complete\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49ae8d2592a89380d6dd07cdbd42c8564bd4d774"},"cell_type":"markdown","source":"<a id=\"a5\"></a>\n## <font color=\"#703bdb\">Step 5. Trigger Function : Component A  </font><hr>\n\nNext, we compile all the corresponding functions of Component A together, and trigger it.  Finally creating the well defined raw data source which makes the analysis very quick and accessible. "},{"metadata":{"trusted":true,"_uuid":"5b9abb4ff143ae144e70838b958836513c802968"},"cell_type":"code","source":"def _run_standardization_pipeline():\n    _create_repository_structure()\n    _standardize_shapefiles()\n    _standardize_acs()\n\n_run_standardization_pipeline()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2734d0b2cf2e7459fd24eea4df27f44f9067f522"},"cell_type":"markdown","source":"After the sucessful run of this pipeline part, a new well defined structured repository is created having the following layout. This type of directory makes it consistent to work with different departments and makes the analysis and modelling part easier and accessible.  \n\n<br>\n\n<a id=\"b\"></a>\n# <font color=\"#703bdb\">Component B - Department Level Processing  </font><hr>\n\nIn the previous component, the core level processing, integration, and standardization was performed in which all the universal data such as ACS information, Police Shape File Information was processed. Now, in the next component, a particular department is selected, and its department level information is processed. The key tasks are:\n\n - Find the overlapping the shape files of department with the census tracts  \n - Enriching them with the acs level information  \n - Standardize the police incidents \n- Further enriching the overlapped districts with the police incidents  \n \nThe overview of Part B is shown below: \n\n![](https://i.imgur.com/XBBolXR.png)\n\n<br> \n\n<a id=\"b1\"></a>\n## <font color=\"#703bdb\">Step 1 : Set Global Config Parameters - ShapeFiles  </font><hr>\n\nAs the first step, we define a global config file in which we will store two essential information: \n\n- **_rowid:** Represents the unique identifier present in the corresponding shape file of a department  \n- **ct_num:** Represents the Corresponding Census Tract State Number which can be used to map the census tracts shape files with the unique department shape files. \n\nI have created the following depts_config in which I have added the details of the given departments. When new departments are added to this data, same config can be updated. \n\n"},{"metadata":{"trusted":true,"_uuid":"f19aa0a1eeac4a3d6865b5cc818341c0f9d74884"},"cell_type":"code","source":"## Provide the config file for the departments\ndepts_config = {\n    'Dept_23-00089' : {'_rowid' : \"DISTRICT\", \"ct_num\" : \"18\"},  \n    'Dept_49-00035' : {'_rowid' : \"pol_dist\", \"ct_num\" : \"06\"},  \n    'Dept_24-00013' : {'_rowid' : \"OBJECTID\", \"ct_num\" : \"27\"},  \n    'Dept_24-00098' : {'_rowid' : \"gridnum\",  \"ct_num\" : \"27\"},   \n    'Dept_49-00033' : {'_rowid' : \"number\",   \"ct_num\" : \"06\"},    \n    'Dept_11-00091' : {'_rowid' : \"ID\",       \"ct_num\" : \"25\"},         \n    'Dept_49-00081' : {'_rowid' : \"company\",  \"ct_num\" : \"06\"},   \n    'Dept_37-00049' : {'_rowid' : \"Name\",     \"ct_num\" : \"48\"},      \n    'Dept_37-00027' : {'_rowid' : \"CODE\",     \"ct_num\" : \"48\"},     \n    'Dept_49-00009' : {'_rowid' : \"objectid\", \"ct_num\" : \"53\"}, \n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d0d12da9f1d4abf986d1001b9edcb3cdcb76f5"},"cell_type":"markdown","source":"<a id=\"b2\"></a>\n## <font color=\"#703bdb\">Step 2 : Find Overlapping Census Tracts with Department Districts  </font><hr>\n\nThis is one of the most essential step of this component. In this step, the overlapping census tract along with the percentage of overlap are found. Before writing the core function, we will define the utilities few functions first that will help us to process the department level shape files\n\n> **_read_shape_gdf():**  Function to read shape files for a department and return the corresponding shape file geodataframe  \n> **_read_ctfile():**  Function to read the corresponding census tract file for a department  \n> **_plot_shapefile_base():**  Function to plot the base / overlapped shape file on a map   \n"},{"metadata":{"trusted":true,"_uuid":"b9919f6222d95a4cca634e2e70889253eaf3a6f2"},"cell_type":"code","source":"## Function to read a shapefile\ndef _read_shape_gdf(_dept):\n    shape_pth = _root_dir + _dept + \"/shapefiles/department.shp\"\n    ## ensure that CRS are consistent\n    shape_gdf = gpd.read_file(shape_pth).to_crs(epsg=4326)\n    return shape_gdf\n\n## Read the CT File\ndef _read_ctfile(_dept):\n    ## find the corresponding CT number from the config\n    _ct = depts_config[_dept][\"ct_num\"]\n    ## generate the base CT path \n    ct_path = ct_base_path.replace(\"<NUM>\", _ct)\n    ## load the geo data frame for CT \n    state_cts = gpd.read_file(ct_path).to_crs(epsg='4326')\n    return state_cts\n\n## Function to get the centroid of a polygon\ndef _get_latlong_point(point):\n    _ll = str(point).replace(\"POINT (\",\"\").replace(\")\", \"\")\n    _ll = list(reversed([float(_) for _ in _ll.split()]))\n    return _ll\n\n## Function to plot a shapefile\n## Function to plot a shapefile\ndef _plot_shapefile_base(shape_gdf, _dept, overlapped_cts = {}):\n    ## obtain the center most point of the map \n    \n    if \"center_ll\" not in depts_config[_dept]:\n        center_pt = shape_gdf.geometry.centroid[0]\n        center_pt = _get_latlong_point(center_pt)\n    else:\n        center_pt = depts_config[_dept][\"center_ll\"]\n    \n    ## initialize the folium map \n    mapa = folium.Map(center_pt,  zoom_start=10, tiles='CartoDB dark_matter')\n    if len(overlapped_cts) == 0:\n        ## only the base map\n        folium.GeoJson(shape_gdf).add_to(mapa)\n    else:\n        ## overlapped map\n        ct_style = {'fillColor':\"red\",'color':\"red\",'weight':1,'fillOpacity':0.5}\n        base_style = {'fillColor':\"blue\",'color':\"blue\",'weight':1,'fillOpacity':0.5}\n        folium.GeoJson(overlapped_cts, style_function = lambda feature: ct_style).add_to(mapa)\n        folium.GeoJson(shape_gdf, style_function = lambda feature: base_style).add_to(mapa)\n    return mapa","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8626269983b94534d759989e4e23d9dad2e5efcf"},"cell_type":"markdown","source":"**Overlapping CTs Percentage Calculations**\n\nNext, define the function to find the overlapping Census Tracts with the department shape files. This complete function is executed in four main steps: \n\nStep A: Initialize the overlapping percentage dictionary   \nStep B: Find overlap between district and CT layers   \nStep C: Calculate and save the overlapping percentage    \nStep D: Find the unique overlapping census tracts separately  "},{"metadata":{"trusted":true,"_uuid":"f178c1907733893ac45cb2fb742bfa45dc48ea75"},"cell_type":"code","source":"## Find Overlapping Census Tracts\ndef find_overlapping_cts(dept_gdf, state_cts, _identifier, _threshold = 10.0):\n    \"\"\"\n    :params:\n    dept_gdf : the geo dataframe loaded from shape file for the department \n    state_cts : the geo dataframe of the corresponding ct file\n    _identifier : the unique row identifier for the department \n    _threshold : the overlapping threshold percentage to consider \n    \"\"\"\n    \n    \n    ## Step 1: Initialize\n    olaps_percentages, overlapped_idx = {}, []\n    for i, row in dept_gdf.iterrows():\n        if row[_identifier] not in olaps_percentages: \n            olaps_percentages[row[_identifier]] = {}\n\n        ## Step 2: Find overlap bw district and ct layer\n        layer1 = row[\"geometry\"] # district layer\n        for j, row2 in state_cts.iterrows():\n            layer2 = row2[\"geometry\"] # ct layer\n            layer3 = layer1.intersection(layer2) # overlapping layer\n            \n            ## Step 3: Save overlapping percentage\n            overlap_percent = layer3.area / layer2.area * 100\n            if overlap_percent >= _threshold: \n                olaps_percentages[row[_identifier]][row2[\"GEOID\"]] = overlap_percent\n                overlapped_idx.append(j)\n    \n    ## Step 4: Find unique overlapping census tracts\n    overlapped_idx = list(set(overlapped_idx))\n    overlapped_cts = state_cts.iloc[overlapped_idx]\n    return overlapped_cts, olaps_percentages\n\n## function to convert overlapping percentages dictionary to a dataframe \ndef _prepare_olaps_df(olaps_percentages):\n    temp = pd.DataFrame()\n    distid, ct, pers = [], [], []\n    for k, vals in olaps_percentages.items():\n        for v, per in vals.items():\n            distid.append (k)\n            ct.append(v)\n            pers.append(round(per, 2))\n    temp[\"DistId\"] = distid\n    temp[\"CensusTract\"] = ct\n    temp[\"Overlap %\"] = pers\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acf815979567dc355680aa838b556758795d5a4f"},"cell_type":"markdown","source":"<a id=\"b3\"></a>\n## <font color=\"#703bdb\">Step 3 : Setting up global config parameters - ACS  </font><hr>\n\nAs the next step, define the parameters related to ACS data. Only two variables need to be defined: \n\n> - **metrics_config:** Dictionary that defines which all ACS metrics needs to be processed. An important field **measure** is added which states that what is the type of computation that needs to be performed, For example: To find the number of blacks / whites / hispanics etc prportion is used, To find the median income of a population, median will be used and similarly to find the umployment ratio / unemployment ratio mean will be used. \n> - **_column_names:** Dictionary that saves the human defined cleaned column names corresponding to the actual column names given in the ACS data. "},{"metadata":{"trusted":true,"_uuid":"5154f440cd52dc52f19a25e0a04208a4859d282b"},"cell_type":"code","source":"## Specific Metrics and their measures \nmetrics_config = {\n            'race-sex-age': {'metrics':['race','age','sex'], \"measure\":\"proportion\"},\n            'income':       {'metrics':['median_income'],    \"measure\":\"median\"},\n            'poverty':      {'metrics':['below_poverty'],    \"measure\":\"proportion\"},\n            'employment':   {'metrics':['ep_ratio', 'unemp_ratio'], \"measure\" : \"mean\"}\n            }\n\n## Cleaned Column Names \n_column_names = {\"race\" : { \"HC01_VC43\" : \"total_pop\",\n                            \"HC01_VC49\" : \"white_pop\",\n                            \"HC01_VC50\" : \"black_pop\",\n                            \"HC01_VC56\" : \"asian_pop\",\n                            \"HC01_VC88\" : \"hispanic_pop\"},\n                \"age\" : {\n                            \"HC01_VC12\" : \"20_24_pop\", \n                            \"HC01_VC13\" : \"25_34_pop\", \n                            \"HC01_VC14\" : \"35_44_pop\", \n                            \"HC01_VC15\" : \"45_54_pop\", \n                            \"HC01_VC16\" : \"55_59_pop\", \n                },\n                \"sex\": {\n                            \"HC01_VC04\" : \"male_pop\",\n                            \"HC01_VC05\" : \"female_pop\",\n                },\n                \"median_income\" : {\n                            \"HC02_EST_VC02\" : \"pop_income\",\n                            \"HC02_EST_VC04\" : \"whites_income\",\n                            \"HC02_EST_VC05\" : \"blacks_income\",\n                            \"HC02_EST_VC07\" : \"asian_income\",\n                            \"HC02_EST_VC12\" : \"hispanic_income\",\n                },\n                \"below_poverty\" : {\n                            \"HC02_EST_VC01\" : \"below_pov_pop\"},\n                 \"ep_ratio\" : {\n                             \"HC03_EST_VC15\" : \"whites_ep_ratio\",\n                             \"HC03_EST_VC16\" : \"blacks_ep_ratio\"\n                  },\n                 \"unemp_ratio\" : {\n                             \"HC04_EST_VC15\" : \"whites_unemp_ratio\",\n                             \"HC04_EST_VC16\" : \"blacks_unemp_ratio\"}\n                }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0aaab91633c629b61b9040e90c0785b7ec64a2d"},"cell_type":"markdown","source":"Additionally, we write the utilitiy functions to be used for this step.\n\n> - **_cleanup_metrics_data()**:  Function to perform basic pre-processing on metrics data, load all the metrics data, save it in a dictionary, and returns as the object.  \n> - **_flatten_gdf()**:  Function to flatten the details present in the dataframe, "},{"metadata":{"trusted":true,"_uuid":"7ae8908a4873693b895a598d622dc8fe449b0b3f"},"cell_type":"code","source":"## Function to perform basic pre-processing on metrics data \ndef _cleanup_metrics_data(_dept):\n    metrics_df = {}\n    for _metric in metrics_names: ## metrics_name is deinfed in config \n        mpath = _root_dir + _dept + \"/metrics/\" + _metric + \".csv\"\n        mdf = pd.read_csv(mpath, low_memory=False).iloc[1:]\n        mdf = mdf.reset_index(drop=True).rename(columns={'GEO.id2':'GEOID'})\n        metrics_df[_metric] = mdf\n    \n    ## returns metrics_df that contains all the dataframe for ACS metrics \n    return metrics_df\n\n## Function to Flatten the details\ndef _flatten_gdf(df, _identifier):\n    relevant_cols = [_identifier]\n    flatten_df = df[relevant_cols]\n    for c in df.columns:\n        if not c.startswith(\"_\"):\n            continue\n        _new_cols = list(df[c].iloc(0)[0].keys())\n        for _new_col in _new_cols:\n            _clean_colname = _column_names[c[1:]][_new_col]\n            flatten_df[_clean_colname] = df[c].apply(lambda x : x[_new_col]\\\n                                                if type(x) == dict else 0.0)\n            relevant_cols.append(_clean_colname)\n    return flatten_df[relevant_cols]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dffe7c2316e048cc559928bb523d6e659c50b6aa"},"cell_type":"markdown","source":"<a id=\"b4\"></a>\n## <font color=\"#703bdb\">Step 4 : Enrich ACS Information in Overlapped Districts  </font><hr>\n\nHere we will define the final function that will use the overlapped percentages, ACS information and perform the necessary calculation to generate the estimated information linked with the police department zones. The enrichment process performs different calculations to find the estimated numbers associated with a district of a department. \n\n- **Proportion:** For population estimates such as black population, white population, total population, etc, proportion is used to get the estimated number. The actual number is multiplied with the overlapped percentage in order to get the estimated number.  \n    \n        estimated_value = true_value * overlapping_percentage\n\n- **Median:** Median is used to compute the metrics such as median income of the overlapped groups.\n- **Mean:** Mean is used to compute the average estimates of the overlapped population, for example - average unemployment rate of the overlapped population.  "},{"metadata":{"trusted":true,"_uuid":"51b6f6bb758cd5dd60a7f7b92a52761cc9b8c3b6"},"cell_type":"code","source":"## Function that enriches the information using overlapped percentage\ndef _enrich_info(idf, percentages, m_df, columns, m_measure):\n    \"\"\"\n    :params:\n    idf : unique identifier for the police department information\n    percentages : The overalapped CTs and their percentages\n    m_df : the dataframe of the metric containing all the information\n    columns : the corresponding column names of the metric, defined in config\n    m_measure : the measure (mean, median, proportion) to perform\n    \"\"\"\n    \n    ## define the updated_metrics object that will store the estimated information\n    updated_metrics = {}\n    \n    ## return None if no overlapping CTs\n    if len(percentages[idf]) == 0:\n        return ()\n    \n    ## Iterate in all Districts with the overlapped CTs and percentage\n    for idd, percentage in percentages[idf].items(): \n        ## find the corresponding row for an overlapped CT in the metric data \n        ct_row = m_df[m_df[\"GEOID\"] == idd]\n        for rcol in columns:\n            if rcol not in updated_metrics:\n                updated_metrics[rcol] = []\n            \n            ## Perform the necessary calculation to find the estimated number \n            try:\n                actual_value = ct_row[rcol].iloc(0)[0].replace(\"-\",\"\")\n                actual_value = actual_value.replace(\",\",\"\")\n                actual_value = float(actual_value.replace(\"+\",\"\"))\n                if m_measure == \"proportion\":\n                    updated_value = actual_value * percentage / 100\n                else:\n                    updated_value = actual_value\n                updated_metrics[rcol].append(updated_value)\n            except Exception as E:\n                pass\n        \n    ## Update the information in updated_metrics\n    for rcol in columns:\n        if len(updated_metrics[rcol]) == 0:\n            updated_metrics[rcol] = 0\n        else:\n            if m_measure == \"proportion\":\n                updated_metrics[rcol] = sum(updated_metrics[rcol])\n            elif m_measure == \"median\":\n                updated_metrics[rcol] = median(updated_metrics[rcol])\n            elif m_measure == \"mean\":\n                _mean = float(sum(updated_metrics[rcol])) / len(updated_metrics[rcol])\n                updated_metrics[rcol] = _mean\n    return updated_metrics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"883a517005c74575f7c29d41cd2938f6c969fb65"},"cell_type":"markdown","source":"We will define another function that will call the enrich information function for different metrics. "},{"metadata":{"trusted":true,"_uuid":"5631168aa80d5d4a88cb760eaffb83110e1019c9"},"cell_type":"code","source":"## Master Function to process the ACS info in dept df\ndef _process_metric(metrics_df, dept_df, _identifier, olaps_percentages, metric_name):\n    \"\"\"\n    :params:\n    metrics_df : the complete dataframe containing the metrics data\n    dept_df : the geodataframe for police shape files \n    _identifier : the row identifier column corresponding to the police dept shape file \n    olaps_percentages : the overlapping percentage object calculated in previous step\n    metric_name : Name of the metric, example - education / poverty / income \n    \"\"\"\n    \n    m_df = metrics_df[metric_name]\n    m_measure = metrics_config[metric_name][\"measure\"]\n    for flag in metrics_config[metric_name]['metrics']:\n        cols = list(_column_names[flag].keys())\n        dept_df[\"_\"+flag] = dept_df[_identifier].apply(lambda x : \\\n                            _enrich_info(x, olaps_percentages, m_df, cols, m_measure))\n    return dept_df ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82192af3363c64df6f95c370fd4c347451e138bf"},"cell_type":"markdown","source":"Now enriched data contains the information about: \n\n- the overlapped census tracts  \n- the percentage of overlap  \n- the estimated / calculated numbers for department districts  \n\n<a id=\"b5\"></a>\n## <font color=\"#703bdb\">Step 5: Standardize Police Incidents Data </font><hr>\n\nIn this step we add our target information, which in this case is the police incidents information. This data is part of <a href=\"http://policingequity.org/national-justice-database/\">National Justice Database</a> in which data from different departments is ingested. The biggest challenge is that there is no standardization followed. For the standardization purposes, I have considered following points: \n\n1. Standardization of File Names   \n2. Standardization of Key Fields  \n    - Subject Race\n    - Subject Gender\n    - Incident Date   \n3. Standardization of Column Names  \n    \n    \n### Standardization Process \n   \nThe standardization process is a three step proces : \n\n1. Otain all the unique values of the column to standardize from every police department incident file.  \n2. Combine all the values together, and remove duplicates  \n3. Quickly, paste them on an excel sheet (see example file [here](https://docs.google.com/spreadsheets/d/1mM9c6CYt7KRR9NK0QuulW5G66cXU_Q0geC1xAgE5Vsc/edit?usp=sharing) ) and update the standardized_column column.  \n4. Export the file as csv, and pass it to this pipeline which performs the automatic standardization. Moreover, the pipeline also handles the standardization of column names. For instance, in District: \"Dept_23-00089\", race column is given as \"SUBJECT_RACT\" instead of \"SUBJECT_RACE\". \n\n\n<img src=\"https://i.imgur.com/nTItNxI.png\" height=500 width=500>\n\n<br>\n\nIn the current implementation of the pipeline, subject race and subject gender are integrated, more columns can be added. Just need to update the conlumn config file as shown below : \n\n> column_config = { <br>\n    \"SUBJECT_RACE\" : { \"variations\": [\"SUBJECT_RACT\"],  \"values_map\" : subject_race_map }, <br>\n    \"SUBJECT_GENDER\" : { \"variations\": [],  \"values_map\" : subject_gender_map }  <br>\n    }  <br>\n    \nSo we write the utility functions to perform this standardization process: \n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"80bf96c9a7b1ad79d54fa52b74dd707831ef7dab"},"cell_type":"code","source":"subject_race_csv_content = \"\"\"W\tWhite\nW(White)\tWhite\nWhite\tWhite\nB\tBlack\nB(Black)\tBlack\nBlack\tBlack\nBlack or African American\tBlack\nBlack, Black\tBlack\nUnk\tUnknown\nUnknown\tUnknown\nUNKNOWN\tUnknown\nNo Data\tUnknown\nNO DATA ENTERED\tUnknown\nnot recorded\tUnknown\nNot Specified\tUnknown\nP\tPacific Islander\nPacific Islander\tPacific Islander\nO\tOther\nOther\tOther\nOther / Mixed Race\tOther\nNative Am\tNative American\nNative Amer\tNative American\nNative American\tNative American\nLatino\tLatino\nH\tHispanic\nH(Hispanic)\tHispanic\nHispanic\tHispanic\nHispanic or Latino\tHispanic\nA\tAsian\nA(Asian or Pacific Islander)\tAsian\nAsian\tAsian\nAsian or Pacific islander\tAsian\nAmerican Ind\tAmerican Indian\nAmerican Indian/Alaska Native\tAmerican Indian\"\"\"\n\nsubject_gender_csv_content = \"\"\"F\tFemale\nFemale\tFemale\nFEMALE\tFemale\nM\tMale\nM, M\tMale\nMale\tMale\nMALE\tMale\nNo Data\tUnknown\nnot recorded\tUnknown\nNot Specified\tUnknown\nUnk\tUnknown\nUnknown\tUnknown\nUNKNOWN\tUnknown\n-\tUnknown\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a279d9fb69e55965c1d0972d9e7a50e3dae0d220"},"cell_type":"code","source":"## utility function to get the map of raw -> standardized\ndef _get_map(content):\n    _map = {}\n    for line in content.split(\"\\n\"):\n        raw = line.split(\"\t\")[0]\n        standardized = line.split(\"\t\")[1]\n        _map[raw] = standardized\n    return _map\n\n## utility function to get the frequency count of elements \ndef _get_count(x):\n    return dict(Counter(\"|\".join(x).split(\"|\")))\n\n## utility function to cleanup the name \ndef _cleanup_dist(x):\n    try:\n        x = str(int(float(x)))\n    except Exception as E:\n        x = \"NA\"\n    return x \n\n## Create the raw-standardized maps after reading the csv content as shown in image above \nsubject_race_map = _get_map(subject_race_csv_content)\nsubject_gender_map = _get_map(subject_gender_csv_content)\n\ncolumn_config = {\n    \"SUBJECT_RACE\" : { \"variations\": [\"SUBJECT_RACT\"],  \"values_map\" : subject_race_map },\n    \"SUBJECT_GENDER\" : { \"variations\": [],  \"values_map\" : subject_gender_map },\n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2722ee932592862493892cd7b5a031c5f1e7462e"},"cell_type":"markdown","source":"Now, we write our master function to standardize the police files, column names, and values. "},{"metadata":{"trusted":true,"_uuid":"e454acc7797bb72cb4a3204095a2f57448421ba8"},"cell_type":"code","source":"## master function to standardize the column names and values\ndef _standardize_columns(datadf):\n    for col, col_dict in column_config.items():\n        col_dict[\"variations\"].append(col)\n        _map = col_dict[\"values_map\"]\n        for colname in col_dict[\"variations\"]:\n            if colname in datadf.columns:\n                datadf[col] = datadf[colname].apply(lambda x : _map[x] if x in _map else \"-\")\n                \n    ## Standardize Date Column, add Year and Month\n    if \"INCIDENT_DATE\" in datadf.columns:\n        datadf[\"INCIDENT_DATE\"] = pd.to_datetime(datadf[\"INCIDENT_DATE\"])\n        datadf[\"INCIDENT_YEAR\"] = datadf[\"INCIDENT_DATE\"].dt.year\n        datadf[\"INCIDENT_MONTH\"] = datadf[\"INCIDENT_DATE\"].dt.month\n    \n    if \"LOCATION_DISTRICT\" in datadf.columns:\n        datadf[\"LOCATION_DISTRICT\"] = datadf[\"LOCATION_DISTRICT\"].astype(str)    \n\n    return datadf\n\n## Function to standardize the events data file\ndef _standardize_filename(_dept):\n    _file = [f for f in os.listdir(_base_dir + _dept) if f.endswith(\".csv\")][0]\n    old_path = _base_dir + _dept + \"/\" + _file\n    new_path = _root_dir + _dept + \"/events/\" + _file\n    shutil.copy(old_path, new_path)\n    return _file","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fa0ed20a75473c9259c22fbbf56edbcb55d76d9"},"cell_type":"markdown","source":"Now, we define the function to process the police level information.  "},{"metadata":{"trusted":true,"_uuid":"ebd9f0467752ef7fe3131164b8613f70a16e908a"},"cell_type":"code","source":"def _process_events(pol_config):\n    ## load the given police incidents file and cleanup some missing info\n    ppath = _root_dir + _dept + \"/events/\" + pol_config[\"police_file\"]\n    events_df = pd.read_csv(ppath, low_memory=False)[1:]\n    events_df = _standardize_columns(events_df)\n\n    ## Slice the data for the given years, if given by user\n    years_to_process = pol_config[\"years_to_process\"]\n    if len(years_to_process) != 0: \n        events_df = events_df[events_df['INCIDENT_YEAR'].isin(years_to_process)]\n    \n    ## Aggregate the events by every district of the department\n    police_df = events_df.groupby(\"LOCATION_DISTRICT\")\n\n    ## [Extendable] Obtain the distribution by gender, race etc\n    police_df = police_df.agg({\"SUBJECT_GENDER\" : lambda x : _get_count(x),\\\n                               \"SUBJECT_RACE\"   : lambda x : _get_count(x)})\n    police_df = police_df.reset_index()\n    police_df = police_df.rename(columns={\n                    \"SUBJECT_GENDER\" : pol_config['event_type'] + \"_sex\",\\\n                    \"SUBJECT_RACE\" : pol_config['event_type'] + \"_race\"})\n    return police_df, events_df ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66902775f24e067308f7b6f1226dde9ed3f9097a"},"cell_type":"markdown","source":"<a id=\"b6\"></a>\n## <font color=\"#703bdb\">Step 6: Extending Police Data using External Datasets </font><hr>\n\nSometimes external data can be useful to measure police behaviour. so we will define a function to load any external data. "},{"metadata":{"trusted":true,"_uuid":"dbf1040b2ae8d6165071117c9ecebe311a003d16"},"cell_type":"code","source":"def _load_external_dataset(pol_config):\n    ## load the dataset \n    _path = external_datasets_path + pol_config[\"path\"]\n    events2 = pd.read_csv(_path, parse_dates=[pol_config[\"date_col\"]])\n\n    ## basic standardization\n    events2['year'] = events2[pol_config[\"date_col\"]].dt.year\n    years_to_process = pol_config[\"years_to_process\"]\n    events2 = events2[events2['year'].isin(years_to_process)]\n    events2[pol_config[\"race_col\"]] = events2[pol_config[\"race_col\"]].fillna(\"\")\n    events2[pol_config[\"gender_col\"]] = events2[pol_config[\"gender_col\"]].fillna(\"\")\n    \n    ## Aggregate and cleanup\n    events2[\"LOCATION_DISTRICT\"] = events2[pol_config['identifier']].apply(\n                                                lambda x : _cleanup_dist(x))\n    temp_df = events2.groupby(\"LOCATION_DISTRICT\").agg({\n                                pol_config['gender_col'] : lambda x : _get_count(x),\\\n                                pol_config['race_col'] : lambda x : _get_count(x)})\n    \n    ## cleanup the column names\n    temp_df = temp_df.reset_index().rename(columns={\n                                pol_config['gender_col'] : pol_config[\"event_type\"]+\"_sex\", \n                                pol_config['race_col'] : pol_config[\"event_type\"]+\"_race\"})\n    return temp_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3c218b8c05379ff18917c9169ae12542a0bb427"},"cell_type":"markdown","source":"<a id=\"b7\"></a>\n## <font color=\"#703bdb\">Step 7: Save Final Cleaned Datasets  </font><hr>\n\nThe output of previous 6 steps will be the enriched data containing overlapped census tracts with corresponding acs data numbers. Along with this, police incidents data (or any external data) will also be loaded. In this step, we will write a function to save the final data frames to the disk / (or database)"},{"metadata":{"trusted":true,"_uuid":"7880ea1158765ec923d1954a338d8b500df3b95e"},"cell_type":"code","source":"def _save_final_data(enriched_df, police_df, events_df):\n    enriched_df.to_csv(_root_dir +\"/\"+ _dept + \"/enriched_df.csv\", index = False)\n    police_df.to_csv(_root_dir +\"/\"+ _dept + \"/police_df.csv\", index = False)\n    events_df.to_csv(_root_dir +\"/\"+ _dept + \"/events/events_df.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d752bfbb40d66e605d36d2836f5b590f7db80e4e"},"cell_type":"markdown","source":"<a id=\"b8\"></a>\n## <font color=\"#703bdb\">Step 8: Component B Trigger Function </font><hr>\n\nFinally, we will write a function that will trigger the component B of this pipeline and execute all the steps, finally producing the enriched data and police data which can be easilly used for analysis and modelling purposes. "},{"metadata":{"trusted":true,"_uuid":"4d489998686350a8599f3fa62f989d322870979b"},"cell_type":"code","source":"def _execute_district_pipeline(_dept, _police_config1, _police_config2=None):\n    print (\"Selected Department: \", _dept)\n    \n    ## department shape file\n    print (\". Loading Shape File Data\")\n    dept_shape_gdf = _read_shape_gdf(_dept)\n    base_plot = _plot_shapefile_base(dept_shape_gdf, _dept, overlapped_cts = {})    \n\n    ## finding overlapped CTs percentages\n    print (\".. Finding Overlapping CTs\")\n    _identifier = depts_config[_dept][\"_rowid\"]\n    state_cts = _read_ctfile(_dept)\n    overlapped_cts, olaps_percentages = find_overlapping_cts(dept_shape_gdf, state_cts, _identifier)\n    overlapped_plot = _plot_shapefile_base(dept_shape_gdf, _dept, overlapped_cts)\n    \n    ## Adding the Metrics Data\n    print (\"... Loading ACS Metrics Data\")\n    metrics_df = _cleanup_metrics_data(_dept)\n\n    ## Add Metrics to the dept df\n    print (\".... Enrichment of ACS Metrics with Overlapped Data\")\n    dept_enriched_gdf = dept_shape_gdf.copy(deep=True)\n    for metric_name in metrics_config.keys():\n        dept_enriched_gdf = _process_metric(metrics_df, dept_enriched_gdf, _identifier, \n                                            olaps_percentages, metric_name=metric_name)\n    \n    ## Find Enriched DF\n    enriched_df = _flatten_gdf(dept_enriched_gdf, _identifier)\n    enriched_df = enriched_df.rename(columns={_identifier : \"LOCATION_DISTRICT\"})\n    \n    ## Processing Police DF\n    if _police_config1 != None:\n        print (\"..... Standardizing the Police Events\")\n        police_file1 = _standardize_filename(_dept)\n        _police_config1[\"police_file\"] = police_file1\n        police_df, events_df = _process_events(_police_config1)\n    else:\n        police_df, events_df = pd.DataFrame(), pd.DataFrame()\n    \n    ## Adding any other external Police Data \n    if _police_config2 != None:\n        print (\"..... Standardizing the External Data\")\n        external_df = _load_external_dataset(_police_config2)\n        police_df = police_df.merge(external_df, on=\"LOCATION_DISTRICT\")\n    \n    ## Save Final Data\n    print (\"...... Saving the Final Data in New Repository\")\n    _save_final_data(enriched_df, police_df, events_df)\n    \n    response = {\n                \"dept_shape_gdf\" : dept_shape_gdf,\n                \"base_plot\" : base_plot,\n                \"olaps_percentages\" : _prepare_olaps_df(olaps_percentages),\n                \"overlapped_plot\" : overlapped_plot,\n                \"dept_enriched_gdf\" : dept_enriched_gdf,\n                \"enriched_df\" : enriched_df,\n                \"police_df\" : police_df,\n                \"events_df\" : events_df\n                }\n    return response","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36589ed0b1074024cf125f26000c05425689b437"},"cell_type":"markdown","source":"So this completes the end of component B. Now, let's run the pipeline for one of the department. In the next kernel, pipeline is run for multiple departments. \n\n<a id=\"c\"></a>\n## <font color=\"#703bdb\">Example Run of the Pipeline </font><hr>\n\n### <a href=\"https://www.kaggle.com/shivamb/3-example-runs-data-processing-pipeline\">NEXT KERNEL:</a> Pipeline Run for 8 departments \n<br>\n\nIn this kernel, Let's run it only for a single department, Derpartment : 49-00033. We define three inputs : \n\n> **_dept :** \"Dept_49-00033\"  \n> **police_config1:** config file for given police incidents data  \n> **police_config2:** any external police incidents data to be integrated  "},{"metadata":{"trusted":true,"_uuid":"9f9cf5349b43f9aa5df0ac13aea8d2c28d5c1945"},"cell_type":"code","source":"## select department \n_dept = \"Dept_49-00033\"\n\n## given police data config \n_police_config1 = { 'event_type' : 'arrest', \"years_to_process\" : []}\n\n# ## external police data config\n_police_config2 = {  'path' : \"la_stops/vehicle-and-pedestrian-stop-data-2010-to-present.csv\", \n                     'event_type' : 'vstops',\n                     'identifier' : \"Officer 1 Division Number\" , \n                     'gender_col' : 'Sex Code', \n                     'race_col' : 'Descent Code', \n                     'date_col' : \"Stop Date\", \n                     'years_to_process' : [2015] }\n\n## call the trigger for the given department and their configurations\npipeline_resp = _execute_district_pipeline(_dept, _police_config1, _police_config2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b85e732366e7faff586f46dadb09093ef76ee9bb"},"cell_type":"markdown","source":"At the end, a well structured flat dataset is produced which can be easily analyzed to measure racial bias, or any other analysis. This gives quick access to the users to the hidden information, and they can quicly generate custom reports. \n\n<a id=\"d\"></a>\n## <font color=\"#703bdb\">Features of this pipeline :  </font><hr>\n\n1. **Scalable:** In the next kernel, I have shown the example runs of this pipeline for several departments that produces well structured, cleaned datasets for the analysis purposes. \n2. **Robust:** Performs different levels of error handling throughout the flow.  \n3. **Automated:** Least human intervention, users only need to define **\"_config objects\"**  \n\n<a href=\"https://www.kaggle.com/shivamb/3-example-runs-of-automation-pipeline\"> Next Kernel </a> - The output's and the showcase of these pipeline features."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}