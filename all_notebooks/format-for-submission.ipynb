{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, AutoModelForTokenClassification\nimport torch\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename == 'test_predictions.txt':\n            print(os.path.join(dirname, filename))\n        if filename == 'cleaned_test.csv':\n            print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Import model, labels, test dataset\nModel is trained using:\nhttps://www.kaggle.com/numerator/run-ner-indobert-base-p1/ version 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\"/kaggle/input/run-ner-indobert-base-p1/indobert-ner/\")\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/run-ner-indobert-base-p1/indobert-ner/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_list = [\"128\", \"O\", \"POI\", \"ST\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_prediction_path = '/kaggle/input/run-ner-indobert-base-p1/indobert-ner/test_predictions.txt'\ntest_df = pd.read_csv('/kaggle/input/scl2021data/cleaned_test.csv').fillna('')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_pred(sequence):\n    # Bit of a hack to get the tokens with the special tokens\n    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n    inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n\n    outputs = model(inputs)[0]\n    predictions = torch.argmax(outputs, dim=2)\n\n    pred = [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].tolist())]\n    pred_df = pd.DataFrame(pred)\n    poi = ''\n    st = ''\n    for i in range(len(pred_df)):\n        if pred_df.iloc[i,1] == 'POI':\n            poi = poi + ' ' + pred_df.iloc[i,0]\n        if pred_df.iloc[i,1] == 'ST':\n            st = st + ' ' + pred_df.iloc[i,0]\n    return (poi + '/' + st).replace(' ##', '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference on dev set"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"dev_df = pd.read_csv('/kaggle/input/scl2021data/dev.csv')\ndev_df['POI/street'] = dev_df[['POI','street']].fillna('').agg('/'.join, axis=1)\ndev_df"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\ndev_df['pred'] = dev_df['raw_address'].apply(lambda x: return_pred(x))\ndev_df.head()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"dev_df['result'] = np.where(df['raw_address'] == df['pred'], 1, 0)\nprint(dev_df['result'].sum() / dev_df['result'].count())"},{"metadata":{},"cell_type":"markdown","source":"# Inference on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_df['POI/street'] = test_df['raw_address'].apply(lambda x: return_pred(x))\ntest_df.to_csv('/kaggle/working/submit.csv', index=False)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def preprocess_test_data(raw_df, output_name):\n    start_counter = 1\n    end_counter = 0\n    raw_df['start'] = start_counter\n    raw_df['end'] = end_counter\n    with open(output_name, 'a') as text_file:\n        for index, row in tqdm(raw_df.iterrows()):\n            raw_df.loc[index, 'start'] = start_counter\n            address = row['raw_address'].replace(\",\", \"\").split()\n            for address_word in address:\n                text_file.write(address_word + '\\n')\n                start_counter += 1\n                end_counter += 1\n            raw_df.loc[index, 'end'] = end_counter\n    return raw_df\n\ntest_processed_df = preprocess_test_data(test_df, 'test_temp.txt')\ntest_processed_df.head()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def extract_word(file, row_start, row_end):\n    '''\n    format model prediction output in submission format\n    '''\n    poi = ''\n    st = ''\n    f=open(file)\n    lines = f.readlines()\n    i = row_start\n    while i >= row_start and i <= row_end:\n        if len(lines[i]) > 1 :\n            word = lines[i].split()[0]  \n            tag = lines[i].split()[1]\n        else:\n            tag = 'O'\n        if tag == 'POI':\n            poi = poi + ' ' + word\n        elif tag == 'ST':\n            st = st + ' ' + word\n        i += 1\n    return poi.strip() + '/' + st.strip()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_processed_df['POI/street']=''\nprintcounter = 0\nfor i in tqdm(range(0, len(test_processed_df))):\n    row_start = test_processed_df.loc[i, 'start']\n    row_end = test_processed_df.loc[i, 'end']\n    test_processed_df.loc[i, 'POI/street'] = extract_word(test_prediction_path, row_start, row_end)\n    # add checkpoints\n    if (printcounter == 1000):\n        test_processed_df[['id','POI/street']].to_csv('submit.csv', index=False)\n        printcounter = 0\n        printcounter += 1\n        \ntest_processed_df[['id','POI/street']].to_csv('submit.csv', index=False)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}