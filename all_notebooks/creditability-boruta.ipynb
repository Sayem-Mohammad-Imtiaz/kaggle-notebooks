{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-25T14:19:39.963939Z","iopub.execute_input":"2021-07-25T14:19:39.964557Z","iopub.status.idle":"2021-07-25T14:19:39.974502Z","shell.execute_reply.started":"2021-07-25T14:19:39.964519Z","shell.execute_reply":"2021-07-25T14:19:39.973561Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQIqEpVVea3zN0yAD0CIUFsV6aBpejsv68qPw&usqp=CAU)ushisky.com","metadata":{}},{"cell_type":"markdown","source":"#Got it now? It's Boruta and Python the Universal language in Kaggle. \n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7zrJJafcYRJZVjD9pommXfSOhTpTgIdXIIA&usqp=CAU)listendata.com","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/cusersmarildownloadsgermancsv/german.csv',encoding ='ISO-8859-1',sep=\";\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:19:44.551517Z","iopub.execute_input":"2021-07-25T14:19:44.551882Z","iopub.status.idle":"2021-07-25T14:19:44.58264Z","shell.execute_reply.started":"2021-07-25T14:19:44.551851Z","shell.execute_reply":"2021-07-25T14:19:44.581764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection","metadata":{}},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# lets create a feature matrix\nfeature_names =    ['Creditability','Account_Balance', 'Duration_of_Credit_monthly', 'Payment_Status_of_Previous_Credit',  'Credit_Amount', 'Value_Savings_Stocks',  'Length_of_current_employment',\n   'Most_valuable_available_asset', 'Age_years', 'No_of_Credits_at_this_Bank', 'Occupation', 'Foreign_Worker']","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:19:49.379167Z","iopub.execute_input":"2021-07-25T14:19:49.379652Z","iopub.status.idle":"2021-07-25T14:19:49.385373Z","shell.execute_reply.started":"2021-07-25T14:19:49.379612Z","shell.execute_reply":"2021-07-25T14:19:49.384168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Discarding FeaturesAccount_Balance\n\nSo, only the first 6 features should be picked by the model and the rest should be discarded. Now, y should be a complex function of the dependent variables and should include linear and non_linear relationships along with interaction effects between the features.\n\nThe function below will be used to determine y.\n\ny = X_linear + (X_nonlinear_square)^2 + sin(3 * X_nonlinear_sin) + (X_interaction_1 + X_interaction_2 + X_interaction_3)","metadata":{}},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# define function for creating y\ndef yfromX(X):\n    y = X['Creditability'] + X['Duration_of_Credit_monthly']**2 + np.sin(3 * X['Account_Balance']) + (X['Payment_Status_of_Previous_Credit'] * X['Credit_Amount'] * X['Age_years'])\n    return y","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:19:55.019485Z","iopub.execute_input":"2021-07-25T14:19:55.019848Z","iopub.status.idle":"2021-07-25T14:19:55.024884Z","shell.execute_reply.started":"2021-07-25T14:19:55.019819Z","shell.execute_reply":"2021-07-25T14:19:55.023796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Create X and Y datasets.","metadata":{}},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\nfrom sklearn.model_selection import train_test_split\n\n# create x and y\nnp.random.seed(0)\n\nX = pd.DataFrame(np.random.normal(size = (20000, len(feature_names))), columns = feature_names)\ny = yfromX(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:20:00.310688Z","iopub.execute_input":"2021-07-25T14:20:00.311044Z","iopub.status.idle":"2021-07-25T14:20:00.336816Z","shell.execute_reply.started":"2021-07-25T14:20:00.311011Z","shell.execute_reply":"2021-07-25T14:20:00.335657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Feature Selection - The Usual Way\n\nGet the features the usual way by running a few popular and unpopular models.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom eli5.sklearn import PermutationImportance","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:20:05.31426Z","iopub.execute_input":"2021-07-25T14:20:05.314688Z","iopub.status.idle":"2021-07-25T14:20:05.319894Z","shell.execute_reply.started":"2021-07-25T14:20:05.314654Z","shell.execute_reply":"2021-07-25T14:20:05.318865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# linear regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nlr_train_preds = lr.predict(X_train)\nlr_test_preds = lr.predict(X_test)\n\nlr_train_mae = mean_absolute_error(y_train, lr_train_preds)\nlr_test_mae = mean_absolute_error(y_test, lr_test_preds)\n\nlr_fi = PermutationImportance(lr, cv = 'prefit', n_iter = 3).fit(X_train, y_train).feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:20:11.402481Z","iopub.execute_input":"2021-07-25T14:20:11.40286Z","iopub.status.idle":"2021-07-25T14:20:11.635133Z","shell.execute_reply.started":"2021-07-25T14:20:11.402826Z","shell.execute_reply":"2021-07-25T14:20:11.633984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#That will take some time","metadata":{}},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# KNN\n#knn = KNeighborsRegressor(n_neighbors = int(np.sqrt(len(X_train))))\n#knn.fit(X_train, y_train)\n\n#knn_train_preds = knn.predict(X_train)\n#knn_test_preds = knn.predict(X_test)\n\n#knn_train_mae = mean_absolute_error(y_train, knn_train_preds)\n#knn_test_mae = mean_absolute_error(y_test, knn_test_preds)\n\n#knn_fi = PermutationImportance(knn).fit(X_train, y_train).feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:12:18.824417Z","iopub.execute_input":"2021-07-25T14:12:18.824968Z","iopub.status.idle":"2021-07-25T14:19:09.217562Z","shell.execute_reply.started":"2021-07-25T14:12:18.824935Z","shell.execute_reply":"2021-07-25T14:19:09.214928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# Support Vector Regression \n#svr = SVR(C = .1)\n#svr.fit(X_train, y_train)\n\n#svr_train_preds = svr.predict(X_train)\n#svr_test_preds = svr.predict(X_test)\n\n#svr_train_mae = mean_absolute_error(y_train, svr_train_preds)\n#svr_test_mae = mean_absolute_error(y_test, svr_test_preds)\n\n#svr_fi = PermutationImportance(svr).fit(X_train, y_train).feature_importances_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# Random Forest \nrf = RandomForestRegressor(max_depth=5)\nrf.fit(X_train, y_train)\n\nrf_train_preds = rf.predict(X_train)\nrf_test_preds = rf.predict(X_test)\n\nrf_train_mae = mean_absolute_error(y_train, rf_train_preds)\nrf_test_mae = mean_absolute_error(y_test, rf_test_preds)\n\nrf_fi = rf.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:20:21.832276Z","iopub.execute_input":"2021-07-25T14:20:21.832796Z","iopub.status.idle":"2021-07-25T14:20:29.52869Z","shell.execute_reply.started":"2021-07-25T14:20:21.832752Z","shell.execute_reply":"2021-07-25T14:20:29.527886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# XGBOOST \n#xgb = XGBRegressor(max_depth=5)\n#xgb.fit(X_train, y_train)\n\n#xgb_train_preds = xgb.predict(X_train)\n#xgb_test_preds = xgb.predict(X_test)\n\n#xgb_train_mae = mean_absolute_error(y_train, xgb_train_preds)\n#xgb_test_mae = mean_absolute_error(y_test, xgb_test_preds)\n\n#xgb_fi = xgb.feature_importances_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# Light GBM\nlgb = LGBMRegressor(max_depth=5)\nlgb.fit(X_train, y_train)\n\nlgb_train_preds = lgb.predict(X_train)\nlgb_test_preds = lgb.predict(X_test)\n\nlgb_train_mae = mean_absolute_error(y_train, lgb_train_preds)\nlgb_test_mae = mean_absolute_error(y_test, lgb_test_preds)\n\nlgb_fi = lgb.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:50:25.538478Z","iopub.execute_input":"2021-07-25T14:50:25.540467Z","iopub.status.idle":"2021-07-25T14:50:25.833365Z","shell.execute_reply.started":"2021-07-25T14:50:25.540353Z","shell.execute_reply":"2021-07-25T14:50:25.832404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# create a dataframe for feature importances and mean absolute error\n#mae_df = pd.DataFrame(columns=['Train','Test'])\n\n# add mae's to the mae_df\n#mae_df.loc['Linear Regression','Train'] =  lr_train_mae\n#mae_df.loc['Linear Regression','Test'] =  lr_test_mae\n\n#mae_df.loc['KNN','Train'] =  knn_train_mae\n#mae_df.loc['KNN','Test'] =  knn_test_mae\n\n#mae_df.loc['Support Vector Regression','Train'] =  svr_train_mae\n#mae_df.loc['Support Vector Regression','Test'] =  svr_test_mae\n\n#mae_df.loc['Random Forest','Train'] =  rf_train_mae\n#mae_df.loc['Random Forest','Test'] =  rf_test_mae\n\n#mae_df.loc['XGBoost','Train'] =  xgb_train_mae\n#mae_df.loc['XGBoost','Test'] =  xgb_test_mae\n\n#mae_df.loc['Light GBM','Train'] =  lgb_train_mae\n#mae_df.loc['Light GBM','Test'] =  lgb_test_mae\n\n#mae_df['Model'] = mae_df.index\n#mae_df['Train'] = mae_df['Train'].astype(float)\n#mae_df['Test'] = mae_df['Test'].astype(float)\n\n#mae_df = pd.melt(mae_df, id_vars=['Model'], value_vars=['Train','Test'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter \nimport matplotlib.ticker as mtick","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:20:47.537265Z","iopub.execute_input":"2021-07-25T14:20:47.537677Z","iopub.status.idle":"2021-07-25T14:20:47.6183Z","shell.execute_reply.started":"2021-07-25T14:20:47.537644Z","shell.execute_reply":"2021-07-25T14:20:47.617434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# plot mae \n#fig,ax = plt.subplots(figsize=(10,4))\n#ax = sns.barplot(x='value',y='Model', hue='variable',data=mae_df.sort_values(by='value',ascending=False))\n#plt.xlabel('Mean Absolute Error')\n#plt.ylabel('')\n#plt.title('Mean Absolute Error Comparison')\n#plt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n# create feature importance dataframe \n#fi_df = pd.DataFrame(columns=['LR', 'KNN','SVR','RF','XGB','LGBM','Features'])\n\n#fi_df['Features'] = feature_names\n#fi_df['LR'] = lr_fi\n#fi_df['KNN'] = knn_fi\n#fi_df['SVR'] = svr_fi\n#fi_df['RF'] = rf_fi\n#fi_df['XGB'] = xgb_fi\n#fi_df['LGBM'] = lgb_fi/1000","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\n#fig = plt.figure(figsize=(18,8))\n\n#plt.subplot(2, 3, 1)\n#ax = sns.barplot(x='LR',y='Features',data=fi_df.sort_values(by='LR',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Linear Regression')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 2)\n#ax = sns.barplot(x='KNN',y='Features',data=fi_df.sort_values(by='KNN',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('K-Nearest Neighbors')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 3)\n#ax = sns.barplot(x='SVR',y='Features',data=fi_df.sort_values(by='SVR',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Support Vector Regression')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 4)\n#ax = sns.barplot(x='RF',y='Features',data=fi_df.sort_values(by='RF',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Random Forest')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 5)\n#ax = sns.barplot(x='XGB',y='Features',data=fi_df.sort_values(by='XGB',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('XGBoost')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()\n\n#plt.subplot(2, 3, 6)\n#ax = sns.barplot(x='LGBM',y='Features',data=fi_df.sort_values(by='LGBM',ascending=False),color='b')\n#ax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n#plt.xlabel('Feature Importance')\n#plt.ylabel('')\n#plt.title('Light GBM')\n#plt.axvline(x=0.1, color='r', linestyle='dashed')\n#plt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\nfrom boruta import BorutaPy\n\nnew_rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n\nboruta_selector = BorutaPy(new_rf, n_estimators = 'auto', random_state = 0)\nboruta_selector.fit(np.array(X_train), np.array(y_train))\n\nboruta_ranking = boruta_selector.ranking_\nselected_features = np.array(feature_names)[boruta_ranking <= 2]","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:21:47.069323Z","iopub.execute_input":"2021-07-25T14:21:47.06973Z","iopub.status.idle":"2021-07-25T14:25:09.634057Z","shell.execute_reply.started":"2021-07-25T14:21:47.069697Z","shell.execute_reply":"2021-07-25T14:25:09.633006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\nboruta_ranking = pd.DataFrame(data=boruta_ranking, index=X_train.columns.values, columns=['values'])\nboruta_ranking['Variable'] = boruta_ranking.index\nboruta_ranking.sort_values(['values'], ascending=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:25:17.550912Z","iopub.execute_input":"2021-07-25T14:25:17.551274Z","iopub.status.idle":"2021-07-25T14:25:17.559474Z","shell.execute_reply.started":"2021-07-25T14:25:17.551236Z","shell.execute_reply":"2021-07-25T14:25:17.558086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Ajay Sampath https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n\nfig,ax = plt.subplots(figsize=(8,4))\nax = sns.barplot(x='values',y='Variable',data=boruta_ranking, color='b')\nplt.title('Boruta Feature Ranking')\nplt.xlabel('')\nplt.ylabel('')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T14:25:40.837529Z","iopub.execute_input":"2021-07-25T14:25:40.837882Z","iopub.status.idle":"2021-07-25T14:25:41.222646Z","shell.execute_reply.started":"2021-07-25T14:25:40.837851Z","shell.execute_reply":"2021-07-25T14:25:41.221859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Boruta's ranking output (Rank 1: confirmed, Rank 2: some influence, Above Rank 3: Rejected \n\nThe output from Boruta is a ranking number. So we can divide them into categories are need. All features with a rank 1 mean that these have a confirmed effect on the target variable. A rank 2 indicates that there is some influence. Anything above 3 can be rejected as it does not have any infuence.\n\nUsing Boruta, we picked all the correct dependent variables and rejected the noise(??).\n\nBoruta can be a very powerful tool to use and check feature importances from traditional methods.","metadata":{}},{"cell_type":"markdown","source":"#Thank you A.J. (Ajay Sampath) https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection\n","metadata":{}}]}