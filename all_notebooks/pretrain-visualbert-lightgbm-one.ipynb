{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### imports\n***","metadata":{}},{"cell_type":"code","source":"#!wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231786/sample/multimodal_train_sampleset.zip\n!wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231786/multimodal_valid.zip\n!wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231786/multimodal_testA.zip\n!wget http://tianchi-public-us-east-download.oss-us-east-1.aliyuncs.com/231786/multimodal_testB.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!unzip  multimodal_train_sampleset.zip\n!unzip  multimodal_valid.zip\n!unzip  multimodal_testA.zip\n!unzip  multimodal_testB.zip\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch import optim\nfrom torch.optim.lr_scheduler import LambdaLR\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom gensim import corpora, similarities, models\nfrom gensim.matutils import corpus2csc\n\nimport pandas as pd \nimport numpy as np\nfrom collections import Counter\nimport base64\nfrom tqdm import tqdm\nimport pickle, random\nimport matplotlib.pyplot as plt\nfrom multiprocessing import Pool\nimport sys, csv, json, os, gc, time","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch import optim\nfrom torch.optim.lr_scheduler import LambdaLR\n\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom gensim import corpora, similarities, models\nfrom gensim.matutils import corpus2csc\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\n\nimport pandas as pd \nimport numpy as np\nfrom collections import Counter\nimport base64\nfrom tqdm.auto import tqdm\nimport pickle, random\nimport matplotlib.pyplot as plt\nfrom multiprocessing import Pool\nimport sys, csv, json, os, gc, time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testCode=False\nif testCode:\n    Data = pd.read_csv(r'./train.sample.tsv', sep='\\t',error_bad_lines=False, quoting=csv.QUOTE_NONE, encoding='utf-8')\n    Data.reset_index(inplace=True)\n    for i in range(0, 20):\n        Data[i*500 : (i+1)*500].to_pickle('./train_sample_{}.pkl'.format(i))\n    del Data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splitTrainData=False\nif splitTrainData:\n    Data = pd.read_csv(r'../data/train.tsv', sep='\\t', quoting=csv.QUOTE_NONE, encoding='utf-8').sample(frac=0.1)\n    Data.reset_index(inplace=True)\n    for i in range(0, 20):\n        Data[i*15000 : (i+1)*15000].to_pickle('../kaggleData/train_{}.pkl'.format(i))\n    del Data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### parameters\n***","metadata":{}},{"cell_type":"code","source":"no = '0'\ndevice = torch.device('cuda:'+no) if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nk = 10\nlr = 1e-5\n# true batch size = batch_size * grad_step\nbatch_size = 64\nmargin = 8\ngrad_step = 1\nmax_lang_len = 15\nmax_img_len = 30\nepochs = 1\nMOD = 20000\nshuffle_fold = True\nseed = 9527\nrandom.seed(seed)\ntorch.manual_seed(seed)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class params:\n    IMG_FEAT_SIZE = 2048+6\n    WORD_EMBED_SIZE = 1024\n    LAYER = 6\n    HIDDEN_SIZE = 1024\n    MULTI_HEAD = 8\n    DROPOUT_R = 0.1\n    FLAT_MLP_SIZE = 512\n    FLAT_GLIMPSES = 1\n    FLAT_OUT_SIZE = 2048\n    FF_SIZE = HIDDEN_SIZE*4\n    HIDDEN_SIZE_HEAD = HIDDEN_SIZE // MULTI_HEAD\n    OPT_BETAS = (0.9, 0.98)\n    OPT_EPS = 1e-9\n    TRAIN_SIZE = 300000#三十万\n\n__C = params()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### load data\n***","metadata":{}},{"cell_type":"code","source":"trash = {'!', '$', \"'ll\", \"'s\", ',', '&', ':', 'and', 'cut', 'is', 'are', 'was'}\ntrash_replace = ['\"hey siri, play some', 'however, ', 'yin and yang, ',\n                 'shopping mall/']\n\ndef process(x):\n    tmp = x.split()\n    if tmp[0] in trash: x = ' '.join(tmp[1:])\n    if tmp[0][0] == '-': x = x[1:]\n    for tr in trash_replace:\n        x = x.replace(tr, '')\n    return x\n\ndef normalize(x):\n    ret = x['boxes'].copy()\n    ret[:,0] /= x['image_h']\n    ret[:,1] /= x['image_w']\n    ret[:,2] /= x['image_h']\n    ret[:,3] /= x['image_w']\n    wh = (ret[:,2]-ret[:,0]) * (ret[:,3]-ret[:,1])\n    wh2 = (ret[:,2]-ret[:,0]) / (ret[:,3]-ret[:,1]+1e-6)\n    ret = np.hstack((ret, wh.reshape(-1,1), wh2.reshape(-1,1)))\n    return ret\n\ndef sort_by_area(x):\n    return np.array(sorted(x.tolist(), key=lambda x: x[-1], reverse=True))\n\ndef load_data(file_name, reset=False, decode=True,pickle=False):\n    if pickle:\n        ret=pd.read_pickle(file_name)\n    else:\n        ret = pd.read_csv(file_name, sep='\\t')\n    if decode:\n        ret['boxes'] = ret['boxes'].apply(lambda x: np.frombuffer(base64.b64decode(x), dtype=np.float32).reshape(-1, 4))\n        ret['features'] = ret['features'].apply(lambda x: np.frombuffer(base64.b64decode(x), dtype=np.float32).reshape(-1, 2048))\n        ret['class_labels'] = ret['class_labels'].apply(lambda x: np.frombuffer(base64.b64decode(x), dtype=np.int64).reshape(-1, 1))\n        ret['boxes'] = ret.apply(lambda x: normalize(x), axis=1)\n        ret['features'] = ret.apply(lambda x: np.concatenate((x['features'], x['boxes']), axis=1)[:max_img_len], axis=1)\n    ret['query'] = ret['query'].apply(lambda x: process(x))\n    # reset query_id\n    if reset:\n        query2qid = {query: qid for qid, (query, _) in enumerate(tqdm(ret.groupby('query')))}\n        ret['query_id'] = ret.apply(lambda x: query2qid[x['query']], axis=1)\n    return ret","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/graduationkaggle/'\nvalidDf = load_data('./valid.tsv')\ntestADf = load_data('./testA.tsv')\ntestBDf = load_data('./testB.tsv')\nvalidAnswer = json.loads(open('./valid_answer.json', 'r').read())\ntestAAnswer = json.loads(open('../input/graduationtesta-banswer/testA_answer.json', 'r').read())\ntestBAnswer = json.loads(open('../input/graduationtesta-banswer/testB_answer.json', 'r').read())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### preprocess\n***","metadata":{}},{"cell_type":"code","source":"# load pre-trained model\ntake = 'bert-large-uncased'\nemb_size = __C.WORD_EMBED_SIZE\ntokenizer = AutoTokenizer.from_pretrained(take)\npretrained_emb = AutoModel.from_pretrained(take)\npad_id = tokenizer.pad_token_id\nsep_id = tokenizer.sep_token_id\n\nqid2token = {qid: tokenizer.encode(group['query'].values[0]) for qid, group in tqdm(testADf.groupby('query_id'))}\ntestADf['token'] = testADf['query_id'].apply(lambda x: qid2token[x])\ntestADf['token'] = testADf['token'].apply(lambda x: x[:max_lang_len-1]+[sep_id] if len(x) > max_lang_len else x)\n\nqid2token = {qid: tokenizer.encode(group['query'].values[0]) for qid, group in tqdm(testBDf.groupby('query_id'))}\ntestBDf['token'] = testBDf['query_id'].apply(lambda x: qid2token[x])\ntestBDf['token'] = testBDf['token'].apply(lambda x: x[:max_lang_len-1]+[sep_id] if len(x) > max_lang_len else x)\n\nqid2token = {qid: tokenizer.encode(group['query'].values[0]) for qid, group in tqdm(validDf.groupby('query_id'))}\nvalidDf['token'] = validDf['query_id'].apply(lambda x: qid2token[x])\nvalidDf['token'] = validDf['token'].apply(lambda x: x[:max_lang_len-1]+[sep_id] if len(x) > max_lang_len else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testBDf['target'] = testBDf.apply(lambda x: 1 if x['product_id'] in testBAnswer[str(x['query_id'])] else 0, axis=1)\ntestADf['target'] = testADf.apply(lambda x: 1 if x['product_id'] in testAAnswer[str(x['query_id'])] else 0, axis=1)\nvalidDf['target'] = validDf.apply(lambda x: 1 if x['product_id'] in validAnswer[str(x['query_id'])] else 0, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### training data\n***","metadata":{}},{"cell_type":"code","source":"def one(x):\n    return (x+1e-10)/(x+1e-10)\n\ndef get_negs(train):\n    qid2idxs = {}\n    corpus = []\n    idx2qid = {}\n\n    for idx, (qid, group) in enumerate(train.groupby('query_id')):\n        qid2idxs[qid] = group.index\n        corpus.append(group['query'].values[0])\n        idx2qid[idx] = qid\n\n    topk = len(max(qid2idxs.values(), key=lambda x: len(x)))*3\n    corpus = [sent.split() for sent in corpus]\n    dictionary = corpora.Dictionary(corpus)\n    corpus = [dictionary.doc2bow(text) for text in corpus]\n    tfidf_model = models.TfidfModel(corpus, wlocal=one, dictionary=dictionary)\n    corpus_tfidf = corpus2csc(tfidf_model[corpus])\n    sm = corpus_tfidf.T.dot(corpus_tfidf)\n    qid2negs = {}\n    \n    for idx, (le, ri) in enumerate(tqdm(zip(sm.indptr[:-1], sm.indptr[1:]), total=sm.shape[0])):\n        n_row_pick = min(topk, ri-le)\n        top_n_idx = sm.indices[le+np.argpartition(sm.data[le:ri], -n_row_pick)[-n_row_pick:]].tolist()\n        if n_row_pick < topk: top_n_idx += random.sample(range(sm.shape[0]), topk-n_row_pick)\n        qid2negs[idx2qid[idx]] = [idx2qid[neg] for neg in top_n_idx if neg != idx]\n        \n    return qid2negs, qid2idxs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(train, qid2negs, qid2idxs):\n    train_x = [] # [tokens, feature1, feature2]\n    \n    for qid, group in tqdm(train.groupby('query_id')):\n        # positive\n        pos = group.index\n        token = group['token'].iloc[0]\n        # negative\n        for i in range(k):\n            neg = random.sample(qid2negs[qid], len(pos))\n            neg = [random.choice(qid2idxs[n]) for n in neg]\n            train_x += [[token, pos[i], neg[i]] for i in range(len(pos))]\n    \n    print('number of training data:', len(train_x))\n    return train_x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model\n***","metadata":{}},{"cell_type":"code","source":"class FC(nn.Module):\n    def __init__(self, in_size, out_size, dropout_r=0., use_relu=True):\n        super(FC, self).__init__()\n        self.dropout_r = dropout_r\n        self.use_relu = use_relu\n\n        self.linear = nn.Linear(in_size, out_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(dropout_r)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return x\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n        super(MLP, self).__init__()\n\n        self.fc = FC(in_size, mid_size, dropout_r=dropout_r, use_relu=use_relu)\n        self.linear = nn.Linear(mid_size, out_size)\n\n    def forward(self, x):\n        return self.linear(self.fc(x))\n\n\nclass VisualBERT(nn.Module):\n    def __init__(self, __C, bert):\n        super(VisualBERT, self).__init__()\n        \n        self.bert = bert\n        self.linear_img = nn.Linear(__C.IMG_FEAT_SIZE, __C.WORD_EMBED_SIZE)\n        self.out = MLP(__C.WORD_EMBED_SIZE, __C.WORD_EMBED_SIZE//2, 1)\n        \n    def forward(self, ques_ix, img_feats):\n        proj_feats = []\n        for img_feat in img_feats:\n            # Make mask & token type ids\n            mask = self.make_mask(ques_ix.unsqueeze(2), img_feat, pad_id)\n            token = self.get_token_type(ques_ix, img_feat)\n            # Preprocess features\n            lang_feat = self.bert.embeddings.word_embeddings(ques_ix)\n            img_feat = self.linear_img(img_feat)\n            combine_feat = torch.cat((lang_feat, img_feat), dim=1)\n            # Token embeddings & position embeddings\n            position_ids = torch.arange(token.size(1), dtype=torch.long, device=device)\n            position_ids = position_ids.unsqueeze(0).expand(token.size())\n            position_embeddings = self.bert.embeddings.position_embeddings(position_ids)\n            token_type_embeddings = self.bert.embeddings.token_type_embeddings(token)\n            # Add all\n            embeddings = combine_feat+position_embeddings+token_type_embeddings\n            embeddings = self.bert.embeddings.dropout(self.bert.embeddings.LayerNorm(embeddings))\n            # Go through the rest of BERT\n            head_mask = self.bert.get_head_mask(None, self.bert.config.num_hidden_layers)\n            extended_attention_mask = self.bert.get_extended_attention_mask(mask, mask.size(), device)\n            encoder_outputs = self.bert.encoder(embeddings,\n                                                attention_mask=extended_attention_mask,\n                                                head_mask=head_mask,\n                                                encoder_hidden_states=None,\n                                                encoder_attention_mask=None)\n            # CLS embedding & output value\n            outputs = encoder_outputs[0][:,0,:]\n            proj_feats.append(self.out(outputs))\n        return proj_feats\n            \n    # Masking\n    def make_mask(self, lang_feat, img_feat, target):\n        # 1 for NOT masked; 0 for masked\n        # [batch, len]\n        lang_mask = (torch.sum(torch.abs(lang_feat), dim=-1) != target).long()\n        img_mask = (torch.sum(torch.abs(img_feat), dim=-1) != 0).long()\n        return torch.cat((lang_mask, img_mask), dim=1)\n    \n    # Token type ids\n    def get_token_type(self, lang_feat, img_feat):\n        #    lang      img\n        # 0 0 0 0 0 0 1 1 1 1 1\n        lang_token = torch.zeros(lang_feat.size(0), lang_feat.size(1)).to(device)\n        img_token = torch.ones(img_feat.size(0), img_feat.size(1)).to(device)\n        return torch.cat((lang_token, img_token), dim=1).long()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train\n***","metadata":{}},{"cell_type":"code","source":"def predict(model,test):\n    model = model.eval()\n    preds = {}\n    \n    with torch.no_grad():\n        for qid, group in tqdm(test.groupby('query_id')):\n            # prepare batch\n            tokens, features = group['token'].values.tolist(), group['features'].values.tolist()\n            max_len_f = len(max(features, key=lambda x: len(x)))\n            features = [np.concatenate((feature, np.zeros((max_len_f-feature.shape[0], feature.shape[1]))), axis=0) for feature in features]\n            # # to tensor\n            tokens = torch.LongTensor(tokens).to(device)\n            features = torch.FloatTensor(features).to(device)\n            # predict\n            out = model(tokens, (features,))[0].view(-1)\n            pred = [(pid, val) for pid, val in zip(group['product_id'].values.tolist(), out.tolist())]\n            pred.sort(key=lambda x: x[1], reverse=True)\n            preds[qid] = [pid for pid, _ in pred]\n            \n    model = model.train()\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(data.Dataset):\n    def __init__(self, train_x):\n        self.train_x = train_x\n        \n    def __getitem__(self, index):\n        tokens, pos_features, neg_features = self.train_x[index][0], self.train_x[index][1], self.train_x[index][2]\n        return [tokens, pos_features, neg_features]\n    \n    def __len__(self):\n        return len(self.train_x)\n    \ndef collate_fn(batch):\n    tokens, pos_features, neg_features = zip(*batch)\n    max_len_t, max_len_pf, max_len_nf = len(max(tokens, key=lambda x: len(x))), len(max(pos_features, key=lambda x: len(x))), len(max(neg_features, key=lambda x: len(x)))\n    tokens, pos_features, neg_features = [token+[pad_id]*(max_len_t-len(token)) for token in tokens], [np.concatenate((feature, np.zeros((max_len_pf-feature.shape[0], feature.shape[1]))), axis=0) for feature in pos_features], [np.concatenate((feature, np.zeros((max_len_nf-feature.shape[0], feature.shape[1]))), axis=0) for feature in neg_features]\n    return torch.LongTensor(tokens), torch.FloatTensor(pos_features), torch.FloatTensor(neg_features)\n\ndef custom_schedule(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, amplitude=0.1, last_epoch=-1):\n    \n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = 2.0 * math.pi * float(num_cycles) * float(current_step-num_warmup_steps) / float(max(1, num_training_steps-num_warmup_steps))\n        linear = float(num_training_steps-current_step) / float(max(1, num_training_steps-num_warmup_steps))\n        return abs(linear + math.sin(progress)*linear*amplitude)\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\ndef shuffle(x):\n    idxs = [i for i in range(x.shape[0])]\n    random.shuffle(idxs)\n    return x[idxs]\n\ndef nDCG_score(preds, answers):\n    iDCG = sum([sum([np.log(2)/np.log(i+2) for i in range(min(len(answer), 5))]) \\\n                for answer in list(answers.values())])\n    DCG = sum([sum([np.log(2)/np.log(i+2) if preds[qid][i] in answers[str(qid)] else 0 \\\n                    for i in range(len(preds[qid]))]) for qid in list(preds.keys())])\n    return DCG/iDCG\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_merge(idx):\n#     train = load_data(path+'train.sample.tsv', reset=True)\n    if testCode:\n        train = load_data(path+'train_sample_{}.pkl'.format(idx), reset=True, decode=False,pickle=True)\n    else:\n        train = load_data(path+'train_{}.pkl'.format(idx), reset=True, decode=False,pickle=True)\n    train.reset_index(inplace=True)\n    qid2token = {qid: tokenizer.encode(group['query'].values[0]) for qid, group in train.groupby('query_id')}\n    train['token'] = train['query_id'].apply(lambda x: qid2token[x])\n    qid2negs, qid2idxs = get_negs(train)\n    if testCode:\n        pickle.dump(get_data(train, qid2negs, qid2idxs),\n                    open(path+'data/train_sample_x_{}_{}.pkl'.format(idx, seed), 'wb'))\n    else:\n        pickle.dump(get_data(train, qid2negs, qid2idxs),\n                    open(path+'data/train_x_{}_{}.pkl'.format(idx, seed), 'wb'))\n    return None\n\ndef load_features(idx):\n    print('load  data idx ---->:',idx)\n#     train = load_data(path+'train.sample.tsv', reset=True)\n    if testCode:\n        train = load_data(path+'train_sample_{}.pkl'.format(idx), reset=True,pickle=True)\n        train_x = pickle.load(open(path+'train_sample_x_{}_{}.pkl'.format(idx, seed), 'rb'))\n    else:\n        train = load_data(path+'train_{}.pkl'.format(idx), reset=True,pickle=True)\n        train_x = pickle.load(open(path+'train_x_{}_{}.pkl'.format(idx, seed), 'rb'))\n    train.reset_index(inplace=True)\n    train_x = [[t, train['features'].iloc[p], train['features'].iloc[n]] \\\n               for t, p, n in train_x]\n    return train_x\n\ndef get_data_chunk(start, end):\n    ret = []\n    for fold in range(start, end):\n        t0 = time.time()\n        print('reading fold:', fold)\n\n        try:\n            temp=load_features(fold_idxs[fold])\n        except:\n            print(fold_idxs[fold],' is wrong !!!!!!!!!!!!!!!!!!!')\n        else:\n            ret+=temp\n        t = round((time.time()-t0))\n        print('time consumed: {} min {} sec'.format(t//60, t%60))\n    return ret","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduceDataset=False\nif reduceDataset:\n    t0 = time.time()\n    with Pool(5) as pool:\n        pool.map(get_data_merge, [i for i in range(20)])\n    t = round((time.time()-t0)/60)\n    print('time consumed: {} hr {} min'.format(t//60, t%60))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('initializing model...')\nnDCGs=[]\nbest_nDCG = 0.0\nmodel = VisualBERT(__C, pretrained_emb).to(device)\nnum_training_steps = np.ceil(__C.TRAIN_SIZE*k / (batch_size*grad_step)) * epochs\nnum_warmup_steps = int(num_training_steps*0.1)\neval_steps = num_training_steps//40*grad_step + 100\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\nscheduler = custom_schedule(optimizer,\n                            num_warmup_steps=num_warmup_steps,\n                            num_training_steps=num_training_steps,\n                            num_cycles=6,\n                            amplitude=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('../input/graduationkagglemedmodal/model_Visual-BERT_pair_box_tfidf-neg_focal_all_7328_39',map_location=device))\nmodel=model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndef train_empohs():\n    global model\n    for chunk in range(5):\n        train_x_all = get_data_chunk(chunk*3, min((chunk+1)*3,20))\n        gc.collect()\n        random.shuffle(train_x_all)\n        steps = (min((chunk+1)*3, 20) - chunk*3) * 2\n        step_size = len(train_x_all)//steps\n\n        for step in range(steps):\n            train_loader = data.DataLoader(CustomDataset(train_x_all[step_size*step:step_size*(step+1)]),\n                                           batch_size=batch_size,\n                                           shuffle=True,\n                                           collate_fn=collate_fn,\n                                           num_workers=4)\n            model = model.train()\n            criterion = FocalLoss()\n            total_loss = 0.0\n            step = len(train_loader)\n            optimizer.zero_grad()\n            pbar = tqdm(enumerate(train_loader), total=step)\n\n            for i, batch in pbar:\n                # prepare batch\n                tokens, pos_features, neg_features = batch\n                # # to device\n                tokens = tokens.to(device)\n                pos_features = pos_features.to(device)\n                neg_features = neg_features.to(device)\n                # predict\n                pos, neg = model(tokens, (pos_features, neg_features))\n                pos = torch.sigmoid(pos).view(-1)\n                neg = torch.sigmoid(neg).view(-1)\n                l = criterion(pos, torch.ones(pos.size()).to(device))\n                l.backward()\n                total_loss += l.item()\n                l = criterion(neg, torch.zeros(neg.size()).to(device))\n                l.backward()\n                total_loss += l.item()\n                pbar.set_postfix({'loss': total_loss/(i+1)})\n                # optim step\n                if (i+1)%grad_step == 0:\n                    optimizer.step()\n                    scheduler.step()\n                    optimizer.zero_grad()\n\n            if step%grad_step:\n                optimizer.step()\n                scheduler.step()\n            # delete garbage\n            del train_loader\n        del train_x_all\n        gc.collect()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getNDCG(preds, answers,k):\n    iDCG = sum([sum([np.log(2)/np.log(i+2) for i in range(min(len(answer), k))]) \\\n                for answer in list(answers.values())])\n    DCG = sum([sum([np.log(2)/np.log(i+2) if preds[qid][i] in answers[str(qid)] else 0 \\\n                    for i in range(min(len(preds[qid]), k))]) for qid in list(preds.keys())])\n    return DCG/iDCG","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Recall(pred,target,k):\n    k=min(len(pred), k)\n    n = len( np.intersect1d(pred[:k],target))\n    if len(target)>0:\n        return n / len(target)\n    else:\n        return 0\ndef getRecall(preds,targets,k):\n    scores=[]\n    for query in list(preds.keys()):\n        scores.append(Recall(preds[query],targets[str(query)],k))\n    return np.mean(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Precision(pred,target,k):\n    k=min(len(pred), k)\n    n = len( np.intersect1d(pred[:k],target))\n    if k>0:\n        return n / k\n    else:\n        return 0\ndef getPrecision(preds,targets,k):\n    scores=[]\n    for query in list(preds.keys()):\n        scores.append(Precision(preds[query],targets[str(query)],k))\n    return np.mean(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getF1(preds,targets,k):\n    scores=[]\n    for query in list(preds.keys()):\n        p=Precision(preds[query],targets[str(query)],k)\n        r=Recall(preds[query],targets[str(query)],k)\n        if p+r>0:\n            f1=(2*p*r)/(p+r)\n        else:\n            f1=0\n        scores.append(f1)\n    return np.mean(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coding=utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef plotCurve(x,y):\n    plt.rcParams['font.sans-serif'] = ['Arial']  # 如果要显示中文字体,则在此处设为：SimHei\n    plt.rcParams['axes.unicode_minus'] = False  # 显示负号\n\n    x = np.array(x)\n    y = np.array(y)\n    plt.figure(figsize=(10, 5))\n    plt.grid(linestyle=\"--\")  # 设置背景网格线为虚线\n    ax = plt.gca()\n    ax.spines['top'].set_visible(False)  # 去掉上边框\n    ax.spines['right'].set_visible(False)  # 去掉右边框\n\n\n    plt.plot(x, y, marker='o', color=\"blue\", label=\"unpretrain\", linewidth=1.5)\n    plt.xlabel(\"epochs\", fontsize=13, fontweight='bold')\n    plt.ylabel(\"nDCG@5 on testB\", fontsize=13, fontweight='bold')\n    plt.xlim(0, 10)  # 设置x轴的范围\n    plt.ylim(0, 0.6)\n\n  # plt.legend()          #显示各曲线的图例\n    plt.legend(loc=0, numpoints=1)\n    leg = plt.gca().get_legend()\n    ltext = leg.get_texts()\n    plt.setp(ltext, fontsize=12, fontweight='bold')  # 设置图例字体的大小和粗细\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_embedding(model, ques_ix, img_feat):\n    # Make mask & token type ids\n    mask = model.make_mask(ques_ix.unsqueeze(2), img_feat, pad_id)\n    token = model.get_token_type(ques_ix, img_feat)\n    # Preprocess features\n    lang_feat = model.bert.embeddings.word_embeddings(ques_ix)\n    img_feat = model.linear_img(img_feat)\n    combine_feat = torch.cat((lang_feat, img_feat), dim=1)\n    # Token embeddings & position embeddings\n    position_ids = torch.arange(token.size(1), dtype=torch.long, device=device)\n    position_ids = position_ids.unsqueeze(0).expand(token.size())\n    position_embeddings = model.bert.embeddings.position_embeddings(position_ids)\n    token_type_embeddings = model.bert.embeddings.token_type_embeddings(token)\n    # Add all\n    embeddings = combine_feat+position_embeddings+token_type_embeddings\n    embeddings = model.bert.embeddings.dropout(model.bert.embeddings.LayerNorm(embeddings))\n    # Go through the rest of BERT\n    head_mask = model.bert.get_head_mask(None, model.bert.config.num_hidden_layers)\n    extended_attention_mask = model.bert.get_extended_attention_mask(mask, mask.size(), device)\n    encoder_outputs = model.bert.encoder(embeddings,\n                                         attention_mask=extended_attention_mask,\n                                         head_mask=head_mask,\n                                         encoder_hidden_states=None,\n                                         encoder_attention_mask=None)\n    # CLS embedding & output value\n    outputs = encoder_outputs[0][:,0,:]\n    return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cls(model, n_splits,test):\n    model.eval()\n    qids = [[qid] for qid, _ in test.groupby('query_id')]\n    kf = KFold(n_splits=n_splits, shuffle=True)\n    train_x, train_y = [], []\n    test_x, test_y = [], []\n    qid2fold = {qids[idx][0]: i \\\n                for i, (train_index, test_index) in enumerate(kf.split(qids)) \\\n                for idx in test_index}\n    \n    with torch.no_grad():\n        for qid, group in tqdm(test.groupby('query_id')):\n            # prepare batch\n            tokens, features = group['token'].values.tolist(), group['features'].values.tolist()\n            max_len_f = len(max(features, key=lambda x: len(x)))\n            features = [np.concatenate((feature, np.zeros((max_len_f-feature.shape[0], feature.shape[1]))), axis=0) for feature in features]\n            # # to tensor\n            tokens = torch.LongTensor(tokens).to(device)\n            features = torch.FloatTensor(features).to(device)\n            # predict\n            tmp_x = extract_embedding(model, tokens, features).tolist()\n            tmp_y = group['target'].values.tolist()\n            # only use first fold\n            if qid2fold[qid]:\n                train_x += tmp_x\n                train_y += tmp_y\n            else:\n                test_x += tmp_x\n                test_y += tmp_y\n    \n    train_x, train_y = np.array(train_x), np.array(train_y)\n    test_x, test_y = np.array(test_x), np.array(test_y)\n    print('train:test = {}:{}'.format(train_x.shape[0], test_x.shape[0]))\n    cls = LGBMClassifier(random_state=0, n_jobs=24)\n    cls.fit(train_x, train_y,\n            eval_set=[(test_x, test_y)],\n            early_stopping_rounds=100,\n            verbose=100)\n        \n    return cls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictCLS(model, test, thd, cls):\n    model.eval()\n    counts = Counter(test['product_id'].values.tolist())\n    preds = {}\n    \n    with torch.no_grad():\n        for qid, group in tqdm(test.groupby('query_id')):\n            # prepare batch\n            tokens, features = group['token'].values.tolist(), group['features'].values.tolist()\n            max_len_f = len(max(features, key=lambda x: len(x)))\n            features = [np.concatenate((feature, np.zeros((max_len_f-feature.shape[0], feature.shape[1]))), axis=0) for feature in features]\n            # # to tensor\n            tokens = torch.LongTensor(tokens).to(device)\n            features = torch.FloatTensor(features).to(device)\n            # predict\n            embeddings = np.array(extract_embedding(model, tokens, features).tolist())\n            out = cls.predict_proba(embeddings)[:,1]\n            pred = [(pid, val) for pid, val in zip(group['product_id'].values.tolist(), out.tolist())]\n            thd_tmp = thd\n            pred2 = [x for x in pred]\n            pred2.sort(key=lambda x: x[1], reverse=True)\n            preds[qid] = [pid for pid, _ in pred2]\n            \n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_all(model, test, pad_len, cls):\n    model.eval()\n    preds = {}\n    \n    with torch.no_grad():\n        for qid, group in tqdm(test.groupby('query_id')):\n            # prepare batch\n            tokens, features = group['token'].values.tolist(), group['features'].values.tolist()\n            max_len_f = len(max(features, key=lambda x: len(x)))\n            features = [np.concatenate((feature, np.zeros((max_len_f-feature.shape[0], feature.shape[1]))), axis=0) for feature in features]\n            # # to tensor\n            tokens = torch.LongTensor(tokens).to(device)\n            features = torch.FloatTensor(features).to(device)\n            # predict\n            embeddings = np.array(extract_embedding(model, tokens, features).tolist())\n            out = cls.predict_proba(embeddings)[:,1]\n            pred = [(pid, val) for pid, val in zip(group['product_id'].values.tolist(), out.tolist())]\n            pred.sort(key=lambda x: x[1], reverse=True)\n            assert len(pred) <= pad_len\n            pid, score = [p for p, s in pred], [s for p, s in pred]\n            pid, score = pid+[np.nan]*(pad_len-len(pred)), score+[np.nan]*(pad_len-len(pred))\n            preds[qid] = pid+score\n            \n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-------------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./content/\n!mkdir ./content/drive/\n!mkdir ./content/drive/MyDrive/\n!mkdir ./content/drive/MyDrive/baseline_bertBert/\n!mkdir ./content/drive/MyDrive/baseline_bertBert_lightGBM/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nrecallA=[]\nprecisionA=[]\nf1A=[]\nndcgA=[]\n\nrecallB=[]\nprecisionB=[]\nf1B=[]\nndcgB=[]\n\nrecallValid=[]\nprecisionValid=[]\nf1Valid=[]\nndcgValid=[]\n\n#------------------------\nLrecallA=[]\nLprecisionA=[]\nLf1A=[]\nLndcgA=[]\n\nLrecallB=[]\nLprecisionB=[]\nLf1B=[]\nLndcgB=[]\n\nLrecallValid=[]\nLprecisionValid=[]\nLf1Valid=[]\nLndcgValid=[]\npad_len = 30\nthd = 9999 \nn_splits = 10\n\ndef saveFile():\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/recallA.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(recallA, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/precisionA.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(precisionA, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/f1A.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(f1A, f)\n    # 关闭文件\n    f.close()\n\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/ndcgA.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(ndcgA, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/recallB.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(recallB, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/precisionB.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(precisionB, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/f1B.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(f1B, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/ndcgB.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(ndcgB, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/recallValid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(recallValid, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/precisionValid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(precisionValid, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/f1Valid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(f1Valid, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert/ndcgValid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(ndcgValid, f)\n    # 关闭文件\n    f.close()\n\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/recallA.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LrecallA, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/precisionA.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LprecisionA, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/f1A.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(Lf1A, f)\n    # 关闭文件\n    f.close()\n\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/ndcgA.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LndcgA, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/recallB.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LrecallB, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/precisionB.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LprecisionB, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/f1B.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(Lf1B, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/ndcgB.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LndcgB, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/recallValid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LrecallValid, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/precisionValid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LprecisionValid, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/f1Valid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(Lf1Valid, f)\n    # 关闭文件\n    f.close()\n\n    # 存储变量的文件的名字\n    filename = './content/drive/MyDrive/baseline_bertBert_lightGBM/ndcgValid.pkl'\n    # 以二进制写模式打开目标文件\n    f = open(filename, 'wb')\n    # 将变量存储到目标文件中区\n    pickle.dump(LndcgValid, f)\n    # 关闭文件\n    f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nepochs = 1\nfor t in range(epochs):\n    #train_empohs()\n    global model\n    testAPreds=predict(model,testADf)\n    testBPreds=predict(model,testBDf)\n    validPreds=predict(model,validDf)\n    #recall\n    rA=[]\n    rB=[]\n    rValid=[]\n    for i in range(1,35,5):\n        rA.append(getRecall(testAPreds,testAAnswer,i))\n        rB.append(getRecall(testBPreds,testBAnswer,i))\n        rValid.append(getRecall(validPreds,validAnswer,i))\n    recallA.append(rA)\n    recallB.append(rB)\n    recallValid.append(rValid)\n\n    #precision\n    pA=[]\n    pB=[]\n    pValid=[]\n    for i in range(1,35,5):\n        pA.append(getPrecision(testAPreds,testAAnswer,i))\n        pB.append(getPrecision(testBPreds,testBAnswer,i))\n        pValid.append(getPrecision(validPreds,validAnswer,i))\n    precisionA.append(pA)\n    precisionB.append(pB)\n    precisionValid.append(pValid)\n\n    #f1\n    f1a=[]\n    f1b=[]\n    f1valid=[]\n    for i in range(1,35,5):\n        f1a.append(getF1(testAPreds,testAAnswer,i))\n        f1b.append(getF1(testBPreds,testBAnswer,i))\n        f1valid.append(getF1(validPreds,validAnswer,i))\n    f1A.append(f1a)\n    f1B.append(f1b)\n    f1Valid.append(f1valid)\n\n    #ndcg\n    ndcga=[]\n    ndcgb=[]\n    ndcgvalid=[]\n    for i in range(1,8):\n        ndcga.append(getNDCG(testAPreds,testAAnswer,i))\n        ndcgb.append(getNDCG(testBPreds,testBAnswer,i))\n        ndcgvalid.append(getNDCG(validPreds,validAnswer,i))\n    ndcgA.append(ndcga)\n    ndcgB.append(ndcgb)\n    ndcgValid.append(ndcgvalid)\n    y=[]\n    for i in range(len(ndcgB)):\n        y.append(ndcgB[i][-3])\n    plotCurve(range(1,len(ndcgB)+1),y)\n    #------------------------------------------------------\n    cls = get_cls(model, n_splits,validDf)\n        # train\n    testAPreds=predictCLS(model, testADf, thd, cls)\n    testBPreds=predictCLS(model, testBDf, thd, cls)\n    validPreds=predictCLS(model, validDf, thd, cls)\n    #recall\n    rA=[]\n    rB=[]\n    rValid=[]\n    for i in range(1,35,5):\n        rA.append(getRecall(testAPreds,testAAnswer,i))\n        rB.append(getRecall(testBPreds,testBAnswer,i))\n        rValid.append(getRecall(validPreds,validAnswer,i))\n    LrecallA.append(rA)\n    LrecallB.append(rB)\n    LrecallValid.append(rValid)\n\n    #precision\n    pA=[]\n    pB=[]\n    pValid=[]\n    for i in range(1,35,5):\n        pA.append(getPrecision(testAPreds,testAAnswer,i))\n        pB.append(getPrecision(testBPreds,testBAnswer,i))\n        pValid.append(getPrecision(validPreds,validAnswer,i))\n    LprecisionA.append(pA)\n    LprecisionB.append(pB)\n    LprecisionValid.append(pValid)\n\n    #f1\n    f1a=[]\n    f1b=[]\n    f1valid=[]\n    for i in range(1,35,5):\n        f1a.append(getF1(testAPreds,testAAnswer,i))\n        f1b.append(getF1(testBPreds,testBAnswer,i))\n        f1valid.append(getF1(validPreds,validAnswer,i))\n    Lf1A.append(f1a)\n    Lf1B.append(f1b)\n    Lf1Valid.append(f1valid)\n\n    #ndcg\n    ndcga=[]\n    ndcgb=[]\n    ndcgvalid=[]\n    for i in range(1,8):\n        ndcga.append(getNDCG(testAPreds,testAAnswer,i))\n        ndcgb.append(getNDCG(testBPreds,testBAnswer,i))\n        ndcgvalid.append(getNDCG(validPreds,validAnswer,i))\n    LndcgA.append(ndcga)\n    LndcgB.append(ndcgb)\n    LndcgValid.append(ndcgvalid)\n    y=[]\n    for i in range(len(LndcgB)):\n        y.append(LndcgB[i][-3])\n    plotCurve(range(1,len(LndcgB)+1),y)\n    \n    saveFile()\nprint(\"Done!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"epochs = 1\n\nrecallA=[]\nprecisionA=[]\nf1A=[]\nndcgA=[]\n\nrecallB=[]\nprecisionB=[]\nf1B=[]\nndcgB=[]\n\nrecallValid=[]\nprecisionValid=[]\nf1Valid=[]\nndcgValid=[]\n\nfor t in range(epochs):\n    train_empohs()\n    global model\n    testAPreds=predict(model,testADf)\n    testBPreds=predict(model,testBDf)\n    validPreds=predict(model,validDf)\n    #recall\n    rA=[]\n    rB=[]\n    rValid=[]\n    for i in range(1,35,5):\n        rA.append(getRecall(testAPreds,testAAnswer,i))\n        rB.append(getRecall(testBPreds,testBAnswer,i))\n        rValid.append(getRecall(validPreds,validAnswer,i))\n    recallA.append(rA)\n    recallB.append(rB)\n    recallValid.append(rValid)\n\n    #precision\n    pA=[]\n    pB=[]\n    pValid=[]\n    for i in range(1,35,5):\n        pA.append(getPrecision(testAPreds,testAAnswer,i))\n        pB.append(getPrecision(testBPreds,testBAnswer,i))\n        pValid.append(getPrecision(validPreds,validAnswer,i))\n    precisionA.append(pA)\n    precisionB.append(pB)\n    precisionValid.append(pValid)\n\n    #f1\n    f1a=[]\n    f1b=[]\n    f1valid=[]\n    for i in range(1,35,5):\n        f1a.append(getF1(testAPreds,testAAnswer,i))\n        f1b.append(getF1(testBPreds,testBAnswer,i))\n        f1valid.append(getF1(validPreds,validAnswer,i))\n    f1A.append(f1a)\n    f1B.append(f1b)\n    f1Valid.append(f1valid)\n\n    #ndcg\n    ndcga=[]\n    ndcgb=[]\n    ndcgvalid=[]\n    for i in range(1,8):\n        ndcga.append(getNDCG(testAPreds,testAAnswer,i))\n        ndcgb.append(getNDCG(testBPreds,testBAnswer,i))\n        ndcgvalid.append(getNDCG(validPreds,validAnswer,i))\n    ndcgA.append(ndcga)\n    ndcgB.append(ndcgb)\n    ndcgValid.append(ndcgvalid)\n    y=[]\n    for i in range(len(ndcgB)):\n        y.append(ndcgB[i][-3])\n    plotCurve(range(1,len(ndcgB)+1),y)\nprint(\"Done!\")","metadata":{}},{"cell_type":"code","source":"import pickle\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/recallA.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(recallA, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/precisionA.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(precisionA, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/f1A.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(f1A, f)\n# 关闭文件\nf.close()\n\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/ndcgA.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(ndcgA, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/recallB.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(recallB, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/precisionB.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(precisionB, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/f1B.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(f1B, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/ndcgB.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(ndcgB, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/recallValid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(recallValid, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/precisionValid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(precisionValid, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/f1Valid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(f1Valid, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert/ndcgValid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(ndcgValid, f)\n# 关闭文件\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"print('initializing model...')\nnDCGs=[]\nbest_nDCG = 0.0\nmodel = VisualBERT(__C, pretrained_emb).to(device)\nnum_training_steps = np.ceil(__C.TRAIN_SIZE*k / (batch_size*grad_step)) * epochs\nnum_warmup_steps = int(num_training_steps*0.1)\neval_steps = num_training_steps//40*grad_step + 100\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\nscheduler = custom_schedule(optimizer,\n                            num_warmup_steps=num_warmup_steps,\n                            num_training_steps=num_training_steps,\n                            num_cycles=6,\n                            amplitude=0.3)","metadata":{}},{"cell_type":"markdown","source":"epochs = 5\n\nrecallA=[]\nprecisionA=[]\nf1A=[]\nndcgA=[]\n\nrecallB=[]\nprecisionB=[]\nf1B=[]\nndcgB=[]\n\nrecallValid=[]\nprecisionValid=[]\nf1Valid=[]\nndcgValid=[]\n\npad_len = 30\nthd = 9999\nn_splits = 10\nfor t in range(epochs):\n    train_empohs()\n    global model\n    cls = get_cls(model, n_splits,validDf)\n        # train\n    testAPreds=predictCLS(model, testADf, thd, cls)\n    testBPreds=predictCLS(model, testBDf, thd, cls)\n    validPreds=predictCLS(model, validDf, thd, cls)\n    #recall\n    rA=[]\n    rB=[]\n    rValid=[]\n    for i in range(1,35,5):\n        rA.append(getRecall(testAPreds,testAAnswer,i))\n        rB.append(getRecall(testBPreds,testBAnswer,i))\n        rValid.append(getRecall(validPreds,validAnswer,i))\n    recallA.append(rA)\n    recallB.append(rB)\n    recallValid.append(rValid)\n\n    #precision\n    pA=[]\n    pB=[]\n    pValid=[]\n    for i in range(1,35,5):\n        pA.append(getPrecision(testAPreds,testAAnswer,i))\n        pB.append(getPrecision(testBPreds,testBAnswer,i))\n        pValid.append(getPrecision(validPreds,validAnswer,i))\n    precisionA.append(pA)\n    precisionB.append(pB)\n    precisionValid.append(pValid)\n\n    #f1\n    f1a=[]\n    f1b=[]\n    f1valid=[]\n    for i in range(1,35,5):\n        f1a.append(getF1(testAPreds,testAAnswer,i))\n        f1b.append(getF1(testBPreds,testBAnswer,i))\n        f1valid.append(getF1(validPreds,validAnswer,i))\n    f1A.append(f1a)\n    f1B.append(f1b)\n    f1Valid.append(f1valid)\n\n    #ndcg\n    ndcga=[]\n    ndcgb=[]\n    ndcgvalid=[]\n    for i in range(1,8):\n        ndcga.append(getNDCG(testAPreds,testAAnswer,i))\n        ndcgb.append(getNDCG(testBPreds,testBAnswer,i))\n        ndcgvalid.append(getNDCG(validPreds,validAnswer,i))\n    ndcgA.append(ndcga)\n    ndcgB.append(ndcgb)\n    ndcgValid.append(ndcgvalid)\n    y=[]\n    for i in range(len(ndcgB)):\n        y.append(ndcgB[i][-3])\n    plotCurve(range(1,len(ndcgB)+1),y)\nprint(\"Done!\")","metadata":{}},{"cell_type":"code","source":"import pickle\n \n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/recallA.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LrecallA, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/precisionA.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LprecisionA, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/f1A.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(Lf1A, f)\n# 关闭文件\nf.close()\n\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/ndcgA.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LndcgA, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/recallB.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LrecallB, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/precisionB.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LprecisionB, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/f1B.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(Lf1B, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/ndcgB.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LndcgB, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/recallValid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LrecallValid, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/precisionValid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LprecisionValid, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/f1Valid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(Lf1Valid, f)\n# 关闭文件\nf.close()\n\n# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/ndcgValid.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(LndcgValid, f)\n# 关闭文件\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 存储变量的文件的名字\nfilename = './content/drive/MyDrive/baseline_bertBert_lightGBM/validPreds.pkl'\n# 以二进制写模式打开目标文件\nf = open(filename, 'wb')\n# 将变量存储到目标文件中区\npickle.dump(validPreds, f)\n# 关闭文件\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./content/drive/MyDrive/baseline_bertBert/","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}