{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.metrics import fbeta_score\nfrom tqdm import tqdm\nimport cv2\nfrom PIL import Image\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import fbeta_score\nimport time\n%matplotlib inline\n\npal = sns.color_palette()\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load train and test CSVs\ndf_train = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\ndf_test = pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore train labels distribution\nlabels = df_train['tags'].apply(lambda x: x.split(' '))\nfrom collections import Counter, defaultdict\ncounts = defaultdict(int) #dictionary containing each individual label\nfor l in labels:\n    for l2 in l:\n        counts[l2] += 1\n\n# data=[go.Bar(x=list(counts.keys()), y=list(counts.values()))]\n# layout=dict(height=800, width=800, title='Distribution of training labels')\n# fig=dict(data=data, layout=layout)\n# py.iplot(data, filename='train-label-dist')\n# plt.show()\ntag_list=list(counts.keys()) \ny=list(counts.values())\nsns.barplot(x=tag_list, y=y);\nplt.xlabel('labels');\nplt.xticks(rotation = 90);\nplt.title('Tag count for train set');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Explore test labels distribution\nlabels_test = df_test['tags'].apply(lambda x: x.split(' '))\nfrom collections import Counter, defaultdict\ncounts_test = defaultdict(int)\nfor l in labels_test:\n    for l2 in l:\n        counts_test[l2] += 1\n\ntag_list_test=list(counts_test.keys()) \ntest_count=list(counts_test.values())\nsns.barplot(x=tag_list_test, y=test_count);\nplt.xlabel('labels');\nplt.xticks(rotation = 90);\nplt.title('Tag counts for test set');\n\n#These are not actual labels, just placeholders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#View some of the train images\n\nnew_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(20, 20))\ni = 0\nfor f, l in df_train[:9].values:\n    img = cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(f))\n    ax[i // 3, i % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i // 3, i % 3].set_title('{} - {}'.format(f, l))\n    #ax[i // 4, i % 4].show()\n    i += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a dictionary assigning a numerical value to each label\nlabel_map = {i:j for j, i in enumerate(tag_list)}\nlabel_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encode the training labels. Convert the images into pixels and resize them\nX_train, Y_train = [], []\nfor img, label in tqdm(df_train.values, miniters = 1000):\n  target = np.zeros(17)\n  for tag in label.split(' '):\n    target[label_map[tag]]=1\n  X_train.append(cv2.resize(cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(img)), (64,64)))\n  Y_train.append(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the test images to pixels and resize them as well\nX_test=[]\nfor img, label in tqdm(df_test[:40669].values, miniters = 1000):\n  X_test.append(cv2.resize(cv2.imread('../input/planets-dataset/planet/planet/test-jpg/{}.jpg'.format(img)), (64,64)))\nfor img, label in tqdm(df_test[40669:].values, miniters = 1000):\n  X_test.append(cv2.resize(cv2.imread('../input/planets-dataset/test-jpg-additional/test-jpg-additional/{}.jpg'.format(img)), (64,64)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confirm the dimensions\nlen(X_test), len(X_train), len(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.getsizeof(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change lists to numpy arrays and normalize\nx_train = np.array(X_train, np.float16)/255\ny_train = np.array(Y_train, np.uint8)\n#x_test = np.array(X_test, np.float16)/255\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, shuffle = True, random_state = 1)\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(X_test, X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into 5 folds and train on 4 folds while validating on 1 fold\n# nfolds = 5\n# num_fold = 0\n# sum_score = 0\nyfull_test = []\nyfull_train = []\nX_train_, X_val_, Y_train_, Y_val_ = train_test_split(x_train, y_train, test_size = 0.2, random_state = 1)\n# kf = KFold(n_splits = nfolds, shuffle = True, random_state = 1)\n# print(kf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train a convolutional neural network using KFold cross validation to prevent overfitting\n# for train_index, test_index in kf.split(x_train):\n#     start_time_model_fitting = time.time()\n        \n# X_train_ = x_train[train_index]\n# Y_train_ = y_train[train_index]\n# X_val_ = x_train[test_index]\n# Y_val_ = y_train[test_index]\n\n#     #Track the progress through the K folds\n#     num_fold += 1\n#     print('Start KFold number {} from {}'.format(num_fold, nfolds))\n#     print('Split train: ', len(X_train_), len(Y_train_))\n#     print('Split valid: ', len(X_val_), len(Y_val_))\n    \n\n        \n#Build a five layer CNN model\n\n#Create a path to save the weights\nkfold_weights_path = os.path.join('', 'weights_kfold_' + '.h5')\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(64, 64,3)))\nmodel.add(Conv2D(32, kernel_size=(3, 3),padding='same', activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3),padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n        \nmodel.add(Conv2D(128, kernel_size=(3, 3),padding='same', activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n        \nmodel.add(Conv2D(256, kernel_size=(3, 3),padding='same', activation='relu'))\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n        \nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(17, activation='sigmoid'))\n\n    #Try a combination of epoch lengths and learning rates\nepochs = 20\nlearn_rate = 0.0001\nopt  = optimizers.Adam(lr=learn_rate)\nmodel.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0)]\n#    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0)] #save the weights of the best performing model\n\nmodel.fit(x = X_train_, y= Y_train_, validation_data=(X_val_, Y_val_),batch_size=128,verbose=2, epochs=epochs,callbacks=callbacks,shuffle=True)\n        \np_val = model.predict(X_val_, batch_size = 32, verbose=2)\nprint(fbeta_score(Y_val_, np.array(p_val) > 0.2, beta=2, average='samples')) #Check the model performance on the validation set\n\np_train = model.predict(x_train, batch_size =128, verbose=2) #save the training predictions\nyfull_train.append(p_train)\n        \np_test = model.predict(x_test, batch_size = 128, verbose=2) #save the test predictions\nyfull_test.append(p_test)\n\nresult = np.array(yfull_test[0])\n# for i in range(1, nfolds):\n#     result += np.array(yfull_test[i])\n# result /= nfolds\nresult = pd.DataFrame(result, columns = labels)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base model. Feel free to try out other architectures and ideas to improve fbeta score\n\nimport numpy as np\nfrom keras import backend as K\n\n\ndef fbeta(y_true, y_pred, threshold_shift=0):\n    beta = 2\n\n    # just in case of hipster activation at the final layer\n    y_pred = K.clip(y_pred, 0, 1)\n\n    # shifting the prediction threshold from .5 if needed\n    y_pred_bin = K.round(y_pred + threshold_shift)\n\n    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n\n    beta_squared = beta ** 2\n    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n\n\n\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, RMSprop\n\n\nmodel = keras.Sequential()\nmodel.add(Conv2D(64, 5, 2, activation = \"relu\", input_shape = (64, 64, 3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(128, 3, 2, activation = \"relu\"))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dense(17, activation = \"sigmoid\"))\n\nmodel.compile(loss = \"binary_crossentropy\", optimizer = Adam(), metrics = [fbeta])\nmodel.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 20, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the test images to pixels and resize them as well\nX_test=[]\nfor img, label in tqdm(df_test[:40669].values, miniters = 1000):\n  X_test.append(cv2.resize(cv2.imread('../input/planets-dataset/planet/planet/test-jpg/{}.jpg'.format(img)), (64,64)))\nfor img, label in tqdm(df_test[40669:].values, miniters = 1000):\n  X_test.append(cv2.resize(cv2.imread('../input/planets-dataset/test-jpg-additional/test-jpg-additional/{}.jpg'.format(img)), (64,64)))\n\nx_test = np.array(X_test, np.float16)/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(x_test, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.DataFrame(predictions, columns =  tag_list)\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}