{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 安装aix360\n!pip install git+https://github.com/Trusted-AI/AIX360.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入包\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport xgboost\nimport os\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint('x' in np.arange(5))   #returns False, without Warning\nfrom sklearn.model_selection import train_test_split\nfrom aix360.algorithms.protodash import ProtodashExplainer\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入数据并区分X和y\nheloc = pd.read_csv('../input/home-equity-line-of-creditheloc/heloc_dataset_v1 (1).csv')\nX = heloc.drop(columns = 'RiskPerformance')\ny = heloc.RiskPerformance.replace(to_replace=['Bad', 'Good'], value=[1, 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 切分训练集及测试集并转换成array\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=0)\nX_train_array = np.array(X_train)\nX_test_array = np.array(X_test)\ny_train_array = np.array(y_train)\ny_test_array = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 基于XGBOOST模型构建解释器\nmodel = xgboost.XGBClassifier().fit(X_train_array, y_train_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 计算模型预测准确率\npred_train = model.predict(X_train_array)\npred_test = model.predict(X_test_array)\nscore_train = sklearn.metrics.accuracy_score(y_train, pred_train)\nscore_test = sklearn.metrics.accuracy_score(y_test,pred_test)\nprint(\"train accuracy:\",score_train)\nprint(\"test accuracy:\",score_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def protodash_explain(X_train,X_test,model,idx,m):\n    # 将训练数据集与模型的预测值合并成同一个array，并区分预测结果分别为好/坏的样本\n    X_train_array = np.array(X_train)\n    X_test_array = np.array(X_test)\n    pred_train = model.predict(X_train_array)\n    pred_train = pred_train.reshape((pred_train.shape[0],1)) \n    z_train = np.hstack((X_train_array, pred_train)) \n    z_train_good = z_train[z_train[:,-1]==0, :]\n    z_train_bad = z_train[z_train[:,-1]==1, :]\n    # 从测试数据集中随机选取待解释样本\n    print(\"Chosen Sample:\", idx)\n    print(\"Prediction made by the model:\", model.predict(X_test_array)[idx])\n    print(\"Prediction probabilities:\", model.predict_proba(X_test_array[idx].reshape(1,X_test_array.shape[1])))\n    print(\"\")\n    Target = np.hstack((X_test_array[idx].reshape((1,23)), model.predict(X_test_array)[idx].reshape((1,1))))\n    print(\"待解释样本特征取值如下：\")\n    print(X_test.iloc[idx])\n    # 创建Protodash解释器\n    explainer = ProtodashExplainer()\n    (W, S, setValues) = explainer.explain(Target, z_train_good, m) \n    # 展示各Prototype特征取值以及相应的Weight\n    col = X_train.columns.tolist()\n    col.append('RiskPerformance')\n    if model.predict(X_test_array)[idx]==0:\n        prototypes = pd.DataFrame(z_train_good[S, :])\n    else:\n        prototypes = pd.DataFrame(z_train_bad[S, :])\n    prototypes.columns = col\n    prototypes[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5))\n    prototypes = prototypes.transpose()\n    # 展示各Prototype的各个特征的Weight\n    if model.predict(X_test_array)[idx]==0:\n        z = z_train_good[S, 0:-1] # Store chosen prototypes\n    else:\n        z = z_train_bad[S, 0:-1] # Store chosen prototypes\n    eps = 1e-10 # Small constant defined to eliminate divide-by-zero errors\n    feature_weight = np.zeros(z.shape)\n    for i in range (z.shape[0]):\n        for j in range(z.shape[1]):\n            feature_weight[i, j] = np.exp(-1 * abs(Target[0, j] - z[i,j])/(np.std(z[:, j])+eps)) # Compute feature similarity in [0,1]\n    # move wts to a dataframe to display\n    feature_weight_table = pd.DataFrame.from_records(np.around(feature_weight.astype('double'), 2))\n    feature_weight_table.columns = X_train.columns.tolist()\n    feature_weight_table = feature_weight_table.transpose() \n    return prototypes,feature_weight_table\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"protodash_explain(X_train=X_train,X_test=X_test,model=model,idx=8,m=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 将训练数据集与模型的预测值合并成同一个array，并区分预测结果分别为好/坏的样本\npred_train = pred_train.reshape((pred_train.shape[0],1)) \nz_train = np.hstack((X_train_array, pred_train)) \nz_train_good = z_train[z_train[:,-1]==0, :]\nz_train_bad = z_train[z_train[:,-1]==1, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 从测试数据集中随机选取待解释样本\nidx = 9\nprint(\"Chosen Sample:\", idx)\nprint(\"Prediction made by the model:\", model.predict(X_test_array)[idx])\nprint(\"Prediction probabilities:\", model.predict_proba(X_test_array[idx].reshape(1,X_test_array.shape[1])))\nprint(\"\")\nTarget = np.hstack((X_test_array[idx].reshape((1,23)), model.predict(X_test_array)[idx].reshape((1,1))))\nprint(\"待解释样本特征取值如下：\")\nX_test.iloc[idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建Protodash解释器\nexplainer = ProtodashExplainer()\n(W, S, setValues) = explainer.explain(Target, z_train_good, m=5) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = X_train.columns.tolist()\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col.append('RiskPerformance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 展示各Prototype特征取值以及相应的Weight\nprototypes = pd.DataFrame(z_train_good[S, :])\nprototypes.columns = col\nprototypes[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5))\nprototypes.transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 展示各Prototype的各个特征的Weight\nz = z_train_good[S, 0:-1] # Store chosen prototypes\neps = 1e-10 # Small constant defined to eliminate divide-by-zero errors\nfeature_weight = np.zeros(z.shape)\nfor i in range (z.shape[0]):\n    for j in range(z.shape[1]):\n        feature_weight[i, j] = np.exp(-1 * abs(Target[0, j] - z[i,j])/(np.std(z[:, j])+eps)) # Compute feature similarity in [0,1]\n                \n# move wts to a dataframe to display\nfeature_weight_table = pd.DataFrame.from_records(np.around(feature_weight.astype('double'), 2))\nfeature_weight_table.columns = X_train.columns.tolist()\nfeature_weight_table.transpose() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}