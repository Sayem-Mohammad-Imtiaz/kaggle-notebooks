{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## upgrading pip\n!pip install -q --upgrade pip\n\n## installing the latest transformers version from pip\n!pip install --use-feature=2020-resolver -q transformers==3.0.2\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## importing packages\nimport gc\nimport os\nimport random\nimport transformers\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import AutoTokenizer, TFAutoModel\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Transformers version: {transformers.__version__}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## defining configuration\nclass Configuration():\n    \"\"\"\n    All configuration for running an experiment\n    \"\"\"\n    def __init__(\n        self,\n        model_name,\n        max_length = 64,\n        padding = True,\n        batch_size = 128,\n        epochs = 5,\n        learning_rate = 1e-5,\n        metrics = [\"sparse_categorical_accuracy\"],\n        verbose = 1,\n        train_splits = 5,\n        accelerator = \"TPU\",\n        seed_number = 13\n    ):\n        # seed and accelerator\n        self.SEED = seed_number\n        self.ACCELERATOR = accelerator\n\n        # paths\n        self.PATH_TRAIN = Path(\"../input/text-entailment-snli/train.csv\")\n        self.PATH_TEST  = Path(\"../input/text-entailment-snli/test.csv\")\n\n        # splits\n        self.TRAIN_SPLITS = train_splits\n\n        # model configuration\n        self.MODEL_NAME = model_name\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n\n        # model hyperparameters\n        self.MAX_LENGTH = max_length\n        self.PAD_TO_MAX_LENGTH = padding\n        self.BATCH_SIZE = batch_size\n        self.EPOCHS = epochs\n        self.LEARNING_RATE = learning_rate\n        self.METRICS = metrics\n        self.VERBOSE = verbose\n        \n        # initializing accelerator\n        self.initialize_accelerator()\n\n    def initialize_accelerator(self):\n        \"\"\"\n        Initializing accelerator\n        \"\"\"\n        # checking TPU first\n        if self.ACCELERATOR == \"TPU\":\n            print(\"Connecting to TPU\")\n            try:\n                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n                print(f\"Running on TPU {tpu.master()}\")\n            except ValueError:\n                print(\"Could not connect to TPU\")\n                tpu = None\n\n            if tpu:\n                try:\n                    print(\"Initializing TPU\")\n                    tf.config.experimental_connect_to_cluster(tpu)\n                    tf.tpu.experimental.initialize_tpu_system(tpu)\n                    self.strategy = tf.distribute.experimental.TPUStrategy(tpu)\n                    self.tpu = tpu\n                    print(\"TPU initialized\")\n                except:\n                    print(\"Failed to initialize TPU\")\n            else:\n                print(\"Unable to initialize TPU\")\n                self.ACCELERATOR = \"GPU\"\n\n        # default for CPU and GPU\n        if self.ACCELERATOR != \"TPU\":\n            print(\"Using default strategy for CPU and single GPU\")\n            self.strategy = tf.distribute.get_strategy()\n\n        # checking GPUs\n        if self.ACCELERATOR == \"GPU\":\n            print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n\n        # defining replicas\n        self.AUTO = tf.data.experimental.AUTOTUNE\n        self.REPLICAS = self.strategy.num_replicas_in_sync\n        print(f\"REPLICAS: {self.REPLICAS}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## data preparation functions\ndef encode_text(df, tokenizer, max_len, padding):\n    \"\"\"\n    Preprocessing textual data into encoded tokens.\n    \"\"\"\n    text = df[[\"premise\", \"hypothesis\"]].values.tolist()\n\n    # encoding text using tokenizer of the model\n    text_encoded = tokenizer.batch_encode_plus(\n        text,\n        pad_to_max_length = padding,\n        max_length = max_len\n    )\n\n    return text_encoded\n\n\ndef get_tf_dataset(X, y, auto, labelled = True, repeat = False, shuffle = False, batch_size = 128):\n    \"\"\"\n    Creating tf.data.Dataset for TPU.\n    \"\"\"\n    if labelled:\n        ds = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\n    else:\n        ds = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(2048)\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(auto)\n\n    return ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building model\ndef build_model(model_name, max_len, learning_rate, metrics):\n    \"\"\"\n    Building the Deep Learning architecture\n    \"\"\"\n    # defining encoded inputs\n    input_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\n    \n    # defining transformer model embeddings\n    transformer_model = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings = transformer_model(input_ids)[0]\n    \n    # defining output layer\n    output_values = Dense(3, activation = \"softmax\")(transformer_embeddings[:, 0, :])\n\n    # defining model\n    model = Model(inputs = input_ids, outputs = output_values)\n    opt = Adam(learning_rate = learning_rate)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n    metrics = metrics\n\n    model.compile(optimizer = opt, loss = loss, metrics = metrics)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## stratified k-fold over language and label\ndef run_model(config):\n    \"\"\"\n    Running the model\n    \"\"\"\n    ## reading data\n    df_train = pd.read_csv(config.PATH_TRAIN)\n    df_test = pd.read_csv(config.PATH_TEST)\n    \n    # stratified K-fold on label\n    skf = StratifiedKFold(n_splits = config.TRAIN_SPLITS, shuffle = True, random_state = config.SEED)\n\n    # initializing predictions\n    preds_val = np.zeros((df_train.shape[0], 3))\n    preds_test = np.zeros((df_test.shape[0], 3))\n    acc_val = []\n\n    # iterating over folds\n    for (fold, (train_index, valid_index)) in enumerate(skf.split(df_train, df_train.label)):\n        # initializing TPU\n        if config.ACCELERATOR == \"TPU\":\n            if config.tpu:\n                config.initialize_accelerator()\n\n        # building model\n        K.clear_session()\n        with config.strategy.scope():\n            model = build_model(config.MODEL_NAME, config.MAX_LENGTH, config.LEARNING_RATE, config.METRICS)\n            if fold == 0:\n                print(model.summary())\n\n        print(\"\\n\")\n        print(\"#\" * 19)\n        print(f\"##### Fold: {fold + 1} #####\")\n        print(\"#\" * 19)\n\n        # splitting data into training and validation\n        X_train = df_train.iloc[train_index]\n        X_valid = df_train.iloc[valid_index]\n\n        y_train = X_train.label.values\n        y_valid = X_valid.label.values\n\n        print(\"\\nTokenizing\")\n\n        # encoding text data using tokenizer\n        X_train_encoded = encode_text(df = X_train, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n        X_valid_encoded = encode_text(df = X_valid, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # creating TF Dataset\n        ds_train = get_tf_dataset(X_train_encoded, y_train, config.AUTO, repeat = True, shuffle = True, batch_size = config.BATCH_SIZE * config.REPLICAS)\n        ds_valid = get_tf_dataset(X_valid_encoded, y_valid, config.AUTO, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        n_train = X_train.shape[0]\n\n        if fold == 0:\n            X_test_encoded = encode_text(df = df_test, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # saving model at best accuracy epoch\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            \"model.h5\",\n            monitor = \"val_sparse_categorical_accuracy\",\n            verbose = 0,\n            save_best_only = True,\n            save_weights_only = True,\n            mode = \"max\",\n            save_freq = \"epoch\"\n        )\n        \n        print(\"\\nTraining\")\n\n        # training model\n        model_history = model.fit(\n            ds_train,\n            epochs = config.EPOCHS,\n            callbacks = [sv],\n            steps_per_epoch = n_train / config.BATCH_SIZE // config.REPLICAS,\n            validation_data = ds_valid,\n            verbose = config.VERBOSE\n        )\n\n        print(\"\\nValidating\")\n\n        # scoring validation data\n        model.load_weights(\"model.h5\")\n        ds_valid = get_tf_dataset(X_valid_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        preds_valid = model.predict(ds_valid, verbose = config.VERBOSE)\n        acc = accuracy_score(y_valid, np.argmax(preds_valid, axis = 1))\n\n        preds_val[valid_index] = preds_valid\n        acc_val.append(acc)\n        \n        print(\"\\nWrong predictions: \")\n        preds_valid_arg = np.argmax(preds_valid, axis = 1)\n        indices = [i for i in range(len(y_valid)) if y_valid[i] != preds_valid_arg[i]]\n        wrong_predictions = X_valid.iloc[indices,:]\n        print(wrong_predictions)\n        \n        print(\"\\nInferencing\")\n        # scoring test data\n        ds_test = get_tf_dataset(X_test_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n        preds_test += model.predict(ds_test, verbose = config.VERBOSE) / config.TRAIN_SPLITS\n        print(f\"\\nFold {fold + 1} Accuracy: {round(acc, 4)}\\n\")\n            \n        g = gc.collect()\n\n    #model.save('trained_model.h5')\n    \n    # overall CV score and standard deviation\n    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_val), 4)}\")\n    print(f\"CV StdDev Accuracy: {round(np.std(acc_val), 4)}\\n\")\n\n    return preds_val, preds_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model: Bert Base Cased\n#config_1 = Configuration(\"bert-base-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n#preds_train_1, preds_test_1 = run_model(config_1)\n\nconfig_1 = Configuration(\"jplu/tf-xlm-roberta-large\", max_length = 84, batch_size = 64, epochs = 16, train_splits = 3)\npreds_train_1, preds_test_1 = run_model(config_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(config_1.PATH_TEST)\ndf_result = pd.DataFrame({\"premise\": df_test.premise.values, \"hypothesis\": df_test.hypothesis.values, \"prediction\": np.argmax(preds_test_1, axis = 1)})\nprint(df_result)\ndf_result.prediction.value_counts()\ndf_result.to_csv(\"result.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}