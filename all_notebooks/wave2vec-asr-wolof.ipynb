{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook we are going to see how to convert speech into text using Facebook Wav2Vec 2.0 model.Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded using Wav2Vec2Tokenizer.For learning more about it click on this [link](https://huggingface.co/transformers/model_doc/wav2vec2.html)","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import librosa\nimport torch\nimport IPython.display as display\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load pre-trained Wav2Vec model","metadata":{}},{"cell_type":"code","source":"#load pre-trained model and tokenizer\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Audio file","metadata":{}},{"cell_type":"code","source":"#load audio file \naudio, sampling_rate = librosa.load(\"../input/automatic-speech-recognition-in-wolof/clips/clips/0031672b4484f963c8a07babe6f713dd559539d44140e80ac19708db36d9712d81dd5b170c016f65bbd6763372c35bfc984a55448e356f3161dbf8d7c28aa047.mp3\",sr=16000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio,sampling_rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Play the Audio","metadata":{}},{"cell_type":"code","source":"# audio\ndisplay.Audio(\"../input/automatic-speech-recognition-in-wolof/clips/clips/0031672b4484f963c8a07babe6f713dd559539d44140e80ac19708db36d9712d81dd5b170c016f65bbd6763372c35bfc984a55448e356f3161dbf8d7c28aa047.mp3\", autoplay=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Speech to Text","metadata":{}},{"cell_type":"markdown","source":"First of all tokenize the input values,take the maximum prediction from the logit and then extraxt the text","metadata":{}},{"cell_type":"code","source":"input_values = tokenizer(audio, return_tensors = 'pt').input_values\ninput_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store logits (non-normalized predictions)\nlogits = model(input_values).logits\nlogits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store predicted id's\n# pass the logit values to softmax to get the predicted values\npredicted_ids = torch.argmax(logits, dim =-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass the prediction to the tokenzer decode to get the transcription\ntranscriptions = tokenizer.decode(predicted_ids[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transcriptions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntest = pd.read_csv(\"../input/automatic-speech-recognition-in-wolof/Test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_trans=[]\nfor x in test.ID:\n    audio, sampling_rate = librosa.load(\"../input/automatic-speech-recognition-in-wolof/audio_wav_16000/tmp/WOLOF_ASR_dataset/audio_wav_16000/\"+str(x)+\".wav\",sr=16000)\n    input_values = tokenizer(audio, return_tensors = 'pt').input_values\n    logits = model(input_values).logits\n    predicted_ids = torch.argmax(logits, dim =-1)\n    transcriptions = tokenizer.decode(predicted_ids[0])\n    test_trans.append(transcriptions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"transcription\"]=test_trans\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_trans=[]\nfor x in test[\"transcription\"]:\n    lower_trans.append(x.lower())\ntest[\"transcription\"] = lower_trans\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"ID\",\"transcription\"]].to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(test[\"ID\"]==\"e3a74a8998f03c320f5a4923272247\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/automatic-speech-recognition-in-wolof/SampleSubmission.csv\")\nsub[\"transcription\"]=test_trans\nlower_trans_1=[]\nfor x in sub[\"transcription\"]:\n    lower_trans_1.append(x.lower())\nsub[\"transcription\"] = lower_trans_1\n\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[[\"ID\",\"transcription\"]].to_csv(\"submission1.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}