{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nIn this notebook I am trying to build an Arabic based chatbot with a reddit flare \"Dardesh\", using Hugging face open GPT2. I'll be using a pre-trained arabic model and fine tune it. ","metadata":{"id":"Ki7fLEiE8LkV"}},{"cell_type":"markdown","source":"# Data Collection and Processing\n\n\nI've collected data from 2 subreddits, Egypt and Arabs and formatted them into chat bot structure and split them into testing and training. \n\nI used pushshift io to crawl all sumbmissions in a sub reddit as reddit api only returns 1000 submissions per subreddit as max.\n\nI followed these tutorials: \n\nhttps://www.storybench.org/how-to-scrape-reddit-with-python/ \n\nhttps://www.textjuicer.com/2019/07/crawling-all-submissions-from-a-subreddit/\n\n\nI have mapped each subreddit title to all the comments, mimkcing question answer pairs. I did this for the Arabic and Egypt subreddits. \n\nI have also removed subreddits or comments that were entirely wrriteen in english. I was left with csv files for each subreddit. Each csv file had a coloumn for the title, comment, date and link. ","metadata":{"id":"BoLVK9XW8LkY"}},{"cell_type":"code","source":"!pip install praw","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:32:46.532448Z","iopub.execute_input":"2021-06-02T12:32:46.532847Z","iopub.status.idle":"2021-06-02T12:32:56.294141Z","shell.execute_reply.started":"2021-06-02T12:32:46.532811Z","shell.execute_reply":"2021-06-02T12:32:56.293191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langdetect","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:32:56.295691Z","iopub.execute_input":"2021-06-02T12:32:56.296007Z","iopub.status.idle":"2021-06-02T12:33:06.35887Z","shell.execute_reply.started":"2021-06-02T12:32:56.29597Z","shell.execute_reply":"2021-06-02T12:33:06.357496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport praw\nimport pandas as pd\nimport datetime as dt\nfrom praw.models import MoreComments\nfrom langdetect import detect\nimport pickle","metadata":{"id":"ZAj9NKe88LkY","execution":{"iopub.status.busy":"2021-06-02T12:33:06.361076Z","iopub.execute_input":"2021-06-02T12:33:06.361395Z","iopub.status.idle":"2021-06-02T12:33:06.413739Z","shell.execute_reply.started":"2021-06-02T12:33:06.361339Z","shell.execute_reply":"2021-06-02T12:33:06.41287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\nurl = \"https://api.pushshift.io/reddit/search/submission\"\n\ndef crawl_page(subreddit: str, last_page = None):\n    \"\"\"\n    Crawl a page of results from a given subreddit.\n\n    :param subreddit: The subreddit to crawl.\n    :param last_page: The last downloaded page.\n\n    :return: A page or results.\n    \"\"\"\n    params = {\"subreddit\": subreddit, \"size\": 500, \"sort\": \"desc\", \"sort_type\": \"created_utc\"}\n    if last_page is not None:\n        if len(last_page) > 0:\n            # resume from where we left at the last page\n            params[\"before\"] = last_page[-1][\"created_utc\"]\n        else:\n            # the last page was empty, we are past the last page\n            return []\n    results = requests.get(url, params)\n    if not results.ok:\n        # something wrong happened\n        raise Exception(\"Server returned status code {}\".format(results.status_code))\n    return results.json()[\"data\"]","metadata":{"id":"KySebGkw8LkZ","execution":{"iopub.status.busy":"2021-06-02T12:33:06.415094Z","iopub.execute_input":"2021-06-02T12:33:06.41552Z","iopub.status.idle":"2021-06-02T12:33:06.422025Z","shell.execute_reply.started":"2021-06-02T12:33:06.41549Z","shell.execute_reply":"2021-06-02T12:33:06.42125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef crawl_subreddit(subreddit, max_submissions=2000):\n    \"\"\"\n    Crawl submissions from a subreddit.\n    :param subreddit: The subreddit to crawl.\n    :param max_submissions: The maximum number of submissions to download.\n    :return: A list of submissions.\n    \"\"\"\n    submissions = []\n    last_page = None\n    while last_page != [] and len(submissions) < max_submissions:\n        last_page = crawl_page(subreddit, last_page)\n        submissions += last_page\n        time.sleep(3)\n    return submissions[:max_submissions]","metadata":{"id":"PiVEWszj8LkZ","execution":{"iopub.status.busy":"2021-06-02T12:33:06.423418Z","iopub.execute_input":"2021-06-02T12:33:06.42381Z","iopub.status.idle":"2021-06-02T12:33:06.438061Z","shell.execute_reply.started":"2021-06-02T12:33:06.423781Z","shell.execute_reply":"2021-06-02T12:33:06.437008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def subredditArabic(subredditName,limit):\n    lastest_submissions = crawl_subreddit(subredditName,limit)\n    topics_data=praw_submissions_comments(lastest_submissions,subredditName,limit)\n    return topics_data, lastest_submissions","metadata":{"id":"Lk4C1Tb48Lka","execution":{"iopub.status.busy":"2021-06-02T12:33:06.439163Z","iopub.execute_input":"2021-06-02T12:33:06.439578Z","iopub.status.idle":"2021-06-02T12:33:06.453627Z","shell.execute_reply.started":"2021-06-02T12:33:06.439549Z","shell.execute_reply":"2021-06-02T12:33:06.452434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def praw_submissions_comments(lastest_submissions,subredditName,limit_sub):\n    reddit = praw.Reddit(client_id='client', \n                     client_secret='secret', \n                     user_agent='user', \n                     username='username', \n                     password='dardesh')\n    subreddit = reddit.subreddit(subredditName)\n    top_subreddit = subreddit.top(limit=limit_sub)\n    topics_dict = { \"title\":[], \n                    \"id\":[], \n                    \"url\":[],\n                    \"comment\":[]\n                }\n    for sub in lastest_submissions:\n        submission = reddit.submission(id=sub[\"id\"])\n        try:\n            if(detect(submission.title)=='ar'):\n                for top_level_comment in submission.comments:\n                    if isinstance(top_level_comment, MoreComments, ):\n                        continue\n                    try:\n                        if(detect(top_level_comment.body)=='ar'):\n                            topics_dict[\"title\"].append(submission.title)\n                            topics_dict[\"id\"].append(submission.id)\n                            topics_dict[\"url\"].append(submission.url)\n                            topics_dict[\"comment\"].append(top_level_comment.body)\n                    except:\n                        pass\n        except:\n            pass\n    topics_data = pd.DataFrame(topics_dict)\n    topics_data.to_pickle(subredditName) \n    topics_data.to_csv(r'Egypt_subreddit.csv', index = False)\n    return topics_data","metadata":{"id":"zulMP-5V8Lka","execution":{"iopub.status.busy":"2021-06-02T12:33:06.455316Z","iopub.execute_input":"2021-06-02T12:33:06.456011Z","iopub.status.idle":"2021-06-02T12:33:06.471921Z","shell.execute_reply.started":"2021-06-02T12:33:06.455963Z","shell.execute_reply":"2021-06-02T12:33:06.470925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Cleaning and Formatting\n\nFirst I shuffled the csv file. Then I did some cleaning and removed links and unknown characters.Then Formated the text to be in chatbot format while splitting the file into training and testing. \n","metadata":{"id":"UrTm-tQz8Lkb"}},{"cell_type":"code","source":"import csv\nimport pandas as pd \n\ndef process_reddits(file_train,file_test,subredditFile):\n    \n    df = pd.read_csv(subredditFile)\n    #shuffling the file\n    print(df.shape[0])\n    df = df.sample(frac=1)\n    print(df.shape[0])\n    train_row=round(df.shape[0]*0.8)\n    counter=0\n    for index, row in df.iterrows():\n        \n        q = '[انت] : ' + row['title']\n        a = '[دردش] : ' + clean_comment(row['comment'])\n        if(counter<train_row):\n            file_train.write(q)\n            file_train.write('\\n')\n            file_train.write(a)\n            file_train.write('\\n')\n        else:\n            file_test.write(q)\n            file_test.write('\\n')\n            file_test.write(a)\n            file_test.write('\\n')\n        counter+=1\n            ","metadata":{"id":"Pld8F_pc8Lkb","execution":{"iopub.status.busy":"2021-06-02T12:33:06.475113Z","iopub.execute_input":"2021-06-02T12:33:06.47558Z","iopub.status.idle":"2021-06-02T12:33:06.487504Z","shell.execute_reply.started":"2021-06-02T12:33:06.475529Z","shell.execute_reply":"2021-06-02T12:33:06.486438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n\ndef clean_comment(original_text):\n    original_text = re.sub(r'http\\S+','', original_text)\n    original_text = original_text.replace(\"[\", \"\") \n    original_text = original_text.replace(\"]\", \"\") \n    original_text = original_text.replace(\"{\", \"\") \n    original_text = original_text.replace(\"}\", \"\") \n    original_text = original_text.replace(\"(\", \"\") \n    original_text = original_text.replace(\")\", \"\") \n    original_text = original_text.replace(\"�\", \"\") \n    original_text = original_text.replace(\"\", \"\") \n    original_text = original_text.replace(\"**\", \"\")\n    original_text = original_text.replace(\"##\", \"\") \n    original_text = original_text.replace(\"&#x200B;\", \"\") \n    original_text = original_text.replace(\"\\u202c\", \"\") \n\n\n    return original_text","metadata":{"id":"CpYFPZEG8Lkc","execution":{"iopub.status.busy":"2021-06-02T12:33:06.489733Z","iopub.execute_input":"2021-06-02T12:33:06.490185Z","iopub.status.idle":"2021-06-02T12:33:06.507624Z","shell.execute_reply.started":"2021-06-02T12:33:06.490148Z","shell.execute_reply":"2021-06-02T12:33:06.506495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used the above method to create dardesh_train_ar files and dardesh_train_ar_eg files. Where the ar files consits of both subreddits and the eg consits of only Egypt subreddit. \n","metadata":{"id":"j70BQTvQ8Lkc"}},{"cell_type":"markdown","source":"# Model Training\n\n\nI have used these tutorials as a refrence and starting point for training my model.\n\nhttps://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface\n\nhttps://colab.research.google.com/drive/1Bz-P-ucyLMaCBmgTjS_QR8RoGsZ5WHwo?usp=sharing\n\n\nI have used a pre-trained arabic gpt2 model, developed by Wessam Antoun and AUC Brain Lab.\n\nhttps://github.com/aub-mind/arabert/tree/master/examples","metadata":{"id":"Trx1xVwQ8Lkd"}},{"cell_type":"code","source":"!pip install transformers==4.2.1\n!pip install pyarabic\n!git clone https://github.com/aub-mind/arabert","metadata":{"id":"9XGqOnyI8Lkd","execution":{"iopub.status.busy":"2021-06-02T12:33:06.50917Z","iopub.execute_input":"2021-06-02T12:33:06.509508Z","iopub.status.idle":"2021-06-02T12:33:27.252322Z","shell.execute_reply.started":"2021-06-02T12:33:06.509474Z","shell.execute_reply":"2021-06-02T12:33:27.251475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import importlib, pkg_resources, tokenizers\nimportlib.reload(pkg_resources)\nimportlib.reload(tokenizers)","metadata":{"id":"NQ67WAdN8Lkd","execution":{"iopub.status.busy":"2021-06-02T12:33:27.253608Z","iopub.execute_input":"2021-06-02T12:33:27.253921Z","iopub.status.idle":"2021-06-02T12:33:27.603518Z","shell.execute_reply.started":"2021-06-02T12:33:27.253886Z","shell.execute_reply":"2021-06-02T12:33:27.60241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#textwrap enables formating of long text\nimport textwrap\n\nfrom transformers import pipeline, GPT2TokenizerFast\nfrom arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\nfrom arabert.preprocess import ArabertPreprocessor\n\n#you can choose any aragpt2 model since they all have the same preprocessing\n\narabert_processor = ArabertPreprocessor(model_name=\"aragpt2-base\")","metadata":{"id":"1xqw2QCf8Lke","execution":{"iopub.status.busy":"2021-06-02T12:33:27.604801Z","iopub.execute_input":"2021-06-02T12:33:27.605086Z","iopub.status.idle":"2021-06-02T12:33:27.63904Z","shell.execute_reply.started":"2021-06-02T12:33:27.605059Z","shell.execute_reply":"2021-06-02T12:33:27.6381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"YBx4RKyl8Lke","execution":{"iopub.status.busy":"2021-06-02T12:33:27.640312Z","iopub.execute_input":"2021-06-02T12:33:27.640635Z","iopub.status.idle":"2021-06-02T12:33:28.386455Z","shell.execute_reply.started":"2021-06-02T12:33:27.640604Z","shell.execute_reply":"2021-06-02T12:33:28.385361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = 0 if torch.cuda.is_available() else -1\nprint(device)","metadata":{"id":"q-EI48gM8Lke","execution":{"iopub.status.busy":"2021-06-02T12:33:28.389163Z","iopub.execute_input":"2021-06-02T12:33:28.38961Z","iopub.status.idle":"2021-06-02T12:33:28.394803Z","shell.execute_reply.started":"2021-06-02T12:33:28.389558Z","shell.execute_reply":"2021-06-02T12:33:28.394035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"aubmindlab/aragpt2-base\"\n\naragpt2_pipeline = pipeline(\"text-generation\",model=model_name,device=device)","metadata":{"id":"Pk_AzUUF8Lke","execution":{"iopub.status.busy":"2021-06-02T12:33:28.396068Z","iopub.execute_input":"2021-06-02T12:33:28.396911Z","iopub.status.idle":"2021-06-02T12:34:07.304743Z","shell.execute_reply.started":"2021-06-02T12:33:28.396871Z","shell.execute_reply":"2021-06-02T12:34:07.303505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r '../input/arabicreddit/dardesh_train_ar_eg.txt' ./\n!cp -r '../input/arabicreddit/dardesh_test_ar_eg.txt' ./","metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:34:07.307253Z","iopub.execute_input":"2021-06-02T12:34:07.307584Z","iopub.status.idle":"2021-06-02T12:34:08.853781Z","shell.execute_reply.started":"2021-06-02T12:34:07.307552Z","shell.execute_reply":"2021-06-02T12:34:08.852567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\n\ntrain_path = './dardesh_train_ar_eg.txt'\ntest_path = './dardesh_test_ar_eg.txt'","metadata":{"id":"QeRwT_1m8Lke","execution":{"iopub.status.busy":"2021-06-02T12:34:08.855199Z","iopub.execute_input":"2021-06-02T12:34:08.855564Z","iopub.status.idle":"2021-06-02T12:34:11.554661Z","shell.execute_reply.started":"2021-06-02T12:34:08.855524Z","shell.execute_reply":"2021-06-02T12:34:11.553542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset,DataCollatorForLanguageModeling\n\ndef load_dataset(train_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=128)\n\n    test_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=test_path,\n          block_size=128)\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,test_dataset,data_collator\n\ntrain_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)","metadata":{"id":"xZG6Fpk08Lkf","execution":{"iopub.status.busy":"2021-06-02T12:34:11.556273Z","iopub.execute_input":"2021-06-02T12:34:11.556616Z","iopub.status.idle":"2021-06-02T12:34:13.481217Z","shell.execute_reply.started":"2021-06-02T12:34:11.556584Z","shell.execute_reply":"2021-06-02T12:34:13.479965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n\nmodel = AutoModelWithLMHead.from_pretrained(\"aubmindlab/aragpt2-base\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./trained_model\", #The output directory\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs=5, # number of training epochs\n    per_device_train_batch_size=32, # batch size for training\n    per_device_eval_batch_size=32,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps between two evaluations.\n    save_steps=800, # after # steps model is saved\n    warmup_steps=500,# number of warmup steps for learning rate scheduler\n    )\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset)","metadata":{"id":"1SDSBPp88Lkf","execution":{"iopub.status.busy":"2021-06-02T12:34:13.482688Z","iopub.execute_input":"2021-06-02T12:34:13.483041Z","iopub.status.idle":"2021-06-02T12:34:21.529777Z","shell.execute_reply.started":"2021-06-02T12:34:13.483012Z","shell.execute_reply":"2021-06-02T12:34:21.528388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"arBedRu88Lkf","execution":{"iopub.status.busy":"2021-06-02T12:34:21.532516Z","iopub.execute_input":"2021-06-02T12:34:21.533083Z","iopub.status.idle":"2021-06-02T15:39:10.680542Z","shell.execute_reply.started":"2021-06-02T12:34:21.533025Z","shell.execute_reply":"2021-06-02T15:39:10.679691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('./trained_model')","metadata":{"id":"sHVbdUaX8Lkf","execution":{"iopub.status.busy":"2021-06-02T15:39:10.682274Z","iopub.execute_input":"2021-06-02T15:39:10.682855Z","iopub.status.idle":"2021-06-02T15:39:11.547053Z","shell.execute_reply.started":"2021-06-02T15:39:10.682807Z","shell.execute_reply":"2021-06-02T15:39:11.545849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip ./trained_model","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:53:56.588127Z","iopub.execute_input":"2021-06-02T15:53:56.588639Z","iopub.status.idle":"2021-06-02T15:54:30.715421Z","shell.execute_reply.started":"2021-06-02T15:53:56.588541Z","shell.execute_reply":"2021-06-02T15:54:30.71393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./file.zip\"> Download File </a>\n","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nbot = pipeline('text-generation',model='./trained_model', tokenizer='aubmindlab/aragpt2-base',config={'max_length':35})\n","metadata":{"id":"zXgtUfhx8Lkg","execution":{"iopub.status.busy":"2021-06-02T16:21:53.95179Z","iopub.execute_input":"2021-06-02T16:21:53.952531Z","iopub.status.idle":"2021-06-02T16:22:03.691576Z","shell.execute_reply.started":"2021-06-02T16:21:53.952362Z","shell.execute_reply":"2021-06-02T16:22:03.69014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interacting with the model","metadata":{"id":"s2i2llqm8Lkg"}},{"cell_type":"code","source":"\nwhile True:\n    ques = input(\"Question : \")\n\n    inp = '[انت] : '+ques+'\\n'+'[دردش] : '\n\n    result = bot(inp)[0]['generated_text']\n\n    print(result)\n\n","metadata":{"id":"zskmGCBE8Lkg","execution":{"iopub.status.busy":"2021-06-02T16:22:14.35735Z","iopub.execute_input":"2021-06-02T16:22:14.357926Z"},"trusted":true},"execution_count":null,"outputs":[]}]}