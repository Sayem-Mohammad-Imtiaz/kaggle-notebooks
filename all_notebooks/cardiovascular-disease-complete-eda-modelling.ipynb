{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing important libraries.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nplt.style.use('seaborn')\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/cardiovascular-disease-dataset/cardio_train.csv',sep=';')\nprint(f'Dataset Shape: {data.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Below are some of the key assumptions that we can make about the data and will look to validate them \nwith the data in hand.\n1. With the increase in age chances of heart disease increases.\n2. Effect of height and weight. We assume that with more BMI chances of heart diesease is more.\n4. ap_hi > ap_lo. With the increaes of bp the chances of heart attack are more. Check if we have patients \n   with low bp but still have the disease.\n5. With increase of cholesterol the chances of heart disease increases as per scientific tests.\n6. Increase in blood glucose levels could be a cause of increased heart risk.\n7. Check about how patient drinking and smoking habbits would increase the chances of heart risk. \n   Are drinking men/women more prone to having a heart disease ?\n8. Physical Activity is assumed to help in lower cholesterol and thus lower chances of heart disease.\n\nFEATURE ENGINEERING STEPS.\n1. Use height and weight to calculate BMI of a patient and see if it has some impact on the target variable.\n2. Combine smoking and alcohol as a single feature using feature interaction.\n3. We can think of creating a feature based on age and gender of a person to check if he/she is more likely    to  have diseased.\n\nEDA \n1. Mens below 65 are more prone to disease than women. However, above 65 both of them share a almost common rate.\n2. Normal blood pressure range is 120/80 for ap_hi/ap_lo respectively. Check if we are having a heart diseased person with low bp. \n3. Cholesterol, glucose and hi bp effect on patient health."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying missing values and duplicates first.\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicates = len(data) - len(data.drop(['id'],axis=1).drop_duplicates())\ndata.drop(['id'],axis=1,inplace=True)\ndata.drop_duplicates(inplace=True)\nprint(f'{duplicates} duplicate records dropped.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that we do not have any missing values into our dataset and also have removed 24 duplicate records."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now begin first with finding some quick descriptive stats about our data.\nprint(f'{data.dtypes.value_counts()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Let us now get a quick summary of features available.')\ndata.describe().T.round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us first have a look at our target variable.\nfig, ax = plt.subplots(1,1)\nsns.countplot(data['cardio'], ax = ax)\nfor i in ax.patches:\n    height = i.get_height()\n    ax.text(i.get_x()+i.get_width()/2,height,'{:.2f}'.format((i.get_height()/len(data['cardio']))*100,'%'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow. Looks like target variable is pretty balanced, so we need not to worry about class imbalance in our problem.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age is given in days. Transforming it into years for better understanding and checking relation with the target variable.\ndata['age'] = data['age']/365","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,10))\nsns.distplot(data['age'][data['cardio']==0], ax = ax1, color='green')\nsns.distplot(data['age'][data['cardio']==1], ax = ax1,color='coral')\nax1.set_title('Age Distribution')\nax1.legend()\n\nsns.distplot(data['age'][(data['gender']==1) & (data['cardio']==1)],ax = ax2,color='pink')\nsns.distplot(data['age'][(data['gender']==2) & (data['cardio']==1)],ax = ax2,color='blue')\nax2.set_title('Disease count distribution by  gender, aged below 54.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People above the age of 54 are more likely to have diseased then below, also males below 50 are more likely to have been diagnosed with heart disease than females which confirms our assumption, even though the difference is not that drastic."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1) = plt.subplots(1,1, figsize=(10,10))\nsns.boxenplot(data['cardio'],(data['height']*0.0328084),ax=ax1)\nax1.set_title('Height / Diseased')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we can see that there are certain outliers in the feature.\nFor eg:\nThere are persons with more than 8 foot height which definitely looks and outlier \nAlso, there are few with even less then 3 foot in height which could be children. \nTo confirm this we need to check their weight and age and decide if they are outliers or could be a valid entry.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,10))\nsns.scatterplot(data['age'],data['height'][(data['height']*0.0328084)<4]*0.0328084,hue=data['cardio'],ax=ax1)\nax1.set_title('Height vs Age')\nsns.scatterplot(data['weight'],data['height'][(data['height']*0.0328084)<4]*0.0328084,hue=data['cardio'],ax=ax2)\nax2.set_title('Height vs Weight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that the people with below 4 foot in height are mostly aged above 40 and have a weight above 40kg mostly.\nThis definitely confirms that they are not children. Now for our analytical purposes we can delete such records from our data as they are hinting more towards outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting height in cms to foot.\ndata['height'] = data['height']*0.0328084 \nfilt =(data['height']>8) | (data['height']<3) \n\ndata.drop(index = list(data[filt].index),inplace=True)\nprint(f'Dataset: {data.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,5))\nsns.boxenplot(data['cardio'],(data['weight']),ax=ax1)\nax1.set_title('Weight / Diseased')\nsns.scatterplot(data['weight'],data['height'],ax=ax2,hue=data['cardio'])\nax2.set_title('height vs weight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plots we can see that there are persons with more than 155 kgs of weight with height less than 4.5 foot which seems like a bit abnormal.\nAlso, there are people with less than 25kg of weight and there are ones with more than 175 kg of weight which looks like an outlier to me.\nWe will eliminate all such records from our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Weight < 25 kg\nfilt1 = data['weight']<25\ndata.drop(index=list(data[filt1].index),inplace=True)\n\n# 2. Weight > 175 kg\nfilt2 = data['weight']>175\ndata.drop(index=list(data[filt2].index),inplace=True)\n\n# 3. Height < 4.5 & Weight > 150 kg\nfilt3 = (data['height']<4.5) & (data['weight']>150)\ndata.drop(index=list(data[filt3].index),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender\nfig,(ax) = plt.subplots(1,1)\ntmp = pd.crosstab(data['gender'],data['cardio'],normalize='index').round(4)*100\ntmp.reset_index()\ntmp.columns = ['Not Diseased','Diseased']\nax1 = sns.countplot(data['gender'],order = list(tmp.index))\nax2 = ax1.twinx()\nsns.pointplot(tmp.index,tmp['Diseased'],order = list(tmp.index),ax=ax2, color='red')\nfor x in ax1.patches:\n    height = x.get_height()\n    ax1.text(x.get_x()+x.get_width()/2,height,'{:.2f}{}'.format((height/len(data))*100,'%'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like men are more likely to have diseased then women."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ap_hi\nfilt = (data['ap_hi']<90) | (data['ap_hi']>140)\nprint(f'Normal systolic blood pressure range is between 90 and 120. However, from our dataset we can see that we have {len(data[filt])} records that are not falling within the normal range. We can replace them with their median values.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ap_hi'].replace(data[filt]['ap_hi'].values,data['ap_hi'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filt =  (data['ap_lo']>90) | (data['ap_lo']<60)\nfig, ax = plt.subplots(1,1, figsize = (30,10))\nsns.distplot(data['ap_lo'][data['ap_lo']<200],bins = 25, kde = True, ax = ax)\nxticks = [i*10 for i in range(-5,20)]\nax.set_xticks(xticks)\nax.tick_params(x,labelrotation='v')\nplt.show()\nprint(f'Similar to Systolic Blood Pressure Range the diastolic bp range should be between 60-90 for a healthy individual. However, in this case we have median values for AP_LO as {data.ap_lo.median()} which does not look correct to me. Considering this in mind we would have to do some further analysis if the data source is correct or not.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data.replace(data[filt]['ap_lo'].values,data['ap_lo'].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxenplot(data['cardio'],data['ap_lo'][data['ap_lo']<150])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cholesterol\ntmp = pd.crosstab(data['cholesterol'],data['cardio'],normalize='index')\ntmp.reset_index()\ntmp.columns = ['not diseased','diseased']\nfig, ax = plt.subplots(1,1)\nsns.countplot(data['cholesterol'],order=list(tmp.index), ax=ax)\nplot2 = ax.twinx()\nsns.pointplot(tmp.index,tmp['diseased'],order=list(tmp.index),ax=plot2)\nfor patch in ax.patches:\n    height = patch.get_height()\n    ax.text(patch.get_x()+patch.get_width()/2,height,'{:.2f}{}'.format(height/len(data['cholesterol'])*100,'%'))\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot shows that cholesterol has a great impact over the diseased state of a person."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Glucose\ntmp = pd.crosstab(data['gluc'],data['cardio'],normalize='index')\ntmp.reset_index()\ntmp.columns = ['not diseased','diseased']\nfig, ax = plt.subplots(1,1)\nsns.countplot(data['gluc'],order=list(tmp.index), ax=ax)\nplot2 = ax.twinx()\nsns.pointplot(tmp.index,tmp['diseased'],order=list(tmp.index),ax=plot2)\nfor patch in ax.patches:\n    height = patch.get_height()\n    ax.text(patch.get_x()+patch.get_width()/2,height,'{:.2f}{}'.format(height/len(data['gluc'])*100,'%'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to cholesterol, a person with high glucose levels is also more prone to have got diseased. Diabetic people BEWARE !"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We would now combine the smoking and drinking habbits of a person into a single feature **'***smoke/drink***' **and study its impact."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['smoke/drink'] = data['smoke'].apply(str)+'|'+data['alco'].apply(str)\n\ntmp = pd.crosstab(data['smoke/drink'],data['cardio'],normalize='index')\ntmp.reset_index()\ntmp.columns = ['Not diseased','diseased']\n\nfig, ax = plt.subplots(1,1)\nsns.countplot(data['smoke/drink'],order=list(tmp.index), ax=ax)\nplot2 = ax.twinx()\nsns.pointplot(tmp.index,tmp['diseased'],order=list(tmp.index),ax=plot2)\nfor patch in ax.patches:\n    height = patch.get_height()\n    ax.text(patch.get_x()+patch.get_width()/2,height,'{:.2f}{}'.format(height/len(data['smoke/drink'])*100,'%'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amongst all the people who dosen't smoke but drink seems to have the highest chances of having diseased. This seems a bit off from what the normal belief."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_smoke_drink = pd.get_dummies(data['smoke/drink'],prefix='smoke/drink',drop_first=True)\ndata = pd.concat([data,df_smoke_drink],axis=1)\ndata.drop(['smoke/drink'],axis=1,inplace=True)\n# data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would also now create a feature BMI using the height and weight of a person and see it's impact on target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# BMI = weight(kg)/height(m2)\ndata['BMI'] = data['weight']/(((data['height']/0.0328084)*.01)**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(20,10))\nsns.boxenplot(data['cardio'],data['BMI'],ax=ax1)\nsns.distplot(data[data['cardio']==0]['BMI'],color='g',ax=ax2)\nsns.distplot(data[data['cardio']==1]['BMI'],color='b',ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the above plot we can see that chances of people getting diseased is more when there BMI increases beyond 25."},{"metadata":{},"cell_type":"markdown","source":"# Modelling** ( WIP )\nStill working on it but have posted it. Might help someone.\n\nPlease UPVOTE if you liked the kernel.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The very first thing that we need to do is to break our data into training and test sets. \nfrom sklearn.model_selection import train_test_split\ntrain,test = train_test_split(data, test_size = 0.25, random_state=42)\nprint (f'The shapes of our train & test data is {train.shape} and {test.shape} respectively.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression model assumes that there should be no multi-colinearity amongst the variables. \nfig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.heatmap(train.corr().sort_values(by='cardio'), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Here we will be implementing Logistic Regression both in statsmodel and sklearn. Both of them have there own pros.\n\n\neg:- sklearn provides ease of implementation while the logistic regression gives us better model statistics "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regresssion - Selecting best penalty value for our Regularized model in scikit- learn\n\nX = np.array(train.drop(['cardio','height','weight','gender','alco','smoke'], axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio','height','weight','gender','alco','smoke'], axis=1))\ny_act = np.array(test['cardio'])\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=10, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\nlog_classifier = LogisticRegression()\n\nfrom sklearn.model_selection import GridSearchCV\nparams = {'C':[0.001, 0.1,1,10,100,1000]}\ngrid = GridSearchCV(log_classifier, cv=kfold, param_grid=params)\ngrid.fit(X,y)\ngrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nlog_classifier = LogisticRegression(C=10)\n\nlog_classifier.fit(X,y)\nprint(f'Train Score: {log_classifier.score(X,y)}')\n\ny_pred = log_classifier.predict(X_test)\nprint(f'Test Score: {accuracy_score(y_act,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statsmodel implementation.\n\nimport statsmodels.api as sm\nx1 = sm.add_constant(X)\nfeatures = list(train.drop(['cardio','height','weight','gender','alco','smoke'],axis=1).columns)\nfeatures.insert(0,'const')\nlog_reg = sm.Logit(y,x1)\nresults = log_reg.fit()\nresults.summary(xname=features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Accuracy {(results.pred_table()[0][0]+results.pred_table()[1][1])/len(train)}')\nprint(f'{results.pred_table()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test.drop(['cardio','height','weight','gender','alco','smoke'], axis=1)\nx1_test = sm.add_constant(X_test)\ny_pred = results.predict(x1_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that we have received an overall accuracy of around 69% using our Logistic Regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Classifier\n\nX = np.array(train.drop(['cardio'], axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio'], axis=1))\ny_act = np.array(test['cardio'])\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'gini', random_state=42, max_depth=10)\ndt.fit(X,y)\nprint(f'Train Accuracy for Decision Tree is : {dt.score(X,y)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Test Accuracy Score for Decision Tree is ; {accuracy_score(y_act,dt.predict(X_test))}')\nconfusion_matrix(y_act,dt.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\nX = np.array(train.drop(['cardio'],axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio'],axis=1))\ny_act = test['cardio']\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.svm import SVC\nsvc = SVC()\n\n# params = {'C':[0.1, 1, 10, 100], 'gamma':[1, 0.1, 0.01, 0.001]}\n# grid = GridSearchCV(svc, param_grid=params)\n# grid.fit(X,y)\n# grid.best_params_\n\nsvc.fit(X,y)\nprint(f'Train Score: {svc.score(X,y)}')\n\n\ny_pred = svc.predict(X_test)\nprint(f'Test Score: {accuracy_score(y_act,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes Classifier\nX = np.array(train.drop(['cardio'],axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio'],axis=1))\ny_act = test['cardio']\n\nfrom sklearn.naive_bayes import GaussianNB\ngb = GaussianNB()\ngb.fit(X,y)\nprint(f'Train Score: {gb.score(X,y)}')\n\n\ny_pred = gb.predict(X_test)\nprint(f'Test Score: {accuracy_score(y_act,y_pred)}')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\n\nX = np.array(train.drop(['cardio'], axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio'], axis=1))\ny_act = np.array(test['cardio'])\n\nfrom sklearn.ensemble import RandomForestClassifier\nparam = {'n_estimators': [10, 20, 40, 80, 160, 300], 'random_state': [42], 'criterion': ['gini'], 'max_depth': [2, 4, 8, 16, 32]}\nrf = RandomForestClassifier()\ngrid = GridSearchCV(rf,param)\ngrid.fit(X,y)\ngrid.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_upd = RandomForestClassifier(n_estimators=40, criterion='gini', max_depth=8, random_state=42)\nrf_upd.fit(X,y)\nprint(f'Train Score: {rf_upd.score(X,y)}')\n\ny_pred = rf_upd.predict(X_test)\nprint(f'Test Accuracy: {accuracy_score(y_act,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_importances = pd.concat([pd.Series(test.drop(['cardio','ap_lo'], axis=1).columns),pd.Series(rf_upd.feature_importances_)],axis=1).sort_values(by=1, ascending=False)\nFeature_importances.columns = ['Feature','Weights']\nFeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting \nX = np.array(train.drop(['cardio'],axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio'],axis=1))\ny_act = test['cardio']\n\nfrom  xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X,y)\nprint(f'Train Score: {xgb.score(X,y)}')\n\ny_pred = xgb.predict(X_test)\nprint(f'Test Accuracy: {accuracy_score(y_act,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Learning "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ANN\nimport tensorflow as tf\nimport keras as ks\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nX = np.array(train.drop(['cardio'],axis=1))\ny = np.array(train['cardio'])\nX_test = np.array(test.drop(['cardio'],axis=1))\ny_act = test['cardio']\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.fit_transform(X_test)\n\nclassifier = Sequential()\n\n\nclassifier.add(Dense(8, input_shape=(15,), activation = 'relu'))\nclassifier.add(Dense(8, activation = 'relu'))\n\nclassifier.add(Dense(1, activation = 'sigmoid'))\n\nclassifier.compile('adam',loss='binary_crossentropy', metrics=['accuracy'])\n\nclassifier.fit(X,y, batch_size= 10, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred >= 0.5)\nprint(f'Test Accuracy for Deep Learning: {accuracy_score(y_act,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The max accuracy that we have achieved by running our deep learning model with default parameters and running for 100 epochs is 71.52% on test data.\n\nHowever, there is lot of scope for improvement in all the above models which I will be working on in the next version.\n\nPlease UPVOTE if you really find it helpful and also provide your valuable feedback in comments.\nThis would really help me as well as others to learn better."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}