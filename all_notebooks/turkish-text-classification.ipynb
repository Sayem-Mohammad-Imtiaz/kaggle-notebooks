{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expression libary.\nimport nltk # Natural Language toolkit\nnltk.download(\"stopwords\")  #downloading stopwords\nnltk.download('punkt')\nfrom nltk import word_tokenize,sent_tokenize\nnltk.download('wordnet')\nimport nltk as nlp\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom wordcloud import WordCloud \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,precision_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/ttc4900/7allV03.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns \n\nsns.countplot(\"category\",data=df_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nlabels=le.fit_transform(df_copy.category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\"\"\"\n4==Siyaset\n0==Dunya\n1==ekonomi\n2==kultur\n3==saglik\n5==spor\n6==teknoloji\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_list=[]\n\nfor text in df_copy.text:\n    text = text.lower()  #Büyük harften -Küçük harfe çevirme\n    text = re.sub(\"[^abcçdefgğhıijklmnoöprsştuüvyz]\",\" \",text)\n    text=nltk.word_tokenize(text) # splits the words that are in the sentence from each other.\n    text =[word for word in text if not word in set(stopwords.words(\"turkish\"))]\n    lemma=nlp.WordNetLemmatizer()\n    text=[lemma.lemmatize(word) for word in text] # this code finds the root of the word for a word in the sentence and change them to their root form.\n    text=\" \".join(text)\n    text_list.append(text) # store sentences in list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer #Bag of Words\n\nmax_features=500 # \"number\" most common(used) words in reviews\n\ncount_vectorizer=CountVectorizer(max_features=max_features) \n\nsparce_matrix=count_vectorizer.fit_transform(text_list).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparce_matrix.shape #4900 sentences 500 most used words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparce_matrix[0:10,0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Top {} the most used word by reviewers: {}\".format(max_features,count_vectorizer.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.DataFrame(count_vectorizer.get_feature_names(),columns=[\"Words\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(12,12))\nwordcloud=WordCloud(background_color=\"black\",width=1024,height=768).generate(\" \".join(data.Words[5:]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=sparce_matrix\ny=labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nprint(\"x_train\",X_train.shape)\nprint(\"x_test\",X_test.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM "},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_model=LGBMClassifier()\n\nlgbm_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=lgbm_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params={\"n_estimators\":[100,500,100,2000],\n            \"subsample\":[0.6,0.8,1.0],\n            \"learning_rate\":[0.1,0.01,0.02,0.05],\n            \"min_child_samples\":[5,20,10],\n            \"max_depth\":[3,4,5,6]}\n\nlgbm=LGBMClassifier()\nlgbm_cv=RandomizedSearchCV(lgbm,lgbm_params,cv=10,n_jobs=-1,verbose=2)\nlgbm_cv_model=lgbm_cv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm=LGBMClassifier(learning_rate= 0.01,max_depth= 3,min_child_samples= 10,n_estimators= 2000,subsample= 0.6)\nlgbm_tuned_model=lgbm.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=lgbm_tuned_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') #835 true predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model=RandomForestClassifier(random_state=42)\nrf_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=rf_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))\n\ncm=confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_params={\"max_depth\":[2,5,8,10],\n           \"max_features\":[2,5,8],\n           \"n_estimators\":[10,500,1000],\n          \"min_samples_split\":[2,5,10]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier(random_state=42)\nrf_cv=RandomizedSearchCV(rf,rf_params,cv=10,n_jobs=-1,verbose=2)\nrf_cv_model=rf_cv.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned_model=RandomForestClassifier(random_state=42,max_depth=10,max_features= 2,min_samples_split= 5,n_estimators= 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tuned_model.fit(X_train,y_train)\n\ny_pred=rf_tuned_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))\n\ncm=confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb=XGBClassifier()\n\nxgb_model=xgb.fit(X_train,y_train)\n\ny_pred=xgb_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))\n\ncm=confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred)) \n# classification report is good function for seeing how well our model predict labels in each class  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I didt tune my Xgboost model because it took too much time."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[10] # I use this text for prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[10] # true label of X_test[10] 3== health","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model.predict(X_test[10].reshape(-1,500)) # predicted label of X_test[10] \"Predicted Correctly\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections \n\nfor index,liste in enumerate(sparce_matrix):\n    if collections.Counter(liste) == collections.Counter(X_test[10]): # searching X_test[10] in space matrix and Turn its index\n        print(index)\n        \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer.inverse_transform(sparce_matrix[2827]) \n# these are words from X_test[10](tokenized and cleared) and as you see there are related word with health(3) like ilaç==medicine,bilim==science","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy.text[2827] # original text ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Turkish Text Classification with Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df_copy.text.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_list=[] # store text in this list\n\nfor text in X:\n    text = text.lower()\n    text = re.sub(\"[^abcçdefgğhıijklmnoöprsştuüvyz]\",\" \",text)\n    text = text.split()\n    text =[word for word in text if not word in set(stopwords.words(\"turkish\"))]\n    text=\" \".join(text)\n    X_list.append(text) # store sentences in list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_label=[]\n\nfor i in labels:\n    y_label.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_label[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_list, y_label, test_size=0.2, random_state=42,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train)) # 3920 sentences\nprint(len(y_train)) #3920 Labels\nprint(len(X_test))  # 980 sentences\nprint(len(y_test)) # 980 labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_lenght=100\n\ntokenizer = Tokenizer() \ntokenizer.fit_on_texts(X_train)\n\n\nword_index = tokenizer.word_index # creating word dict for words in training\n\nsequences = tokenizer.texts_to_sequences(X_train)  # replacing words with the number corresponding to them in the dictionary(word_index)\n\nX_train_padded = pad_sequences(sequences, padding='post',maxlen=max_lenght) # padding words\n\nprint(len(word_index)) # I have 94836 words in my dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Original Version:\",X_train[0])\nprint(\"---------------------------------\")\nprint(\"Padded version\",X_train_padded[0]) \nprint(\"---------------------------------\")\nprint(\"Tokenized version:\",sequences[0])  # change words with number that corresponding to word word_index\nprint(\"---------------------------------\")\nprint(\"Shape after the padding:\",X_train_padded.shape) # make our input same size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sequences = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_sequences,padding=\"post\",maxlen=max_lenght)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Original Version:\",X_test[0])\nprint(\"---------------------------------\")\nprint(\"Padded version\",X_test_padded[0])   # make inputs same size\nprint(\"---------------------------------\")\nprint(\"Tokenized version:\",X_test_sequences[0]) # change words with number that corresponding to word word_index\nprint(\"---------------------------------\")\nprint(\"Shape after the padding:\",X_test_padded.shape) # make inputs same size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nvocab_size = len(tokenizer.word_index)+1\nembedding_dim=16\n\nmodel = tf.keras.Sequential([\n    \n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=100),\n    \n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.BatchNormalization(),\n    \n    tf.keras.layers.Flatten(),\n    \n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(7, activation='softmax')\n])\n\nadam= tf.keras.optimizers.Adam(lr=0.01) \n\n\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_label=np.asarray(y_train).reshape(-1,1)\ny_test_label=np.asarray(y_test).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 10\nBATCH_SIZE=64\nhistory=model.fit(X_train_padded,y_train_label,batch_size=BATCH_SIZE ,epochs=num_epochs, validation_data=(X_test_padded,y_test_label))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nplt.plot(history.history[\"accuracy\"],color=\"green\")\nplt.plot(history.history[\"loss\"],color=\"red\")\nplt.title(\"Train accuracy and Train loss\")\nplt.legend([\"Accuracy\",\"Loss\"])\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history[\"val_accuracy\"],color=\"blue\")\nplt.plot(history.history[\"val_loss\"],color=\"orange\")\nplt.title(\"Test accuracy and Test loss\")\nplt.legend([\"Val_accuracy\",\"Val_loss\"])\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train_padded,y_train_label)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test_padded,y_test_label)[1]*100 , \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM model is overfitted on training data "},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict_classes(X_test_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[10:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_label[10:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,pred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,linecolor=\"white\",fmt='')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test_label,pred)) # model is better when classify 3 and 5 classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So,This is end of my notebook. Thanks for you to took a look at  my notebook.I hope you like it :)\n\nI changed my LSTM model more than 15 times but I didnt prevent overfitting. \nI think machine learning algorithm is much better than LSTM model for this dataset but You may achieve higher score then me.\n\nif you have a solution please feel free to write your thoughts on comment section"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}