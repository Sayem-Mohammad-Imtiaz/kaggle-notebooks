{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Real Estate Price Prediction - Bengaluru House Price Dataset\nIn this notebook we will do E.D.A. (Exploratory Data Analysis) and trainig on the `Bengaluru House Price Dataset` using sklearn libraries and matplotlib.\nSklearn is a library machine-learning and matplotlib is also a popular library for data visualization.\n\nDo Upvote and Comment below!","metadata":{}},{"cell_type":"markdown","source":"## Let's Get Started!","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (20,10)\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nimport pickle\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Reading Data\nWe will use pandas `read_csv` function to read the data in csv file.","metadata":{}},{"cell_type":"code","source":"filepath = '../input/bengaluru-house-price-data/Bengaluru_House_Data.csv'    #filepath of the dataset\ndata = pd.read_csv(filepath)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Preprcessing\nData preprocessing is a data mining technique that involves transforming raw data into an understandable format.","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby('area_type')['area_type'].agg('count')    # taking the 'area_type' column as a group and counting its values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`groupby()` function is used to split the data into groups based on some criteria\nand\n`agg()` function abbreviation of aggregate is used to define what we want to do with the grouped data.\n","metadata":{}},{"cell_type":"code","source":"data2 = data.drop(['area_type','society','balcony','availability'], axis='columns')     # dropping useless columns\ndata2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.isnull().sum()     # finding total empty values in each column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = data2.dropna()    # dropping 'NA' values\ndata3.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3['size'].unique()    # checking unique values in size column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3['bhk'] = data3['size'].apply(lambda x: int(x.split(' ')[0]))    # getting the number of bedrooms from size column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3['bhk'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3[data3.bhk>20]    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding rows with extraordinary values!ðŸ˜… (Outliers)","metadata":{}},{"cell_type":"code","source":"data3.total_sqft.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_float(x):\n    '''\n    Function to convert values into float dtype.\n    '''\n    try:\n        float(x)\n    except:\n        return False\n    return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3[~data3['total_sqft'].apply(is_float)].head(20)    # finding values those not got converted","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_sqft_to_sum(x):\n    '''\n    Function to convert those unusual format of data\n    '''\n    tokens = x.split('-')\n    if len(tokens) == 2:\n        return (float(tokens[0]) + float(tokens[1])) / 2\n    try:\n        return float(x)\n    except:\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convert_sqft_to_sum('2100 - 2850')    # usage of the function","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4 = data3.copy()\ndata4['total_sqft'] = data4['total_sqft'].apply(convert_sqft_to_sum)\ndata4.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4.loc[30]    # loc function is used to see data row-wise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5 = data4.copy()    # copy function is used to copy the whole dataframe\ndata5.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5['price_per_sqft'] = data5['price'] * 100000/data5['total_sqft']    # making a new column in the dataframe named `price_per_sqft` and see the logic to create it\ndata5.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data5.location.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.location = data5.location.apply(lambda x : x.strip())    # strip is used to remove the white spaces around the data points\n\nlocation_stats = data5.groupby('location')['location'].agg('count').sort_values(ascending=False)    # sorting the location column in descending order\nlocation_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(location_stats[location_stats<=10])    # totaling the minor locations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_stats_less_than_10 = location_stats[location_stats<=10]\nlocation_stats_less_than_10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.location = data5.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)    # changing minor locations into `other`","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data5.location.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5[data5.total_sqft / data5.bhk < 300].head()    # checking for outliners; like a house with 1407 sq. area can't have 6 bedrooms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data6 = data5[~(data5.total_sqft/data5.bhk<300)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data6.price_per_sqft.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data6.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Cleaning\nIn this section, we will find outliers and try to remove them.","metadata":{}},{"cell_type":"code","source":"def remove_pps_outliers(df):\n    '''\n    Function to clear stuff (outliers) in the price_per_sqft column so that we don't live in a hypothetical dataset. ðŸ˜„\n    '''\n    df_out = pd.DataFrame()\n    for key, subdf in df.groupby('location'):\n        m = np.mean(subdf.price_per_sqft)\n        st = np.std(subdf.price_per_sqft)\n        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data7 = remove_pps_outliers(data6)\ndata7.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data7.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_bhk_outliers(df):\n    '''\n    Function to clear stuff (outliers) in the bhk column so that we don't live in a hypothetical dataset. ðŸ˜„\n    '''\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.price_per_sqft),\n                'std': np.std(bhk_df.price_per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\n\ndata8 = remove_bhk_outliers(data7)\ndata8.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Visualization\nTime to visualize our data","metadata":{}},{"cell_type":"code","source":"def plot_scatter_chart(df,location):\n    '''\n    Function that will help us to visualize the data of the different locations \n    '''\n    bhk2 = df[(df.location==location) & (df.bhk==2)]\n    bhk3 = df[(df.location==location) & (df.bhk==3)]\n    matplotlib.rcParams['figure.figsize'] = (15, 10)\n    plt.scatter(bhk2.total_sqft, bhk2.price,color='blue', label='2 BHK', s=50)\n    plt.scatter(bhk3.total_sqft, bhk3.price,marker='+',color='green', label='3 BHK', s=50)\n    plt.xlabel('Total Square Feet Area')\n    plt.ylabel('Price Per Square Feet')\n    plt.title(location)\n    plt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_scatter_chart(data7, 'Whitefield')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_scatter_chart(data7,\"Hebbal\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(data8.price_per_sqft, rwidth=0.8)    # visualization the price_per_sqft column\nplt.xlabel('Price per Square Feet')\nplt.ylabel('Count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data8.bath.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data8[data8.bath>10]    # Again some idiot outliersðŸ˜†","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(data8.bath, rwidth=0.8)\nplt.xlabel('Number of bathrooms')\nplt.ylabel('Count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data8[data8.bath > data8.bhk + 2]    # plz explain me how can someone have more bathrooms that bedrooms ðŸ˜‚","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data9 = data8[data8.bath < data8.bhk + 2]\ndata9.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data9.sample()    # sample return a random row from the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data10 = data9.drop(['size',\"price_per_sqft\"],axis='columns')    # removing or dropping 'size' and 'prize_per_sqft' as we don't require them any more\ndata10.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Creating Dummies\nWe will use pandas' `get_dummies()` to create dummies variables.\nIt is used for data manipulation. It converts categorical data into dummy or indicator variables.","metadata":{}},{"cell_type":"code","source":"dummies = pd.get_dummies(data10.location)     \ndummies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data11 = pd.concat([data10, dummies.drop('other', axis='columns')], axis='columns')    # joining the dummy values again with the dataset except 'other' column\ndata11.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data12 = data11.drop('location', axis='columns')    # dropping original location as now we have dummmies in its place.\ndata12.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Splitting Data for Training and Testing\nBefore Training the model, it is required to split the data into train and test data. For this we will use, sklearn's `train_test_split`\n","metadata":{}},{"cell_type":"code","source":"X = data12.drop('price', axis='columns')    # dropping price column as we don't want it in our train dataset\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data12.price    # taking the price column as our target to predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)    # splitting the the data into train and test data as 80 : 20 ratio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Training the Models\nPreciesly, we will be trying more than one model, therefore, Training the `Models`","metadata":{}},{"cell_type":"code","source":"lr_clf = LinearRegression()     # first trying training with LinearRegression \nlr_clf.fit(X_train, y_train)\nlr_clf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0)     # ShuffleSplit is just a another type of splitting data\ncross_val_score(LinearRegression(), X, y, cv=cv)     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross Validation is mainly used for the comparison of different models. For each model, you may get the average generalization error on the k validation sets. Then you will be able to choose the model with the lowest average generation error as your optimal model.","metadata":{}},{"cell_type":"code","source":"def find_best_model_using_gridsearchcv(X,y):\n    '''\n    Function to try different models at once of the data with different parameters to find the best ones.\n    '''\n    algos = {\n        'linear_regression':{\n            'model': LinearRegression(),\n            'params':{\n                'normalize':[True,False]\n            }\n        },\n        'lasso':{\n            'model': Lasso(),\n            'params':{\n                'alpha' : [1,2],\n                'selection':['random','cyclic']\n            }\n        },\n        'decision_tree':{\n            'model': DecisionTreeRegressor(),\n            'params':{\n                'criterion':['mse','friedman_mse'],\n                'splitter':['best','random']\n            }\n        }\n    }\n    scores = []\n    cv = ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n    for algo_name, config in algos.items():\n        gs = GridSearchCV(config['model'],config['params'], cv=cv, return_train_score=False)     # GridSearchCV is the main focus as it helps to try out the different parameters for the different models.\n        gs.fit(X,y)\n        scores.append({\n            'model': algo_name,\n            'best_score':gs.best_score_,\n            'best_params':gs.best_params_\n        })\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])     # At last binding the results of the models with best params. into a DataFrame.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_best_model_using_gridsearchcv(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the LinearRegression model performed the best, so we are going to use as it for prediction.","metadata":{}},{"cell_type":"markdown","source":"# 9. Prediction Time\nPredicting the prices using LinearRegression in Lakhs.","metadata":{}},{"cell_type":"code","source":"def predict_price(location,sqft,bath,bhk):\n    '''\n    Function which helps to actually predict the prices.\n    '''\n    loc_index = np.where(X.columns==location)[0][0]     # np.where() function returns the indices of elements in an input array where the given condition is satisfied.\n    \n    x = np.zeros(len(X.columns))    # np.zeros() function returns a new array of given shape and type, with zeros.\n    x[0] = sqft\n    x[1] = bath\n    x[2] = bhk\n    if loc_index >= 0:\n        x[loc_index] = 1\n        \n    return lr_clf.predict([x])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predict_price('1st Phase JP Nagar', 1000, 2, 3).round(3),'Lakhs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Saving Model\nFor saving the model, we will be using `pickle` module and `json` module for saving the locations' names.","metadata":{}},{"cell_type":"code","source":"with open('BHP_model.pickle','wb') as f:\n    pickle.dump(lr_clf,f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = {\n    'data_columns' : [col.lower() for col in X.columns]\n}\nwith open('columns.json','w') as f:\n    f.write(json.dumps(columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bravo! We completed the model training for Bengaluru Home Prices Dataset.\n### Please upvote this notebook if you like and find it helpful and please comment down below your views and suggestions.\n\n## Thank You!ðŸ˜Š","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}