{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"font-family: 'Garamond'; font-size:50px\"><center>Práctica 2: Técnicas de Análisis.</center></h1>\n<h2 style=\"font-family: 'Garamond'; font-size:30px\"><center>Análisis de datos epidemiológicos de Covid-19.</center></h2>\n<h2 style=\"font-family: 'Garamond'; font-size:30px\"><center>Universidad de Alicante. Master Ciencias de Datos.</center></h2>\n<h2 style=\"font-family: 'Garamond'; font-size:30px\"><center>Mineria de Datos.</center></h2>\n<h3 style=\"font-family: 'Garamond'; font-size:20px\"><center>Nahuel Emiliano García D'Urso</center></h3>\n<h3 style=\"font-family: 'Garamond'; font-size:20px\"><center>2021</center></h3>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Introducción\"></a>\n# Introducción","metadata":{}},{"cell_type":"markdown","source":"Esta práctica estara dividada en dos grandes apartados:\n   * Parte 1. Regresión y Clasificación\n   * Parte 2. Ensembles, clustering y reglas de asociación\n\nEn la primera parte se realizaran tareas de regresión y clasificación, mientras que en la segunda parte se realizarán tareas de ensambling y clustering. Además de esto, realizaré una trabajo de obtención y pre-proceso de los datos utilizados para el trabajo.\n\n* [Introducción](#Introducción)\n* [Dataset](#Datasets)\n    - [Datos](#Datos)\n* [Limpieza y preparación de los datos](#Limpieza)\n* [Parte 1. Regresión y Clasificación](#Parte1)\n* [Parte 2. Ensembles, clustering y reglas de asociación](#Parte2)\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Datasets\"></a>\n# Dataset","metadata":{}},{"cell_type":"markdown","source":"En esta práctica se analizarán los datos que se pueden obtener del repositorio [COVID-19 Open Data](https://github.com/GoogleCloudPlatform/covid-19-open-data). Este repositorio se ha creado con le objetido de crear una de las mayores bases de datos sobre datos relacionados con el COVID-19. Contiene datos públicos bajo licencia realacionados con la demografía, económicos, epidemiológicos, geográficos, salud, hospitalarios, movilidad, respuesta del gobierno, clima, etc. Los datos son creados gracias a la fusión de mas de 20.000 fuentes globales.\n\nTodas las regiones tienen asignado un código específico e único. Esto soluciona el problema actual de que las diferentes bases de datos utilizen diferentes tipos de códigos como: ISO/NUTS/FIPS, etc.\n\nLos datos extraidos están agrupados en diferentes CSV. En la siguiente tabla podemos ver los diferentes CSV que podremos obtener.\n\n\n| Table | Keys<sup>1</sup> | Content | URL | Source<sup>2</sup> |\n| ----- | ---------------- | ------- | --- | ------------------ |\n| **Main** | `[key][date]` | Flat table with records from (almost) all other tables joined by `date` and/or `key`; see below for more details | [main.csv](https://storage.googleapis.com/covid19-open-data/v2/main.csv) | All tables below |\n| **Index** | `[key]` | Various names and codes, useful for joining with other datasets | [index.csv](https://storage.googleapis.com/covid19-open-data/v2/index.csv), [index.json](https://storage.googleapis.com/covid19-open-data/v2/index.json) | Wikidata, DataCommons, Eurostat |\n| **Demographics** | `[key]` | Various (current<sup>3</sup>) population statistics | [demographics.csv](https://storage.googleapis.com/covid19-open-data/v2/demographics.csv), [demographics.json](https://storage.googleapis.com/covid19-open-data/v2/demographics.json) | Wikidata, DataCommons, WorldBank, WorldPop, Eurostat |\n| **Economy** | `[key]` | Various (current<sup>3</sup>) economic indicators | [economy.csv](https://storage.googleapis.com/covid19-open-data/v2/economy.csv), [economy.json](https://storage.googleapis.com/covid19-open-data/v2/economy.json) | Wikidata, DataCommons, Eurostat |\n| **Epidemiology** | `[key][date]` | COVID-19 cases, deaths, recoveries and tests | [epidemiology.csv](https://storage.googleapis.com/covid19-open-data/v2/epidemiology.csv), [epidemiology.json](https://storage.googleapis.com/covid19-open-data/v2/epidemiology.json) | Various<sup>2</sup> |\n| **Emergency Declarations** | `[key][date]` | Government emergency declarations and mitigation policies | [lawatlas-emergency-declarations.csv](https://storage.googleapis.com/covid19-open-data/v2/lawatlas-emergency-declarations.csv) | LawAtlas Project |\n| **Geography** | `[key]` | Geographical information about the region | [geography.csv](https://storage.googleapis.com/covid19-open-data/v2/geography.csv), [geography.json](https://storage.googleapis.com/covid19-open-data/v2/geography.json) | Wikidata |\n| **Health** | `[key]` | Health indicators for the region | [health.csv](https://storage.googleapis.com/covid19-open-data/v2/health.csv), [health.json](https://storage.googleapis.com/covid19-open-data/v2/geography.json) | Wikidata, WorldBank, Eurostat |\n| **Hospitalizations** | `[key][date]` | Information related to patients of COVID-19 and hospitals |  [hospitalizations.csv](https://storage.googleapis.com/covid19-open-data/v2/hospitalizations.csv), [hospitalizations.json](https://storage.googleapis.com/covid19-open-data/v2/hospitalization.json) | Various<sup>2</sup> |\n| **Mobility** | `[key][date]` | Various metrics related to the movement of people.<br/><br/>To download or use the data, you must agree to the Google [Terms of Service](https://policies.google.com/terms). | [mobility.csv](https://storage.googleapis.com/covid19-open-data/v2/mobility.csv), [mobility.json](https://storage.googleapis.com/covid19-open-data/v2/mobility.json) | Google |\n| **Search Trends** | `[key][date]` | Trends in symptom search volumes due to COVID-19.<br/><br/>To download or use the data, you must agree to the Google [Terms of Service](https://policies.google.com/terms). | [google-search-trends.csv](https://storage.googleapis.com/covid19-open-data/v2/google-search-trends.csv) | Google |\n| **Government Response** | `[key][date]` | Government interventions and their relative stringency | [oxford-government-response.csv](https://storage.googleapis.com/covid19-open-data/v2/oxford-government-response.csv), [oxford-government-response.json](https://storage.googleapis.com/covid19-open-data/v2/oxford-government-response.json) | University of Oxford |\n| **Weather** | `[key][date]` | Dated meteorological information for each region | [weather.csv](https://storage.googleapis.com/covid19-open-data/v2/weather.csv) | NOAA |\n| **WorldBank** | `[key]` | Latest record for each indicator from WorldBank for all reporting countries | [worldbank.csv](https://storage.googleapis.com/covid19-open-data/v2/worldbank.csv), [worldbank.json](https://storage.googleapis.com/covid19-open-data/v2/worldbank.json) | WorldBank |\n| **By Age** | `[key][date]` | Epidemiology and hospitalizations data stratified by age | [by-age.csv](https://storage.googleapis.com/covid19-open-data/v2/by-age.csv), [by-age.json](https://storage.googleapis.com/covid19-open-data/v2/by-age.json) | Various<sup>2</sup> |\n| **By Sex** | `[key][date]` | Epidemiology and hospitalizations data stratified by sex | [by-sex.csv](https://storage.googleapis.com/covid19-open-data/v2/by-sex.csv), [by-sex.json](https://storage.googleapis.com/covid19-open-data/v2/by-sex.json) | Various<sup>2</sup> |\n\n<sup>1</sup> `key` es una clave única para cada region geográfica construida a partir de una combinacion de claves, como por ejemplo ISO 3166 o NUTS\\\n<sup>2</sup> Todos los datos son obtenidos automáticamente. Cuando es posible, son obtenidos directamente de fuentes oficiales, como pueden ser las fuentes del ministerio de salud. El listado de las fuentes segun la tabla la podemos encontrar en [docs](https://github.com/GoogleCloudPlatform/covid-19-open-data/tree/main/docs)\\\n<sup>3</sup> Datasets sin la columna de 'date' contienen datos con la informacion más actualizada.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Datos\"></a>\n# Datos","metadata":{}},{"cell_type":"markdown","source":"Los datos se pueden obtener de forma sencilla de la siguiente forma:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n#style.use('seaborn-colorblind') or plt.style.use('seaborn-colorblind')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Este repositorio de GitHub nos permite descargar los datos de diferentes manera. En esta práctica se han obtenido los datos mediante la url y haciendo uso de las fuciones de la libreria pandas.","metadata":{}},{"cell_type":"markdown","source":"Con el siguiente código obtendriamos los datos actualizado hasta el último día. Una cosa a tener en cuenta, es la opción *keep_defaul_na=False*. Hay que poner este argumento a False ya que, el país de Namibia tiene como código NA, y esto puede generar errores a lo hora de trabajar con los datos.","metadata":{}},{"cell_type":"code","source":"# Obtención de los datos (solo los últimos)\ndata = pd.read_csv(\n    \"https://storage.googleapis.com/covid19-open-data/v2/latest/main.csv\",\n    keep_default_na=False,\n    na_values=[\"\"],\n)\n\n# Obtención de los datos (todos los datos)\n\"\"\"\ndata = pd.read_csv(\n    \"https://storage.googleapis.com/covid19-open-data/v2/main.csv\",\n    keep_default_na=False,\n    na_values=[\"\"],\n)\n\"\"\"\nprint('El dataset contiene un total de', len(data), 'filas')\nprint('En ejemplo de dataset:')\ndata.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El dataset completo está compuesto por un total de 111 columnas.","metadata":{}},{"cell_type":"code","source":"print('Nº de columnas: ',len(data.columns))\nprint(data.columns[0:50])\nprint(data.columns[50:111])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nDado que se quiere trabajar con todos los datos no solo con los últimos, deberiamos utilizar la segunda opción. La razón por la que está comentada es porque la cantidad de datos es muy grande (3.4G) y kaggle no lo permite. Una forma de obtener los datos seria descargar cada uno de las tablas por separado y juntarlas.\n\nEste método sería el correcto ya que, siempre trabjariamos sobre los datos actualizados. Pero tendría como problema que las conclusiones obtenidas deberian variar en cada ejecución. Por eso se ha optado por crear un dataset en kaggle (covid19fulldataset). Este dataset es el resultado de juntar las tablas con las columnas y filas que se van a utilizar. \n\nLas columnas seleccionadas son:\n\n**Datos utilizados para filtrar por pais (key) y fecha**\n* `key`: Indica el país (código).\n* `date`: Fecha\n\n**Por edad**\n* `new_confirmed_age_00`: Nuevos casos confirmados entre las edades de 0 y 9\n* `new_confirmed_age_01`: Nuevos casos confirmados entre las edades de 10 y 19\n* `new_confirmed_age_02`: Nuevos casos confirmados entre las edades de 20 y 29\n* `new_confirmed_age_03`: Nuevos casos confirmados entre las edades de 30 y 39\n* `new_confirmed_age_04`: Nuevos casos confirmados entre las edades de 40 y 49\n* `new_confirmed_age_05`: Nuevos casos confirmados entre las edades de 50 y 59\n* `new_confirmed_age_06`: Nuevos casos confirmados entre las edades de 60 y 69\n* `new_confirmed_age_07`: Nuevos casos confirmados entre las edades de 70 y 79\n* `new_confirmed_age_08`: Nuevos casos confirmados entre las edades de 80 y 89\n* `total_confirmed_age_00`: Casos acumulados entre las edades de 0 y 9\n* `total_confirmed_age_01`: Casos acumulados entre las edades de 10 y 19\n* `total_confirmed_age_02`: Casos acumulados entre las edades de 20 y 29\n* `total_confirmed_age_03`: Casos acumulados entre las edades de 30 y 39\n* `total_confirmed_age_04`: Casos acumulados entre las edades de 40 y 49\n* `total_confirmed_age_05`: Casos acumulados entre las edades de 50 y 59\n* `total_confirmed_age_06`: Casos acumulados entre las edades de 60 y 69\n* `total_confirmed_age_07`: Casos acumulados entre las edades de 70 y 79\n* `total_confirmed_age_08`: Casos acumulados entre las edades de 80 y 89\n\n**Datos meteorológicos**\n* `average_temperature`: Temperatura media de todo el dia en una región concreta.\n* `rainfall`: Lluvia durante el día en la regiín (mm).\n* `dew_point`: Temperatura a la que debe enfriarse el aire para que se sature de vapor de agua.\n* `relative_humidity`: La cantidad de vapor de agua presente en el aire expresada como porcentaje de la cantidad necesaria para la saturación a la misma temperatura.\n\n**Datos hospitalarios:**\n* `new_hospitalized`: Número de nuevos casos hospitalizados despues de dar positivo en un test.\n* `new_intensive_care`: Número de nuevos casos hospitalizados (UCI) despues de dar positivo en un test.\n* `total_hospitalized`: Número total de casos hospitalizados despues de dar positivo en un test.\n* `total_intensive_care`: Número total de casos hospitalizados (UCI) despues de dar positivo en un test.\n\n**Datos sobre restricciones**\n* `school_closing`: Indica en que grado están cerrados los colegios, siendo 0 abiertos con normalidad y 3 cerrados completamente.\n\nPara esta práctica solo se trataran aquellos datos realacionados con la Comunidad Valenciana.","metadata":{}},{"cell_type":"markdown","source":"> #### **Cargamos la tabla que nos dira cual es el código de la region que queramos buscar**","metadata":{}},{"cell_type":"code","source":"url = \"https://storage.googleapis.com/covid19-open-data/v2\"\nidName = pd.read_csv(url + \"/index.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtenemos la fila que nos dira cual es el codigo de la comunidad valenciana\n# key = countrycode_subregion1_code\n# En este caso nuestra key es: ES_VC\ncode =  idName[idName.subregion1_name == 'Comunidad Valenciana']['country_code'].values[0] +'_'+ \\\n        idName[idName.subregion1_name == 'Comunidad Valenciana']['subregion1_code'].values[0]\nprint('Código: ', code)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> #### **Cargamos los datasets que contienen las columnas nombradas anteriormente.**","metadata":{}},{"cell_type":"code","source":"# Creacion de dataset para kaggle\ndata1 = pd.read_csv(url + \"/epidemiology.csv\")\ndata2 = pd.read_csv(url + \"/hospitalizations.csv\")\ndata3 = pd.read_csv(url + \"/oxford-government-response.csv\")\ndata4 = pd.read_csv(url + \"/by-age.csv\")\ndata5 = pd.read_csv(url + \"/weather.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ####  **Filtramos por región y obtenemos solo las columnas que queremos**","metadata":{}},{"cell_type":"markdown","source":"Primero, vamos a preprocesar la tabla que va por edades. Crearemos un diccionario para cambiar los nombres de las columnas por el correcto. Luego obtendremos los datos de la Comunidad Valenciana y por último nos quedaremos con las columnas que queremos y les cambiaremos el nombre.","metadata":{}},{"cell_type":"markdown","source":"Diccionario que nos servira posteriormente para cambiar el nombre de las columnas en la tabla **data4**","metadata":{}},{"cell_type":"code","source":"new_names = {'new_confirmed_age_00':'new-0-9',\n            'new_confirmed_age_01':'new-10-19',\n            'new_confirmed_age_02':'new-20-29',\n            'new_confirmed_age_03':'new-30-39',\n            'new_confirmed_age_04':'new-40-49',\n            'new_confirmed_age_05':'new-50-59',\n            'new_confirmed_age_06':'new-60-69',\n            'new_confirmed_age_07':'new-70-79',\n            'new_confirmed_age_08':'new-80-89',\n            'total_confirmed_age_00':'total-0-9',\n            'total_confirmed_age_01':'total-10-19',\n            'total_confirmed_age_02':'total-20-29',\n            'total_confirmed_age_03':'total-30-39',\n            'total_confirmed_age_04':'total-40-49',\n            'total_confirmed_age_05':'total-50-59',\n            'total_confirmed_age_06':'total-60-69',\n            'total_confirmed_age_07':'total-70-79',\n            'total_confirmed_age_08':'total-80-89'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epidemiology = data1[data1.key == code]\nepidemiology = epidemiology[{'key','date','new_confirmed', 'new_deceased', 'total_confirmed', 'total_deceased'}]\n\nhospitalizations = data2[data2.key == code]\nhospitalizations = hospitalizations[{'key','date','new_hospitalized', 'new_intensive_care', 'total_hospitalized', 'total_intensive_care'}]\n\n# De esta tabla obtendremos los datos a nivel nacional (España). Ya que al estar en estado de alarma es el \n# estado español el que pone las normas de restricciones\n# nos quedaremos con las restricciones en los colegios. Estas se utilizarán más adelante en la clasificación\nschool = data3[data3.key == 'ES']\nschool = school[{'key','date','school_closing'}]\n\nby_age = data4[data4.key == code]\ncols = data4.columns\nby_age = by_age[cols[np.r_[0:11, 12:21]]]\nby_age.rename(columns=new_names,inplace=True)\n\nweather = data5[data5.key == code]\nweather = weather[{'key','date','average_temperature','rainfall','dew_point','relative_humidity'}]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ####  **Convertimos la fecha en formato datetime**","metadata":{}},{"cell_type":"code","source":"epidemiology['date'] = pd.to_datetime(epidemiology['date'])\nhospitalizations['date'] = pd.to_datetime(hospitalizations['date'])\nschool['date'] = pd.to_datetime(school['date'])\nby_age['date'] = pd.to_datetime(by_age['date'])\nweather['date'] = pd.to_datetime(weather['date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ####  **Juntamos las tablas en una solo tabla**","metadata":{}},{"cell_type":"code","source":"df = epidemiology.merge(hospitalizations, how='left', on='date')\ndf = df.merge(school, how='left', on='date')\ndf = df.merge(by_age, on='date')\ndf = df.merge(weather, on='date')\ndf = df.dropna()\ndf = df.drop(['key_x', 'key_y'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# En caso de que querramos guardar los datos.\n# df.to_csv('covid19_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como he dicho anteriormente para que no haya cambios en los datos y así cualquier persona pueda replicar los mismos resultados se va a utilizar un dataset que está actualizado hasta el día 29 del 03 de 2021. Este dataset fue creado con todo el proceso nombrado anteriormente y se encuentra en [covid19fulldataset](https://www.kaggle.com/nawue6/covid19fulldataset).","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/covid19fulldataset/covid19_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se elimina la columna Unnamed: 0. Esta columna antes era el indice.\ndata = data.drop(['Unnamed: 0'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Columnas: ', data.columns)\nprint('Número de filas: ', len(data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Parte1\"></a>\n# Parte 1. Regresión y Clasificación","metadata":{}},{"cell_type":"markdown","source":"En esta parte de la práctica se van a aplicar diversos métodos de regresión y clasificación para los datos expuestos en la primera parte.\nAntes de nada, se van a mostrar las matrices de correlaciones entre las diferentes variables escogidas para el desarrollo de esta parte.\n\n\nUna forma sencilla de ver las relaciones entre las variables de nuestro dataframe es utilizar la función `pairplot` del paquete **seaborn**. Esta función nos devuelve un scaterplot para cada variable con el resto de variables y tambien un histograma para cada variable.","metadata":{}},{"cell_type":"code","source":"# Escojemos las columnas que queremos plotear\ndf = data[{'average_temperature', 'relative_humidity',\n           'new-80-89', 'new_confirmed', 'new_deceased',\n           'new_hospitalized', 'new_intensive_care', 'new-20-29'}]\n\n# Ordenamos las columnas\ndf = df.reindex(sorted(df.columns), axis=1)\n\n# Fijamos el tamaño de la figura\nplt.figure(figsize=(20, 10))\nsns.pairplot(df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con la función `corr()` del paquete de **pandas** podemos obtener de una forma rápida la matriz de correlación entre las diferentes variables del dataframe.","metadata":{}},{"cell_type":"code","source":"df = data[{'new-0-9','new-10-19', 'new-20-29', 'new-30-39', 'new-40-49', 'new-50-59',\n       'new-60-69', 'new-70-79', 'new-80-89', 'new_confirmed', 'new_deceased',\n       'new_hospitalized', 'new_intensive_care'}]\ndf = df.reindex(sorted(df.columns), axis=1)\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar que, cuanto mayor es la edad mayor es la correlación con la cnatidad de casos hospitalizados, en tratmientos intensivos o fallecidos.","metadata":{}},{"cell_type":"code","source":"df = data[{'average_temperature', 'relative_humidity', 'new_confirmed', 'new_deceased',\n       'new_hospitalized', 'new_intensive_care'}]\ndf = df.reindex(sorted(df.columns), axis=1)\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No pasa lo mismo con la columnas que nos dicen la media de temperatura y la humedad en cada uno de esos días.","metadata":{}},{"cell_type":"markdown","source":"## Regresión","metadata":{}},{"cell_type":"markdown","source":"Se han aplicado un total de 4 algoritmos de regresión. Estos algoritmos son: \n\n* Linear Regression\n* Ridge\n* Lasso\n* Random Forest Regression.\n* Deci\n\nSobre todos estos métodos se van a ir guardando métricas sobre los datos de test y entrenamiento. Se han utilizado como métricas:\n* MSE\n* RMSE\n* R2\n\nAntes de nada, vamos a importar todas los paquetes y funciones necesarias para este análisis. También, se dividirán los datos en el conjunto de entrenamiento y test.","metadata":{}},{"cell_type":"code","source":"# IMPORTS\n\n# LinearRegression\nfrom sklearn.linear_model import LinearRegression\n# Linear SVR\nfrom sklearn.svm import LinearSVR\n# To apply crossvalidation\nfrom sklearn.model_selection import KFold\n# Para visualizar las tablas con los resultados\nfrom IPython.display import display, HTML\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copia auxiliar y labels\n\n# Solo utilizamos aquellos nuevos confirmados que esten entre 40 y 89 años\naux = data[{'new_hospitalized','new-40-49','new-50-59','new-60-69','new-70-79','new-80-89','average_temperature', 'relative_humidity'}].copy()\nX = aux.reset_index(drop=True)\n\n# Labels\ny = aux['new_hospitalized']\nX.drop(['new_hospitalized'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dividimos nuestro datos en datos de entrenamiento y de test. Para ello utilizamos la funcion `train_test_split` de sklearn.model_selection","metadata":{}},{"cell_type":"code","source":"# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### **Linear Regression**","metadata":{}},{"cell_type":"markdown","source":"Este es un método que funciona muy bien para casos donde hay una relación lineal muy fuerte entre las variables.","metadata":{}},{"cell_type":"code","source":"# Linear Regression\nlin_reg_mod = LinearRegression()\nlin_reg_mod.fit(X_train, y_train)\n\npred_train = lin_reg_mod.predict(X_train)\npred_test  = lin_reg_mod.predict(X_test)\n\ntrain_mse  = mean_squared_error(y_train, pred_train)\ntest_mse   = mean_squared_error(y_test, pred_test)\ntrain_rmse = (np.sqrt(mean_squared_error(y_train, pred_train)))\ntrain_r2   = r2_score(y_train, pred_train)\ntest_rmse  = (np.sqrt(mean_squared_error(y_test, pred_test)))\ntest_r2    = r2_score(y_test, pred_test)\n\n# Creamos un dataframe donde iremos guardando los resultados.\nresults = pd.DataFrame(columns = ['MSE Train', 'MSE Test', 'RMSE Train', 'RMSE Test', 'R_Squared Train', 'R_Squared Test'])\nresults.loc[len(results)] =  [train_mse,test_mse,train_rmse,test_rmse,train_r2,test_r2]\nresults.index = ['LinearRegression']\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se irán mostrando los resultados pero se comentarán al final de este apartado.","metadata":{}},{"cell_type":"markdown","source":"> ## **Ridge Regression**","metadata":{}},{"cell_type":"markdown","source":"La regresion ridge es una extension de la regresión lineal donde la funcion de perdida es modificada para minimizar la complejidad del modelo.\n\n$\nLoss\\_function = OLS(Ordinary\\_least\\_squares) + alpha * \\sum{coefficient\\_value^{2}_{i}}\n$\n\nDonde $i$ son cada uno de los coeficientes.\n\nAlpha es el parametro que deberemos seleccionar. Un valor de alpha pequeño tenderá a generar overfitting, mientras que uno más grande tenderá a crear underfitting.\n\nPara seleccionar un valor adecuado de lambda, se an creado 200 valores que van desde $5*10^{09}$ a $5*10^{-03}$.","metadata":{}},{"cell_type":"code","source":"n_alphas = 200\nalphas = np.logspace(-10, 4, n_alphas)\nprint('Max: ', max(alphas))\nprint('Min: ', min(alphas))\nprint('Length: ', len(alphas))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize = True, se divide el conjunto de datos por su L2 normal.\nridge = Ridge(normalize = True)\ncoefs = []\n\nfor a in alphas:\n    ridge.set_params(alpha = a)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`coef` es un array donde se han ido guardando los coeficientes para cada una de las variables predictoras para los diferentes valores de alpha.","metadata":{}},{"cell_type":"code","source":"ax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.title('Coeficientes del modelo en función de la regularización')\nplt.xlabel('alpha')\nplt.ylabel('coef')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se puede ver como, a medida que aumenta el valor de lambda la regularizacion es mayor y el valor de los coeficientes se reduce.","metadata":{}},{"cell_type":"markdown","source":"Para no escojer un valor arbitrario de alpha, se va a realizar validación cruzada para escojer aquel parámetro alpha que obtenga los mejores resultados. En python esto se puede hacer con la función RidgeCV().","metadata":{}},{"cell_type":"code","source":"# RidgeCV\nridgecv = RidgeCV(alphas = alphas, scoring ='neg_mean_squared_error', normalize=True)\nridgecv.fit(X_train, y_train)\nridgecv.alpha_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge = Ridge(alpha = ridgecv.alpha_, normalize=True)\nridge.fit(X_train, y_train)\n\npred_train = ridge.predict(X_train)\npred_test  = ridge.predict(X_test)\n\ntrain_mse  = mean_squared_error(y_train, pred_train)\ntest_mse   = mean_squared_error(y_test, pred_test)\ntrain_rmse = (np.sqrt(mean_squared_error(y_train, pred_train)))\ntrain_r2   = r2_score(y_train, pred_train)\ntest_rmse  = (np.sqrt(mean_squared_error(y_test, pred_test)))\ntest_r2    = r2_score(y_test, pred_test)\n\nresults.loc['RidgeRegression'] = [train_mse,test_mse,train_rmse,test_rmse,train_r2,test_r2]\n\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dentro del dataframe `coeficientes` encontraremos los valores de los coeficientes para la regresión Ridge y Lasso.","metadata":{}},{"cell_type":"code","source":"coeficientes = pd.DataFrame({'Ridge':ridge.coef_}, index=X.columns)\ncoeficientes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## **Lasso**","metadata":{}},{"cell_type":"code","source":"lasso = Lasso(max_iter = 10000, normalize = True)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train, y_train)\n    coefs.append(lasso.coef_)\n    \nax = plt.gca()\nax.plot(alphas*2, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lo mismo para Lasso. Para no escojer un valor arbitrario de alpha he realizado cross validation.","metadata":{}},{"cell_type":"code","source":"lassocv = LassoCV(alphas = None, cv = 10, max_iter = 10000, normalize = True)\nlassocv.fit(X_train, y_train)\n\nlasso.set_params(alpha=lassocv.alpha_)\nlasso.fit(X_train, y_train)\n\n\npred_train = lasso.predict(X_train)\npred_test  = lasso.predict(X_test)\n\ntrain_mse  = mean_squared_error(y_train, pred_train)\ntest_mse   = mean_squared_error(y_test, pred_test)\ntrain_rmse = (np.sqrt(mean_squared_error(y_train, pred_train)))\ntrain_r2   = r2_score(y_train, pred_train)\ntest_rmse  = (np.sqrt(mean_squared_error(y_test, pred_test)))\ntest_r2    = r2_score(y_test, pred_test)\n\nresults.loc['LassoRegression'] = [train_mse,test_mse,train_rmse,test_rmse,train_r2,test_r2]\n\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coeficientes['Lasso'] = lasso.coef_\ncoeficientes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" El indice positivo en la variable new-70-79 es significativo ya que es el que más influencia está teniendo con la variable a predecir. En otras palabras, cuando más aumenten lso casos entre las edades de 70 y 79 más casos de hospitalizados habrá en los hospitales de la Comunidad Valenciana.","metadata":{}},{"cell_type":"markdown","source":"> ## **Random Forest**","metadata":{}},{"cell_type":"code","source":"#RF model\nmodel_rf = RandomForestRegressor(n_estimators=1000, random_state=100)\nmodel_rf.fit(X_train, y_train) \n\npred_train = model_rf.predict(X_train)\npred_test  = model_rf.predict(X_test)\n\ntrain_mse  = mean_squared_error(y_train, pred_train)\ntest_mse   = mean_squared_error(y_test, pred_test)\ntrain_rmse = (np.sqrt(mean_squared_error(y_train, pred_train)))\ntrain_r2   = r2_score(y_train, pred_train)\ntest_rmse  = (np.sqrt(mean_squared_error(y_test, pred_test)))\ntest_r2    = r2_score(y_test, pred_test)\n\nresults.loc['RandomForest'] = [train_mse,test_mse,train_rmse,test_rmse,train_r2,test_r2]\n\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## **Decision Tree Regressor**","metadata":{}},{"cell_type":"code","source":"dtree = DecisionTreeRegressor(criterion='mse', max_depth=3, random_state=3, splitter='best')\ndtree.fit(X_train, y_train)\n\npred_train = dtree.predict(X_train)\npred_test  = dtree.predict(X_test)\n\ntrain_mse  = mean_squared_error(y_train, pred_train)\ntest_mse   = mean_squared_error(y_test, pred_test)\ntrain_rmse = (np.sqrt(mean_squared_error(y_train, pred_train)))\ntrain_r2   = r2_score(y_train, pred_train)\ntest_rmse  = (np.sqrt(mean_squared_error(y_test, pred_test)))\ntest_r2    = r2_score(y_test, pred_test)\n\nresults.loc['DecisionTree'] = [train_mse,test_mse,train_rmse,test_rmse,train_r2,test_r2]\n\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En la siguiente tabla podemos ver las diferentes métricas obtenidas con los diferentes algoritmos de regresión.","metadata":{}},{"cell_type":"code","source":"ax = results[{'MSE Train', 'MSE Test'}].plot(kind='bar', rot=45,\n             figsize=[8,8], style='seaborn-colorblind')\nfor p in ax.patches:\n    ax.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()/2., p.get_height()),\n                ha='center',va='center',   \n                xytext=(0, 7),textcoords='offset points')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Comparación","metadata":{}},{"cell_type":"markdown","source":"Podemos observar como el modelo obtenido con Random Forest es el que obitene mejores resultados tanto sobre el conjunto de entramiendo como sobre el conjunto de test. Random Forest sería el modelo escogido si tuvieramos que predecir el número de casos hospitalarias con mejor exactitud.","metadata":{}},{"cell_type":"code","source":"ax = results[{'R_Squared Train', 'R_Squared Test'}].plot(kind='bar', rot=45,\n             figsize=[8,8], style='seaborn-colorblind')\nfor p in ax.patches:\n    ax.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()/2., p.get_height()),\n                ha='center',va='center',   \n                xytext=(0, 7),textcoords='offset points')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los modelos que obtengan un mayor valor de $R^2$ indicarán que predicen con mejores resultados.","metadata":{}},{"cell_type":"markdown","source":"> ## **Clasificación**","metadata":{}},{"cell_type":"markdown","source":"En este apartado de la Parte 1, se hará una clasificación utilizando como variables: new_confirmed, new_deceased, new_intensive_care, new_hospitalized. Y como etiqueta la variable school_closing. school_closing dispone de 4 labels diferentes (0,1,2,3). Cuanto mayor sea el número mayor es la restriccion aplicada a los colegios.","metadata":{}},{"cell_type":"code","source":"# IMPORTS\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Import the classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc,confusion_matrix,classification_report\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import train_test_split\n\nfrom itertools import cycle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copia auxiliar y labels\n\n# Solo utilizamos aquellos nuevos confirmados que esten entre 40 y 89 años\naux = data[{'new_confirmed', 'new_deceased', 'new_intensive_care', 'new_hospitalized','school_closing'}].copy()\nX = aux.reset_index(drop=True)\n\n# Labels\ny = aux['school_closing']\nX.drop(['school_closing'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con el argumento `stratify` dentro de la función `train_test_split` nos aseguramos que las clases esten lo más balanceadas posible. Además, vamos a obtener tres conjuntos. Uno para el entrenamiento, otro para los test y otro para validación.","metadata":{}},{"cell_type":"code","source":"#y_bin = label_binarize(y, classes=[0, 1, 2, 3])\n\n# Splitting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)\nX_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=.25, random_state=42, stratify=y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('=== Tamaños de los diferentes conjuntos de datos ===')\nprint(\"Datos de entrenamiento: \", len(X_train))\nprint(\"Datos de test: \", len(X_test))\nprint(\"Datos de validación: \", len(X_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En nuestro caso, podemos ver como RandomForest y KNeighbors son los clasificadores que obtienen mejores resultados en las diferentes clases. Mietras que, Gaussian Naive Bayes es el clasificador que peores resultados obtiene.\n\nSobre los datos aplicares los métodos de KNeighbors Classifier, Decision Tree Classifier y Random Forest Classifier","metadata":{}},{"cell_type":"markdown","source":"> ### **KNeighbors Classifier**","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nprint(\"Accuracy sobre los datos de test: \", knn.score(X_test,y_test))\nprint(\"Accuracy sobre los datos de validación: \", knn.score(X_val,y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se han otenido buenos resultados de accuracy sobre los conjuntos de test y validación.","metadata":{}},{"cell_type":"code","source":"pred = knn.predict(X_test)\n\ncm = confusion_matrix(y_test, pred) \ndf_cm = pd.DataFrame(cm, range(4), range(4))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, linewidths=.5) # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nmatrix = classification_report(y_test, pred)\nprint('Summary : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = knn.predict(X_val)\n\ncm = confusion_matrix(y_val, pred) \ndf_cm = pd.DataFrame(cm, range(4), range(4))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, linewidths=.5) # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nmatrix = classification_report(y_val, pred)\nprint('Summary : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar como el F1-valor obtenido es muy bueno para las cuatro clases.","metadata":{}},{"cell_type":"markdown","source":"> ## **Decision Tree Classifier**","metadata":{}},{"cell_type":"code","source":"dtree = DecisionTreeClassifier(criterion='entropy',max_depth=3, random_state=42)\ndtree.fit(X_train, y_train)\n\nprint(\"Accuracy sobre los datos de test: \", dtree.score(X_test,y_test))\nprint(\"Accuracy sobre los datos de validación: \", dtree.score(X_val,y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = dtree.predict(X_val)\n\ncm = confusion_matrix(y_val, pred) \ndf_cm = pd.DataFrame(cm, range(4), range(4))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, linewidths=.5) # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nmatrix = classification_report(y_val, pred)\nprint('Summary : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso, el F1-valor es mas bajo, podemos observar como para las clases 2 y 3 se otiene una precision mas baja. Además, los valores de acccuracy son peores con respecto al modelo anterior.","metadata":{}},{"cell_type":"markdown","source":"> ## **Random Forest Classifier**","metadata":{}},{"cell_type":"code","source":"rforest = RandomForestClassifier(random_state=42)\nrforest.fit(X_train, y_train)\n\nprint(\"Accuracy sobre los datos de test: \", rforest.score(X_test,y_test))\nprint(\"Accuracy sobre los datos de validación: \", rforest.score(X_val,y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = rforest.predict(X_val)\n\ncm = confusion_matrix(y_val, pred) \ndf_cm = pd.DataFrame(cm, range(4), range(4))\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, linewidths=.5) # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nmatrix = classification_report(y_val, pred)\nprint('Summary : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest ha sido el metodo que ha obtenido mejores resultados sobre el conjunto de test y validación.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Parte2\"></a>\n# Parte 2. Ensembles, clustering y reglas de asociación","metadata":{}},{"cell_type":"markdown","source":"En este apartado de la práctica se van aplicar técnicas de Boosting sobre los datos. En este caso se han escogido como métodos:\n\n* AdaBoost\n* GradientBoosting\n\nSobre estos métodos se aplicará GridSearch para obtener el mejor conjunto de parámetros. Sobre el modelo generado con lo mejores parámetros se comprobara su accuracy sobre un conjunto de datos de test y de validación.","metadata":{}},{"cell_type":"markdown","source":"### **Ensembles**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para esta parte vamos a utilizar la siguientes columnas. En este caso, solo vamos a tener en cuenta dos posibles casos. Los días donde no habia ningun tipo de restriccion en los colegios (label = 0 - 'low'), y los casos donde los colegios estaban cerrados debido a las restricciones (label = 1 - 'high').","metadata":{}},{"cell_type":"code","source":"# Copia auxiliar y labels\n\n# Solo utilizamos aquellos nuevos confirmados que esten entre 40 y 89 años\naux = data[{'new-70-79', 'new_confirmed', 'new_deceased', 'new_intensive_care', 'new_hospitalized','school_closing'}].copy()\nX = aux.reset_index(drop=True)\n\n# Labels\ny = aux['school_closing']\nX.drop(['school_closing'], axis=1, inplace=True)\n\n# Si quisieramos binarizar el problema por ejemplo en casos donde esta abierto o cerrado realizariamos lo siguiente:\n#aux['risk_label'] = aux.school_closing.apply(lambda q: 'low' if q <= 1 else 'medium' if q <= 2 else 'high')\n#aux['risk_binary'] = aux.school_closing.apply(lambda q: -1 if q <= 1 else 0 if q <= 2 else 1)\n#risk_binary = aux[(aux.risk_binary == 1) | (aux.risk_binary == -1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cantidad de instancias por cada una de las clases.\ny.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Al igual que en la Parte 1, en esta parte se dividirán los datos en tres conjuntos (entrenamiento, test y validación). Se hará uso del argumento `stratify`, el cual nos permite asegurarnos que los diferentes conjuntos de datos esten lo mejor balanceados posible.","metadata":{}},{"cell_type":"code","source":"# En el caso de querer binarizar el problema\n# X=risk_binary[[\"new_intensive_care\", \"new_hospitalized\", 'new-70-79', 'new_deceased']]\n# y=risk_binary['risk_binary'].to_numpy()\n\n# Splitting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)\nX_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=.25, random_state=42, stratify=y_train)\n\n# En el caso de que quisieramos escalar los datos. En nuestro caso no es necesario\n# sc = StandardScaler()\n# sc.fit(X_train)\n# X_train_scaled = pd.DataFrame(sc.transform(X_train),columns = X_train.columns)\n# sc.fit(X_test)\n# X_test_scaled = pd.DataFrame(sc.transform(X_test),columns = X_test.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AdaBoostClassifier()\n# Definimos el diccionario que contendrá los parámetros a probar.\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100, 500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n# Definimos el procedimiento de evaluación\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# Definimos Grid Search\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n# Ejecutamos Grid Search\ngrid_result = grid_search.fit(X_train, y_train)\n# Mostramos el mejor score y configuración\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# Mostramos todos los scores y configuracion\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a crear nuestro modelo con el mejor obtenido despues de aplicar Grid Seach sobre los diferentes parámetros escojidos.","metadata":{}},{"cell_type":"code","source":"adaboost = AdaBoostClassifier(n_estimators=500, learning_rate=0.0001).fit(X_train, y_train)\n\nprint(\"\\t\\t ========= Results ==========\")\nprint(\"AdaBoost Classifier Model Accuracy with Train Data:\", adaboost.score(X_train, y_train))\nprint(\"AdaBoost Classifier Model Accuracy with Test Data:\", adaboost.score(X_test, y_test))\nprint(\"AdaBoost Classifier Model Accuracy with Validation Data:\", adaboost.score(X_val, y_val))\nprint(\"\\t\\t ============================\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GradientBoostingClassifier()\n# Definimos el diccionario que contendrá los parámetros a probar.\ngrid = dict()\ngrid['n_estimators'] = [10, 50, 100, 500]\ngrid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n# Definimos el procedimiento de evaluación\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# Definimos Grid Search\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n# Ejecutamos Grid Search\ngrid_result = grid_search.fit(X_train, y_train)\n# Mostramos el mejor score y configuración\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# Mostramos todos los scores y configuracion\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0).fit(X_train, y_train)\n\nprint(\"\\t\\t ========= Results ==========\")\nprint(\"Gradient Boosting Classifier Model Accuracy with Train Data:\", gb.score(X_train, y_train))\nprint(\"Gradient Boosting Classifier Model Accuracy with Test Data:\", gb.score(X_test, y_test))\nprint(\"Gradient Boosting Classifier Model Accuracy with Validation Data:\", gb.score(X_val, y_val))\nprint(\"\\t\\t ============================\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Conclusiones\n\nPodemos observar como Gradient Boosting Classifier obtiene mejores resultados en los tres conjunto de datos (train,test y val). El hecho de utilizar GridSearch supone que se cree un modelo mucho mejor. Podmeos ver como con otros parámetros llegamos a obtener tan solo un 0.37 de accuracy en el modelo. ","metadata":{}},{"cell_type":"markdown","source":"## **Clustering**","metadata":{}},{"cell_type":"markdown","source":"> ### **K-Means**","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\naux = data[{'new_confirmed','new_hospitalized'}].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\n\n# run kmeans with many different k\ndistortions = []\nK = range(2, 30)\nfor k in K:\n    k_means = KMeans(n_clusters=k, random_state=42).fit(aux)\n    k_means.fit(aux)\n    distortions.append(sum(np.min(cdist(aux, k_means.cluster_centers_, 'euclidean'), axis=1)) / aux.shape[0])\n    #print('Found distortion for {} clusters'.format(k))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_line = [K[0], K[-1]]\nY_line = [distortions[0], distortions[-1]]\n\n# Plot the elbow\nplt.figure(figsize=(20, 5))\nplt.plot(K, distortions, 'b-')\nplt.plot(X_line, Y_line, 'r')\nplt.xticks(K)\nplt.tick_params(axis='x', which='major', labelsize=8)\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('K óptimo con el método del codo')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar como los mejores valores de K están entre 13 y 3. He escogido como valor de K: 6","metadata":{}},{"cell_type":"code","source":"k = 6\nkmeans = KMeans(n_clusters = k, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(aux)\ny_kmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aux.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = aux\ndf['clusters'] = y_kmeans\nsns.lmplot(x='new_confirmed', y='new_hospitalized', \n           data=df, \n           fit_reg=False, \n           hue=\"clusters\",  \n           scatter_kws={\"marker\": \"D\", \"s\": 100})\n\nplt.title('Nuevos casos confirmados vs. nuevos casos hospitalizados')\nplt.ylabel('Nuevos casos hospitalizados')\nplt.xlabel('Nuevos casos confirmados')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos observar que:\n\n* La clase 0 indica un grado medio de casos confirmados y hospitalizados.\n* La clase 1 indica que hay pocos casos confirmados pero puede darse que haya una gran cnatidad de hospitalizados.\n* La clase 2 indica que hay muchos casos confirmados y un nivel alto tambien de casos hospitalizados.\n* La clase 3 define aquellos dias donde no hay o hay muy pocos casos confirmados y hispitalizados.\n* La clase 4 muestra un grado medio-bajo sobre la cantidad de casos confirmados y hospitalizados.\n* La clase 4 muestra un grado medio-alto sobre la cantidad de casos confirmados y hospitalizados.","metadata":{}},{"cell_type":"markdown","source":"> ### Nuevos datos","metadata":{}},{"cell_type":"code","source":"X_new = pd.DataFrame([[1200, 150]], columns = ['new_confirmed', 'new_hospitalized'])\nnew_labels = kmeans.predict(X_new)\n\nprint(\"Cluster asignado: \", new_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = aux\ndf['clusters'] = y_kmeans\nsns.lmplot(x='new_confirmed', y='new_hospitalized', \n           data=df, \n           fit_reg=False, \n           hue=\"clusters\",  \n           scatter_kws={\"marker\": \"D\", \"s\": 100})\n\nplt.title('Nuevos casos confirmados vs. nuevos casos hospitalizados')\nplt.ylabel('Nuevos casos hospitalizados')\nplt.xlabel('Nuevos casos confirmados')\nplt.plot(X_new.new_confirmed,X_new.new_hospitalized, '*', color = 'lime', markersize = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver como un nuevo dato lo clasifica correctamente.","metadata":{}}]}