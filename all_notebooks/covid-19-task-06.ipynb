{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19: Open Research Dataset Challenge (CORD-19)\n# Task 06: What do we know about non-pharmaceutical interventions?\n\n\n### Purpose:\n\n- Our purpose is to extract relevant text sections related to the proposed questions from all the research dataset. \n\n### Description of the methodology steps:\n\n- Upload and structure all researches paper's sections separately.\n- Process the sections' text using the nltk package (lowercase, remove punctuations, tokenization, Lemmatization).\n- To search for the relevant results about each task:   \n    - Select the keywords and their synonyms.   \n    - Search in the dataset those keywords (or most of them) on the lemmatized tokens of the sections.\n- While the obtained results are very large, a ranking of the resulted papers' sections is recommended using both: the freshness degree and the rank score of the papers' institutions, according to the following Formula: \n    \n    ***Score = Affiliation Score * 0.2 + Publication Year * 2020***\n    \n- The obtained results now are ranked, and ready to get used!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Meta data :\nmeta = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Charge all .Json files:\nimport glob\npapers = glob.glob(f'/kaggle/input/CORD-19-research-challenge/**/*.json', recursive=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(papers)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# import json\n# papers_data = pd.DataFrame(columns=['PaperID','Title','Section','Text','Affilations'], index=range(len(papers)*50))\n\n# # Remove duplicates in a list:\n# def my_function(x):\n#     return list(dict.fromkeys(x))\n\n# i=0\n# for j in range(len(papers)):\n#     with open(papers[j]) as file:\n#         content = json.load(file)\n        \n#         # ID and Title:\n#         pap_id = content['paper_id']\n#         title =  content['metadata']['title']\n        \n#         # Affiations:\n#         affiliation = []\n#         for sec in content['metadata']['authors']:\n#             try:\n#                 affiliation.append(sec['affiliation']['institution'])\n#             except:\n#                 pass\n#         affiliation = my_function(affiliation)\n        \n#         # Abstract\n#         for sec in content['abstract']:\n#             papers_data.iloc[i, 0] = pap_id\n#             papers_data.iloc[i, 1] = title\n#             papers_data.iloc[i, 2] = sec['section']\n#             papers_data.iloc[i, 3] = sec['text']\n#             papers_data.iloc[i, 4] = affiliation\n#             i = i + 1\n            \n#         # Body text\n#         for sec in content['body_text']:\n#             papers_data.iloc[i, 0] = pap_id\n#             papers_data.iloc[i, 1] = title\n#             papers_data.iloc[i, 2] = sec['section']\n#             papers_data.iloc[i, 3] = sec['text']\n#             papers_data.iloc[i, 4] = affiliation\n#             i = i + 1\n\n# papers_data.dropna(inplace=True)\n# papers_data = papers_data.astype(str).drop_duplicates() \n\n# # Text processing:\n# import nltk\n# nltk.download('punkt')\n# # Lowercase:\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 1] = papers_data.iloc[i, 1].lower()\n#         papers_data.iloc[i, 2] = papers_data.iloc[i, 2].lower()\n#         papers_data.iloc[i, 3] = papers_data.iloc[i, 3].lower()\n#         papers_data.iloc[i, 4] = papers_data.iloc[i, 4].lower()\n#     except:\n#         pass\n    \n# # Tokenization:\n\n# from nltk.tokenize import word_tokenize, sent_tokenize , RegexpTokenizer\n\n# tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation\n# papers_data[\"Title_Tokens_words\"] = [list() for x in range(len(papers_data.index))]\n# papers_data[\"Text_Tokens_words\"] = [list() for x in range(len(papers_data.index))]\n\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 5] = tokenizer.tokenize(papers_data.iloc[i, 1])\n#         papers_data.iloc[i, 6] = tokenizer.tokenize(papers_data.iloc[i, 3])\n#     except:\n#         pass\n    \n# # Remove stopwords:\n# nltk.download('stopwords')\n# from nltk.corpus import stopwords\n# stop_words = set(stopwords.words('english')) \n\n# for i in range(len(papers_data)):\n#     try:\n#         papers_data.iloc[i, 5] = [w for w in papers_data.iloc[i, 5] if not w in stop_words] \n#         papers_data.iloc[i, 6] = [w for w in papers_data.iloc[i, 6] if not w in stop_words]\n#     except:\n#         pass\n    \n# # Words count:  \n# papers_data[\"Words_count\"] = 0\n\n# # for i in range(len(papers_data)):\n# #     try:\n# #         papers_data.iloc[i, 7] = len(papers_data.iloc[i, 6])\n# #     except:\n# #         pass\n    \n# # Lemmatization :\n# nltk.download('wordnet')\n\n# from nltk.stem import WordNetLemmatizer\n\n# wordnet_lemmatizer = WordNetLemmatizer()\n\n# papers_data[\"Text_Lem_words\"] = [list() for x in range(len(papers_data))]\n\n# for i in range(len(papers_data)):\n#     for j in range(len(papers_data.iloc[i, 6])):\n#         papers_data.iloc[i, 8].append(wordnet_lemmatizer.lemmatize(papers_data.iloc[i, 6][j]))\n        \n# papers_data[\"Title_Lem_words\"] = [list() for x in range(len(papers_data))]\n\n# for i in range(len(papers_data)):\n#     for j in range(len(papers_data.iloc[i, 5])):\n#         papers_data.iloc[i, 9].append(wordnet_lemmatizer.lemmatize(papers_data.iloc[i, 5][j]))\n        \n# papers_data.to_csv(\"/kaggle/input/processed-researches-data/papers_data_final.csv\")\nprint(\"Preprocessing done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_data = pd.read_csv(\"/kaggle/input/processed-researches-data/papers_data_final.csv\")\ndel papers_data['Unnamed: 0']\nimport ast\npapers_data['Affilations'] = papers_data['Affilations'].apply(lambda x: ast.literal_eval(x))\npapers_data['Text_Lem_words'] = papers_data['Text_Lem_words'].apply(lambda x: ast.literal_eval(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Search for papers sections containing task's keywords:"},{"metadata":{},"cell_type":"markdown","source":"### Task 06 : What do we know about diagnostics and surveillance?\n\n#### Task Details\n\n- What do we know about diagnostics and surveillance? What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\n\n- Specifically, we want to know what the literature reports about:\n\n    - How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\n    - Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\n    - Recruitment, support, and coordination of local expertise and capacity (public, privateâ€”commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\n    - National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\n    - Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\n    - Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\n    - Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\n    - Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance/detection schemes.\n    - Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\n    - Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\n    - Policies and protocols for screening and testing.\n    - Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\n    - Technology roadmap for diagnostics.\n    - Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\n    - New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\n    - Coupling genomics and diagnostic testing on a large scale.\n    - Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\n    - Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\n    - One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors."},{"metadata":{},"cell_type":"markdown","source":"### Task 6.1: ***How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nwordnet_lemmatizer = WordNetLemmatizer()\ndef my_function(x):\n    return list(dict.fromkeys(x))\n\n\nkeywords =['widespread','exposure', 'policy', 'recommendation','mitigation','measures','denominator','mechanism','test','sharing','demographic','asymptomatic','disease','serosurvey','convalescent','detection','screening','neutralizing','antibodies','elisas']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# At least 11 of words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_1 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_1.append(i)\n    \nlen(task6_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.1 :\nfor i in task6_1:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by most Recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 6.1:\nprint(task6_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_1_rank = papers_data.iloc[task6_1, :]\ntask6_1_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\nmeta = meta.rename(columns={\"title\": \"Title\"})\nfor i in range(len(meta)):\n    meta.iloc[i, 2] = str(meta.iloc[i, 2]).lower()\nmeta['Title'] = meta['Title'].astype(str) \ntask6_1_rank['Title'] = task6_1_rank['Title'].astype(str) \n\ntask6_1_rank = pd.merge(task6_1_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_1_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_1_rank['publish_time'] = task6_1_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask6_1_rank['publish_time'] = task6_1_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_1_rank['publish_time'] = pd.to_numeric(task6_1_rank['publish_time'])\ntask6_1_rank = task6_1_rank.sort_values(by='publish_time', ascending=False)\ntask6_1_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations Scores:\n\n- Ranking using the : http://www.shanghairanking.com/arwu2019.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"rank = pd.read_csv(\"/kaggle/input/shanghai-ranking/rank-univ.csv\")\ndel rank['Unnamed: 0']\nrank.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_1_rank['Aff_Score'] = 0\nfor i in range(len(task6_1_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_1_rank.iloc[i, 4]:\n            task6_1_rank.iloc[i, 11] = rank.iloc[j, 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_1_rank[\"Ranking_Score\"] = task6_1_rank[\"publish_time\"]*0.8 + task6_1_rank[\"Aff_Score\"]*0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_1_rank = task6_1_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_1_rank.reset_index(inplace=True,drop=True)\ntask6_1_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.1 :\n\nfor i in range(len(task6_1_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_1_rank.iloc[i, 0])\n    print(\"Title: \", task6_1_rank.iloc[i, 1])\n    print(\"Section: \", task6_1_rank.iloc[i, 2])\n    print(\"Text: \", task6_1_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.2: ***Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['efforts','capacity', 'diagnostic', 'platforms','surveillance','covid19','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask6_2 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_2.append(i)\n    \nlen(task6_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.2 :\nfor i in task6_2:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by most Recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 6.2:\nprint(task6_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_2_rank = papers_data.iloc[task6_2, :]\ntask6_2_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\n\ntask6_2_rank['Title'] = task6_2_rank['Title'].astype(str) \n\ntask6_2_rank = pd.merge(task6_2_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_2_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_2_rank['publish_time'] = task6_2_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_2_rank['publish_time'] = pd.to_numeric(task6_2_rank['publish_time'])\ntask6_2_rank = task6_2_rank.sort_values(by='publish_time', ascending=False)\ntask6_2_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations Scores:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_2_rank['Aff_Score'] = 0\nfor i in range(len(task6_2_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_2_rank.iloc[i, 4]:\n            task6_2_rank.iloc[i, 11] = rank.iloc[j, 3]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"task6_2_rank[\"Ranking_Score\"] = task6_2_rank[\"publish_time\"]*0.8 + task6_2_rank[\"Aff_Score\"]*0.2\ntask6_2_rank = task6_2_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_2_rank.reset_index(inplace=True,drop=True)\ntask6_2_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.2 :\n\nfor i in range(len(task6_2_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_2_rank.iloc[i, 0])\n    print(\"Title: \", task6_2_rank.iloc[i, 1])\n    print(\"Section: \", task6_2_rank.iloc[i, 2])\n    print(\"Text: \", task6_2_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.3: ***Recruitment, support, and coordination of local expertise and capacity (public, privateâ€”commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['recruitment', 'support','coordination','local','expertise','capacity','public','private','commercial','non-profit','academic','legal','ethical','communication','operational','issues']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 12 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_3 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_3.append(i)\n    \nlen(task6_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.3 :\nfor i in task6_3:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 6.3:\nprint(task6_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_3_rank = papers_data.iloc[task6_3, :]\ntask6_3_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\ntask6_3_rank['Title'] = task6_3_rank['Title'].astype(str) \n\ntask6_3_rank = pd.merge(task6_3_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_3_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_3_rank['publish_time'] = task6_3_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_3_rank['publish_time'] = pd.to_numeric(task6_3_rank['publish_time'])\ntask6_3_rank = task6_3_rank.sort_values(by='publish_time', ascending=False)\ntask6_3_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations Scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_3_rank['Aff_Score'] = 0\nfor i in range(len(task6_3_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_3_rank.iloc[i, 4]:\n            task6_3_rank.iloc[i, 11] = rank.iloc[j, 3]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"task6_3_rank[\"Ranking_Score\"] = task6_3_rank[\"publish_time\"]*0.8 + task6_3_rank[\"Aff_Score\"]*0.2\ntask6_3_rank = task6_3_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_3_rank.reset_index(inplace=True,drop=True)\ntask6_3_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.3 :\n\nfor i in range(len(task6_3_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_3_rank.iloc[i, 0])\n    print(\"Title: \", task6_3_rank.iloc[i, 1])\n    print(\"Section: \", task6_3_rank.iloc[i, 2])\n    print(\"Text: \", task6_3_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.4: ***National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['national', 'guidance','guidelines','practices','states','universities','private','laboratories','test','public','health','officials','public','covid19','covid-19','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 12 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_4 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_4.append(i)\n    \nlen(task6_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.4 :\nfor i in task6_4:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by teh most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 6.4:\nprint(task6_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_4_rank = papers_data.iloc[task6_4, :]\ntask6_4_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\n\ntask6_4_rank['Title'] = task6_4_rank['Title'].astype(str) \n\ntask6_4_rank = pd.merge(task6_4_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_4_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_4_rank['publish_time'] = task6_4_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask6_4_rank['publish_time'] = task6_4_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_4_rank['publish_time'] = pd.to_numeric(task6_4_rank['publish_time'])\ntask6_4_rank = task6_4_rank.sort_values(by='publish_time', ascending=False)\ntask6_4_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations score:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_4_rank['Aff_Score'] = 0\nfor i in range(len(task6_4_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_4_rank.iloc[i, 4]:\n            task6_4_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_4_rank[\"Ranking_Score\"] = task6_4_rank[\"publish_time\"]*0.8 + task6_4_rank[\"Aff_Score\"]*0.2\ntask6_4_rank = task6_4_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_4_rank.reset_index(inplace=True,drop=True)\ntask6_4_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.4 :\n\nfor i in range(len(task6_4_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_4_rank.iloc[i, 0])\n    print(\"Title: \", task6_4_rank.iloc[i, 1])\n    print(\"Section: \", task6_4_rank.iloc[i, 2])\n    print(\"Text: \", task6_4_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.5: ***Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['development', 'point-of-care','covid19','covid-19','test','influenza','bed-side','recognizing','dischtradeoffs','speed','accessibility','accuracy']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 6: \n        return(True)  \n    return(False)    \n  \ntask6_5 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_5.append(i)\n    \nlen(task6_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.5 :\nfor i in task6_5:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 6.5 \nprint(task6_5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_5_rank = papers_data.iloc[task6_5, :]\ntask6_5_rank.reset_index(inplace=True,drop=True)\n\n# Grab the publish year from the meta data file:\n\ntask6_5_rank['Title'] = task6_5_rank['Title'].astype(str) \n\ntask6_5_rank = pd.merge(task6_5_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_5_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_5_rank['publish_time'] = task6_5_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_5_rank['publish_time'] = pd.to_numeric(task6_5_rank['publish_time'])\ntask6_5_rank = task6_5_rank.sort_values(by='publish_time', ascending=False)\ntask6_5_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affilitions Scores:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_5_rank['Aff_Score'] = 0\nfor i in range(len(task6_5_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_5_rank.iloc[i, 4]:\n            task6_5_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_5_rank[\"Ranking_Score\"] = task6_5_rank[\"publish_time\"]*0.8 + task6_5_rank[\"Aff_Score\"]*0.2\ntask6_5_rank = task6_5_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_5_rank.reset_index(inplace=True,drop=True)\ntask6_5_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.5 :\n\nfor i in range(len(task6_5_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_5_rank.iloc[i, 0])\n    print(\"Title: \", task6_5_rank.iloc[i, 1])\n    print(\"Section: \", task6_5_rank.iloc[i, 2])\n    print(\"Text: \", task6_5_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.6: ***Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['rapid', 'design','execution','target','surveillance','experiments','testers','pcr','area','report','entity','aid','longitudinal','sample','critical','impact','ad-hoc','intervention','local','recorded','covid19','covidd-19','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 16 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 15: \n        return(True)  \n    return(False)    \n  \ntask6_6 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_6.append(i)\n    \nlen(task6_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.6 :\nfor i in task6_6:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 6.6:\nprint(task6_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_6_rank = papers_data.iloc[task6_6, :]\ntask6_6_rank.reset_index(inplace=True,drop=True)\n\ntask6_6_rank['Title'] = task6_6_rank['Title'].astype(str) \n\ntask6_6_rank = pd.merge(task6_6_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_6_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_6_rank['publish_time'] = task6_6_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_6_rank['publish_time'] = pd.to_numeric(task6_6_rank['publish_time'])\ntask6_6_rank = task6_6_rank.sort_values(by='publish_time', ascending=False)\ntask6_6_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations Score:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_6_rank['Aff_Score'] = 0\nfor i in range(len(task6_6_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_6_rank.iloc[i, 4]:\n            task6_6_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_6_rank[\"Ranking_Score\"] = task6_6_rank[\"publish_time\"]*0.8 + task6_6_rank[\"Aff_Score\"]*0.2\ntask6_6_rank = task6_6_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_6_rank.reset_index(inplace=True,drop=True)\ntask6_6_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.6 :\n\nfor i in range(len(task6_6_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_6_rank.iloc[i, 0])\n    print(\"Title: \", task6_6_rank.iloc[i, 1])\n    print(\"Section: \", task6_6_rank.iloc[i, 2])\n    print(\"Text: \", task6_6_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.7: ***Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['separation', 'assay','virus','development','issues','instruments','role','private','sector','devices','covid19','coronavirus','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 13 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 12: \n        return(True)  \n    return(False)    \n  \ntask6_7 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_7.append(i)\n    \nlen(task6_7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.7 :\nfor i in task6_7:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_7_rank = papers_data.iloc[task6_7, :]\ntask6_7_rank.reset_index(inplace=True,drop=True)\n\ntask6_7_rank['Title'] = task6_7_rank['Title'].astype(str) \n\ntask6_7_rank = pd.merge(task6_7_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_7_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_7_rank['publish_time'] = task6_7_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_7_rank['publish_time'] = pd.to_numeric(task6_7_rank['publish_time'])\ntask6_7_rank = task6_7_rank.sort_values(by='publish_time', ascending=False)\ntask6_7_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations Scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_7_rank['Aff_Score'] = 0\nfor i in range(len(task6_7_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_7_rank.iloc[i, 4]:\n            task6_7_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_7_rank[\"Ranking_Score\"] = task6_7_rank[\"publish_time\"]*0.8 + task6_7_rank[\"Aff_Score\"]*0.2\ntask6_7_rank = task6_7_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_7_rank.reset_index(inplace=True,drop=True)\ntask6_7_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.7 :\n\nfor i in range(len(task6_7_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_7_rank.iloc[i, 0])\n    print(\"Title: \", task6_7_rank.iloc[i, 1])\n    print(\"Section: \", task6_7_rank.iloc[i, 2])\n    print(\"Text: \", task6_7_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.8: ***Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance/detection schemes.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['efforts', 'track','evolution','virus','genetic','mutation','locking','reagent','surveillance','detection','scheme','covid19','covid-19','coronavirus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 9 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 9: \n        return(True)  \n    return(False)    \n  \ntask6_8 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_8.append(i)\n    \nlen(task6_8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.8 :\nfor i in task6_8:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_8_rank = papers_data.iloc[task6_8, :]\ntask6_8_rank.reset_index(inplace=True,drop=True)\n\ntask6_8_rank['Title'] = task6_8_rank['Title'].astype(str) \n\ntask6_8_rank = pd.merge(task6_8_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_8_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_8_rank['publish_time'] = task6_8_rank['publish_time'].apply(lambda x:  str(x).replace('Winter',''))\ntask6_8_rank['publish_time'] = task6_8_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_8_rank['publish_time'] = pd.to_numeric(task6_8_rank['publish_time'])\ntask6_8_rank = task6_8_rank.sort_values(by='publish_time', ascending=False)\ntask6_8_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliation scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_8_rank['Aff_Score'] = 0\nfor i in range(len(task6_8_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_8_rank.iloc[i, 4]:\n            task6_8_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_8_rank[\"Ranking_Score\"] = task6_8_rank[\"publish_time\"]*0.8 + task6_8_rank[\"Aff_Score\"]*0.2\ntask6_8_rank = task6_8_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_8_rank.reset_index(inplace=True,drop=True)\ntask6_8_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.8 :\n\nfor i in range(len(task6_8_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_8_rank.iloc[i, 0])\n    print(\"Title: \", task6_8_rank.iloc[i, 1])\n    print(\"Section: \", task6_8_rank.iloc[i, 2])\n    print(\"Text: \", task6_8_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.9: ***Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['latency', 'issues','viral','detect','pathogen', 'needed','biological','environment','sample']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 11 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 10: \n        return(True)  \n    return(False)    \n  \ntask6_9 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_9.append(i)\n    \nlen(task6_9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.9 :\nfor i in task6_9:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_9_rank = papers_data.iloc[task6_9, :]\ntask6_9_rank.reset_index(inplace=True,drop=True)\n\ntask6_9_rank['Title'] = task6_9_rank['Title'].astype(str) \n\ntask6_9_rank = pd.merge(task6_9_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_9_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_9_rank['publish_time'] = task6_9_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_9_rank['publish_time'] = pd.to_numeric(task6_9_rank['publish_time'])\ntask6_9_rank = task6_9_rank.sort_values(by='publish_time', ascending=False)\ntask6_9_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_9_rank['Aff_Score'] = 0\nfor i in range(len(task6_9_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_9_rank.iloc[i, 4]:\n            task6_9_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_9_rank[\"Ranking_Score\"] = task6_9_rank[\"publish_time\"]*0.8 + task6_9_rank[\"Aff_Score\"]*0.2\ntask6_9_rank = task6_9_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_9_rank.reset_index(inplace=True,drop=True)\ntask6_9_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.9 :\n\nfor i in range(len(task6_9_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_9_rank.iloc[i, 0])\n    print(\"Title: \", task6_9_rank.iloc[i, 1])\n    print(\"Section: \", task6_9_rank.iloc[i, 2])\n    print(\"Text: \", task6_9_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.10: ***Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['diagnostics', 'host','response','markers','cytokines','detect','early','disease','predict','progression','clinical','practice','efficacy','therapeutic','interventions','covid-19','covid19','virus']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 11 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 11: \n        return(True)  \n    return(False)    \n  \ntask6_10 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_10.append(i)\n    \nlen(task6_10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.10 :\nfor i in task6_10:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_10_rank = papers_data.iloc[task6_10, :]\ntask6_10_rank.reset_index(inplace=True,drop=True)\n\ntask6_10_rank['Title'] = task6_10_rank['Title'].astype(str) \n\ntask6_10_rank = pd.merge(task6_10_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_10_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_10_rank['publish_time'] = task6_10_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_10_rank['publish_time'] = pd.to_numeric(task6_10_rank['publish_time'])\ntask6_10_rank = task6_10_rank.sort_values(by='publish_time', ascending=False)\ntask6_10_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By afffiliation scores:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_10_rank['Aff_Score'] = 0\nfor i in range(len(task6_10_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_10_rank.iloc[i, 4]:\n            task6_10_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_10_rank[\"Ranking_Score\"] = task6_10_rank[\"publish_time\"]*0.8 + task6_10_rank[\"Aff_Score\"]*0.2\ntask6_10_rank = task6_10_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_10_rank.reset_index(inplace=True,drop=True)\ntask6_10_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.10 :\n\nfor i in range(len(task6_10_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_10_rank.iloc[i, 0])\n    print(\"Title: \", task6_10_rank.iloc[i, 1])\n    print(\"Section: \", task6_10_rank.iloc[i, 2])\n    print(\"Text: \", task6_10_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.11: ***Policies and protocols for screening and testing.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['policies','protocol','coronavirus','corona','covid','testing','screening','covid19','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask6_11 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_11.append(i)\n    \nlen(task6_11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.11 :\nfor i in task6_11:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_11_rank = papers_data.iloc[task6_11, :]\ntask6_11_rank.reset_index(inplace=True,drop=True)\n\ntask6_11_rank['Title'] = task6_11_rank['Title'].astype(str) \n\ntask6_11_rank = pd.merge(task6_11_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_11_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_11_rank['publish_time'] = task6_11_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_11_rank['publish_time'] = pd.to_numeric(task6_11_rank['publish_time'])\ntask6_11_rank = task6_11_rank.sort_values(by='publish_time', ascending=False)\ntask6_11_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliation scores:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_11_rank['Aff_Score'] = 0\nfor i in range(len(task6_11_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_11_rank.iloc[i, 4]:\n            task6_11_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_11_rank[\"Ranking_Score\"] = task6_11_rank[\"publish_time\"]*0.8 + task6_11_rank[\"Aff_Score\"]*0.2\ntask6_11_rank = task6_11_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_11_rank.reset_index(inplace=True,drop=True)\ntask6_11_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.11 :\n\nfor i in range(len(task6_11_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_11_rank.iloc[i, 0])\n    print(\"Title: \", task6_11_rank.iloc[i, 1])\n    print(\"Section: \", task6_11_rank.iloc[i, 2])\n    print(\"Text: \", task6_11_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.12: ***Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['policies', 'effects','control','supplies','mass','testing','reagents','swabs','covid19','virus','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 10 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 10: \n        return(True)  \n    return(False)    \n  \ntask6_12 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_12.append(i)\n    \nlen(task6_12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.12 :\nfor i in task6_12:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_12_rank = papers_data.iloc[task6_12, :]\ntask6_12_rank.reset_index(inplace=True,drop=True)\n\ntask6_12_rank['Title'] = task6_12_rank['Title'].astype(str) \n\ntask6_12_rank = pd.merge(task6_12_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_12_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_12_rank['publish_time'] = task6_12_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask6_12_rank['publish_time'] = task6_12_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_12_rank['publish_time'] = pd.to_numeric(task6_12_rank['publish_time'])\ntask6_12_rank = task6_12_rank.sort_values(by='publish_time', ascending=False)\ntask6_12_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations scores:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_12_rank['Aff_Score'] = 0\nfor i in range(len(task6_12_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_12_rank.iloc[i, 4]:\n            task6_12_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_12_rank[\"Ranking_Score\"] = task6_12_rank[\"publish_time\"]*0.8 + task6_12_rank[\"Aff_Score\"]*0.2\ntask6_12_rank = task6_12_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_12_rank.reset_index(inplace=True,drop=True)\ntask6_12_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.12 :\n\nfor i in range(len(task6_12_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_12_rank.iloc[i, 0])\n    print(\"Title: \", task6_12_rank.iloc[i, 1])\n    print(\"Section: \", task6_12_rank.iloc[i, 2])\n    print(\"Text: \", task6_12_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.13: ***Technology roadmap for diagnostics.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['technology', 'roadmap','diagnostics','covid19','virus','covid-19']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 3 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 2: \n        return(True)  \n    return(False)    \n  \ntask6_13 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_13.append(i)\n    \nlen(task6_13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.13 :\nfor i in task6_13:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_13_rank = papers_data.iloc[task6_13, :]\ntask6_13_rank.reset_index(inplace=True,drop=True)\n\ntask6_13_rank['Title'] = task6_13_rank['Title'].astype(str) \n\ntask6_13_rank = pd.merge(task6_13_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_13_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_13_rank['publish_time'] = task6_13_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_13_rank['publish_time'] = pd.to_numeric(task6_13_rank['publish_time'])\ntask6_13_rank = task6_13_rank.sort_values(by='publish_time', ascending=False)\ntask6_13_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By affiliations scores:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_13_rank['Aff_Score'] = 0\nfor i in range(len(task6_13_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_13_rank.iloc[i, 4]:\n            task6_13_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_13_rank[\"Ranking_Score\"] = task6_13_rank[\"publish_time\"]*0.8 + task6_13_rank[\"Aff_Score\"]*0.2\ntask6_13_rank = task6_13_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_13_rank.reset_index(inplace=True,drop=True)\ntask6_13_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 1.13 :\n\nfor i in range(len(task6_13_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_13_rank.iloc[i, 0])\n    print(\"Title: \", task6_13_rank.iloc[i, 1])\n    print(\"Section: \", task6_13_rank.iloc[i, 2])\n    print(\"Text: \", task6_13_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.14: ***Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['barriers','scaling','diagnostic','market','coalition','accelerator','model','epidemic','preparedness','innovation','critical','funding','opportunities','streamlined','regulatory','environment']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 9 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 8: \n        return(True)  \n    return(False)    \n  \ntask6_14 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_14.append(i)\n    \nlen(task6_14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.14 :\nfor i in task6_14:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ranking by the most recent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_14_rank = papers_data.iloc[task6_14, :]\ntask6_14_rank.reset_index(inplace=True,drop=True)\n\ntask6_14_rank['Title'] = task6_14_rank['Title'].astype(str) \n\ntask6_14_rank = pd.merge(task6_14_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_14_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\nimport dateutil.parser as parser\ntask6_14_rank['publish_time'] = task6_14_rank['publish_time'].apply(lambda x:  str(x).replace('May 8 Summer',''))\ntask6_14_rank['publish_time'] = task6_14_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_14_rank['publish_time'] = pd.to_numeric(task6_14_rank['publish_time'])\ntask6_14_rank = task6_14_rank.sort_values(by='publish_time', ascending=False)\ntask6_14_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By the effiliations scores:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_14_rank['Aff_Score'] = 0\nfor i in range(len(task6_14_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_14_rank.iloc[i, 4]:\n            task6_14_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_14_rank[\"Ranking_Score\"] = task6_14_rank[\"publish_time\"]*0.8 + task6_14_rank[\"Aff_Score\"]*0.2\ntask6_14_rank = task6_14_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_14_rank.reset_index(inplace=True,drop=True)\ntask6_14_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_14_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_14_rank.iloc[i, 0])\n    print(\"Title: \", task6_14_rank.iloc[i, 1])\n    print(\"Section: \", task6_14_rank.iloc[i, 2])\n    print(\"Text: \", task6_14_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.15: ***New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['platforms','technology','response','time','employ','holistic','approaches','epidemic','covid19','didease','covid-19','future']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# At least 8 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 7: \n        return(True)  \n    return(False)    \n  \ntask6_15 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_15.append(i)\n    \nlen(task6_15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Results for task 6.15 :\nfor i in task6_15:\n    print(\"\\n\")\n    print(\"PaperID: \", papers_data.iloc[i, 0])\n    print(\"Title: \", papers_data.iloc[i, 1])\n    print(\"Section: \", papers_data.iloc[i, 2])\n    print(\"Text: \", papers_data.iloc[i, 3])  \n    print(\"\\n\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_15_rank = papers_data.iloc[task6_15, :]\ntask6_15_rank.reset_index(inplace=True,drop=True)\n\ntask6_15_rank['Title'] = task6_15_rank['Title'].astype(str) \n\ntask6_15_rank = pd.merge(task6_15_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_15_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_15_rank['publish_time'] = task6_15_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_15_rank['publish_time'] = pd.to_numeric(task6_15_rank['publish_time'])\ntask6_15_rank = task6_15_rank.sort_values(by='publish_time', ascending=False)\ntask6_15_rank.reset_index(inplace=True,drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the affiliations score to the task's results:\ntask6_15_rank['Aff_Score'] = 0\nfor i in range(len(task6_15_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_15_rank.iloc[i, 4]:\n            task6_15_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_15_rank[\"Ranking_Score\"] = task6_15_rank[\"publish_time\"]*0.8 + task6_15_rank[\"Aff_Score\"]*0.2\ntask6_15_rank = task6_15_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_15_rank.reset_index(inplace=True,drop=True)\ntask6_15_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_15_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_15_rank.iloc[i, 0])\n    print(\"Title: \", task6_15_rank.iloc[i, 1])\n    print(\"Section: \", task6_15_rank.iloc[i, 2])\n    print(\"Text: \", task6_15_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.16: ***Coupling genomics and diagnostic testing on a large scale.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['coupling','genomic','diagnostic','test','large-scale','covid19','covid-19','epidemic','didease']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 5 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 4: \n        return(True)  \n    return(False)    \n  \ntask6_16 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_16.append(i)\n    \nlen(task6_16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_16_rank = papers_data.iloc[task6_16, :]\ntask6_16_rank.reset_index(inplace=True,drop=True)\n\ntask6_16_rank['Title'] = task6_16_rank['Title'].astype(str) \n\ntask6_16_rank = pd.merge(task6_16_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_16_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_16_rank['publish_time'] = task6_16_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_16_rank['publish_time'] = pd.to_numeric(task6_16_rank['publish_time'])\ntask6_16_rank = task6_16_rank.sort_values(by='publish_time', ascending=False)\ntask6_16_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_16_rank['Aff_Score'] = 0\nfor i in range(len(task6_16_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_16_rank.iloc[i, 4]:\n            task6_16_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_16_rank[\"Ranking_Score\"] = task6_16_rank[\"publish_time\"]*0.8 + task6_16_rank[\"Aff_Score\"]*0.2\ntask6_16_rank = task6_16_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_16_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_16_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_16_rank.iloc[i, 0])\n    print(\"Title: \", task6_16_rank.iloc[i, 1])\n    print(\"Section: \", task6_16_rank.iloc[i, 2])\n    print(\"Text: \", task6_16_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.17: ***Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['enhance','capabilities','sequencing','bioinformatics','target','region','genome','variant','covid19','covid-19','epidemic','didease']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 9: \n        return(True)  \n    return(False)    \n  \ntask6_17 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_17.append(i)\n    \nlen(task6_17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_17_rank = papers_data.iloc[task6_17, :]\ntask6_17_rank.reset_index(inplace=True,drop=True)\n\ntask6_17_rank['Title'] = task6_17_rank['Title'].astype(str) \n\ntask6_17_rank = pd.merge(task6_17_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_17_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_17_rank['publish_time'] = task6_17_rank['publish_time'].apply(lambda x: str(x).replace('Feb 5 Nov-Dec',''))\n\ntask6_17_rank['publish_time'] = task6_17_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_17_rank['publish_time'] = pd.to_numeric(task6_17_rank['publish_time'])\ntask6_17_rank = task6_17_rank.sort_values(by='publish_time', ascending=False)\ntask6_17_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_17_rank['Aff_Score'] = 0\nfor i in range(len(task6_17_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_17_rank.iloc[i, 4]:\n            task6_17_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_17_rank[\"Ranking_Score\"] = task6_17_rank[\"publish_time\"]*0.8 + task6_17_rank[\"Aff_Score\"]*0.2\ntask6_17_rank = task6_17_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_17_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_17_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_17_rank.iloc[i, 0])\n    print(\"Title: \", task6_17_rank.iloc[i, 1])\n    print(\"Section: \", task6_17_rank.iloc[i, 2])\n    print(\"Text: \", task6_17_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.18: ***Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['enhance','capacity','people','technology','data','sequencing','analytics','pathogens','covid19','covid-19','epidemic','didease','capabilities','natural','intentional']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 7 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 6: \n        return(True)  \n    return(False)    \n  \ntask6_18 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_18.append(i)\n    \nlen(task6_18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_18_rank = papers_data.iloc[task6_18, :]\ntask6_18_rank.reset_index(inplace=True,drop=True)\n\ntask6_18_rank['Title'] = task6_18_rank['Title'].astype(str) \n\ntask6_18_rank = pd.merge(task6_18_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_18_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_18_rank['publish_time'] = task6_18_rank['publish_time'].apply(lambda x: str(x).replace('Feb 5 Nov-Dec',''))\n\ntask6_18_rank['publish_time'] = task6_18_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_18_rank['publish_time'] = pd.to_numeric(task6_18_rank['publish_time'])\ntask6_18_rank = task6_18_rank.sort_values(by='publish_time', ascending=False)\ntask6_18_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_18_rank['Aff_Score'] = 0\nfor i in range(len(task6_18_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_18_rank.iloc[i, 4]:\n            task6_18_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_18_rank[\"Ranking_Score\"] = task6_18_rank[\"publish_time\"]*0.8 + task6_18_rank[\"Aff_Score\"]*0.2\ntask6_18_rank = task6_18_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_18_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_18_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_18_rank.iloc[i, 0])\n    print(\"Title: \", task6_18_rank.iloc[i, 1])\n    print(\"Section: \", task6_18_rank.iloc[i, 2])\n    print(\"Text: \", task6_18_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Task 6.19: ***One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords =['health','surveillance','humans','sources','spillover','future','exposure','pathogens','covid19','covid-19','epidemic','didease','organism','evolutionary','host','bats','transmission','trafficked','farm','wildlife','domestic','food','companion','species','environment','demographic','occupation','risk','factor']\nkw = []\nfor i in keywords:\n    kw.append(wordnet_lemmatizer.lemmatize(i))\n         \n# Gett synonyms: \nsynonyms = []\nfor k in kw:\n    for syn in wordnet.synsets(k):\n        for l in syn.lemmas():\n            synonyms.append(wordnet_lemmatizer.lemmatize(l.name()))\nfor i in synonyms:\n    kw.append(i)\n    \nkw = [ x for x in kw if \"_\" not in x ]\nkw = my_function(kw)\n\nprint(kw)   \n\n# At least 15 words in common\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if len(a_set.intersection(b_set)) > 14: \n        return(True)  \n    return(False)    \n  \ntask6_19 =[]\nfor i in range(len(papers_data)):\n    if common_member(kw, papers_data.iloc[i, 8]):\n        task6_19.append(i)\n    \nlen(task6_19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_19_rank = papers_data.iloc[task6_19, :]\ntask6_19_rank.reset_index(inplace=True,drop=True)\n\ntask6_19_rank['Title'] = task6_19_rank['Title'].astype(str) \n\ntask6_19_rank = pd.merge(task6_19_rank, meta[['Title','publish_time']], left_on='Title', right_on='Title')\ntask6_19_rank.dropna(inplace=True)\n\n# Extract the year from the string publish time\ntask6_19_rank['publish_time'] = task6_19_rank['publish_time'].apply(lambda x: str(x).replace('Feb 5 Nov-Dec',''))\n\ntask6_19_rank['publish_time'] = task6_19_rank['publish_time'].apply(lambda x: parser.parse(x).year)\n\n# Rank the task's results by time (Freshness)\ntask6_19_rank['publish_time'] = pd.to_numeric(task6_19_rank['publish_time'])\ntask6_19_rank = task6_19_rank.sort_values(by='publish_time', ascending=False)\ntask6_19_rank.reset_index(inplace=True,drop=True)\n\n## Ranking by affiliations\n\n# Extract the affiliations score to the task's results:\ntask6_19_rank['Aff_Score'] = 0\nfor i in range(len(task6_19_rank)):\n    for j in range(len(rank)):\n        if rank.iloc[j, 1] in task6_19_rank.iloc[i, 4]:\n            task6_19_rank.iloc[i, 11] = rank.iloc[j, 3]\n            \ntask6_19_rank[\"Ranking_Score\"] = task6_19_rank[\"publish_time\"]*0.8 + task6_19_rank[\"Aff_Score\"]*0.2\ntask6_19_rank = task6_19_rank.sort_values(by='Ranking_Score', ascending=False)\ntask6_19_rank.reset_index(inplace=True,drop=True)\n\n\n## 20 - Ranked Results for task 6.14 :\n\nfor i in range(len(task6_19_rank)):\n    print(\"\\n\")\n    print(\"PaperID: \", task6_19_rank.iloc[i, 0])\n    print(\"Title: \", task6_19_rank.iloc[i, 1])\n    print(\"Section: \", task6_19_rank.iloc[i, 2])\n    print(\"Text: \", task6_19_rank.iloc[i, 3])  \n    print(\"\\n\")\n    if i == 19:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save all sub-tasks results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"task6_1_rank.to_csv(\"task6_1_rank.csv\")\ntask6_2_rank.to_csv(\"task6_2_rank.csv\")\ntask6_3_rank.to_csv(\"task6_3_rank.csv\")\ntask6_4_rank.to_csv(\"task6_4_rank.csv\")\ntask6_5_rank.to_csv(\"task6_5_rank.csv\")\ntask6_6_rank.to_csv(\"task6_6_rank.csv\")\ntask6_7_rank.to_csv(\"task6_7_rank.csv\")\ntask6_8_rank.to_csv(\"task6_8_rank.csv\")\ntask6_9_rank.to_csv(\"task6_9_rank.csv\")\ntask6_10_rank.to_csv(\"task6_10_rank.csv\")\ntask6_11_rank.to_csv(\"task6_11_rank.csv\")\ntask6_12_rank.to_csv(\"task6_12_rank.csv\")\ntask6_13_rank.to_csv(\"task6_13_rank.csv\")\ntask6_14_rank.to_csv(\"task6_14_rank.csv\")\ntask6_15_rank.to_csv(\"task6_15_rank.csv\")\ntask6_16_rank.to_csv(\"task6_16_rank.csv\")\ntask6_17_rank.to_csv(\"task6_17_rank.csv\")\ntask6_18_rank.to_csv(\"task6_18_rank.csv\")\ntask6_19_rank.to_csv(\"task6_19_rank.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}