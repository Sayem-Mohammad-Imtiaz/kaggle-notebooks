{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/brasilian-houses-to-rent/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import some methods for anomality detection: [LocalOulierFactor](https://en.wikipedia.org/wiki/Local_outlier_factor) and [IsolationForest](https://en.wikipedia.org/wiki/Isolation_forest). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.set_cmap('Dark2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is full and have no any NA values"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/brasilian-houses-to-rent/houses_to_rent_v2.csv')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets visualize `area` and `total` features in regular and log scales (for `total`). "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].scatter(df['area'], df['total (R$)'])\nax[1].scatter(np.log(df['area']), np.log(df['total (R$)']))\n\nax[0].set_title('Base area and total $')\nax[1].set_title('Log scale')\n\nax[0].set_xlabel('Area');\nax[0].set_ylabel('Total $');\n\nax[1].set_xlabel('Area');\nax[1].set_ylabel('Total $');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both figures shows that dataset have outliers in both columns. Lets apply simplest methods to detect abnormal objects. Methods uses standart deviation and quantiles. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data for illustration\nx = np.random.normal(0, 1, 950)\nx_out = np.random.randint(6, 10, 50)\nx_final = np.append(x_out, x)\nx_final_std = np.std(x_final); x_final_mean = np.mean(x_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure below shows regions with 2 and 3 standart deviations in both \"sides\" around mean value (navy line near zero on xaxis). All values outside the colored areas can be considered as outliers. We can use the approach in the task. \n\nBoth approaches are highly depends on distribution of data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))\nax1.axvspan(x_final_mean + 3*x_final_std, \n            x_final_mean - 3*x_final_std, alpha=.2, color='r')\nax1.axvspan(x_final_mean + 2*x_final_std, \n            x_final_mean - 2*x_final_std, alpha=.2, color='g')\nax1.axvspan(x_final_mean + 1*x_final_std, \n            x_final_mean - 1*x_final_std, alpha=.2, color='blue')\nax1.axvspan(x_final_mean-.05, \n            x_final_mean+.05, color='navy')\nax1.text(8, .25, 'Outliers')\nax1.text(4.5, .25, '3 stds')\nax1.text(2.5, .25, '2 stds')\nax1.text(1, .25, '1 std')\nax1.set_title('Standart deviation approach')\nsns.distplot(x_final, ax=ax1, color='black')\n\nax2.axvspan(max(x_final[x_final < np.quantile(x_final, .05)]), \n            min(x_final[x_final > np.quantile(x_final, 1-.05)]), \n            alpha=.2, color='r')\nax2.axvspan(max(x_final[x_final < np.quantile(x_final, .15)]), \n            min(x_final[x_final > np.quantile(x_final, 1-.15)]), \n            alpha=.2, color='g')\nax2.axvspan(max(x_final[x_final < np.quantile(x_final, .25)]), \n            min(x_final[x_final > np.quantile(x_final, 1-.25)]), \n            alpha=.2, color='b')\n\nax2.text(8, .25, 'Outliers')\nax2.text(4, .25, '95% quantile')\nax2.text(1, .35, '85% quantile',rotation=90)\nax2.text(.3, .25, '75% quantile',rotation=90)\nax2.set_title('Quantile approach')\nsns.distplot(x_final, ax=ax2, color='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions for filtering data are below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def quantile_outlier(x, t=.25):\n    x = np.array(x)\n    low_thr = np.quantile(x, t)\n    upp_thr = np.quantile(x, 1-t)\n    return np.array((low_thr < x) & (x < upp_thr), dtype=int)\n\ndef std_outlier(x, t):\n    mu = x.mean()\n    low_thr = mu - t * x.std()\n    upp_thr = mu + t * x.std()\n    return np.array((low_thr < x) & (x < upp_thr), dtype=int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets compare Isolation Forest, Local Oulier Factor with simple approaches above. "},{"metadata":{"trusted":true},"cell_type":"code","source":"iso = IsolationForest(n_estimators=300, contamination=.01, bootstrap=True)\nlof = LocalOutlierFactor(n_neighbors=250, algorithm='brute', contamination=.005)\n\n# Make predictions\ny_pred_lof = lof.fit_predict(df[['area', 'total (R$)']])\ny_pred_iso = iso.fit_predict(df[['area', 'total (R$)']])\ny_pred_qua = quantile_outlier(df['total (R$)'], .01) & quantile_outlier(df['area'], .01)\ny_pred_std = std_outlier(df['total (R$)'], 3) & std_outlier(df['area'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n\nax[0, 0].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_iso, marker='.')\nax[0, 0].set_title('IsolationForest')\nax[0, 1].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_lof, marker='.')\nax[0, 1].set_title('LocalOutlierFactor')\nax[1, 0].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_qua, marker='.')\nax[1, 0].set_title('Quantile based')\nax[1, 1].scatter(np.log(df['area']), np.log(df['total (R$)']), \n                 c=y_pred_std, marker='.')\nax[1, 1].set_title('Std based')\nplt.xlabel('area');\nplt.ylabel('total$');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some regression analysis"},{"metadata":{},"cell_type":"markdown","source":"In this part we can apply simple regression approaches to predict total price. Also we compare results on full dataset and filtered dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total (R$)'] = np.log(df['total (R$)'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tr = pd.get_dummies(df, columns=['city', 'animal', 'furniture'], prefix='d')\ndf_tr['floor'].replace('0', 0, inplace=True)\ndf_tr.replace('-', 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_tr.drop('total (R$)',axis=1)\ny = df['total (R$)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove ouliers and make filtered datasets\nlof = LocalOutlierFactor()\nmask = np.array(lof.fit_predict(X)+1, dtype='bool')\nX_filtered = X[mask]\ny_filtered = y[mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nrid = Ridge()\nlas = Lasso()\nela = ElasticNet()\nrnf = RandomForestRegressor()\n\n\nparams_lr = {\n    'normalize' : [True, False]\n}\nparams_rid = {\n    'alpha' : [.0, .01, .05, .5, 1], \n    'normalize' : [True, False]\n}\nparams_las = {\n    'alpha' : [.0, .01, .05, .5, 1], \n    'normalize' : [True, False]\n}\nparams_ela = {\n    'alpha' : [.0, .01, .05, .5, 1], \n    'l1_ratio' : [.0, .01, .05, .5, 1, 2],\n    'normalize' : [True, False]\n}\nparams_rnf = {\n    #'n_estimators' : np.arange(1, 150, 10), #for time reasons\n    #'max_depth' : [1, 5, 10, 20, 35, 50]\n    'max_depth': [20, 50], 'n_estimators': [31, 141]\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_grid_results(estimator, params, X, y, ncv=6):\n    gs = GridSearchCV(estimator, params, cv=ncv, n_jobs=-1, \n                      return_train_score=True, \n                      scoring='neg_mean_squared_error')\n    gs.fit(X, y)\n    print(gs.best_params_)\n    return gs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"clf_names = ['lr', 'rid', 'las', 'ela', 'rnf']\nmean_train_scores = []\nmean_test_scores = []\nbest_clfs = []\n\nmean_train_scores_filtered = []\nmean_test_scores_filtered = []\nbest_clfs_filtered = []\n\nfor c, p in zip([lr, rid, las, ela, rnf], \n                [params_lr, params_rid, params_las, params_ela, params_rnf]):\n    clf = get_grid_results(c, p, X, y)\n    mean_train_scores.append(-np.mean(clf.cv_results_['mean_train_score']))\n    mean_test_scores.append(-np.mean(clf.cv_results_['mean_test_score']))\n    best_clfs.append(clf.best_estimator_)\n    \n    clf_filtered = get_grid_results(c, p, X_filtered, y_filtered)\n    mean_train_scores_filtered.append(-np.mean(clf_filtered.cv_results_['mean_train_score']))\n    mean_test_scores_filtered.append(-np.mean(clf_filtered.cv_results_['mean_test_score']))\n    best_clfs_filtered.append(clf_filtered.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see on the figure below filtering data can be useful for making correct prediction. In this case I use LocalOutlierFactor model with default parameters, but tuning params can make prediction procedure more accurate. \nWe can see the minimum difference in the results of RandomForest algorithm which more robust to outliers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1) = plt.subplots(1, 1, figsize=(14, 5))\nax1.bar(np.arange(len(mean_train_scores))-.3, \n        mean_train_scores, width=.2, \n        label='Mean train full', color='b')\nax1.bar(np.arange(len(mean_test_scores))-.1,\n        mean_test_scores, width=.2, \n        label='Mean test full', color='b',alpha=.5)\nax1.bar(np.arange(len(mean_train_scores_filtered))+.1, \n        mean_train_scores_filtered, width=.2, \n        label='Mean train filtered', color='r')\nax1.bar(np.arange(len(mean_test_scores_filtered))+.3, \n        mean_test_scores_filtered, \n        width=.2, label='Mean test filtered', color='r', alpha=.5)\n\nax1.set_title('Mean train and test scores on full dataset and filtered datasets')\nax1.set_xticks(np.arange(len(clf_names)));\nax1.set_xticklabels(clf_names, fontsize=16)\nax1.set_yticks(np.arange(0, .6, .1));\nax1.set_yticklabels([f'{i:.1f}' for i in np.arange(0, .6, .10, )], fontsize=16)\nax1.grid();\nax1.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}