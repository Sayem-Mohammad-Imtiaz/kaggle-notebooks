{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Observation and predictions of chicken weight on different diets\n\n* observe the dataset  \n* predict chick's weight using some Regressor \n\n# Observation of chicks dataset 🐣\n\n\n## 1. Import modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np     # linear algebra\nimport pandas as pd    # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt    # plotting data\nimport seaborn as sns \nsns.set(color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Check the dataset\n\n> The body weights of chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups of chicks on different protein diets."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"chicks = pd.read_csv('/kaggle/input/weight-vs-age-of-chicks-on-different-diets/ChickWeight.csv')\nchicks = chicks.drop([chicks.columns[0]], axis='columns')\nchicks.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"chicks.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The dataframe has exactly 578 rows and 4 int columns from an experiment on the effect of diet on early growth of chicks.\n\n## 3. Create count plot with number of chicks on different diets  \n**Diet 1** had more 'participants' than diets 2, 3 or 4 (more than 100 chicks!)."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"Diet\", data=chicks)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Create a scatter plot corresponding Time (days) and Weight (gm) of chicks  \n* Colored markers show diet type.  \n* 21 days of experiment, 12 days of measures"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='Time',y='weight', hue=\"Diet\", size='Diet', data=chicks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"🐔 If the dataframe is correct we should have 12 weights for every chicken.\n\n# Pivoting chicks dataset\n\n## 5. Pivoting data  \nPivot the data from long to wide, where columns `Chick` and `Diet` are the indexes and the column `Time` indicates different observations (weights) for each index. The observations were taken every other day - 0, 2, 4, ..., 18, 20, and last 21 day, so we have **12 days of measures**."},{"metadata":{"trusted":true},"cell_type":"code","source":"chicks_pivot = chicks.pivot_table(values=\"weight\", index=['Chick', 'Diet'], columns='Time')\nchicks_pivot.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 50 chicks but not all of them have complete observations in a period (f.e., NaN for chick 8 - day 21).\n\n## 6. Check and drop None  \nThere are 5 chickens for those mesurements were made not every specified day:"},{"metadata":{"trusted":true},"cell_type":"code","source":"chicks_pivot.isnull().any(axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop poor chicks with not full observation list:"},{"metadata":{"trusted":true},"cell_type":"code","source":"chicks_pivot = chicks_pivot.dropna();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unpivoting chicks dataset  \n## 7. Create four scatter plots to show weight of different chicks by diet\n* x - time, y - weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"chicks = chicks_pivot.stack().reset_index(name='weight')\ng = sns.FacetGrid(chicks, col=\"Diet\", margin_titles=True)\ng.map(sns.regplot, \"Time\", \"weight\",fit_reg=False, x_jitter=.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diet 4 has the least variation.  \nDiet 3 probabaly has the highest mean effect.\n\n## 8. Create four box plots showing weight of different chicks by diet\n* x - diet, y - weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"Time\", y=\"weight\", col=\"Diet\", data=chicks, kind=\"box\", col_wrap=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diet 3 has the highest mean weight gain but also it has more variation than Diet 4.\n\n## 9. Get statistics by Diets\nMaximum weight was achieved on Diet 3."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"chicks.groupby('Diet').agg(\n    max_weight=('weight', max),\n    min_weight=('weight', min),\n    avg_weight=('weight', 'mean'),\n    total_weight=('weight', sum),\n    num_chicks=('Chick', 'count')    \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the best model to predict chicks weight\n\nWe need to use some regression algorithm because we are aimed to predict a continuous number (weight).\n\n## 1. Prepare the data  \n* Create list of predictions variable: **y - weight**  \n* Create list of features: **X - ['Diet', 'Time']**  \n* Create train data (80%) and test data (validation, 20%) for both lists using `train_test_split` function from `sklearn` which shiffles the dataset using the pseudorandom number generator. Parameter `random_state = 1` is a fixed seed to make the output determenistic."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# y = WEIGHT -> target\ny = chicks.weight\n# Create X = [\"Diet\", \"Time\"] -> data\nfeatures = [\"Diet\", \"Time\"]\nX = chicks[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Apply linear models  \n### **2.1** Specify a model and fit it with parameters\n* **LinearRegression** - a linear approach to modeling the relationship between a dependent variable and one or more independent variables. Here we have a SIMPLE linear regression because there is only one independent (exploratory) variable - `Weight`.  \n* **Ridge** - or Tikhonov regularization - a linear approach which is useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters (**L2**).  \n* **Lasso** - least absolute shrinkage and selection operator - was originally formulated for least squares models and this simple case reveals a substantial amount about the behavior of the estimator, including its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding (**L1**).  \n* **ElasticNet** - Linear regression with combined L1 and L2 priors as regularizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_absolute_error\n\n# Specify Model and fit it\nC_modelRegr =  LinearRegression().fit(train_X, train_y)\nC_modelRidge = Ridge().fit(train_X, train_y)\nC_modelLasso = Lasso().fit(train_X, train_y)\nC_modelElasticNet = ElasticNet(alpha = 0.1).fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.2** Evaluating the model \n\n* Calculate mean absolute error (MAE) and scores on the training and test data. The MAE list is the absolute values of each error (positive numbers), so MAE is the average of those absolute errors. \n* Find best score for training and test data using `model.score()` function"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Linear Regression:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelRegr.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelRegr.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelRegr.score(val_X, val_y)))\n\nprint(\"Ridge Regression:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelRidge.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelRidge.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelRidge.score(val_X, val_y)))\n\nprint(\"Lasso Regression:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelLasso.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelLasso.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelLasso.score(val_X, val_y)))\n\nprint(\"Elastic Net:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelElasticNet.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelElasticNet.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelElasticNet.score(val_X, val_y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scores on the training and test data are very close together that means it is likely underfitting than overfitting (anyway, we have only two features).  \nClassical linear regression model as well as Ridge Regression model show best score on test data (72.9%). It means that regression model `C_modelRegr` predicted the weight correctly for 72.9% of the samples in the test data.\n\n## 3. Apply KNN Regressor  \nk-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression.\n* **KNeighborsRegressor** - regression based on k-nearest neighbors algorithm (k-NN). The output is the property value for the object. This value is the average of the values of k nearest neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nC_modelKNN =  KNeighborsRegressor(n_neighbors = 25).fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"KNeighbors Regressor:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelKNN.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelKNN.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelKNN.score(val_X, val_y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNeighbors Regressor model shows better score on test data (77.8%) than any linear model. \n\n## 4. Using tree, ensemble and neural network  \n\n* **DecisionTreeRegressor** - a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n* **RandomForestRegressor** - or random decision forests - an ensemble learning method that operates by constructing a multitude of decision trees at training time and for regression outputting the mean prediction of the individual trees.\n* **MLPRegressor** - Multi-layer Perceptron regressor - model which optimizes the squared-loss using LBFGS (Limited-memory BFGS) or sgd (stochastic gradient descent)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n# Specify Model and fit it\nC_modelDTreeRegr = DecisionTreeRegressor(max_leaf_nodes = 25, random_state=1).fit(train_X, train_y)\nC_modelForest = RandomForestRegressor(n_estimators=70, max_depth=3, random_state=1).fit(train_X, train_y)\nC_modelMLPRegr = MLPRegressor(solver = 'lbfgs', random_state = 1, hidden_layer_sizes =[10]).fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Decision Tree Regressor:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelDTreeRegr.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelDTreeRegr.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelDTreeRegr.score(val_X, val_y)))\n\nprint(\"Random Forest Regressor:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelForest.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelForest.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelForest.score(val_X, val_y)))\n\nprint(\"MLP Regressor:\")\nprint(\"\\tValidation MAE: {:,.0f}\".format(mean_absolute_error(C_modelMLPRegr.predict(val_X), val_y)))\nprint(\"\\tAccuracy on train data: {:.3f}\".format(C_modelMLPRegr.score(train_X, train_y)))\nprint(\"\\tAccuracy on test data: {:.3f}\".format(C_modelMLPRegr.score(val_X, val_y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Conclusion\nRandom Forest Regressor model shows best score on test data which is **77.5%** and is better than any regression model or a single decision tree. Anyway, MAE of Random Forest Regressor model is 18 and is bigger than KNeighbors Regressor (17)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}