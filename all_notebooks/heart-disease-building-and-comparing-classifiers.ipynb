{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier \nfrom sklearn.metrics import accuracy_score,roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read data\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get number of rows and columns of data\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As seen, all of data's datatypes are numeric values (int, float)\nand there aren't null values in dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the statistics from data\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our data will need feature scaling since there are big differences in value ranges of features and in std**\n_________________________________________________________________________________","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Data Exploration**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get count and percentage for independant variable (target)\ndisplay(df.target.value_counts())\ndf.target.value_counts()/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='target',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get count and percentage for variable (sex)\ndisplay(df.sex.value_counts())\ndf.sex.value_counts()/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='sex', data=df, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heart Disease Frequencies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ferq_visulaization(x, color , xlabel ,title, figsize= (8,6)):\n    pd.crosstab(x,df.target).plot(kind=\"bar\",figsize=figsize,color=color)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('Frequency')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heart Disease Frequency for age\nget_ferq_visulaization(df.age,color=['#5bc0de','#d9534f'],xlabel='Age',title='Heart Disease Frequency for Ages',figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heart Disease Frequency for sex\nget_ferq_visulaization(df.sex,color=['#11A5AA','#AA1190'],xlabel='Sex',title='Heart Disease Frequency for Sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heart Disease Frequency for fbs\nget_ferq_visulaization(df.fbs,color=['#FFC300','#581845'],xlabel='fbs',title='Heart Disease Frequency for FBS')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ferq_visulaization(df.slope,color=['#11A5AA','#AA1190'],xlabel='Slope',title='Heart Disease Frequency for Slopes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ferq_visulaization(df.cp,color=['#1CA53B','#AA1111'],xlabel='cp',title='Heart Disease Frequency for cp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing & Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#take copy from data and apply on in data preprocessing\ndf_copy=df.copy()\ndf_copy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove outliers by removing data geater than 3 std\nmean = df_copy[['chol','thalach','oldpeak']].mean()\nstd = df_copy[['chol','thalach','oldpeak']].std()\ncut_off = std * 3\nlower, upper = mean - cut_off, mean + cut_off\nnew_df = df_copy[(df_copy[['chol','thalach','oldpeak']] < upper) & (df_copy[['chol','thalach','oldpeak']] > lower)]\nnew_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data lies in 3 std. No outliers removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get one-hot-encoding to categorical variables\na = pd.get_dummies(df_copy['cp'], prefix = \"cp\")\nb = pd.get_dummies(df_copy['thal'], prefix = \"thal\")\nc = pd.get_dummies(df_copy['slope'], prefix = \"slope\")\n\n\n# concat the one-hot-encoding variables with data\ndata = [df_copy, a, b, c]\ndf_copy = pd.concat(data, axis = 1)\ndf_copy = df_copy.drop(columns = ['cp', 'thal', 'slope'])\ndf_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df_copy['target']\nX=df_copy.drop(['target'],axis=1)\n#split the data to train and test data\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building and comparing classifiers accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogisticRegression\nlr=LogisticRegression(random_state = 1)\n# KNN Model\nknn = KNeighborsClassifier(n_neighbors = 2)  \n#DecisionTree model\ndt = DecisionTreeClassifier(random_state=1)\n# Random Forest Classification\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n# Instantiate a VotingClassifier 'vc'\nvc = VotingClassifier(estimators=classifiers[0:4])\n# Instantiate a BaggingClassifier 'bc'\nbc_knn = BaggingClassifier(base_estimator=knn, n_estimators=300, n_jobs=-1)\n# Instantiate a BaggingClassifier 'bc'\nbc_lr = BaggingClassifier(base_estimator=lr, n_estimators=300, n_jobs=-1)\n# Instantiate a classification-tree 'dt' for AdaBoost\ndt_adb = DecisionTreeClassifier(max_depth=1, random_state=1)\n# Instantiate an AdaBoost classifier 'adab_clf'\nadb_clf = AdaBoostClassifier(base_estimator=dt_adb, n_estimators=100)\n# Instantiate a GradientBoostingRegressor 'gbt'\ngbt = GradientBoostingClassifier(n_estimators=300, max_depth=1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Define a list called classifier that contains the tuples (classifier_name, classifier)\nclassifiers = [('Logistic Regression', lr),\n               ('K Nearest Neighbours', knn),\n               ('Classification Tree', dt),\n               ('Random Forest',rf),\n               ('Voting Classifier',vc),\n               ('Bagging Classifier for knn',bc_knn),\n               ('Bagging Classifier for logistic regression',bc_lr),\n               ('AdaBoost Classifier',adb_clf),\n               ('GradientBoosting Classifier',gbt)]\n\n# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}