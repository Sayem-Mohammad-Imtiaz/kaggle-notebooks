{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Roteiro do trabalho\n1. Criar um som com as 10 categorias do UrbanSounb8k\n1. Separar esse arquivo novamente, identificando cada sim\n1. Carregar os arquivos de exemplo para o trabalho\n1. Aplicar no modelo treinado na primeira parte"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n!pip install pydub\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy, scipy, matplotlib.pyplot as plt\nimport os\n# Input import numpy, scipy, matplotlib.pyplot as plt files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1 Aproveitando as funções criadas na parte 1 para criar um novo som composto"},{"metadata":{"trusted":true},"cell_type":"code","source":"csv = pd.read_csv('/kaggle/input/urbansound8k/UrbanSound8K.csv')\ncsv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retorna o caminho completo de cada arquivo do CSV. \ndef path_class(filename):\n    #cria um filtro com a linha onde o arquivo está. \n    excerpt = csv[csv['slice_file_name'] == filename]\n    \n    #cria o path completo\n    path_name = os.path.join('/kaggle/input/urbansound8k/', 'fold'+str(excerpt.fold.values[0]), filename)\n    return path_name, excerpt['class'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retorna o audio e informações do mesmo. \ndef to_dataset (df, fold=[]):\n    # df = dataframe a ser percorrido.\n    # fold = qual/quais folds ler. \n    # quando fold nulo, significa para ler tudo.     \n    audio = []\n    audio_signals = []\n    label= []\n    labels=[]\n    paths=[]   \n    sampling_rate=[]\n    librosa_sampling_rate = []\n    \n    \n    if fold != []:\n        #filtra somentes os folds que foram enviados.  \n        filter_fold = df.fold.isin(fold)\n        df = df[filter_fold]\n\n        \n    #df = df.head(100)   \n    #para cada fold, pega todos os arquivos de dentro.            \n    for i in (df.fold.unique()):\n        #filtra o cada fold em cada iteração\n        filter_slice =  df['fold']==i\n        dt_fold = df[filter_slice]\n        \n        #Iteração para ler os arquivos da pasta fold da vez.\n        for p in dt_fold['slice_file_name']:\n            # Librosa já converte os dois canais para um canal e normaliza os dados entre 1 e -1. \n            audio,librosa_sampling_rate  = librosa.load('/kaggle/input/urbansound8k/fold'+str(i)+'/' + p)\n            audio_signals.append(audio)\n            sampling_rate.append(librosa_sampling_rate)\n            \n            #busca as classes e paths.\n            path, label = path_class(p)\n            paths.append(path)\n            labels.append(label)\n\n    \n    print('Reading...')    \n    \n    #audio = contém os arquivos de audio\n    #paths = contém o caminho completo do arquivo.\n    #labels = contem as classificações, \n    #librosa_sampling_rate = contém o sampling rate, que pode ser visto adiante. \n    return audio_signals,paths,labels,sampling_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Para anlisar cada classe, primeiro é necessário extrair cada uma e criar um dataframe. \ndt_class = pd.DataFrame()\ndt_fold = (csv[csv['fold']==1])\nfor i in (dt_fold.classID.unique()):\n    dt_class = dt_class.append(dt_fold[dt_fold['classID']==i].head(1))    \ndt_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa\nsample, sample_path, sample_label, sample_S_Rate = to_dataset(dt_class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pydub import AudioSegment\n\ncombined_sounds = AudioSegment.silent(duration=1000)\n\nfor x in sample_path:\n    sound = AudioSegment.from_wav(x)\n    combined_sounds =  combined_sounds +  AudioSegment.silent(duration=2000)  + sound\n\ncombined_sounds.export(\"joinedFile.wav\", format=\"wav\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import struct\nimport IPython.display as ipd\nipd.Audio('../working/joinedFile.wav')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 - Separando o arquivo em mais de um"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy, scipy, matplotlib.pyplot as plt, IPython.display as ipd\nimport librosa, librosa.display\nplt.rcParams['figure.figsize'] = (14, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-muted')\nplt.rcParams['figure.figsize'] = (14, 5)\nplt.rcParams['axes.grid'] = True\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.bottom'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.xmargin'] = 0\nplt.rcParams['axes.ymargin'] = 0\nplt.rcParams['image.cmap'] = 'gray'\nplt.rcParams['image.interpolation'] = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando dois exemplos. O que eu criei e o dado para o trabalho.\n#../working/joinedFile.wav\n#../input/exemplo2/exemplo2.wav\nsignal1, sr1 = librosa.load('../input/exemplos-recebidos/exemplo.wav')\nsignal2, sr2 = librosa.load('../input/exemplos-recebidos/exemplo2.wav')\nsignal3, sr3 = librosa.load('../input/exemplos-recebidos/exemplo3.wav')\nsignaljoined, srjoined = librosa.load('../working/joinedFile.wav')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotando o audio\nlibrosa.display.waveplot(signal1, sr=sr1)\n# Audio lido\nipd.Audio(signal1, rate=sr1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotando o audio\nlibrosa.display.waveplot(signal2, sr=sr2)\nipd.Audio(signal2, rate=sr2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotando o audio\nlibrosa.display.waveplot(signal3, sr=sr3)\nipd.Audio(signal3, rate=sr3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotando o audio\nlibrosa.display.waveplot(signaljoined, sr=srjoined)\nipd.Audio(signaljoined, rate=srjoined)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# funcao plotar a energia e o RMS\ndef plot_RMSE(signal,sr):\n    # Determina o hop e o frame para começar a calcular a energia e o RMS\n    hop_length = 256\n    frame_length = 512\n    energy = numpy.array([\n        sum(abs(signal[i:i+frame_length]**2))\n        for i in range(0, len(signal), hop_length)\n    ])\n    rmse = librosa.feature.rms(signal, frame_length=frame_length, hop_length=hop_length, center=True)\n    rmse = rmse[0]\n    frames = range(len(energy))\n    t = librosa.frames_to_time(frames, sr=sr, hop_length=hop_length)\n    librosa.display.waveplot(signal, sr=sr, alpha=0.4)\n    plt.plot(t, energy/energy.max(), 'r--')             # normalized for visualization\n    plt.plot(t[:len(rmse)], rmse/rmse.max(), color='g') # normalized for visualization\n    plt.legend(('Energy', 'RMSE'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Exemplo.wav')\nplot_RMSE(signal1,sr1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Exemplo2.wav')\nplot_RMSE(signal2,sr2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Exemplo3.wav')\nplot_RMSE(signal3,sr3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Arquivo com 10 Classes.wav')\nplot_RMSE(signaljoined,srjoined)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#funcao para pegar o inicio de cada aumento de energia e seu final. \ndef strip(signal, frame_length, hop_length, index, thresh):\n\n    # Compute RMSE.\n    rms = librosa.feature.rms(signal, frame_length=frame_length, hop_length=hop_length, center=True)  \n\n    # inicializa um ponteiro na primeira posicao.     \n    frame_index = index\n\n    # Anda ate achar aumento no rms\n    while rms[0][frame_index] < thresh and frame_index < len(rms[0])-1:\n        frame_index += 1\n     \n    # Converte os frames em samples\n    start_sample_index = librosa.frames_to_samples(frame_index, hop_length=hop_length)\n    \n    #remove a parte do vetor que ja foi lida.         \n    frame_index_2 = frame_index\n    \n    #anda ate encontrar queda na energia abaixo do threeshold.\n    while  rms[0][frame_index_2] > thresh and frame_index_2 < len(rms[0])-1:\n        frame_index_2 += 1       \n    \n    #Converte os frames    \n    end_sample_index = librosa.frames_to_samples(frame_index_2, hop_length=hop_length)\n       \n    #signal é o pedaço de audio.\n    # frame_index_2 é o ponteiro onde a leitura parou\n    # len(rms...) é para que o loop externo pare caso o arquivo tenha acabado. \n    return signal[start_sample_index:end_sample_index], frame_index_2, len(rms[0])-1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separação do Arquivo\n\n1. Manual\n1. Automática (Librosa)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pegando o primeiro arquivo e colocando em uma variável unica que vai ser usada pra todos. \nsignal = signaljoined\nsr = srjoined\nipd.Audio(signal, rate=sr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Manual (Descontinuado neste trabalho...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#loop para pegar cada som do arquivo. \nextracted_signals = []\nindex = 0\nstop=0\nthresh = 0.002\nhop_length = 256\nframe_length = 512\n# Enquanto nao identificar o fim do arquivo...\nwhile stop !=1:\n    y,index,quit = strip(signal, frame_length, hop_length,index,thresh)\n    extracted_signals.append(y)\n    if index >= quit:\n        stop=1\n\n#deleta a ultima posicao que não armazena nada.\nextracted_signals= extracted_signals[:-1]\n\nprint ('Quantidade De Audios Encontrados')\nprint(len(extracted_signals))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#exemplo de audio extraido\nipd.Audio(extracted_signals[1], rate=sr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observação Importante\n** ! Na primeira versão deste trabalho, usando a separação manual do arquivo e aplicando no modelo tive as seguintes conclusões **\n- Houveram erros na predição, mas mesmo em um audio que foi separado em mais partes errôneamente, o modelo não acusou classes diferentes. \n- É possível que eu tenha errado ao buscar o dicionário.\n- No resultado final errou em torno de 20% das predições.\n- Ele confundiu o som de Drilling com o de Crianças. O resto acertou. Isso é um ponto que pode ser melhorado no treinamento do modelo. \n- Na extração manual não consegui fazer os latidos sairem no mesmo audio. \n"},{"metadata":{},"cell_type":"markdown","source":"### 2. Automática (Librosa)\n\nEsta que será usado para a predição, pois se mostrou melhor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# O top_db varia de acordo com o arquivo. O melhor resultado foi 62 que conseguiu separar um audio com 10 classes.\n# Nos demais arquivos, fica entre 40 e 50. \ndef cut_signal(signal, top_db):\n    y = librosa.effects.split(signal,top_db=top_db)\n    extracted_signals = []\n    for i in y:\n        extracted_signals.append( signal[i[0]:i[1]] )\n        #emphasized_signal = np.concatenate(l,axis=0)\n\n    return extracted_signals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lembrando de cada arquivo:\n#signal1, sr1 = librosa.load('../input/exemplos-recebidos/exemplo.wav')\n#signal2, sr2 = librosa.load('../input/exemplos-recebidos/exemplo2.wav')\n#signal3, sr3 = librosa.load('../input/exemplos-recebidos/exemplo3.wav')\n\n#Realiza o corte para cada arquivo e armazena em diferentes variáveis. \nextracted_signals1 = cut_signal(signal1, 40)\nextracted_signals2 = cut_signal(signal2, 50)\nextracted_signals3 = cut_signal(signal3, 50)\nextracted_signalsjoined = cut_signal(signaljoined, 62)\n\n#exemplo de audio extraido\nipd.Audio(extracted_signals1[0], rate=sr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extração das Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extrai as features.\ndef extract_features(signal):\n    mfccs = librosa.feature.mfcc(y=signal,  n_mfcc=40)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n     \n    return mfccs_processed    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cria um array para predição para cada audio\npredict1 = []\nfor x in extracted_signals1:\n    predict1.append(extract_features(x))    \n\npredict2 = []\nfor x in extracted_signals2:\n    predict2.append(extract_features(x))    \n\npredict3 = []\nfor x in extracted_signals3:\n    predict3.append(extract_features(x))    \n    \npredictjoined = []\nfor x in extracted_signalsjoined:\n    predictjoined.append(extract_features(x))        \n# A extração foi bem melhor. \nprint('Classes encontradas no Exemplo.wav')\nprint(len(predict1))\n\nprint('Classes encontradas no Exemplo2.wav')\nprint(len(predict2))\n\nprint('Classes encontradas no Exemplo3.wav')\nprint(len(predict3))\n\nprint('Classes encontradas no Exemplojoined.wav')\nprint(len(predictjoined))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O corte se mostrou correto!"},{"metadata":{},"cell_type":"markdown","source":"## Aplicação do Modelo (Predict)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importa o modelo \nfrom keras.models import Sequential, load_model\nmodel = load_model('../input/model-keras/best_model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# realiza a predição\nresults1 = model.predict_classes(np.array(predict1))\nresults2 = model.predict_classes(np.array(predict2))\nresults3 = model.predict_classes(np.array(predict3))\nresultsjoined = model.predict_classes(np.array(predictjoined))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef translate(results):\n    # Para mostrar as classes como texto, usamos o dicionario criado com base no encoder do modelo.\n    d = pickle.load( open( \"../input/dictionary/dict\", \"rb\" ) )\n\n    # Converte o dicionario\n    d = {v: k for k, v in d.items()}\n\n    # Cria a coluna de labels com base do dicionario. \n    label = [d[x] for x in results]\n    df = pd.DataFrame({'classID':results, 'label':label})\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd_results_1 = translate(results1)\npd_results_2 = translate(results2)\npd_results_3 = translate(results3)\npd_results_joined = translate(resultsjoined)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparando os resultados!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare o dataframe com a predição e o audio. \nprint('Arquivo.wav')\nprint(pd_results_1)\nipd.Audio(signal1, rate=sr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare o dataframe com a predição e o audio. \nprint('Arquivo2.wav')\nprint(pd_results_2)\nipd.Audio(signal2, rate=sr2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare o dataframe com a predição e o audio. \nprint('Arquivo3.wav')\nprint(pd_results_3)\nipd.Audio(signal3, rate=sr3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare o dataframe com a predição e o audio. \nprint('JoinedFile.wav')\nprint(pd_results_joined)\nipd.Audio(signaljoined, rate=srjoined)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Conclusão\n\n - No primeiro arquivo Exemplo.wav\n     - Das 3 classes, acertou 2. Confundiu som de sirene com cachorro. \n     - top_db usado foi 40, assim consegui extrair correto. \n - No arquivo Exemplo2.wav\n     - O top_db foi 50, dessa forma trazendo as 3 classes corretas. Como o arquivo anterior, gerou 6 classes. \n     - O modelo acertou 100% \n - No arquivo Exemplo3.wav\n     - O modelo errou apenas um dos 4 audios. Confundiu musica com crianças. \n     - Mantive o top_db como 50. \n - No arquivo Joined (10 classes)\n     - Confundiu tiro com latidos.\n     - Confundiu arcondicionado com musica de rua\n     - Errou 20%, ou 2 de 10. Top_db 62 conseguiu separar todos corretamente. \n     \nEmbora a separação esteja adequada, e o modelo esteja acertando uma quantidade boa, há como melhorar, principalmente se for aplicar mais pré-processamentos. Acredito que alguns sons, embora da mesma classe, estando com diferentes volumes, pode atrapalhar o aprendizado. \n\nOs resultados acima foram obtidos com os dois modelos testados.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}