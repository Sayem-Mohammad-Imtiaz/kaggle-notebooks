{"cells":[{"metadata":{"_uuid":"53aa01bd56183394829a61d637a7cfab78f57bec"},"cell_type":"markdown","source":"# Poisonous vs. Edible Mushroom Classification "},{"metadata":{"_uuid":"448f2908fe77bf2ba4594b3c460133d4b52b9e79"},"cell_type":"markdown","source":"### Table of contents\n1. [Introduction](#introduction)\n2. [Data Import/Cleaning](#import)\n3. [Feature Selection](#feat_select)\n    1. [Data Exploration](#exp)\n4. [PCA Visualiztion](#pca)\n5. [Kmeans Visualization](#kmean)\n6. [Linear Classifiers](#lin)\n    1. [Logistic Regression](#logreg)\n    2. [SVM](#svm)\n7. [Bagging Classifiers](#bag)\n    1. [GradientBoost](#bag)\n    2. [XGBoost](#xgb)\n8. [Random Forest Classifier](#trees)"},{"metadata":{"_uuid":"c4328d8f43fe473d7796054af88c26ecbf3dc9b0"},"cell_type":"markdown","source":"#### Synopsis <a name=\"introduction\"></a>\nThe following analysis will explore the kaggle Mushroom Classification dataset (https://www.kaggle.com/uciml/mushroom-classification). Several ML models will be explored for their ability to classify mushrooms as poisonous or edible based off of the provided data. \n\nBoth unsupervised and supervised methods will be used, as well as regression and ensemble methods for a rounded look at how different models work with categorical datasets. "},{"metadata":{"_uuid":"e3e2f7a38ff5bb618835adfbfd7d9649584a7141"},"cell_type":"markdown","source":"#### Mushrooms:\nMushrooms come in all shapes, sizes, colours, and flavours--as the saying goes: every mushroom is edible at least <i>once</i>.\n\nMushroom identification is a multifaceted process, where several important features of the fruiting body are taken into account before determining edibility. In addition to physical factors, the time of season and where a mushroom fruits (dirt, grass, manure, on a tree, on a fallen log, etc.) are also important considerations when id'ing fungi. Id'ing should always be done by an experienced mushroom hunter with local knowledge. \n\n#### Quick Poisonous Mushroom Identifiers\n* Spore Print: Spores are collected by placing a mushroom cap facedown over a sheet of paper or mirror. Different species' spores will be specific shades/colours, for example genus <a href=\"https://en.wikipedia.org/wiki/Amanita\"><i>Amanita</i></a> will spore print white--poison. \n* Fruiting Body: Several dispersal mechanisms for spores have evolved; between gilled, porous, sac or puffball fungi poisonous species may all mimic edible look-a-likes. \n* Bruising/Color: Certain species that bruise dark when handled can sometimes indicate poison or inedibility. \n* Morphology: \n   * The genus <a href=\"https://en.wikipedia.org/wiki/Amanita\"><i>Amanita</i></a> carries some of the some deadliest mushrooms in the world. Destroying Angel, Death Cap and Fool's Mushroom are all fatal, however share characteristics of the Amanita class making them easily identifiable. While the cap colour may alter, typically White cap, gills, and spore print, along with a physical structure called the volva are telltale signs of  <a href=\"https://en.wikipedia.org/wiki/Amanita\"><i>Amanita's</i></a>. \n   <img src=\"https://atrium.lib.uoguelph.ca/xmlui/bitstream/handle/10214/6850/Amanita_virosa_Destroying_Angel_amanitin_and_phalloidin.jpg?sequence=1&isAllowed=y\" alt=\"Identifying a Destroying angel\" width=\"40%\"></img>\n   <div align=\"center\"><small>Source: University of Guelph</small></div>\n   * The genus <a href=\"https://en.wikipedia.org/wiki/Gyromitra_esculenta\"><i>Gyromitra</i></a>, better known as the False Morel, is an example of a poisonous mushroom that looks like the famously delicious Morel. Inexperienced mushroom hunters could potentially mix this type of mushroom up with an edible counterpart and suffer the consequences. However, false morels have a full stem and are tellingly <i>not</i> hollow. \n   <img src=\"https://cdn0.wideopenspaces.com/wp-content/uploads/2017/03/morel-mushroom-real-fake.jpg\" alt=\"Two true morels on the left, false morel on the right\" width=\"50%\"></img>\n   <div align=\"center\"><small><a href=\"https://www.wideopenspaces.com/learn-important-difference-real-false-morel-mushrooms/\">Source: Wide Open Spaces</a></small></div>"},{"metadata":{"_uuid":"91d725db98415d402cf4c9d3012dfb0e2e0ffcd1"},"cell_type":"markdown","source":"Let's first import packages and data. <a name=\"import\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:07:38.106796Z","start_time":"2018-10-03T23:07:38.052666Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#import cleaning and visualization modules\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams, gridspec\n\n#import analysis modules\nfrom sklearn.svm import SVC\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom xgboost import XGBClassifier\nfrom sklearn import preprocessing\nfrom sklearn.svm import LinearSVC\nfrom xgboost import plot_importance\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#pandas configuration\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f2b8ec8c6cddf8280095735095484c2c1eef8bb"},"cell_type":"markdown","source":"Data is read into a dataframe and displayed. Next dataypes, counts and columns are shown and a check if any null values are present."},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:27.333524Z","start_time":"2018-10-03T22:35:27.216826Z"},"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#read into dataframe, display first 5 values\ndf = pd.read_csv('../input/mushrooms.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:27.382072Z","start_time":"2018-10-03T22:35:27.336974Z"},"trusted":true,"_uuid":"08a5ec5c3200c9ba39b64e124ba900252a031e26"},"cell_type":"code","source":"#Look into df for datatypes, if nulls exist \nnull_count = 0\nfor val in df.isnull().sum():\n    null_count += val\nprint('There are {} null values.\\n'.format(null_count))\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"640b69190e627bfa9d614262f0eeacc426095519"},"cell_type":"markdown","source":"We've seen that the data is categorical and there are no nulls in our set. Let's check how many classes are in each column. <a name=\"feat_select\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:27.432977Z","start_time":"2018-10-03T22:35:27.386187Z"},"trusted":true,"_uuid":"1dfb6438cdcc2565a2e075d1c801dbc24b7f279f"},"cell_type":"code","source":"def show_features(df):\n    '''Takes a dataframe and outputs the columns, number of classes and category variables.'''\n    col_count, col_var = [], []\n    for col in df:\n        col_count.append(len(df[col].unique()))\n        col_var.append(df[col].unique().sum())\n    df_dict = {'Count': col_count, 'Variables': col_var}\n    df_table = pd.DataFrame(df_dict, index=df.columns)\n    print(df_table)\n    \nshow_features(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ae5d3506f5b2ead8b47d42e716c02f098101ddf"},"cell_type":"markdown","source":"Right away there are some interesting things to note: Veil-type has only one class and can be removed from our set, several classes are binary and can be reduced to a single feature, and there is a '?' class in the stalk-root class. One-hot encoding will transform our data into a usable format for our models, remove excess feature columns and prepare our independent variable for supervised learning. \n\nNext, let's see how many '?' values are present in the stalk-root category."},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:28.719918Z","start_time":"2018-10-03T22:35:28.707671Z"},"trusted":true,"_uuid":"e0785aeb2d6dfe8812aec12b36339d1ea677c73f"},"cell_type":"code","source":"df['stalk-root'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f77ee5b3f23ad4ef5bfee0afac4324dcae43022e"},"cell_type":"markdown","source":"There are a few options--remove the class entirely but lose potential information in our dataset, delete any row with a '?' but lose data across all variables, or encode the data and treat it as an unknown variable. For the purposes of this study we'll keep the class and use encoding to transform '?' into a feature. "},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:29.740416Z","start_time":"2018-10-03T22:35:29.648291Z"},"trusted":true,"_uuid":"9196ae63f8b9c1b16211be6127a251ac184f4b0e"},"cell_type":"code","source":"df_dum = pd.get_dummies(df, drop_first=True)\ndf_dum.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7f22f520f01322fa0849c67e8d2307744ef7fc5"},"cell_type":"markdown","source":"### Mushroom Hunting & Important Features of Determining Edibility  <a name=\"exp\"></a>\n\nAs discussed earlier, the art of mushroom foraging can be difficult at times when ID'ing unknown species. In the field, we'll collect a spore print, look at morphological features, mark time of year, use smell, test for bruising, note the conditions it was found in: healthy or rotted terrain, neighbouring trees and plant life, near other fungi--all important steps in correctly identifying whether a find is edible or poisonous. \n\nWe can graph some of the data to determine if there are any features that are more associated with poisonous species at a glance before running our models and testing for important features.\n"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:58:19.154687Z","start_time":"2018-10-03T22:58:15.277301Z"},"trusted":true,"_uuid":"4517df6b156c25f7cd7bb91dc9a0922b34b1715d"},"cell_type":"code","source":"plt.figure(figsize=[16,12])\n\nplt.subplot(231)\nsns.countplot(x='odor', hue='class', data=df)\nplt.title('Odor')\nplt.xticks(np.arange(10),('Pungent', 'Almond', 'Anise', 'None', 'Foul', 'Creosote', 'Fish', 'Spicy', 'Musty'), rotation='vertical')\nplt.ylabel('Count')\n\nplt.subplot(232)\nsns.countplot(x='spore-print-color', hue='class', data=df)\nplt.title('Spore Print Color')\nplt.xticks(np.arange(10),('Black', 'Brown','Purple','Chocolate','White','Green','Orange','Yellow','Brown'), rotation='vertical')\nplt.legend(loc='upper right')\n\nplt.subplot(233)\nsns.countplot(x='cap-color', hue='class', data=df)\nplt.title('Cap Color')\nplt.xticks(np.arange(11),('Brown', 'Yellow','White','Gray','Red','Pink','Buff','Purple','Cinnamon','Green'), rotation='vertical')\nplt.legend(loc='upper right')\n\nplt.subplot(234)\nsns.countplot(x='bruises', hue='class', data=df)\nplt.title('Bruising')\nplt.xticks(np.arange(2),('Bruise', 'No Bruise'), rotation='vertical')\nplt.legend(loc='upper right')\n\nplt.subplot(235)\nsns.countplot(x='habitat', hue='class', data=df)\nplt.title('Habitat')\nplt.xticks(np.arange(8),('Urban', 'Grasses','Meadows','Woods','Paths','Waste','Leaves'), rotation='vertical')\nplt.legend(loc='upper right')\n\nplt.subplot(236)\nsns.countplot(x='population', hue='class', data=df)\nplt.title('Population')\nplt.xticks(np.arange(7),('Scattered', 'Numerous','Abundant','Several','Solitary','Clustered'), rotation='vertical')\nplt.legend(loc='upper right')\n\nplt.tight_layout()\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a793e3512339819cd20642963a4d0239822c425"},"cell_type":"markdown","source":"At first glance, odour, spore print color and bruising are fairly good features to predict edibility, while cap colour, habitat and population-type show a bit more variance between species. These graphs are interesting from a foraging standpoint and reinforce how it's important to use multiple traits to ID a mushroom. \n\nNext we'll run some classification models to try and predict whether a mushroom is poison or edible. The data is split into our X-dependent feature columns and y-independent label, then split again into a 70:30 train:test set. "},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:46.605774Z","start_time":"2018-10-03T22:35:46.580105Z"},"trusted":true,"_uuid":"75672de36a8c394bbd0b3d6fa879999d10ff73a4"},"cell_type":"code","source":"#set features\nX = df_dum.drop('class_p', axis=1)\n#set independent variable\ny = df_dum['class_p']\n#split the training and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n#print shapes of training/testing sets\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09bf2a053bb49342ee8d6ca011defe2dd010cfb2"},"cell_type":"markdown","source":"### PCA & KMeans Visualization <a name=\"pca\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:01:52.501777Z","start_time":"2018-10-03T23:01:51.32119Z"},"_uuid":"57ebd622e1614b7f55f09e344c9e6c87fb36970c","trusted":true},"cell_type":"code","source":"#visualize edible vs poison classes\npca = PCA(n_components=2)\n\nx_pca = X.values\nx_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0], x_pca[:,1], c=y, s=40, edgecolor='k')\nplt.title('Visualizing Edible vs. Poison Classes')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f74a12f5eab9d499dd5d661262a5f06f25804ba"},"cell_type":"markdown","source":"PCA is used to visualize the dataset by transforming our features into 2 dimensions. Right away we can see a clear separation of classes with some overlap in the left cluster. We can see if an unsupervised KMeans with K=2 clusters is able to classify our data with any accuracy. <a name=\"kmean\"></a>"},{"metadata":{"_uuid":"6b0163ccb1735db2a5a549bc4c06b2df2e455789"},"cell_type":"markdown","source":"### KMeans"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:52.823593Z","start_time":"2018-10-03T22:35:51.791012Z"},"trusted":true,"_uuid":"93f9b48aa488057abe48ade3ac8ab24e7175e029"},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.cluster import KMeans\n\n#Specify the model and fit to training set\nkm = KMeans(n_clusters = 2)\nkm.fit(X_train)\n\n#PCA X_test for visualization\npca_test = PCA(n_components = 2)\npca_test.fit(X_test)\nX_test_pca = X_test.values\nX_test_pca = pca_test.fit_transform(X_test)\n\n#KMeans prediction\ny_pred_km = km.predict(X_test)\n\n#Plot the data\nplt.figure(figsize=(8,6))\nplt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred_km, \n            s=40, edgecolor='k')\nplt.title('KMeans: Test Data')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3bce448f824c60d06d7135f279f49d29f9c13a9"},"cell_type":"markdown","source":"Visually this looks OK, but not great. The model acheives 89% accuracy. KMeans is fast, and works by measuring the distance from the centroid of a cluster to classify points. Since there is some overlap in the left-most cluster, it groups everything to the same class.   "},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:53.268794Z","start_time":"2018-10-03T22:35:53.255372Z"},"trusted":true,"_uuid":"2bb4dc70bf937a82e6094e014fc0835940ceb58c"},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, fontsize=15,\n                          normalize=False, title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    cm_num = cm\n    cm_per = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title.replace('_',' ').title()+'\\n', size=fontsize)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, size=fontsize)\n    plt.yticks(tick_marks, classes, size=fontsize)\n\n    fmt = '.5f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        # Set color parameters\n        color = \"white\" if cm[i, j] > thresh else \"black\"\n        alignment = \"center\"\n\n        # Plot perentage\n        text = format(cm_per[i, j], '.5f')\n        text = text + '%'\n        plt.text(j, i,\n            text,\n            fontsize=fontsize,\n            verticalalignment='baseline',\n            horizontalalignment='center',\n            color=color)\n        # Plot numeric\n        text = format(cm_num[i, j], 'd')\n        text = '\\n \\n' + text\n        plt.text(j, i,\n            text,\n            fontsize=fontsize,\n            verticalalignment='center',\n            horizontalalignment='center',\n            color=color)\n        \n    plt.tight_layout()\n    plt.ylabel('True label'.title(), size=fontsize)\n    plt.xlabel('Predicted label'.title(), size=fontsize)\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:35:54.207175Z","start_time":"2018-10-03T22:35:53.891981Z"},"trusted":true,"_uuid":"288776d3248acf87b3a5aa5a4b4318b2da8612d5"},"cell_type":"code","source":"cm_km = metrics.confusion_matrix(y_test, y_pred_km)\nplot_confusion_matrix(cm_km, classes=['Edible','Poison'])\nprint(f'KMeans accuracy: {str(accuracy_score(y_test, y_pred_km)*100)[:5]}%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"665ad7adb8c869e81533231288154d914efdbfae"},"cell_type":"markdown","source":"Overall an unsupervised KMeans approach was interesting but by reducing our features into 2 dimensions there was a loss in accuracy. Next, we're going to try some regression models on our training set, starting with a Logistic Regression. Features can be auto-selected for and tuned using Recursive Feature Elimination and Cross-validation. <a name=\"lin\"></a>"},{"metadata":{"_uuid":"05242f1eb714c3b2ba285a7139a5d7dbe1077089"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:36:25.865754Z","start_time":"2018-10-03T22:35:57.312172Z"},"trusted":true,"_uuid":"a48a3a77b26284c40389c7bc3dd328546a89cb3c"},"cell_type":"code","source":"#set the model and fit entire data to RFECV--train/test splits are done automatically and cross-validated.\nlm = linear_model.LogisticRegression()\nrfecv = RFECV(estimator=lm, step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\n\nprint('Optimal number of features: %d' % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n#plot features vs. validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel('Number of features selected')\nplt.ylabel('Cross validation score')\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d6cbb7c1457fb4f75d1395d5198d045e903ccb6"},"cell_type":"markdown","source":"Wow! 14/95 of the features give us the optimal amount for modelling. The rest is noise. This selection tests helps us to avoid overfitting and multicollinearity. Let's move forward with only those 14 features and see how our models perform. <a name=\"logreg\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:37:51.875283Z","start_time":"2018-10-03T22:37:51.861729Z"},"_uuid":"2ed5ea73196f40176943e48eb2459d5d13b5083b","scrolled":true,"trusted":true},"cell_type":"code","source":"#set optimal features and assign new X, train/test split\nopt_features = ['odor_c', 'odor_f', 'odor_l', 'odor_n', 'odor_p', \n                'gill-spacing_w', 'gill-size_n', 'stalk-surface-above-ring_k', \n                'ring-type_f', 'spore-print-color_k', 'spore-print-color_n', \n                'spore-print-color_r', 'spore-print-color_u', 'population_c']\n#new dependent variables\nX_opt = X[opt_features] \n\n#split the training and test data\nXo_train, Xo_test, yo_train, yo_test = train_test_split(X_opt, y, test_size=0.3)\n#print shapes of training/testing sets\nprint(Xo_train.shape, Xo_test.shape, yo_train.shape, yo_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:37:57.13627Z","start_time":"2018-10-03T22:37:57.0903Z"},"trusted":true,"_uuid":"a91e28a8f1f04e2fee87d152dc4779a4e9c81022"},"cell_type":"code","source":"#logistic regression\nlm = linear_model.LogisticRegression()\nlm.fit(Xo_train, yo_train)\nlog_probs = lm.predict_proba(Xo_test)\nloss = log_loss(yo_test, log_probs)\nprint(f'Loss value: {loss}')\nprint(f'Training accuracy: {str(lm.score(Xo_train, yo_train)*100)[:5]}%')\nprint(f'Test accuracy: {str(lm.score(Xo_test, yo_test)*100)[:5]}%')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:38:00.81568Z","start_time":"2018-10-03T22:38:00.471661Z"},"trusted":true,"_uuid":"523807e759c8bd341dde4dfd78084eda56c18982"},"cell_type":"code","source":"y_pred_lm = lm.predict(Xo_test)\n\ncm_lm = metrics.confusion_matrix(yo_test, y_pred_lm)\nplot_confusion_matrix(cm_lm, ['Edible','Poison'])\nprint(f'Logistic Regression accuracy: {str(accuracy_score(yo_test, y_pred_lm)*100)[:5]}%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d31f917148ad28c350b83ce762081ceb90b358e1"},"cell_type":"markdown","source":"As shown, a logistic regression performs very well on this smaller categorical dataset. With only one false negative and no false positives the model makes quick work of this problem. Now we have our features and benchmarks for performance, let's try an SVM (Support Vector Machine) with different kernals to see what our best fit is. <a name=\"svm\"></a>"},{"metadata":{"_uuid":"3a191901ca7a84776888fd3e9487cf52d4d5e07d"},"cell_type":"markdown","source":"### SVM"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:38:18.807942Z","start_time":"2018-10-03T22:38:18.5715Z"},"trusted":true,"_uuid":"928a769caf5421c889631f34d6cae79ade333bb8"},"cell_type":"code","source":"#test out different SVMs using the different kernals\nkerns = ['linear', 'rbf', 'sigmoid']\nfor i in kerns:\n    #Kernel trick\n    svm_kern = SVC(kernel=f'{i}')\n    svm_kern.fit(Xo_train,yo_train)\n    \n    #Get the score\n    print(f'{i} kernal SVM score: {str(100*svm_kern.score(Xo_test,yo_test))[:6]}%')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:38:22.675519Z","start_time":"2018-10-03T22:38:22.647404Z"},"trusted":true,"_uuid":"1d6b7535bea11f72495a2d60e6da543d119943a2"},"cell_type":"code","source":"#fit SVM model to scaled data\nsvm = LinearSVC()\nsvm.fit(Xo_train, yo_train)\nprint(f'Linear SVM Training accuracy is: {svm.score(Xo_train, yo_train)*100}%')\nprint(f'Linear SVM Test accuracy is: {svm.score(Xo_test, yo_test)*100}%')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:41:40.880115Z","start_time":"2018-10-03T22:41:40.584559Z"},"trusted":true,"_uuid":"8a1b786ea7d783548c9c8971df3bb0ba068b2e5f"},"cell_type":"code","source":"y_pred_svm = svm.predict(Xo_test)\n\ncm_svm = metrics.confusion_matrix(yo_test, y_pred_svm)\nplot_confusion_matrix(cm_svm, ['Edible','Poison'])\nprint(f'SVM accuracy: {str(accuracy_score(yo_test, y_pred_svm)*100)[:5]}%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95fcaeec65f743f5419f487d54d2ed1413854004"},"cell_type":"markdown","source":"Both of our linear models performed spectacularly on the mushroom dataset. Let's see how tree and bagging classifiers handle the data. <a name=\"bag\"></a>\n\n### Gradient Boost and XGBoost Classifiers"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T22:49:39.774874Z","start_time":"2018-10-03T22:49:39.069708Z"},"trusted":true,"_uuid":"46e87851cec9dbcdfd8c27d6e329bf8c515ecdae"},"cell_type":"code","source":"#initialize gradientboost and xgboost\ngb = GradientBoostingClassifier()\nxgb = XGBClassifier()\n#fit models\ngb.fit(Xo_train,yo_train)\nxgb.fit(Xo_train,yo_train)\n#score models\nprint(f'Gradient Boost score: {(100 * gb.score(Xo_test,yo_test))}%')\nprint(f'XG Boost score: {(100 * xgb.score(Xo_test,yo_test))}%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68ec9251f227ab57f70febde7ce96a7100b8dee7"},"cell_type":"markdown","source":"It's apparent that our binary classification of mushroom toxicity is not a difficult problem for bagging and regression models to solve. We can see how XGBoost weighted the feature columns below. <a name=\"xgb\"></a>"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:32:26.709392Z","start_time":"2018-10-03T23:32:26.273726Z"},"trusted":true,"_uuid":"7438b12f441024efbae78348a1cbbafd9cd9d970"},"cell_type":"code","source":"#plot feature importance XGBoost\nplot_importance(xgb)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13746eb8a9570af0b41082b5c8b10368edfe7d3f"},"cell_type":"markdown","source":"XGBoost puts narrow-gills and odourless as it's top predictors of edibility--this makes sense after exploring our data and knowing how mushrooms are ID'd. Next we'll run the same analysis but with a Random Forest. <a name=\"trees\"></a>\n\n### Random Forest Classifier"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:08:45.740153Z","start_time":"2018-10-03T23:08:45.671129Z"},"trusted":true,"_uuid":"eb123ed93f0d987f69dcf203a6d6a19a6e8b97aa"},"cell_type":"code","source":"#fitting a random forest\nrf = RandomForestClassifier()\nrf.fit(Xo_train, yo_train)\nprint(\"Default RFR: %3.1f\" % (rf.score(Xo_test, yo_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecf181612f08c77f77e76300d3bff91328ce8fb7"},"cell_type":"markdown","source":"The default Random Forest achieves a 100% accuracy as well, but will assign different weights and importance to features. Next, we'll run a Grid Search with 10-fold cross-validation to observe optimal parameters--this will illustrate how to tune for hyperparameters and to avoid overfitting. "},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:08:59.052071Z","start_time":"2018-10-03T23:08:59.044697Z"},"trusted":true,"_uuid":"f7f23375651d6e475a87b4138857efe81eee655a"},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [50, 100, 200],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:23:02.070672Z","start_time":"2018-10-03T23:18:23.102909Z"},"trusted":true,"_uuid":"67a3bf6397d50c029c8241a8fd6166ac7c1f4e96"},"cell_type":"code","source":"CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 10)\nCV_rfc.fit(Xo_train, yo_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:30:28.669496Z","start_time":"2018-10-03T23:30:28.644071Z"},"trusted":true,"_uuid":"c765671ce3da0b77d585666a22579852df0b3bb1"},"cell_type":"code","source":"CV_rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:35:30.364923Z","start_time":"2018-10-03T23:35:30.219303Z"},"trusted":true,"_uuid":"71ad905c0c0e83ebdbaa313be72be24dd8795fda"},"cell_type":"code","source":"rfcv = RandomForestClassifier(criterion= 'gini',\n max_depth= 6,\n max_features= 'auto',\n n_estimators= 50)\nrfcv.fit(Xo_train, yo_train)\nprint(f'GridSearchCV RFR: {(rfcv.score(Xo_test, yo_test)*100)}%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed037407c56fad37f8c5daef233219ee694162c"},"cell_type":"markdown","source":"The optimized Random Forest classifier still achieves a 100% accuracy, which is to be expected. Let's visualize the difference between feature importance."},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:33:35.657482Z","start_time":"2018-10-03T23:33:35.645179Z"},"trusted":true,"_uuid":"1b72a08a7c9d87e8afaaa984f032140062fe6afb"},"cell_type":"code","source":"feature_imp = pd.Series(rfcv.feature_importances_,index=Xo_train.columns).sort_values(ascending=False)\nprint(feature_imp)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-03T23:37:14.798711Z","start_time":"2018-10-03T23:37:14.193763Z"},"trusted":true,"_uuid":"4b7564bf54b4ee126b01ee6e0b84f817a48fbbd5"},"cell_type":"code","source":"#plot feature importance for RFR\nplt.figure(figsize=(12,8))\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.title('Random Forest Feature Importance');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"948a68507db2ba0e1ba92ed3db066579ee3adf2d"},"cell_type":"markdown","source":"The features are slightly different than XGBoost, however the first two, odourless and narrow-gilled are still the strongest predictors. \n\nThat sums up our look at Mushroom Classifications. Happy foraging!"},{"metadata":{"trusted":false,"_uuid":"a4bfc9521d5b20e23336917cef01fa95928dd909"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}