{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip uninstall tensorflow -y","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T22:12:25.67372Z","iopub.execute_input":"2021-08-19T22:12:25.674054Z","iopub.status.idle":"2021-08-19T22:12:27.411097Z","shell.execute_reply.started":"2021-08-19T22:12:25.674024Z","shell.execute_reply":"2021-08-19T22:12:27.410072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install tensorflow-gpu==2.6","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T22:12:27.414546Z","iopub.execute_input":"2021-08-19T22:12:27.414815Z","iopub.status.idle":"2021-08-19T22:12:34.007505Z","shell.execute_reply.started":"2021-08-19T22:12:27.414786Z","shell.execute_reply":"2021-08-19T22:12:34.006497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T22:12:34.011369Z","iopub.execute_input":"2021-08-19T22:12:34.011637Z","iopub.status.idle":"2021-08-19T22:12:34.020953Z","shell.execute_reply.started":"2021-08-19T22:12:34.011609Z","shell.execute_reply":"2021-08-19T22:12:34.020065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing all libreries we need.","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pearsonr, spearmanr\nimport seaborn as sns\nimport requests\n!pip install openpyxl \n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:12:34.022779Z","iopub.execute_input":"2021-08-19T22:12:34.023259Z","iopub.status.idle":"2021-08-19T22:12:40.207211Z","shell.execute_reply.started":"2021-08-19T22:12:34.023221Z","shell.execute_reply":"2021-08-19T22:12:40.206196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A little code that better managing the cach memory use by dictionary cell","metadata":{}},{"cell_type":"code","source":"# Managinig the place dic in Cach memory \n\ncache = dict()\ndef get_article_from_server(url):\n    print(\"Fetching article from server...\")\n    response = requests.get(url)\n    return response.text\ndef get_article(url):\n    print(\"Getting article...\")\n    if url not in cache:\n        cache[url] = get_article_from_server(url)\n    return cache[url]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:12:40.21055Z","iopub.execute_input":"2021-08-19T22:12:40.210828Z","iopub.status.idle":"2021-08-19T22:12:40.215957Z","shell.execute_reply.started":"2021-08-19T22:12:40.210799Z","shell.execute_reply":"2021-08-19T22:12:40.215168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing 3 tables ","metadata":{}},{"cell_type":"code","source":"# Prepairing Datas @ Louding of Datas\n# Prepairing Datas @ Louding of Datas\n# Most of the work is to work on the Data.\n\nx_features = pd.read_excel(\"../input/datas-features/Features data set.xlsx\") # features \ny_sales = pd.read_excel('../input/datas-features/sales data-set.xlsx') # labels\nz_stores = pd.read_csv(\"../input/datas-features/stores data-set.csv\") # stores\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:12:40.217601Z","iopub.execute_input":"2021-08-19T22:12:40.218242Z","iopub.status.idle":"2021-08-19T22:13:28.084861Z","shell.execute_reply.started":"2021-08-19T22:12:40.218206Z","shell.execute_reply":"2021-08-19T22:13:28.083975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inquering all Datas\n\nprint(x_features.head())\nprint(y_sales.head())\nprint(z_stores.head())\nprint('-'*100)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:28.086057Z","iopub.execute_input":"2021-08-19T22:13:28.086425Z","iopub.status.idle":"2021-08-19T22:13:28.102885Z","shell.execute_reply.started":"2021-08-19T22:13:28.086388Z","shell.execute_reply":"2021-08-19T22:13:28.102027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"\"\"\n# After looking the Datas .\n# I choose as best as I can the featurs that fit the economic aspects predict.\n# Following this vision I don't take the data of the size stores - it doesn't fit the predict economic sales problem.\n\"\"\"","metadata":{}},{"cell_type":"code","source":"# The groupby drop all missing rows\n# Merging all Datas\n# Merging all Datas\n# As an inner mean - marge just the common Datas.\n\ny_sales['Date'] = pd.to_datetime(y_sales['Date'])\n# optimize the sales via store in a unique date with all Deparments to be preper \ny_sales = y_sales.groupby(['Store', 'Date']).sum()\n# we have to repeat its again to fit x_features y_sales \nx_features['Date'] = pd.to_datetime(x_features['Date'])\n\nx_features = x_features.groupby(['Store', 'Date']).sum()\n\n\nprint(x_features.dtypes)\nprint('-'*100)\nprint('\\n')\nprint(y_sales.dtypes)\nprint('-'*100)\nprint('\\n')\nprint(z_stores.dtypes)\nprint('-'*100)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:28.106052Z","iopub.execute_input":"2021-08-19T22:13:28.106673Z","iopub.status.idle":"2021-08-19T22:13:28.258239Z","shell.execute_reply.started":"2021-08-19T22:13:28.106632Z","shell.execute_reply":"2021-08-19T22:13:28.25741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Combined_table = pd.merge(x_features, y_sales['Weekly_Sales'], how='inner', right_index=True, left_index=True)\n# Inqueries with code.\nCombined_table.isna().sum()\nCombined_table.info\n\nCombined_table['Weekly_Sales'].describe()\n# Very well... It seems that minimum price is larger than zero. Excellent!","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:28.260225Z","iopub.execute_input":"2021-08-19T22:13:28.260748Z","iopub.status.idle":"2021-08-19T22:13:28.35026Z","shell.execute_reply.started":"2021-08-19T22:13:28.260711Z","shell.execute_reply":"2021-08-19T22:13:28.349317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inquery with Plotting !\n# ------------------------","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:28.351637Z","iopub.execute_input":"2021-08-19T22:13:28.351988Z","iopub.status.idle":"2021-08-19T22:13:28.355996Z","shell.execute_reply.started":"2021-08-19T22:13:28.351951Z","shell.execute_reply":"2021-08-19T22:13:28.354857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  --  First Plot as histogram plot.","metadata":{}},{"cell_type":"code","source":"# 1) histogram plot\n# let see how Weekly_Sales corellative to other features ?\n# sns.distplot(Combined_table['Weekly_Sales'])\nsns.displot(data = Combined_table, x='Weekly_Sales', kde=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:28.357685Z","iopub.execute_input":"2021-08-19T22:13:28.358231Z","iopub.status.idle":"2021-08-19T22:13:29.07556Z","shell.execute_reply.started":"2021-08-19T22:13:28.358195Z","shell.execute_reply":"2021-08-19T22:13:29.074561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1) Deviate from the normal distribution.\n# 2) Have appreciable positive skewness.\n# 3) Have appreciable positive skewness.\n\nprint(\"Skewness: %f\" % Combined_table['Weekly_Sales'].skew())\nprint(\"Kurtosis: %f\" % Combined_table['Weekly_Sales'].kurt())\n\n\n#  --  I try to inquery Datas by Skewness and Kurtosis ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:29.080928Z","iopub.execute_input":"2021-08-19T22:13:29.083946Z","iopub.status.idle":"2021-08-19T22:13:29.094814Z","shell.execute_reply.started":"2021-08-19T22:13:29.083906Z","shell.execute_reply":"2021-08-19T22:13:29.092373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  --  Second plot as box plot","metadata":{}},{"cell_type":"code","source":"# 2) box plot store / Weekly_Sales\n# first convert the store from index into a column\n# Here I Takea a look over each sotre outlayers via \"Weekly_Sales'.\n# It's other way to look the components behind the features.\n\nCombined_table = Combined_table.reset_index(level=0)\nvar = 'Store'\ndata = pd.concat([Combined_table['Weekly_Sales'], Combined_table[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Weekly_Sales\", data=data)\nfig.axis(ymin=180000, ymax=4000000)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:29.099253Z","iopub.execute_input":"2021-08-19T22:13:29.099622Z","iopub.status.idle":"2021-08-19T22:13:30.501957Z","shell.execute_reply.started":"2021-08-19T22:13:29.099596Z","shell.execute_reply":"2021-08-19T22:13:30.501159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"#  -- Thierd plot - comparative features","metadata":{}},{"cell_type":"code","source":"# 3) Plotting all the features via the label (prediction of sales)\nCombind_graf = Combined_table.copy()\nCombind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()\n\n# After looking this plot there in no feature that could be pridictable for 'Weekly_Sales'.\n# But at the end we will see that all features together could pridictable.\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:30.503238Z","iopub.execute_input":"2021-08-19T22:13:30.503569Z","iopub.status.idle":"2021-08-19T22:13:32.991768Z","shell.execute_reply.started":"2021-08-19T22:13:30.503539Z","shell.execute_reply":"2021-08-19T22:13:32.990823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#  --  4th Confusionall Correlation metrix ","metadata":{}},{"cell_type":"code","source":"# 4) Plotting Confusional Correlation matrix between numerical values\n# In the following Matrix we will see how mach the features are confusing?.\n# As we see the features are not corelative, except two. \ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()\n#The Pearson accurecy is around 90%","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:32.993217Z","iopub.execute_input":"2021-08-19T22:13:32.993569Z","iopub.status.idle":"2021-08-19T22:13:33.713306Z","shell.execute_reply.started":"2021-08-19T22:13:32.993535Z","shell.execute_reply":"2021-08-19T22:13:33.712539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  --  5th Confusional Correlation matrix after reducing one of the colerative feature.","metadata":{}},{"cell_type":"code","source":"# 5) Plotting Confusional Correlation matrix between numerical values with no features correlative\n# Since I have two Prameters with correlasition, I drop 'MarkDown1', Why ?\n# To prevent  Linkage featurs that are corelatived.\n# Once I reduse feature the training forced being better.\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()\n\n#Finally The linckage is no so big.\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:33.714697Z","iopub.execute_input":"2021-08-19T22:13:33.715056Z","iopub.status.idle":"2021-08-19T22:13:34.342224Z","shell.execute_reply.started":"2021-08-19T22:13:33.715019Z","shell.execute_reply":"2021-08-19T22:13:34.341432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  -- 6th Plotting all features ","metadata":{}},{"cell_type":"code","source":"# 6) scatterplot\n# let see what happend in with any feature \nsns.set()\ncols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']\nsns_plot = sns.pairplot(Combind_graf[cols].sample(250), height = 2.5) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:13:34.343498Z","iopub.execute_input":"2021-08-19T22:13:34.343835Z","iopub.status.idle":"2021-08-19T22:19:51.893004Z","shell.execute_reply.started":"2021-08-19T22:13:34.3438Z","shell.execute_reply":"2021-08-19T22:19:51.891899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nprint('Just to be on the safe side I take the correlation Pearson as a Metric')\n\"\"\"\n# =====================================================================================================\n# Economic vision analization was taken  ----\n# I analize only two table : features and sales arreies, to predict future weekly sales  growing.\n# Union Datas here are with an economic vision and not blind merge Datas.\n# We can later adding Stores and Some parmeters to mesure efficiency  per m*m\n\n# The loss model is almost 10**11 (train and val was fitten) , mean that the model is good !!.\n# No fitting and no under fitting.\n# It is hardly train and practice small Datas. even that the prediction was done good. \n# In mean while when i Take Pearson correlation the correlation is around 88%-93%.\n# prediction label is not 100%, mean no fiiting\n\n# As a model of economic predicts, I can't do any augmantation with no meaning.\n# just to \"fix\" Pearson correlation. \n# Even no augmantation Model Dence Deep Learning give us an excelent predicts and answers .\n\n\n# ------------------------------------------------------------------------------------------------------\n# =======================================================================================================\n\"\"\"","metadata":{}},{"cell_type":"code","source":"\n# Let see the first 10 variable\n\nprint(\"Dats shape = {}\".format(Combined_table.shape))\nprint()\nprint(\"Lets see some feature:\")\nprint(Combined_table[1:10])\n\nCombined_table = Combined_table.drop(['MarkDown1'], axis=1)\n \n# Definition of Y (label) and X (inputs)\n\n# Define label for the new merge table: \"combined_table\"\n# The 'Weekly_Sales' isIndexial so it dosn't take as a label\ny = Combined_table['Weekly_Sales']\n# Define features for the new merge table: \"combined_table\"\nx = Combined_table.drop(['Weekly_Sales'], axis=1)\n\n\n\n# Changing Farenhight to Celcius temperature.\n\nCombined_table['Temperature'] = (Combined_table['Temperature']- 32) * 5./9.\n\nprint(Combined_table)\nprint('-'*100)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:19:51.894421Z","iopub.execute_input":"2021-08-19T22:19:51.894747Z","iopub.status.idle":"2021-08-19T22:19:51.931965Z","shell.execute_reply.started":"2021-08-19T22:19:51.894714Z","shell.execute_reply":"2021-08-19T22:19:51.931063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that I filtering and Inquery we can go to preprocessing and Training.","metadata":{}},{"cell_type":"markdown","source":"Making NormalizationMaking Normalization","metadata":{}},{"cell_type":"code","source":"# preprocessing normalization values between 0 and 1\n\nscaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)\ny.shape\ny = y.values.reshape(6435, 1)\ny_scaled = scaler.fit_transform(y)\n\nx.head()\nx.tail()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:19:51.933325Z","iopub.execute_input":"2021-08-19T22:19:51.933672Z","iopub.status.idle":"2021-08-19T22:19:51.961229Z","shell.execute_reply.started":"2021-08-19T22:19:51.933637Z","shell.execute_reply":"2021-08-19T22:19:51.960266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the Datas in train, valid, and test.","metadata":{}},{"cell_type":"code","source":"# splitting the Datas to train @ valid @ test \n# we need to take x_scaled after being notmalized\n# In the first step we will split the data in training and remaining dataset\nx_train, x_valid, y_train, y_valid = train_test_split(x_scaled, y, train_size=0.80, random_state=42)\n\n\n\n# Now since we want the valid and test size to be equal (10% each of overall data). \n# we have to define valid_size=0.5 (that is 50% of remaining data)\ntest_size = 0.5\nx_test, x_valid, y_test, y_valid = train_test_split(x_scaled,y, test_size=0.5)\n\n\n# we need to add an additional dimantion to get numpy arrey to keras\n\nx_train = x_train.reshape(x_train.shape[0],1,x_train.shape[1]) \nx_valid = x_valid.reshape(x_valid.shape[0],1,x_valid.shape[1])\nx_test = x_test.reshape(x_test.shape[0],1,x_test.shape[1]) \n\n\nTest_Data = (x_test, y_test)\n\nprint('-'*100)\nprint('\\n')\n\n\nprint(x_train.shape), print(y_train.shape)\nprint(x_valid.shape), print(y_valid.shape)\nprint(x_test.shape), print(y_test.shape)\n\n\nprint('-'*100)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:19:51.962589Z","iopub.execute_input":"2021-08-19T22:19:51.962982Z","iopub.status.idle":"2021-08-19T22:19:51.980309Z","shell.execute_reply.started":"2021-08-19T22:19:51.962901Z","shell.execute_reply":"2021-08-19T22:19:51.979214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Input prameters for LSTM Model","metadata":{}},{"cell_type":"code","source":"# Training of the basic LSTM model\n# Training of the basic LSTM model\n\n# Initializing the Recurrent Neural Network AS LSTM\ninputs = tf.random.normal([32, 50, 10])\nmodel = Sequential()\n\n#Adding the first LSTM layer with a sigmoid activation function and some Dropout regularization\n#Units - dimensionality of the output space\n\n\nmodel.add(LSTM(units = 32, return_sequences = False, input_shape =(1,x_train.shape[2])))\n# Adding the output layer\nmodel.add(Dense(units = 128))\nmodel.add(Dense(units = 64))\nmodel.add(Dense(units = 1, activation=\"relu\", input_shape=(4,)))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:19:51.982206Z","iopub.execute_input":"2021-08-19T22:19:51.982757Z","iopub.status.idle":"2021-08-19T22:19:52.218816Z","shell.execute_reply.started":"2021-08-19T22:19:51.98271Z","shell.execute_reply":"2021-08-19T22:19:52.217986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training of the basic LSTM model","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters and then Model Dense (Deep Learning) ","metadata":{}},{"cell_type":"code","source":"# ----- Hyperparameters\n# learning rate need to be check !!\n# Adam was config as Adaptive Learning Rate Methods\nopt = tf.keras.optimizers.Adam(learning_rate = 0.1, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n# model.compile(loss = 'mean_absolute_error', metrics=[soft_acc], optimizer=opt)\nmodel.compile(loss = 'mse', optimizer=opt)\n\n# A logger was created for logs the best whieghts of the training\nmy_callbacks = [tf.keras.callbacks.ModelCheckpoint(save_best_only=True, filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n    tf.keras.callbacks.TensorBoard(log_dir='./logs')]\n\n\n# Create a TensorBoard logger need to becheck\nlogger = tf.keras.callbacks.TensorBoard(log_dir='logs', write_graph=True,\n    histogram_freq=5)\n\n\nhistory = model.fit(x_train,y_train,epochs = 1000, batch_size = 16, validation_data=(x_valid,y_valid), callbacks=my_callbacks)\n\n\np = history.history['loss']\nprint('-'*100)\nprint('\\n')\n# list all data in history\nprint(history.history.keys())\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:19:52.220244Z","iopub.execute_input":"2021-08-19T22:19:52.220612Z","iopub.status.idle":"2021-08-19T22:37:18.240468Z","shell.execute_reply.started":"2021-08-19T22:19:52.220575Z","shell.execute_reply":"2021-08-19T22:37:18.23953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6th Plot of Training and Validation","metadata":{}},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.legend(['x_train', 'x_valid'], loc='upper left')\nplt.legend(['y_train', 'y_valid'], loc='upper left')\n\n\n#     PLOT Traing and Validation\n# No Fitting, no under Fitting.","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:37:18.244014Z","iopub.execute_input":"2021-08-19T22:37:18.244266Z","iopub.status.idle":"2021-08-19T22:37:18.484036Z","shell.execute_reply.started":"2021-08-19T22:37:18.244239Z","shell.execute_reply":"2021-08-19T22:37:18.483008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ****   Prediction   *****\n# # ****   Prediction   *****\n# # ****   Prediction   *****\n\n\n\n# verification of the prediction\n# ===============================\n\n\n# for Validation\npreds_val = model.predict(x_valid)\npreds_val = preds_val.squeeze()\nresult_val = y_valid - preds_val\n\n# for Test\npreds_test = model.predict(x_test)\npreds_test = preds_test.squeeze()\nresult_test = y_test - preds_val\n\n\n# Forecasting Accuracy = Pearson\n# Spearman was checked for correlation, but Spearman is better for this problem.\n# ------------------------------------------------------------------------------\n# pearson correlation first way\n\ndef correlation_coefficient_var(y_valid, preds_coef_val):\n    pearson_r_val = tfp.stats.correlation(preds_coef_val, y_valid)\n    return(pearson_r_val)\n    print(pearson_r_val)\n    \n    \ndef correlation_coefficient_test(y_test, preds_coef_test):\n    pearson_r_test = tfp.stats.correlation(preds_coef_test, y_test)\n    return(pearson_r_test)\n    print(pearson_r_test)\n    \n\n\n\nprint('-'*100)\nprint('\\n')\n\npreds_val = preds_val.reshape(-1, 1)\ny_valid = y_valid.astype('float32')\nprint(correlation_coefficient_var(y_valid, preds_val))\n\npreds_test = preds_test.reshape(-1, 1)\ny_test = y_test.astype('float32')\nprint(correlation_coefficient_test(y_test, preds_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:37:18.485725Z","iopub.execute_input":"2021-08-19T22:37:18.486083Z","iopub.status.idle":"2021-08-19T22:37:19.283166Z","shell.execute_reply.started":"2021-08-19T22:37:18.486047Z","shell.execute_reply":"2021-08-19T22:37:19.282345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction","metadata":{}},{"cell_type":"code","source":"# 7) Plotting the label prediction of sales.\nCombind_graf = Combined_table.copy()\nCombind_graf[['Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.title('model Weekly sales predict')\nplt.ylabel('Income')\nplt.xlabel('Weekly Time Sales')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:37:19.284405Z","iopub.execute_input":"2021-08-19T22:37:19.284879Z","iopub.status.idle":"2021-08-19T22:37:19.794507Z","shell.execute_reply.started":"2021-08-19T22:37:19.284834Z","shell.execute_reply":"2021-08-19T22:37:19.793527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7th plot of the 'weekly_Sales to comperative with the beginning plots","metadata":{}},{"cell_type":"code","source":"\n(print(preds_val))\ncorr, p_val = pearsonr(y_valid.squeeze(), preds_val.squeeze())\nprint('Pearson corr validation')\nprint(corr)\n\n\n(print(preds_test))\ncorr, p_test = pearsonr(y_test.squeeze(), preds_test.squeeze())\nprint('Pearson corr test')\nprint(corr)\n\n#==============================================================================\n# Via Pearson on Validation and Test verificatuin after  lots of epocs.\n# WE could declair that the model prediction fits the Datas training \n# Aproximitly around 88% to 92% Pearson accurecy.\n#==============================================================================\nprint('Via Pearson on Validation and Test verificatuin after lots of epocs.')\nprint('WE could declair')\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T22:37:19.79595Z","iopub.execute_input":"2021-08-19T22:37:19.796307Z","iopub.status.idle":"2021-08-19T22:37:19.80797Z","shell.execute_reply.started":"2021-08-19T22:37:19.796257Z","shell.execute_reply":"2021-08-19T22:37:19.806934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"\"\"\n#                           Summary\n# ==================================================================\n# Finally we saw sequential Loss 'validation' via 'training' even:\n# 1) All the faurues arn't correlative each on to others.                           \n# 2) All features stores are with outlayers.                                    \n# 3) Correlation graph are not a simetric Gaussian.                                 \n# 4) Small Datas.                                                                    \n# 5) Big numbers sales cause big differances complex calaulation.       \n# Loss 'test' is sequential via 'validation'.\n# so: 'Training'-->'Validation'-->'Test'---->sequentially. \n# ++++++++++++++++++++++++++  Conclussiotion  +++++++++++++++++++++++\n#        Model: Dense Deep Learning with LSTM work very good  !!!!!!                 \n# ===================================================================\n# Prediction predict that the weekly sales will be the same'\n# In the future as it were in the past in accurecy of 88%-93%.\n# Base on this model !! looking via all features together.\n# Else none of the features alone could be pridictable !!!.\n# ===================================================================\n\"\"\"","metadata":{}},{"cell_type":"code","source":"\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}