{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Churn Model Prediction"},{"metadata":{},"cell_type":"markdown","source":"## Import the libraries needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignore local warnings\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n# Model packages\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.wrappers.scikit_learn import KerasClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/churn-modelling\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"Churn_Modelling.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing data in the dataset. We will proceed with exploring each of the feature and try to find the best approach to each of the features.\n\nWe won't need to include the first three columns in our model since \"CustomerId\", \"Surname\" and \"RowNumber\" do not  have any logical contribution to our prediction."},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, 3:-1].values\ny = df.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Categorical Variable Encoding\n\nWe will encode two categorical variables: \"Geography\" and \"Gender\".\nThe way to encode feature \"Geography\" is different from \"Gender\" as we only have two possible values for \"Gender\" (Female or Male). We can simply encode one of the to be \"1\" and the other one to be \"0\". But in \"Geography\" case, we have more than two possible values. If we encode this feature the same way as \"Gender\" there will be some numerical order in the value of this feature which would create some confusion in our model (we do not want to create a correlation between the numerical order in the feature and our target variable) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender\nle = LabelEncoder()\nX[:,2] = le.fit_transform(X[:,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Geography\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1])] , remainder = \"passthrough\")\nX = np.array(ct.fit_transform(X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Scaling\n\nWe will need to scale some of the features (some numerical features). The reason we are doing this is because we do not want one feature to overweight the other because of their relatively bigger range number. We will use methods of standardization to scale some of the numerical values.\n\nHere's the formula for standardization:\n\n#### $ x_{standardized} = \\frac{x - \\mu(X)}{\\sigma(X)} $"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train[:, [3,5,6,7,8,11]] = scaler.fit_transform(X_train[:, [3,5,6,7,8,11]])\nX_test[:, [3,5,6,7,8,11]] = scaler.transform(X_test[:, [3,5,6,7,8,11]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying different classification models\n### 1) XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_1 = XGBClassifier()\nclassifier_1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_1 = classifier_1.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_1 = cross_val_score(estimator = classifier_1, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_1.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_1.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_2 = RandomForestClassifier(n_estimators = 100, random_state = 0)\nclassifier_2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_2 = classifier_2.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_2 = cross_val_score(estimator = classifier_2, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_2.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_2.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_3 = GaussianNB()\nclassifier_3.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_3 = classifier_3.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_3 = cross_val_score(estimator = classifier_3, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_3.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_3.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4) Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_4 = LogisticRegression(random_state = 0)\nclassifier_4.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_4 = classifier_4.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_4 = cross_val_score(estimator = classifier_4, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_4.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_4.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5) K-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_5 = KNeighborsClassifier(n_neighbors = 10, metric= \"minkowski\", p=2)\nclassifier_5.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_5 = classifier_5.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_5 = cross_val_score(estimator = classifier_5, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_5.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_5.std()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6) Artifical Neural Network"},{"metadata":{},"cell_type":"markdown","source":"#### Initializing"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann = tf.keras.models.Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Adding layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\", input_dim=12)) #first layer\nann.add(tf.keras.layers.Dense(units=6, activation=\"relu\")) #Second layer \nann.add(tf.keras.layers.Dense(units=6, activation=\"relu\")) #Third layer\nann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\")) #Output layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compile"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.compile(optimizer=\"adam\", loss= \"binary_crossentropy\", metrics= [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann.fit(X_train, y_train, batch_size=32, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_6 = ann.predict(X_test)\ny_pred_6 = (y_pred_6 > 0.5)\nprint(confusion_matrix(y_test, y_pred_6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_6 = accuracy_score(y_test, y_pred_6)\nprint(\"Accuracy: {:.2f}%\".format(acc_6*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are so many improvement that could be made such as tuning the hyperparameter, adding extra layers (for ANN), etc. All in all, the best classification algorithm that we get for now are Artifical Neural Network (ANN) and Random Forest Classifier, with accuracy score of approximately 86%."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}