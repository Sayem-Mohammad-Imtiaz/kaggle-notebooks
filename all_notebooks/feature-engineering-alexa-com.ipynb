{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"nRowsRead = 12203 # specify 'None' if want to read whole file\n# alexa.com_site_info.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf = pd.read_csv('/kaggle/input/sites-information-data-from-alexacom-dataset/alexa.com_site_info.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'alexa.com_site_info.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\n\ncount = 0\nfor i in range(len(df['keyword_opportunities_breakdown_optimization_opportunities'])):\n    if df['keyword_opportunities_breakdown_optimization_opportunities'][i] == 'NaN':\n        count += 1\n        print([i])\nprint(count)\n\n# if (any(is.na(df[,'keyword_opportunities_breakdown_optimization_opportunities']))) {next}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = 0\n# for item in df:\n#     for row in item:\n#         try:\n#             int(row)\n#             df.loc[cnt, item] = np.nan\n#         except ValueError:\n#             pass\n#     cnt += 1\n#     print(\" is null of item : \")\n#     print(df[item].isnull())\n\n\n# for row in df['keyword_opportunities_breakdown_optimization_opportunities']:\n#     try:\n#         int(row)\n#         df.loc[cnt, 'keyword_opportunities_breakdown_optimization_opportunities'] = np.nan\n#     except ValueError:\n#         pass\n# cnt += 1\n\n\n# print(\"changed item : \")\n# print(df[item])\n# print(\" is null of item : \")\n# print(df['keyword_opportunities_breakdown_optimization_opportunities'].isnull())\n\nnumeric_columns = []\nstr_counter = 0\nfor item in df:\n    try:\n        y = 0\n        x = df[item][1] * 2\n        z = x + y\n        numeric_columns.append(item)\n    except:\n        str_counter += 1\nprint(str_counter)\n\n# for i in range(len(numeric_columns)):\n#     for j in range(len(df[numeric_columns[i]].isnull())):\n#         try:\n#             # print(df['keyword_opportunities_breakdown_optimization_opportunities'].isnull()[i])\n#             if df[numeric_columns[i]].isnull()[j] == True:\n#                 df[numeric_columns[i]][j] = min(numeric_columns[i])\n#                 print(min(numeric_columns[i]))\n#     #             print(df[numeric_columns[i]][j])\n#             else:\n#                 df[numeric_columns[i]][j] = int(df[numeric_columns[i]][j])\n#         except:\n#             pass\ndf = df.fillna(df.mean())\n\nfor i in range(len(numeric_columns)):\n    for item in df[numeric_columns[i]].isnull():\n        try:\n            if item == True:\n                print(\"shit\")\n        except:\n            pass\n\n# for i in range(len(numeric_columns)):\n#     df[numeric_columns[i]].dtypes\n# for i in range(len(df['keyword_opportunities_breakdown_optimization_opportunities'].isnull())):\n#     if df['keyword_opportunities_breakdown_optimization_opportunities'].isnull()[i] == True:\n#         print(\"null item : \")\n#         print(item)\n\n# count = 0\n# for i in range(len(df['keyword_opportunities_breakdown_optimization_opportunities'])):\n#     if df['keyword_opportunities_breakdown_optimization_opportunities'][i] == 'NaN':\n#         count += 1\n# #         print([i])\n# print(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.dtypes\nnumeric_columns = []\nstr_counter = 0\nfor item in df:\n    try:\n        y = 0\n        x = df[item][1] * 2\n        z = x + y\n        numeric_columns.append(item)\n    except:\n        str_counter += 1\nprint(str_counter)\n\n# for i in range(len(numeric_columns)):\n#     if df[numeric_columns[i]].dtypes == 'int64':\n#         df[numeric_columns[i]].dtypes = 'float64'\n#         print(df[numeric_columns[i]].dtypes)\nfor i in range(len(numeric_columns)):\n    if df[numeric_columns[i]].dtypes == 'int64':\n        for j in range(len(df[numeric_columns])):\n            df[numeric_columns[i]][j] = np.float64(df[numeric_columns[i]][j])\n        \n        print(df[numeric_columns[i]].dtypes)\n\n            \nfor i in range(len(numeric_columns)):\n    if df[numeric_columns[i]].dtypes == 'int64':\n        print(df[numeric_columns[i]].dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature extraction is a process of dimensionality reduction**\n\nPrincipal Component Analysis (PCA) helps reduce dimensions by creating a new set of variables that are smaller than the original set without losing any information\n\nThis efficient reduction of the number of variables is achieved by obtaining orthogonal linear combinations of the original variables â€“ the so-called Principal Components (PCs).\n\nPCA is useful for the compression of data and to find patterns in high-dimensional data.\n\nPCA doesn't improve the performance of Random Forests much because Random Forests check feature importance and builds a model based on the important features\n\nlet's run a logistic regression model instead on the data\n\nthen run another logistic regression model on the data after we have reduced the number of dimensions using PCA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\ncounter = 0\nfor item in df:\n    if item not in numeric_columns:\n        del(df[item])\n        counter += 1\nprint(counter)\n\nfor item in numeric_columns:\n    x, y = df.loc[:, df.columns != item], df[item]\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n    logreg = LogisticRegression(solver='lbfgs')\n    try:\n        logreg.fit(x_train, y_train)\n        y_pred = logreg.predict(x_test)\n        if logreg.score(x_test, y_test) > 0.5:\n            print(x_train.shape, y_train.shape)\n            print(x_test.shape, y_test.shape)\n            print('category : ')\n            print(item)\n            print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale\nx = df.values #convert the data into a numpy array\nx = scale(x);x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ncovar_matrix = PCA(n_components = 20) #we have 20 features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covar_matrix.fit(x)\nvariance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\nvar #cumulative sum of variance explained with [n] features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.ylabel('% Variance Explained')\nplt.xlabel('20 Features')\nplt.title('PCA Analysis')\nplt.ylim(0,110)\nplt.style.context('seaborn-whitegrid')\n\n\nplt.plot(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=13)\n\nfrom sklearn.feature_selection import SelectKBest\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_union\ncombined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n\n# Use combined features to transform dataset:\nx_features = combined_features.fit(x, y).transform(x)\nprint(\"Combined space has\", x_features.shape[1], \"features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_features, y, test_size=0.2)\nprint (x_train.shape, y_train.shape)\nprint (x_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(solver='lbfgs')\nlogreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(x_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**dealing with target labels**\n\nIn this case, our target labels are imbalanced: we have a lot more creditable people than not-creditable people because that's how lending works (risk averse)\n\nLet's try to best deal with an imbalanced dataset\n\n**Method 1:**\ndon't use accuracy as a metric\n\n**other methods:**\n\nconfusion matrix\n\nprecision\n\nrecall","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"recall = tp / (tp + fn) : ability to find true positives\n\nprecision = tp / (tp + fp) : ability to correctly label a sample as positive","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom scipy.io import arff\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nimport scikitplot as skplt\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_union\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import LabelBinarizer # one hot encoding\nfrom sklearn.preprocessing import PolynomialFeatures # add polynomial features\nfrom sklearn.metrics import classification_report # classification report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr = df.corr(method ='pearson')\n# sns.heatmap(corr, \n#             xticklabels=corr.columns.values,\n#             yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline model : ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Let's remove these variables and see if model performance changes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx, y = df.drop(['This_site_rank_in_global_internet_engagement', 'all_topics_easy_to_rank_keywords_relevance_to_site_parameter_1', 'audience_overlap_sites_overlap_scores_parameter_1'], axis =1), df['Daily_time_on_site']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nlogreg = LogisticRegression(solver='lbfgs')\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))\npd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_of_logistic_regression():\n    numeric_columns = []\n    str_counter = 0\n    for item in df:\n        try:\n            y = 0\n            x = df[item][1] * 2\n            z = x + y\n            numeric_columns.append(item)\n        except:\n            str_counter += 1\n    print(str_counter)    \n    \n    best_col = []\n    for item in numeric_columns:\n        x, y = df.loc[:, df.columns != item], df[item]\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n        logreg = LogisticRegression(solver='lbfgs')\n        try:\n            logreg.fit(x_train, y_train)\n            y_pred = logreg.predict(x_test)\n    #         print(logreg.score(x_test, y_test))\n            if logreg.score(x_train, y_train) > 0.5:\n                best_col.append(item)\n                print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_train, y_train)))\n                pd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])\n                print(classification_report(y_test, y_pred))\n        except:\n            pass\n    return best_col\nprint(accuracy_of_logistic_regression())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# imputing missing values\nalexa.com siteinfo dataset\n\nmissing values are marked as minimum value in column\n\n**columns with missing values:\n* all_topics_keyword_gaps_Avg_traffic_parameter_1\n\n* all_topics_buyer_keywords_Avg_traffic_parameter_1'\n\n* audience_overlap_sites_overlap_scores_parameter_4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = []\nstr_counter = 0\nfor item in df:\n    try:\n        y = 0\n        x = df[item][1] * 2\n        z = x + y\n        numeric_columns.append(item)\n    except:\n        str_counter += 1\nprint(str_counter)\n\nfrom ipykernel import kernelapp as app\n\n\n\nfor item in df:\n#     df[item].fillna(df[item].mean())\n    if item not in numeric_columns:\n        del(df[item])\n        \nprint(df)\n\n        \nfor item in df:\n        df[item] = pd.to_numeric(df[item], errors='coerce')\n        df = df.dropna(subset=[item])\n        df[item] = df[item].astype(int)\n#         # if type(df[item][i]) is str:\n        \n#         if df[item][i] != 'NaN':\n#             df[item][i] = int(df[item][i])\n# #             print(df[item][i])\n\n\nX, y = df.loc[:, df.columns != 'all_topics_buyer_keywords_Avg_traffic_parameter_1'], df['all_topics_buyer_keywords_Avg_traffic_parameter_1']\n\n# for i in range(len(X)):\n#     print(X[i])\n#     X[i] = int(X[i])\n    \nprint('X : ')\nprint(X)\nprint('----------------------------')\nprint('y : ')\nprint(y)\nprint('----------------------------')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlogreg = LogisticRegression()\nprint('X_train : ')\nprint(X_train)\nprint('----------------------------')\n\nprint('y_train : ')\nprint(y_train)\nprint('----------------------------')\n\n\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\npd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# impute missing values using the mean of the column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmissing_columns = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']]\nmissing_columns = missing_columns.replace(0, np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'all_topics_keyword_gaps_Avg_traffic_parameter_1' , 'all_topics_buyer_keywords_Avg_traffic_parameter_1'\n\nall_topics_buyer_keywords_Avg_traffic_parameter_1 audience_overlap_sites_overlap_scores_parameter_4 0.5766367059682227","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# remove outliers using the inter-quartile range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nnRowsRead = 12203 # specify 'None' if want to read whole file\n# alexa.com_site_info.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf = pd.read_csv('/kaggle/input/sites-information-data-from-alexacom-dataset/alexa.com_site_info.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'alexa.com_site_info.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# insulin looks like it might have some outliers\nmean, quartlies a lot smaller than the max value\nlet's try to remove these outliers to check if they help with model performance\n\n# calculate the inter-quartile range\nIn descriptive statistics, the interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 âˆ’ Q1. In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data. (https://en.wikipedia.org/wiki/Interquartile_range)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# first we should clean data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = []\nstr_counter = 0\nfor item in df:\n    try:\n        y = 0\n        x = df[item][1] * 2\n        z = x + y\n        numeric_columns.append(item)\n    except:\n        str_counter += 1\nprint(str_counter)\n\nfrom ipykernel import kernelapp as app\n\n# all_topics_buyer_keywords_Avg_traffic_parameter_1\n\nfor item in df:\n#     df[item].fillna(df[item].mean())\n    if item not in numeric_columns:\n        del(df[item])\n\n# print(df)\n\n        \nfor item in df:\n        df[item] = pd.to_numeric(df[item], errors='coerce')\n        df = df.dropna(subset=[item])\n        df[item] = df[item].astype(int)\n# calculate interquartile range\nq25, q75 = np.percentile(df['all_topics_buyer_keywords_Avg_traffic_parameter_1'], 25), np.percentile(df['all_topics_buyer_keywords_Avg_traffic_parameter_1'], 75)\niqr = q75 - q25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iqr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# let's remove all observations that have insulin higher than 21.0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf2 = df[df['all_topics_buyer_keywords_Avg_traffic_parameter_1'] <= 21.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('original df size:', df.shape)\nprint('new df size:', df2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# let's compare models\n# baseline logistic regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df.loc[:, df.columns != 'Daily_time_on_site'], df['Daily_time_on_site']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\npd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# logistic regression on data without all_topics_buyer_keywords_Avg_traffic_parameter_1 outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df2.loc[:, df2.columns != 'Daily_time_on_site'], df2['Daily_time_on_site']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\npd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# removing outliers using standard deviation\nStandard deviation is a metric of variance i.e. how much the individual data points are spread out from the mean.\n\nless reliable than IQR because the mean and standard deviation are impacted by the outliers\n\ndata must follow a Gaussian or normal distribution\n\n# let's remove outliers in the insulin variable\nremove points that are above (Mean + 2 SD) and any points below (Mean - 2 SD)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = []\nstr_counter = 0\nfor item in df:\n    try:\n        y = 0\n        x = df[item][1] * 2\n        z = x + y\n        numeric_columns.append(item)\n    except:\n        str_counter += 1\nprint(str_counter)\n\nfrom ipykernel import kernelapp as app\n\n\nfor item in df:\n#     df[item].fillna(df[item].mean())\n    if item not in numeric_columns:\n        del(df[item])\n\nmean = np.mean(df['all_topics_buyer_keywords_Avg_traffic_parameter_1'])\nsd = np.std(df['all_topics_buyer_keywords_Avg_traffic_parameter_1'])\n\ndf2 = df[(df['all_topics_buyer_keywords_Avg_traffic_parameter_1'] > mean - 2 * sd) & (df['all_topics_buyer_keywords_Avg_traffic_parameter_1'] < mean + 2 * sd)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# compare models\n# baseline logistic regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df.loc[:, df.columns != 'Daily_time_on_site'], df['Daily_time_on_site']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\npd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# logistic regression model after removing outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df2.loc[:, df2.columns != 'Daily_time_on_site'], df2['Daily_time_on_site']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\npd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# removing outliers using the median absolute deviationÂ¶\nRobust Z-Score method\n\nsource: https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mad_based_outlier(points, thresh=3.5):\n    if len(points.shape) == 1:\n        points = points[:,None]\n    median = np.median(points, axis=0)\n    diff = np.sum((points - median)**2, axis=-1)\n    diff = np.sqrt(diff)\n    med_abs_deviation = np.median(diff)\n\n    modified_z_score = 0.6745 * diff / med_abs_deviation\n\n    return modified_z_score > thresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[mad_based_outlier(df['all_topics_buyer_keywords_Avg_traffic_parameter_1'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# bad result !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# df2 size is 0 , so we should choose another column for prediction !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# X, y = df2.loc[:, df2.columns != 'Daily_time_on_site'], df2['Daily_time_on_site']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n# logreg = LogisticRegression()\n# logreg.fit(X_train, y_train)\n# y_pred = logreg.predict(X_test)\n# print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n# pd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}