{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Score_data(pred, real):\n    # computing errors\n    errors = np.abs(pred - real).flatten()\n    # estimation\n    mean = sum(errors)/len(errors)\n    cov = 0\n    for e in errors:\n        cov += (e - mean)**2\n    cov /= len(errors)\n\n    print('mean : ', mean)\n    print('cov : ', cov)\n    return errors, cov, mean\n\n# calculate Mahalanobis distance\ndef Mahala_distantce(x,mean,cov):\n    return (x - mean)**2 / cov\n\n\ndef scale(A):\n    return (A-np.min(A))/(np.max(A) - np.min(A))\n\n\ndef stats_dfs(path):\n    df = pd.read_csv(path,sep=\";\")\n    print(\"\\n_________________\\n\")\n    print(path)\n    print(\"\\n_________________\\n\")\n    print(df.shape)\n    print(\"\\n_________________\\n\")\n    print(df.anomaly.value_counts())\n    print(\"\\n_________________\\n\")\n    print(df.anomaly.value_counts()/df.shape[0]*100)\n    print(\"\\n_________________\\n\")\n    print(df.changepoint.value_counts())\n    print(\"\\n_________________\\n\")\n    print(df.changepoint.value_counts()/df.shape[0]*100)\n    return df\n\n\ndef stats_dfs_freeanomaly(path):\n    df = pd.read_csv(path,sep=\";\")\n    print(\"\\n_________________\\n\")\n    print(path)\n    print(\"\\n_________________\\n\")\n    print(df.shape)\n    print(\"\\n_________________\\n\")\n    return df\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_df_1 = [\"/kaggle/input/skoltech-anomaly-benchmark-skab/SKAB/valve2/1.csv\"]\ndf = stats_dfs(list_df_1[0])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df.loc[:550]\ntest = df.loc[550:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_a_free = [\"/kaggle/input/skoltech-anomaly-benchmark-skab/SKAB/anomaly-free/anomaly-free.csv\"]\ndf_a_free = stats_dfs_freeanomaly(list_a_free[0])\ndf_a_free.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = pd.read_csv(\"../input/benckmark-anomaly-timeseries-skab/alldata_skab.csv\")\nprint(raw_data.columns)\nprint(raw_data.head())\n\nprint(\"anomaly \", raw_data.anomaly.value_counts())\nprint(\"changepoint \",raw_data.changepoint.value_counts())\n\n# # Plotting\npd.DataFrame(raw_data[['Volume Flow RateRMS', 'anomaly', 'changepoint']].values, columns=['Volume Flow RateRMS', 'anomaly', 'changepoint'], index = raw_data.index).plot(figsize=(12,6))\n\nplt.xlabel('Values')\nplt.ylabel('Values')\nplt.title('Residuals')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = df.copy()\nraw_data.set_index('datetime')\n# # Plotting\npd.DataFrame(raw_data.values, columns=raw_data.columns, index = raw_data.index).plot(figsize=(12,6))\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt# Standardize/scale the dataset and apply PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n# Extract the names of the numerical columns\n\n# x = df[['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure', 'Temperature', 'Thermocouple', 'Voltage', 'Volume Flow RateRMS']]\nx = df[['Volume Flow RateRMS']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler, pca)\n# pipeline.fit(x.values.reshape(-8, 8))\npipeline.fit(x.values.reshape(-1, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the principal components against their inertia\nfeatures = range(pca.n_components_)\n_ = plt.figure(figsize=(15, 5))\n_ = plt.bar(features, pca.explained_variance_)\n_ = plt.xlabel('PCA feature')\n_ = plt.ylabel('Variance')\n_ = plt.xticks(features)\n_ = plt.title(\"Importance of the Principal Components based on inertia\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Calculate PCA with 8 components\n# pca = PCA(n_components=8)\n# principalComponents = pca.fit_transform(x.values.reshape(-8,8))\n# principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8'])\n\n# Calculate PCA with 1 components\npca = PCA(n_components=1)\nprincipalComponents = pca.fit_transform(x.values.reshape(-1,1))\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['pc1'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n# Run Augmented Dickey Fuller Test\nresult = adfuller(principalDf['pc1'])\n# Print p-value\nprint(result[1] >0.05, result[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test have value very small number (much smaller than 0.05). Thus, I will reject the Null Hypothesis and say the data is stationary","metadata":{}},{"cell_type":"markdown","source":"## Using PCA1 component with AR model","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\n\n# follow lag\nmodel_ar = ARIMA(principalDf['pc1'].loc[550:], order=(1,1,0))  \nresults_ARIMA_ar = model_ar.fit(disp=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Forecast\nfc, se, conf = results_ARIMA_ar.forecast(513, alpha=0.05)  # 95% conf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make as pandas series\nfc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)\n\n# Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(principalDf['pc1'].loc[550:], label='training') # 550, train\nplt.plot(principalDf['pc1'].loc[:550], label='actual')  # 513, test\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors, cov, mean = Score_data(fc_series.values , principalDf['pc1'].loc[550:].values)\n\nmahala_dist = []\nfor e in errors:\n    mahala_dist.append(Mahala_distantce(e, mean, cov))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['pca1_value'] = principalDf['pc1'].loc[550:]\ntest['pca1_scores'] = mahala_dist\n\ntest['pca1_scores_norm'] = scale(mahala_dist)\nplt.figure(figsize=(12, 8))\nplt.hist(test['pca1_scores_norm'], bins=50);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1_pc1, q3_pc1 = test['pca1_scores'].quantile([0.10, 0.60])\niqr_pc1 = q3_pc1 - q1_pc1\n\n# Calculate upper and lower bounds for outlier for pc1\nlower_pc1 = q1_pc1 - (1.5*iqr_pc1)\nupper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n# Filter out the outliers from the pc1\ntest['outlier_pca1'] = ((test['pca1_scores']>upper_pc1) | (test['pca1_scores']<lower_pc1)).astype('int')\ntest['outlier_pca1'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, axes = plt.subplots(nrows=2, figsize=(15,10))\n# axes[0].plot(test[['pca1_scores']], color='blue')\n# axes[1].plot(np.array(mahala_dist).ravel(), color='red')\n\n# axes[0].set_title('original data', fontsize=20)\n# axes[1].set_title('outlier score', fontsize=20)\n\n# # axes[0].grid()\n# # axes[1].grid()\n# plt.tight_layout()\n# plt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization\na = test.loc[test['anomaly'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(test[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('True Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization\na = test.loc[test['outlier_pca1'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(test[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = test.shape[0]\nplt.scatter(range(N),test['pca1_scores_norm'][:N].cumsum(),marker='1',label='PCA ')\nplt.xlabel('Readings')\nplt.ylabel('anomalies frequency')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2 -- Distributions of Predicted Probabilities of both classes\nlabels=['Positive','Negative']\nplt.hist(test[test['outlier_pca1']==1]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='green',  label=labels[0])\nplt.hist(test[test['outlier_pca1']==0]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='red', label=labels[1])\nplt.axvline(.5, color='blue', linestyle='--', label='decision boundary')\n# plt.xlim([0,1])\nplt.title('Distributions', size=13)\nplt.xlabel('Norm values', size=13)\nplt.ylabel('Readings (norm.)', size=13)\nplt.legend(loc=\"upper right\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint(classification_report(test['anomaly'], test['outlier_pca1']))\nconfusion_matrix(test['anomaly'], test['outlier_pca1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test['changepoint'], test['outlier_pca1']))\nconfusion_matrix(test['changepoint'], test['outlier_pca1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(test['outlier_pca1'], test['anomaly'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(test['outlier_pca1'], test['changepoint'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}