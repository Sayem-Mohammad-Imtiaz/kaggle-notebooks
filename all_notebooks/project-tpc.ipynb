{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n#importing our cancer dataset\n\ndataset = pd.read_csv(\"../input/breast-cancer-data/Breast_cancer_data.csv\")\ndataset.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-17T18:21:50.935805Z","iopub.execute_input":"2021-06-17T18:21:50.936562Z","iopub.status.idle":"2021-06-17T18:21:51.861787Z","shell.execute_reply.started":"2021-06-17T18:21:50.936431Z","shell.execute_reply":"2021-06-17T18:21:51.860795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Cancer data set dimensions : {}\".format(dataset.shape))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop the column with all missing values\ntrain = dataset.dropna(axis = 1)\n#Get a count of the number of malignant(1) and benign(0) cells\ntrain['diagnosis'].value_counts()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize the count\nsns.countplot(train['diagnosis'], label = 'count')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look at the data types to see which columns need to be encoded\ntrain.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a pair plot\nsns.pairplot(train.iloc[: ,0:6], hue = 'diagnosis')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the correlation of the columns\ntrain.corr()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize the correlation\nf,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(train.corr(), annot = True, fmt= '.2f')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Split the dataset into independent(X) and dependent(Y) datasets\nX = train.iloc[:,0:4].values\nY = train.iloc[:,5].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split the dataset into 75% training and 25% testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scale the data (Feature Scaling)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a function for models\ndef models(X_train, Y_train):\n    \n    #Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(max_iter=100)\n    log.fit(X_train, Y_train)\n    \n    #Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier()\n    tree.fit(X_train, Y_train)\n    \n    #Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier()\n    forest.fit(X_train, Y_train)\n    \n    #Naive Bayes\n #   from sklearn.naive_bayes import MultinomialNB\n #   NB = MultinomialNB(alpha = 1.0, class_prior=None, fit_prior=True)\n #   NB.fit(X_train, Y_train)\n    \n    #K-nearest neighbors\n    from sklearn.neighbors import KNeighborsClassifier\n    KNN = KNeighborsClassifier()\n    KNN.fit(X_train, Y_train)\n    \n    #Support Vector Machines\n    from sklearn.svm import SVC\n    SVM = SVC()\n    SVM.fit(X_train, Y_train)\n    \n    #Gradient Boosting Classifier\n    from sklearn.ensemble import GradientBoostingClassifier\n    GBR = GradientBoostingClassifier()\n    GBR.fit(X_train,Y_train)\n    \n\n    #Print the model accuracy of training data\n    print('[0]Logistic Regression Training Accuracy               : ',log.score(X_train, Y_train))\n    print('[1]Decision Tree Training Accuracy                     : ',tree.score(X_train, Y_train))\n    print('[2]Random Forest Training Accuracy                     : ',forest.score(X_train, Y_train))\n#    print('[3]Naive Bayes Training Accuracy                       : ',NB.score(X_train, Y_train)) (negative value error)\n    print('[3]K-nearest neighbors Training Accuracy               : ',KNN.score(X_train, Y_train))\n    print('[4]Support Vector Machines Training Accuracy : ',SVM.score(X_train, Y_train))\n    print('[5]Gradient Boosting Classifier Training Accuracy : ',GBR.score(X_train, Y_train))\n    \n    return log, tree, forest, KNN, SVM, GBR","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets Make a function for Grid Search CV\nfrom sklearn.model_selection import GridSearchCV\ndef Classification_model_gridsearchCV(model,param_grid,data_X,data_y):\n    clf = GridSearchCV(model,param_grid,cv=10,scoring=\"accuracy\")\n    # this is how we use grid serch CV we are giving our model\n    # the we gave parameters those we want to tune\n    # Cv is for cross validation\n    # scoring means to score the classifier\n    \n    clf.fit(X_train,Y_train)\n    print(\"The best parameter found on development set is :\")\n    # this will gie us our best parameter to use\n    print(clf.best_params_)\n    print(\"the bset estimator is \")\n    print(clf.best_estimator_)\n    print(\"The best score is \")\n    # this is the best score that we can achieve using these parameters#\n    print(clf.best_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we have to take parameters that are used for Decison tree Classifier\n# you will understand these terms once you follow the link above\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_split': [2,3,4,5,6,7,8,9,10], \n              'min_samples_leaf':[2,3,4,5,6,7,8,9,10] }\n# here our gridasearchCV will take all combinations of these parameter and apply it to model \n# and then it will find the best parameter for model\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\nClassification_model_gridsearchCV(tree,param_grid,X_train,Y_train)\n# call our function","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_range = list(range(1, 30))\nleaf_size = list(range(1,30))\nweight_options = ['uniform', 'distance']\nparam_grid = {'n_neighbors': k_range, 'leaf_size': leaf_size, 'weights': weight_options}\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier()\nClassification_model_gridsearchCV(KNN,param_grid,X_train,Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n              {'C': [1, 10, 100, 1000], \n               'kernel': ['linear']\n              },\n              {'C': [1, 10, 100, 1000], \n               'gamma': [0.001, 0.0001], \n               'kernel': ['rbf']\n              },\n ]\nfrom sklearn.svm import SVC\nSVM = SVC()\nClassification_model_gridsearchCV(SVM,param_grid,X_train,Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_grid = {'learning_rate': [0.01,0.02,0.03,0.04],\n#                   'subsample'    : [0.9, 0.5, 0.2, 0.1],\n#                   'n_estimators' : [100,500,1000, 1500],\n#                   'max_depth'    : [4,6,8,10]\n#                  }\n# from sklearn.ensemble import GradientBoostingClassifier\n# GBR = GradientBoostingClassifier()\n# Classification_model_gridsearchCV(GBR,param_grid,X_train,Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting all the models\nmodel = models(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test model accuracy on test data using confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfor i in range (len(model)):\n    print('Model :',model[i])\n    cm = confusion_matrix(Y_test,model[i].predict(X_test))\n\n    TP = cm[0][0]\n    FP = cm[0][1]\n    FN = cm[1][0]\n    TN = cm[1][1]\n\n    print(cm)\n    print('Testing Accuracy =',(TP + TN)/(TP + FP + FN + TN))\n    print('Precision =',(TP)/(TP + FP))\n    print('Recall =',(TP)/(TP + FN))\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Another way to get matrix of the models\nfrom sklearn.metrics import classification_report, accuracy_score\nfor i in range (len(model)):\n    print('Model :',model[i])\n    print(classification_report(Y_test,model[i].predict(X_test)))\n    print(accuracy_score(Y_test,model[i].predict(X_test)))\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_mean = list(dataset.columns[0:5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_function = {0: \"blue\", 1: \"red\"} # Here Red color will be 1 which means M and blue foo 0 means B\ncolors = dataset[\"diagnosis\"].map(lambda x: color_function.get(x))# mapping the color fuction with diagnosis column\npd.plotting.scatter_matrix(dataset[features_mean], c=colors, alpha = 0.5, figsize = (5, 5)); # plotting scatter plot matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape(398,4,1)\nX_test = X_test.reshape(171,4,1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CNN model\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D,Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\n\nmodelCNN = Sequential()\nmodelCNN.add(Conv1D(filters=16,kernel_size=2,activation='relu',input_shape=(4,1)))\nmodelCNN.add(BatchNormalization())\nmodelCNN.add(Dropout(0.2))\n\nmodelCNN.add(Conv1D(32,2,activation='relu'))\nmodelCNN.add(BatchNormalization())\nmodelCNN.add(Dropout(0.2))\n\nmodelCNN.add(Flatten())\nmodelCNN.add(Dense(32,activation='relu'))\nmodelCNN.add(Dropout(0.2))\n\nmodelCNN.add(Dense(1,activation='sigmoid'))\nmodelCNN.summary()\nmodelCNN.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = 100\nhistory = modelCNN.fit(X_train,Y_train,epochs=epoch,verbose=1,validation_data=(X_test,Y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  plt.plot(epochRange,history.history['accuracy'])\n  plt.plot(epochRange,history.history['val_accuracy'])\n  plt.title('Model Accuracy')\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()\n\n  plt.plot(epochRange,history.history['loss'])\n  plt.plot(epochRange,history.history['val_loss'])\n  plt.title('Model Loss')\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotLearningCurve(history,epoch)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pred = modelCNN.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"asd = []\nfor x in X_pred.tolist():\n    if x[0]>0.8:\n        asd.append(1)\n    else:\n        asd.append(0)\nprint(asd)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test model accuracy on test data using confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test,asd)\n\nTP = cm[0][0]\nFP = cm[0][1]\nFN = cm[1][0]\nTN = cm[1][1]\n\nprint(cm)\nprint('Testing Accuracy =',(TP + TN)/(TP + FP + FN + TN))\nprint('Precision =',(TP)/(TP + FP))\nprint('Recall =',(TP)/(TP + FN))","metadata":{},"execution_count":null,"outputs":[]}]}