{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Import essential libaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read data from csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bank-personal-loan-modelling/Bank_Personal_Loan_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check of occurance of null"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking random sample from data to take a look at data\ndf.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check distribution of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['Personal Loan']).size()\n#df.apply(lambda x: print('\\nColumn {}, value {}'.format(x.name,x.unique())))\n# Only few people around 9.6% accepted the personal loan in the past year.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Personal Loan'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Negative values to postive in experience column.\ndf['Experience'] = df['Experience'].abs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data analyis and insights\n1. Age groups has approx same size.\n2. Experience feature has -3 as minimum values which is invalid.\n3. Maximun people have income in range betweeen $98 to $234. Q4 is the biggest quntile.\n4. There is high standard deviation for income feature.\n5. Very low expenditure from credits is clealry visible.\n6. Education is a catageorical variable\n7. Mortgage is highly right skewed.\n8. Almost 60% user use online banking.\n9. Almot 30% user use credit card from Universal bank.\n10. Very low number of CD account holders.\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#analyizing correlation between data\ndf.corr().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analysis from correlations\n1. Usage of credit card is high postivelycorrelated to Income of person.(64%)\n2. If a person will take perosnal loan or not is also high postively correlated to Income of person(50%), credit card expenditure per month(37%) and wether having a CD account with bank or not(32%).\n3. We can remove ZIP Code and ID.\n4. Bigger families tends have lower expenditure from credit card and lower income.\n5. Certificate of deposite accounts tends to increase as if person has Securities account with the bank.\n6. Having a credit card of universal bank in releated to if person has CD account with Thara Bank.\n7. We can remove ID, Age,Experience, ZIP Code, Family, Security Account, Online, Credit Card columns as these have very low correlations.  \n"},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing data using seaborn"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['ID','ZIP Code'], axis=1, inplace=True)\nsns.pairplot(df, corner=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2,2, figsize=(14,7))\nsns.countplot(df['Family'], ax=axes[0,0])\nsns.countplot(df['Education'], ax=axes[0,1])\nsns.countplot(df['Securities Account'], ax=axes[1,0])\nsns.countplot(df['CD Account'], ax=axes[1,1])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2,2, figsize=(14,7))\nsns.distplot(df['Income'], ax=axes[0,0])\nsns.distplot(df['Mortgage'], ax=axes[0,1])\nsns.distplot(df['Age'], ax=axes[1,0])\nsns.distplot(df['Experience'], ax=axes[1,1])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split data in test and training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny = df['Personal Loan']\nX = df.drop(['Personal Loan'], axis=1).copy()\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state=1)\n#X_train.sample(2)\n#X_valid.sample(2)\n#y_train.sample(2)\n#y_valid.sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscaler = MinMaxScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_valid = scaler.fit_transform(X_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling Section"},{"metadata":{},"cell_type":"markdown","source":"## Helper Fucntions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def confusion_heatmap_metrics(predictions):\n    df_table = confusion_matrix(y_valid,predictions, [1,0])\n    sns.heatmap(df_table, annot=True, fmt='0.2f', xticklabels=['Accept', 'Reject'], yticklabels=['Accept', 'Reject'])\n    plt.ylabel('Predicted Values')\n    plt.xlabel('Actual Values')\n    plt.show()\n    all_metrics(df_table)\n    \ndef all_metrics(df_table):\n    TP = df_table[0,0]\n    FN = df_table[1,0]\n    FP = df_table[0,1]\n    TN = df_table[1,1]\n    accuracy = (TP+TN)/(TP+FN+FP+TN)\n    precision = TP/(TP+FP)\n    recall = TP/(TP+FN)\n    f1 = (2 * accuracy * precision)/ (accuracy + precision)\n    print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(accuracy,precision,recall,f1))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model: RandomForest\n\nrf_model = RandomForestRegressor()\nrf_model.fit(X_train,y_train)\npredictions = rf_model.predict(X_valid)\nmae = mean_absolute_error(predictions,y_valid)\nrf_model.score(X_valid,y_valid)\n#values in predictions are coming as float64, converting them back to int\npredictions = predictions.astype(int)\nconfusion_heatmap_metrics(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Logistic Regression\n\nlr_model = LogisticRegression(max_iter=5000)\nlr_model.fit(X_train,y_train)\nlr_predicts = lr_model.predict(X_valid)\nprint('Logistic Regression score with training data :{} '.format(lr_model.score(X_train,y_train)))\nprint('Logistic Regression score with test data :{} '.format(lr_model.score(X_valid,y_valid)))\nconfusion_heatmap_metrics(lr_predicts)\n#for scaled data\nsc_lr_model = LogisticRegression(max_iter=5000)\nsc_lr_model.fit(scaled_X_train,y_train)\nsc_lr_predicts = lr_model.predict(scaled_X_valid)\nprint('Logistic Regression score with scaled training data :{} '.format(lr_model.score(scaled_X_train,y_train)))\nprint('Logistic Regression score with scaled test data :{} '.format(lr_model.score(scaled_X_valid,y_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: KNN\nMSE = []\nneighbors = []\ndef knn_model(n):\n    KNN_model = KNeighborsClassifier(n_neighbors=n)\n    KNN_model.fit(X_train,y_train)\n    score_train = KNN_model.score(X_train,y_train)\n    score_test = KNN_model.score(X_valid,y_valid)\n    #print('\\nKNeighborsClassifier(n_neighbors: {}) score with training data :{} '.format(n,score_train))\n    #print('KNeighborsClassifier(n_neighbors: {}) with test data :{} '.format(n,score_test))\n    return score_test\n\nfor n in np.arange(1,30):\n    if n%2 !=0:\n        neighbors.append(n)\n        test_score_val = knn_model(n)\n        MSE.append(1-test_score_val)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot misclassification error\nbest_neighbors_val = neighbors[MSE.index(min(MSE))]\nplt.plot(neighbors,MSE)\nprint('Best Neighbor is', best_neighbors_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_KNN_model = KNeighborsClassifier(n_neighbors=best_neighbors_val)\nbest_KNN_model.fit(X_train,y_train)\npredictions = best_KNN_model.predict(X_valid)\nprint('KNN modle score: ', best_KNN_model.score(X_valid,y_valid))\nconfusion_heatmap_metrics(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Navie Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model: Multinomial Naive Bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_model = MultinomialNB()\nnb_model.fit(X_train,y_train)\nnb_predictions = nb_model.predict(X_valid)\ntrain_score = nb_model.score(X_train,y_train)\ntest_score = nb_model.score(X_valid,y_valid)\nprint('MultinomialNB score with training data :{} '.format(train_score))\nprint('MultinomialNB score with test data :{} '.format(test_score))\nconfusion_heatmap_metrics(nb_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Conlusion"},{"metadata":{},"cell_type":"markdown","source":"1. Which model performes better?\n    The best model in the case of this dataset(personal lona classifier) Logistic Regression perfomes better than other given model in question.\n2. Why this model performes better?\n    Logistic REgression is mostly used for classification problem in which the outcome is binary."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}