{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using Novel Language Models and Web scraping to Effectively Identify Articles related to Therapeutics and Vaccines\n * Team: MD-Lab, ASU\n * Author: Jitesh Pabla, Email: jpabla1@asu.edu, Kaggle ID: jiteshpabla\n * Team Members: Rishab Banerjee, Hong Guan, Ashwin Karthik Ambalavanan, Mihir Parmar, Murthy Devarakonda\n * Email ID: loccapollo@gmail.com, hguan6@asu.edu, aambalav@asu.edu, mparmar3@asu.edu, Murthy.Devarakonda@asu.edu\n * Kaggle ID: loccapollo, hongguan, ashwinambal96, mihir3031, murthydevarakonda\n * This is a Team Submission\n * Here are the links to our teams Kernels:\n     - https://www.kaggle.com/jiteshpabla/classifying-cord-19-articles-using-elasticbert/edit\n     - https://www.kaggle.com/ashwinambal96/scibert-based-article-identification\n     - https://www.kaggle.com/hongguan/micro-scorers-for-covid-19-open-challenge/\n     - https://www.kaggle.com/loccapollo/lexicon-based-similarity-scoring-with-bert-biobert\n     - https://www.kaggle.com/mihir3031/bert-sts-for-searching-relevant-research-papers\n     - The final ensembling that combines everything together: http://https://www.kaggle.com/hongguan/ensemble-model-for-covid-19-open-challenge/"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis repository deals with the \"cord19-vaccines-and-therapeutics\" dataset which is based on the [\"What do we know about vaccines and therapeutics?\"](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=561) task of the COVID-19 Open Research Dataset Challenge (CORD-19)."},{"metadata":{},"cell_type":"markdown","source":"# part1: Extracting Articles from Google"},{"metadata":{},"cell_type":"markdown","source":"The \"cord19-vaccines-and-therapeutics\" dataset is extracted from Google scholar using the [Publish or Perish tool](https://harzing.com/resources/publish-or-perish) by using the queries like:\n - \"vaccine\"\n - \"therapeutics\"\n - \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\"\n - etc.\n along with the keyword \"coronavirus\""},{"metadata":{},"cell_type":"markdown","source":"## Code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"load the metadata file and remove duplicates (if any)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"metadf = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nmetadf = metadf.drop_duplicates()\nmetadf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"load all the files from the google search results dataset into a dictionary of dataframes (key being the query; value being the dataframe)"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/cord19-vaccines-and-therapeutics-google-results/\"\ngoogle_files = glob.glob(DATA_DIR + \"4*.csv\")\ngoogle_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"google_files_df = {}\nfor filename in google_files:\n    print(filename)\n    df = pd.read_csv(filename)\n    #remove unnecessary or empty columns\n    df = df.drop(columns=['QueryDate', \"Type\", \"DOI\", \"ISSN\", \"CitationURL\", \"Volume\", \"Issue\", \"StartPage\", \"EndPage\", \"ECC\", \"CitesPerAuthor\", \"AuthorCount\"])\n    # remove all empty rows\n    df = df.dropna(how='all')\n    # rename title coumns\n    df = df.rename(columns={\"Title\": \"title\"}, errors=\"raise\")\n    # only take first 400\n    df = df[:400]\n    # string matching for keys\n    result_index = filename.find('results/')\n    google_files_df[filename[result_index+8:-4]] = df\ngoogle_files_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate a normalized score from 0-1 based on the ranks using the formula:\n```\n1 - (rank/total)\n```\nwhere\n- rank = rank of article\n- total = total number of articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score(rank_col):\n    score = 1 - (rank_col/len(rank_col))\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for current_item in google_files_df.items():\n    current_query, current_df = current_item\n    current_df['score from rank'] = get_score(current_df['GSRank'])\ngoogle_files_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform a merge on the \"title\" column the metadata and the Google search results data to find the common articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"matched_df = {}\nfor current_item in google_files_df.items():\n    current_query, current_df = current_item\n    matched_df[current_query] = pd.merge(current_df, metadf, on=\"title\")\nmatched_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"View a single row"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = matched_df['400Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers'].loc[1]\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis"},{"metadata":{},"cell_type":"markdown","source":"Find the number of common articles in each dataframe (per query dataframe) to guage how much overlap of information exists between each query"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, current_item in enumerate(matched_df.items()):\n    current_query, current_df = current_item\n    print(\"#######Common articles for \", current_query)\n    #matched_df[current_query] = pd.merge(current_df, metadf, on=\"title\")\n    for j, compare_item in enumerate(matched_df.items()):\n        #if not i==j:\n        compare_query, compare_df = compare_item\n        merged_df = current_df.merge(compare_df, on=['title'], how='inner', indicator=True)\n        print(compare_query[2:40], \"--\", len(merged_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using word cloud to visualize the common wordsin the articles of various queries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, current_item in enumerate(matched_df.items()):\n    current_query, tempdf = current_item\n    tit_text = \" \".join(str(tit) for tit in tempdf.title)\n    abs_text = \" \".join(str(abs) for abs in tempdf.Abstract)\n    text = tit_text + \" \" + abs_text \n    print(current_query)\n    # Create and generate a word cloud image:=\n    # lower max_font_size, change the maximum number of word and lighten the background:\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    fig = plt.figure(figsize=(10,5))\n    #fig.title(current_query)\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part2: creating training and testing dataset"},{"metadata":{},"cell_type":"markdown","source":"Give class=1 to vaccine; class=2 for therpeutics; and class=0 for other"},{"metadata":{"trusted":true},"cell_type":"code","source":"vacc_df = matched_df[\"420vaccine\"]\nvacc_df[\"class\"] = 1\nther_df = matched_df[\"410therapeutics\"]\nther_df[\"class\"] = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#positive samples\nconcat_google_df = pd.concat([vacc_df, ther_df], axis=0, ignore_index=True)\nconcat_google_df = concat_google_df.drop_duplicates(subset='title', keep=\"first\")\nconcat_google_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The nagative samples are extracted by taking a completely seperate query from the 9 given challenges of the CORD-19 challenge which are semantically very different from \"vaccines and therapeutics\" to make sure that the negative samples do not contain any information that should be a part of the positive samples.\n\nThey are extracted and matched in the same manner as the postive samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"#negative samples\ngoogle_files_neg = glob.glob(DATA_DIR + \"NEGATIVE*.csv\")\n\ngoogle_df_neg = []\nfor filename in google_files_neg:\n    df = pd.read_csv(filename)\n    google_df_neg.append(df)\n\nconcat_google_df_neg = pd.concat(google_df_neg, axis=0, ignore_index=True)\nconcat_google_df_neg = concat_google_df_neg.drop_duplicates(subset=\"title\")\nconcat_google_df_neg[\"class\"] = 0\nconcat_google_df_neg\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find comman samples between postive and negative samples\nmerged_df = concat_google_df.merge(concat_google_df_neg, on=['title'], \n                   how='inner', indicator=True, suffixes=('', '_y'))\nmerged_df.drop(list(merged_df.filter(regex='_y$')), axis=1, inplace=True)\nmerged_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the common articles from the nagative samples\nwithout_commom_df = pd.concat([merged_df, concat_google_df_neg])\nwithout_commom_df = without_commom_df.drop_duplicates(keep=False, subset=\"title\")\nwithout_commom_df =  without_commom_df.drop(columns=[\"_merge\"])\nwithout_commom_df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"splitting the samples equally into train (80%) and test data (20%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split into train test\ndef split_traintest(df):\n  #shuffle\n  df = df.sample(frac=1)\n  #first 20%\n  index = int(len(df)*0.2)\n  print(len(df))\n  print(index)\n  test_df = df.iloc[:index]\n  train_df = df.iloc[index:]\n  return test_df, train_df\n\nther_test, ther_train = split_traintest(ther_df)\nvacc_test, vacc_train = split_traintest(vacc_df)\nneg_test, neg_train = split_traintest(without_commom_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_csv(df_list, name):\n  final_df = pd.concat([df_list[0],df_list[1], df_list[2]])\n  final_df.to_csv(\"GOOGLE_CLASSIFIED_samples_\"+name+'.csv')\n  print(len(final_df))\n\nsave_csv([ther_test, vacc_test, neg_test], \"test\")\nsave_csv([ther_train, vacc_train, neg_train], \"train\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3: Training SciBERT on the Google extracted labels to get labels for entire dataset"},{"metadata":{},"cell_type":"markdown","source":"The analysis shows that there is not many common articles between \"vaccine\" and other long queries like \"Approaches to evaluate risk for enhanced disease after vaccination\" etc. The same goes for \"therapeutics\" query.\nSince the long queries are very complex and specific we decide to classify the articles into the \"vaccines\" and \"therapeutics\" categories first."},{"metadata":{},"cell_type":"markdown","source":"### part 3-a: load/install"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow_gpu>=2.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q ktrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"GOOGLE_CLASSIFIED_samples_test.csv\")\ntrain = pd.read_csv(\"GOOGLE_CLASSIFIED_samples_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.fillna(\"\")\ntrain = train.fillna(\"\")\n\ntest[\"text\"] = test[\"title\"] +\";\"+ test[\"abstract\"] +\";\"+ test[\"journal\"]\ntrain[\"text\"] = train[\"title\"] +\";\"+ train[\"abstract\"] +\";\"+ train[\"journal\"]\n\n\nprint('size of training set: %s' % (len(train)))\nprint('size of validation set: %s' % (len(test)))\n\nx_train = train.text.to_list()\ny_train = train[\"class\"].to_list()\nx_test = test.text.to_list()\ny_test = test[\"class\"].to_list()\n\nfor x in x_train[:10]:\n  print(x)\n\nprint(y_train[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ktrain\nfrom ktrain import text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### part 3-b: train/load model\n### (Option 1) finetuning the SciBERT model on the dataset"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"MODEL_NAME = 'allenai/scibert_scivocab_uncased' #'distilbert-base-uncased'\nt = text.Transformer(MODEL_NAME, maxlen=500, classes=['0', '1', '2'])\ntrn = t.preprocess_train(x_train, y_train)\nval = t.preprocess_test(x_test, y_test)\nmodel = t.get_classifier()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_onecycle(5e-5, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.validate(class_names=t.get_classes())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (Option 2) load the pretrained model from the dataset files"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor = ktrain.load_predictor(DATA_DIR+'GOOGLE_scibert_predictor/GOOGLE_scibert_predictor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### part 3-c: do the final predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"metadf = metadf.fillna(\"\")\n\nmetadf[\"text\"] = metadf[\"title\"] +\";\"+ metadf[\"abstract\"] +\";\"+ metadf[\"journal\"]\n\nmetadf[\"class\"] = \"\"\nmetadf[\"0\"] = 0\nmetadf[\"1\"] = 0\nmetadf[\"2\"] = 0\n\nmetadf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test on 1 sample\npredictor.predict_proba(metadf.text.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfor index, row in metadf.iterrows():\n    probs = predictor.predict_proba(row[\"text\"])\n    classif = predictor.predict(row[\"text\"])\n    metadf.loc[index, \"class\"] = classif\n    metadf.loc[index, \"0\"] = probs[0]\n    metadf.loc[index, \"1\"] = probs[1]\n    metadf.loc[index, \"2\"] = probs[2]\n    if i%200 == 0:\n        print(index)\n    #if i%500 == 0:\n        # uncomment to save after every 500 samples\n        #metadf_diff.to_csv(\"GOOGLE_CLASSIFIED_metadata_diff_\"+str(i)+'.csv')\n    i = i+1\n\nmetadf_diff\n#metadf_diff.to_csv(\"GOOGLE_CLASSIFIED_metadata_\"+\"final\"+'.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}