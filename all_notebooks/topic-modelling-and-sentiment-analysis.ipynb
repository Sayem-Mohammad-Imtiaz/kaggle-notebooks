{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"54cf712d-ec23-40bd-adfc-95425744b13f","_uuid":"055de88c53835ad68c3c3fe8b87e95719a286e90"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport nltk\nfrom nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport string\n\nimport textblob as tb\nfrom tqdm import tqdm\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"597d1423-7573-4e49-b429-940d24ae2f1a","_uuid":"0964f0cdb2de981f2ffb2276dc88243d8645483b"},"source":"news_feed = pd.read_csv('../input/news-week-aug24.csv', dtype={'publish_time': object})\n\nnews_feed['publish_hour'] = news_feed.publish_time.str[:10]\nnews_feed['publish_date'] = news_feed.publish_time.str[:8]\nnews_feed['publish_hour_only'] = news_feed.publish_time.str[8:10]\nnews_feed['publish_time_only'] = news_feed.publish_time.str[8:12]\ndays=news_feed['publish_date'].unique().tolist()\n\nnews_feed['dt_time'] = pd.to_datetime(news_feed['publish_time'], format='%Y%m%d%H%M')\nnews_feed['dt_hour'] = pd.to_datetime(news_feed['publish_hour'], format='%Y%m%d%H')\nnews_feed['dt_date'] = pd.to_datetime(news_feed['publish_date'], format='%Y%m%d')","execution_count":2},{"cell_type":"code","outputs":[],"metadata":{},"source":"news_feed.head()","execution_count":3},{"cell_type":"code","outputs":[],"metadata":{},"source":"feed_count = news_feed['feed_code'].value_counts()\nfeed_count = feed_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(feed_count.index , feed_count.values, alpha = 0.8)\nplt.title(\"Top 10 feed\")\nplt.ylabel('No of Occurances', fontsize = 12)\nplt.xlabel('feed code', fontsize = 12)\nplt.xticks(rotation=70)\nplt.show()","execution_count":4},{"cell_type":"code","outputs":[],"metadata":{},"source":"news_feed = news_feed.dropna()\nnews_feed.count()","execution_count":5},{"cell_type":"code","outputs":[],"metadata":{},"source":"englishStopWords = set(nltk.corpus.stopwords.words('english'))\nnonEnglishStopWords = set(nltk.corpus.stopwords.words()) - englishStopWords\n","execution_count":6},{"cell_type":"code","outputs":[],"metadata":{},"source":"stopWordsDictionary = {lang: set(nltk.corpus.stopwords.words(lang)) for lang in nltk.corpus.stopwords.fileids()}\n","execution_count":7},{"cell_type":"code","outputs":[],"metadata":{},"source":"news_feed.headline_text.dropna()","execution_count":8},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"def get_language(text):\n    if type(text) is str:\n        text = text.lower()\n    words = set(nltk.wordpunct_tokenize(text))\n    return max(((lang, len(words & stopwords)) for lang, stopwords in stopWordsDictionary.items()), key = lambda x: x[1])[0]","execution_count":9},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"news_feed['language'] = news_feed['headline_text'].apply(get_language)","execution_count":10},{"cell_type":"code","outputs":[],"metadata":{},"source":"language_count = news_feed['language'].value_counts()\nlanguage_count = language_count[:10]\nplt.figure(figsize = (10,5))\nsns.barplot(language_count.index, language_count.values, alpha = 0.8)\nplt.title(\"Top 10 languages feed\")\nplt.ylabel('No of Occurances', fontsize = 12)\nplt.xlabel('Language', fontsize = 12)\nplt.xticks(rotation=70)\nplt.show()","execution_count":11},{"cell_type":"code","outputs":[],"metadata":{},"source":"news_feed_english_df = news_feed[news_feed['language'] == 'english']\nnews_feed_english = news_feed_english_df['headline_text']\n\ndef showWordCloud(data):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()])\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                         background_color = 'black',\n                         width = 2500,\n                         height = 2500\n                         ).generate(cleaned_word)\n    plt.figure(1,figsize = (13,13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nshowWordCloud(news_feed_english)","execution_count":12},{"cell_type":"markdown","metadata":{},"source":"***Topic Modelling***\n \nLDA is based on probabilistic graphical modeling while NMF relies on linear algebra. Both algorithms take as input a bag of words matrix (i.e., each document represented as a row, with each columns containing the count of words in the corpus). The aim of each algorithm is then to produce 2 smaller matrices; a document to topic matrix and a word to topic matrix that when multiplied together reproduce the bag of words matrix with the lowest error.\n                 \n \n "},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx , topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words -1:-1]]))","execution_count":13},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"no_features = 1000\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 2,max_features=no_features, stop_words = 'english')\ntfidf = tfidf_vectorizer.fit_transform(news_feed_english)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","execution_count":14},{"cell_type":"code","outputs":[],"metadata":{},"source":"tf_vectorizer = CountVectorizer(max_df = 0.95, min_df = 2, max_features=no_features, stop_words='english')\ntf = tf_vectorizer.fit_transform(news_feed_english)\ntf_feature_names = tf_vectorizer.get_feature_names()","execution_count":15},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"no_topic = 5","execution_count":16},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"nmf = NMF(n_components=no_topic, random_state = 1, alpha =.1, l1_ratio=.5, init = 'nndsvd').fit(tfidf)","execution_count":17},{"cell_type":"code","outputs":[],"metadata":{},"source":"lda = LatentDirichletAllocation(n_topics=no_topic, max_iter = 5, learning_method = 'online', learning_offset=50., random_state=0).fit(tf)","execution_count":18},{"cell_type":"code","outputs":[],"metadata":{},"source":"no_top_words = 10\ndisplay_topics(nmf ,tfidf_feature_names, no_top_words)\n","execution_count":19},{"cell_type":"code","outputs":[],"metadata":{},"source":"display_topics(lda , tf_feature_names , no_top_words)","execution_count":20},{"cell_type":"code","outputs":[],"metadata":{},"source":"def sent(x):\n    t = tb.TextBlob(x)\n    return t.sentiment.polarity, t.sentiment.subjectivity","execution_count":21},{"cell_type":"code","outputs":[],"metadata":{},"source":"tqdm.pandas(leave = False, mininterval = 25)\nvals = news_feed_english_df.headline_text.progress_apply(sent)","execution_count":22},{"cell_type":"code","outputs":[],"metadata":{},"source":"news_feed_english_df['polarity'] = vals.str[0]\nnews_feed_english_df['sub'] = vals.str[1]","execution_count":23},{"cell_type":"code","outputs":[],"metadata":{},"source":"def plot_data(df , col):\n    mean_pol = list(dict(df.groupby(col)['polarity'].mean()).items())\n    mean_pol.sort(key=lambda x: x[0])\n\n    plt.subplots(figsize=(15, 10))\n    plt.subplot(2, 2, 1)\n    plt.plot([i[0] for i in mean_pol], [i[1] for i in mean_pol])\n    plt.xticks(rotation=70)\n    plt.title('Mean polarity over time')\n\n    plt.subplot(2, 2, 2)\n    mean_pol = list(dict(df.groupby(col)['sub'].mean()).items())\n    mean_pol.sort(key=lambda x: x[0])\n    plt.plot([i[0] for i in mean_pol], [i[1] for i in mean_pol])\n    plt.xticks(rotation=70)\n    plt.title('Mean subjectivity over time')\n\n    plt.subplot(2, 2, 3)\n    mean_pol = list(dict(df.groupby(col)['polarity'].std()).items())\n    mean_pol.sort(key=lambda x: x[0])\n    plt.plot([i[0] for i in mean_pol], [i[1] for i in mean_pol])\n    plt.xticks(rotation=70)\n    plt.title('Std Dev of polarity over time')\n\n    plt.subplot(2, 2, 4)\n    mean_pol = list(dict(df.groupby(col)['sub'].std()).items())\n    mean_pol.sort(key=lambda x: x[0])\n    plt.plot([i[0] for i in mean_pol], [i[1] for i in mean_pol])\n    plt.xticks(rotation=70)\n    plt.title('Std dev of subjectivity over time')","execution_count":24},{"cell_type":"code","outputs":[],"metadata":{},"source":"plot_data(news_feed_english_df , 'dt_hour')","execution_count":25},{"cell_type":"code","outputs":[],"metadata":{},"source":"plot_data(news_feed_english_df , 'dt_time')","execution_count":26},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","version":"3.6.4","pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"}}}