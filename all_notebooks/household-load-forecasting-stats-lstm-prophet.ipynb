{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Load Forecasting \n***\n\nIn this Kernel, I delve into different aspects of Time-series forecasting and the problems enccountered while modelling Load forecasting using different Time-series techniques. \n\n\n**Contents:**\n\n[1. Data Understanding]('1')\n\n[2. Preprocessing Data]('2')\n\n[3. Univariate Time-series modelling]('3')  \n    [3.1 Holt-winters exponential smoothing]('3.1')  \n    [3.2 SARIMAX]('3.2')  \n    [3.3 Auto-ARIMA]('3.3')  \n    [3.4 LSTM]('3.4')  \n    [3.5 Facebook-Prophet]('3.5')  \n   \n[4. Multivariate Time-series modelling]('4')\n\n[5. Conclusion]('5')"},{"metadata":{},"cell_type":"markdown","source":" ### <div id= '1'>1. Data Understanding</div>"},{"metadata":{},"cell_type":"markdown","source":"We start with understanding the types of variables, length, different variable names, and  their spread. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Loading packages and Data\n\nfrom IPython.display import Image\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom datetime import datetime\nfrom random import random\nfrom math import sqrt\nfrom numpy import concatenate\nfrom numpy import array\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.seasonal  import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n!pip install pyramid-arima\nfrom pyramid.arima import auto_arima\n\n\n#!pip install plotly==3.10.0\n\nfrom fbprophet import Prophet\n#from plotly.plotly import plot_mpl\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\npd.plotting.register_matplotlib_converters()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#Loading csv\ndata=pd.read_csv('../input/smart-home-dataset-with-weather-information/HomeC.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time has 1 record more than others. Let's check why?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, the last record is truncated!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing the truncated record\ndata=data[:-1]\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#given that the time is in UNIX format, let's check \ntime = pd.to_datetime(data['time'],unit='s')\ntime.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, Time step is in increments of seconds but specified as Minute time steps. So, we create a new daterange in increments of minute"},{"metadata":{"trusted":true},"cell_type":"code","source":"#new daterange in increments of minutes\ntime_index = pd.date_range('2016-01-01 05:00', periods=len(data),  freq='min')  \ntime_index = pd.DatetimeIndex(time_index)\ndata['time']=time_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing column names before doing some calculation as they look weird with \"[kw]\"\ndata.columns=['time', 'use', 'gen', 'House overall', 'Dishwasher',\n       'Furnace 1', 'Furnace 2', 'Home office', 'Fridge',\n       'Wine cellar', 'Garage door', 'Kitchen 12',\n       'Kitchen 14', 'Kitchen 38', 'Barn', 'Well',\n       'Microwave', 'Living room', 'Solar', 'temperature',\n       'icon', 'humidity', 'visibility', 'summary', 'apparentTemperature',\n       'pressure', 'windSpeed', 'cloudCover', 'windBearing', 'precipIntensity',\n       'dewPoint', 'precipProbability']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check power generated from sources other than Solar"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gen'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Solar'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['gen']-data['Solar']).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * It seems \"solar\" and \"gen\" are simillar columns. So we drop 'gen' column as it is the only power generated by Solar "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data=data.drop('gen',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also Let's check 'House overall' and 'use' "},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['House overall']-data['use']).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'House overall' and 'use' are simillar columns. we drop 'House overall'"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('House overall',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div id= '2'>2. Preprocessing Data</div>\n\n**Feature Engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting  hour, day,week, month from the date column\ndata['day']= data['time'].dt.day\ndata['month']= data['time'].dt.month\ndata['week']= data['time'].dt.week\ndata['hour']= data['time'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we could see, there are simillar names for variables. we first check their energy consumption patterns over the day, week, month and then if they look simillar, we will merge them to a single variable!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndef visualize(label, cols):\n    fig,ax=plt.subplots(figsize=(14,8))\n    colour= ['red','green','blue','yellow']\n    for colour,col in zip(colour,cols):\n            data.groupby(label)[col].mean().plot(ax=ax,label=col,color=colour)\n    plt.legend()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('hour',['Furnace 1','Furnace 2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('day',['Furnace 1','Furnace 2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('month',['Furnace 1','Furnace 2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Furnace 2 power consumption is simillar to  Furnace 1, so we will combine both of them and make it as single variable representing Furnace power"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Furnace']= data['Furnace 1']+data['Furnace 2']\ndata=data.drop(['Furnace 1','Furnace 2'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we check for kitechs too"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('month',['Kitchen 12','Kitchen 14','Kitchen 38'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('week',['Kitchen 12','Kitchen 14','Kitchen 38'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('day',['Kitchen 12','Kitchen 14','Kitchen 38'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('hour',['Kitchen 12','Kitchen 14','Kitchen 38'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see what's happening with \"Kitchen 38\""},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Kitchen 38'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(2,2,figsize=(15,10))\n\ndata.groupby('hour')['Kitchen 38'].mean().plot(ax=ax[0,0],color='green',label= 'kitchen 38')\ndata.groupby('day')['Kitchen 38'].mean().plot(ax=ax[0,1],color='green',label= 'kitchen 38')\ndata.groupby('week')['Kitchen 38'].mean().plot(ax=ax[1,0],color='green',label= 'kitchen 38')\ndata.groupby('month')['Kitchen 38'].mean().plot(ax=ax[1,1],color='green',label= 'kitchen 38')\n\n                                                     \n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is consumption but very little comparing to other kitchens, we will keep them like that"},{"metadata":{},"cell_type":"markdown","source":"Before building models, Let us check for datatypes that are not int or float"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['icon'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As these reports  are genererated by data acquisition system, we will  remove these variables, because the real temperature data will be enough for us instead of these variables.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['summary'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check how solar energy got produced in different days"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('summary')['Solar'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected clear, partly cloudy, drizzle, light rain days produced a lot more power than other days. Also the number of clear days outnumbered other days. So, this number would be large compared to other day's"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(['icon','summary'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will check for 'cloudCover' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover'].dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are lot of unique values, let us check what are they"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover'].replace(['cloudCover'], method='bfill', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we need to impute 'cloudCover' with the nearest values as the records are taken in minute steps. We would use backward fill to replace"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cloudCover']=data['cloudCover'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Now, our dataset doesn't have any null values and no categorical variables. Now, we split our dataset for training and testing and start building time-series models and forecast load. We first resample on Day and forecast daily load. Later, we try to build multi-variate time-series models using other variables"},{"metadata":{},"cell_type":"markdown","source":"### **Resampling and Visualization**"},{"metadata":{},"cell_type":"markdown","source":"We need to resample the data and convert the data into a time-series first as data is in minute steps. Resampling over day allows us to forecast the day wise load"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.index= data['time']\n#daily resampling\ndataD=data.resample('D').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"dataD.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hourly resampling\ndataH=data.resample('H').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"weathercols= ['temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nHousecols = ['Dishwasher','Furnace', 'Home office', 'Fridge','Wine cellar', 'Garage door', 'Kitchen 12','Kitchen 14', \n             'Kitchen 38', 'Barn', 'Well','Microwave', 'Living room']\nuseweather=['use','temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nsolarweather=['Solar','temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nusesolar=['use','Solar']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# load dataset\ndef series_visualize(data, cols):\n    dataset = data[cols]\n    values = dataset.values\n    # specify columns to plot    \n    groups = [i for i in range(len(cols))]\n    j = 1\n# plot each column\n    plt.figure(figsize=(18,13))\n    for group in groups:\n        plt.subplot(len(groups), 1, j)\n        plt.plot(values[:, group])\n        plt.title(dataset.columns[group], y=0.5, loc='right')\n        j += 1\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#series_visualize(dataH,Housecols)\nseries_visualize(dataD,Housecols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In the months of June, July,and August, \"office\", \"winecellar\", \"Fridge\" power consumption rose. And in December, January, February months Furnace's power consumption rose."},{"metadata":{"trusted":true},"cell_type":"code","source":"series_visualize(dataH,usesolar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                             Hourly power usage and hourly solar power generation "},{"metadata":{"trusted":true},"cell_type":"code","source":"series_visualize(dataD,usesolar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                    Daily power usage and generation"},{"metadata":{},"cell_type":"markdown","source":"For now, we concentrate on Daily Load forecasting"},{"metadata":{},"cell_type":"markdown","source":"### <div id= '3'>3.Univariate Models</div>"},{"metadata":{},"cell_type":"markdown","source":"Load forecasting would be helpful to optimize energy consumption and plan household energy needs accordingly, saving solar energy and utilizing it optimally. Let us build univariate time-series models first!"},{"metadata":{},"cell_type":"markdown","source":"Understanding the time-series would let us know whether the series is having linear or exponential trend, additive or multiplicative seasonality which aides us in using appropriate techniques for considering these effects."},{"metadata":{},"cell_type":"markdown","source":"Decomposing Time-series "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datause=dataD.iloc[:,0].values\n#fig,ax=plt.subplots(figsize=(15,10))\nplt.rcParams['figure.figsize'] = (14, 9)\nseasonal_decompose(dataD[['use']]).plot()\nresult = adfuller(datause)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additive seasonality with no trend"},{"metadata":{},"cell_type":"markdown","source":"#### Testing Stationarity and plotting Trend "},{"metadata":{},"cell_type":"markdown","source":"Ad-Fuller Test for stationarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= dataD.iloc[:,0].values\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" P-value < 0.05, the series is stationary "},{"metadata":{},"cell_type":"markdown","source":"Now, we can start building Time-series models. As the trend is not linear. we start with Holtz-winters exponential smooting "},{"metadata":{},"cell_type":"markdown","source":"### <div id= '3.1'>3.1 HOLTZ-WINTERS Exponential Smoothing </div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and tests\ntrain=dataD[dataD['month']<12].iloc[:,0]\ntest=dataD[dataD['month']>=12].iloc[:,0]\nprint(\"train has {} records, test has {} records\".format(len(train),len(test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(18,6))\ntrain.plot(ax=ax)\ntest.plot(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# fit model withweekly seasonality \nmodel = ExponentialSmoothing(train.values,seasonal='add',seasonal_periods=7)\nmodel_fit = model.fit()\n\n# make prediction\n\ny = model_fit.forecast(len(test))\ny_predicted=pd.DataFrame(y,index=test.index,columns=['Holtwinter'])\n\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Test')\nplt.plot(y_predicted, label='Holtwinter')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(test,y_predicted))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div id= '3.2'>3.2 ARIMA </div>"},{"metadata":{},"cell_type":"markdown","source":"To find (p,d,q) for ARIMA, we first need to plot Auto-correlation(ACF) and Partial Auto-correlation(PACF) plots. Because of seasonality, we use seasonal ARIMA model SARIMAX from statsmodels "},{"metadata":{},"cell_type":"markdown","source":"#### ACF and PACF plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n# Draw Plot\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nx=plot_acf(train.tolist(), lags=50,ax=axes[0])\ny=plot_pacf(train.tolist(), lags=50, ax=axes[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first differencing\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nx=plot_acf(train.diff().dropna(), lags=50,ax=axes[0])\ny=plot_pacf(train.diff().dropna(), lags=50, ax=axes[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use  P is 1, d is 0 and q is 2 , because i'll be coservative at first,\n\n      1. ACF plots shows gradually decreasing to 0 with few lags above\n      2. PACF plot cuts off quicky at 1\n      3. After differencing once we see the series has negative spikes which means over differencing. so we choose d as 0\n        \nFor Seasonal terms,(P,D,Q)m - "},{"metadata":{"trusted":true},"cell_type":"code","source":"seasonal=seasonal_decompose(dataD[['use']]).seasonal\n#fig.ax=plt.subplots(figsize=(16,5))\nseasonal.plot()\nseasonal.diff(1).dropna().plot(color='orange')\nseasonal.diff(7).dropna().plot(color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we see after 7 differences, seasonality got removed completely. And as a general rule D=1 and only we keep SMA and test with RMS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\ny_hat = test.copy()\nfit1 = SARIMAX(train.values, order=(1, 0, 2),seasonal_order=(0,1,1,7)).fit()\ny_pred  =  fit1.predict(dynamic=True)\ny = fit1.forecast(len(test),dynamic=True)\ny_predicted=pd.DataFrame(y,index=y_hat.index,columns=['sarima'])\n\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Test')\nplt.plot(y_predicted, label='SARIMA')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(test,y_predicted))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div id= '3.3'>3.3 Auto ARIMA</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the model\nmodel = auto_arima(train,start_p=1,d=1,start_q=1,max_p=3,max_d=2,max_q=3,start_P=1,D=1,start_Q=1,\n                   max_P=2,max_D=1,max_Q=2,seasonal =True, m=7, max_order=5,stationary=False, trace=True, error_action='ignore', suppress_warnings=True)\nmodel.fit(train)\n\nforecast = model.predict(n_periods=len(test))\nforecast = pd.DataFrame(forecast,index = test.index,columns=['Prediction'])\n\n#plot the predictions for validation set\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Valid')\nplt.plot(forecast, label='Prediction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(test,forecast))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  <div id= '3.4'>3.4 LSTM</div>"},{"metadata":{},"cell_type":"markdown","source":"First, Lets us split sequence to prepare data in the required form for LSTM\n\n       given sequence [10, 20, 30, 40, 50, 60, 70, 80, 90] into\n            X,\t\t\ty\n        10, 20, 30\t\t40\n        20, 30, 40\t\t50\n        30, 40, 50\t\t60\n        ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we need to prepare dataset as a 3D matrix for LSTM from [samples, timesteps] to  [samples, timesteps, features] . Here, we have only 1 feature i.e., use,  and timesteps are the sequence of steps, here, we choose 28 timesteps and the output timesteps to predict as 16, because we need to validate on the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = train[:307].values.tolist()\n# choose a number of time steps\nn_steps_in, n_steps_out = 28, 16\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=50, verbose=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nx_input = train[307:].values\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat=yhat.reshape(16,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nforecast = pd.DataFrame(yhat,index = test.index,columns=['Prediction'])\n\n#plot the predictions for validation set\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Valid')\nplt.plot(forecast, label='Prediction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(test,forecast))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM couldn't produce better predictions than Statistical models. Because, LSTMs need more data to tune their parameters. And, in our case, we have only 1 year data. Also, LSTM's are better at forecasting longterm not at shortterm"},{"metadata":{},"cell_type":"markdown","source":"### <div id= '3.5'>3.5 Facebook-Prophet</div>"},{"metadata":{},"cell_type":"markdown","source":"The advantages of [Prophet](https://facebook.github.io/prophet/) are, \nwe can model holiday effects, weekly, yearly seasonalities, Saturation checks etc. Prophet requires variables as 'ds' and 'y'. Also, we can add regressors with out much effort!"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train= pd.DataFrame(train)\nnew_train['ds']=new_train.index\nnew_train['y']=new_train['use']\nnew_train.drop(['use'],axis = 1, inplace = True)\nnew_train=new_train.reset_index()\nnew_train.drop(['time'],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model\nm = Prophet(mcmc_samples=300, holidays_prior_scale=0.25, changepoint_prior_scale=0.01, seasonality_mode='additive', \\\n           seasonality_prior_scale=0.4, weekly_seasonality=True, \\\n            daily_seasonality=False)\n\nm.fit(new_train)\nfuture = m.make_future_dataframe(periods=16)\n#prediction\nforecast = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=m.plot_components(forecast)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=m.plot(forecast)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=pd.DataFrame(forecast[335:]['yhat'])\npredictions.index=test.index\n\nfig,ax=plt.subplots(figsize=(15,8))\ntest.plot(ax=ax)\npredictions.plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(test,forecast[['yhat']][335:]))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding other variables to the series to get causal relationship effects"},{"metadata":{"trusted":true},"cell_type":"code","source":"temperature=dataD[dataD['month']<12].loc[:,'temperature']\nrain=dataD[dataD['month']<12].loc[:,'precipIntensity']\nwind=dataD[dataD['month']<12].loc[:,'windSpeed']\n\ntemperature=temperature.reset_index().drop('time',axis=1)\nrain=rain.reset_index().drop('time',axis=1)\nwind=wind.reset_index().drop('time',axis=1)\n\ntrain_regressor=pd.concat([new_train,temperature,rain,wind],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = Prophet(mcmc_samples=300, holidays_prior_scale=0.25, changepoint_prior_scale=0.01, seasonality_mode='additive', \\\n           seasonality_prior_scale=0.4, weekly_seasonality=True, \\\n            daily_seasonality=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.add_regressor('temperature', prior_scale=0.5, mode='additive')\nm.add_regressor('precipIntensity', prior_scale=0.5, mode='additive')\nm.add_regressor('windSpeed', prior_scale=0.5, mode='additive')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.fit(train_regressor)\nfuture = m.make_future_dataframe(periods=16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testtemp=dataD.loc[:,'temperature']\ntestrain=dataD.loc[:,'precipIntensity']\ntestwind=dataD.loc[:,'windSpeed']\ntesttemp=testtemp.reset_index().drop('time',axis=1)\ntestrain=testrain.reset_index().drop('time',axis=1)\ntestwind=testwind.reset_index().drop('time',axis=1)\nfuture['temperature']=testtemp\nfuture['precipIntensity']=testrain\nfuture['windSpeed']=testwind","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"future.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = m.plot_components(forecast)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=m.plot(forecast)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=pd.DataFrame(forecast[335:]['yhat'])\npredictions.index=test.index\n\nfig,ax=plt.subplots(figsize=(15,8))\ntest.plot(ax=ax)\npredictions.plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms = sqrt(mean_squared_error(test,forecast[['yhat']][335:]))\nprint(rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <div id= '4'>4.Multi-Variate models</div>"},{"metadata":{},"cell_type":"markdown","source":"### Before building Multi-variate models, we will check for *Granger's Causaulity*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"maxlag=12\ntest = 'ssr_chi2test'\ndef grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. The values in the table \n    are the P-Values. P-Values lesser than the significance level (0.05), implies \n    the Null Hypothesis that the coefficients of the corresponding past values is \n    zero, that is, the X does not cause Y can be rejected.\n\n    data      : pandas dataframe containing the time series variables\n    variables : list containing names of the time series variables.\n    \"\"\"\n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + '_x' for var in variables]\n    df.index = [var + '_y' for var in variables]\n    return df\n\ngrangers_causation_matrix(dataD[useweather], variables = useweather)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" The usage is not caused by any of the variables. As p>=0.05, We couldn't reject the null hypothesis"},{"metadata":{},"cell_type":"markdown","source":"**VAR model**"},{"metadata":{},"cell_type":"markdown","source":"The basis behind Vector AutoRegression is that each of the time series in the system influences each other. That is, you can predict the series with past values of itself along with other series in the system. So, we won't be able to model VAR for this problem because the variables failed Granger's Causaulity"},{"metadata":{},"cell_type":"markdown","source":"### <div id= '5'>5.Conclusion</div>"},{"metadata":{},"cell_type":"markdown","source":"To conclude, Auto-Arima performed better than LSTM and Prophet. Due to limited amount of data, statsmodels outshined neural networks"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/conclusion/kagglextreme.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"Future Work: Understanding [fireTs](https://pypi.org/project/fireTS/) and non-linear modelling of time-series."},{"metadata":{},"cell_type":"markdown","source":"                                                            * * *"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}