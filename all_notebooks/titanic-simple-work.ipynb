{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Purpose:\n## 1_overview on data;\n## 2_a little bit of feature engineering;\n## 3_use of simple classifiers: decision tree, random forest.","metadata":{}},{"cell_type":"markdown","source":"## Load datasets.\n#### Load train and test files, apply the same steps to each dataset","metadata":{}},{"cell_type":"code","source":"#df dataframe from train data\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf.info()\n\n#dt dataframe from test data\ndt = pd.read_csv('/kaggle/input/titanic/test.csv')\ndt.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cabin feature has too few values, Ticket feature provides no useful information.\n#### Ticket and Cabin will be dropped out.\n#### Convert Sex feature to numeric in order to visualize its correlation with Survived.","metadata":{}},{"cell_type":"code","source":"df1 = df.drop(columns=['Ticket', 'Cabin'])\ndt1 = dt.drop(columns=['Ticket', 'Cabin'])\n\nfor i, row in df1.iterrows():\n    if df1.iloc[i, 4] == 'male':\n        df1.iloc[i, 4] = 0\n    if df1.iloc[i, 4] == 'female':\n        df1.iloc[i, 4] = 1\ndf1 = df1.astype({'Sex' : int})\n\nfor i, row in dt1.iterrows():\n    if dt1.iloc[i, 3] == 'male':\n        dt1.iloc[i, 3] = 0\n    if dt1.iloc[i, 3] == 'female':\n        dt1.iloc[i, 3] = 1\ndt1 = dt1.astype({'Sex' : int})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot heatmap graph for correlations of features vs. Survived.","metadata":{}},{"cell_type":"code","source":"#dfx = df.drop('PassengerId', axis=1)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorr = np.abs(df1.drop('PassengerId', axis=1).corr())\nmask = np.zeros_like(corr)\nmask[np.tril_indices_from(mask)] = True\n\nfig, ax = plt.subplots(figsize=(6,6))\nax = sns.heatmap(corr, mask=mask, annot=True, cmap='YlGnBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bar graph with ordered values of correlations.","metadata":{}},{"cell_type":"code","source":"l = abs(df1.corr()['Survived']).sort_values(axis=0, ascending=False)\nl[1:-1].plot(kind='bar', color='r');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sex, Pclass and Fare show best correlation with Survived.\n#### Initially, I thought Age would be a good predictor, but it has almost the same value of correlation of Parch and SbSp, and all under 0.1.\n#### So, keep Sex, Pclass and Fare and try to manipulate Age, Parch and SibSp in order to create better predictors.","metadata":{}},{"cell_type":"markdown","source":"# Fill missing values.\n#### First of all, fill missing values in train and test datasets.\n#### In Age feature replace missing data with substituted values.\n#### An initial approach might be imputing the mean age value (mean value of age) in every empty slot; I prefer to try to infer the value on the basis of the title contained in Name feature: as a rule, a “Ms” should be younger than a “Mrs”;  I hope this might keep a better distribution in Age.\n#### Divide Name column into three columns: name, surname and title; name feature is useless, surname feature may be used to analyze family groups, but with regards to age, what matters is the title, so I'll use this feature and discard the other two.\n","metadata":{}},{"cell_type":"code","source":"title = pd.DataFrame(df1.Name.str.split(',', expand=True))[1].str.split('.', expand=True)[0].str.strip()\n#title\nsurname = pd.DataFrame(df1.Name.str.split(',', expand=True))[0].str.split('.', expand=True)[0].str.strip()\n#surname\nname = pd.DataFrame(df1.Name.str.split(',', expand=True))[1].str.split('.', expand=True)[1].str.strip()\n#name\ndf1['title']=title\ndf1['surname']=surname\ndf1['name']=name\ndf1.drop(columns=['Name', 'surname', 'name'], inplace=True)\n#df1.head(3)\n\ntitle = pd.DataFrame(dt1.Name.str.split(',', expand=True))[1].str.split('.', expand=True)[0].str.strip()\n#title\nsurname = pd.DataFrame(dt1.Name.str.split(',', expand=True))[0].str.split('.', expand=True)[0].str.strip()\n#surname\nname = pd.DataFrame(dt1.Name.str.split(',', expand=True))[1].str.split('.', expand=True)[1].str.strip()\n#name\ndt1['title']=title\ndt1['surname']=surname\ndt1['name']=name\ndt1.drop(columns=['Name', 'surname', 'name'], inplace=True)\n#dt1.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of different titles in train dataset:', len(df1.title.unique()))\nprint(df1.title.unique())\nprint('****************')\nprint('Number of different titles in test dataset:', len(dt1.title.unique()))\nprint(dt1.title.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.groupby('title').count()['Age'], dt1.groupby('title').count()['Age']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I'll keep only the most frequent titles. The remaining titles and related data will be allocated according to the pattern as detailed below:\n\n##### Capt, Col, Don, Major, Rev, Sir ---> Mr\n##### Dr: 1 is female 5 are male.\n##### ---------------------------Dr male ---> Mr\n##### ---------------------------Dr female ---> Mrs\n##### Jonkheer ---> Mr\n##### Lady, the Countess, Dona ---> Mrs\n##### Mlle, Mme, Ms ---> Miss\n##### Master is used for children (male) under 13\n","metadata":{}},{"cell_type":"code","source":"df1.at[(df1.title=='Dr') & (df1.Sex==1), 'title']='Mrs'\n\ndf1.at[(df1.title=='Mlle'), 'title']='Miss'\ndf1.at[(df1.title=='Mme'), 'title']='Miss'\ndf1.at[(df1.title=='Ms'), 'title']='Miss'\ndf1.at[(df1.title=='Lady'), 'title']='Mrs'\ndf1.at[(df1.title=='the Countess'), 'title']='Mrs'\n\ndf1.at[(df1.title=='Dr'), 'title']='Mr'\ndf1.at[(df1.title=='Jonkheer'), 'title']='Mr'\ndf1.at[(df1.title=='Sir'), 'title']='Mr'\ndf1.at[(df1.title=='Rev'), 'title']='Mr'\ndf1.at[(df1.title=='Major'), 'title']='Mr'\ndf1.at[(df1.title=='Don'), 'title']='Mr'\ndf1.at[(df1.title=='Col'), 'title']='Mr'\ndf1.at[(df1.title=='Capt'), 'title']='Mr'\n\n#df1.title.unique()\n\ndt1.at[(df1.title=='Ms'), 'title']='Miss'\ndt1.at[(dt1.title=='Ms'), 'title']='Miss'\ndt1.at[(dt1.title=='Dona'), 'title']='Mrs'\n\ndt1.at[(dt1.title=='Dr'), 'title']='Mr'\ndt1.at[(dt1.title=='Rev'), 'title']='Mr'\ndt1.at[(dt1.title=='Col'), 'title']='Mr'\n\n#dt1.title.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_mr = df1.groupby('title').mean()['Age'].loc['Mr'].round(0)\nage_mrs = df1.groupby('title').mean()['Age'].loc['Mrs'].round(0)\nage_miss = df1.groupby('title').mean()['Age'].loc['Miss'].round(0)\nage_master = df1.groupby('title').mean()['Age'].loc['Master'].round(0)\n#age_mr, age_mrs, age_miss, age_master\n\ndf1.at[(df1.Age.isnull()) & (df1.title=='Mr'), 'Age']=age_mr\ndf1.at[(df1.Age.isnull()) & (df1.title=='Mrs'), 'Age']=age_mrs\ndf1.at[(df1.Age.isnull()) & (df1.title=='Miss'), 'Age']=age_miss\ndf1.at[(df1.Age.isnull()) & (df1.title=='Master'), 'Age']=age_master","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_mr = dt1.groupby('title').mean()['Age'].loc['Mr'].round(0)\nage_mrs = dt1.groupby('title').mean()['Age'].loc['Mrs'].round(0)\nage_miss = dt1.groupby('title').mean()['Age'].loc['Miss'].round(0)\nage_master = dt1.groupby('title').mean()['Age'].loc['Master'].round(0)\n#age_mr, age_mrs, age_miss, age_master\n\ndt1.at[(dt1.Age.isnull()) & (dt1.title=='Mr'), 'Age']=age_mr\ndt1.at[(dt1.Age.isnull()) & (dt1.title=='Mrs'), 'Age']=age_mrs\ndt1.at[(dt1.Age.isnull()) & (dt1.title=='Miss'), 'Age']=age_miss\ndt1.at[(dt1.Age.isnull()) & (dt1.title=='Master'), 'Age']=age_master","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.info(), dt1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### At this point, we still have missing values in Embarked in train dataset and in Fare in test dataset. In this case, I’ll replace the empty values with the most common value in Embarked and with the mean value in Fare.","metadata":{}},{"cell_type":"code","source":"df1.groupby('Embarked')['PassengerId'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.fillna(value={'Embarked':'S'}, inplace=True)\ndf1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dt1.Fare.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1.fillna(value={'Fare':dt1.Fare.mean()}, inplace=True)\ndt1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now, the two datasets show no missing values and we can start to manipulate them.\n#### Pclass and Sex, no change is required: they're already numeric features with acceptable correlation with Survived.\n#### Embarked is a categorical feature that can be easily converted into a numeric.\n","metadata":{}},{"cell_type":"code","source":"for i, row in df1.iterrows():\n    if df1.iloc[i, 8] == 'Q':\n        df1.iloc[i, 8] = 0\n    if df1.iloc[i, 8] == 'C':\n        df1.iloc[i, 8] = 1\n    if df1.iloc[i, 8] == 'S':\n        df1.iloc[i, 8] = 2\n\ndf1.Embarked = df1['Embarked'].astype('int64')\n#df1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, row in dt1.iterrows():\n    if dt1.iloc[i, 7] == 'Q':\n        dt1.iloc[i, 7] = 0\n    if dt1.iloc[i, 7] == 'C':\n        dt1.iloc[i, 7] = 1\n    if dt1.iloc[i, 7] == 'S':\n        dt1.iloc[i, 7] = 2\n        \ndt1.Embarked = dt1['Embarked'].astype('int64')\n#dt1.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SibSp and Parch have low correlations with Survived; basically, they tell us if a passenger is travelling alone or not. I summarized this information in a single and more basic feature, 'solo', in order to see if this new feature has a stronger correlation with Survived.","metadata":{}},{"cell_type":"code","source":"df1['solo'] = 0\ndf1.at[(df1.SibSp==0) & (df1.Parch==0), 'solo']= 1\n#df1.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1['solo'] = 0\ndt1.at[(dt1.SibSp==0) & (dt1.Parch==0), 'solo']= 1\n#dt1.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = np.abs(df1.drop('PassengerId', axis=1).corr())\nmask = np.zeros_like(corr)\nmask[np.tril_indices_from(mask)] = True\n\nfig, ax = plt.subplots(figsize=(6,6))\nax = sns.heatmap(corr, mask=mask, annot=True, cmap='YlGnBu')\n\nprint(' Correlation Survived/SibSp: ', df1.Survived.corr(df1.SibSp), '\\n',\n      'Correlation Survived/Parch: ', df1.Survived.corr(df1.Parch), '\\n',\n      'Correlation Survived/alone: ', df1.Survived.corr(df1.solo), '\\n'\n     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = abs(df1.corr()['Survived']).sort_values(axis=0, ascending=False)\nl[['solo', 'Parch', 'SibSp']].plot(kind='bar', color='r');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It seems that “solo” has a better correlation with Survived, so I’ll keep it and discard the two original features (SibSp and Parch). ","metadata":{}},{"cell_type":"code","source":"df1.drop(columns=['SibSp', 'Parch'], inplace=True)\ndf1.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1.drop(columns=['SibSp', 'Parch'], inplace=True)\ndt1.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert title into numerical feature.","metadata":{}},{"cell_type":"code","source":"for i, row in df1.iterrows():\n    if df1.iloc[i, 7] == 'Mr':\n        df1.iloc[i, 7] = 0\n    if df1.iloc[i, 7] == 'Master':\n        df1.iloc[i, 7] = 1\n    if df1.iloc[i, 7] == 'Miss':\n        df1.iloc[i, 7] = 2\n    if df1.iloc[i, 7] == 'Mrs':\n        df1.iloc[i, 7] = 3\ndf1.title = df1['title'].astype('int64')\ndf1.head(3) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, row in dt1.iterrows():\n    if dt1.iloc[i, 6] == 'Mr':\n        dt1.iloc[i, 6] = 0\n    if dt1.iloc[i, 6] == 'Master':\n        dt1.iloc[i, 6] = 1\n    if dt1.iloc[i, 6] == 'Miss':\n        dt1.iloc[i, 6] = 2\n    if dt1.iloc[i, 6] == 'Mrs':\n        dt1.iloc[i, 6] = 3\n\ndt1.title = dt1['title'].astype('int64')\ndt1.head(3) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now look at Age: it has a low correlation with Survived . Convert its value into log scale and see if it improves correlation.\n\n#### This way, I can drop out Age and keep agelog instead. \n","metadata":{}},{"cell_type":"code","source":"df1['agelog'] = 0\ndf1.agelog = (df1.Age.transform(np.log))\ndf1.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(' Correlation Survived/Age: ', df1.Survived.corr(df1.Age), '\\n',\n      'Correlation Survived/agelog: ', df1.Survived.corr(df1.agelog), '\\n'\n     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1['agelog'] = 0\ndt1.agelog = (dt1.Age.transform(np.log))\n#dt1.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Not a big improvement, but I can drop out Age and keep agelog.","metadata":{}},{"cell_type":"code","source":"df1.drop(columns=['Age'], inplace=True)\ndf1.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1.drop(columns=['Age'], inplace=True)\ndt1.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Set proper index.","metadata":{}},{"cell_type":"code","source":"df1.set_index('PassengerId', inplace=True)\ndf1.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt1.set_index('PassengerId', inplace=True)\ndt1.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scale values.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\ndf2 = pd.DataFrame(scaler.fit_transform(df1),\n                   columns = df1.columns,\n                   index = df1.index)\n\n# print('_______________________')\n# print('Original data:')\n# print('Standard Deviation:')\n# print(np.std(df1, axis = 0))\n# print('_______________________')\n# print('Scaled data:')\n# print('Standard Deviation:')\n# print(np.std(df2, axis = 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt2 = pd.DataFrame(scaler.fit_transform(dt1),\n                   columns = dt1.columns,\n                   index = dt1.index)\n\n# print('_______________________')\n# print('Original data:')\n# print('Standard Deviation:')\n# print(np.std(dt1, axis = 0))\n# print('_______________________')\n# print('Scaled data:')\n# print('Standard Deviation:')\n# print(np.std(dt2, axis = 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot graphs of new correlations.","metadata":{}},{"cell_type":"code","source":"corr = np.abs(df2.corr())\nmask = np.zeros_like(corr)\nmask[np.tril_indices_from(mask)] = True\n\nfig, ax = plt.subplots(figsize=(6,6))\nax = sns.heatmap(corr, mask=mask, annot=True, cmap='YlGnBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = abs(df2.corr()['Survived']).sort_values(axis=0, ascending=False)\nl[1:].plot(kind='bar', color='r');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup models for machine learning.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef dtree(x_train, y_train):\n    clf = DecisionTreeClassifier(max_depth=9) #some tuning is possible here\n    clf.fit(x_train, y_train)\n    return clf\n\ndef rfc(x_train, y_train):\n    clf = RandomForestClassifier(n_estimators=200, max_depth=9, random_state=1) #some tuning is possible here\n    clf.fit(x_train, y_train)\n    return clf \n\n\ndef setup_model(x_train, y_train, cl_type):\n    model = cl_type(x_train, y_train)\n    y_pred = model.predict(x_test)\n    \n    train_score = model.score(x_train, y_train)\n    test_score = accuracy_score(y_test, y_pred)\n    print('Score on train data: ', train_score)\n    print('Score on test data: ', test_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare training data.","metadata":{}},{"cell_type":"code","source":"col = ['Survived', 'title', 'Sex', 'Pclass', 'Fare', 'solo', 'agelog', 'Embarked']\n# modify this list for changing predictors in model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df2[col]\n\nX = data.drop(['Survived'], axis=1)\ny = data['Survived']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choose model, train it, evaluate it.","metadata":{}},{"cell_type":"code","source":"setup_model(x_train, y_train, rfc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up test data and make prediction.","metadata":{}},{"cell_type":"code","source":"test = dt2[col[1:]]\ntest.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = rfc(x_train, y_train)\nprediction = model.predict(test)\nprediction = prediction.astype(int)\n#prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create file for submission.","metadata":{}},{"cell_type":"code","source":"#next lines from my first tutorial, thanks to Alexis Cook!\noutput = pd.DataFrame({'PassengerId': test.index, 'Survived': prediction})\noutput.to_csv('submission_00.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Your most recent submission\n### Name: submission_00.csv\n### Submitted: a few seconds ago (07/04/2020 21:51)\n### Wait time: 1 seconds\n### Execution time: 1 seconds\n### Score: 0.78468\n### Complete\n\n### Used: rfc RandomForestClassifier(n_estimators=200, max_depth=9, random_state=1)\n### Predictors: 'title', 'Pclass', 'Fare', 'solo', 'agelog', 'Embarked'","metadata":{}}]}