{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Machine Learning Classification on Campus Recruitment\n"},{"metadata":{},"cell_type":"markdown","source":"Hello. I conducted an EDA and ML study on Campus Recruitment Dataset in this notebook. I performed Data Analysis on the dataset using visualization tools. Next, I tried using the data to predict whether a candidate would be hired on campus. I've added my comments and inferences under the code snippets."},{"metadata":{},"cell_type":"markdown","source":"## I have been denied access to my account. That's why I'm sharing it again."},{"metadata":{},"cell_type":"markdown","source":"### If you have questions please ask them on the comment section."},{"metadata":{},"cell_type":"markdown","source":"### I will be glad if you can give feedback."},{"metadata":{},"cell_type":"markdown","source":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Basic Data Analysis](#7)\n   1. [gender](#8)\n   1. [ssc_b](#9)\n   1. [hsc_b](#10)\n   1. [degree_t](#11)\n   1. [workex](#12)\n   1. [specialisation](#13)\n   1. [Triple Review](#14)\n1. [Questions](#27)\n   1. [Does percentage matters for one to get placed?](#28)\n   1. [Which degree specialization is much demanded by corporate?](#29)\n1. [Pandas Profiling](#15)\n1. [Correlation](#16)\n1. [Anomaly Detection](#17)\n1. [Missing Values](#18)\n   1. [salary](#19)\n1. [Encoding](#20)\n   1. [Label Encoding](#21)\n   1. [One-Hot Encoding](#22)\n1. [Train-Test Split](#23)\n1. [Scores of Models](#24)\n1. [Evaluation of Models](#25)\n   1. [Another Question: Which factor influenced a candidate in getting placed?](#40)\n1. [Conclusion](#26)      "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> \n# Importing the Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> \n# Read Datas & Explanation of Features & Information About Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pandas.read_csv('/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\ndataset.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***I dropped 'id' column because it can cause unwanted correlation.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(\"sl_no\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> \n## Variable Descriptions"},{"metadata":{},"cell_type":"markdown","source":"***sl_no:** Serial Number\n\n***gender:** Gender- Male='M',Female='F'\n\n***ssc_p:** Secondary Education percentage- 10th Grade\n\n***ssc_b:** Board of Education- Central/ Others\n\n***hsc_p:** Higher Secondary Education percentage- 12th Grade\n\n***hsc_b:** Board of Education- Central/ Others\n\n***hsc_s:** Specialization in Higher Secondary Education\n\n***degree_p:** Degree Percentage\n\n***degree_t:** Under Graduation(Degree type)- Field of degree education\n\n***workex:** Work Experience\n\n***etest_p:** Employability test percentage ( conducted by college)\n\n***specialisation:** Post Graduation(MBA)- Specialization\n\n***mba_p:** MBA percentage\n\n***status:** Status of placement- Placed/Not placed\n\n***salary:** Salary offered by corporate to candidates\n\nSource: https://www.kaggle.com/benroshan/factors-affecting-campus-placement"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> \n## Univariate Variable Analysis"},{"metadata":{},"cell_type":"markdown","source":"*** Categorical Variables:** ['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation', 'status']\n\n*** Numerical Variables:** ['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary']"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> \n### Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nfor c in categorical_list:\n    bar_plot(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> \n### Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n in numerical_float64_list:\n    plot_hist(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.histplot(dataset['ssc_p'], color = 'red', kde = True).set_title('ssc_p Interval and Counts')\n\nplt.subplot(2,3,2)\nsns.histplot(dataset['hsc_p'], color = 'green', kde = True).set_title('hsc_p Interval and Counts')\n\nplt.subplot(2,3,3)\nsns.histplot(dataset['degree_p'], kde = True, color = 'blue').set_title('degree_p Interval and Counts')\n\nplt.subplot(2,3,4)\nsns.histplot(dataset['etest_p'], kde = True, color = 'pink').set_title('etest_p Interval and Counts')\n\nplt.subplot(2,3,5)\nsns.histplot(dataset['mba_p'], kde = True, color = 'yellow').set_title('mba_p Interval and Counts')\n\nplt.subplot(2,3,6)\nsns.histplot(dataset['salary'], kde = True, color = 'black').set_title('salary Interval and Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,3,1)\nsns.boxenplot(x=dataset['status'], y=dataset['ssc_p'],\n              color=\"r\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,2)\nsns.boxenplot(x=dataset['status'], y=dataset['hsc_p'],\n              color=\"g\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,3)\nsns.boxenplot(x=dataset['status'], y=dataset['degree_p'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,4)\nsns.boxenplot(x=dataset['status'], y=dataset['etest_p'],\n              color=\"pink\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,5)\nsns.boxenplot(x=dataset['status'], y=dataset['mba_p'],\n              color=\"yellow\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,6)\nsns.boxenplot(x=dataset['status'], y=dataset['salary'],\n              color=\"black\", \n              scale=\"linear\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> \n# Basic Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['status'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"status_mapping = {'Not Placed': 0, 'Placed': 1}\ndataset['status'] = dataset['status'].map(status_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> \n## Gender - Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"gender\",\"status\"]].groupby([\"gender\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dataset\n\nlabels = dataset['gender'].value_counts().index\npie1 = dataset['gender'].value_counts().values\n# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"\",\n      \"hoverinfo\":\"label+percent+name+value\",\n      \"hole\": .2,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Distribution of Genders\",\n        \"annotations\": [\n            { \"font\": { \"size\": 25},\n              \"showarrow\": True,\n              \"text\": \"Genders\",\n                \"x\": 1,\n                \"y\": 1,\n            },\n        ]\n    }\n}\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> \n## ssc_b - status"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"ssc_b\",\"status\"]].groupby([\"ssc_b\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['ssc_b'].value_counts().index\nsizes = dataset['ssc_b'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'ssc_b'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> \n## hsc_b - status"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"hsc_b\",\"status\"]].groupby([\"hsc_b\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['hsc_b'].value_counts().index\nsizes = dataset['hsc_b'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'hsc_b'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a> \n## degree_t - status"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"degree_t\",\"status\"]].groupby([\"degree_t\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['degree_t'].value_counts().index\nsizes = dataset['degree_t'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'degree_t'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> \n## workex - status"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"workex\",\"status\"]].groupby([\"workex\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = dataset['workex'].value_counts()\n\nplt.figure(figsize=(10,7))\nsns.barplot(x=counts.index, y=counts.values, palette=\"Set3\")\n\nplt.ylabel('Count')\nplt.xlabel('workex', style = 'normal', size = 24)\n\nplt.xticks(rotation = 45, size = 12)\nplt.yticks(rotation = 45, size = 12)\n\nplt.title('Distribution of workex',color = 'black',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a> \n## specialisation - status"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[[\"specialisation\",\"status\"]].groupby([\"specialisation\"], as_index = False).count().sort_values(by=\"status\",ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataset['specialisation'].value_counts().index\nsizes = dataset['specialisation'].value_counts().values\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title(\"Distribution of Samples by 'specialisation'\",color = 'black',fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a> \n## Triple review"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,3,1)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"ssc_b\", data=dataset, palette=\"PRGn\")\n\nplt.subplot(2,3,2)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"hsc_b\", data=dataset, palette=\"Wistia_r\")\n\nplt.subplot(2,3,3)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"hsc_s\", data=dataset, palette=\"gist_ncar_r\")\n\nplt.subplot(2,3,4)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"degree_t\", data=dataset, palette=\"gist_earth\")\n\nplt.subplot(2,3,5)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"workex\", data=dataset, palette=\"rocket_r\")\n\nplt.subplot(2,3,6)\nsns.swarmplot(x = dataset[dataset['status'] == 1]['status'], y=\"salary\",hue=\"specialisation\", data=dataset, palette=\"twilight\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"27\"></a> \n# Questions"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"28\"></a> \n## Does percentage matters for one to get placed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\n\n\nsns.boxenplot(x=dataset['status'], y=dataset['degree_p'],\n              color=\"b\", \n              scale=\"linear\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=\"status\", y=\"degree_p\", data=dataset, palette=\"PRGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**When we look at the graphics that placed above, we can see that percentage is important to get placed. The percentage value of those who are 'placed' starts from 55-60, while the percentage value of those who are 'not placed' starts from around 50. It can be said that the higher the 'degree_p' value, the higher the probability of being 'placed'.**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"29\"></a> \n## Which degree specialization is much demanded by corporate?"},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = dataset['specialisation'].value_counts()\n\n#dataset[dataset['status'] == 1]['status']\n\nplt.figure(figsize=(10,7))\nsns.barplot(x=counts.index, y=counts.values, palette=\"Set3\")\n\nplt.ylabel('Count')\nplt.xlabel('workex', style = 'normal', size = 24)\n\nplt.xticks(rotation = 45, size = 12)\nplt.yticks(rotation = 45, size = 12)\n\nplt.title('Distribution of specialisation',color = 'black',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As you can see in this barplot, Mkt&Fin specialization is much demanded by corporate.**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a> \n# Pandas Profiling"},{"metadata":{},"cell_type":"markdown","source":"Pandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a> \n# Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, hue = 'status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a> \n# Anomaly Detection"},{"metadata":{},"cell_type":"markdown","source":"Anomaly is one that differs / deviates significantly from other observations in the same sample. An anomaly detection pattern produces two different results. The first is a categorical tag for whether the observation is abnormal or not; the second is a score or trust value. Score carries more information than the label. Because it also tells us how abnormal the observation is. The tag just tells you if it's abnormal. While labeling is more common in supervised methods, the score is more common in unsupervised and semisupervised methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This code is retrieved from here: https://www.kaggle.com/kanncaa1/dataiteam-titanic-eda#Introduction\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.loc[detect_outliers(dataset,['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p', 'salary'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***As you can see, there is no outliar data.***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a> \n# Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 67 null values in total. salary includes all. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\"></a> \n## salary"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[dataset['salary'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**When we look at those who do not have 'Salary' data, we see that 'status' = 0. This means that those with 'status' = 0 do not receive a salary. Therefore, I will replace these people's salary column with 0.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['salary'] = dataset['salary'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[dataset['salary'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no null values anymore."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"20\"></a> \n# Encoding"},{"metadata":{},"cell_type":"markdown","source":"I will handle Categorical Values."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var = ['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation']\n\nfor i in range (0, len(cat_var)):\n    print(f'Unique Values for {cat_var[i]}', dataset[f'{cat_var[i]}'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will perform Label Encoding for 'gender', 'ssc_b', 'hsc_b', 'workex' and 'specialisation'. For the others I will make One-Hot Encoding."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"21\"></a> \n## Label Encoding"},{"metadata":{},"cell_type":"markdown","source":"Label Encoding is an encoding technique for handling categorical variables. In this technique, each data is assigned a unique integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_mapping = {'M': 0, 'F': 1}\ndataset['gender'] = dataset['gender'].map(gender_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ssc_b_mapping = {'Others': 0, 'Central': 1}\ndataset['ssc_b'] = dataset['ssc_b'].map(ssc_b_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hsc_b_mapping = {'Others': 0, 'Central': 1}\ndataset['hsc_b'] = dataset['hsc_b'].map(hsc_b_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"workex_mapping = {'No': 0, 'Yes': 1}\ndataset['workex'] = dataset['workex'].map(workex_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specialisation_mapping = {'Mkt&HR': 0, 'Mkt&Fin': 1}\ndataset['specialisation'] = dataset['specialisation'].map(specialisation_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"22\"></a> \n## One-Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"One Hot Encoding is the binary representation of categorical variables. This process requires categorical values to be mapped to integer values first. Next, each integer value is represented as a binary vector with all values zero except the integer index marked with 1.\n\nOne Hot Encoding makes the representation of categorical data more expressive and easy. Many machine learning algorithms cannot work directly with categorical data, so categories must be converted to numbers. This operation is required for input and output variables that are categorical.\n\nIn this part, I converted categorical datas to the binary values. This operation increases the accuracy.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"onehotencoder = OneHotEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot = ['hsc_s', 'degree_t']\n\nfor i in range(0, len(one_hot)):\n    dataset[f'{one_hot[i]}'] = pd.Categorical(dataset[f'{one_hot[i]}'])\n    dummies = pd.get_dummies(dataset[f'{one_hot[i]}'], prefix = f'{one_hot[i]}_encoded')\n    dataset.drop([f'{one_hot[i]}'], axis=1, inplace=True)\n    dataset = pd.concat([dataset, dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we don't have categorical variables. Dataset is ready for Machine Leraning algorithms."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"23\"></a> \n# Train - Test Split"},{"metadata":{},"cell_type":"markdown","source":"**Why am I dropping the 'salary' column?**\n\n* When you train ML algorithms without dropping the 'salary' column, you will find that the accuracy is 100% for most. This is because those who are 'placed' have a salary value and those who are 'not placed' have a 'salary' value of 0.\n* When a classification is made in this way, those with 'salary' = 0 are directly classified as 'not placed'. While correct, we want to determine whether the candidates are 'placed' or 'not placed' based on their given qualifications.\n* Therefore, I will classify it by dropping the 'salary' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(\"salary\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = dataset.columns.drop('status')\n\nlabel = ['status']\n\nX = dataset[features]\ny = dataset[label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardization is a method in which the mean value is 0 and the standard deviation is 1, and the distribution approaches the normal. The formula is as follows, we subtract the average value from the value we have, then divide it by the variance value."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"24\"></a> \n# Scores of Models"},{"metadata":{},"cell_type":"markdown","source":"These are the ML algorithms that will apply to dataset. Results will contain train-validation-test scores, confusion matrix, statistical information and classification reports for each algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  score = model.score(X_valid, y_valid)\n  #print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) / (tp + fp + tn + fn)\n  precision = tp / (tp + fp)\n  recall    = tp / (tp + fn)\n  f1score  = 2 * precision * recall / (precision + recall)\n  specificity = tn / (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n  print(\"\")\n   \n  for m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**When we look at the accuracy scores, we are predicting the 'status' with 88% accuracy with GBC and SVM. Also, when we look at the another statistical values such as, precision, recall, f1 score and specifity, we can conclude that out predictions are accurate.**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"25\"></a> \n# Evaluation of Models"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"40\"></a> \n## Another Question: Which factor influenced a candidate in getting placed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")\n\n  imp_feature = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})\n  plt.figure(figsize=(10,4))\n  plt.title(\"Feature Importance Graphic\")\n  plt.xlabel(\"importance \")\n  plt.ylabel(\"features\")\n  plt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source for Feature Importance Code: https://www.kaggle.com/umutalpaydn/heart-disease-analysis-classification#Feature-Importance"},{"metadata":{},"cell_type":"markdown","source":"**When we ask this question to the three classification algorithms, we see that the 'ssc_p' value is the most important property for the candidate to be 'placed'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"26\"></a> \n# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I examined Campus Recruitment Dataset. Firstly, I made Exploratory Data Analysis, Visualization, then I applied Machine Learning algorithms to this dataset. \n\nIf you liked this notebook, you may want to see my other notebooks :)\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, please let me know :)\n\n* ***Thank you for your time.***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}