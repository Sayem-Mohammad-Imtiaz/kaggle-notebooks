{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction "},{"metadata":{},"cell_type":"markdown","source":"##### This study predicts which bank customers will churn by means of machine learning modelling techniques. It presents a full machine learning work flow, use 11 Machine Learning algorithms, tune their parameters and ensemble the best n (e.g. 3) of them using their accuracy scores for the validation set. "},{"metadata":{},"cell_type":"markdown","source":"<font color = 'blue'>\n CONTENTS:  \n    \n   1. [Introduction](#1)\n       * 1.1 [Summary Information about the variables and their types in the data](#1.1)\n   2. [Exploratory Data Analysis](#2)\n       * 2.1 [Importing Libraries and Loading Data](#2.1)\n       * 2.2 [Basic summary statistics about the data](#2.2)            \n       * 2.3 [Visualizations](#2.3)\n           * 2.3.1 [Correlation matrix as heatmap](#4.1)\n           * 2.3.2 [Tenure and Exited](#4.2)\n           * 2.3.3 [Gender versus Exited](#4.3)\n           * 2.3.4 [Age versus Exited](#4.4)\n           * 2.3.5 [Balance versus Survived](#4.5)\n           * 2.3.6 [EstimatedSalary versus Exited](#4.6)\n           * 2.3.7 [Creeditscore versus Exited](#4.7)\n   3. [Data Preprocessing](#3)\n       * 3.1 [Splitting the data as train and validation data](#3.1)  \n       * 3.2 [Handling Categorical Variables](#3.6)\n           * 3.2.1 [Label encoding of gender variable and removing surname](#3.6.1)            \n           * 3.2.3 [One hot encoding of Geography (Country)](#3.6.2)   \n       * 3.3 [Memory Reduction](#3.3)\n   4. [Modeling, Model Evaluation and Model Tuning](#6)\n       * 4.1 [Validation Set Test Accuracy for the default models](#6.2) \n       * 4.2 [Cross validation accuracy and std of the default models for all the train data](#6.3)    \n       * 4.3 [Model tuning using crossvalidation](#6.4)   \n       * 4.4 [Ensembling first n (e.g. 5) models](#6.6) \n\n "},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Summary Information about the variables and their types in the data <a id = '1.1'></a><br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"* Surname          : The surname of the customer\n* CreditScore      : The credit score of the customer\n* Geography        : The country of the customer(Germany/France/Spain)\n* Gender           : The gender of the customer (Female/Male)\n* Age              : The age of the customer  \n* Tenure           : The customer's number of years in the in the bank \n* Balance          : The customer's account balance\n* NumOfProducts    : The number of bank products that the customer uses \n* HasCrCard        : Does the customer has a card? (0=No,1=Yes) \n* IsActiveMember   : Does the customer has an active mebership (0=No,1=Yes) \n* EstimatedSalary  : The estimated salary of the customer\n* Exited           : Churned or not? (0=No,1=Yes)"},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis <a id = '2'></a><br> "},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Importing Libraries and Loading Data <a id = '2.1'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\nimport re\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport sys\nif not sys.warnoptions:\n    import os, warnings\n    warnings.simplefilter(\"ignore\") \n    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\n#timer\nimport time\nfrom contextlib import contextmanager\n\n# Importing modelling libraries\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\npd.options.display.float_format = \"{:,.2f}\".format\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} done in {:.0f}s\".format(title, time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read train and test data with pd.read_csv():\ndf = pd.read_csv(\"../input/churn-modelling/Churn_Modelling.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Basic summary statistics about the data <a id = '2.2'></a><br>"},{"metadata":{},"cell_type":"markdown","source":"##### Descriptive statistics excluding CustomerId and row number which do not carry any meaningful information for Survival."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[:,2:len(df)].describe([0.01,0.1,0.25,0.5,0.75,0.99]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    for var in df:\n        if var != 'Exited':\n            if len(list(df[var].unique())) <= 11:\n                    print(pd.DataFrame({'Mean_Exited': df.groupby(var)['Exited'].mean()}), end = \"\\n\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Visualizations <a id = '2.3'></a><br> "},{"metadata":{},"cell_type":"markdown","source":"In this section we are going to illustrate the relationship between variables by using visualization tools."},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Correlation matrix <a id = '4.1'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's visualize the correlations between numerical features of the train set.\nfig, ax = plt.subplots(figsize=(12,6)) \nsns.heatmap(df.loc[:,'Surname':'Exited'].corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 Tenure and Exited <a id = '4.2'></a><br>       "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In the mid tenure level there is less exit."},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.factorplot(x = \"Tenure\", y = \"Exited\", data = df, kind = \"bar\", size = 4)\ng.set_ylabels(\"Churn Probability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"df['Tenure'].head()"},{"metadata":{},"cell_type":"markdown","source":" ### 2.3.3 Gender and Exited <a id = '4.3'></a><br>       "},{"metadata":{},"cell_type":"markdown","source":"Females exits more."},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.factorplot(x = \"Gender\", y = \"Exited\", data = df, kind = \"bar\", size = 5)\ng.set_ylabels(\"Churn Probability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 2.3.4 Age versus Exited <a id = '4.4'></a><br>   "},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 2.3.5 Balance versus Exited <a id = '4.4'></a><br>   "},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"Balance\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### 2.6 EstimatedSalary versus Exited <a id = '4.4'></a><br>   "},{"metadata":{},"cell_type":"markdown","source":"Although the they are similar, there seems to be slightly higher salaries for the exited customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"EstimatedSalary\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## 2.3.7 Creeditscore versus Exited <a id = '4.7'></a><br>   "},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"CreditScore\", bins = 25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Preprocessing <a id = '3'></a><br> "},{"metadata":{},"cell_type":"markdown","source":"There is no missing value in the data as seen in section 2.2. In addition, from decriptive statistics we can see that  median and mean values are very similar for most of the numerical variables."},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Splitting the data as train and validation data <a id = '3.1'></a><br>"},{"metadata":{},"cell_type":"raw","source":"The given data is splitted into train and validation sets to test the accuracy of training with the untrained 20% of the sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"##\nxs = df.drop(['RowNumber',\"Exited\"], axis=1)\ntarget = df[\"Exited\"]\nx_train, x_val, y_train, y_val = train_test_split(xs, target, test_size = 0.20, random_state = 0)\n\nval_ids = x_val['CustomerId']\ntrain_ids=x_train['CustomerId']\n\nx_train = x_train.drop(['CustomerId'], axis=1)\nx_val= x_val.drop(['CustomerId'], axis=1)\n\ndf_train=df[df['CustomerId'].isin(train_ids)]\ndf_val=df[df['CustomerId'].isin(val_ids)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Handling Categorical Variables <a id = '3.6'></a><br>"},{"metadata":{},"cell_type":"markdown","source":"### 3.3.1 Label encoding of gender variable and removing surname <a id = '3.6.1'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [x_train,x_val]:\n    df[\"Gender\"]=df[\"Gender\"].map(lambda x: 0 if x=='Female' else 1)\n    df.drop(['Surname'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.2 One hot encoding of Geography (Country) <a id = '3.6.2'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_val= [ pd.get_dummies(data, columns = ['Geography']) for data in [x_train,x_val]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Memory Reduction <a id = '3.4'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [x_train,x_val]:\n    reduce_mem_usage(df)    ","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"<a id = '6'></a><br> \n# 4. Modeling, Evaluation and Model Tuning  "},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Validation Set Accuracy for the default models <a id = '6.2'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"r=1309\nmodels = [LogisticRegression(random_state=r),GaussianNB(), KNeighborsClassifier(),\n          SVC(random_state=r,probability=True),BaggingClassifier(random_state=r),DecisionTreeClassifier(random_state=r),\n          RandomForestClassifier(random_state=r), GradientBoostingClassifier(random_state=r),\n          XGBClassifier(random_state=r), MLPClassifier(random_state=r),\n          CatBoostClassifier(random_state=r,verbose = False)]\nnames = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"SVC\",\"Bagging\",\n             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"Art.Neural_Network\",\"CatBoost\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bcfef788-53b1-47b3-9415-89b551840bd7","_uuid":"7d3745861c316a25489e7c03c7de706fa00f0303","trusted":true},"cell_type":"code","source":"print('Default model validation accuracies for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val) \n    print(name,':',\"%.3f\" % accuracy_score(y_pred, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Cross validation accuracy and std of the default models for all the train data <a id = '6.3'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors=pd.concat([x_train,x_val])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nprint('10 fold Cross validation accuracy and std of the default models for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    kfold = KFold(n_splits=10, random_state=1001)\n    cv_results = cross_val_score(model, predictors, target, cv = kfold, scoring = \"accuracy\")\n    results.append(cv_results)\n    print(\"{}: {} ({})\".format(name, \"%.3f\" % cv_results.mean() ,\"%.3f\" %  cv_results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Model tuning using crossvalidation <a id = '6.4'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Possible hyper parameters\nlogreg_params= {\"C\":np.logspace(-1, 1, 10),\n                    \"penalty\": [\"l1\",\"l2\"], \"solver\":['lbfgs', 'liblinear', 'sag', 'saga'], \"max_iter\":[1000]}\n\nNB_params = {'var_smoothing': np.logspace(0,-9, num=100)}\nknn_params= {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nsvc_params= {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1, 5, 10 ,50 ,100],\n                 \"C\": [1,10,50,100,200,300,1000]}\nbag_params={\"n_estimators\":[50,120,300]}\ndtree_params = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\nrf_params = {\"max_features\": [\"log2\",\"auto\",\"sqrt\"],\n                \"min_samples_split\":[2,3,5],\n                \"min_samples_leaf\":[1,3,5],\n                \"bootstrap\":[True,False],\n                \"n_estimators\":[50,100,150],\n                \"criterion\":[\"gini\",\"entropy\"]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\nxgb_params ={\n        'n_estimators': [50, 100, 200],\n        'subsample': [ 0.6, 0.8, 1.0],\n        'max_depth': [1,2,3,4],\n        'learning_rate': [0.1,0.2, 0.3, 0.4, 0.5],\n        \"min_samples_split\": [1,2,4,6]}\n\nmlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\"max_iter\":[1000]}\ncatb_params =  {'depth':[2, 3, 4],\n              'loss_function': ['Logloss', 'CrossEntropy'],\n              'l2_leaf_reg':np.arange(2,31)}\nclassifier_params = [logreg_params,NB_params,knn_params,svc_params,bag_params,dtree_params,rf_params,\n                     gbm_params, xgb_params,mlpc_params,catb_params]               \n                  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning by Cross Validation  \ncv_result = {}\nbest_estimators = {}\nfor name, model,classifier_param in zip(names, models,classifier_params):\n    with timer(\">Model tuning\"):\n        clf = GridSearchCV(model, param_grid=classifier_param, cv =10, scoring = \"accuracy\", n_jobs = -1,verbose = False)\n        clf.fit(x_train,y_train)\n        cv_result[name]=clf.best_score_\n        best_estimators[name]=clf.best_estimator_\n        print(name,'cross validation accuracy : %.3f'%cv_result[name])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies={}\nprint('Validation accuracies of the tuned models for the train data:', end = \"\\n\\n\")\nfor name, model_tuned in zip(best_estimators.keys(),best_estimators.values()):\n    y_pred =  model_tuned.fit(x_train,y_train).predict(x_val)\n    accuracy=accuracy_score(y_pred, y_val)\n    print(name,':', \"%.3f\" %accuracy)\n    accuracies[name]=accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Ensembling first n (e.g. 3) models <a id = '6.6'></a><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"n=3\naccu=sorted(accuracies, reverse=True, key= lambda k:accuracies[k])[:n]\nfirstn=[[k,v] for k,v in best_estimators.items() if k in accu]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensembling First n Score\n\nvotingC = VotingClassifier(estimators = firstn, voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(x_train, y_train)\nprint(accuracy_score(votingC.predict(x_val),y_val))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"toc-autonumbering":false,"toc-showcode":true,"toc-showmarkdowntxt":true,"toc-showtags":true},"nbformat":4,"nbformat_minor":4}