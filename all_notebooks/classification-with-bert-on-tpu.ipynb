{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_PATH = '../input/bert-base-uncased'\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\nMAX_LEN = 192\nBATCH_SIZE = 128\nV_BATCH_SIZE = 32\nTRAIN_PATH = '../input/ag-news-classification-dataset/train.csv'\nEPOCHS = 5\n# MODEL_PATH = ''\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#class to fetch a row from dataframe one by one and return dataset in well defined format\n\nclass prepare_dataset():\n    def __init__(self, text, label):\n        \n        self.text = text\n        self.label = label\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        \n        text = self.text[idx]\n        text = \" \".join(text.split())\n        label = self.label[idx]\n        \n        #Torkenize the text\n        \n        inputs = self.tokenizer.encode_plus(text , None,\n                                           add_special_tokens=True,\n                                           max_length = self.max_len,\n                                           pad_to_max_length=True)\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n        \n        padding_length = self.max_len - len(ids)\n        \n        \n        #pad the tokenized vectors so that each has the same length of 192\n        \n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids' : torch.tensor(ids, dtype=torch.long),\n            'masks' : torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(label, dtype=torch.float)\n        }\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#class to load the model\n\nclass BertBaseUncased(nn.Module):\n    def __init__(self):\n        super(BertBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.dropout = nn.Dropout(0.2)\n        \n        \n        # 4 output units in last layer since there are 4 classes \n        self.out = nn.Linear(768,4)                      \n        \n    def forward(self,ids, masks, token_type_ids):\n        _, out = self.bert(ids, masks, token_type_ids)\n        out = self.dropout(out)\n        out = self.out(out)\n        \n        return out      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# function to calculate loss\n# BCE with logitsloss is used because model will output logits [1.2, 0.7, 4.3, 2.3]\n# BCE with logiloss will apply sigmoid to these outputs and calculate the BCE loss\ndef loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs,targets)\n\n\ndef train_loop(dataloader, model ,optimizer, device,scheduler):\n    \n    model.train()\n    \n    epoch_loss = 0 \n    \n    counter = 0 \n    for idx, batch in enumerate(dataloader):\n        counter +=1\n        ids = batch['ids']\n        masks = batch['masks']\n        token_type_ids =  batch['token_type_ids']\n        targets = batch['targets']\n         \n        #move data to the accelerator device  GPU or TPU\n        ids = ids.to(device, dtype=torch.long)\n        masks = masks.to(device, dtype=torch.long)\n        token_type_ids= token_type_ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype= torch.float)\n        \n        \n        optimizer.zero_grad()\n        \n        outputs = model(ids=ids, masks=masks, token_type_ids=token_type_ids)\n        \n        loss = loss_fn(outputs,targets)\n        loss.backward()   \n        \n        xm.optimizer_step(optimizer)\n#         optimizer.step()\n\n        scheduler.step()\n    \n    \n        if idx %50 == 0:\n            xm.master_print(f'Batch: {idx} train_loss: {loss.item()}')\n        \n        epoch_loss+=loss.item()\n        \n\n        \n    return epoch_loss/counter\n        \n        \n        \n        \n        \ndef eval_loop(dataloader, model , device):\n    \n    model.eval()  \n    epoch_acc = 0\n    epoch_loss= 0\n    counter = 0 \n    for idx, batch in enumerate(dataloader):\n        counter +=1\n        ids = batch['ids']\n        masks = batch['masks']\n        token_type_ids =  batch['token_type_ids']\n        targets = batch['targets']\n        \n        ids = ids.to(device, dtype=torch.long)\n        masks = masks.to(device, dtype=torch.long)\n        token_type_ids= token_type_ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype= torch.float)\n        \n        \n        \n        outputs = model(ids=ids, masks=masks, token_type_ids=token_type_ids)\n        \n        loss = loss_fn(outputs,targets)\n        \n        #get the index of the maximum value \n        outputs = torch.argmax(outputs,axis=1)\n        targets = torch.argmax(targets,axis=1)\n\n        #calulate the accracy score\n        acc = metrics.accuracy_score(targets.cpu().detach().numpy(),outputs.cpu().detach().numpy())\n        \n        epoch_acc+=acc\n        epoch_loss+= loss.item()\n    \n    final_acc = epoch_acc/counter\n    epoch_loss = epoch_loss/counter\n    return final_acc, epoch_loss\n        \n        \n        \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to convert the integer classes to one hot encoded array\n#example 2 -> [0, 0, 1, 0]\n\ndef ohe(df,target_col):\n    \n    encoded = pd.get_dummies(df.sort_values(by=[target_col])[target_col])\n    \n    df = df.join(encoded)\n    \n    return df\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train():\n    \n    df = pd.read_csv(TRAIN_PATH).fillna('None')\n    \n    #split the data into train and validation sets\n    train, valid = model_selection.train_test_split(df, test_size = 0.15, random_state=42, stratify=df['Class Index'].values)\n    \n    train = train.reset_index(drop=True)\n    valid = valid.reset_index(drop=True)\n    \n    #one hot encode the classes\n    train= ohe(train, 'Class Index')\n    valid = ohe(valid, 'Class Index')\n    \n    train_labels = train[train.columns[-4:]].values\n    valid_labels = valid[valid.columns[-4:]].values\n    \n    \n    train_data = prepare_dataset(text=train['Description'].values,\n                                label=train_labels)\n    \n    valid_data = prepare_dataset(text=valid['Description'].values,\n                                label=valid_labels)\n    \n    \n    train_sampler = torch.utils.data.DistributedSampler(train_data,\n                                                       num_replicas=xm.xrt_world_size(),\n                                                       rank= xm.get_ordinal(),\n                                                       shuffle=True)\n\n    valid_sampler = torch.utils.data.DistributedSampler(valid_data,\n                                                       num_replicas=xm.xrt_world_size(),\n                                                       rank= xm.get_ordinal(),\n                                                       shuffle=False)\n    \n    train_dataloader = torch.utils.data.DataLoader(train_data,batch_size=BATCH_SIZE,num_workers=4,sampler=train_sampler,drop_last=True)\n    valid_dataloader = torch.utils.data.DataLoader(valid_data,batch_size=V_BATCH_SIZE,num_workers=4,sampler=valid_sampler,drop_last=True)\n    \n    \n    \n#     device= torch.device('cuda')\n    \n    \n    device = xm.xla_device()\n        \n\n    model = BertBaseUncased()\n    model.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    \n    num_train_steps = int(len(train_data)/BATCH_SIZE/xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n    \n    lr = 1e-4 * xm.xrt_world_size()\n    \n    optimizer = AdamW(optimizer_parameters,lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n    )\n    \n    best_acc=0\n    \n    for epoch in range(EPOCHS):\n        \n        para_loader = pl.ParallelLoader(train_dataloader, [device])\n        \n        train_loss = train_loop(para_loader.per_device_loader(device),model=model, optimizer=optimizer,scheduler=scheduler,device=device)\n        \n        para_loader = pl.ParallelLoader(valid_dataloader, [device])\n        \n        val_acc, val_loss = eval_loop(para_loader.per_device_loader(device), model, device)\n        \n#         print(f\"EPOCH: {epoch} train_loss: {train_loss} val_loss: {val_loss} val_acc: {val_acc}\")\n        \n        if val_acc > best_acc:\n            torch.save({'model':model.state_dict(), 'optimizer': optimizer.state_dict()},'best_model.bin')\n            \n            best_acc=val_acc\n            \n        \n        xm.master_print(f'Epoch: {epoch+1} train_loss: {train_loss} val_loss: {val_loss} Accracy: {val_acc}')\n            \n            \n        \n        \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = train()\n    \n    \nFlags ={}\nxmp.spawn(_mp_fn,args=(Flags,), nprocs=1,start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}