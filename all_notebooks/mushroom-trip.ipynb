{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.1","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"cells":[{"source":"# Mushroom Trip\n\nJo√£o Pedro Evangelista,\n\nSeptember 03, 2017\n\n## Introduction\n\nHello, this is a analysis towards a model development exploring the mushroom dataset and it's features, we will:\n- Clean the data\n- Explore the features and relationships\n- Encode categories\n- Select best feature to reduce the amount of data needed from future input.\n\n### Start Of\n\nLet's start importing the dataset from the file and seeing it's contents","cell_type":"markdown","metadata":{"_cell_guid":"ff362a11-1d01-49db-b950-afc5e31d527b","_uuid":"1803247326f01dc318dee0c1a147b8170955cdd1"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"019a43f6-4e10-45eb-bac8-7e550e84bf83","_uuid":"c067578c459c7cf4a7d17dfe19965cbe2b131c98"},"source":"import numpy as np\nimport pandas as pd"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9b0428af-2148-4947-ae8f-37e0f05b591d","_uuid":"09b800777c2b9777b92c01d7aa2e8dfb49c20507"},"source":"df = pd.read_csv('../input/mushrooms.csv')"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f0c91030-a4f0-40e7-badc-bdacd38b2289","_uuid":"3e9c73970bdec58c899c25e7c41d7330fa3d5a6f"},"source":"df.describe()"},{"source":"Looking at the describe table, we can find that features are in a good distribution, but the *veil-type* as the same value for count and freq, meaning it has only one unique value, this isn't very important for our model to learn from it, so let's drop it. ","cell_type":"markdown","metadata":{"_cell_guid":"a235e1a8-67a2-4d68-ad70-002c9d2587b0","_uuid":"072215668cfbaece5ac854a2e13d3a742b1143fd"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"31eb2d58-ae5c-4c8a-8237-12ff1e5b8b7c","_uuid":"4d8547115c2e5a4a16dacce91aa0bc6e50e335d9"},"source":"df.drop('veil-type', axis=1, inplace=True)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5f20e2bc-306e-4368-aa87-d392de6940d6","_uuid":"163fd21a401c46d7d4af93a22c22cfc473e52201"},"source":"# find missing values\nfor col in df.columns:\n    u = df[col].unique()\n    if 'nan' in u or 'NaN' in u:\n        print('Missing values at', col)\n    else:\n        print(col, 'ok')"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"8b8db049-380e-40fe-b9f0-400bbc281e7d","_uuid":"5c4218cbfd12e5e7089fd9cde5394759dc0179e9"},"source":"# inspect deeply the uniqueness\nfor col in df.columns:\n    print(col, \"->\", \", \".join(df[col].unique()))"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"975404de-a7c4-4ea3-acab-9a3a927fe91c","_uuid":"a485a2f6a6981f1742561154f416fdc36bc31145"},"source":"# wait, there is only letters, and a ? on stalk-root, a missed missing value?\ndf[df['stalk-root'] == '?']['stalk-root'].count()"},{"source":"### 2480 missing values, but what is 'stalk-root' by the way ?\n\nFrom [wikipedia](https://en.wikipedia.org/wiki/Stipe_%28mycology%29):\n\n>  In mycology, a stipe (/Ààsta…™p/) is the stem or stalk-like feature supporting the cap of a mushroom.\n\n>  The evolutionary benefit of a stipe is generally considered to be in mediating spore dispersal. An elevated mushroom will more easily release its spores into wind currents or onto passing animals. Nevertheless, many mushrooms do not have stipes","cell_type":"markdown","metadata":{"_cell_guid":"6723fcde-191b-499d-8e58-6b7a5717b045","_uuid":"47d36af01c0a645f9c83880d04d50b773e8c4895"}},{"source":"Indeed there is no missing value, but instead a missing category. So let's create a new category for `?` instead of considering a missing value","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"61a4a186-a469-4b51-a1b4-7bd8b3754bf9","_uuid":"f151a149f6ae9d5c753f45f47846fbfeea3ecbe9"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"ab052b0a-1fe6-45d6-93e4-4e27914804ee","_uuid":"f80c62bbe74e99bb6414d18604a8c14a2837ee01"},"source":"df['stalk-root'] = df['stalk-root'].apply(lambda x: 'no-presence' if x == '?' else x)\nu = df['stalk-root'].unique()\nprint('stalk-root ->', ', '.join(u))"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"14f4570a-c55d-4abf-86bd-004dc5458fb1","_uuid":"1d7999aea35653330052cfd604caada1330d1674"},"source":"import missingno as mgo\nmgo.bar(df)"},{"source":"Seems we are good to proceed. There is not missing values, neither visible skewness on our dataset","cell_type":"markdown","metadata":{"_cell_guid":"68c5c5ee-d14d-4b15-a8a2-f3f7a29d3291","_uuid":"56ddb2ca352aa99b1c3936dc58623740150bc1da"}},{"source":"## Analysing Features\n\n### Remember the problem: Classify whenether the mush is safe to eat or not.\n\nü§î One way to think about it, is to incorporate a human expert. Since we want oour model to predict as good as a human expert, who the latter would solve the classification, what features would be useful for them ?\nThinking that way we will search for which features are the most important and how they influence on the resulting class.\n\n### Preparing the field\n\nSince all of our data is based on categories, we will need to somehow make them numerics, because most of visualization tools we will use and the model does not know how to treat strings\nWe will use `sklearn.preprocessing` module to do our job on  a copy of original dataset.","cell_type":"markdown","metadata":{"_cell_guid":"d39a0efe-9530-4ca9-93c3-7a4b150e695e","_uuid":"cfbc6d2a18169b09993a2b43dcd006035b62d7f6"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"66e51d67-046c-4660-a627-80d125fefd1a","_uuid":"d807fb71d04192b2b3d8f7fdf7dae5867c423e31"},"source":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_features(df, encoder=LabelEncoder):\n    \"\"\"Encodes the given df features using an encoder.\n    Returns an array with dict elements mapping the column name and the instance of fitted encoder\n    to be used onwards on inverse transformation, and the transformed dataframe\n    \"\"\"\n    acc = []\n    for name in df.columns:\n        fitted = encoder().fit(df[name].values)\n        df[name] = fitted.transform(df[name].values)\n        dic = (name, fitted)\n        acc.append(dic)\n    return acc, df\n\ndef get_encoder(qname, encoders):\n    \"Search for the encoder of given column name, returning it when found, otherwise None.\"\n    for name, encoder in encoders:\n        if qname == name:\n            return encoder\n    return None"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f542cc34-ce2a-496d-9437-9f925d36760b","_uuid":"b81e6757321c44fe0352d4fb044e23418b30ec4e"},"source":"encoders, edf = encode_features(df.copy())"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"73a04ec3-2948-4f7f-94d3-51adecd98f70","_uuid":"40b0c92d5ebda115316f7e57649d0df7d5ee05b8"},"source":"# check the encoding\nclass_encoder = get_encoder('class', encoders)\nencoded_values_of_class = edf['class'].values\noriginal_values_of_class =  class_encoder.inverse_transform(encoded_values_of_class)\n\npd.DataFrame({'Encoded': encoded_values_of_class, 'Original': original_values_of_class}).head()"},{"source":"üçÑ  Seems our encoder is given *p*, assumed *posion*, the positive label, when our problem is to give a positive class for the ones safe to eat, i.e. the ones named *e*.\n\nThe encoder is a bit hard to refit in order to keep simple when used on a pipeline, because of that we will change our target classification.\n\nBefore our proble was defined as:\n\n$$y = \\begin{cases}1 & edible\\\\0 & poisonous\\end{cases}$$\n\nBut since the encoder is a bit harsh on us, we will invert the question to be: *Is this mushroom poisonous ?*:\n\n$$y = \\begin{cases}1 & poisonous\\\\0 & edible\\end{cases}$$\n\n","cell_type":"markdown","metadata":{"_cell_guid":"b69200f6-ed33-45c4-aa84-c02b6b25c04b","_uuid":"cdfe5807377c9be6dadcaad311ae40e8b3406f94"}},{"source":"### It is time to find the features that influence the most the resulting classification\n\nLet's start with the correlation aproach then decide if we need to move on to a more MLish approach","cell_type":"markdown","metadata":{"_cell_guid":"85872cf0-15f7-45f0-8eaa-2351b71f9fa0","_uuid":"25e6fbdb4a100f3c44157a6750e175767bee2fb9"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9bf8e067-f41f-4e2e-8739-d7762ac7b84e","_uuid":"e9b0c476e00d8dad81d9b9f79b26e9df6d153d4d"},"source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_palette('Set2')"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"94a84a98-19e2-42bc-8b35-c240b4dcf7a0","_uuid":"900b60ca260b621f56b717df96f98ce5d87b77f3"},"source":"corr = edf.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(20, 15))\nax = sns.heatmap(corr, annot=True, linecolor='w', linewidths=0.2, fmt='.2f', mask=mask)\nax.tick_params(axis='both', which='major', labelsize=14)\nplt.show()"},{"source":"Looking at the correlation matrix, we see that are a strong correlation ($ \\geq |0.5| $) with class against the following features:\n\n - bruises\n - gill-color\n - gill-size\n \n Now we will see a classifier to see what features it takes the most importance","cell_type":"markdown","metadata":{"_cell_guid":"a819fb60-e8fc-4034-9702-fbffc3a93dc7","_uuid":"4b7f2437a5aeebb2ee95890eb56c8170a2aa7bca"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"84576d3d-62cd-4047-b63a-00a3304b076b","_uuid":"db143212272afcfc07153564ae8ef829b15607bf"},"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import accuracy_score"},{"source":"### Why Random Forest ?\n\nA Random Forest seems to be a good choice, basically because we are dealing with decisions using categories, which make the algorithm more confident when learning, also it as advantages over a simple Decision Tree such as **usage of a percentage of features per Tree**,  reduced variance and they are fast!","cell_type":"markdown","metadata":{"_cell_guid":"15c38913-fc83-40c0-bd81-6908f538e233","_uuid":"eb347032731c7f01aef9f0f6b38887f47e73fcff"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"60caaa96-ef70-47de-85e6-928172cf0960","_uuid":"6653667e5a7a67994d8b228cc3d49fcdccc1cdbd"},"source":"rfclf = RandomForestClassifier()\nX = edf.drop('class', axis=1)\ny = edf['class'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nrfclf.fit(X_train, y_train)\npred = rfclf.predict(X_test)\nprint('Prediction Accuracy:',accuracy_score(y_test, pred) * 100, '%')"},{"source":"### Yeaaah!! No one dies by mushroom!\n(I hope)","cell_type":"markdown","metadata":{"_cell_guid":"4b4ba020-d209-4cbb-8286-93c32a6e87ce","_uuid":"60f7fc3014607d3e572912848c75d6127019355e"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6751211e-e294-4300-b997-d9b0a6937a7b","_uuid":"ee6b02371d2cf7bc4297bb03301cd8a68645241d"},"source":"importance = rfclf.feature_importances_\nfeats = edf.drop('class', axis=1).columns\nimportance_df = pd.DataFrame({'Features': feats, 'Importance': importance})"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"b9e384fe-7aa8-4c5b-b9fc-1b672f4e37a5","_uuid":"48fc9780296d132694dfa8a73905b62663cf74fe"},"source":"feats = edf.drop('class', axis=1).columns\nimportance_df = pd.DataFrame({'Features': feats, 'Importance': importance})"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"c0912bfd-2ade-437e-bcd1-0e60a3ae29fc","_uuid":"7018a2b30309b0e5125dab98719cd60e6583611a"},"source":"plt.figure(figsize=(20,10))\nplt.title('Feature Importance with RandomForestClassifier', fontsize=16)\nax = sns.barplot(data=importance_df, x='Features', y='Importance')\nax.tick_params(axis='both', which='major', labelsize=14)\nplt.xticks(rotation=90)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('mean(Importance)', fontsize=15)\nplt.show()"},{"source":"Even with the randomness of the RTClassifier, the most appearing features of all runs I did, I get those:\n- odor\n- gill-size\n- gill-color\n- bruises\n\nSo let's explore more on how they are distributed","cell_type":"markdown","metadata":{"_cell_guid":"6e971e2a-e252-4503-a172-32aef56d7243","_uuid":"654028023239708839f3b2f00d6eb733d7b10839"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f901cb33-9545-4cac-aba0-e2ed7aa43729","_uuid":"baa17f58ecb3eebfc1e1893450bc6f7eb55d4e75"},"source":"fig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle('Recurrent features with most importance', fontsize=20)\ngillsize_ax = axs[0][0]\ngillcolor_ax = axs[0][1]\nodor_ax = axs[1][0]\nbruises_ax = axs[1][1]\n\nax = sns.countplot(x='gill-size', data=edf, ax=gillsize_ax, hue='class')\nax.legend(['edible', 'posionous'], loc='best')\nax.set_xticklabels(get_encoder('gill-size', encoders).inverse_transform(edf['gill-size'].unique()))\n\nax = sns.countplot(x='gill-color', data=edf, ax=gillcolor_ax, hue='class')\nax.legend(['edible', 'posionous'], loc='best')\nax.set_xticklabels(get_encoder('gill-color', encoders).inverse_transform(edf['gill-color'].unique()))\n\nax = sns.countplot(x='odor', data=edf, ax=odor_ax, hue='class')\nax.legend(['edible', 'posionous'], loc='best')\nax.set_xticklabels(get_encoder('odor', encoders).inverse_transform(edf['odor'].unique()))\n\nax = sns.countplot(x='bruises', data=edf, ax=bruises_ax, hue='class')\nax.legend(['edible', 'posionous'], loc='best')\nax.set_xticklabels(get_encoder('bruises', encoders).inverse_transform(edf['bruises'].unique()))\nplt.show()"},{"source":"As we can see the bins of each category are distinguished easly, making each contribution a weight one for the output prediction.\n\nNow let's make sklearn selection the features again, for demostration and assurance that our hypothesis is on the right way.","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"3653895b-1f39-4831-bd70-26c08f5eed99","_uuid":"06a556d759a41e9314183fb37f07a7901c7bbc64"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"dda30211-bf64-4a12-8121-ddf649832ff6","_uuid":"a74cde711d2b89fc636cee175f9f3aa710e76ab0"},"source":"from sklearn.feature_selection import SelectKBest, f_classif\n\nselection = SelectKBest(f_classif, k=4)\nkbest_X = selection.fit_transform(X, y)\n# source: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\nmask = selection.get_support()\nkbest_X_acc = []\nfor b, feat_name in zip(mask, edf.columns):\n    if b:\n        kbest_X_acc.append(feat_name)\n\n\npd.DataFrame(kbest_X, columns=kbest_X_acc)"},{"source":"Damn, looks like sklearn is against our hypothesis. It gives us only gill-size as the only feature we also selected. Let's compare with the RF","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"581195a9-2dba-4935-954e-29710d70ef82","_uuid":"c9eeab3f8c2b78a2c239cd0117383ce895c1bb63"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"1c1d58ab-1745-4560-80f4-2e9a506d3900","_uuid":"336c778d64835e3d10f4e2ffb46ff12b3f100de4"},"source":"# with KBest\nkb_clf = RandomForestClassifier()\nX_train, X_test, y_train, y_test = train_test_split(kbest_X, y)\nkb_clf.fit(X_train, y_train)\npred = kb_clf.predict(X_test)\nprint('Accuracy with KBest Features on RandomForest: {:.2f}%'.format(accuracy_score(y_test, pred) *100))"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"a57d101b-62ed-49f2-9602-331f115886f0","_uuid":"c47c5518a7fa59e1fefbb25cd42b2ad37dc7c8f6"},"source":"# with our selected features\nhclf = RandomForestClassifier()\nhX = edf[['gill-size', 'gill-color', 'odor', 'bruises']].values\nX_train, X_test, y_train, y_test = train_test_split(hX, y)\nhclf.fit(X_train, y_train)\npred = hclf.predict(X_test)\nprint('Accuracy with Our Hypothesis Features on RandomForest: {:.2f}%'.format(accuracy_score(y_test, pred) *100))"},{"source":"ü§ò AHA!\nLooks like our classifier fitting only with the features we selected runs better than the one that sklearn selected the features, Even if seems biased because we used RandomForest to select the features, our features perform better on other algorithms.","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"f99b652d-453c-4057-89ad-3d251b028975","_uuid":"4df9511a825dfe62bf853c1057be0c6c92c1a8f9"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"df2faf5d-f1cb-433d-af86-d26c501539e1","_uuid":"6a7fcf18763560f732140ef8d5039c011ca4fad0"},"source":"from sklearn.linear_model import LogisticRegression\n# with KBest\nkb_clf = LogisticRegression()\nX_train, X_test, y_train, y_test = train_test_split(kbest_X, y)\nkb_clf.fit(X_train, y_train)\npred = kb_clf.predict(X_test)\nprint('Accuracy with KBest Features on Logistic Regression: {:.2f}%'.format(accuracy_score(y_test, pred) *100))\n‚Äã\n# with our selected features\nhclf = LogisticRegression()\nhX = edf[['gill-size', 'gill-color', 'odor', 'bruises']].values\nX_train, X_test, y_train, y_test = train_test_split(hX, y)\nhclf.fit(X_train, y_train)\npred = hclf.predict(X_test)\nprint('Accuracy with Our Hypothesis Features on Logistic Regression: {:.2f}%'.format(accuracy_score(y_test, pred) *100))"},{"source":"## Conclusion\n\nHere are some points we can learn from this:\n\n - Not all missing data are useless, sometimes we need to understand why is missing, it could improve our hypothesis.\n- Selecting the best features could also made by hand, well when the data size allows it and you have an expert that helps you, even I did not count on one to select the features from here.\n- Do not forget to explore the data and the correlations, it could give you more insight about your problem\n\n-------------------------------------\nThanks!","cell_type":"markdown","metadata":{"_cell_guid":"5fd29272-b7fb-43b5-91b4-d0af7e551700","_uuid":"d63701dfdb638f73a02924a8dbe7aad87b63e01e"}}]}