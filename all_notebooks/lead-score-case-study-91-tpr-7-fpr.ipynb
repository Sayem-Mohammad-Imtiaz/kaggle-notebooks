{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy             as np \nimport pandas            as pd \nimport matplotlib.pyplot as plt\nimport seaborn           as sns\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. IMPORTING DATA AND ANALYZING","metadata":{}},{"cell_type":"code","source":"#Imporitng the data into enviroment \nleads = pd.read_csv(r'../input/leadscore/Leads.csv')\npd.set_option('display.max_columns',None)\nleads.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Analyzing the data \nleads.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Analyzing the null values \nimport missingno as msno\nmsno.matrix(leads)\nmsno.bar(leads)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. DATA CLEANING","metadata":{}},{"cell_type":"code","source":"#Function to Check the null values in terms of perecentage \ndef null_values_check(leads):\n    null_values    = round((leads.isnull().sum()/len(leads) * 100),2).to_frame().rename(columns={0:'Null_values_percentage'})\n    null_values    = pd.DataFrame(null_values)\n    null_values.reset_index(inplace=True)\n    return null_values.sort_values(by='Null_values_percentage',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to impute the null values with most frequent values \ndef impute(df):\n    from sklearn.impute import SimpleImputer\n    my_imputer           = SimpleImputer(strategy='most_frequent')\n    imputed_data         = pd.DataFrame(my_imputer.fit_transform(df))\n    imputed_data.columns = df.columns\n    return imputed_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the null values \nnull_values_check(leads)[:17]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eliminating the columns having 45 or more than 45% of the null values \nprint('The shape of leads df before deleting columns:{}'.format(leads.shape))\ncol_eliminated = ['Lead Quality','Asymmetrique Activity Index','Asymmetrique Profile Score',\n                 'Asymmetrique Activity Score','Asymmetrique Profile Index']\nleads.drop(columns=col_eliminated,axis=1,inplace=True)\nprint('The shape of leads df after deleting columns:{}'.format(leads.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Analysing the tag,Tags, Lead Profile, What matters most to you in choosing a course,\n#What is your current occupation,Country,How did you hear about X Education,Specialization,City\nleads['Lead Profile'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads['What matters most to you in choosing a course'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads['What is your current occupation'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads['Country'].value_counts()[:13]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads['How did you hear about X Education'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads['Specialization'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads['City'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### IMPUTING the null values with most frequent values","metadata":{}},{"cell_type":"code","source":"#Imputing the null values with the most frequently occuring null values \nleads = impute(leads)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the null values again to verify  \nnull_values_check(leads)[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"code","source":"#Checking the target column to check if our data is balanced or imbalanced \n#Exploring the Target variable\nsns.countplot(x='Converted',data=leads);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the percentage the target values\nround(leads['Converted'].value_counts()/len(leads['Converted'])*100,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. #### We can observe that the Dataset is Imbalanced because the converted Leads are far more less as comapred to the not-converted leads we will handle this further","metadata":{}},{"cell_type":"code","source":"#Function to analyse the categorical variables wrt target variable\ndef eda(col_name1,col_name2,df,l,b):\n    plt.figure(figsize=(l,b))\n    g = sns.countplot(x=col_name1,hue=col_name2,data=df)\n    g.set_xticklabels(labels=g.get_xticklabels(),rotation=90);\n    plt.legend(loc='upper right');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Lead Origin','Converted',leads,13,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Lead Source','Converted',leads,13,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Country','Converted',leads,14,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to analyse the categorical variables wrt target variable\ndef analysing(col_name,df):\n    unique   = df[col_name].unique()\n    Analysis = pd.DataFrame(columns=[col_name,'1_per','0_per','1_count','0_count','Total'])\n    Analysis[col_name] = unique\n    for value in unique:\n        Total_values = len(df[(df[col_name] == value)])\n        Analysis.loc[Analysis[col_name] ==  value,'1_per']   = round((len(df[(df[col_name] == value) & (df['Converted'] == 1)])/Total_values)*100,2)\n        Analysis.loc[Analysis[col_name] ==  value,'0_per']   = round((len(df[(df[col_name] == value) & (df['Converted'] == 0)])/Total_values)*100,2)\n        Analysis.loc[Analysis[col_name] ==  value,'1_count'] = len(df[(df[col_name] == value) & (df['Converted'] == 1)])\n        Analysis.loc[Analysis[col_name] ==  value,'0_count'] = len(df[(df[col_name] == value) & (df['Converted'] == 0)])\n        Analysis.loc[Analysis[col_name] ==  value,'Total']   = Total_values\n    return Analysis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Country',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)[:15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reason why I have analysed using percentage of leads conversion, per country is some information was lost in the graph.\n\n### eg we can see that UNITED STATES graph is not even visible but its second higest country where the Leads come from followed by UAE\n","metadata":{}},{"cell_type":"code","source":"eda('Specialization','Converted',leads,14,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Specialization',leads)\nAnalysis_1.sort_values(by='1_count',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('How did you hear about X Education','Converted',leads,14,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('What is your current occupation','Converted',leads,14,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('What matters most to you in choosing a course','Converted',leads,10,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('What matters most to you in choosing a course',leads)\nAnalysis_1.sort_values(by='1_count',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It is evident that more than 99% of students have same reason to join the course so this column will not help us in decision making, so we will eliminate this column.","metadata":{}},{"cell_type":"code","source":"eda('Tags','Converted',leads,14,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Tags',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columns like this will be useful in decision making as they have various values & conversion rate differs for each one of them.","metadata":{}},{"cell_type":"code","source":"eda('Receive More Updates About Our Courses','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Update me on Supply Chain Content','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Get updates on DM Content','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('I agree to pay the amount through cheque','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can observe that the above columns have only one value, which will not help us in decision making so we need to eliminate these columns","metadata":{}},{"cell_type":"code","source":"eda('Lead Profile','Converted',leads,9,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('City','Converted',leads,9,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('A free copy of Mastering The Interview','Converted',leads,9,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Last Notable Activity','Converted',leads,11,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Last Notable Activity',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Last Activity','Converted',leads,11,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Last Activity',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The columns 'LAST NOTABLE ACTIVITY' & 'LAST ACTIVITY' are having similar values but they have different outcomes so we can retain them both even though they are similar","metadata":{}},{"cell_type":"code","source":"eda('Through Recommendations','Converted',leads,11,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Through Recommendations',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It is evident that more than 95% of students have same values so this column will not help us in decision making, so we will eliminate this column.","metadata":{}},{"cell_type":"code","source":"eda('Do Not Email','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Do Not Email',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can observe that 92% values are assigned to one type, so this column will make our model baised so its better to eliminate it","metadata":{}},{"cell_type":"code","source":"eda('Do Not Call','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Analysis_1 = analysing('Do Not Call',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can observe that 99% values are assigned to one type, so this column will make our model baised so its better to eliminate it coz it wont help in any decision making","metadata":{}},{"cell_type":"code","source":"eda('TotalVisits','Converted',leads,16,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Search','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Magazine','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Newspaper Article','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('X Education Forums','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Newspaper','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('Digital Advertisement','Converted',leads,6,6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We will analyse the columns under \n#Indicating whether the customer had seen the ad in any of the listed items.\n#with a different method\nleads['ad'] = leads['Digital Advertisement'] + leads['Newspaper'] + leads['X Education Forums']+ leads['Newspaper Article'] + leads['Magazine'] + leads['Search']\nleads['ad'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As it is evident that all columns have maximum of no's in it so it wont help us in any decision making as it is reduntant we will eliminate all these columns at once\n\n### Also we can see that the Newspaper column and Newspaper article are same, It is obivious that if a customer has seen the ad in a newspaper article it has come from Newspaper","metadata":{}},{"cell_type":"code","source":"col_to_be_eliminated = ['What matters most to you in choosing a course','Receive More Updates About Our Courses',\n                       'Update me on Supply Chain Content','Get updates on DM Content',\n                       'I agree to pay the amount through cheque','Through Recommendations','Do Not Email',\n                       'Do Not Call','Digital Advertisement','Newspaper','X Education Forums',\n                       'Newspaper Article','Magazine','Search','ad']\nprint('The shape of df before deleting reduntant columns: {}'.format(leads.shape))\nprint('The no of columns to be removed:{}'.format(len(col_to_be_eliminated)))\nleads.drop(columns=col_to_be_eliminated,axis=1,inplace=True)\nprint('The shape of df after  deleting reduntant columns: {}'.format(leads.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We are finally left with only 18 columns lets have a look at the df\nleads.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. DATA PRE-PROCESSING ","metadata":{}},{"cell_type":"markdown","source":"### Converting all the numeric columns to Numeric datatype\n","metadata":{}},{"cell_type":"code","source":"leads = leads.apply(pd.to_numeric, errors='ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Categorical Values","metadata":{}},{"cell_type":"code","source":"#As we are going to build a logistic regression model we need to convert all the categorical\n#variables in numerical values\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncat_list        = leads.select_dtypes('O').columns\n\n#Printing the no of categorical columns before conversion \nprint('Before LabelEncoding')\nprint('The no of categorical columns in dataset are {}'.format(len(leads.select_dtypes('O').columns)))\n\n#Converting the cat columns into numerical \nfor column_name in cat_list:\n    leads[column_name] = le.fit_transform(leads[column_name])\n\n#Printing the no of categorical columns before conversion \nprint('---'*30)\nprint('After LabelEncoding')\nprint('The no of categorical columns in dataset are {}'.format(len(leads.select_dtypes('O').columns)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Analyzing the column once more \nleads.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the data into Train and Test ","metadata":{}},{"cell_type":"code","source":"#Splitting the data \nfrom sklearn.model_selection import train_test_split\ncol_list = ['Prospect ID', 'Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit','Last Activity', 'Country', \n          'Specialization','How did you hear about X Education', 'What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity'] \n\nX = leads[col_list].copy()\ny = leads['Converted'].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling the Data Imbalance in the dataset using SMOTE technique","metadata":{}},{"cell_type":"code","source":"#Oversampling the dataset to get better results \nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 33)\nX_train_new, y_train_new = sm.fit_resample(X_train, y_train.ravel())\npd.Series(y_train_new).value_counts().plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. #### We can observe that now the TARGET Variable is having equal classes. This will help our model not being baised towards one class","metadata":{}},{"cell_type":"markdown","source":"### Standardizing all the numerical features","metadata":{}},{"cell_type":"code","source":"#Standardising the values \nfrom sklearn.preprocessing import StandardScaler\nSS = StandardScaler()\n\nX_train_new = pd.DataFrame(SS.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\nX_test      = pd.DataFrame(SS.transform(X_test), columns=X_test.columns, index=X_test.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. MODEL BUILDING","metadata":{}},{"cell_type":"code","source":"#Function to create a table with pred values for LOGISTIC REGRESSION MODEL\ndef prediction(model_name,x_test,y_test,thre):\n    y_pred                        = model_name.predict(x_test)\n    y_pred_final                  = pd.DataFrame({'train_Prob':y_pred})\n    y_pred_final['real_op']       = y_test\n    y_pred_final['pred_op']       = y_pred_final['train_Prob'].apply(lambda x:1 if x>thre else 0)\n    return y_pred_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to Evaluate LOGISTIC REGRESSION MODEL based on various parameters\ndef validating_lr(y_real,y_pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    import seaborn as sns\n    confusion = confusion_matrix(y_pred,y_real)\n    sns.heatmap(confusion,annot=True,fmt='',cmap='Blues')\n    print('Accuracy Score',(accuracy_score(y_pred,y_real)*100))\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    TPR = round(((TP / float(TP+FP)*100)),2)\n    FPR = round(((FP/ float(TN+FP)*100)),2)\n    #print('True Positive rate                         :',round((TP / float(TP+FP)*100)),2)\n    #print('False postive rate(predicting 1 when its 0):',round((FP/ float(TN+FP)*100)),2)\n    print('True Positive rate                         :{}'.format(TPR))\n    print('False postive rate(predicting 1 when its 0):{}'.format(FPR))\n    print('\\n')\n    #print('Negative predictive value:',(TN / float(TN+ FN)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to Plot the ROC curve & find the optimal threshold value for LOGISTIC REGRESSION MODEL\ndef draw_roc( actual, probs ):\n    from sklearn.metrics import roc_curve,roc_auc_score\n    fpr, tpr, thresholds = roc_curve( actual, probs,drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    return None\n    fpr, tpr, thresholds = roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )\n    draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to check the VIF for a set of features\ndef vif_validation(X_train):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    # Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n    vif = pd.DataFrame()\n    vif['Features']  = X_train.columns\n    vif['VIF']       = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n    vif['VIF']       = round(vif['VIF'], 2)\n    vif              = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model building using statsmodel & checking the performance\n#MODEL NO 1\nimport statsmodels.api as sm\n\n#Features for model no 1\nfcol_list = ['Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit','Last Activity', 'Country', \n          'Specialization','How did you hear about X Education', 'What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity']\n\n#Adding constants \nX_train_new_sm = sm.add_constant(X_train_new[fcol_list])\nlr   = sm.GLM(y_train_new,X_train_new_sm, family = sm.families.Binomial())\nlr_1 = lr.fit() \nlr_1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EVALUATION FOR MODEL NO 1","metadata":{}},{"cell_type":"code","source":"#Evaluting MODEL NO 1 on TRAIN dataset:\npred_df = prediction(lr_1,X_train_new_sm,y_train_new,0.4)\n\n#Evaluating MODEL NO 1\nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluting MODEL NO 1 on Test dataset:\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\n#Predicting the values for MODEL NO 1\npred_df = prediction(lr_1,X_test_new_sm,y_test,0.4)\n\n#Checking the Evaluation parameters\nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BUILDING MODEL NO 2 BY ELIMINATING THE INSIGNIFICANT FEATURES","metadata":{}},{"cell_type":"code","source":"#Building MODEL NO 2 by eliminating insignificant Features\nimport statsmodels.api as sm\n\n#We will eliminate columns like 'Country','How did you hear about X Education'\n#as the pvalue is more than 0.05\n\nfcol_list = ['Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit','Last Activity', \n          'Specialization','What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity']\n\n#Adding constants & model building\nX_train_new_sm = sm.add_constant(X_train_new_sm[fcol_list])\nlr   = sm.GLM(y_train_new,X_train_new_sm, family = sm.families.Binomial())\nlr_2 = lr.fit() \nlr_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EVALUATION FOR MODEL NO 2","metadata":{}},{"cell_type":"code","source":"#Evaluting MODEL 2 on training dataset \npred_df = prediction(lr_2,X_train_new_sm,y_train_new,0.5)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the optimal threshold value of MODEL NO 2 using ROC curve\ndraw_roc(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluting MODEL NO 2 on Test dataset:\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\npred_df = prediction(lr_2,X_test_new_sm,y_test,0.4)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. We will chose TPR & FPR as our metrics to evaluate the model. We have to classify the Leads correctly & reduce mis-calssification to avoid unecessary resource allocation. ","metadata":{}},{"cell_type":"code","source":"#Evaluting MODEL NO 2 on Test dataset: using optimum threshold value of 0.2\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\npred_df = prediction(lr_2,X_test_new_sm,y_test,0.2)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We are getting 94% TPR and amongst them only 7% are being misclassified so overall its a good model lets further optimise the model","metadata":{}},{"cell_type":"markdown","source":"#### CHECKING MULTICOLINEARITY FOR FEATURES OF MODEL NO2","metadata":{}},{"cell_type":"code","source":"#Evaluating multicolinearity for features of MODEL NO 2\nvif_validation(X_train_new_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. As we had earlier seen in the EDA Process that the 2 columns '**LAST ACTIVITY**' & **'LAST NOTABLE ACTIVITY'** is almost same.\n#### 2. We did not eliminate any feature back then coz we were not sure which column to be eliminated.\n#### 3.Now, we have checked the VIF values and the VIF value for '**LAST ACTIVITY**' is high, so we will eliminate it & later check if the MULTICOLINEARITY is reduced.\n","metadata":{}},{"cell_type":"markdown","source":"#### BUILDING MODEL NO 3 BY ELIMINATING MULTICOLINEARITY IN FEATURES","metadata":{}},{"cell_type":"code","source":"#Building MODEL NO 3\nimport statsmodels.api as sm\n\n#Lets eliminate 'LAST ACTIVITY' feature and check if the multicolinearity reduces\nfcol_list = ['Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit', \n          'Specialization','What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity']\n\n#Adding constants & Model training\nX_train_new_sm = sm.add_constant(X_train_new_sm[fcol_list])\nlr   = sm.GLM(y_train_new,X_train_new_sm, family = sm.families.Binomial())\nlr_3 = lr.fit() \nlr_3.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking multicolinearity for features of MODEL NO 3\nvif_validation(X_train_new_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. We can observe that the MULTICOLINEARITY has been reduced below 2. We can say that our model is now the optimum model as MULTICOLINEARITY is eliminated & all features are significant.\n#### 2. The genral heuristic that we will take for checking MULTICOLINEARITY is 2 for this model building process.","metadata":{}},{"cell_type":"code","source":"#Evaluting MODEL NO 3 on TRAIN dataset\npred_df = prediction(lr_3,X_train_new_sm,y_train_new,0.4)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the optimal threshold value of MODEL NO 3 using ROC curve\ndraw_roc(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluting MODEL NO 3 on Test dataset: using optimum threshold value of 0.25\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\npred_df = prediction(lr_3,X_test_new_sm,y_test,0.25)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. This will be our Final Model to be deployed in the Production as we are getting TPR of 91%.\n2. In our Model FPR is only 9.51% that means only (127) values are being miscalssified.\n3. Although the Accuracy of the model is only 68.5%, we have choosen TPR & FPR to be our metrics for evaluation.\n4. Our Final Model is the best model as it do not have Multicolinearlity, non-significant features.\n5. With this model we wont spend unecessary resources on the misclassified leads.\n6. Leads which are correctly classified will help in saving resources & increase the Lead conversion rate.","metadata":{}}]}