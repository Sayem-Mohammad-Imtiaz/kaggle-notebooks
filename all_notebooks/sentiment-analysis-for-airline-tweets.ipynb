{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# print date and time for given type of representation\ndef date_time(x):\n    if x==1:\n        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==2:    \n        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n    if x==3:  \n        return 'Date now: %s' % datetime.datetime.now()\n    if x==4:  \n        return 'Date today: %s' % datetime.date.today() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tweet= pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\ntweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# discover missing data for each column\n(len(tweet)-tweet.count())/len(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove columns that have high ratio of missing values\n\ntweet.drop(['airline_sentiment_gold', 'negativereason_gold', 'tweet_coord'], axis=1, inplace=True ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.airline_sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sentiment_mood = tweet['airline_sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar([1,2,3],Sentiment_mood)\nplt.xticks([1,2,3], ['negative','neutral','positive'],rotation=45)\nplt.ylabel('Count')\nplt.xlabel('Mood')\nplt.title('Sentiment Moodel count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS shown from graph, there is a bad mood from airlines users,\nbut we need to see which airlines have highest bad mood"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet['airline'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_Airline_sentiment(Airline):\n    df=tweet[tweet['airline']==Airline]\n    count=df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count)\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Sentiment  Moods of '+Airline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(12, 12))\nplt.subplot(231)\nplot_Airline_sentiment('US Airways')\nplt.subplot(232)\nplot_Airline_sentiment('United')\nplt.subplot(233)\nplot_Airline_sentiment('American')\nplt.subplot(234)\nplot_Airline_sentiment('Southwest')\nplt.subplot(235)\nplot_Airline_sentiment('Delta')\nplt.subplot(236)\nplot_Airline_sentiment('Virgin America')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as shown in graphs all airlines have bad moods, but in first three data skewed towords negative and \nin last three data more normally distributed "},{"metadata":{"trusted":true},"cell_type":"code","source":"nr_Count=dict(tweet['negativereason'].value_counts(sort=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nr_Count():\n    df=tweet\n    count=dict(df['negativereason'].value_counts())\n    Unique_reason=list(tweet['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_reason():\n    df=nr_Count()\n    count=df['count']\n    Index = range(1,(len(df)+1))\n    plt.bar(Index,count)\n    plt.xticks(Index,df['Reasons'],rotation=90)\n    plt.ylabel('Count')\n    plt.xlabel('Reason')\n    plt.title('Count of Reasons ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_reason()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as shown, custmoer service have a bad service"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see what people say about flights using Wordcloud\nfrom wordcloud import WordCloud,STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=tweet[tweet['airline_sentiment']=='negative']\nwords = ' '.join(df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Cleaning_tweets(tweets):\n    non_letters_removed = re.sub(\"[^a-zA-Z]\", \" \",tweets) \n    words = non_letters_removed.lower().split()                             \n    stop_words = set(stopwords.words(\"english\"))                  \n    stop_words_removed = [w for w in words if not w in stop_words] \n    return ( \" \".join( stop_words_removed ))\n                                                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet['sentiment']=tweet['airline_sentiment'].apply(lambda x: 0 if x=='negative' else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet['cleaned_tweet']=tweet['text'].apply(lambda x: Cleaning_tweets(x))\ntweet['Tweet_length']=tweet['cleaned_tweet'].apply(lambda x: len(x.split(' ')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data to training and testing\ntrain,test = train_test_split(tweet,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train.cleaned_tweet.values\ntest_data = test.cleaned_tweet.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert data to vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"vector =CountVectorizer(analyzer = \"word\")\ntrain_features = vector.fit_transform(train_data)\ntest_features= vector.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##import machine learning classicla models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert data features to array\ntrain_to_array =train_features.toarray()\ntest_to_array = test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(train_to_array,train['sentiment'])\n        pred = fit.predict(test_to_array)\n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Index = [1,2,3,4,5,6]\nplt.bar(Index,Accuracy)\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS we can see RandomForestClassifieris  and AdaBoostClassifieris  do better than others"},{"metadata":{},"cell_type":"markdown","source":"###  2. Move To deep learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing import sequence\n\nimport time\nimport datetime\n\n# Packages for modeling\nfrom keras.layers import Embedding, LSTM, Bidirectional\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Flatten\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers\nimport collections\n\nMAX_LEN = 40\nNUM_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\nVAL_SIZE = 1000  # Size of the validation set\nEPOCHS = 20  # Number of epochs we usually start to train with\nBATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data to train and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_x = train.cleaned_tweet.values\ndata_y =train['sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train['cleaned_tweet'], train['sentiment'], test_size=0.1, random_state=37)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=NUM_WORDS,\n               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntokenizer.fit_on_texts(X_train)\n\nprint('Fitted tokenizer on {} documents'.format(tokenizer.document_count))\nprint('{} words in dictionary'.format(tokenizer.num_words))\nprint('Top 5 most common words are:', collections.Counter(tokenizer.word_counts).most_common(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_seq(seqs, nb_features = NUM_WORDS):\n    ohs = np.zeros((len(seqs), nb_features))\n    for i, s in enumerate(seqs):\n        ohs[i, s] = 1.\n    return ohs\n\nX_train_oh = one_hot_seq(X_train_seq)\nX_test_oh = one_hot_seq(X_test_seq)\n\nprint('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\nprint('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_oh.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\n\n#Convert the labels to One hot encoding vector for softmax for neural network\n\nnum_labels = len(np.unique(y_train))\nY_oh_train = np_utils.to_categorical(y_train, num_labels)\nY_oh_test = np_utils.to_categorical(y_test, num_labels)\nprint(Y_oh_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_2, X_valid, y_train_2, y_valid = train_test_split(X_train_oh, Y_oh_train, test_size=0.1, random_state=37)\n\n\nprint('Shape of validation set:',X_train_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train_2[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CALLbacks\n# Deep Learning Callbacs - Keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=1,\n    verbose=1)\n\n\ncallbacks = [reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def deep_model(model):\n    model.compile(optimizer='rmsprop',\n                  loss = 'categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    history = model.fit(X_train_2\n                       , y_train_2\n                       , epochs=EPOCHS\n                       , batch_size=BATCH_SIZE\n                       , callbacks=callbacks\n                       , validation_data=(X_valid, y_valid)\n                       , verbose=1)\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Dense_model():\n    model = models.Sequential()\n    model.add(layers.Dense(128, activation='relu', input_shape=(NUM_WORDS,)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(2, activation='softmax'))\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Dense_model()\nmodel_history = deep_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_metric(history, metric_name):\n    metric = history.history[metric_name]\n    val_metric = history.history['val_' + metric_name]\n\n    e = range(1, EPOCHS + 1)\n\n    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_metric(model_history, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build LSTM MODEL "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_performance(history=None, figure_directory=None, ylim_pad=[0, 0]):\n    xlabel = 'Epoch'\n    legends = ['Training', 'Validation']\n\n    plt.figure(figsize=(20, 5))\n\n    y1 = history.history['accuracy']\n    y2 = history.history['val_accuracy']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[0]\n    max_y = max(max(y1), max(y2))+ylim_pad[0]\n\n\n    plt.subplot(121)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Accuracy\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Accuracy', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n\n    y1 = history.history['loss']\n    y2 = history.history['val_loss']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[1]\n    max_y = max(max(y1), max(y2))+ylim_pad[1]\n\n\n    plt.subplot(122)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Loss', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n    if figure_directory:\n        plt.savefig(figure_directory+\"/history\")\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ltsm_model():\n    \n    model = models.Sequential()\n    \n    model.add(Embedding(NUM_WORDS, 100, input_length=MAX_LEN))\n    \n    model.add(LSTM(64, recurrent_dropout=0.2))    \n\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(256, activation='relu'))\n    \n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n\n    model.add(Dense(2, activation='softmax'))\n  \n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify sequence model to max 25 in length\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_train_deep = sequence.pad_sequences(X_train_seq, maxlen=MAX_LEN)\nX_test_deep = sequence.pad_sequences(X_test_seq, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data for lstm model\nX_train_2, X_valid, y_train_2, y_valid = train_test_split(X_train_deep, Y_oh_train, test_size=0.1, random_state=37)\n\n\nprint('Shape of validation set:',X_train_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ltsm_model()\nmodel_history = deep_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_performance(history=model_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"as shown in figure above there is overfitting so we need more work like preprossessing data and change some model parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as shown in figure above there is overfitting so we need more work like preprossessing data and change some model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}