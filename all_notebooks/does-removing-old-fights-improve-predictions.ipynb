{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ufcdata/data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Does using the whole dataset hurt future predictions?\n\n#### I have been playing around with this wonderful dataset for months now.  The insights that can be learned from it seem to be endless.  One thing I noticed though my analysis is that every winner prior to 01/04/2010 is recorded as \"Red\".  \n\n#### This does not mean that red when on a crazy win streak, or that the wrong winning fighter was recorded.  It does mean that the winning fighter was entered as the \"Red\" fighter and the losing fighter was recorded as the \"Blue\" fighter.  I want to do a quick analysis to see how this affects precictions of future fights"},{"metadata":{},"cell_type":"markdown","source":"### First let's see what I'm talking about..."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's change the 'date' from object to date\ndf['date'] = pd.to_datetime(df['date'])\n\nblue_wins = sum(df['Winner'] == 'Blue')\nred_wins = sum(df['Winner'] == 'Red')\n\ndf_recent = df.loc[df['date'] >'01/03/2010']\ndf_old = df.loc[df['date'] <'01/04/2010']\n\nblue_wins_recent = sum(df_recent['Winner'] == 'Blue')\nred_wins_recent = sum(df_recent['Winner'] == 'Red')\nblue_wins_old = sum(df_old['Winner'] == 'Blue')\nred_wins_old = sum(df_old['Winner'] == 'Red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using the whole dataset you can see that Red wins 68.5% of the time"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_labels = ('Blue', 'Red')\ny_pos = np.arange(len(x_labels))\nwins = ((blue_wins / (blue_wins + red_wins))*100, (red_wins / (blue_wins + red_wins))*100)\nplt.bar(y_pos, wins, align='center', edgecolor=['blue', 'red'], color='lightgrey')\nplt.xticks(y_pos, x_labels)\nplt.title(\"Winning Percentage (Whole Dataset)\")\nplt.ylabel(\"Percent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_labels = ('Blue', 'Red')\ny_pos = np.arange(len(x_labels))\nwins = (blue_wins_old, red_wins_old)\nplt.bar(y_pos, wins, align='center', edgecolor=['blue', 'red'], color='lightgrey')\nplt.xticks(y_pos, x_labels)\nplt.title(\"Total Wins (Prior to 1/4/2010)\")\nplt.ylabel(\"# of Wins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using the data from after 1/4/2010 you can see that Red only wins 58.3% of the time."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_labels = ('Blue', 'Red') \ny_pos = np.arange(len(x_labels)) \nwins = ((blue_wins_recent / (blue_wins_recent + red_wins_recent))*100, (red_wins_recent / (blue_wins_recent + red_wins_recent))*100) \nplt.bar(y_pos, wins, align='center', edgecolor=['blue', 'red'], color='lightgrey') \nplt.xticks(y_pos, x_labels)\nplt.title(\"Winning Percentage (After 1/3/2010)\")\nplt.ylabel(\"Percent\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I am of the opinion that the greatest purpose of models created from the dataset is to predict winners of future fights.  With this in mind I will do some analyses using the \"whole\" dataset and the \"recent\" dataset to see if there is a visible difference."},{"metadata":{},"cell_type":"markdown","source":"### Let's clean the data.  We are just going to remove any feature that has over 10% nulls.  The other features we will fill in."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing=(df.isnull().sum() / len(df)) * 100\nmissing = pd.DataFrame({'missing-ratio' :missing})\nmissing['feature'] = missing.index\nover_10 = missing[missing['missing-ratio'] > 10]\n\nremove_features = over_10['feature'].tolist()\n\n#Remove features with over 10% missing\ndf_filtered = df.drop(remove_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### That leaves us with 46 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_missing=(df_filtered.isnull().sum() / len(df_filtered)) * 100\nf_missing = pd.DataFrame({'missing-ratio' :f_missing})\nf_missing['feature'] = f_missing.index\nf_missing = f_missing[f_missing['missing-ratio'] > 0]\ndisplay(f_missing)\n#Here are the other features we need to deal with.  We are going to do a mix of filling in averages or\n#the most common value depending on what makes the most sense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered['Referee'] = df_filtered['Referee'].fillna('Unknown')\ndf_filtered['B_Stance'] = df_filtered['B_Stance'].fillna('Orthodox')\ndf_filtered['R_Stance'] = df_filtered['R_Stance'].fillna('Orthodox')\ndf_filtered['R_Height_cms'] = df_filtered['R_Height_cms'].fillna((df_filtered['R_Height_cms'].mean()))\ndf_filtered['B_Height_cms'] = df_filtered['B_Height_cms'].fillna((df_filtered['B_Height_cms'].mean()))\ndf_filtered['B_Weight_lbs'] = df_filtered['B_Weight_lbs'].fillna((df_filtered['B_Weight_lbs'].mean()))\ndf_filtered['R_Reach_cms'] = df_filtered['R_Reach_cms'].fillna((df_filtered['R_Reach_cms'].mean()))\ndf_filtered['R_Weight_lbs'] = df_filtered['R_Weight_lbs'].fillna((df_filtered['R_Weight_lbs'].mean()))\ndf_filtered['B_age'] = df_filtered['B_age'].fillna((df_filtered['B_age'].mean()))\ndf_filtered['R_age'] = df_filtered['R_age'].fillna((df_filtered['R_age'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To keep the number of features manageable after dummification we are going to remove B_fighter, R_fighter,\n#and Referee.  Draws also needs to be removed.\n\nto_drop = ['R_fighter', 'B_fighter', 'Referee']\ndf_filtered.drop(to_drop, axis=1, inplace=True)\ndf_filtered = df_filtered[df_filtered.Winner != 'Draw']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_filtered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set the label column\ndf_filtered[\"Winner\"] = df_filtered[\"Winner\"].astype('category')\ndf_filtered[\"label\"] = df_filtered[\"Winner\"].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total = df_filtered\ny_total = df_total[\"label\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now to remove some test data.  Since we are to predict future fights I will take approximately the most recent 300 fights and call that the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's make the test_set\ndf_test = df_total.loc[df_total['date'] >'11/11/2018']\ny_test = df_test['label']\n\n#Let's make the total train set\ntotal_train = df_total.loc[df_total['date'] <'11/12/2018']\ntotal_train_y = total_train['label']\n\n#Let's make the recent train set\nrecent_train = total_train.loc[total_train['date']>'01/03/2010']\nrecent_train_y = recent_train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets remove the date, winner, and label from the training sets....\nfinal_drop = ['date', 'Winner', 'label']\ndf_test.drop(final_drop, axis=1, inplace=True)\ntotal_train.drop(final_drop, axis=1, inplace=True)\nrecent_train.drop(final_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Takes a training set, training labels, test set, test labels, and a model.  Returns some\n#stats and visualization\n\ndef run_model(X_train, y_train, X_test, y_test, model):\n    #dummify and model\n    X_test = pd.get_dummies(X_test)\n    X_train = pd.get_dummies(X_train)\n    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n    X_test.fillna(value=0, inplace=True)\n    X_train.fillna(value=0, inplace=True)\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)    \n    \n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    class_names = ['Blue', 'Red']\n    \n    titles_options= [(f\"Total Confusion matrix\", None),\n                     (\"Normalized confusion matrix\", 'true')]\n    \n    title = f\"Confusion matrix\"\n    normalize=None\n    \n    \n    disp = plot_confusion_matrix(model, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize,\n                                 values_format='.5g'\n                                 )\n    disp.ax_.set_title(title)\n    plt.rcParams.update({'font.size': 16})\n    #print(title)\n    #print(disp.confusion_matrix)\n    plt.grid(False)\n    plt.show()    \n    cm = confusion_matrix(predictions, y_test)\n    tp = cm[0][0] \n    tn = cm[1][1]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    total = tp + tn + fp + fn\n    print(f\"tp for total: {tp}\")\n    print(f\"tn: {tn}\")\n    print(f\"fp: {fp}\")\n    print(f\"fn: {fn}\")\n    accuracy = (tp + tn) / total\n    precision = tp / (tp + fp)\n    #***I think that True Positive Rate may be the indicator of a good\n    #model....\n    true_positive = tp / (tp + fn)\n    print(f\"The precision is: {precision}\")\n    print(f\"The accuracy is {accuracy}\")\n    print(f\"The prevalence of blue is {(tp + fn) / total}\")\n    print(f\"The true_positive rate for total is {true_positive}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix and stats for total dataset using LogisticRegression()"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model(total_train, total_train_y, df_test, y_test, LogisticRegression())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix and stats using dataset only featuring fights after 1/3/2010 using LogisticRegression()"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model(recent_train, recent_train_y, df_test, y_test, LogisticRegression())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ignoring data from before 1/3/2010 we see better predictions across the board using Logistic Regression with no tinkering.  Let's see how a Random Forest behaves..."},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model(total_train, total_train_y, df_test, y_test, RandomForestClassifier(random_state=0, min_samples_leaf=2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model(recent_train, recent_train_y, df_test, y_test, RandomForestClassifier(random_state=0, min_samples_leaf=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With only minor tinkering to try to prevent extreme overfitting the 'whole' dataset performs closer to the 'recent' dataset when using a Random Forest.  Although the recent dataset does a much better job of predicting 'blue' winners.  Since the blue fighter is normally the underdog this fact alone could mean the recent dataset is more valuable in some applications (gambling)"},{"metadata":{},"cell_type":"markdown","source":"# What does this mean?\n\n#### It may be beneficial to ignore fights before 1/3/2010 because of the way the the winner was recorded.\n\n#### The data could be fixed either by analyzing the fights and correctly marking the blue and red fighter.  Another possibility would include randomizing the blue and red fighter for these early fights to match the ratio of the rest of the dataset (~58% red winners)"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}