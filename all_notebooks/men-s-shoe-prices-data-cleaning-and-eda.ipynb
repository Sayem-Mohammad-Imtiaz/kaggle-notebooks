{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Men's Shoe Prices \nThe goal of this kernel is to complete the **[task](https://www.kaggle.com/sureshmecad/mens-shoe-prices/tasks?taskId=4437)** associated with the dataset. The specific goals include:\n> - What is the average price of each distinct brand listed?\n> - Which brands have the highest prices?\n> - Which ones have the widest distribution of prices?\n> - Is there a typical price distribution (e.g., normal) across brands or within specific brands?\n> - Correlate specific product features with changes in price.\n\nOne thing to note before we begin... This dataset includes a lot of information on products that aren't actually shoes! Items such as watches and other accessories seem to have been mistakenly included during the original data gathering process (likely due to mislabeled categories). Fully correcting for this would require a decent amount of manual work / advanced regex and NLP and is outside the scope of this kernel. As a result, please interpret the final outputs with a grain of salt and as not fully reflective of reality beyond this dataset. That being said, the general code and methodologies used within should still be a useful learning experience!\n\nLet's dive in!","metadata":{}},{"cell_type":"markdown","source":"# Imports & Settings","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:37.896404Z","iopub.execute_input":"2021-07-20T20:09:37.896872Z","iopub.status.idle":"2021-07-20T20:09:38.34551Z","shell.execute_reply.started":"2021-07-20T20:09:37.896807Z","shell.execute_reply":"2021-07-20T20:09:38.344544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(context='talk')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:38.349605Z","iopub.execute_input":"2021-07-20T20:09:38.350036Z","iopub.status.idle":"2021-07-20T20:09:38.354341Z","shell.execute_reply.started":"2021-07-20T20:09:38.349999Z","shell.execute_reply":"2021-07-20T20:09:38.353551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning / Preprocessing\n\n## Summary Info\nTo start, let's get a general feel for the data we're working with.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/mens-shoe-prices/train.csv')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:38.35569Z","iopub.execute_input":"2021-07-20T20:09:38.356132Z","iopub.status.idle":"2021-07-20T20:09:39.277753Z","shell.execute_reply.started":"2021-07-20T20:09:38.356092Z","shell.execute_reply":"2021-07-20T20:09:39.276666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.279197Z","iopub.execute_input":"2021-07-20T20:09:39.279519Z","iopub.status.idle":"2021-07-20T20:09:39.329186Z","shell.execute_reply.started":"2021-07-20T20:09:39.279488Z","shell.execute_reply":"2021-07-20T20:09:39.32798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.330558Z","iopub.execute_input":"2021-07-20T20:09:39.330878Z","iopub.status.idle":"2021-07-20T20:09:39.361242Z","shell.execute_reply.started":"2021-07-20T20:09:39.33084Z","shell.execute_reply":"2021-07-20T20:09:39.360114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(df.isna().sum() / df.shape[0], 3)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.362563Z","iopub.execute_input":"2021-07-20T20:09:39.362866Z","iopub.status.idle":"2021-07-20T20:09:39.449275Z","shell.execute_reply.started":"2021-07-20T20:09:39.362836Z","shell.execute_reply":"2021-07-20T20:09:39.448447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropping Columns\nThe data is fairly messy as it currently stands. There are lots of missing values (some columns are even entirely missing) and there are various data types that need corrected. Given the limited number of questions being answered for this task, a good chunk of the available data is irrelevant. Right off the bat, let's eliminate any columns that are missing 80% or more of their values. This will help reduce the size of the data we're working with and make further cleaning processes a bit easier.","metadata":{}},{"cell_type":"code","source":"cols_to_drop = [col for col in df.columns if df[col].isna().sum() >= 0.8*df.shape[0]]\ncols_to_drop","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.450428Z","iopub.execute_input":"2021-07-20T20:09:39.450925Z","iopub.status.idle":"2021-07-20T20:09:39.547578Z","shell.execute_reply.started":"2021-07-20T20:09:39.450875Z","shell.execute_reply":"2021-07-20T20:09:39.546764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns=cols_to_drop, inplace=True)\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.549881Z","iopub.execute_input":"2021-07-20T20:09:39.550364Z","iopub.status.idle":"2021-07-20T20:09:39.567889Z","shell.execute_reply.started":"2021-07-20T20:09:39.550329Z","shell.execute_reply":"2021-07-20T20:09:39.566767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting to get somewhere, but there's still more columns to drop:","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['id', 'dateadded', 'dateupdated', 'descriptions', 'ean', \n                 'features', 'imageurls', 'keys', 'manufacturer',\n                 'manufacturernumber', 'merchants', 'prices_condition',\n                 'prices_dateadded', 'prices_dateseen', 'prices_issale',\n                 'prices_merchant', 'prices_offer', 'prices_shipping',\n                 'prices_sourceurls', 'sizes', 'skus', 'sourceurls', 'upc'],\n       inplace=True)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.570039Z","iopub.execute_input":"2021-07-20T20:09:39.570397Z","iopub.status.idle":"2021-07-20T20:09:39.604322Z","shell.execute_reply.started":"2021-07-20T20:09:39.57036Z","shell.execute_reply":"2021-07-20T20:09:39.602973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adjusting Data Types","metadata":{}},{"cell_type":"markdown","source":"Some finishing touches by adjusting the data types:","metadata":{}},{"cell_type":"code","source":"df.prices_amountmin = pd.to_numeric(df.prices_amountmin, errors='coerce', downcast='float')\ndf.prices_amountmax = pd.to_numeric(df.prices_amountmax, errors='coerce', downcast='float')\ndf.info()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-20T20:09:39.605745Z","iopub.execute_input":"2021-07-20T20:09:39.606086Z","iopub.status.idle":"2021-07-20T20:09:39.658012Z","shell.execute_reply.started":"2021-07-20T20:09:39.606053Z","shell.execute_reply":"2021-07-20T20:09:39.656922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.659522Z","iopub.execute_input":"2021-07-20T20:09:39.660309Z","iopub.status.idle":"2021-07-20T20:09:39.678158Z","shell.execute_reply.started":"2021-07-20T20:09:39.660254Z","shell.execute_reply":"2021-07-20T20:09:39.677018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better! \n\n## Cleaning Final Dataframe\n### Prices\nThe two columns for price (`prices_amountmin` and `prices_amountmax`) caught my eye since the first five rows in the dataframe have the same value for both. To investigate this further, let's check what percentage of the rows have the same value in both columns.","metadata":{}},{"cell_type":"code","source":"sum(df.prices_amountmin == df.prices_amountmax) / df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.679561Z","iopub.execute_input":"2021-07-20T20:09:39.680039Z","iopub.status.idle":"2021-07-20T20:09:39.696502Z","shell.execute_reply.started":"2021-07-20T20:09:39.679991Z","shell.execute_reply":"2021-07-20T20:09:39.695148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Over 96%! As a result, I'm okay with combining the two columns into a single column called `price` by taking the average.","metadata":{}},{"cell_type":"code","source":"df['price'] = np.mean([df.prices_amountmin, df.prices_amountmax], axis=0)\ndf_cleaned = df.drop(columns=['prices_amountmin', 'prices_amountmax'])\ndf_cleaned.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.697803Z","iopub.execute_input":"2021-07-20T20:09:39.69813Z","iopub.status.idle":"2021-07-20T20:09:39.728767Z","shell.execute_reply.started":"2021-07-20T20:09:39.6981Z","shell.execute_reply":"2021-07-20T20:09:39.727662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's check the value counts for the `prices_currency` column.","metadata":{}},{"cell_type":"code","source":"df_cleaned.prices_currency.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.730266Z","iopub.execute_input":"2021-07-20T20:09:39.730577Z","iopub.status.idle":"2021-07-20T20:09:39.745633Z","shell.execute_reply.started":"2021-07-20T20:09:39.730546Z","shell.execute_reply":"2021-07-20T20:09:39.744338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that some data was incorrectly placed in the `prices_currency` column in the original dataset. The overwhelming majority is priced in USD though. We could either work on converting the other currencies to USD for accurate comparisons, or simply drop those rows. For the sake of simplicity in this kernel, let's drop the rows.","metadata":{}},{"cell_type":"code","source":"df_cleaned = df_cleaned[df_cleaned.prices_currency == 'USD']\ndf_cleaned.prices_currency.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.746993Z","iopub.execute_input":"2021-07-20T20:09:39.747578Z","iopub.status.idle":"2021-07-20T20:09:39.768965Z","shell.execute_reply.started":"2021-07-20T20:09:39.747535Z","shell.execute_reply":"2021-07-20T20:09:39.767898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"df_cleaned.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.770354Z","iopub.execute_input":"2021-07-20T20:09:39.77101Z","iopub.status.idle":"2021-07-20T20:09:39.791564Z","shell.execute_reply.started":"2021-07-20T20:09:39.770945Z","shell.execute_reply":"2021-07-20T20:09:39.789827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The missing values in the `color` column are fine for now but the 17 missing values for the `brand` column need handled.","metadata":{}},{"cell_type":"code","source":"df_cleaned.dropna(axis=0, subset=['brand'], inplace=True)\ndf_cleaned.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.793449Z","iopub.execute_input":"2021-07-20T20:09:39.793991Z","iopub.status.idle":"2021-07-20T20:09:39.823824Z","shell.execute_reply.started":"2021-07-20T20:09:39.793875Z","shell.execute_reply":"2021-07-20T20:09:39.822984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standardizing Brands\nFinally, let's change all of the brand names to lowercase. This is to help prevent any variations in capitalization from being classified as different brands. While a more serious method of cleaning the brand names would need to be employed in any sort of production code, this will suffice for now.","metadata":{}},{"cell_type":"code","source":"df_cleaned.brand = df_cleaned.brand.apply(lambda x: x.lower(), convert_dtype=False)\ndf_cleaned.brand.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.825013Z","iopub.execute_input":"2021-07-20T20:09:39.825498Z","iopub.status.idle":"2021-07-20T20:09:39.84099Z","shell.execute_reply.started":"2021-07-20T20:09:39.825455Z","shell.execute_reply":"2021-07-20T20:09:39.840097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Answering the Questions\n## What is the average price of each distinct brand listed?\nTo begin answering this questions, let's first take a look at how many unique brands are in the data.","metadata":{}},{"cell_type":"code","source":"df_cleaned.brand.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.842058Z","iopub.execute_input":"2021-07-20T20:09:39.84251Z","iopub.status.idle":"2021-07-20T20:09:39.862452Z","shell.execute_reply.started":"2021-07-20T20:09:39.842476Z","shell.execute_reply":"2021-07-20T20:09:39.86109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! That's way too many to effectively visualize. Let's limit the brands we look at to only those with more than 50 rows of data. This will help eliminate random, unheard of brands with only a couple entries as well as entries which aren't actually","metadata":{}},{"cell_type":"code","source":"brands_above_50 = df_cleaned.brand.value_counts()[df_cleaned.brand.value_counts() > 50]\nbrands_above_50","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.864193Z","iopub.execute_input":"2021-07-20T20:09:39.864996Z","iopub.status.idle":"2021-07-20T20:09:39.891682Z","shell.execute_reply.started":"2021-07-20T20:09:39.864927Z","shell.execute_reply":"2021-07-20T20:09:39.890259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_brands = df_cleaned[df_cleaned.brand.isin(brands_above_50.index)].groupby('brand').mean()\ndf_brands.sort_values('price', ascending=False, inplace=True)\ndf_brands","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.893363Z","iopub.execute_input":"2021-07-20T20:09:39.893719Z","iopub.status.idle":"2021-07-20T20:09:39.921298Z","shell.execute_reply.started":"2021-07-20T20:09:39.893686Z","shell.execute_reply":"2021-07-20T20:09:39.920006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 16))\nsns.barplot(data=df_brands, x='price', y=df_brands.index, palette='crest')\nplt.title('Average Price by Brand')\nplt.xlabel('Price')\nplt.ylabel('Brand')\nplt.annotate(text='Note: only brands with more than 50 observations are included.',\n             xy=(60, 50),\n             fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:39.923096Z","iopub.execute_input":"2021-07-20T20:09:39.923538Z","iopub.status.idle":"2021-07-20T20:09:41.175664Z","shell.execute_reply.started":"2021-07-20T20:09:39.923492Z","shell.execute_reply":"2021-07-20T20:09:41.174499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Which brands have the highest prices?","metadata":{}},{"cell_type":"code","source":"df_max_prices = df_cleaned[df_cleaned.brand.isin(brands_above_50.index)].sort_values('price', ascending=False)\ndf_max_prices = df_max_prices.drop_duplicates('brand')\ndf_max_prices[['brand', 'name', 'price']]","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:41.177191Z","iopub.execute_input":"2021-07-20T20:09:41.177826Z","iopub.status.idle":"2021-07-20T20:09:41.213608Z","shell.execute_reply.started":"2021-07-20T20:09:41.177779Z","shell.execute_reply":"2021-07-20T20:09:41.212461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 16))\nsns.barplot(data=df_max_prices, x='price', y='brand', palette='crest')\nplt.title('Max Price by Brand')\nplt.xlabel('Price')\nplt.ylabel('Brand');\nplt.annotate(text='Note: only brands with more than 50 observations are included.',\n             xy=(300, 50),\n             fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:41.215221Z","iopub.execute_input":"2021-07-20T20:09:41.215602Z","iopub.status.idle":"2021-07-20T20:09:42.563147Z","shell.execute_reply.started":"2021-07-20T20:09:41.215565Z","shell.execute_reply":"2021-07-20T20:09:42.561814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Which ones have the widest distribution of prices?","metadata":{}},{"cell_type":"code","source":"df_medians = df_cleaned[df_cleaned.brand.isin(brands_above_50.index)]\ndf_medians = df_medians.groupby('brand').median()\ndf_medians = df_medians.sort_values('price', ascending=False)\ndf_medians.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:42.564827Z","iopub.execute_input":"2021-07-20T20:09:42.565299Z","iopub.status.idle":"2021-07-20T20:09:42.586694Z","shell.execute_reply.started":"2021-07-20T20:09:42.565251Z","shell.execute_reply":"2021-07-20T20:09:42.58591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The answer to this question is up for debate depending on the interpretation of the question. \"Widest distribution\" could be defined in a number of ways including:\n- Standard deviation (nominal or as a percentage)\n- The interquratile range (IQR)\n- Difference between maximum and minimum values\n\nMost of this information can be captured by utilizing boxplots. The boxplots could be ordered in a number of ways, but using the median tends to yield a fairly orderly result.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 16))\nsns.boxplot(data=df_cleaned[df_cleaned.brand.isin(brands_above_50.index)],\n            x='price',\n            y='brand',\n            order=df_medians.index,\n            palette='crest',\n            orient='h')\nplt.title('Boxplots for Price by Brand')\nplt.xlabel('Price')\nplt.ylabel('Brand')\nplt.annotate(text='Note: only brands with more than 50 observations are included.',\n             xy=(300, 50),\n             fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:42.590424Z","iopub.execute_input":"2021-07-20T20:09:42.590753Z","iopub.status.idle":"2021-07-20T20:09:44.398112Z","shell.execute_reply.started":"2021-07-20T20:09:42.59072Z","shell.execute_reply":"2021-07-20T20:09:44.396933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Is there a typical price distribution (e.g., normal) across brands or within specific brands?","metadata":{}},{"cell_type":"code","source":"df_all_dist = df_cleaned[df_cleaned.brand.isin(brands_above_50.index)]\nfig = plt.figure(figsize=(8, 6))\nsns.kdeplot(df_all_dist['price'], clip=(0, None))\nplt.title('Distribution of Prices')\nplt.xlabel('Price');\nplt.annotate(text='Note: only brands with more than 50 observations are included.',\n             xy=(300, 0.0065),\n             fontsize=12);","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:44.400146Z","iopub.execute_input":"2021-07-20T20:09:44.400741Z","iopub.status.idle":"2021-07-20T20:09:44.756012Z","shell.execute_reply.started":"2021-07-20T20:09:44.400674Z","shell.execute_reply":"2021-07-20T20:09:44.754902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of all prices for brands with more than 50 observations is heavily skewed to the right. Let's takes a look at what happens when we apply a log transformation:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nsns.kdeplot(df_all_dist['price'], clip=(0, None), log_scale=True)\nplt.title('Distribution of Prices')\nplt.xlabel('Log Price');","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:44.757593Z","iopub.execute_input":"2021-07-20T20:09:44.757922Z","iopub.status.idle":"2021-07-20T20:09:46.031678Z","shell.execute_reply.started":"2021-07-20T20:09:44.757889Z","shell.execute_reply":"2021-07-20T20:09:46.030534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it appears that the prices are log-normally distributed. We can check the distribution of an individual brand with more than 50 observations by creating a simple function.","metadata":{}},{"cell_type":"code","source":"def plot_brand_distribution(brand, log=False):\n    '''\n    Description: Takes a brand name and boolean value for log and plots a KDE plot for the distribution\n    of that specific brand's prices (log prices if log == True).\n    \n    Inputs:\n    - brand : str\n         The name of the brand to visualize. Must have more than 50 observations in the original data set.\n         Case insensitive as it will automatically be lowercased.\n    \n    - log : boolean, default = False\n         Determines whether a log transformation should be applied to the prices.\n    '''\n    fig = plt.figure(figsize=(8, 6))\n    sns.kdeplot(df_all_dist[df_all_dist.brand == brand.lower()]['price'], clip=(0, None), log_scale=log)\n    plt.title(f'Distribution of {brand.title()}\\'s Prices')\n    plt.xlabel('Log Price' if log else 'Price')\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:46.03313Z","iopub.execute_input":"2021-07-20T20:09:46.033439Z","iopub.status.idle":"2021-07-20T20:09:46.041117Z","shell.execute_reply.started":"2021-07-20T20:09:46.033408Z","shell.execute_reply":"2021-07-20T20:09:46.039819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_brand_distribution('nike', False)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:46.042669Z","iopub.execute_input":"2021-07-20T20:09:46.042994Z","iopub.status.idle":"2021-07-20T20:09:46.277356Z","shell.execute_reply.started":"2021-07-20T20:09:46.042937Z","shell.execute_reply":"2021-07-20T20:09:46.27611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_brand_distribution('Adidas', True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:09:46.278733Z","iopub.execute_input":"2021-07-20T20:09:46.27905Z","iopub.status.idle":"2021-07-20T20:09:47.001776Z","shell.execute_reply.started":"2021-07-20T20:09:46.279022Z","shell.execute_reply":"2021-07-20T20:09:47.000276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThat's it for this kernel! Correlating prices with specific product attributes may be added in a future update, but would require significantly more data cleaning and preprocessing.\n\n**If you liked this notebook or have any feedback, please let me know in the comments. I'm always looking to improve!**","metadata":{}}]}