{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Import the libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"card_data = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Attribute Information**\n------------------------\n* **CLIENTNUM**                : Client number. Unique identifier for the customer holding the account\n* **Attrition_Flag**           : Internal event (customer activity) variable - if the account is closed then 1 else 0\n* **Customer_Age**             : Customer's Age in Years\n* **Gender**                   : M=Male, F=Female\n* **Dependent_count**          : Number of dependents\n* **Education_Leel**           : Educational Qualification of the account holder (example: high school, college graduate, etc.)\n* **Marital_status**           : Married, Single, Divorced, Unknown\n* **Income _Category**         : Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, >\n* **Card_Category**            : Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n* **Months_On_Book**           : Period of relationship with bank\n* **Total_Relationship_Count** : Total no. of products held by the customer\n* **Months_Inactive_12_mon**   : No. of months inactive in the last 12 months\n* **Contacts_Count_12_mon**    : No. of Contacts in the last 12 months\n* **Credit_Limit**             : Credit Limit on the Credit Card                                                                                                           \n* **Total_Revolving_Bal**      : Total Revolving Balance on the Credit Card\n* **Avg_Open_To_Buy**          : Open to Buy Credit Line (Average of last 12 months)\n* **Total_Amt_Chng_Q4_Q1**     : Change in Transaction Amount (Q4 over Q1)\n* **Total_Trans_Amt**          : Total Transaction Amount (Last 12 months)\n* **Total_Trans_Ct**           : Total Transaction Count (Last 12 months)\n* **Total_Ct_Chng_Q4_Q1**      : Change in Transaction Count (Q4 over Q1)\n* **Avg_Utilization_Ratio**    : Average Card Utilization Ratio\n* **Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1**  \n* **Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2**"},{"metadata":{},"cell_type":"markdown","source":"# Business Understanding"},{"metadata":{},"cell_type":"markdown","source":"From this data set we can provide couple of sloutions:\n    1. We can predict the total amount that will be spent by any customer for an year.\n        Using this model company can make sure they get profitted from new customers who are coming in and also decide weather to issue credit card to the new customers based on the amount that they are going to spend by looking at their profile data.\n    2. Who are the customers that are going to stop using credit cards.\n         Using this model/result company can make offer to employess to retain them.\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"In this step prepare the data for the analysis. Data might have null values or  be of different data types or might contain outliers. So lets see if data need to be treated"},{"metadata":{},"cell_type":"markdown","source":"**Check for the Null Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for percentage of Null Values\ncard_data.isnull().sum()/card_data.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results we can see that there are no null values so lets proceed check for data types and outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Dependent_count'] = pd.to_numeric(card_data['Dependent_count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the featuers look to be in correct data types. Lets procced for now and change if need in further steps."},{"metadata":{},"cell_type":"markdown","source":"**Outlier Treatment**\n*     This is needed because sometimes the extreme outliers may affect the final Model. So we must identify the extreme outliers and then treat them for the better Analysis and better performance of the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\ncard_data.boxplot(column=['Customer_Age','Dependent_count','Months_on_book','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon','Credit_Limit',\n                          'Total_Revolving_Bal','Avg_Open_To_Buy','Total_Amt_Chng_Q4_Q1','Total_Trans_Amt','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1'])\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no Extreme outliers, so we can proceed with EDA."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"Not much weightage is given to EDA at this point."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Customer_Age'].plot(kind='hist')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that age group of ppl between 35 to 55 use credit cards more"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.groupby('Marital_Status').agg({'Total_Trans_Amt':'sum'}).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Single and Married ppl contribute to most of the total credit cards transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.groupby('Education_Level').agg({'Total_Trans_Amt':'sum'}).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Education isnt really playing a role in credit card amount being spent by customers"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=card_data['Total_Trans_Ct'],y=card_data['Total_Trans_Amt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Model\nAs a credit card company, we are interested in knowing how much a card lender can spend which will in turn benifit the company. with the predefined parameters if we know that if a person is not sepeding too much and if he inactive all the time, then company will not benifit much from that customer. \n  So lets build a ML model to predict how much a credit card holder will spennd in an year so that company can decide weather to give the card or provide him some exiting offers or reject the issue of credit card. "},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression Model.\nThis is to address the first solution that we discussed above. This model is used to predict that amount spent by any given customer in 12 months."},{"metadata":{},"cell_type":"markdown","source":"Linear Regresseion requires data preparation and data scaling. So lets have a relook at the data and scale the data and convert all the categorical variables to numberic ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.head()\n#card_data.Income_Category.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets Drop the Columns which are not/least important to our model like CLIENTNUM and last two columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n                'CLIENTNUM'],axis=1,inplace=True)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Attrition_Flag**"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.Attrition_Flag.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is about the existed customer or attrited customer, since we are interested in know how much a customer is going to spend using credit card be it existing or attrited customer. So we can exclude this feature from our analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop(columns=['Attrition_Flag'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gender**\n*     Lets Convert Gender to Numberic Variable. Since this is nominal data, we can use one hot encoding type of techniques. I will be using simple logic to convert this.\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Gender'] = card_data['Gender'].apply(lambda x: 1 if x=='M' else 0)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We cannot build regression models with categorical variables in a dataset. So lets convert all the categorical variables to numeric. Since I am using dummy encoding to convert categorical variables, we need to drop one column otherwise it will lead to multi collinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to convert categorical variables\ndef convert_cat_variables(col,prefix,df):\n    #Get dummies for a column uisng pandas\n    dummies = pd.get_dummies(df[col],prefix=prefix)\n    #Lets append this to Original Dataset\n    df = df.join(dummies)\n    #Now Lets drop the original Sex column\n    df.drop(col,axis=1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data = convert_cat_variables('Education_Level','Education_Level',card_data)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop('Education_Level_Unknown',axis=1,inplace=True)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data = convert_cat_variables('Marital_Status','Marital_Status',card_data)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop('Marital_Status_Unknown',axis=1,inplace=True)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Card Cateogry and Income Category** are Ordinal data, hence different method is used to conecert these two features"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.Card_Category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transformCategory(x):\n    #print(x)\n    if x=='Blue': return 0\n    elif x=='Gold': return 2 \n    elif x=='Silver': return 1 \n    elif x=='Platinum': return 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Card_Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Card_Category'] = card_data['Card_Category'].apply(transformCategory)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.Income_Category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.Income_Category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transformIncomeCat(x):\n    #print(x)\n    if x=='Less than $40K': return 0\n    elif x=='$40K - $60K': return 1 \n    elif x=='$60K - $80K': return 2 \n    elif x=='$80K - $120K': return 3\n    elif x=='$120K +': return 4\n    elif x=='Unknown': return 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Income_Category'] = card_data['Income_Category'].apply(transformIncomeCat)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now that data is cleaned, we need to check if there are any colinearity between features and eliminate them. Colinearity will affect in model building and we will not get the right co efficient values if not removed from the model. So lets eliminate multicolinearity using Variance Inflation Factor method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate VIF to eliminate Multicolinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef cal_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return vif.sort_values('VIF',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = card_data.drop('Total_Trans_Amt',axis=1)\ncal_vif(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets eliminate all the columns with VIF > 10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop(columns=['Avg_Open_To_Buy','Total_Revolving_Bal','Credit_Limit','Customer_Age','Months_on_book','Total_Amt_Chng_Q4_Q1','Total_Ct_Chng_Q4_Q1'],axis=1,inplace=True)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = card_data.drop('Total_Trans_Amt',axis=1)\ncal_vif(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n#function to perform Linear Regression\ndef doLinearRegression(data,predictColName):\n    result = dict()\n    #split the data \n    x_train,x_test,y_train,y_test = train_test_split(data.drop(predictColName,axis=1),data[predictColName],test_size=0.3,random_state=112)\n    x_train = sm.add_constant(x_train)\n    x_test = sm.add_constant(x_test)\n    model = sm.OLS(y_train,x_train)\n    model_result = model.fit()\n    result['model_result'] = model_result\n    result['x_train'] = x_train\n    result['x_test'] = x_test\n    result['y_train'] = y_train\n    result['y_test'] = y_test\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1 = doLinearRegression(card_data,'Total_Trans_Amt')\nresult1['model_result'].summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now Lets look at the Probability of all variables by defining Null Hypothesis(H0) and alternate hypothesisH(a).**\n**H(0)=Feature depends on total transaction amount**\n**H(a)=Feature doesnt depends on total transaction amount**\n**Now By keeping threshold of 5%, lets eliminate all the features which have P(t)>5%**"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop('Contacts_Count_12_mon',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1 = doLinearRegression(card_data,'Total_Trans_Amt')\nresult1['model_result'].summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop('Months_Inactive_12_mon',axis=1,inplace=True)\nresult1 = doLinearRegression(card_data,'Total_Trans_Amt')\nresult1['model_result'].summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop(columns=['Education_Level_College','Education_Level_Doctorate','Education_Level_Graduate','Education_Level_High School',\n                       'Education_Level_Post-Graduate','Education_Level_Uneducated'],axis=1,inplace=True)\nresult1 = doLinearRegression(card_data,'Total_Trans_Amt')\nresult1['model_result'].summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop(columns=['Marital_Status_Divorced','Marital_Status_Married'],axis=1,inplace=True)\nresult1 = doLinearRegression(card_data,'Total_Trans_Amt')\nresult1['model_result'].summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notice that R2 and Adjusted R2 havent changed even after eliminating all the unaffected features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = result1['model_result'].params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef[1:].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above graph we can see how each feature is dependent on total transaction and by how much magnitiude and also politively or negatively co related**"},{"metadata":{},"cell_type":"markdown","source":"# Accuracy of the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error\n#function to print metric values on predicted results\ndef predict_print_results(result,x_test,y_test,strTypeOfData):\n    predicted_y = result.predict(x_test)\n    print('--------------------------')\n    print(' Measures on '+strTypeOfData+' Data    ')\n    print('--------------------------')\n    print('R2  is '+str(r2_score(y_test,predicted_y)))\n    print('MSE is '+str(mean_squared_error(y_test,predicted_y)))\n    print('MAE is '+str(mean_absolute_error(y_test,predicted_y)))\n    return predicted_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_print_results(result1['model_result'],result1['x_train'],result1['y_train'],'Train') \ny_predicted1 = predict_print_results(result1['model_result'],result1['x_test'],result1['y_test'],'Test') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notice that model is 70%**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to plot a graph of predicted and actual y values\ndef plot_scatter_predicted_actuaal(y_test,y_predicted):\n    # Plotting Scatter graph to show the prediction  \n    plt.scatter(y_test, y_predicted, c = 'green') \n    plt.xlabel(\"Price: in $1000's\") \n    plt.ylabel(\"Predicted value\") \n    plt.title(\"True value vs predicted value : Linear Regression\") \n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter_predicted_actuaal(result1['y_test'],y_predicted1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier\nThis is to address the second part of our business problem. To check if company is going to loose the customer."},{"metadata":{},"cell_type":"markdown","source":"reload the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"drop unwanted columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n                'CLIENTNUM'],axis=1,inplace=True)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert all categorical variables into numeric. In this case I have used Label Encoding as my conversion algorithm. Since this model is not going to get affected by the magnitude of the feature data. No scaling is done**"},{"metadata":{},"cell_type":"markdown","source":"Prediction variable. Attrition flag."},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Attrition_Flag'] = card_data['Attrition_Flag'].apply(lambda x: 1 if x=='Existing Customer' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"card_data['Gender'] = card_data['Gender'].apply(lambda x: 1 if x=='M' else 0)\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Label Encoder.. I am converting all the categorical variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n\n# Encode labels in column 'Education_Level'. \ncard_data['Education_Level']= label_encoder.fit_transform(card_data['Education_Level']) \n\n# Encode labels in column 'Education_Level'. \ncard_data['Marital_Status']= label_encoder.fit_transform(card_data['Marital_Status']) \n \n# Encode labels in column 'Education_Level'. \ncard_data['Income_Category']= label_encoder.fit_transform(card_data['Income_Category']) \n \n# Encode labels in column 'Education_Level'. \ncard_data['Card_Category']= label_encoder.fit_transform(card_data['Card_Category']) \n\ncard_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = card_data.drop('Attrition_Flag',axis=1)\ny = card_data['Attrition_Flag']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the data for the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Random Forest Classifer model and fit the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuray of Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix\n\ndef find_model_accuracy(threshold,y_test,predictions):\n    \n    #Build predicted class using predictions with threshold value\n    predicted_classes = np.where(predictions>threshold, 1, 0)\n    acc_score = accuracy_score(y_test, predicted_classes)\n    print('***********************************************')\n    print('   Accuracy Score of Model is '+str(acc_score))\n    print('***********************************************')\n    \n    #Build the consuion matrix on test and predicted results\n    confusion_mat = confusion_matrix(y_test,predicted_classes)\n    #Build data frame of consuion matrix\n    confusion_df  = pd.DataFrame(confusion_mat,index=['Actual Neg', 'Actual Pos'],columns=['Predicted Neg','Predicted Pos'])\n    print('             Model Results                     ')\n    print('             *************                     ')\n    #Calculate True Positive and False Positive Accuracy\n    TN = confusion_mat[0][0]\n    TP = confusion_mat[1][1]\n    FN = confusion_mat[1][0]\n    FP = confusion_mat[0][1]\n    total = TN+TP+FN+FP\n    acc = (TN + TP)/total\n    missClassification = (FN+FP)/total\n    nullErrorRate = (TN+FP)/total\n    print('Accuray Of the Model '+ str(acc))\n    print('Misclassification Rate '+str(missClassification))\n    print('Null Error Rate '+ str(nullErrorRate))\n    print('***********************************************')\n    print('             Confusion Matrix                  ')\n    print('             ----------------                  ')\n    #Plot a head map using sns\n    sns.heatmap(data=confusion_df,cmap='coolwarm',annot=True)\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_model_accuracy(0.75,y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note: This is my first Kaggle work and I am still persuing Data Science. Please feel free to provide feedback for my improvements. Lets learn together :) "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}