{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.146259Z","iopub.execute_input":"2021-07-20T14:37:17.146675Z","iopub.status.idle":"2021-07-20T14:37:17.159711Z","shell.execute_reply.started":"2021-07-20T14:37:17.14664Z","shell.execute_reply":"2021-07-20T14:37:17.158394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries\n\n**Import the usual libraries for pandas and plotting. You can import sklearn later on.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.171076Z","iopub.execute_input":"2021-07-20T14:37:17.171679Z","iopub.status.idle":"2021-07-20T14:37:17.186666Z","shell.execute_reply.started":"2021-07-20T14:37:17.171642Z","shell.execute_reply":"2021-07-20T14:37:17.185095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the Data\n\n** Use pandas to read loan_data.csv as a dataframe called loans.**","metadata":{}},{"cell_type":"code","source":"loans = pd.read_csv('/kaggle/input/loan-data/loan_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.208946Z","iopub.execute_input":"2021-07-20T14:37:17.209468Z","iopub.status.idle":"2021-07-20T14:37:17.235089Z","shell.execute_reply.started":"2021-07-20T14:37:17.209417Z","shell.execute_reply":"2021-07-20T14:37:17.234223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** Check out the info(), head(), and describe() methods on loans.**","metadata":{}},{"cell_type":"code","source":"loans.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.251781Z","iopub.execute_input":"2021-07-20T14:37:17.25244Z","iopub.status.idle":"2021-07-20T14:37:17.270931Z","shell.execute_reply.started":"2021-07-20T14:37:17.25239Z","shell.execute_reply":"2021-07-20T14:37:17.269411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loans.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.310007Z","iopub.execute_input":"2021-07-20T14:37:17.310639Z","iopub.status.idle":"2021-07-20T14:37:17.38103Z","shell.execute_reply.started":"2021-07-20T14:37:17.310588Z","shell.execute_reply":"2021-07-20T14:37:17.380081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loans.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.383539Z","iopub.execute_input":"2021-07-20T14:37:17.384069Z","iopub.status.idle":"2021-07-20T14:37:17.406873Z","shell.execute_reply.started":"2021-07-20T14:37:17.384022Z","shell.execute_reply":"2021-07-20T14:37:17.4058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recheck for nulls","metadata":{}},{"cell_type":"code","source":"print(loans.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.408392Z","iopub.execute_input":"2021-07-20T14:37:17.408904Z","iopub.status.idle":"2021-07-20T14:37:17.433162Z","shell.execute_reply.started":"2021-07-20T14:37:17.408856Z","shell.execute_reply":"2021-07-20T14:37:17.430934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count values**","metadata":{}},{"cell_type":"code","source":"print(loans['credit.policy'].value_counts())\nprint(loans['purpose'].value_counts())\nprint(loans['not.fully.paid'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.434963Z","iopub.execute_input":"2021-07-20T14:37:17.435529Z","iopub.status.idle":"2021-07-20T14:37:17.459977Z","shell.execute_reply.started":"2021-07-20T14:37:17.435482Z","shell.execute_reply":"2021-07-20T14:37:17.458817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nIn that part, we peform the data visualization!\n\n**Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nloans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='Credit.Policy=1')\nloans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='Credit.Policy=0')\nplt.legend()\nplt.xlabel('FICO')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.461965Z","iopub.execute_input":"2021-07-20T14:37:17.462315Z","iopub.status.idle":"2021-07-20T14:37:17.863613Z","shell.execute_reply.started":"2021-07-20T14:37:17.462281Z","shell.execute_reply":"2021-07-20T14:37:17.862446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a similar figure, except this time select by the not.fully.paid column.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nloans[loans['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='not.fully.paid=1')\nloans[loans['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red',\n                                              bins=30,label='not.fully.paid=0')\nplt.legend()\nplt.xlabel('FICO')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:17.865465Z","iopub.execute_input":"2021-07-20T14:37:17.86592Z","iopub.status.idle":"2021-07-20T14:37:18.247961Z","shell.execute_reply.started":"2021-07-20T14:37:17.865874Z","shell.execute_reply":"2021-07-20T14:37:18.246785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(11,7))\nsns.countplot(x='purpose',hue='not.fully.paid',data=loans,palette='Set1')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:18.250671Z","iopub.execute_input":"2021-07-20T14:37:18.251388Z","iopub.status.idle":"2021-07-20T14:37:18.530455Z","shell.execute_reply.started":"2021-07-20T14:37:18.251335Z","shell.execute_reply":"2021-07-20T14:37:18.529549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's see the trend between FICO score and interest rate. Recreate the following jointplot.**","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:18.532285Z","iopub.execute_input":"2021-07-20T14:37:18.532963Z","iopub.status.idle":"2021-07-20T14:37:19.230158Z","shell.execute_reply.started":"2021-07-20T14:37:18.53291Z","shell.execute_reply":"2021-07-20T14:37:19.22878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create the following lmplots to see if the trend differed between not.fully.paid and credit.policy. Check the documentation for lmplot() if you can't figure out how to separate it into columns.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(11,7))\nsns.lmplot(y='int.rate',x='fico',data=loans,hue='credit.policy',\n           col='not.fully.paid',palette='Set1')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:19.231651Z","iopub.execute_input":"2021-07-20T14:37:19.232076Z","iopub.status.idle":"2021-07-20T14:37:21.040443Z","shell.execute_reply.started":"2021-07-20T14:37:19.232037Z","shell.execute_reply":"2021-07-20T14:37:21.038352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plots for categorical variables**","metadata":{}},{"cell_type":"code","source":"categorical_columns = ['credit.policy', 'purpose', 'fico', 'inq.last.6mths', 'delinq.2yrs','pub.rec']\n\nprint(categorical_columns)\nnumerical_columns = ['int.rate', 'installment', 'log.annual.inc', 'dti', 'days.with.cr.line', 'revol.util']\nprint(numerical_columns)\n\nfig,axes = plt.subplots(3,2,figsize=(35,35))\nfor idx,cat_col in enumerate(categorical_columns):\n    row,col = idx//2,idx%2\n    sns.countplot(x=cat_col,data=loans,hue='not.fully.paid',ax=axes[row,col])\n\n\nplt.subplots_adjust(hspace=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:21.042568Z","iopub.execute_input":"2021-07-20T14:37:21.042948Z","iopub.status.idle":"2021-07-20T14:37:23.986785Z","shell.execute_reply.started":"2021-07-20T14:37:21.042915Z","shell.execute_reply":"2021-07-20T14:37:23.985654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe Plots convey the following things for our dataset. The non-full paid loans are the small portion compared to the other one in the classification for every plot.\n\nNow, let's oberve the Numerical Columns:\n","metadata":{}},{"cell_type":"markdown","source":"**Plots for numerical variables**","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(1,6,figsize=(40,5))\nfor idx,cat_col in enumerate(numerical_columns):\n    sns.boxplot(y=cat_col,data= loans,x='not.fully.paid',ax=axes[idx])\n\nprint(loans[numerical_columns].describe())\nplt.subplots_adjust(hspace=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:23.9888Z","iopub.execute_input":"2021-07-20T14:37:23.989518Z","iopub.status.idle":"2021-07-20T14:37:24.903488Z","shell.execute_reply.started":"2021-07-20T14:37:23.989462Z","shell.execute_reply":"2021-07-20T14:37:24.902423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nFor Numercical Columns, there is significant relation to non paid Loan and days.with.cr.line which is the number of days the borrower has had a credit line as well with revol.util which is the borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available). Also, we can observe similar bevahiors either the features or features-labels.\n","metadata":{}},{"cell_type":"markdown","source":"***Correlation Pearson and Spearman***","metadata":{}},{"cell_type":"code","source":"features_num = ['int.rate', 'installment', 'log.annual.inc',\n                'dti', 'fico', 'days.with.cr.line',\n                'revol.bal', 'revol.util',\n                'inq.last.6mths', 'delinq.2yrs']\ncorr_pearson = loans[features_num].corr(method='pearson')\ncorr_spearman = loans[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (9,7))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nfig = plt.figure(figsize = (9,7))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:24.906769Z","iopub.execute_input":"2021-07-20T14:37:24.907791Z","iopub.status.idle":"2021-07-20T14:37:26.521218Z","shell.execute_reply.started":"2021-07-20T14:37:24.907703Z","shell.execute_reply":"2021-07-20T14:37:26.519917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Features\n\nNotice that the **purpose** column as categorical\n\nThat means we need to transform them using dummy variables so sklearn will be able to understand them. Let's do this in one clean step using pd.get_dummies.\n\nLet's show you a way of dealing with these columns that can be expanded to multiple categorical features if necessary.\n\n**Create a list of 1 element containing the string 'purpose'. Call this list cat_feats.**","metadata":{}},{"cell_type":"code","source":"cat_feats = ['purpose']","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:26.523047Z","iopub.execute_input":"2021-07-20T14:37:26.523393Z","iopub.status.idle":"2021-07-20T14:37:26.527966Z","shell.execute_reply.started":"2021-07-20T14:37:26.52336Z","shell.execute_reply":"2021-07-20T14:37:26.526854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Histograms**","metadata":{}},{"cell_type":"code","source":"loans.iloc[:,:].hist(figsize=(15,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:26.529445Z","iopub.execute_input":"2021-07-20T14:37:26.529803Z","iopub.status.idle":"2021-07-20T14:37:28.494403Z","shell.execute_reply.started":"2021-07-20T14:37:26.52977Z","shell.execute_reply":"2021-07-20T14:37:28.492716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now use pd.get_dummies(loans,columns=cat_feats,drop_first=True) to create a fixed larger dataframe that has new feature columns with dummy variables. Set this dataframe as final_data.**","metadata":{}},{"cell_type":"code","source":"final_data = pd.get_dummies(loans,columns=cat_feats,drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.496516Z","iopub.execute_input":"2021-07-20T14:37:28.49693Z","iopub.status.idle":"2021-07-20T14:37:28.511042Z","shell.execute_reply.started":"2021-07-20T14:37:28.496891Z","shell.execute_reply":"2021-07-20T14:37:28.509928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.512392Z","iopub.execute_input":"2021-07-20T14:37:28.512733Z","iopub.status.idle":"2021-07-20T14:37:28.539814Z","shell.execute_reply.started":"2021-07-20T14:37:28.512698Z","shell.execute_reply":"2021-07-20T14:37:28.53829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split\n\nNow its time to split our data into a training set and a testing set!\n\n** Use sklearn to split your data into a training set and a testing set as we've done in the past.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score, roc_auc_score, roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.541728Z","iopub.execute_input":"2021-07-20T14:37:28.542334Z","iopub.status.idle":"2021-07-20T14:37:28.548174Z","shell.execute_reply.started":"2021-07-20T14:37:28.542291Z","shell.execute_reply":"2021-07-20T14:37:28.546518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = final_data.drop('not.fully.paid',axis=1)\ny = final_data['not.fully.paid']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.550027Z","iopub.execute_input":"2021-07-20T14:37:28.550458Z","iopub.status.idle":"2021-07-20T14:37:28.573422Z","shell.execute_reply.started":"2021-07-20T14:37:28.550421Z","shell.execute_reply":"2021-07-20T14:37:28.571997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training + Test Decision Tree, Random Forest, GradientBoostingClassifier & Logistic Regression Models\n\nLet's start by training our models!\n\n**Import DecisionTreeClassifier, RandomForestClassifier, LogisticRegression, GradientBoostingClassifier**","metadata":{}},{"cell_type":"markdown","source":"# **Model 1: Decision Tree Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.575168Z","iopub.execute_input":"2021-07-20T14:37:28.575538Z","iopub.status.idle":"2021-07-20T14:37:28.585902Z","shell.execute_reply.started":"2021-07-20T14:37:28.575504Z","shell.execute_reply":"2021-07-20T14:37:28.584381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_clf = DecisionTreeClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.58755Z","iopub.execute_input":"2021-07-20T14:37:28.587888Z","iopub.status.idle":"2021-07-20T14:37:28.60469Z","shell.execute_reply.started":"2021-07-20T14:37:28.587857Z","shell.execute_reply":"2021-07-20T14:37:28.602919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_clf.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.60623Z","iopub.execute_input":"2021-07-20T14:37:28.606586Z","iopub.status.idle":"2021-07-20T14:37:28.711707Z","shell.execute_reply.started":"2021-07-20T14:37:28.60655Z","shell.execute_reply":"2021-07-20T14:37:28.710537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for overfitting","metadata":{}},{"cell_type":"code","source":"y_pred = tree_clf.predict(X_train)\nprint(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\nprint(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:28.71341Z","iopub.execute_input":"2021-07-20T14:37:28.713872Z","iopub.status.idle":"2021-07-20T14:37:29.497683Z","shell.execute_reply.started":"2021-07-20T14:37:28.713835Z","shell.execute_reply":"2021-07-20T14:37:29.496388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overfitting Problem\n\nWe can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps.","metadata":{}},{"cell_type":"markdown","source":"# **Tuning 'Max_Depth' and 'Min_Samples_leaf' of tree**","metadata":{}},{"cell_type":"code","source":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\ntree_depths = []\n\nfor depth in range(1,20):\n    tree_clf = DecisionTreeClassifier(max_depth=depth)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    tree_depths.append(depth)\n    \n\nTuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:29.499014Z","iopub.execute_input":"2021-07-20T14:37:29.499407Z","iopub.status.idle":"2021-07-20T14:37:40.497011Z","shell.execute_reply.started":"2021-07-20T14:37:29.499372Z","shell.execute_reply":"2021-07-20T14:37:40.495812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above graph, we can conclude that keeping 'Max_Depth' = 6 will yield optimum Test accuracy and F1 score. The Optimum Test Accuracy is roughly 0.8 and Optimum F1 Score for validation is around 0.5","metadata":{}},{"cell_type":"markdown","source":"# **Visulazing Decision Tree with Max Depth = 6**","metadata":{}},{"cell_type":"markdown","source":"Visulazing Decision Tree with Max Depth = 6","metadata":{}},{"cell_type":"code","source":"import graphviz \nfrom sklearn import tree\n\ntree_clf = tree.DecisionTreeClassifier(max_depth = 6)\ntree_clf.fit(X_train,y_train)\ndot_data = tree.export_graphviz(tree_clf,feature_names = X.columns.tolist())\ngraph = graphviz.Source(dot_data)\ngraph","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:40.498755Z","iopub.execute_input":"2021-07-20T14:37:40.499224Z","iopub.status.idle":"2021-07-20T14:37:40.642579Z","shell.execute_reply.started":"2021-07-20T14:37:40.49915Z","shell.execute_reply":"2021-07-20T14:37:40.64084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above tree, we could see that some of the leafs have less than 5 samples hence our classifier might overfit. We can sweep hyper-parameter 'min_samples_leaf' to further improve test accuracy by keeping max_depth to 6","metadata":{}},{"cell_type":"code","source":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\nmin_samples_leaf = []\nimport numpy as np\nfor samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf \n    tree_clf = DecisionTreeClassifier(max_depth=6,min_samples_leaf = samples_leaf)\n    tree_clf.fit(X_train,y_train)\n    y_training_pred = tree_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    min_samples_leaf.append(samples_leaf)\n    \n\nTuning_min_samples_leaf = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Min_Samples_leaf\": min_samples_leaf }\nTuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)\n\nplot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Min_Samples_leaf\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:40.644603Z","iopub.execute_input":"2021-07-20T14:37:40.644999Z","iopub.status.idle":"2021-07-20T14:37:52.307932Z","shell.execute_reply.started":"2021-07-20T14:37:40.644962Z","shell.execute_reply":"2021-07-20T14:37:52.306823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above plot, we will choose Min_Samples_leaf to 32 to improve test accuracy and F1-score.","metadata":{}},{"cell_type":"markdown","source":"## Predictions and Evaluation of Decision Tree\n**Create predictions from the test set, a classification report and a confusion matrix.**","metadata":{}},{"cell_type":"markdown","source":"Let's predict off the y_test values and evaluate our model.\n\nPredict the class of not.fully.paid for the X_test data.","metadata":{}},{"cell_type":"markdown","source":"**Feature importance**","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:52.30939Z","iopub.execute_input":"2021-07-20T14:37:52.309703Z","iopub.status.idle":"2021-07-20T14:37:52.314346Z","shell.execute_reply.started":"2021-07-20T14:37:52.309671Z","shell.execute_reply":"2021-07-20T14:37:52.313311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_clf = DecisionTreeClassifier(max_depth=6,min_samples_leaf = 32)\ntree_clf.fit(X_train,y_train)\n# get importance\nimportance = tree_clf.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nind = [x for x in range(len(importance))]\npyplot.bar(ind, importance)\npyplot.title('Decision Tree')\npyplot.xticks(ind, ('1', '2', '3', '4', '5','6','7','8','9','10','11','12','13','14','15','16','17','18'))\npyplot.xlabel('Features')\npyplot.ylabel('Importance')\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:52.319463Z","iopub.execute_input":"2021-07-20T14:37:52.319852Z","iopub.status.idle":"2021-07-20T14:37:52.6055Z","shell.execute_reply.started":"2021-07-20T14:37:52.319818Z","shell.execute_reply":"2021-07-20T14:37:52.604141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"# predict\ny_pred = tree_clf.predict(X_test)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:52.608292Z","iopub.execute_input":"2021-07-20T14:37:52.608664Z","iopub.status.idle":"2021-07-20T14:37:52.677158Z","shell.execute_reply.started":"2021-07-20T14:37:52.608631Z","shell.execute_reply":"2021-07-20T14:37:52.676283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:52.678276Z","iopub.execute_input":"2021-07-20T14:37:52.678593Z","iopub.status.idle":"2021-07-20T14:37:52.684978Z","shell.execute_reply.started":"2021-07-20T14:37:52.67856Z","shell.execute_reply":"2021-07-20T14:37:52.683511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc =[]\nf1 = []\nacc.append(accuracy_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:52.686879Z","iopub.execute_input":"2021-07-20T14:37:52.687361Z","iopub.status.idle":"2021-07-20T14:37:52.70581Z","shell.execute_reply.started":"2021-07-20T14:37:52.687312Z","shell.execute_reply":"2021-07-20T14:37:52.704732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross-validation**","metadata":{}},{"cell_type":"code","source":"print(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:52.707435Z","iopub.execute_input":"2021-07-20T14:37:52.707878Z","iopub.status.idle":"2021-07-20T14:37:53.070751Z","shell.execute_reply.started":"2021-07-20T14:37:52.707833Z","shell.execute_reply":"2021-07-20T14:37:53.068921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reports Precision, Recall, F1-score**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:53.072811Z","iopub.execute_input":"2021-07-20T14:37:53.073271Z","iopub.status.idle":"2021-07-20T14:37:53.094004Z","shell.execute_reply.started":"2021-07-20T14:37:53.073221Z","shell.execute_reply":"2021-07-20T14:37:53.092516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROC CURVE**","metadata":{}},{"cell_type":"code","source":"print(roc_auc_score(y_test, y_pred))\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr, label = \"Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Tree ROC Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:53.095673Z","iopub.execute_input":"2021-07-20T14:37:53.096226Z","iopub.status.idle":"2021-07-20T14:37:53.266659Z","shell.execute_reply.started":"2021-07-20T14:37:53.096004Z","shell.execute_reply":"2021-07-20T14:37:53.265524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mis-classifications**\n\nIt can be seen that majority of the misclassifications are happening on non-full paid loan applicants being classified as full paid loan.\n\nLet's look into Random Forest Classifier (and later for the other 2 methods)if it can reduce mis-classifications\n","metadata":{}},{"cell_type":"markdown","source":"# **Model 2: Random Forest Classifier**","metadata":{}},{"cell_type":"markdown","source":"Create the RandomForestClassifier() called rf_clf and fit it to the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train,y_train)\ny_pred = rf_clf.predict(X_train)\nprint(\"Train F1 Score \", f1_score(y_train,y_pred))\nprint(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n\nprint(\"Validation Mean F1 Score: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:37:53.267943Z","iopub.execute_input":"2021-07-20T14:37:53.268298Z","iopub.status.idle":"2021-07-20T14:38:08.005931Z","shell.execute_reply.started":"2021-07-20T14:37:53.268268Z","shell.execute_reply":"2021-07-20T14:38:08.004464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tuning 'Max_Depth' of RF**","metadata":{}},{"cell_type":"markdown","source":"**Tuning Max_Depth**","metadata":{}},{"cell_type":"code","source":"training_accuracy = []\nval_accuracy = []\ntraining_f1 = []\nval_f1 = []\ntree_depths = []\n\nfor depth in range(1,20):\n    rf_clf = RandomForestClassifier(max_depth=depth)\n    rf_clf.fit(X_train,y_train)\n    y_training_pred = rf_clf.predict(X_train)\n\n    training_acc = accuracy_score(y_train,y_training_pred)\n    train_f1 = f1_score(y_train,y_training_pred)\n    val_mean_f1 = cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n    val_mean_accuracy = cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n    \n    training_accuracy.append(training_acc)\n    val_accuracy.append(val_mean_accuracy)\n    training_f1.append(train_f1)\n    val_f1.append(val_mean_f1)\n    tree_depths.append(depth)\n    \n\nTuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\nTuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n\nplot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:38:08.007985Z","iopub.execute_input":"2021-07-20T14:38:08.008386Z","iopub.status.idle":"2021-07-20T14:41:18.067521Z","shell.execute_reply.started":"2021-07-20T14:38:08.00835Z","shell.execute_reply":"2021-07-20T14:41:18.066315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same philosophy with desicion tree ('Max_Depth' = 10).","metadata":{}},{"cell_type":"markdown","source":"## Random Forest: Predictions and Evaluation\n**Create predictions from the test set, a classification report and a confusion matrix.**","metadata":{}},{"cell_type":"markdown","source":"Let's predict off the y_test values and evaluate our model.\n\nPredict the class of not.fully.paid for the X_test data.","metadata":{}},{"cell_type":"markdown","source":"**Feature importance**","metadata":{}},{"cell_type":"code","source":"rf_clf = RandomForestClassifier(max_depth=10)\nrf_clf.fit(X_train,y_train)\n# get importance\nimportance = rf_clf.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nind = [x for x in range(len(importance))]\npyplot.bar(ind, importance)\npyplot.title('Random Forest')\npyplot.xticks(ind, ('1', '2', '3', '4', '5','6','7','8','9','10','11','12','13','14','15','16','17','18'))\npyplot.xlabel('Features')\npyplot.ylabel('Importance')\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:18.069113Z","iopub.execute_input":"2021-07-20T14:41:18.069493Z","iopub.status.idle":"2021-07-20T14:41:19.441618Z","shell.execute_reply.started":"2021-07-20T14:41:18.069457Z","shell.execute_reply":"2021-07-20T14:41:19.440035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"y_pred = rf_clf.predict(X_test)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:19.443569Z","iopub.execute_input":"2021-07-20T14:41:19.443976Z","iopub.status.idle":"2021-07-20T14:41:19.555948Z","shell.execute_reply.started":"2021-07-20T14:41:19.443941Z","shell.execute_reply":"2021-07-20T14:41:19.554565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc.append(accuracy_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:19.55711Z","iopub.execute_input":"2021-07-20T14:41:19.557444Z","iopub.status.idle":"2021-07-20T14:41:19.567347Z","shell.execute_reply.started":"2021-07-20T14:41:19.557413Z","shell.execute_reply":"2021-07-20T14:41:19.566119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross-Validation**","metadata":{}},{"cell_type":"code","source":"print(\"Validation Mean F1 Score: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:19.569095Z","iopub.execute_input":"2021-07-20T14:41:19.569452Z","iopub.status.idle":"2021-07-20T14:41:29.032031Z","shell.execute_reply.started":"2021-07-20T14:41:19.569418Z","shell.execute_reply":"2021-07-20T14:41:29.030688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reports Precision, Recall, F1-score**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:29.033695Z","iopub.execute_input":"2021-07-20T14:41:29.034107Z","iopub.status.idle":"2021-07-20T14:41:29.054538Z","shell.execute_reply.started":"2021-07-20T14:41:29.03407Z","shell.execute_reply":"2021-07-20T14:41:29.05285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROC CURVE**","metadata":{}},{"cell_type":"code","source":"print(roc_auc_score(y_test, y_pred))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr, label = \"Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"RF ROC Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:29.056492Z","iopub.execute_input":"2021-07-20T14:41:29.056839Z","iopub.status.idle":"2021-07-20T14:41:29.22778Z","shell.execute_reply.started":"2021-07-20T14:41:29.05681Z","shell.execute_reply":"2021-07-20T14:41:29.226231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mis-classifications**\n\nIt can be seen that majority of the misclassifications are happening on non-full paid loan applicants being classified as full paid loan in our prediction.\n\nLet's look the Logistic regression, if it can reduce mis-classifications. \n","metadata":{}},{"cell_type":"markdown","source":"\n# **Model 3: Logistic Regression**","metadata":{}},{"cell_type":"markdown","source":"Create the LogisticRegression() called logreg_clf and fit it to the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:29.229674Z","iopub.execute_input":"2021-07-20T14:41:29.230143Z","iopub.status.idle":"2021-07-20T14:41:29.235651Z","shell.execute_reply.started":"2021-07-20T14:41:29.230096Z","shell.execute_reply":"2021-07-20T14:41:29.23435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tuning based on threshold**","metadata":{}},{"cell_type":"markdown","source":"In the next part,we tune the logistic regression with chaning the threshold","metadata":{}},{"cell_type":"code","source":"train_accuracies = []\ntrain_f1_scores = []\ntest_accuracies = []\ntest_f1_scores = []\nthresholds = []\n\n#for thresh in np.linspace(0.1,0.9,8): ## Sweeping from threshold of 0.1 to 0.9\nfor thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n    logreg_clf = LogisticRegression(solver='liblinear')\n    logreg_clf.fit(X_train,y_train)\n    \n    y_pred_train_thresh = logreg_clf.predict_proba(X_train)[:,1]\n    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n\n    train_acc = accuracy_score(y_train,y_pred_train)\n    train_f1 = f1_score(y_train,y_pred_train)\n    \n    y_pred_test_thresh = logreg_clf.predict_proba(X_test)[:,1]\n    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n    \n    test_acc = accuracy_score(y_test,y_pred_test)\n    test_f1 = f1_score(y_test,y_pred_test)\n    \n    train_accuracies.append(train_acc)\n    train_f1_scores.append(train_f1)\n    test_accuracies.append(test_acc)\n    test_f1_scores.append(test_f1)\n    thresholds.append(thresh)\n    \n    \nThreshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\nThreshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n\nplot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\nfig,ax = plt.subplots(figsize=(15,5))\nsns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:29.237251Z","iopub.execute_input":"2021-07-20T14:41:29.237644Z","iopub.status.idle":"2021-07-20T14:41:30.608484Z","shell.execute_reply.started":"2021-07-20T14:41:29.237608Z","shell.execute_reply":"2021-07-20T14:41:30.607085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above Test/Train curves, we can keep threshold to 0.2.","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression: Predictions and Evaluation\n**Create predictions from the test set, a classification report and a confusion matrix.**","metadata":{}},{"cell_type":"markdown","source":"Let's predict off the y_test values and evaluate our model.\n\nPredict the class of not.fully.paid for the X_test data.","metadata":{}},{"cell_type":"markdown","source":"**Feature importance**","metadata":{}},{"cell_type":"code","source":"# define the model\nlogreg_clf = LogisticRegression()\n# fit the model\nlogreg_clf.fit(X_train, y_train)\n# get importance\nimportance = logreg_clf.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nind = [x for x in range(len(importance))]\npyplot.bar(ind, importance)\npyplot.title('Logistic Regression')\npyplot.xticks(ind, ('1', '2', '3', '4', '5','6','7','8','9','10','11','12','13','14','15','16','17','18'))\npyplot.xlabel('Features')\npyplot.ylabel('Importance')\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:30.609938Z","iopub.execute_input":"2021-07-20T14:41:30.610299Z","iopub.status.idle":"2021-07-20T14:41:31.132304Z","shell.execute_reply.started":"2021-07-20T14:41:30.610262Z","shell.execute_reply":"2021-07-20T14:41:31.130723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"thresh = 0.2 ### Threshold chosen from above Curves\ny_pred_test_thresh = logreg_clf.predict_proba(X_test)[:,1]\ny_pred = (y_pred_test_thresh > thresh).astype(int) \nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:31.133983Z","iopub.execute_input":"2021-07-20T14:41:31.134453Z","iopub.status.idle":"2021-07-20T14:41:31.238245Z","shell.execute_reply.started":"2021-07-20T14:41:31.134406Z","shell.execute_reply":"2021-07-20T14:41:31.235737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc.append(accuracy_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:31.240288Z","iopub.execute_input":"2021-07-20T14:41:31.241254Z","iopub.status.idle":"2021-07-20T14:41:31.253672Z","shell.execute_reply.started":"2021-07-20T14:41:31.241175Z","shell.execute_reply":"2021-07-20T14:41:31.252308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross-Validation**","metadata":{}},{"cell_type":"code","source":"print(\"Validation Mean F1 Score: \",cross_val_score(logreg_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(logreg_clf,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:31.256391Z","iopub.execute_input":"2021-07-20T14:41:31.257612Z","iopub.status.idle":"2021-07-20T14:41:32.90389Z","shell.execute_reply.started":"2021-07-20T14:41:31.257539Z","shell.execute_reply":"2021-07-20T14:41:32.902591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reports Precision, Recall, F1-score**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:32.906033Z","iopub.execute_input":"2021-07-20T14:41:32.907024Z","iopub.status.idle":"2021-07-20T14:41:32.934353Z","shell.execute_reply.started":"2021-07-20T14:41:32.90696Z","shell.execute_reply":"2021-07-20T14:41:32.933058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROC CURVE**","metadata":{}},{"cell_type":"code","source":"print(roc_auc_score(y_test, y_pred))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr, label = \"Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"LogReg ROC Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:32.936469Z","iopub.execute_input":"2021-07-20T14:41:32.937561Z","iopub.status.idle":"2021-07-20T14:41:33.134488Z","shell.execute_reply.started":"2021-07-20T14:41:32.937492Z","shell.execute_reply":"2021-07-20T14:41:33.131997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mis-classifications**\n\nThis method increase the prediciton for non-full paid Loan but decrease the prediction for the full paid Loans. \nLet's look the last model Gradient Boosting Classifier, if it can reduce the mis-classifications. ","metadata":{}},{"cell_type":"markdown","source":"# **Model 4: Gradient Boosting Classifier**","metadata":{}},{"cell_type":"markdown","source":"Create the GradientBoostingClassifier() called logreg_clf and fit it to the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nimport numpy\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom matplotlib import pyplot\nimport matplotlib\nmatplotlib.use('Agg') ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:33.136054Z","iopub.execute_input":"2021-07-20T14:41:33.136475Z","iopub.status.idle":"2021-07-20T14:41:33.142578Z","shell.execute_reply.started":"2021-07-20T14:41:33.13644Z","shell.execute_reply":"2021-07-20T14:41:33.141509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor = GradientBoostingClassifier()\nregressor.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:33.144487Z","iopub.execute_input":"2021-07-20T14:41:33.145447Z","iopub.status.idle":"2021-07-20T14:41:34.868681Z","shell.execute_reply.started":"2021-07-20T14:41:33.145375Z","shell.execute_reply":"2021-07-20T14:41:34.867632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Tuning 'n_estimators' and 'learning_rate' in Gradient Boosting**","metadata":{}},{"cell_type":"markdown","source":"Here, we are tuning our Gradient boosting model which involves creating and adding trees to the model sequentially.\n\nNew trees are created to correct the residual errors in the predictions from the existing sequence of trees.\n\nThe effect is that the model can quickly fit, then overfit the training dataset.\n\nA technique to slow down the learning in the gradient boosting model is to apply a weighting factor for the corrections by new trees when added to the model.\n\nThis weighting is called the shrinkage factor or the learning rate, depending on the literature or the tool.\n\nThe setting values less than 1.0 for learning rate that has the effect of making less corrections for each tree added to the model and also cause the trade-off bias with the n_estimators parameter. This in turn results in more trees that must be added to the model.\n\nFor this reason, our tunning have small values in the range of 0.1 to 0.3 (as well as values less than 0.1).","metadata":{}},{"cell_type":"code","source":"# grid search\nn_estimators = [300, 400, 500]\nlearning_rate = [0.001, 0.01, 0.1]\nparam_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(regressor, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n# plot results\nscores = numpy.array(means).reshape(len(learning_rate), len(n_estimators))\nfor i, value in enumerate(learning_rate):\n    pyplot.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\npyplot.legend()\npyplot.xlabel('n_estimators')\npyplot.ylabel('Log Loss')\npyplot.savefig('n_estimators_vs_learning_rate.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:41:34.870162Z","iopub.execute_input":"2021-07-20T14:41:34.870648Z","iopub.status.idle":"2021-07-20T14:45:06.896877Z","shell.execute_reply.started":"2021-07-20T14:41:34.870553Z","shell.execute_reply":"2021-07-20T14:45:06.895097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our results shows that our optimal learning rate is 0.01 and n_estimators=500","metadata":{}},{"cell_type":"code","source":"regressor = GradientBoostingClassifier(n_estimators=500,learning_rate=0.01)\nregressor.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:45:06.898893Z","iopub.execute_input":"2021-07-20T14:45:06.8993Z","iopub.status.idle":"2021-07-20T14:45:15.372278Z","shell.execute_reply.started":"2021-07-20T14:45:06.899249Z","shell.execute_reply":"2021-07-20T14:45:15.370903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting Classifier: Predictions and Evaluation\n**Create predictions from the test set, a classification report and a confusion matrix.**","metadata":{}},{"cell_type":"markdown","source":"Let's predict off the y_test values and evaluate our model.\n\nPredict the class of not.fully.paid for the X_test data.","metadata":{}},{"cell_type":"markdown","source":"**Feature importance**","metadata":{}},{"cell_type":"code","source":"# get importance\nimportance = regressor.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nind = [x for x in range(len(importance))]\npyplot.bar(ind, importance)\npyplot.title('Gradient Boosting')\npyplot.xticks(ind, ('1', '2', '3', '4', '5','6','7','8','9','10','11','12','13','14','15','16','17','18'))\npyplot.xlabel('Features')\npyplot.ylabel('Importance')\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:45:15.373976Z","iopub.execute_input":"2021-07-20T14:45:15.374439Z","iopub.status.idle":"2021-07-20T14:45:15.641704Z","shell.execute_reply.started":"2021-07-20T14:45:15.37439Z","shell.execute_reply":"2021-07-20T14:45:15.640383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"y_pred = regressor.predict(X_test)\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\nprint(\"Test F1 Score: \",f1_score(y_test,y_pred))\nprint(\"Confusion Matrix on Test Data\")\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:45:15.643671Z","iopub.execute_input":"2021-07-20T14:45:15.6442Z","iopub.status.idle":"2021-07-20T14:45:15.731532Z","shell.execute_reply.started":"2021-07-20T14:45:15.644132Z","shell.execute_reply":"2021-07-20T14:45:15.730511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc.append(accuracy_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:45:15.733159Z","iopub.execute_input":"2021-07-20T14:45:15.734018Z","iopub.status.idle":"2021-07-20T14:45:15.745669Z","shell.execute_reply.started":"2021-07-20T14:45:15.733965Z","shell.execute_reply":"2021-07-20T14:45:15.744253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross-Validation**","metadata":{}},{"cell_type":"code","source":"print(\"Validation Mean F1 Score: \",cross_val_score(regressor,X_train,y_train,cv=5,scoring='f1_macro').mean())\nprint(\"Validation Mean Accuracy: \",cross_val_score(regressor,X_train,y_train,cv=5,scoring='accuracy').mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:45:15.746908Z","iopub.execute_input":"2021-07-20T14:45:15.747254Z","iopub.status.idle":"2021-07-20T14:46:24.760091Z","shell.execute_reply.started":"2021-07-20T14:45:15.74721Z","shell.execute_reply":"2021-07-20T14:46:24.758171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reports Precision, Recall, F1-score**","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:46:24.76184Z","iopub.execute_input":"2021-07-20T14:46:24.762153Z","iopub.status.idle":"2021-07-20T14:46:24.778515Z","shell.execute_reply.started":"2021-07-20T14:46:24.762121Z","shell.execute_reply":"2021-07-20T14:46:24.777386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROC CURVE**","metadata":{}},{"cell_type":"code","source":"print(roc_auc_score(y_test, y_pred))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr, tpr, label = \"Gradient Boosting Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"GB ROC Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:46:24.780122Z","iopub.execute_input":"2021-07-20T14:46:24.780603Z","iopub.status.idle":"2021-07-20T14:46:24.95162Z","shell.execute_reply.started":"2021-07-20T14:46:24.780547Z","shell.execute_reply":"2021-07-20T14:46:24.950509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"Tunning the best model is challenging due to hyperparameter complex and chaotic numbers you can pick within the model. In our analysis, the results is currently relying on a couple of parameters and specific space numbers for tunning the models. However, the techniques are also implemented in predicting which customers will fully pay their loan. As more sophisticated ML, we applied different models, and we try to tune every model with a couple of parameters (max 2 parameters). Our models are poor performance, but if we compare them based on our metrics, we will have the ranks as follow:\n\n**1st Rank Accuracy          Model**\n*    85%      Random Forest Classifier\n\n**1st Rank F1-score          Model**\n*    30%     Logistic Regression\n\nBased on our results, we can conclude that Logistic regression performs better in classification. That model has lower accuracy than the other 3 models, but it has better results for predicting the non fully paid loans and the most important class.\n\n\n","metadata":{}},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['TREE', 'RF', 'Logistic Reg', 'GB']\nax.bar(langs,acc)\nplt.title('ACCURACY')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:47:34.248936Z","iopub.execute_input":"2021-07-20T14:47:34.249536Z","iopub.status.idle":"2021-07-20T14:47:34.396393Z","shell.execute_reply.started":"2021-07-20T14:47:34.249498Z","shell.execute_reply":"2021-07-20T14:47:34.395524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(langs,f1)\nplt.title('F1-Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T14:47:39.157994Z","iopub.execute_input":"2021-07-20T14:47:39.158683Z","iopub.status.idle":"2021-07-20T14:47:39.305097Z","shell.execute_reply.started":"2021-07-20T14:47:39.158646Z","shell.execute_reply":"2021-07-20T14:47:39.303967Z"},"trusted":true},"execution_count":null,"outputs":[]}]}