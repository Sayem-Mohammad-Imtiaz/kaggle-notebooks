{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport re, string, time\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw = pd.read_csv(r'/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv')\ncorpus = raw[raw['source']=='ted']\ncorpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df, language):\n    df[language] = df[language].apply(lambda x:x.lower().strip())\n    df[language] = df[language].apply(lambda x:re.sub(r\"([?.!,])\", r\" \\1 \", x))\n    if language == 'english_sentence':\n        df[language] = df[language].apply(lambda x: re.sub(r\"[^a-zA-Z?.!,]+\", \" \", x))\n    df[language] = df[language].apply(lambda x:'<start> '+x+' <end>')\n    return df[language]\ncorpus['english_sentence'] = preprocessing(corpus, 'english_sentence')\ncorpus['hindi_sentence'] = preprocessing(corpus, 'hindi_sentence')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary=set()\nfor sentence in corpus['english_sentence'].to_list():\n    for word in sentence.split():\n        dictionary.add(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_seq(corpus_ln):\n    ln_tokenizer = Tokenizer()\n    ln_tokenizer.fit_on_texts(corpus_ln)\n    tensor = ln_tokenizer.texts_to_sequences(corpus_ln)\n    tensor = pad_sequences(tensor, padding='post')\n    return tensor, ln_tokenizer\nen_tensor, en_tokenizer = tokenize_seq(corpus['english_sentence'])\nhin_tensor, hin_tokenizer = tokenize_seq(corpus['hindi_sentence'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train, input_val, target_train, target_val = train_test_split(en_tensor, hin_tensor, test_size=0.2)\nprint(input_train.shape, 'English Training Shape')\nprint(input_val.shape, 'English Validation Shape')\nprint(target_train.shape, 'Indie Training Shape')\nprint(target_val.shape, 'Indie Validation Shape')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\nbuffer_size=len(input_train)\nsteps_per_batch=len(input_train)//64\nembed_dim=256\nunits=1024\nen_word_len = len(en_tokenizer.word_index)+1\nhin_word_len = len(hin_tokenizer.word_index)+1\ndataset=tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\ndataset=dataset.batch(batch_size, drop_remainder=True)\nval_dataset = tf.data.Dataset.from_tensor_slices((input_val, target_val)).batch(batch_size, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_input, exp_target = next(iter(dataset))\nprint(exp_input.shape)\nprint(exp_target.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encode(tf.keras.Model):\n    def __init__(self, units, batch, vocab_size, embed_dim):\n        super().__init__()\n        self.units = units\n        self.batch = batch\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, \n                                       recurrent_initializer='glorot_uniform')\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n    def initialization(self):\n        return tf.zeros((self.batch, self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encode(units, batch_size, en_word_len, embed_dim)\nexp_hidden = encoder.initialization()\nexp_input_tensor, exp_hidden = encoder(exp_input, exp_hidden)\nprint(exp_input_tensor.shape)\nprint(exp_hidden.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super().__init__()\n        self.w1 = tf.keras.layers.Dense(units)\n        self.w2 = tf.keras.layers.Dense(units)\n        self.v = tf.keras.layers.Dense(1)\n    def call(self, encoded_input, state):\n        state = tf.expand_dims(state, 1)\n        score = self.v(tf.nn.tanh(self.w1(encoded_input)+self.w2(state)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights*encoded_input\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_func = Attention(units)\nvector, weights = weights_func(exp_input_tensor, exp_hidden)\nprint(vector.shape)\nprint(weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, units, batch, vocab_size, embed_dim):\n        super().__init__()\n        self.units = units\n        self.batch = batch\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True\n                                       , recurrent_initializer='glorot_uniform')\n        self.dense = tf.keras.layers.Dense(vocab_size)\n        self.attention = Attention(self.units)\n    def call(self, x, encoded_input, state):\n        context_vector, weights = self.attention(encoded_input, state)\n        x = self.embedding(x)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        output = self.dense(output)\n        return output, state, weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(units, batch_size, hin_word_len, embed_dim)\noutput, state, weights = decoder(tf.random.uniform((batch_size, 1)), exp_input_tensor, exp_hidden)\nprint(output.shape)\nprint(state.shape)\nprint(weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ndef loss_function(value, output):\n    mask = tf.math.logical_not(tf.math.equal(value, 0))\n    loss = loss_object(value, output)\n    mask = tf.cast(mask, dtype = loss.dtype)\n    loss*=mask\n    return tf.reduce_mean(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(input, target, hidden):\n    loss=0\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(input, hidden)\n        dec_input = tf.expand_dims([hin_tokenizer.word_index['start']]*batch_size, 1)\n        \n        for i in range(1, target.shape[1]):\n            dec_output, dec_state, _ = decoder(dec_input, enc_output, hidden)\n            loss += loss_function(target[:, i], dec_output)\n            dec_input = tf.expand_dims(target[:, i], 1)\n        \n    batch_loss = loss / int(target.shape[1])\n    var = encoder.trainable_variables+decoder.trainable_variables\n    grad = tape.gradient(loss, var)\n    optimizer.apply_gradients(zip(grad, var))\n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 15\nfor epoch in range(epochs):\n    start = time.time()\n    hidden = encoder.initialization()\n    total_loss = 0\n    for (batch, (inputs, targets)) in enumerate(dataset.take(steps_per_batch)):\n        batch_loss = train_step(inputs, targets, hidden)\n        total_loss+=batch_loss\n        \n        if batch % 100 ==0:\n            print('Epoch {} Batch{} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n        \n    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss/steps_per_batch))\n    print('Total Computation Time Per Epoch: {} sec\\n'.format(time.time()-start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(corpus):\n    attention = np.zeros(input_val.shape[1], input_train.shape[1])\n    inputs = [en_tokenizer.word_index[i] for i in corpus.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=en_word_len,\n                                                         padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([hin_tokenizer.word_index['<start>']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n    attention_weights = tf.reshape(attention_weights, (-1, ))\n    attention_plot[t] = attention_weights.numpy()\n\n    predicted_id = tf.argmax(predictions[0]).numpy()\n\n    result += targ_lang.index_word[predicted_id] + ' '\n\n    if hin_tokenizer.index_word[predicted_id] == '<end>':\n        return result, sentence, attention_plot\n\n    # the predicted ID is fed back into the model\n    dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence, attention_plot ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(corpus):\n    result, corpus, attention_plot = evaluate(corpus)\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))\n    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw[raw['source']=='tides']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}