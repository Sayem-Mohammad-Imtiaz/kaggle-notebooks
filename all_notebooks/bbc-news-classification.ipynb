{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nimport nltk\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom collections import  Counter\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nimport re\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nsns.set()\n%matplotlib inline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sklearn\n","metadata":{"id":"cMg2ZMi3vehs","outputId":"6f805fad-a145-482f-ae83-941030d476cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's read our CSV file and rename column text to News_Headline\nbbc_text = pd.read_csv('../input/bbctext/bbc-text.csv')\nbbc_text=bbc_text.rename(columns = {'text': 'News_Headline'}, inplace = False)\nbbc_text.head(5)","metadata":{"id":"G28XZRdSv69A","outputId":"66786e57-6d69-4e03-f49b-d65ae25a334e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display all the categories\nbbc_text.category.unique()","metadata":{"id":"KZonkxJXEcdn","outputId":"82263a74-c739-4bb9-d31f-86c011ef5fe8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorize total number of headlines and plot\nprint('NUMBER OF SAMPLES IN EACH CATEGORY: \\n')\nsns.countplot(bbc_text.category,palette=\"Set1\")","metadata":{"id":"ngsFV93N2XWO","outputId":"08523c30-a883-45a5-e7d0-45aedc3c14c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's check range of the words in the dataset\ndef word_count(text):\n    text.str.split().\\\n        map(lambda x: len(x)).\\\n        hist()\nword_count(bbc_text['News_Headline'])","metadata":{"id":"T7J4C3nD7lfV","outputId":"9a974ce6-5f3b-4889-8471-e1f0be755334","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's find out all the stop words like a,the,or etc and plot them\ndef plot_stopwords(data):\n    stop=set(stopwords.words('english'))\n    data_split= data.str.split()\n    data_list=data_split.values.tolist()\n    corpus=[word for i in data_list for word in i]\n    from collections import defaultdict\n    dictionary_stopwords=defaultdict(int)\n    for word in corpus:\n        if word in stop:\n            dictionary_stopwords[word]+=1\n            \n    top=sorted(dictionary_stopwords.items(), key=lambda x:x[1],reverse=True)[:10] \n    x,y=zip(*top)\n    plt.bar(x,y)\nplot_stopwords(bbc_text['News_Headline'])","metadata":{"id":"irwWBrm08HVV","outputId":"671993c9-a5b5-4d2f-b418-71d2f977f03c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count of the most frequently occurring words\ndef top_frequent_words(data):\n    stop=set(stopwords.words('english'))\n    data_split= data.str.split()\n    data_list=data_split.values.tolist()\n    corpus=[word for i in data_list for word in i]\n    counter=Counter(corpus)\n    mostCommon=counter.most_common()\n    x, y=[], []\n    for word,count in mostCommon[:100]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)   \n    sns.barplot(x=y,y=x)\ntop_frequent_words(bbc_text['News_Headline'])","metadata":{"id":"IEhgiORt8kTZ","outputId":"fc176b09-1195-4873-b8d5-440d64629f03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_wordcloud(data):\n    stop=set(stopwords.words('english'))\n    def _preprocess_text(data):\n        corpus=[]\n        stem=PorterStemmer()\n        lem=WordNetLemmatizer()\n        for news in data:\n            words=[w for w in word_tokenize(news) if (w not in stop)]\n\n            words=[lem.lemmatize(w) for w in words if len(w)>2]\n\n            corpus.append(words)\n        return corpus\n    \n    corpus=_preprocess_text(data)\n    \n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=set(STOPWORDS),\n        max_words=200,\n        max_font_size=25, \n        scale=3,\n        random_state=1)\n    \n    wordcloud=wordcloud.generate(str(corpus))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n \n    plt.imshow(wordcloud)\n    plt.show()\nplot_wordcloud(bbc_text['News_Headline'])","metadata":{"id":"l3RbBNY09Mbv","outputId":"588b8192-c3a8-4473-8e93-de426c7b609a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot all headlines for positive negtaive and neutral semantics\nfrom textblob import TextBlob\ndef polarity_histo(data):\n    \n    def check_semantics(data):\n        return TextBlob(data).sentiment.polarity\n        \n    polarity_val =data.apply(lambda a : check_semantics(a))\n    polarity_val.hist()\npolarity_histo(bbc_text['News_Headline'])\n","metadata":{"id":"TzVqF04z93gi","outputId":"969b90c5-f885-4da5-f897-21985fc81cc0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA CLEANING\n\ndef cleaning(bbc_text):\n  if len(bbc_text)==1:\n      word_tokens = word_tokenize(bbc_text)\n  else:\n      print('Wait! Data is getting cleaned...')\n      # Tokenize : dividing Sentences into words\n      bbc_text['text_clean'] = bbc_text['News_Headline'].apply(nltk.word_tokenize)\n      print('Step 1-Tokenization Done!.')\n      print(bbc_text.head(5))\n  \n  # Remove stop words\n  if len(bbc_text)==1:\n      stop_words = set(stopwords.words('english')) \n      filtered_sentence = [w for w in word_tokens if not w in stop_words] \n      filtered_sentence = []   \n      for w in word_tokens: \n          if w not in stop_words: \n              filtered_sentence.append(w)\n  else:\n      stop_words=set(nltk.corpus.stopwords.words(\"english\"))\n      bbc_text['text_clean'] = bbc_text['text_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n      print('Step 2-All stop words are removed from the list.')\n      print(bbc_text.head(5))\n  #Will keep words and remove numbers and special characters\n  if len(bbc_text)!=1:\n      regex = '[a-z]+'\n      bbc_text['text_clean'] = bbc_text['text_clean'].apply(lambda x: [char for char in x if re.match(regex, char)])\n      print('Step3-Numbers and Special Characters are removed.')\n      print(bbc_text.head(5))\n  \n\ncleaning(bbc_text)\n","metadata":{"id":"kMTemMzM2oSF","outputId":"bb5e4336-1c20-468b-c6bf-41a340770c8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(bbc_text))\ndef detokenize(bbc_text):\n  for i in range(len(bbc_text)):\n    bbc_text_w = bbc_text['text_clean'][i]\n    a=TreebankWordDetokenizer().detokenize(bbc_text_w)\n    bbc_text.at[i,'text_clean']=a\ndetokenize(bbc_text)","metadata":{"id":"v5hHdTvdHp4j","outputId":"48581691-6fe2-46fe-bcf7-cd5bac6f138b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bbc_text['text_clean'])","metadata":{"id":"x4Fr0SaRLfIU","outputId":"4a23acc5-044f-47a5-dbb0-d8bcd55d68ea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's assigne numerical values to the unique categories\nbbc_text.category = bbc_text.category.map({'tech':0, 'business':1, 'sport':2, 'entertainment':3, 'politics':4})\nbbc_text.category.unique()\n","metadata":{"id":"rj8QoDRCwMcK","outputId":"60a9f916-a95e-46f8-fbda-27dff4071e74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check for all the null values if any\nbbc_text.isnull().sum()","metadata":{"id":"D9qKz3m2wieN","outputId":"1bde4711-6449-487b-b951-9630a2907029","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's split data into train and test\nX = bbc_text.text_clean\ny = bbc_text.category\n#split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.6, random_state = 1)","metadata":{"id":"UvahE9uzw-p7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vector = CountVectorizer(stop_words = 'english',lowercase=False)\n# fit the vectorizer on the training data\nvector.fit(X_train)\n# print(len(vector.get_feature_names()))\nvector.vocabulary_\nX_transformed = vector.transform(X_train)\n# print(X_transformed.toarray())\nX_transformed.toarray()\n# for test data\nX_test_transformed = vector.transform(X_test)\nprint(X_test_transformed.toarray)","metadata":{"id":"qFcQB757xBgY","outputId":"5ee67a31-499c-4f99-aedf-1fd5f8340bd7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_reg = LogisticRegression()\nlogistic_reg.fit(X_transformed, y_train)","metadata":{"id":"t6WTuiRM1KUB","outputId":"5044e0a3-7582-4d93-be2c-e801bf10c999","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit\nlogistic_reg.fit(X_transformed,y_train)\n\n# predict class\ny_predicted = logistic_reg.predict(X_test_transformed)\n\n# predict probabilities\ny_pred_probability = logistic_reg.predict_proba(X_test_transformed)","metadata":{"id":"W0aMmJyz1O6h","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing the overall accuracy\nmetrics.accuracy_score(y_test, y_predicted)","metadata":{"id":"O85kshBn1SJz","outputId":"9eceb87f-8bad-493b-d14f-d7493fa67ba9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_mat = metrics.confusion_matrix(y_test, y_predicted)\nprint(confusion_mat)\nTrueNeg = confusion_mat[0, 0]\nTruePos = confusion_mat[1, 1]\nFalseNeg = confusion_mat[1, 0]\nFalsePos = confusion_mat[0, 1]\nsensitivity = TruePos / float(FalseNeg + TruePos)\nprint(\"sensitivity\",sensitivity)\n\nspecificity = TrueNeg / float(TrueNeg + FalsePos)\nprint(\"specificity\",specificity)","metadata":{"id":"4skqZLLe1U6B","outputId":"b0e679bf-44cc-4660-b75c-39f4b0f83c4a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRECISION_SCORE = metrics.precision_score(y_test, y_predicted, average = 'micro')\nRECALL_SCORE = metrics.recall_score(y_test, y_predicted, average = 'micro')\nF1_SCORE =metrics.f1_score(y_test, y_predicted, average = 'micro')\n\nprint(\"PRECISION SCORE :\",PRECISION_SCORE)\nprint(\"RECALL SCORE :\",RECALL_SCORE)\nprint(\"F1 SCORE :\",F1_SCORE)","metadata":{"id":"tRxtjAyy1aIg","outputId":"decca898-75ec-46c1-efbb-ea7080f4da28","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"naivebayes = MultinomialNB()\nnaivebayes.fit(X_transformed, y_train)","metadata":{"id":"GmVJIYvK1eTv","outputId":"2b24ab49-a033-4519-d355-adf80d6a2f96","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit\nnaivebayes.fit(X_transformed,y_train)\n# predict class\ny_predict = naivebayes.predict(X_test_transformed)\n# predict probabilities\ny_pred_probability = naivebayes.predict_proba(X_test_transformed)","metadata":{"id":"e7G7V7xz1h1l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing the overall accuracy\nmetrics.accuracy_score(y_test, y_predict)","metadata":{"id":"pdg8XCeG1lX9","outputId":"7e7fdfd3-2deb-43a9-83e4-cd3deb9c5766","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\nmetrics.confusion_matrix(y_test, y_predict)\n# help(metrics.confusion_matrix)","metadata":{"id":"IFnmpJP31mgM","outputId":"552eb382-a793-4a8c-ca8c-7d9f321b4e48","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_test, y_predict)\nprint(confusion)\nTrueNeg = confusion_mat[0, 0]\nTruePos = confusion_mat[1, 1]\nFalseNeg = confusion_mat[1, 0]\nFalsePos = confusion_mat[0, 1]\nsensitivity = TruePos / float(FalseNeg + TruePos)\nprint(\"sensitivity\",sensitivity)\n\nspecificity = TrueNeg / float(TrueNeg + FalsePos)\nprint(\"specificity\",specificity)","metadata":{"id":"hUYN6smF1pKe","outputId":"58225404-0c68-4c1a-ed2a-df04fb1d987b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRECISION_SCORE = metrics.precision_score(y_test, y_predicted, average = 'micro')\nRECALL_SCORE = metrics.recall_score(y_test, y_predicted, average = 'micro')\nF1_SCORE =metrics.f1_score(y_test, y_predicted, average = 'micro')\n\nprint(\"PRECISION SCORE :\",PRECISION_SCORE)\nprint(\"RECALL SCORE :\",RECALL_SCORE)\nprint(\"F1 SCORE :\",F1_SCORE)","metadata":{"id":"9EszHjWP1s_k","outputId":"cd3ce43d-99c4-45e0-ceec-f7335317601c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headline1 = ['Morgan Wallen is a Billboard Music Awards finalist, but he wont be a part of the show']\nvec1 = vector.transform(headline1).toarray()\nprint('Headline:' ,headline1)\nprint(str(list(naivebayes.predict(vec1))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","metadata":{"id":"iwd5b0-V1tQY","outputId":"4517022b-3d20-4961-e0a6-5cd3baae83d7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relabel = {'0': 'tech', '1': 'business', '2': 'sport', '3': 'entertainment', '4': 'politics'}","metadata":{"id":"teERu61U10a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headline2 = ['Intel to invest $600 million to expand chip, Mobileye R&D in Israel']\nvec2 = vector.transform(headline2).toarray()\nprint('Headline:' ,headline2)\nprint(str(list(naivebayes.predict(vec2))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","metadata":{"id":"rip-pFi-13QN","outputId":"dd2b8a06-bd7f-4e0b-e3ac-0e23774179d7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headline3 = ['Tim Scott optimistic about Congress progress on police reform']\nvec3 = vector.transform(headline3).toarray()\nprint('Headline:', headline3)\nprint(str(list(naivebayes.predict(vec3))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","metadata":{"id":"vD8_OG0e18_U","outputId":"f8ffa3e6-95e3-45f4-d27a-26737a874b51","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headline4 = ['Bogdanovic scores 34 as Jazz rally to beat Raptors 106-102']\nvec4 = vector.transform(headline4).toarray()\nprint('Headline:', headline4)\nprint(str(list(naivebayes.predict(vec4))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","metadata":{"id":"6vckywU12CxI","outputId":"e628a6f9-1fa0-49bf-e976-d1e6d484013d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_class(headline):\n  vec4 = vector.transform(headline4).toarray()\n  classify=(str(list(naivebayes.predict(vec4))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))\n  return classify\npredict_class(headline4)","metadata":{"id":"Ro1rVw05DXAf","outputId":"a6b657a6-d688-41e6-95f4-72a0554db936","trusted":true},"execution_count":null,"outputs":[]}]}