{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a>\n# <center style=\"background-color:#63809e; color:white;\">HackerEarth-Deep-Learning-Challenge-Snakes-in-the-hood!</center>\n\n<center><img src=\"https://tblogqus.com/wp-content/uploads/2020/11/HackerEarth-Deep-Learning-Challenge-Snakes-in-the-hood.png\" width=70%></center>"},{"metadata":{},"cell_type":"markdown","source":"## Problem statement\n\nThe government has been facing a long-standing issue of wild animals entering residential areas due to various reasons. It's of critical importance that if any such dangerous animal is encountered, the concerned authority should be notified immediately. Reptiles, especially snakes, are among the most dangerous animals and they often enter residential areas. \n\nRecently due to an incident of a youngster getting bitten by a snake, the government decided to install cameras at every corner of the road to detect snakes and other animals.\n\nYou have been hired as a Deep Learning engineer to create a sophisticated model that can detect the breed of a snake from its image.\n\n## Data description\n\nThis data set consists of the following two columns:\n\nColumn Name\tDescription\n* image_id :\tName of the image file\n* breed    :\tSnake breed [35 different breeds]\n\nThe data folder consists of two folders and two .csv files. The details are as follows:\n\n* train: Contains 5508 images for 35 classes \n* test: Contains 2361 images\n* train.csv: 5508 x 2\n* test.csv: 2361 x 1"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a>\n## <center style=\"background-color:yellow; color:black;\">If you are facing in some import issues of fastai.callbacks or hooks please copy and edit this notebook to resolve the issue.</center>\n\n### <center style=\"background-color:orange; color:black;\">Please upvote this notebook if it helps you to implement the problem. Soon , I will update with a diffreent approach using Tranfer learning.</center>"},{"metadata":{},"cell_type":"markdown","source":"I was facing the same import issues in fastAi. Thanks @rajat ranjan for this kernel."},{"metadata":{},"cell_type":"markdown","source":"## Importing Necessary Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom fastai import *\nfrom fastai.vision import *\nimport torch\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook\n\n# import required libraries \nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16,ResNet101,ResNet101V2\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folder = Path(\"../input/hackerearth-deep-learning-identify-the-snake-breed/dataset\")\ndata_path = \"../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/train\"\npath = os.path.join(data_path , \"*jpg\")\npath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = glob.glob(path)\ndata=[]\nname_img_mapper = {}\nfor file in tqdm_notebook(files):\n    fn = file.split('/')[-1]\n    image = cv2.imread(file)\n    data.append(file)\n    name_img_mapper[fn] = image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## read the csv data files\ntrain_df = pd.read_csv('../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/train.csv')\ntest_df = pd.read_csv('../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatining the .jpg extension for the sake of simplicity\ntrain = train_df.copy()\ntest = test_df.copy()\n\ntrain['image_id'] = train['image_id']+str('.jpg')\ntest['image_id'] = test['image_id']+str('.jpg')\n\nprint(train.head())\nprint(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('breed').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='breed' , data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using Label encoder to encode the training labes into numeric\nle =LabelEncoder()\ntraining_labels=le.fit_transform(train['breed'])\n\n#train_df.loc[train_df['breed']== 'pantherophis-spiloides']['image_id'][:3].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Showing Sample Images\ntrain.loc[train['breed']== 'pantherophis-spiloides']['image_id'][:3].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_class(cat):\n    \n    fetch = train.loc[train_df['breed']== cat][:3]\n    images_names = train.loc[train_df['breed']== cat]['image_id'][:3].tolist()\n#     print(images_names)\n    fig = plt.figure(figsize=(20,15))\n    \n    for i, img_name in enumerate(images_names):\n        plt.subplot(1,3 ,i+1)\n        plt.imshow(name_img_mapper[img_name])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(name_img_mapper['299c932a68.jpg'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_class('pantherophis-spiloides')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##transformations to be done to images\ntfms = get_transforms(do_flip=True,flip_vert=False ,max_rotate=20.0, max_zoom=1.3, max_lighting=0.5, max_warp=0.1, p_affine=0.2,\n                      p_lighting=0.55)\n#, xtra_tfms=zoom_crop(scale=(0.9,1.8), do_rand=True, p=0.8))\n\n## create databunch of test set to be passed\ntest_img = ImageList.from_df(test, path=data_folder, folder='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(145)\n## create source of train image databunch\nsrc = (ImageList.from_df(train, path=data_folder, folder='train')\n       .split_by_rand_pct(0.10)\n       #.split_none()\n       .label_from_df()\n       .add_test(test_img))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (src.transform(tfms, size=224,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n        .databunch(path='.', bs=64, device= torch.device('cuda:0')).normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=3, figsize=(12,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(data=data, base_arch=models.resnet101, metrics=[FBeta(beta=1, average='macro'), accuracy],\n                    callback_fns=ShowGraph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets find the correct learning rate to be used from lr finder\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-03\nlearn.fit_one_cycle(10, slice(lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets plot the lr finder record\nlearn.unfreeze()\nlearn.lr_find()\n\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10,slice(1e-06,lr/10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see the most mis-classified images (on validation set)\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(9, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix(figsize=(12,12),dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##learn.TTA improves score further. lets see for the validation set\npred_val,y = learn.TTA(ds_type=DatasetType.Valid)\nfrom sklearn.metrics import f1_score, accuracy_score\nvalid_preds = [np.argmax(pred_val[i])+1 for i in range(len(pred_val))]\nvalid_preds = np.array(valid_preds)\ny = np.array(y+1)\naccuracy_score(valid_preds,y),f1_score(valid_preds,y, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,_ = learn.TTA(ds_type=DatasetType.Test)\n#preds,_ = learn.get_preds(ds_type = DatasetType.Test)\nlabelled_preds = [np.argmax(preds[i])+1 for i in range(len(preds))]\n\nlabelled_preds = np.array(labelled_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'image_id':test_df['image_id'], 'breed':labelled_preds})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = {'agkistrodon-contortrix':1,\n            'agkistrodon-piscivorus':2, \n            'coluber-constrictor':3, \n            'crotalus-atrox':4, \n            'crotalus-horridus':5, \n            'crotalus-ruber':6, \n            'crotalus-scutulatus':7, \n            'crotalus-viridis':8, \n            'diadophis-punctatus':9, \n            'haldea-striatula':10, \n            'heterodon-platirhinos':11, \n            'lampropeltis-californiae':12, \n            'lampropeltis-triangulum':13, \n            'masticophis-flagellum':14, \n            'natrix-natrix':15, \n            'nerodia-erythrogaster':16, \n            'nerodia-fasciata':17, \n            'nerodia-rhombifer':18, \n            'nerodia-sipedon':19, \n            'opheodrys-aestivus':20, \n            'pantherophis-alleghaniensis':21, \n            'pantherophis-emoryi':22, \n            'pantherophis-guttatus':23, \n            'pantherophis-obsoletus':24, \n            'pantherophis-spiloides':25, \n            'pantherophis-vulpinus':26, \n            'pituophis-catenifer':27, \n            'rhinocheilus-lecontei':28, \n            'storeria-dekayi':29, \n            'storeria-occipitomaculata':30, \n            'thamnophis-elegans':31, \n            'thamnophis-marcianus':32, \n            'thamnophis-proximus':33, \n            'thamnophis-radix':34, \n            'thamnophis-sirtalis':35}\n\n\nrev_category = {val: key for key, val in category.items()}\ndf['breed'] = df['breed'].map(rev_category)\ndf.to_csv('submission_test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(filename = 'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U efficientnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n# import required libraries \nimport os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load train and test csv file for image class\ntrain_original = pd.read_csv('../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/train.csv')\ntest_original = pd.read_csv('../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_original.copy()\ntest = test_original.copy()\n\ntrain['image_id'] = train['image_id']+str('.jpg')\ntest['image_id'] = test['image_id']+str('.jpg')\n\nprint(train.head())\nprint(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using Label encoder to encode the training labes into numeric\nle =LabelEncoder()\ntrain['breed'] = le.fit_transform(train['breed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img=[]\ntrain_label=[]\nj=0\npath='../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/train'\nfor i in tqdm(train['image_id']):\n    final_path=os.path.join(path,i)\n    img=cv2.imread(final_path)\n    img=cv2.resize(img,(224,224))\n    img=img.astype('float32')\n    train_img.append(img)\n    train_label.append(train['breed'][j])\n    j=j+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_img=[]\npath='../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/test'\nfor i in tqdm(test['image_id']):\n    final_path=os.path.join(path,i)\n    img=cv2.imread(final_path)\n    img=cv2.resize(img,(224,224))\n    img=img.astype('float32')\n    test_img.append(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras import applications\nfrom efficientnet.model import EfficientNetB7\nfrom keras import callbacks\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagenerator = ImageDataGenerator(\n        rescale=1/255,\n        validation_split=0.10,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest') \n\n\ndatagenerator.fit(train_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img=np.array(train_img)\ntest_img=np.array(test_img)\ntrain_label=np.array(train_label)\nprint(train_img.shape)\nprint(test_img.shape)\nprint(train_label.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\n\nbase_model = EfficientNetB7(weights='imagenet',input_shape=(224,224,3),include_top=False,pooling='avg')\n\nbase_model.trainable = False\n\nmodel = Sequential([\n  base_model, \n  Dense(1024, activation='relu'),\n  Dropout(0.5),\n  Dense(256, activation='relu'),\n  Dense(35, activation='softmax'),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}