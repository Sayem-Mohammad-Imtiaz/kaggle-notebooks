{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Handwritten Letters Autoencoder"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet import preprocess_input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use grayscale images because letters can be recognized in black and white."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nIMG_SIZE = 32\nCHANNELS = 1\ncolor_mode = 'grayscale' if CHANNELS == 1 else 'rgb'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                   validation_split = 0.1,\n                                   horizontal_flip=False,\n                                   dtype='float16')\ntrain_generator = train_datagen.flow_from_directory(\n            '/kaggle/input/classification-of-handwritten-letters/',\n            target_size=(IMG_SIZE,IMG_SIZE),\n            color_mode=color_mode,\n            batch_size=batch_size,\n            class_mode='input',\n            shuffle=True,\n            subset='training',\n            seed = 1)\nval_generator = train_datagen.flow_from_directory(\n            '/kaggle/input/classification-of-handwritten-letters/',\n            target_size=(IMG_SIZE,IMG_SIZE),\n            color_mode=color_mode,\n            batch_size=batch_size,\n            class_mode='input',\n            shuffle=True,\n            subset='validation',\n            seed = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model"},{"metadata":{},"cell_type":"markdown","source":"The number of dimensions of the compressed image is 40. The original dimension of the image is 32x32 = 1024. That's a 25-fold compression !","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"dim = 40\ninterdim = 120","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The layers are shared between the model to be trained, the model that generates the compression and the model that decompresses the images."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nc2D_1 = Conv2D(32, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same')\nc2D_2 = Conv2D(64, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same')\nc2D_3 = Conv2D(128, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same')\ndense1 = Dense(interdim, activation='relu')\ndense2 = Dense(dim, activation='tanh')\n\ndense2t = Dense(interdim, activation='relu')\ndense1t = Dense(2048, activation='relu')\nc2Dt_3 = Conv2DTranspose(128, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same')\nc2Dt_2 = Conv2DTranspose(64, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same')\nc2Dt_1 = Conv2DTranspose(32, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same')\n\nc2Dout = Conv2D(CHANNELS, kernel_size=(3, 3), activation='tanh', padding='same')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_in = Input(shape = (IMG_SIZE,IMG_SIZE,CHANNELS))\nX = c2D_1(X_in)\nX = MaxPooling2D(pool_size=(2, 2))(X)\nprint(X.shape)\nX = c2D_2(X)\nprint(X.shape)\nX = MaxPooling2D(pool_size=(2, 2))(X)\nX = c2D_3(X)\nprint(X.shape)\nX = MaxPooling2D(pool_size=(2, 2))(X)\nprint(X.shape)\nX = Reshape((2048,))(X)\nprint(X.shape)\nX = dense1(X)\nX_inter = dense2(X)\n\n\nX = dense2t(X_inter)\nX = dense1t(X)\nX = Reshape((4,4,128))(X)\nprint(X.shape)\nX = UpSampling2D((2, 2))(X)\nprint(X.shape)\nX = c2Dt_3(X)\nprint(X.shape)\nX = UpSampling2D((2, 2))(X)\nprint(X.shape)\nX = c2Dt_2(X)\nprint(X.shape)\nX = UpSampling2D((2, 2))(X)\nprint(X.shape)\nX = c2Dt_1(X)\n\nX_out = c2Dout(X)\nprint(X_out.shape)\n\nmodel2=Model(inputs=X_in,outputs=X_out)\n#model2end = Model(inputs=X_in,outputs=X_out)\nmodel2.compile(optimizer='adam',\n                   loss='mse',\n                   metrics=['mse'])\n\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_in = Input(shape=(dim,))\nX = dense2t(X_in)\nX = dense1t(X)\nX = Reshape((4,4,128))(X)\nX = UpSampling2D((2, 2))(X)\nX = c2Dt_3(X)\nX = UpSampling2D((2, 2))(X)\nX = c2Dt_2(X)\nX = UpSampling2D((2, 2))(X)\nX = c2Dt_1(X)\n\nX_out = c2Dout(X)\n\nmodel2end=Model(inputs=X_in,outputs=X_out)\nmodel2end.compile(optimizer='adam',\n                   loss='mse',\n                   metrics=['mse'])\n\nX_in = Input(shape = (IMG_SIZE,IMG_SIZE,CHANNELS))\nX = c2D_1(X_in)\nX = MaxPooling2D(pool_size=(2, 2))(X)\nX = c2D_2(X)\nX = MaxPooling2D(pool_size=(2, 2))(X)\nX = c2D_3(X)\nX = MaxPooling2D(pool_size=(2, 2))(X)\nX = Reshape((2048,))(X)\nX = dense1(X)\nX_out = dense2(X)\n\nmodel2start=Model(inputs=X_in,outputs=X_out)\nmodel2start.compile(optimizer='adam',\n                   loss='mse',\n                   metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nlearning_rate_fn = ExponentialDecay(0.004, 3000, 0.5)\n\nmodel2.compile(optimizer=Adam(learning_rate=learning_rate_fn),\n                   loss='mse',\n                   metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 18\n\nstep_size_train=train_generator.n//train_generator.batch_size\nstep_size_val=val_generator.n//val_generator.batch_size\n\nhistory = model2.fit(train_generator,\n                     steps_per_epoch = step_size_train,\n                     validation_data = val_generator, \n                     validation_steps = step_size_val,\n                     epochs=epochs) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fidelity of the restitution"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_generator.reset()\ntr,_ = val_generator.next()\nr = model2.predict(tr, 1)\nfig, axs = plt.subplots(batch_size, 2, figsize=(10,50))\nfor i in range(batch_size):\n    if CHANNELS == 1:\n        axs[i,0].imshow(r[i,:,:,0],cmap='gray')\n        axs[i,1].imshow(tr[i,:,:,0],cmap='gray')\n    else:\n        axs[i,0].imshow(r[i,:,:,:])\n        axs[i,1].imshow(tr[i,:,:,:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images in the left column are the compressed version of the images on the right. The quality of the restitution is ok, but not great. It can be noted that the grid in some of the images disappears. That compression is a way of removing the grid !"},{"metadata":{},"cell_type":"markdown","source":"## Is the compression maxed out ?"},{"metadata":{},"cell_type":"markdown","source":"Not completly : when taking random 40D-vectors then decompressing them, we do not get letters. That said, images are not that far from what we could get if we \"mixed\" handwritten letters. So the limit is probably not that far."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.random.random((batch_size,dim))*2-1\nr = model2end.predict(x)\nfig, axs = plt.subplots(batch_size//2, 2, figsize=(10,50))\nfor i in range(batch_size//2):\n    if CHANNELS == 1:\n        axs[i,0].imshow(r[i*2,:,:,0],cmap='gray')\n        axs[i,1].imshow(r[i*2+1,:,:,0],cmap='gray')\n    else:\n        axs[i,0].imshow(r[i*2,:,:,:])\n        axs[i,1].imshow(r[i*2+1,:,:,:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.save('/kaggle/working/gmodel2.tf')\nmodel2end.save('/kaggle/working/gmodel2end.tf')\nmodel2start.save('/kaggle/working/gmodel2start.tf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel2 = load_model('/kaggle/working/gmodel2.tf')\nmodel2end = load_model('/kaggle/working/gmodel2end.tf')\nmodel2start = load_model('/kaggle/working/gmodel2start.tf')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interpolation between letters"},{"metadata":{},"cell_type":"markdown","source":"If the compression is good enough, I expect to get believable letters if I take the barycenter between the 40D representation of letters which exist in the dataset. That is what I get, with the limitation that the compressed images are not that close to the \"true\" images found in the four corners."},{"metadata":{"trusted":true},"cell_type":"code","source":"val_generator.reset()\nval_generator.next()\nstart,_ = val_generator.next()\nend,_ = val_generator.next()\ndown,_ = val_generator.next()\nright,_ = val_generator.next()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"startinner = model2start.predict(start[0:1,:,:,:])\nendinner = model2start.predict(end[0:1,:,:,:])\ndowninner = model2start.predict(down[0:1,:,:,:])\nrightinner = model2start.predict(right[0:1,:,:,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 10\nfig, axs = plt.subplots(k+3,k+3, figsize=(50,50))\nif CHANNELS == 1:\n    #axs[0].imshow(end[0,:,:,0],cmap='gray')\n    axs[0,0].imshow(start[0,:,:,0],cmap='gray')\n    axs[-1,0].imshow(end[0,:,:,0],cmap='gray')\n    axs[0,-1].imshow(down[0,:,:,0],cmap='gray')\n    axs[-1,-1].imshow(right[0,:,:,0],cmap='gray')\n\nfor i in range(k+1):\n    for j in range(k+1):\n        intermediate_inner1 = (startinner * (k-i) + endinner * i)/k\n        intermediate_inner2 = (downinner * (k-i) + rightinner * i)/k\n        intermediate_inner = (intermediate_inner1 * (k-j) + intermediate_inner2 * j)/k\n        r = model2end.predict(intermediate_inner)\n        if CHANNELS == 1:\n            axs[i+1,j+1].imshow(r[0,:,:,0],cmap='gray')\n        else:\n            axs[i+1,j+1].imshow(r[0,:,:,:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}