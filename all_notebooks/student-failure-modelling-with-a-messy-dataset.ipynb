{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is my first attempt to clean data, engineer features, and train some machine learning models in Python. Any feedback will be appreciated.","metadata":{}},{"cell_type":"markdown","source":"*  [1. Introduction](#introduction)\n> * 1.1 Acknowledgements\n\n*  [2. Import libraries](#import-libraries)\n\n* [3. Data overview](#data-overview)\n> * [3.1. Assessments info](#ass)\n     * 3.1.1. Missing values and duplicate rows\n     * 3.1.2. Data types\n     * 3.1.3. Inconsistent weights\n         * 3.1.3.1. Fix inconsistent weights\n     * 3.1.4. Check if Assessments Info is the in Results table\n> * [3.2. Assessments results](#results)\n    * 3.2.1. Missing values and duplicate rows\n    * 3.2.2. Data types\n    * 3.2.3. Non-submissions\n> * [3.3. Courses info](#courses)\n    * 3.3.1. Missing values and duplicate rows\n    * 3.3.2. Data types\n> * [3.4. Student registration](#reg)\n    * 3.4.1. Missing values and duplicate rows\n    * 3.4.2. Data types\n    * 3.4.3. Check if in results table\n> * [3.5. VLE resources](#vle)\n    * 3.5.1. Missing values and duplicate rows\n    * 3.5.2. Data types\n> * [3.6. VLE Interactions](#vle-int)\n    * 3.6.1. Missing values and duplicate rows\n    * 3.6.2. Data types\n> * [3.7. Student information](#info)\n    * 3.7.1. Missing values and duplicate rows\n    * 3.7.2. Data types\n\n* [4. Frame the problem](#frame)\n\n* [5. Merge tables and Feature engineering](#merge)\n> * 5.1. VLE + VLE Interactions\n    * 5.1.1. Pre-prosessing\n> * 5.2. Registration info + Courses + Info\n> * 5.3. Assessment info + Assessment Results\n    * 5.3.1. Feature engineering\n        * 5.3.1.2. Late Submission\n        * 5.3.1.3. Fail rate\n    * 5.3.2. Merged all result tables\n> * 5.4. Merge all tables\n\n* [6. Split the dataset](#split)\n\n* [7. Final cleaning](#final-cleaning)\n\n* [8. Univariate analysis: numerical data](#num)\n\n* [9. Univariate analysis: categorical data](#cat)\n\n* [10. Bivariate analysis: final scores vs other variables](#scores-vs-variables)\n\n* [11. Regression](#regression)\n   > * 11.1 Model preparation\n   > * 11.2. Models\n   > * 11.3 Best Regression Model - evaluation\n\n* [12. Classification](#classification)\n   > * 12.1. Model preparation\n   > * 12.2. Models\n   > * 12.3. Best Classification Model - evaluation\n\n* [13. Discussion](#discussion)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n# 1. Introduction\n***","metadata":{}},{"cell_type":"markdown","source":"The dataset for this machine learning project has been provided by the learning analytics research group at the Knowledge Media institute, The Open University. The dataset is publicly available and consists of tables with information on student demographics, modules undertaken, time of year the modules start (module presentations), and information on student academic success in terms of grades for assignments and exams, as well as students’ interactions with the university’s Virtual Learning Environment (VLE).\n\nThe task at hand is to predict which students are to fail or withdraw and which are to pass their modules.\n\nThe dataset is rather messy, with many values missing and some inconsistencies between tables. All cleaning steps are detailed in the first part of the notebook. Various inconsistencies are reported and dealt with or suggestions are made as to how to deal with them in future work.\n\nSome feature engineering is done with suggestions for more features that could be of help in this project.\n\nFinally, several classification and regression models are used to predict student academic success.","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Acknowledgements","metadata":{}},{"cell_type":"markdown","source":"Two notebooks were very helpful in starting this analysis:\n* [Data Cleaning-Feature Generation-EDA-Segmentation by Anil](https://www.kaggle.com/anlgrbz/data-cleaning-feature-generation-eda-segmentation)\n* [Student Performance Prediction: Complete analysis by Victor Régis](https://www.kaggle.com/devassaxd/student-performance-prediction-complete-analysis)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"import-libraries\"></a>\n# 2. Import libraries\n***","metadata":{}},{"cell_type":"code","source":"# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualisations\n# (a) ggplot-like graphs for EDA\nfrom plotnine import *\nimport plotnine\nplotnine.options.figure_size = (5.2,3.2)\n# (b) for plotting other plots\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting random seed for notebook reproducability\nimport random\n\nseed = 123\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"data-overview\"></a>\n# 3. Data overview\n***","metadata":{}},{"cell_type":"markdown","source":"This chapter presents a quick overview of the data before the training/test split.","metadata":{}},{"cell_type":"code","source":"# load datasets\nass = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/assessments.csv')\ncourses = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/courses.csv')\nresults = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/studentAssessment.csv')\ninfo = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/studentInfo.csv')\nreg = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/studentRegistration.csv')\nvle = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/studentVle.csv')\nmaterials = pd.read_csv('/kaggle/input/open-university-learning-analytics-dataset/vle.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ass\"></a>\n## 3.1. Assessments info","metadata":{}},{"cell_type":"markdown","source":"This file contains information about assessments in module-presentations. Usually, every\npresentation has a number of assessments followed by the final exam. CSV contains columns:\n1. **code_module** – identification code of the module, to which the assessment belongs.\n2. **code_presentation** - identification code of the presentation, to which the assessment\nbelongs.\n3. **id_assessment** – identification number of the assessment.\n4. **assessment_type** – type of assessment. Three types of assessments exist: Tutor\nMarked Assessment (TMA), Computer Marked Assessment (CMA) and Final Exam\n(Exam).\n5. **date** – information about the final submission date of the assessment calculated as\nthe number of days since the start of the module-presentation. The starting date of\nthe presentation has number 0 (zero).\n6. **weight** - weight of the assessment in %. Typically, Exams are treated separately and\nhave the weight 100%; the sum of all other assessments is 100%.\nIf the information about the final exam date is missing, it is at the end of the last presentation\nweek.","metadata":{}},{"cell_type":"code","source":"ass.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"# Percentage of missing values\nass.isnull().sum() * 100 / len(ass)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ass.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ass[ass.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2. Data types","metadata":{}},{"cell_type":"code","source":"ass.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assessments IDs are denoted as integers. This is incorrect - IDs by definition are categorical. Below code corrects this.","metadata":{}},{"cell_type":"code","source":"ass['id_assessment'] = ass['id_assessment'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.3. Inconsistent weights","metadata":{}},{"cell_type":"markdown","source":"Project brief states that typically, exams have a weight of 100 and the sum of all other assessments is 100. This would man that a module with one exam only would have a weight of 100 and a module with one exam and some assessments would have a weight of 200. Let’s check if this so in the table provided.","metadata":{}},{"cell_type":"code","source":"# Group by module presentation and sum the weights of assessments\nass\\\n.groupby(['code_module','code_presentation'])\\\n.agg(total_weight = ('weight',sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see most that module presentations have total weight of 200, apart from module CCC which is 300 and module GGG which is 100. Let's have a closer look.","metadata":{}},{"cell_type":"code","source":"# See what are the weights of exams in module presentations\nass[ass['assessment_type'] == 'Exam']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(total_weight = ('weight',sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All modules show weight of 100 for exams apart from module CCC (for both presentations). Let's count the exams in each module presentation.","metadata":{}},{"cell_type":"code","source":"# Count how many exams there are in every module presentation\nass[ass['assessment_type'] == 'Exam'][['code_module', 'code_presentation', 'id_assessment']]\\\n.groupby(['code_module', 'code_presentation'])\\\n.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Module CCC has two exams, this can explain the hight assessments weight for this module. Now let's have a look at all the assignments that are not exams and see if everything is as it should be.","metadata":{}},{"cell_type":"code","source":"# Sum the weights of all course work assignments per module presentation\nass[ass['assessment_type'] != 'Exam']\\\n.groupby(['code_module', 'code_presentation'])\\\n.agg(total_weight = ('weight',sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that module GGG doesn't have any weight to its assignments. Is it because there's no assingments for this module?","metadata":{}},{"cell_type":"code","source":"ass[ass['code_module'] == 'GGG']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(weight_by_type = ('weight', sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Are there any other CMA and TMA assignments with a weight of 0?","metadata":{}},{"cell_type":"code","source":"ass[(ass['assessment_type'] == 'CMA') & (ass['weight'] == 0) & (ass['code_module'] != 'GGG')]['weight'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ass[(ass['assessment_type'] == 'TMA') & (ass['weight'] == 0) & (ass['code_module'] != 'GGG')]['weight'].count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ass[(ass['assessment_type'] == 'TMA') & (ass['weight'] == 0)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ass[ass['code_module'] == 'BBB']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(weight_by_type = ('weight',sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.3.1. Fix inconsistent weights","metadata":{}},{"cell_type":"markdown","source":"What is the usual weight of assignments?","metadata":{}},{"cell_type":"code","source":"column = ass[(ass['assessment_type'] == 'CMA') & (ass['code_module'] != 'GGG')]['weight']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column = ass[(ass['assessment_type'] == 'TMA') & (ass['code_module'] != 'GGG')]['weight']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many total assignments in GGG module are there?\nass[ass['code_module'] == 'GGG'][['code_module', 'code_presentation', 'assessment_type', 'id_assessment']]\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since CMA assignment is often weight 0, we will just assign 100 total weight to TMA assignment for simplicity.","metadata":{}},{"cell_type":"code","source":"# Assign new weights to module GGG assessments\nass.loc[(ass.code_module=='GGG') & (ass.assessment_type=='TMA'),'weight'] = (100/3)\nass.loc[(ass.code_module=='GGG') & (ass.assessment_type=='CMA'),'weight'] = (0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that TMA now sums to 100\nass[ass['code_module'] == 'GGG']\\\n.groupby(['code_module','code_presentation', 'assessment_type'])\\\n.agg(weight_by_type = ('weight', sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that all assessments now sum to 200\nass[ass['code_module'] == 'GGG']\\\n.groupby(['code_module','code_presentation'])\\\n.agg(total_weight = ('weight', sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.4. Check if Assessments Info is the in Results table","metadata":{}},{"cell_type":"code","source":"def compareCols(df1, df2):\n    '''\n    Check what columns are shared between two dataframes\n    and count values of df1 present and absent in df2 (in the shared\n    columns)\n    '''\n\n    # Show shared columns between dataframes\n    # (a) Make lists of columns for each data frame\n    df1Columns = df1.columns.values.tolist()\n    df2Columns = df2.columns.values.tolist()\n\n    # (b) Find column names that are the same\n    diffDict = set(df1Columns) & set(df2Columns)\n    \n    print('Shared columns : ', diffDict, '\\n')\n\n    # (c) Make a list of the dictinary\n    diffList = list(diffDict)\n    # (d) Check that if values in\n    # every shared column match in\n    # the two dataframes\n    for col in diffList:\n        x = df1[col].isin(df2[col]).value_counts()\n        print('Check if values are present in both dataframes:')\n        print(x, '\\n')\n\ncompareCols(ass, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findDiffValues(df1, df2, col):\n    '''\n    Find all df1.col values not present in df2.col\n    '''\n    # Pull out all unique values of col\n    df1_IDs = df1[col].unique()\n    df2_IDs = df2[col].unique()\n\n    # Compare the two lists\n    # (a) Find what values are different\n    diff = set(df1_IDs).difference(set(df2_IDs))\n    # (b) Count how many are different\n    numberDiff = len(diff)\n\n    print(\"Values from df1 not in df2: \" + str(diff))\n    print(\"Number of missing values: \" + str(numberDiff))\n\nfindDiffValues(ass, results, 'id_assessment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printDiffValues(df1, df2, col):\n    '''\n    Show all df1.col values not present in df2.col\n    '''\n    # Pull out all unique values id_assessments\n    df1_IDs = df1[col].unique()\n    df2_IDs = df2[col].unique()\n\n    # Compare the two lists\n    # (a) Find what values are different\n    diff = set(df1_IDs).difference(set(df2_IDs))\n    \n    # Show information for all df1.col values not presentin df2.col\n    # (a) Make a list of missing values\n    missingList = list(diff)\n    # (b) Find these IDs in df2\n    missingDf = df1[df1[col].isin(missingList)]\n\n    return missingDf\n\nprintDiffValues(ass, results, 'id_assessment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All assignments missing from the Results (and consequently Merged) table are exams with 100% module weight. Are there any other 100% weighted assignments in the Assessment table apart from these?","metadata":{}},{"cell_type":"code","source":"# Make a list of missing IDs\nmissingList = [30723, 1763, 34885, 15014, 37444, 14990, 30713, 37424, 15025, 34898, 37434, 40087, 34872, 40088, 15002, 1757, 30718, 34911]\n\n# Get all rows with weight 100 from Assessments table\nweight100 = ass[ass['weight'] == 100]\n# Get all unique assessment IDs\nweight100List = weight100['id_assessment'].unique()\n\n# Compare this list with the list of all assessment IDs missing from results table\ncompare = set(weight100List).difference(set(missingList))\nnumberCompare = len(compare)\n\nprint(\"100 weighted assessments in the Results table (that are not missing exams): \" + str(compare))\nprint(\"Number of 100 weighted assessments (that are not missing exams) in the Results table: \" + str(numberCompare))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show information for weight 100 assessments in the results table\n# (a) Make a list of IDs to look for\nmatchList = [24290, 25354, 24299, 25361, 25368, 25340]\n# (b) Find these IDs in the Assessments table\nmatchDf = ass[ass['id_assessment'].isin(matchList)]\n\nmatchDf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the above we can't say that all final exams are missing from the results table, just some exams.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"results\"></a>\n## 3.2. Assessments results","metadata":{}},{"cell_type":"markdown","source":"This file contains the results of students’ assessments. If the student does not submit the\nassessment, no result is recorded. The final exam submissions is missing, if the result of the\nassessments is not stored in the system. This file contains the following columns:\n1. **id_assessment** – the identification number of the assessment.\n2. **id_student** – a unique identification number for the student.\n3. **date_submitted** – the date of student submission, measured as the number of days\nsince the start of the module presentation.\n4. **is_banked** – a status flag indicating that the assessment result has been transferred\nfrom a previous presentation.\n5. **score** – the student’s score in this assessment. The range is from 0 to 100. The score\nlower than 40 is interpreted as Fail. The marks are in the range from 0 to 100.","metadata":{}},{"cell_type":"code","source":"results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"# Percentage of missing values\nresults.isnull().sum() * 100 / len(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the number of assessments in the results table does not match the number of assessments in the Assessments table.","metadata":{}},{"cell_type":"code","source":"results[results.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2. Data types","metadata":{}},{"cell_type":"code","source":"results.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results['id_assessment'] = results['id_assessment'].astype(object)\nresults['id_student'] = results['id_student'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3. Non-submissions","metadata":{}},{"cell_type":"markdown","source":"We know that if the student does not submit the assessment, no result is recorded. Therefore, all null scores can be interpreted as non-submissions. This means we can fill them out with zeros.\n\nIt is, however, a little strange that there are recorded submission days for assessments with null scores. One would expect a null value for the submission date for an assessment that has not been submitted. Ideally, this should be clarified with data providers.","metadata":{}},{"cell_type":"code","source":"# Have a look at NaN values\nresults[results['score'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace all null values with 0s\nresults.fillna(0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"courses\"></a>\n## 3.3. Courses info","metadata":{}},{"cell_type":"markdown","source":"File contains the list of all available modules and their details. The columns are:\n1. **code_module** – code name of the module, which serves as the identifier.\n2. **code_presentation** – code name of the presentation. It consists of the year and “B” for\nthe presentation starting in February and “J” for the presentation starting in October.\n3. **length** - length of the module-presentation in days.","metadata":{}},{"cell_type":"code","source":"courses.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"# Percentage of missing values\ncourses.isnull().sum() * 100 / len(courses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"courses.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"courses[courses.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.2. Data types","metadata":{}},{"cell_type":"code","source":"courses.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"reg\"></a>\n## 3.4. Student registration","metadata":{}},{"cell_type":"markdown","source":"This file contains information about the time when the student registered for the module\npresentation. For students who unregistered the date of unregistration is also recorded. File\ncontains five columns:\n1. **code_module** – an identification code for a module.\n2. **code_presentation** - the identification code of the presentation.\n3. **id_student** – a unique identification number for the student.\n4. **date_registration** – the date of student’s registration on the module presentation, this\nis the number of days measured relative to the start of the module-presentation (e.g.\nthe negative value -30 means that the student registered to module presentation 30\ndays before it started).\n5. **date_unregistration** – date of student un-registration from the module presentation,\nthis is the number of days measured relative to the start of the module-presentation.\nStudents, who completed the course have this field empty. Students who unregistered\nhave Withdrawal as the value of the final_result column in the studentInfo.csv file.","metadata":{}},{"cell_type":"code","source":"reg.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"# Percentage of missing values\nreg.isnull().sum() * 100 / len(reg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg[reg.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.2. Data types","metadata":{}},{"cell_type":"code","source":"reg.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg['id_student'] = reg['id_student'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4.3. Check if in Results table","metadata":{}},{"cell_type":"markdown","source":"Check if all student IDs recorded in the Registration tables are recorded in the Results table.","metadata":{}},{"cell_type":"code","source":"compareCols(reg, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 5847 students missing from the Results table. Are there any students from the Student Information table missing from the Results table?","metadata":{}},{"cell_type":"code","source":"compareCols(info, results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, there are also 5847 students recorded in the Students Information table missing from the Assessment Results table. Are these the same students?","metadata":{}},{"cell_type":"code","source":"# Pull out all unique values id_assessments\ndf1_IDs = reg['id_student'].unique()\ndf2_IDs = info['id_student'].unique()\n\n# Compare the two lists\n# (a) Find what assessment IDs are different\ndiff = set(df1_IDs).difference(set(df2_IDs))\n# (b) Count how many are different\nnumberDiff = len(diff)\n\nnumberDiff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compareCols(reg, info)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, these are the same students. Let's have a closer look.","metadata":{}},{"cell_type":"code","source":"info_not_in_results = printDiffValues(info, results, 'id_student')\ninfo_not_in_results.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are their final results?\ncolumn = info_not_in_results['final_result']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The brief stated that assignments not recorded in the Results table and not recorded due to the student not submitting them. However, here we have 2 students with no submissions recorded who have passed their modules. This may be due to two reasons:\n* The recorded pass is a clerical error.\n* The brief is wrong.","metadata":{}},{"cell_type":"code","source":"reg_not_in_results = printDiffValues(reg, results, 'id_student')\nreg_not_in_results.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are their unregistration status?\nreg_not_in_results['date_unregistration'].notnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, again, we see an inconsistency as all withdrawn students should have their date_unregistration field filled in. According to the Student Information table 4648 students have withdrawn, however, according to the Registration table 4594 students have unregistered. This leaves 54 withdrawn students without an unregistration date.\n\nLet's check unregistration dates for 2 students with passes that have no recorded assessment results. If we find unregistration dates for these students we'll know it's a clerical error.","metadata":{}},{"cell_type":"code","source":"# Show rows with passes\ninfo_not_in_results[info_not_in_results['final_result'] == 'Pass']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find their date unregistration\nreg_not_in_results[reg_not_in_results['id_student'] == 1336190]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_not_in_results[reg_not_in_results['id_student'] == 1777834]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no unregistration dates for these 2 students, however, we know there are 54 withdrawn students that have no unregistration dates, so it's unclear how much we can trust this data.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"vle\"></a>\n## 3.5. VLE resources","metadata":{}},{"cell_type":"markdown","source":"The csv file contains information about the available materials in the VLE. Typically, these are\nhtml pages, pdf files, etc. Students have access to these materials online and their interactions\nwith the materials are recorded. The table comprises of the following columns:\n1. **id_site** – an identification number of the material.\n2. **code_module** – an identification code for module.\n3. **code_presentation** - the identification code of presentation.\n4. **activity_type** – the role associated with the module material.\n5. **week_from** – the week from which the material is planned to be used.\n6. **week_to** – week until which the material is planned to be used.","metadata":{}},{"cell_type":"code","source":"materials.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"# Percentage of missing values\nmaterials.isnull().sum() * 100 / len(materials)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"materials.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"materials[materials.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5.2. Data types","metadata":{}},{"cell_type":"code","source":"materials.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"materials['id_site'] = materials['id_site'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"vle-int\"></a>\n## 3.6. VLE Interactions","metadata":{}},{"cell_type":"markdown","source":"The studentVle.csv file contains information about each student’s interactions with the\nmaterials in the VLE. This file contains the following columns:\n1. **code_module** – an identification code for a module.\n2. **code_presentation** - the identification code of the module presentation.\n3. **id_student** – a unique identification number for the student.\n4. **id_site** - an identification number for the VLE material.\n5. **date** – the date of student’s interaction with the material measured as the number of\ndays since the start of the module-presentation.\n6. **sum_click** – the number of times a student interacts with the material in that day.","metadata":{}},{"cell_type":"code","source":"vle.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.6.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"# Percentage of missing values\nvle.isnull().sum() * 100 / len(vle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vle.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vle[vle.duplicated()].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Duplication is entirely acceptable here as the system most likely records the clicks at different points on the same day, leading to duplicates.","metadata":{}},{"cell_type":"markdown","source":"### 3.6.2. Data types","metadata":{}},{"cell_type":"code","source":"vle.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vle['id_student'] = vle['id_student'].astype(object)\nvle['id_site'] = vle['id_site'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"info\"></a>\n## 3.7. Student information","metadata":{}},{"cell_type":"markdown","source":"This file contains demographic information about the students together with their results. File\ncontains the following columns:\n1. **code_module** – an identification code for a module on which the student is registered.\n2. **code_presentation** - the identification code of the presentation during which the\nstudent is registered on the module.\n3. **id_student** – a unique identification number for the student.\n4. **gender** – the student’s gender.\n5. **region** – identifies the geographic region, where the student lived while taking the\nmodule-presentation.\n6. **highest_education** – highest student education level on entry to the module\npresentation.\n7. **imd_band** – specifies the Index of Multiple Depravation band of the place where the\nstudent lived during the module-presentation.\n8. **age_band** – band of the student’s age.\n9. **num_of_prev_attempts** – the number times the student has attempted this module.\n10. **studied_credits** – the total number of credits for the modules the student is currently\nstudying.\n11. **disability** – indicates whether the student has declared a disability.\n12. **final_result** – student’s final result in the module-presentation.","metadata":{}},{"cell_type":"code","source":"info.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.7.1. Missing values and duplicate rows","metadata":{}},{"cell_type":"code","source":"info.isnull().sum() * 100 / len(info)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info[info.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.7.2. Data types","metadata":{}},{"cell_type":"code","source":"info.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info['id_student'] = info['id_student'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"frame\"></a>\n# 4. Frame the problem\n***","metadata":{}},{"cell_type":"markdown","source":"We can think of this work as a regression *and* a classification problem designed to predict student academic failure and student withdrawal from module presentations.\n\nConsidering the incompleteness of data, the above is tricky.\n\nThe scores in the Assessment Results table are not complete - all modules but one are missing their final exam results for all students. This means that using the table as a whole with scores as a response variable for regression can lead to less robust results as information is not complete. In other words, it is possible for a student to pass their assignments and fail their final exam resulting in overall fail for the module.\n\nAnother point is that score is the same thing as the final result (in the Student Information table), so predicting the likelihood of someone failing knowing that they got less than 40% as their final mark is not a prediction at all. And, it would be quite interesting to see if it is possible to identify students at risk of withdrawing or failing without knowing anything about their actual academic performance.\n\nAll of these points considered, this is the plan:\n\n1. **Classification problem**: merge all tables apart from Assessment Results and use the final result column from Student Information table as target.\n2. **Regression problem**: merge all tables, deleting the final result column from Student Information and using scores as target.\n\nWe can then see which method gives the best predictions.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"merge\"></a>\n# 5. Merge tables and engineer features\n***","metadata":{}},{"cell_type":"markdown","source":"## 5.1. VLE + VLE Interactions","metadata":{}},{"cell_type":"code","source":"compareCols(materials, vle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 96 entries in id_site in Materials table that are not in the VLE table.","metadata":{}},{"cell_type":"code","source":"findDiffValues(materials, vle, 'id_site')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printDiffValues(materials, vle, 'id_site')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This probbaly means these resources were not used by any students or that these resources did not record activity. And as such, we can merge these two tables with an inner merge as resources with no activity for any student provide zero information. Week_from and week_to columns can be dropped as they are over 82% empty. Drop date as it won't provide any extra information after grouping by module presentation per student.","metadata":{}},{"cell_type":"code","source":"# Merge with an inner join\nVLEmaterials = pd.merge(vle, materials, on=['code_module', 'code_presentation', 'id_site'], how='inner')\n# Drop columns\nVLEmaterials.drop(columns=['week_from', 'week_to', 'date'], inplace=True)\n\nVLEmaterials.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.1. Pre-prosessing","metadata":{}},{"cell_type":"markdown","source":"Get toal clicks per student per module presentation.","metadata":{}},{"cell_type":"code","source":"VLEmaterials\\\n.groupby(['code_module', 'code_presentation', 'id_student'])\\\n.agg(total_click = (\"sum_click\",sum))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_click_per_student = VLEmaterials\\\n.groupby(['code_module', 'code_presentation', 'id_student'])\\\n.agg(total_click = (\"sum_click\",sum))\\\n.reset_index()\n\ntotal_click_per_student.head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2. Registration info + Courses + Info","metadata":{}},{"cell_type":"markdown","source":"Date_registration may turn out to be a predictor of future fail or withdrawal as early registration may predict keen interest and future success, or in an opposite way, early registration means students become disinterested in the module by the time it starts and are likely to withdraw.","metadata":{}},{"cell_type":"code","source":"# Check that all module presentations in\n# Registration table are present in Courses table\ncompareCols(reg, courses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All module presentations from Registration table are present in the Courses table.","metadata":{}},{"cell_type":"markdown","source":"Course length may well be a good predictor of withdrawal simply due to the fact that longer courses will have more time for students to decide to drop out.","metadata":{}},{"cell_type":"code","source":"# Have a look at all unique module lengths\ncourses['module_presentation_length'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lengths of modules are not drastically different, but it might make an impact.","metadata":{}},{"cell_type":"code","source":"# Merge with an inner join\nregCourses = pd.merge(reg, courses, on=['code_module', 'code_presentation'], how='inner')\n\nregCourses.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge with an inner join\nregCoursesInfo = pd.merge(regCourses, info, on=['code_module', 'code_presentation', 'id_student'], how='inner')\n\nregCoursesInfo.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3. Assessment info + Assessment Results","metadata":{}},{"cell_type":"markdown","source":"Assessment information table will provide just that - information on weights for assessmtn scores.","metadata":{}},{"cell_type":"code","source":"# merge with an inner join\nassResults = pd.merge(ass, results, on=['id_assessment'], how='inner')\n# Rearrange column names\nassResults = assResults[['id_student', 'code_module', 'code_presentation', 'id_assessment', 'assessment_type', 'date', 'date_submitted', 'weight', 'is_banked', 'score']]\n\nassResults.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assResults.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that there are null values.","metadata":{}},{"cell_type":"markdown","source":"### 5.3.1. Feature engineering","metadata":{}},{"cell_type":"markdown","source":"#### 5.3.1.1. Weighted score","metadata":{}},{"cell_type":"markdown","source":"**How it will be calculated:**\n\nTo calculate the total weight of all modules, we need to remember that most final exams are missing from the Results table.\n\n1. Multiply the weight of the assignment with its score.\n2. Aggregate the dataframe per weight\\*score per module per module presentation with the sum function.\n3. Calculate total recorded weight of module (recorded total is key here as most modules are missing their final exam).\n4. Now calculate weighted scores - divide summed weight\\*score by total recorded weight of module.","metadata":{}},{"cell_type":"code","source":"# Make a copy of dataset\nscores = assResults\n\n# Count how many exams there are in Results for every module presentation\nscores[scores['assessment_type'] == 'Exam'][['code_module', 'code_presentation', 'id_assessment']]\\\n.groupby(['code_module', 'code_presentation'])\\\n.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **CCC module** only has results for 1 exam when the module should have 2 exams in total.\n* **DDD module** has results for the final exam (DDD module should have one exam in total).","metadata":{}},{"cell_type":"code","source":"### Make helper columns ###\n# (a) Add column multiplying weight and score\nscores['weight*score'] = scores['weight']*scores['score']\n# (b) Aggregate recorded weight*score per student\n    # per module presentation\nsum_scores = scores\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.agg(weightByScore = ('weight*score', sum))\\\n.reset_index()\n# (c) Calculate total recorded weight of module\n# (c.i) Get total weight of modules\ntotal_weight = ass\\\n.groupby(['code_module', 'code_presentation'])\\\n.agg(total_weight = ('weight', sum))\\\n.reset_index()\n# (c.ii) Subtract 100 to account for missing exams\ntotal_weight['total_weight'] = total_weight['total_weight']-100\n# (c.iii) Mark module DDD as having 200 credits \ntotal_weight.loc[(total_weight.code_module == 'DDD'), 'total_weight'] = 200\n\n### Calculate weighted score ###\n# (a) Merge sum_scores and total_weight tables\nscore_weights = pd.merge(sum_scores, total_weight, on=['code_module', 'code_presentation'], how='inner')\n# (b) Calculate weighted score\nscore_weights['weighted_score'] = score_weights['weightByScore'] / score_weights['total_weight']\n# (c) Drop helper columns\nscore_weights.drop(columns=['weightByScore', 'total_weight'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_weights.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One thing to note is that is_banked column is dropped along with date_submitted and assessment_type. We can add these as features to see if it impoves our model after we build a basic model.","metadata":{}},{"cell_type":"markdown","source":"#### 5.3.1.2. Late submission","metadata":{}},{"cell_type":"markdown","source":"Calculate the rate of late submission for the assignments that the student did submit.","metadata":{}},{"cell_type":"markdown","source":"**How will be calculated**\n\n1. Calculate the difference between the deadline and the actual submission date.\n2. Make a new column - if the difference between dates is more that ), the submission was late.\n3. Aggregate by student ID, module, and module presenation.","metadata":{}},{"cell_type":"code","source":"# Calculate the difference between the submission dates\nlateSubmission = assResults.assign(submission_days=assResults['date_submitted']-assResults['date'])\n# Make a column indicating if the submission was late or not \nlateSubmission = lateSubmission.assign(late_submission=lateSubmission['submission_days'] > 0)\n\nlateSubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Null scores will be assigned not a fail. It is ok as most submissions are not fails, so it would make sense to automatically assign them as passes.","metadata":{}},{"cell_type":"markdown","source":"Can exams be late submissions?","metadata":{}},{"cell_type":"code","source":"lateSubmission[(lateSubmission['assessment_type'] == 'Exam') & (lateSubmission['late_submission'] == True)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, exams can be submitted late.","metadata":{}},{"cell_type":"code","source":"# Aggregate per student per module presentation\ntotal_late_per_student = lateSubmission\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.agg(total_late_submission = ('late_submission', sum))\\\n.reset_index()\n\ntotal_late_per_student.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a df with total number of all assessments per student per module presentation\ntotal_count_assessments = lateSubmission[['id_student', 'code_module', 'code_presentation', 'id_assessment']]\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.size()\\\n.reset_index(name='total_assessments')\n\ntotal_count_assessments.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge df with total late assessements and total count assessments\nlate_rate_per_student = pd.merge(total_late_per_student, total_count_assessments, on=['id_student', 'code_module', 'code_presentation'], how='inner')\n# Make a new column with late submission rate\nlate_rate_per_student['late_rate'] = late_rate_per_student['total_late_submission'] / late_rate_per_student['total_assessments']\n# Drop helper columns\nlate_rate_per_student.drop(columns=['total_late_submission', 'total_assessments'], inplace=True)\n\nlate_rate_per_student","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.3.1.3. Fail rate","metadata":{}},{"cell_type":"markdown","source":"Do the same as above to calculate the fail rate.","metadata":{}},{"cell_type":"code","source":"# Define function for marking failed assignments\npassRate = assResults\npassRate = passRate.assign(fail=passRate['score'] < 40)\n\npassRate.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passRate.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregate per student per module presentation\ntotal_fails_per_student = passRate\\\n.groupby(['id_student', 'code_module', 'code_presentation'])\\\n.agg(total_fails = (\"fail\",sum))\\\n.reset_index()\n\ntotal_fails_per_student.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge df with total fails and total count assessments\nfail_rate_per_student = pd.merge(total_fails_per_student, total_count_assessments, on=['id_student', 'code_module', 'code_presentation'], how='inner')\n# Make a new column with late submission rate\nfail_rate_per_student['fail_rate'] = fail_rate_per_student['total_fails'] / fail_rate_per_student['total_assessments']\n# Drop helper columns\nfail_rate_per_student.drop(columns=['total_fails', 'total_assessments'], inplace=True)\n\nfail_rate_per_student","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5.3.2. Merged all result tables","metadata":{}},{"cell_type":"code","source":"assessments = pd.merge(score_weights, late_rate_per_student, on=['id_student', 'code_module', 'code_presentation'], how='inner')\nassessments = pd.merge(assessments, fail_rate_per_student, on=['id_student', 'code_module', 'code_presentation'], how='inner')\n\nassessments.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 Merge all tables","metadata":{}},{"cell_type":"markdown","source":" The dataframes created previously:\n \n 1. VLE + VLE materials = total_click_per_student\n 2. Registration Info + Courses + Student Info = regCoursesInfo\n 3. Assessments + Results = assessments","metadata":{}},{"cell_type":"code","source":"merged = pd.merge(regCoursesInfo, total_click_per_student, on=['id_student', 'code_module', 'code_presentation'], how='left')\n\nmerged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = pd.merge(merged, assessments, on=['id_student', 'code_module', 'code_presentation'], how='left')\n\nmerged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"split\"></a>\n# 6. Split the dataset\n***","metadata":{}},{"cell_type":"markdown","source":"It's important to split the dataset before doing serious exploratory analysis as we do not want to peak at the testing data. Any pre-processing and further feature engineering will also be done to the test set with the same parameters as are set for the training set. We'' stratify by code module to make sure that each module is represented equally in both the test and the training sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(merged, test_size=0.2, random_state=42, stratify=merged['code_module'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"final-cleaning\"></a>\n# 7. Final cleaning\n***","metadata":{}},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a few missing values we need to impute.","metadata":{}},{"cell_type":"markdown","source":"### IMD band","metadata":{}},{"cell_type":"code","source":"train\\\n[train['imd_band'].isnull()]\\\n.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How to fill out missing values here - fill out according to the most frequent band for that region:\n1. Find which regions have null imd_band values\n2. Find what band is the most frequent one for that region\n3. Replace null values with most frequent values for that region.","metadata":{}},{"cell_type":"code","source":"# Find what is the most frequent band in each region\nregions_list = list(train\\\n                    [train['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    result = train[train['region'] == i].imd_band.mode()\n    print(f'{i} IMD band : \\n', result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace all null values with respective most frequent imd_bands\nregions_list = list(train\\\n                    [train['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    train['imd_band'] = np.where( ( (train['imd_band'].isnull()) & (train['region'] == i) ),\n                                           train[train['region'] == i].imd_band.mode(),\n                                           train['imd_band']\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Date registration","metadata":{}},{"cell_type":"code","source":"# Make a new dataframe just with rows that have null values for the registration date\nreg_date_nulls_in_reg = train\\\n[train['date_registration'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are their final results?\ncolumn = reg_date_nulls_in_reg['final_result']\n\nunique, counts = np.unique(column, return_counts = True)\n\ndict(zip(unique, counts))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the withdrawn students let's not put the registration date after unregistation date. Let's substract the median value from the unregistration date to fill these.","metadata":{}},{"cell_type":"code","source":"# Get median registration date\ntrain.date_registration.median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace NaN values with date_unreg minus the median (note, the median is negative)\ntrain['date_registration'] = np.where( (train['date_registration'].isnull()),\n                                           train['date_unregistration'] + train.date_registration.median(),\n                                           train['date_registration']\n                                    )\n# Replace remaining NaNs with -57\ntrain['date_registration'] = np.where( (train['date_registration'].isnull()),\n                                           train.date_registration.median(),\n                                           train['date_registration']\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### total_click","metadata":{}},{"cell_type":"markdown","source":"Those students who have null values for total_click are the students who did not have any records in the VLE Interactions table, meaning they did not interact with VLE. Therefore, we can replace them with 0s.","metadata":{}},{"cell_type":"code","source":"train['total_click'] = train['total_click'].replace(np.nan).fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### weighted_score","metadata":{}},{"cell_type":"markdown","source":"The students who have null values for weighted_score have not submitted any assignments. We can replace the nan values with 0s.","metadata":{}},{"cell_type":"code","source":"train['weighted_score'] = train['weighted_score'].replace(np.nan).fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### late_rate","metadata":{}},{"cell_type":"markdown","source":"Students who have nan values for their late submission rate have not submitted any of their assignments. We can replace the nan values with 1.00 (100% late rate).","metadata":{}},{"cell_type":"code","source":"train['late_rate'] = train['late_rate'].replace(np.nan).fillna(1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### fail_rate","metadata":{}},{"cell_type":"markdown","source":"Students who have nan values for their fail rate have not submitted any assingments. Their fail rate is therefore 100% (1.0).","metadata":{}},{"cell_type":"code","source":"train['fail_rate'] = train['fail_rate'].replace(np.nan).fillna(1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop columns","metadata":{}},{"cell_type":"code","source":"# Make a copy of training and test datasets for classification\ntrain_class = train.copy()\ntest_class = test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For Regression","metadata":{}},{"cell_type":"markdown","source":"Date unregistration has been dropped as it should be the same metric as Withdrawal in the final results column of Student Information table. I'm assuming the prediction for who is likely to withdraw would only be useful if trying to predict future withdrawals, trying to predict if a student has withdrawn when we know they have withdrawn is useless (date_unregistration is essentially another target).\n\nFor the regression problem due to the contuinous nature of the target variable, we can't distinguish between fails and witdrawals, so all withdrawals will be treated as fails (score < 40%).","metadata":{}},{"cell_type":"code","source":"# Drop unneeded columns\ntrain.drop(columns=['id_student'], inplace=True)\ntrain.drop(columns=['final_result'], inplace=True)\ntrain.drop(columns=['date_unregistration'], inplace=True)\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For Classification","metadata":{}},{"cell_type":"code","source":"# Drop unneeded columns\ntrain_class.drop(columns=['id_student'], inplace=True)\ntrain_class.drop(columns=['date_unregistration'], inplace=True)\n# Drop columns on assessments\ntrain_class.drop(columns=['weighted_score'], inplace=True)\ntrain_class.drop(columns=['late_rate'], inplace=True)\ntrain_class.drop(columns=['fail_rate'], inplace=True)\n\n\ntrain_class.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning test sets","metadata":{}},{"cell_type":"markdown","source":"### For Regression","metadata":{}},{"cell_type":"code","source":"'''IMD BAND'''\n# Replace all null values with respective most frequent imd_bands\nregions_list = list(test\\\n                    [test['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    test['imd_band'] = np.where( ( (test['imd_band'].isnull()) & (test['region'] == i) ),\n                                           test[test['region'] == i].imd_band.mode(),\n                                           test['imd_band']\n                                    )\n\n'''DATE REGISTRATION'''\n# Get registration date median\nreg_date_median = test.date_registration.median()\n\n\n# Replace NaN values with date_unreg minus 57 days\ntest['date_registration'] = np.where( (test['date_registration'].isnull()),\n                                           test['date_unregistration'] + reg_date_median,\n                                           test['date_registration']\n                                    )\n# Replace remaining NaNs with -57\ntest['date_registration'] = np.where( (test['date_registration'].isnull()),\n                                           reg_date_median,\n                                           test['date_registration']\n                                    )\n\n'''Rest of null values'''\ntest['total_click'] = test['total_click'].replace(np.nan).fillna(0)\ntest['weighted_score'] = test['weighted_score'].replace(np.nan).fillna(0)\ntest['late_rate'] = test['late_rate'].replace(np.nan).fillna(1.0)\ntest['fail_rate'] = test['fail_rate'].replace(np.nan).fillna(1.0)\n\n'''Drop unneeded columns'''\n# Drop ID, final result, and date unregistration columns\ntest.drop(columns=['id_student'], inplace=True)\ntest.drop(columns=['final_result'], inplace=True)\ntest.drop(columns=['date_unregistration'], inplace=True)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For Classification","metadata":{}},{"cell_type":"code","source":"'''IMD BAND'''\n# Replace all null values with respective most frequent imd_bands\nregions_list = list(test_class\\\n                    [test_class['imd_band'].isnull()]['region']\\\n                    .unique())\n\nfor i in regions_list:\n    test_class['imd_band'] = np.where( ( (test_class['imd_band'].isnull()) & (test_class['region'] == i) ),\n                                           test_class[test_class['region'] == i].imd_band.mode(),\n                                           test_class['imd_band']\n                                    )\n\n'''DATE REGISTRATION'''\n# Get registration date median\nreg_date_median = test_class.date_registration.median()\n\n\n# Replace NaN values with date_unreg minus 57 days\ntest_class['date_registration'] = np.where( (test_class['date_registration'].isnull()),\n                                           test_class['date_unregistration'] + reg_date_median,\n                                           test_class['date_registration']\n                                    )\n# Replace remaining NaNs with -57\ntest_class['date_registration'] = np.where( (test_class['date_registration'].isnull()),\n                                           reg_date_median,\n                                           test_class['date_registration']\n                                    )\n\n'''Rest of null values'''\ntest_class['total_click'] = test_class['total_click'].replace(np.nan).fillna(0)\n\n'''Drop unneeded columns'''\n# Drop ID, final result, and date unregistration columns\ntest_class.drop(columns=['id_student'], inplace=True)\ntest_class.drop(columns=['date_unregistration'], inplace=True)\n# Drop columns on assessments\ntest_class.drop(columns=['weighted_score'], inplace=True)\ntest_class.drop(columns=['late_rate'], inplace=True)\ntest_class.drop(columns=['fail_rate'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"num\"></a>\n# 8. Univariate analysis: numerical data\n***","metadata":{}},{"cell_type":"code","source":"train.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution plots","metadata":{}},{"cell_type":"markdown","source":"Code for the below cell is adapted from a [Kaggle notebook](https://www.kaggle.com/teertha/us-health-insurance-eda) by Anirban Datta.","metadata":{}},{"cell_type":"code","source":"# Create statistics summaries with skew, mean, and median\n# Produce a dataframe with just numerical columns\ndf_num = train.select_dtypes(include=np.number)\n\nfor col in df_num.columns:\n\n    skew = df_num[col].skew()\n    mean = df_num[col].mean()\n    median = df_num[col].median()\n    \n    print(f'\\tSummary for {col.upper()}')\n    print(f'Skewness of {col}\\t: {skew}')\n    print(f'Mean {col} :\\t {mean}')\n    print(f'Median {col} :\\t {median} \\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.hist(bins=50, figsize=(20,15))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a lot of skewed variables in this dataset. Something to keep in mind when using linear models as these assume normal distributions.","metadata":{}},{"cell_type":"markdown","source":"## Target variable - weighted score","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x=0, y='weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per module\")\n    + coord_flip()\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the target variable has two peaks and is not normally distributed, but it doesn't have any outliers. We may wish to transform the target at some point to improve our models. This notebook shows very basic analysis though, so we will not be doing this, but it's something to keep in mind when using certain models (like the linear regression that assumes the distributions are normal).","metadata":{}},{"cell_type":"markdown","source":"## Correlation matrix","metadata":{}},{"cell_type":"code","source":"# Let's make a correlation heatmap\nplt.figure(figsize=(6,4))\nsns.heatmap(df_num.corr(), annot=True, cmap=\"coolwarm\", );","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This can show us that there is little colinearity among the variables. Let's have a closer look at linear correlations between features and the target.","metadata":{}},{"cell_type":"code","source":"train\\\n.drop(columns=['weighted_score'])\\\n.corrwith(train['weighted_score']).plot.bar(\n        figsize = (6, 4), title = \"Correlation with Target\", fontsize = 12,\n        rot = 90, grid = True);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the numbers for this: ","metadata":{}},{"cell_type":"code","source":"train.corrwith(train['weighted_score']).sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Immediately we can see weighted_score is most strongly positively correlated with total_click. The more students engaged with Blackboard, the better results they got. Theyre is also weak negative correlation with the number of the previous attempts. Late_rate and fail_rate also negatively correlated with weighted score, albeit weakly.\n\nThere's no correlation with module_presentation_length or date_registration, or studied_credits.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"cat\"></a>\n# 9. Univariate analysis: categorical data\n***","metadata":{}},{"cell_type":"code","source":"# Produce a dataframe with just categorical columns\ndf_cat = train.select_dtypes(exclude=np.number)\n\ndf_cat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the plot number for the first subplot function\nplot_number = 1\n\n# Set sizes for all plots\nplt.figure(figsize=(15, 15)) # create a figure object\nplt.subplots_adjust(hspace = 0.5) # set the size of subplots\n\nfor col in df_cat[['code_module', 'code_presentation', 'gender', 'region']]:\n    \n    # Call countplot on each column\n    plt.subplot(4, 2, plot_number)\n    sns.countplot(\n        y=col,\n        data=df_cat,\n        order=df_cat[col].value_counts().index\n    )\n    plt.title(f'{col.capitalize()} Countplot')\n    plt.xlabel('')\n    plt.ylabel('')\n\n    plot_number = plot_number + 1 # set a new plot number for the next subplot function\n    \n    # Add relative frequency labels:\n    n_points = df_cat.shape[0]\n    col_counts = df_cat[col].value_counts()\n    locs, labels = plt.yticks()   # get the current tick locations and labels\n\n    # loop through each pair of locations and labels\n    for loc, label in zip(locs, labels):\n\n        # get the text property for the label to get the correct count\n        count = col_counts[label.get_text()]\n        pct_string = '{:0.1f}%'.format(100*count/n_points)\n\n        # print the annotation at the top of the bar\n        plt.text(x=count, y=loc, s=pct_string, ha='left', va='center', color='k')\n    \nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the plot number for the first subplot function\nplot_number = 1\n\n# Set sizes for all plots\nplt.figure(figsize=(15, 15)) # create a figure object\nplt.subplots_adjust(hspace = 0.5) # set the size of subplots\n\nfor col in df_cat[['highest_education', 'imd_band', 'age_band', 'disability']]:\n    \n    # Call countplot on each column\n    plt.subplot(4, 2, plot_number)\n    sns.countplot(\n        y=col,\n        data=df_cat,\n        order=df_cat[col].value_counts().index\n    )\n    plt.title(f'{col.capitalize()} Countplot')\n    plt.xlabel('')\n    plt.ylabel('')\n\n    plot_number = plot_number + 1 # set a new plot number for the next subplot function\n    \n    # Add relative frequency labels:\n    n_points = df_cat.shape[0]\n    col_counts = df_cat[col].value_counts()\n    locs, labels = plt.yticks()   # get the current tick locations and labels\n\n    # loop through each pair of locations and labels\n    for loc, label in zip(locs, labels):\n\n        # get the text property for the label to get the correct count\n        count = col_counts[label.get_text()]\n        pct_string = '{:0.1f}%'.format(100*count/n_points)\n\n        # print the annotation at the top of the bar\n        plt.text(x=count, y=loc, s=pct_string, ha='left', va='center', color='k')\n    \nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Very few students with no formal education (1%)\n* Very few students with post-grad qualifications.\n\nThese two categories should be merged with 'Lower Than A Level' and 'HE Qualification', respectivelly, as with so little data these two categories are not likely to bring much insight.","metadata":{}},{"cell_type":"markdown","source":"## Change education categories","metadata":{}},{"cell_type":"code","source":"# Rename 'no formal quals' into 'lower than a level'\ntrain['highest_education'] = np.where( (train['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           train['highest_education']\n                                    )\n\n# Rename post-grads\ntrain['highest_education'] = np.where( (train['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           train['highest_education']\n                                    )\n\n\n# Do the same for the test set\ntest['highest_education'] = np.where( (test['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           test['highest_education']\n                                    )\n\ntest['highest_education'] = np.where( (test['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           test['highest_education']\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Change age categories","metadata":{}},{"cell_type":"markdown","source":"Same can be done for the age bands, merging 35-55 and 55+ groups into one. First, let's have a closer look at this variable.","metadata":{}},{"cell_type":"code","source":"# Have a closer look at the category\n(\n    ggplot(train)\n    + aes(x='age_band', fill='age_band')\n    + geom_bar()\n    + geom_text(\n     aes(label='stat(prop)*100', group=1),\n     stat='count',\n     nudge_y=0.125,\n     va='bottom',\n     format_string='{:.1f}%'\n )\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's merge the least frequent categories.","metadata":{}},{"cell_type":"code","source":"# Replace 55+ and 35-55 groups with 35+\ntrain['age_band'] = np.where( (train['age_band'] == '55<='),\n                                           '35+',\n                                           train['age_band']\n                                    )\n\ntrain['age_band'] = np.where( (train['age_band'] == '35-55'),\n                                           '35+',\n                                           train['age_band']\n                                    )\n\n# Do the same for the test set\ntest['age_band'] = np.where( (test['age_band'] == '55<='),\n                                           '35+',\n                                           test['age_band']\n                                    )\n\ntest['age_band'] = np.where( (test['age_band'] == '35-55'),\n                                           '35+',\n                                           test['age_band']\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what our count plot for the age variable looks like after merging the categories.","metadata":{}},{"cell_type":"code","source":"# See the changes\n(\n    ggplot(train)\n    + aes(x='age_band', fill='age_band')\n    + geom_bar()\n    + geom_text(\n     aes(label='stat(prop)*100', group=1),\n     stat='count',\n     nudge_y=0.125,\n     va='bottom',\n     format_string='{:.1f}%'\n )\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"scores-vs-variables\"></a>\n# 10. Bivariate analysis: final scores vs other variables\n***","metadata":{}},{"cell_type":"markdown","source":"## Numerical","metadata":{}},{"cell_type":"code","source":"sns.pairplot(train)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are vague. There doesn't seem to be any strong relationships between any variables and the target except, perhaps, the number of total clicks.","metadata":{}},{"cell_type":"markdown","source":"## Categorical","metadata":{}},{"cell_type":"markdown","source":"Let's make a helper column to indicate if the student failed or not so we can compare categorical variables for failed and passing students.","metadata":{}},{"cell_type":"code","source":"train = train.assign(fail_final=train['weighted_score'] < 40)\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### code_module","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='code_module', fill='fail_final')\n    + geom_bar(position='fill')\n    + ggtitle(\"Count frequency of different modules by pass rate\")\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes('code_module', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per module\")\n    + coord_flip()\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that some modules have higher fail rates than others. For example, for module CCC the pass rate is just a little over 50%. The boxplot also reveals some outliers.","metadata":{}},{"cell_type":"markdown","source":"### code_presentation","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='code_presentation', fill='fail_final')\n    + geom_bar(position='fill')\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes('code_presentation', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per presentation\")\n    + coord_flip()\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Module presentation (semester) seems to have no effect on the pass/fail rate.","metadata":{}},{"cell_type":"markdown","source":"### gender","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='gender', fill='fail_final')\n    + geom_bar(position='fill')\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes('gender', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per sex\")\n    + coord_flip()\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, there's not much difference between men and women passing or failing modules.","metadata":{}},{"cell_type":"markdown","source":"### region","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='region', fill='fail_final')\n    + geom_bar(position='fill')\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes('region', 'weighted_score')\n    + geom_boxplot(outlier_color='crimson')\n    + ggtitle(\"Distribution of scores per region\")\n    + coord_flip()\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is very little difference in pass rates between regions.","metadata":{}},{"cell_type":"markdown","source":"### highest_education","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='highest_education', fill='fail_final')\n    + geom_bar(position='fill')\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Students that have lower than A level previous education seem to fail more, however, the difference is so slight it may not be statistically significant.","metadata":{}},{"cell_type":"markdown","source":"### imd_band","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='imd_band', fill='fail_final')\n    + geom_bar(position='fill')\n    + theme(axis_text_x=element_text(rotation=45, hjust=1))\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Very little difference in pass rates amongst students in different deprivation bands, but there does seem to be a trend - the more deprived the area is, the higher the fail rate. 0-10% IMD means the student lives in an area that that falls amongst top 0-10% most deprived small areas (the higher the percentage, the more deprived the area).","metadata":{}},{"cell_type":"markdown","source":"### age_band","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='age_band', fill='fail_final')\n    + geom_bar(position='fill')\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Older people seem to do bettwer academically, however, the difference is fairly small.","metadata":{}},{"cell_type":"markdown","source":"### disability","metadata":{}},{"cell_type":"code","source":"(\n    ggplot(train)\n    + aes(x='disability', fill='fail_final')\n    + geom_bar(position='fill')\n)","metadata":{"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, people with disabilities do worse academically. This is to be expected as disabled students would face more challenges due to ill health.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"regression\"></a>\n# 11. Regression\n***","metadata":{}},{"cell_type":"markdown","source":"## 11.1 Model preparation","metadata":{}},{"cell_type":"code","source":"# Separate features from target\n\n'''Training set'''\n# Drop target and helper columns\nX_train = train.drop(columns=['fail_final', 'weighted_score'])\n# Make an array with target\nY_train = train['weighted_score'].copy()\n\n'''Test set'''\n# Drop target column\nX_test = test.drop(columns=['weighted_score'])\n# Make an array with target\nY_test = test['weighted_score'].copy()\n\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Make a copy for the subsequent last evaluation'''\nX_train_eval = X_train.copy()\nY_train_eval = Y_train.copy()\nX_test_eval = X_test.copy()\nY_test_eval = Y_test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.compose import make_column_transformer\n\n# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'age_band', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band']),\n    (RobustScaler(), ['date_registration', 'module_presentation_length',\n                       'num_of_prev_attempts', 'studied_credits', 'total_click', 'late_rate',\n                       'fail_rate'])\n)\n\n# Apply column transformer to features\nX_encoded = column_transform.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RobustScaler is used to make the models more robust to putliers. More specifically, RobustScaler  scales the data according to the interquartile range.","metadata":{}},{"cell_type":"code","source":"# Have a look at what the scaled and encoded data looks like\npd.DataFrame(X_encoded).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11.2. Models:","metadata":{}},{"cell_type":"markdown","source":"### Linear Regression","metadata":{}},{"cell_type":"markdown","source":"Linear regression is most likely going to be a bad model as the data breaks several of the assumptions of this model.\n\n**The assumptions are as follows**:\n* Linear relationship between the target and features.\n    * The pair plots show this isn't the case.\n* Multivariate normality - all variables must be normal.\n    * The histograms of the numerical variables show that their distributions aren't normal.\n* Little to no multicollinearity - all variables must be independent from each other.\n    * The correlation matrix shows that this isn't so.\n* No auto-correlation - when the value of y(x+1) is independent from the value of y(x).\n* Homoscedasticity - residuals must be equal along the regression line.\n\nLinear regression can show us an example of a bad model, showing how other models can vastly improve the predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Setting up the pipeline\nlm = LinearRegression()\n\nlm_pipeline = make_pipeline(column_transform, lm)\n\n# Fit the training data\nlm_pipeline.fit(X_train, Y_train)\n# Predict the training data\nlm_train_predictions = lm_pipeline.predict(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's evaluate the model\nimport sklearn.metrics as metrics\n\ndef regression_eval(X, y, predictions):\n    MSE = metrics.mean_squared_error(y, predictions)\n    RMSE = np.sqrt(MSE)\n    R2 = metrics.r2_score(y, predictions)\n    adj_R2 = 1 - ( (1-R2)*(len(y)-1)/(len(y)-X.shape[1]-1) )\n\n    print(\"-----------------------\")\n    print('RMSE is {}'.format(RMSE))\n    print('Adjusted R2 score is {}\\n'.format(adj_R2))\n\n### For training set ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, lm_train_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above evaluation of the model is using our uses a randomised training and test set split, then calculates RMSE and adjusted-R2. Adjusted-R2 shows the model explains 35% of the total variance in the sample. RMSE of 23.8 shows us the error - predictions are off by 23.8 points. This is a large error considering the fail mark is only 40%.","metadata":{}},{"cell_type":"code","source":"# Perform cross-validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(lm, column_transform.fit_transform(X_train), Y_train, cv=10, scoring='neg_mean_squared_error')\nlm_rmse_scores = np.sqrt(-scores)\n\ndef display_scores(scores):\n    print('Scores\\t:', scores)\n    print('Mean\\t:', scores.mean())\n    print('SD\\t:', scores.std())\n    \ndisplay_scores(lm_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross-validation uses stratified k-fold cross-validation which is different from validation with randomised values. The training set is split into a smaller training set and an even smaller validation set. Each of these sets are then used for training and validation sequentially. Cross-validation also shows us the model performs abysmally, just as expected.","metadata":{}},{"cell_type":"markdown","source":"### LASSO Regression","metadata":{}},{"cell_type":"markdown","source":"LASSO (least absolute shrinkage and selection operator) regression is a modification of linear regression. In very simple terms, this algorithm can drop some features based on those features' coefficients (if they are too low).\n\nThe assumptions of this model as the same as for the linear model, except normality is not assumed.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha =0.0005, random_state=1)\n\nlasso_pipeline = make_pipeline(column_transform, lasso)\n\nlasso_pipeline.fit(X_train, Y_train)\n\nX_lasso_predictions = lasso_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, X_lasso_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform cross-validation\nscores = cross_val_score(lasso, column_transform.fit_transform(X_train), Y_train, cv=10, scoring='neg_mean_squared_error')\n\nlasso_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(lasso_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model was not in any way an improvement.","metadata":{}},{"cell_type":"markdown","source":"### Support Vector Regression","metadata":{}},{"cell_type":"markdown","source":"Linear models are doing badly, as expected. We will next use non-linear models and see if our predictions improve.\n\nSupport Vector Regression seeks not to minimise the squared error as in the linear regression, but to minimise coefficients.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nSVR = SVR(kernel='rbf')\n\nSVR_pipeline = make_pipeline(column_transform, SVR)\n\nSVR_pipeline.fit(X_train, Y_train)\n\ntrain_SVR_predictions = SVR_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_SVR_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(SVR, column_transform.fit_transform(X_train), Y_train, cv=4, scoring='neg_mean_squared_error')\n\nSVR_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(SVR_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is an improvement on the linear models. Adjusted R2 is a bit higher (39%) and the standard error for RMSE scores in the cross-validated sets is lower (from SD = 0.31 for LASSO to SD = 0.09). It is still not a great predictor for the dataset.","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"markdown","source":"This model uses a very different approach - building a what is essentially a flow chart based on probabilities and likelihoods. It is a simple algorithm with many models using it as a base algorithm (e.g. Random Forest).","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nDtree = DecisionTreeRegressor(min_samples_leaf=15, min_samples_split=10, max_features=13)\n\nDtree_pipeline = make_pipeline(column_transform, Dtree)\n\nDtree_pipeline.fit(X_train, Y_train)\n\ntrain_Dtree_predictions = Dtree_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_Dtree_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(Dtree, column_transform.fit_transform(X_train), Y_train, cv=5, scoring='neg_mean_squared_error')\n\nDtree_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(Dtree_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are much better than the linear models or SVR. Adjusted-R2 is 0.66% and cross-validation shows RMSE of 19. Interestingly, even though the model is capable of explaining significantly more variance, RMSE is not improved drastically.","metadata":{}},{"cell_type":"markdown","source":"### GradientBoost","metadata":{}},{"cell_type":"markdown","source":"This is a predictive model using an ensemble of weak predictive models (decision trees). We expect it will perform better than a simple Decision Tree.\n\nThe model is trained with huber loss, making it more robust to outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nGBoost = GradientBoostingRegressor(n_estimators=400, learning_rate=0.05,\n                                   max_depth=4, max_features=13,\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost_pipeline = make_pipeline(column_transform, GBoost)\n\nGBoost_pipeline.fit(X_train, Y_train)\n\ntrain_GBoost_predictions = GBoost_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_GBoost_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(GBoost, column_transform.fit_transform(X_train), Y_train, cv=4, scoring='neg_mean_squared_error')\n\nGBoost_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(GBoost_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This may be a small improvement. Further tests needed to determine if the results of this model are significantly different than a simple Decision Tree. RMSE is 18.4 based on cross-validation, which is lower than we've seen before. Adjusdted R2 score is one point lower (65%).","metadata":{}},{"cell_type":"markdown","source":"### K Nearest Neighbours Regression","metadata":{}},{"cell_type":"markdown","source":"KNN-Regression examines the point close to the target point and then makes a prediction on which class these data points belong to.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nKNReg = KNeighborsRegressor(n_neighbors=2)\n\nKNReg_pipeline = make_pipeline(column_transform, KNReg)\n\nKNReg_pipeline.fit(X_train, Y_train)\n\ntrain_KNReg_predictions = KNReg_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_KNReg_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE is the lowest yet at 16.9. Let's see how the model fairs with cross-validation.","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(KNReg, column_transform.fit_transform(X_train), Y_train, cv=5, scoring='neg_mean_squared_error')\n\nKNReg_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(KNReg_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Mean RMSE is at 29.1 which is the worst performance among all non-linear models. Validation error is expected to be higher than training error, but this differenc is quite stark. This means the model is overfitting badly.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nRForest = RandomForestRegressor(min_samples_leaf=15, min_samples_split=10,\n                                max_features=13, n_estimators=20)\n\nRForest_pipeline = make_pipeline(column_transform, RForest)\n\nRForest_pipeline.fit(X_train, Y_train)\n\ntrain_RForest_predictions = RForest_pipeline.predict(X_train)\n\n### Evaluating the model ###\nprint(\"Model performance for training set:\")\nregression_eval(X_train, Y_train, train_RForest_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(RForest, column_transform.fit_transform(X_train), Y_train, cv=5, scoring='neg_mean_squared_error')\n\nRForest_rmse_scores = np.sqrt(-scores)\n    \ndisplay_scores(RForest_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model looks great. Training error (RMSE = 17.0) is not much lower than validation error (mean RMSE = 18.6, SD = 0.2). Let's compare all non-linear models below and chose the best performing one.","metadata":{}},{"cell_type":"markdown","source":"## 11.3 Best Regression Model - evaluation","metadata":{}},{"cell_type":"markdown","source":"Let's chose the best model. To do this we need to have a look at how the model performed on the training set and in cross-validation.\n\n\n**On the training set:**\n\n|Models| RMSE score|\n| ----------- | ----------- |\n|**SVR**|23.1|\n|**DT**|17.3|\n|**GB**|17.7|\n|**KNN**|16.9|\n|**RF**|17.0|\n\n**Cross-validation:**\n\n|Models| Mean RMSE|SD|\n| ----------- | ----------- |-------|\n|**SVR**|23.4|0.1|\n|**DT**|20.4|0.5|\n|**GB**|18.4|0.2|\n|**KNN**|29.1|0.3|\n|**RF**|18.6|0.2|\n","metadata":{}},{"cell_type":"markdown","source":"GradientBoost and Random Forest are our best models. Cross-validation for the GBoost model shows it to be the most accurate model, even though the error for the training set without cross-validation for the GBoost is higher than for the RF. KNN had the lowest RMSE score for the training set without cross-validation, but we can see how this model's performance degraded in the cross-validation, meaning this model is overfitting.","metadata":{}},{"cell_type":"code","source":"# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'age_band', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band']),\n    remainder='passthrough')\n    \nGBoost = GradientBoostingRegressor(n_estimators=400, learning_rate=0.05,\n                                   max_depth=4, max_features=13,\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nGBoost_pipeline = make_pipeline(column_transform, GBoost)\n\n# Fit the training data\nGBoost_pipeline.fit(X_train_eval, Y_train_eval)\n\n# Transform the test set (don't fit)\nX_prepared_eval = column_transform.transform(X_test_eval)\n# Predict the test data\ntest_GBoost_predictions = GBoost.predict(X_prepared_eval)\n\nregression_eval(X_test_eval, Y_test_eval, test_GBoost_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is our final regression model with RMSE = 18.04 and adjusted R2 = 63%.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"classification\"></a>\n# 12. Classification\n***","metadata":{}},{"cell_type":"markdown","source":"Next, we approach the task as a classification problem. Same steps as with the regression apply - model is prepared with last cleaning steps, then categorical values are encoded, models are fitted and then evaluated.","metadata":{}},{"cell_type":"markdown","source":"## 12.1 Model preparation","metadata":{}},{"cell_type":"code","source":"train = train_class.copy()\ntest = test_class.copy()\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Last cleaning","metadata":{}},{"cell_type":"code","source":"# Distinction as a Pass\ntrain['final_result'] = np.where( (train['final_result'] == 'Distinction'),\n                                           'Pass',\n                                           train['final_result']\n                                    )\n# Withdrawn as a Fail (to make the target binary)\ntrain['final_result'] = np.where( (train['final_result'] == 'Withdrawn'),\n                                           'Fail',\n                                           train['final_result']\n                                    )\n# Same for test set\ntest['final_result'] = np.where( (test['final_result'] == 'Distinction'),\n                                           'Pass',\n                                           test['final_result']\n                                    )\ntest['final_result'] = np.where( (test['final_result'] == 'Withdrawn'),\n                                           'Fail',\n                                           test['final_result']\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename 'no formal quals' into 'lower than a level'\ntrain['highest_education'] = np.where( (train['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           train['highest_education']\n                                    )\n\n# Rename post-grads\ntrain['highest_education'] = np.where( (train['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           train['highest_education']\n                                    )\n\n\n# Do the same for the test set\ntest['highest_education'] = np.where( (test['highest_education'] == 'No Formal quals'),\n                                           'Lower Than A Level',\n                                           test['highest_education']\n                                    )\n\ntest['highest_education'] = np.where( (test['highest_education'] == 'Post Graduate Qualification'),\n                                           'HE Qualification',\n                                           test['highest_education']\n                                    )\n### Age bands ###\ntrain['age_band'] = np.where( (train['age_band'] == '55<='),\n                                           '35-55',\n                                           train['age_band']\n                                    )\n\ntrain['age_band'] = np.where( (train['age_band'] == '35-55'),\n                                           '35+',\n                                           train['age_band']\n                                    )\n\n# Do the same for the test set\ntest['age_band'] = np.where( (test['age_band'] == '55<='),\n                                           '35-55',\n                                           test['age_band']\n                                    )\n\ntest['age_band'] = np.where( (test['age_band'] == '35-55'),\n                                           '35+',\n                                           test['age_band']\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate features from target\n\n'''Training set'''\n# Drop target column\nX_train = train.drop(columns=['final_result'])\n# Make an array with target\nY_train = train['final_result'].copy()\n\n'''Test set'''\n# Drop target column\nX_test = test.drop(columns=['final_result'])\n# Make an array with target\nY_test = test['final_result'].copy()\n\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding for trees","metadata":{}},{"cell_type":"markdown","source":"Encoding for tree models does not require scaling.    ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import make_column_transformer\n\n# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band', 'age_band']),\n    remainder='passthrough'\n)\n\n# Apply column transformer to features\nX_encoded = column_transform.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X_encoded).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12.2. Models:","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nDtree = DecisionTreeClassifier(min_samples_leaf=15, min_samples_split=10, max_features=8)\n\nDtree_pipeline = make_pipeline(column_transform, Dtree)\n\n# Cross-validate\ndef display_accuracy_scores(pipeline, X, Y):\n    scores = cross_val_score(pipeline, X, Y, cv=5, scoring='accuracy')\n    print('Scores\\t:', scores)\n    print('Mean\\t:', scores.mean())\n    print('SD\\t:', scores.std())\n\n### Cross-validate ###\n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(Dtree_pipeline, X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nRForest = RandomForestClassifier(min_samples_leaf=15, min_samples_split=10,\n                                max_features=8, n_estimators=20)\n\nRForest_pipeline = make_pipeline(column_transform, RForest)\n\n### Cross-validate ###\n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(RForest_pipeline, X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machine","metadata":{}},{"cell_type":"markdown","source":"Encoding for the last two models requires scaling.","metadata":{}},{"cell_type":"code","source":"# Set encoding and scaling instructions\ncolumn_transform = make_column_transformer(\n    (OneHotEncoder(), ['code_module', 'code_presentation', 'gender', 'region', 'disability']),\n    (OrdinalEncoder(), ['highest_education', 'imd_band', 'age_band']),\n    (RobustScaler(), ['date_registration', 'module_presentation_length',\n                       'num_of_prev_attempts', 'studied_credits', 'total_click'])\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nSVClass = SVC(gamma='auto')\n\nSVClass_pipeline = make_pipeline(column_transform, SVClass)\n\n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(SVClass_pipeline, X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SGD","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nSDG = SGDClassifier(max_iter=1000, tol=1e-3)\n\nSDG_pipeline = make_pipeline(column_transform, SDG)\n    \n# Train set\nprint('Evaluation of the training set')\ndisplay_accuracy_scores(SDG_pipeline, X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12.3 Best Classification Model - evaluation ","metadata":{}},{"cell_type":"markdown","source":"Support Vector Machine classifier model performed the best. Althrough its accuracy scores (0.78, SD = 0.002) are similar to the RF model (0.79, SD = 0.004), the SVC model performs with slightly less variance between the scores during cross-validation as shown by lower standard deviation.","metadata":{}},{"cell_type":"code","source":"# Test set evaluation for SVC\nprint('Evaluation of the test set')\n\n# Fit the training data\nSVClass_pipeline.fit(X_train, Y_train)\n# Transform the test data\nX_test_prepared = column_transform.transform(X_test)\n# Predict the test data\nSVClass_predictions_test = SVClass.predict(X_test_prepared)\n\nprint('Accuracy score:', metrics.accuracy_score(Y_test, SVClass_predictions_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy score is almost the same for both the training and the test sets.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"discussion\"></a>\n# 13. Discussion\n***","metadata":{}},{"cell_type":"markdown","source":"To quickly summarise, the best model for the regression task was Gradient Boost (RMSE = 18.4, SD = 0.2 with 4-fold cross-validation) which gave us RMSE = 18.1 and adjusted R2 = 0.63 when evaluated on the test set). This is without any fine-tuning of the hyperparameters. The best model for the classification task was a Support Vector Machine classifier (0.78% accuracy score on the test set). Again, this is without any hyperparameter tuning.","metadata":{}},{"cell_type":"markdown","source":"Next steps to take would be to find which features are most important and which can be dropped. Hyperparameter tuning can be used to find the best set of parameters for the models. Various dimensionality reduction tools can be used to improve the performance of the models. Another point to make is that accuracy score isn't the best way to evaluate classification models, especially when the target is imbalanced. We have an imbalanced target for this classification problem, so dealing with this imbalance and using a different evaluation metric would be advantageous.\n\nIt's possible to engineer some more features too, for example, we know VLE interactions are important for student success, but maybe the type of the resource the student interacts with will be a better signal than total clicks for all resources?","metadata":{}}]}