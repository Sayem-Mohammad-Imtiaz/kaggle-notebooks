{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Abbreviation Disambiguation in Medical Texts - Data Wrangling & EDA\n\nThis Notebook is used to list down:\n\n1. How the Data for Test and Train sets was collected i.e, Data Sources.\n2. How the data has been organized for Training and Testing the model.\n3. Data Cleaning.\n4. Exploratory Data Analysis (EDA)."},{"metadata":{},"cell_type":"markdown","source":"## Step# 1: Downloading Data from Data Sources.\n\n            Data being used in this problem will be downloaded using Kaggle API from the below location:\n\n            https://www.kaggle.com/xhlulu/medal-emnlp"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Uncomment below lines to download the dataset directly from Kaggle\n## To Install Kaggle package in case it is not already present\n# !pip install kaggle\n## Download dataset\n# !kaggle datasets download -d xhlulu/medal-emnlp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Once the dataset has been downloaded lets check the directory contents"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Importing the Required Python Packages\nimport os\nimport shutil\nimport zipfile\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Printing the Current Working Directory"},{"metadata":{"tags":[],"trusted":false},"cell_type":"code","source":"print(os.getcwd())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step# 2: Organizing data into Logical folder structure from where the system will read the data and process it.\n"},{"metadata":{},"cell_type":"markdown","source":"    The Data for this problem will have the following Folder structure:\n\n    Work_Dir\n    |\n    |\n    |----> Data\n    |\n    |\n    |----> Train\n    |\n    |\n    |----> Test\n    |\n    |\n    |----> Validation\n    |\n    |\n    |----> Images\n    \n    Thus, creating the required Folder structure."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Reading the current path in a variable\npath = os.getcwd()\nprint(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a folder by name Train\nos.mkdir(os.path.join(path, 'Train'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a folder by name Test\nos.mkdir(os.path.join(path, 'Test'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a folder by name Data\nos.mkdir(os.path.join(path, 'Data'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a folder by name Validation\nos.mkdir(os.path.join(path, 'Validation'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a folder by name Images\nos.mkdir(os.path.join(path, 'Images'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moving the NLP_Dataset.zip file into the Data folder and then unzipping the file"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Moving the file to Data folder\nsource = os.path.join(path, 'NLP_Dataset.zip')\ndestination = os.path.join(path, 'Data', 'NLP_Dataset.zip')\nshutil.move(source, destination)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check the contents of Data folder\nprint(os.listdir(os.path.join(path, 'Data')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Unzip the dataset file\nwith zipfile.ZipFile(destination, 'r') as zip_ref:\n    zip_ref.extractall(os.path.join(path, 'Data'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check the contents of Data folder\nprint(os.listdir(os.path.join(path, 'Data')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data was successfully extracted and a new folder 'pretrain_subset' was also created, Let's check the contents of that folder."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check the contents of pretrain_subset folder\nprint(os.listdir(os.path.join(path, 'Data', 'pretrain_subset')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretrain_subset folder contains the dataset already divided into 3 different files- Train, test and valid. We will use the the data in this folder due to system memory restrictions."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Deleting full_data.csv and moving train, test and valid files to Data folder.\nsource = os.path.join(path, 'Data', 'pretrain_subset')\ndestination = os.path.join(path, 'Data')\nshutil.move(os.path.join(source, 'train.csv'), os.path.join(destination, 'train.csv'))\nshutil.move(os.path.join(source, 'test.csv'), os.path.join(destination, 'test.csv'))\nshutil.move(os.path.join(source, 'valid.csv'), os.path.join(destination, 'valid.csv'))\nos.remove(os.path.join(destination, 'full_data.csv'))\n\n#Check the contents of Data folder\nprint(os.listdir(os.path.join(path, 'Data')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step# 3: Cleaning Data"},{"metadata":{},"cell_type":"markdown","source":"### Lets have a look at our data."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a path variable directly to the dataset\ndata_path = os.path.join(path, 'Data', 'train.csv')\nprint(data_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Loading the data in a dataframe.\ntextDF = pd.read_csv(data_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking the shape of Dataframe\ntextDF.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Data contains 3 million rows but my System won't be able to work with such a huge dataset hence, will take the first 1 Million rows only for this project."},{"metadata":{"trusted":false},"cell_type":"code","source":"textDF.drop(textDF.index[1000000:], inplace = True)\ntextDF.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking the first 5 rows of the dataframe\ntextDF.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking last 5 rows of the dataframe\ntextDF.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking the summary statistics of the dataframe\ntextDF.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking the datatypes of dataframe columns\ntextDF.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking the unique Abstract_id to see if Abstract_id can be converted to Index\ntextDF['ABSTRACT_ID'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, the Abstract_ID's are not all unique. So, lets check the duplicates in the dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"duplicate = textDF[textDF.duplicated()]\nduplicate.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, none of the rows are duplicates."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets check for null values if any\ntextDF.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, we don't have any Null values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"### At this point the Data Wrangling has been completed and the resulting data is now ready for EDA"},{"metadata":{},"cell_type":"markdown","source":"## Step# 4: EDA"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets look at one row of the dataset in detail\npd.set_option('display.max_colwidth', -1)\ntextDF.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As per dataset specifications, location column signifies the word count after which the Abbreviation occurs and its Label is provided in Lable column."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets check the Abbreviations of first 10 rows of the dataset alongwith their labels\nsplit_text = [ t.split(' ') for t in textDF[:10]['TEXT']]\nlabel = [t for t in textDF[:10]['LABEL']]\nlocation = [t for t in textDF[:10]['LOCATION']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in range(0,10):\n    print(label[i], ' -- ', split_text[i][location[i]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above analysis, the relationship between Location, Label and Text columns are clearly visible. "},{"metadata":{},"cell_type":"markdown","source":"### Let us again check the number of unique ABSTRACT_ID in Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking the unique Abstract_id\ntextDF['ABSTRACT_ID'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking the shape of the Dataset\ntextDF.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen here that there are some Abstract_ID's which are not unique. Lets find those abstracts and check what is the main differences"},{"metadata":{"trusted":false},"cell_type":"code","source":"duplicate = textDF[textDF['ABSTRACT_ID'].duplicated(keep = False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"duplicate.sort_values(by = ['ABSTRACT_ID']).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So, based on the above results for duplicates, it can be seen that a single Text might contain more than 1 Abbreviation at diffrent places. Thus, multiple row for multiple Abbreviations are present."},{"metadata":{},"cell_type":"markdown","source":"### Let's save the above trainDF in a csv file inside Train folder for further use."},{"metadata":{"trusted":false},"cell_type":"code","source":"textDF.to_csv('Train/train.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets load the valid.csv and test.csv as well"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Loading valid.csv\nvalid = pd.read_csv(os.path.join(path, 'Data', 'valid.csv'))\n#Loading test.csv\ntest = pd.read_csv(os.path.join(path, 'Data', 'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the shape of valid dataset\nvalid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Data contains 1 million rows so lets reduce this data to 20% of train data i.e, 20k records."},{"metadata":{"trusted":false},"cell_type":"code","source":"valid.drop(valid.index[200000:], inplace = True)\nvalid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Save this updated Valid.csv to Validation folder\nvalid.to_csv('Validation/valid.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the shape of test dataset\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Data contains 1 million rows so lets reduce this data to 20% of train data i.e, 20k records."},{"metadata":{"trusted":false},"cell_type":"code","source":"test.drop(test.index[200000:], inplace = True)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Save this updated test.csv to Test folder\ntest.to_csv('Test/test.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}