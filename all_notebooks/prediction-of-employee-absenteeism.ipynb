{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Below you can find the code that builds up to the ‘df_reason_mod’ checkpoint.\n# Additionally, in the comments you can see the code that we ran in the lectures to check \n# the current state of a specific object while explaining various programming or data analytics concepts. \n\nraw_csv_data = pd.read_csv(\"../input/Absenteeism-data.csv\")\n\n# type(raw_csv_data)\n# raw_csv_data\n\ndf = raw_csv_data.copy()\n# df\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\ndisplay(df)\n# df.info()\n\n\n\n########## Drop 'ID': ############################\n##################################################\n\n\n# df.drop(['ID'])\n# df.drop(['ID'], axis = 1)\ndf = df.drop(['ID'], axis = 1)\n\n# df\n# raw_csv_data\n\n\n\n########## 'Reason for Absence' ##################\n##################################################\n\n\n# df['Reason for Absence'].min()\n# df['Reason for Absence'].max()\n# pd.unique(df['Reason for Absence'])\n# df['Reason for Absence'].unique()\n# len(df['Reason for Absence'].unique())\n# sorted(df['Reason for Absence'].unique())\n\n\n\n########## '.get_dummies()' an dropping the Reason 0 ######################\n##################################################\n\n\nreason_columns = pd.get_dummies(df['Reason for Absence'])\nreason_columns\n\nreason_columns['check'] = reason_columns.sum(axis=1)\nreason_columns\n\nreason_columns['check'].sum(axis=0)\n# reason_columns['check'].unique()\n\nreason_columns = reason_columns.drop(['check'], axis = 1)\n# reason_columns\n\nreason_columns = pd.get_dummies(df['Reason for Absence'], drop_first = True)\n# reason_columns\n\n\n\n########## Group the Reasons for Absence##########\n##################################################\n\n\n# df.columns.values\n# reason_columns.columns.values\ndf = df.drop(['Reason for Absence'], axis = 1)    #coz we have seperated this column as 'reason_column'\n# df\n\n# reason_columns.loc[:, 1:14].max(axis=1)\nreason_type_1 = reason_columns.loc[:, 1:14].max(axis=1)\nreason_type_2 = reason_columns.loc[:, 15:17].max(axis=1)\nreason_type_3 = reason_columns.loc[:, 18:21].max(axis=1)\nreason_type_4 = reason_columns.loc[:, 22:].max(axis=1)\n\n#reason_type_1\n# reason_type_2\n# reason_type_3\n# reason_type_4\n\n\n\n########## Concatenate Column Values #############\n##################################################\n\n\n# df\n\ndf = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis = 1)\n#df\n\n# df.columns.values\ncolumn_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4']\n\ndf.columns = column_names\ndf.head()\n\n\n\n########## Reorder Columns #######################\n##################################################\n\n\ncolumn_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', \n                          'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours']\n\ndf = df[column_names_reordered]\ndf.head()\n\n\n\n########## Create a Checkpoint 1 ###################\n##################################################\n\n\ndf_reason_mod = df.copy()\n# df_reason_mod\n\n\n\n########## 'Date' ################################\n##################################################\n\n\n# df_reason_mod['Date']         # day/month/year\n#df_reason_mod['Date'][0]\n#type(df_reason_mod['Date'][0])   #Type: string\n\n\n# df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'])\n# df_reason_mod['Date']\n\ndf_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format = '%d/%m/%Y')\n#df_reason_mod['Date']\n#type(df_reason_mod['Date'][0])       #pandas._libs.tslibs.timestamps.Timestamp\n# df_reason_mod.info()\n\n\n\n########## Extract the Month Value ############### (Jan-Dec: 1-12)\n##################################################\n\n\n#df_reason_mod['Date'][0]               #Timestamp('2015-07-07 00:00:00')\n#df_reason_mod['Date'][0].month         # 7\n\nlist_months = []\n# list_months\n\n# df_reason_mod.shape\n\nfor i in range(df_reason_mod.shape[0]):\n    list_months.append(df_reason_mod['Date'][i].month)\n    \n# list_months\n#len(list_months)     # 700\ndf_reason_mod['Month Value'] = list_months\n#df_reason_mod.head(20)\n\n\n\n########## Extract the Day of the week ###############  (Mon-Tues: 0-6)\n##################################################\n\n\n#df_reason_mod['Date'][699].weekday()   #3 i.e. Thursday\n#df_reason_mod['Date'][699]             #Timestamp('2018-05-31 00:00:00')\n\ndef date_to_weekday(date_value):\n    return date_value.weekday()\n\ndf_reason_mod['Day of the Week'] = df_reason_mod['Date'].apply(date_to_weekday)\n\ndf_reason_mod.head()\n\n\n\n########## Remove the Date column & Reorder Month Value and Day of the Week at same place where date column was##############################\n###################\n\n\ndf_reason_mod = df_reason_mod.drop(['Date'], axis = 1)\n# df_reason_mod.head()\n# df_reason_mod.columns.values\ncolumn_names_upd = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month Value', 'Day of the Week',\n       'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education', 'Children',\n       'Pets', 'Absenteeism Time in Hours']\ndf_reason_mod = df_reason_mod[column_names_upd]\ndf_reason_mod.head()\n\n\n\n########## Create a Checkpoint 2 ###################\n##################################################\n\n\ndf_reason_date_mod = df_reason_mod.copy()\ndf_reason_date_mod\n\n########## Analyzing other columns ###################\n##################################################\n\ntype(df_reason_date_mod['Transportation Expense'][0])\ntype(df_reason_date_mod['Distance to Work'][0])\ntype(df_reason_date_mod['Age'][0])\ntype(df_reason_date_mod['Daily Work Load Average'][0])\ntype(df_reason_date_mod['Body Mass Index'][0])    \n     \n\n########## Working on \"Education\", \"Children\", \"Pets\" ###################\n##################################################\n\ndf_reason_date_mod['Education'].unique()\ndf_reason_date_mod['Education'].value_counts()\ndf_reason_date_mod['Education'] = df_reason_date_mod['Education'].map({1:0, 2:1, 3:1, 4:1})\ndf_reason_date_mod['Education'].unique()\ndf_reason_date_mod['Education'].value_counts()\n     \n     \n########## Final Checkpoint ###################\n##################################################  \n     \ndf_preprocessed = df_reason_date_mod.copy()\ndf_preprocessed.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a logistic regression to predict absenteeism**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_preprocessed = df_preprocessed\ndata_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_preprocessed['Absenteeism Time in Hours'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > \n                   data_preprocessed['Absenteeism Time in Hours'].median(), 1, 0)\ntargets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_preprocessed['Excessive Absenteeism'] = targets\ndata_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A comment on the targets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.sum() / targets.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours','Day of the Week',\n                                            'Daily Work Load Average','Distance to Work'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_targets is data_preprocessed\ndata_with_targets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Select the inputs for the regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_targets.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data_with_targets.iloc[:,:14]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"data_with_targets.iloc[:,:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs = data_with_targets.iloc[:,:-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardize the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nabsenteeism_scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n\n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n\n    def transform(self, X, y=None, copy=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columns_to_scale = ['Month Value','Day of the Week', 'Transportation Expense', 'Distance to Work',\n       #'Age', 'Daily Work Load Average', 'Body Mass Index', 'Children', 'Pet']\n\ncolumns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absenteeism_scaler = CustomScaler(columns_to_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absenteeism_scaler.fit(unscaled_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)\nscaled_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_inputs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the data into train & test and shuffle**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_split(scaled_inputs, targets)\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, #train_size = 0.8, \n                                                                            test_size = 0.2, random_state = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (x_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (x_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic regression with sklearn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LogisticRegression()\nreg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Manually check the accuracy**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model_outputs = reg.predict(x_train)\nmodel_outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_outputs == y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum((model_outputs==y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_outputs.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum((model_outputs==y_train)) / model_outputs.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding the intercept and coefficients**"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_name = unscaled_inputs.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table = pd.DataFrame (columns=['Feature name'], data = feature_name)\n\nsummary_table['Coefficient'] = np.transpose(reg.coef_)\n\nsummary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg.intercept_[0]]\nsummary_table = summary_table.sort_index()\nsummary_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Interpreting the coefficients**"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table['Odds_ratio'] = np.exp(summary_table.Coefficient)\nsummary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table.sort_values('Odds_ratio', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predicted_proba = reg.predict_proba(x_test)\npredicted_proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_proba.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predicted_proba[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Save the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open('model', 'wb') as file:\n    pickle.dump(reg, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('scaler','wb') as file:\n    pickle.dump(absenteeism_scaler, file)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}