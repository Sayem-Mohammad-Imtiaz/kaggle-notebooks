{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# File Description\nThe CSV file contains data for time period from Jan 2012 to March 2021 with minute by minute reportings of OHLC (open, high, low, close) and volume. There are missing value, bacause the exchange (or its API) was down or did not exist.\n\n# Aim\nPredict the closing price of bitcoin looking at the market trend.","metadata":{}},{"cell_type":"markdown","source":"# Import File","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas\nimport seaborn\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bit_df = pandas.read_csv('../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bit_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate\ninfo = [[col, bit_df[col].count(), bit_df[col].max(), bit_df[col].min()] for col in bit_df.columns]\nprint(tabulate(info, headers = ['Feature', 'Count', 'Max', 'Min'], tablefmt = 'orgtbl'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and Data Wrangling","metadata":{}},{"cell_type":"code","source":"print(bit_df.isna().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more than **1 million** unrecorded timestamps.","metadata":{}},{"cell_type":"code","source":"bit_df = bit_df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total missing values : ' + str(bit_df.isna().sum().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NOTE\nAs it can be observed, the timestamps are **no longer equally distributed** after removing the Nan values. Since this is a time series data and we are taking previous performance into account, if there are large missing chunks in between the model may get a wrong impression of the ongoing trend.","metadata":{}},{"cell_type":"markdown","source":"### So only a part of latest available data will be used for prediction.","metadata":{}},{"cell_type":"code","source":"bit_df = bit_df[bit_df['Timestamp'] > (bit_df['Timestamp'].max()-650000)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bit_df = bit_df.reset_index(drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bit_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bit_df.hist(figsize = (15,15))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation\nLet's look for correlation between the data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nmat = bit_df.corr()\nseaborn.heatmap(mat, vmin = -1.0, annot = True, square = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Open, High, Low, Close and Weighted_Price** are all highly correlated, so either one of them can be used as a feature. One of either **Volume_(BTC) or Volume_(Currency)** will be the second feature.","metadata":{}},{"cell_type":"code","source":"bit_df = bit_df.drop(['Timestamp', 'Low', 'High', 'Volume_(BTC)', 'Weighted_Price'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info = [[col, bit_df[col].count(), bit_df[col].max(), bit_df[col].min()] for col in bit_df.columns]\nprint(tabulate(info, headers = ['Feature', 'Count', 'Max', 'Min'], tablefmt = 'orgtbl'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualization (recent trends)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,10))\nplt.subplot(2,1,1)\nplt.plot(bit_df['Open'].values[bit_df.shape[0]-500:bit_df.shape[0]])\nplt.xlabel('Time period')\nplt.ylabel('Opening price')\nplt.title('Opening price of Bitcoin for last 500 timestamps')\n\nplt.subplot(2,1,2)\nplt.plot(bit_df['Volume_(Currency)'].values[bit_df.shape[0]-500:bit_df.shape[0]])\nplt.xlabel('Time period')\nplt.ylabel('Volume Traded')\nplt.title('Volume traded of Bitcoin for last 500 timestamps')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note\nOne things for sure cryptocurrency are very volatile, as they are not regulated by any single authority.","metadata":{}},{"cell_type":"markdown","source":"# Create the arrays","metadata":{}},{"cell_type":"code","source":"bit_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(bit_df.drop(['Close'], axis = 1))\ny = np.array(bit_df['Close'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling the data\nWe will normalize the data to remove the mean and have a unit variance using **StandardScaler()** from sklearn","metadata":{}},{"cell_type":"code","source":"print(X.max())\nprint(X.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y.max())\nprint(y.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = np.reshape(y, (-1,1))\ny = StandardScaler().fit_transform(t)\ny = y.reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.max())\nprint(X.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y.max())\nprint(y.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating time series datasets\nConsidering past **500** timestamps, approximately equal to 8 hours, performance.","metadata":{}},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length = 500\nX_temp = []\ny_temp = []\nfor i in range(length,X.shape[0]) :\n    X_temp.append(X[i-length: i])\n    y_temp.append(y[i])\nX_temp = np.array(X_temp)\ny_temp = np.array(y_temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_temp.shape)\nprint(y_temp.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size = 0.2, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models (RNN vs LSTM)","metadata":{}},{"cell_type":"markdown","source":"### SimpleRNN\nSimpleRNN layer in keras is how a vanilla RNN model is implemented. It has only one tanh layer which takes in the previous hidden state and input, and computes new Output and hidden state. The figure depicts a SimpleRNN layer,\n\n<center><img src = \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" alt = \"simplernn\" width = \"700\"/></center>","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import SimpleRNN\nfrom keras.layers import BatchNormalization\n\nfrom keras.layers import Input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def simp_layer (hidden1) :\n    \n    model = Sequential()\n    \n    # add input layer\n    model.add(Input(shape = (500, 2, )))\n    \n    # add rnn layer\n    model.add(SimpleRNN(hidden1, activation = 'tanh', return_sequences = False))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    # add output layer\n    model.add(Dense(1, activation = 'linear'))\n    \n    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = simp_layer(10)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\ncheckp = ModelCheckpoint('./bit_model.h5', monitor = 'val_loss', save_best_only = True, verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nbeg = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test, y_test), callbacks = [checkp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('./bit_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pred.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pred.reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nprint('MSE : ' + str(mean_squared_error(y_test, pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,7))\nplt.plot(y_test[2040:2060])\nplt.plot(pred[2040:2060])\nplt.xlabel('Time')\nplt.ylabel('Price')\nplt.title('Closing Price vs Time (using SimpleRNN)')\nplt.legend(['Actual price', 'Predicted price'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Time taken for SimpleRNN model to learn : ' + str(end-beg) + ' sec.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM\nA simple RNN suffers from the problem of **vanishing gradient**, where it becomes hard to keep the past information and the model might fail. The LSTM handles this problem using gates as shown in the figure below. These gates decide which info to retain, which new info to add and what to output.\n\n<center><img src = \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt = \"lstm\" width = 700/></center>\n\n### Forget gate layer\nThe first portion is **forget gate layer** which has a sigmoid function generating the fraction of previous data to retain from the **cell state**. Cell state is continueous stream of past states.\n\n### Input gate layer\nThis layer has two gates, a tanh layer that outputs new info to add and a sigmoid layer that decides what proportion of these info to add. First we output new candidates of tanh layer and then they are multiplied by the sigmoid values, finally being added to the cell state.\n\n### Output gate layer\nNow we decide what to output from cell state. This is done using tanh layer through which the outputs of the cell state are passed and then it is multiplied with sigmoid layer to decide which parts to output.\n\nMore details can be found in [Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/?source=post_page-----79e5eb8049c9----------------------)","metadata":{}},{"cell_type":"code","source":"def lstm_layer (hidden1) :\n    \n    model = Sequential()\n    \n    # add input layer\n    model.add(Input(shape = (500, 2, )))\n    \n    # add rnn layer\n    model.add(LSTM(hidden1, activation = 'tanh', return_sequences = False))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    # add output layer\n    model.add(Dense(1, activation = 'linear'))\n    \n    model.compile(loss = \"mean_squared_error\", optimizer = 'adam')\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lstm_layer(256)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkp = ModelCheckpoint('./bit_model_lstm.h5', monitor = 'val_loss', save_best_only = True, verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beg = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test, y_test), callbacks = [checkp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model('./bit_model_lstm.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pred.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pred.reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MSE : ' + str(mean_squared_error(y_test, pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,7))\nplt.plot(y_test[2040:2060])\nplt.plot(pred[2040:2060])\nplt.xlabel('Time')\nplt.ylabel('Price')\nplt.title('Closing Price vs Time (using LSTM)')\nplt.legend(['Actual price', 'Predicted price'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Time taken by LSTM to learn : ' + str(end-beg))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nSince there was ample data, the models actually did not face any issue in learning the pattern. If given enough time, either of the models could predict almost perfectly. But this is important to note that the LSTM was able to predict more accurately and was even faster compared to SimpleRNN.","metadata":{}}]}