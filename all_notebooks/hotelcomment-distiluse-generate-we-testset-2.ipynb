{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport shutil\n\nimport random\nimport pickle\n\nfrom ast import literal_eval\nfrom tqdm import tqdm as print_progress\nfrom glob import glob\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nimport torch\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"import spacy\n\n# Download SpaCy models if needed\nspacy_model = 'en_core_web_sm'\ntry:\n    nlp = spacy.load(spacy_model)\nexcept OSError:\n    spacy.cli.download(spacy_model)\n    nlp = spacy.load(spacy_model)\n    \nfrom nltk.tokenize import sent_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def capitalize_documents(text: str) -> str:\n    # sentences = [sent.text for sent in nlp(text).sents]\n    sentences = sent_tokenize(text)\n    sentences = [sent.capitalize() for sent in sentences]\n    text = ' '.join(sentences)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets_path = '../input/hotel-comment'\nsample_dfs = dict()\n\nfor dataset in ['training', 'valuating', 'testing']:\n    print(f'\\n\\n\\nProcessing {dataset} ...')\n    \n    # Read data\n    print('\\tReading data ...')\n    fn = os.path.join(datasets_path, f'{dataset}_data_augmented.csv')\n    sample_dfs[dataset] = pd.read_csv(fn)#.compute()\n    \n    # Data Augmentation\n    # print('\\tAugmenting data ...')\n    # ds_1, ds_2, ds_3 = ds.copy(), ds.copy(), ds.copy()\n    # ds_1.Comment = ds_1.Comment.str.lower()\n    # ds_2.Comment = ds_2.Comment.str.upper()\n    # ds_3.Comment = ds_3.Comment.apply(lambda x: capitalize_documents(x))\n    # sample_dfs[dataset] = pd.concat([ds, ds_1, ds_2, ds_3], ignore_index=True)\n    # sample_dfs[dataset].drop_duplicates(subset=['Comment'], inplace=True)\n    \n    print(f\"{dataset}-set contains {len(sample_dfs[dataset])} samples\")\n    print(sample_dfs[dataset].sample(n=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for dataset in ['training', 'valuating', 'testing']:\n#     sample_dfs[dataset].to_csv(f'{dataset}_data_augmented.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = os.path.join(datasets_path, 'label_encoder.pkl')\nlabel_encoder = pickle.load(open(filename, 'rb'))\nlabels = list(label_encoder.classes_)\nlabels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Sentences Embedding**","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel_version = '../input/sentence-transformers/distilUSE'\nembedder = SentenceTransformer(model_version)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensor_to_nparray(tensor: torch.Tensor) -> np.array:\n    return tensor.cpu().numpy() if torch.cuda.is_available() else tensor.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_vector = embedder.encode(labels, batch_size=16, convert_to_numpy=True, output_value='token_embeddings')\nlabels_vector = [np.mean(tensor_to_nparray(l), axis=0) for l in labels_vector]\nlabels_matrix = np.vstack(labels_vector)\nlabels_matrix = np.expand_dims(labels_matrix, axis=0)\nnp.save('labels_embeddings.npy', labels_matrix)\nlabels_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence, to_categorical\n\nfor dataset, sample_df in sample_dfs.items():\n    print(f'\\n\\n\\nProcessing {dataset} dataset')\n    dir_path = f'/kaggle/working/{dataset}'\n    if not os.path.isdir(dir_path):\n        print(f'Creating {dir_path}')\n        os.makedirs(dir_path)\n    \n    if dataset != 'testing':\n        continue\n        \n    texts = sample_df.Comment.values.tolist()\n    labels = sample_df.label_encoder.values.tolist()\n    \n    n_samples = len(labels)\n    batch_size = 128\n    n_batches = n_samples//batch_size + (0 if n_samples%batch_size==0 else 1)\n    for b_idx in print_progress(range(n_batches)):\n        \n        if not (500 < b_idx <= 1000):\n            continue\n        \n        # Get samples by batch\n        if b_idx != n_batches-1:\n            b_samples = texts[b_idx*batch_size:(b_idx+1)*batch_size]\n            b_labels = labels[b_idx*batch_size:(b_idx+1)*batch_size]\n        else:\n            b_samples = texts[b_idx*batch_size:]\n            b_labels = labels[b_idx*batch_size:]\n        \n        # Apply sentence-BERT for word embeddings\n        embeddings = embedder.encode(sentences=b_samples, \n                                     batch_size=batch_size, \n                                     output_value='token_embeddings', \n                                     show_progress_bar=False)\n        embeddings = [tensor_to_nparray(e) for e in embeddings]\n        \n        # Apply LabelEncoder\n        labels_multihot = []\n        for l in b_labels:\n            l = literal_eval(l)\n            labels_multihot += [np.sum(to_categorical(l, num_classes=len(labels)), axis=0)]\n        \n        # Feed data into DataFrame\n        for w_idx, (w_embs, mt_label) in enumerate(zip(embeddings, labels_multihot)):\n            np.savez_compressed(f'{dir_path}/sample_{b_idx*batch_size+w_idx:07d}.npz', \n                                emb=w_embs, \n                                mtl=mt_label)\n            del w_embs, mt_label\n\n        del b_samples, b_labels\n        del embeddings, labels_multihot\n        _ = gc.collect() \n        \n    del texts, labels\n    _ = gc.collect() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.chdir(r'/kaggle/working')\n# dir_path = '/kaggle/working/'\n# shutil.make_archive(dir_path+\"data\", 'zip', dir_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}