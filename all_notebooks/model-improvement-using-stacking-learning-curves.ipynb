{"cells":[{"metadata":{},"cell_type":"markdown","source":"The techniques used for in this notebook include:\n\n* Visualization of Important Insights using Matplotlib libraray\n* Data Wrangling using Pandas\n* Bagging and Boosting Techniques to improve bias and variance\n* Cross-Validation and Machine Learning Curves for Model Evaluation\n* Stacking: Improvement in bias and Variance by 10%."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/onlinenewspopularity/OnlineNewsPopularity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****1. Expolatory Data Analysis (EDA)****"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['url'][2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\nimport string\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date=[]\ndate_original=[]\n\nfor i in range(data.shape[0]):\n    x=re.findall(r'[0-9]{4}/[0-9]{2}/[0-9]{2}',data['url'][i])\n    date.append(x)\n    \nfor i in date:\n    for r in i:\n        date_original.append(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['date']=date_original","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['date']= pd.to_datetime(data['date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(10,10))\nax=fig.gca()\nplt.plot(data['date'],data[' shares'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data[' timedelta'],data[' shares'],c='r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n1) Time Delta (The days between the dataset compilation and article publishing) v Number of shares\n\n2) There are more outliers such as after 400 days more articles have been shared more than 20k times\n\n3) These outliers are not helping to check the exact distribution of data and are skewing the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(data.shape[0]):\n    if data[' shares'][i] > 28000:\n        data.drop(index=i,inplace=True)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(data[' timedelta'],data[' shares'],c='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(data[' num_imgs'],data[' shares'],'ro',label='Images')\nplt.plot(data[' num_videos'],data[' shares'],'b^',label='Videos')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This was for getting the relation between number of videos and images in an article with their corresponding shares.\n\n1) If number of videos and images exceeds 80 then shares come close to 0-5k.\n\n2) Most of the articles between 0-40 images and videos has been shared 0-20k times"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data[' n_tokens_title'],data[' shares'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data[' n_tokens_content'],data[' shares'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data[' n_tokens_content'],alpha=0.5,color='b')\nplt.hist(data[' shares'],alpha=0.5,color='g')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations:\n\n1) Too short and too long titles are not getting good response. Words between 5-18 are good.\n\n2) Total words between 0-20k are getting the higher response.Above 20k articles have not been shared more than 500 times.\n\n3) Both \"shares\" and \"n_number_tokens\" are right skewed.That means data is concentrated in lower half."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nfig= plt.figure(figsize=(10,10))\nax=fig.gca()\nax.set_title(\"The 'Sharing' distribution of whole dataset\")\nsns.distplot(data[' shares'],ax=ax, fit=norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skew:\",data[' shares'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A Positive Skew means that data is right skewed and it can be corrected with log or square root."},{"metadata":{},"cell_type":"markdown","source":"#### 1. it is positive skewed so more data in lower half.\n#### 2. We cannot use squared error term as it would highlight the higher terms with errors so would make results less interpretable.\n#### 3. We can power transform the target variable or leave it as it is."},{"metadata":{"trusted":true},"cell_type":"code","source":"lifestyle_articles=data[data[' data_channel_is_lifestyle'] == 1][' shares'].sum()\nentertainment_articles=data[data[' data_channel_is_entertainment'] == 1][' shares'].sum()\nbusiness_articles=data[data[' data_channel_is_bus'] == 1][' shares'].sum()\nsocialmedia_articles=data[data[' data_channel_is_socmed'] == 1][' shares'].sum()\ntechnical_articles=data[data[' data_channel_is_tech'] == 1][' shares'].sum()\nworld_articles=data[data[' data_channel_is_world'] == 1][' shares'].sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"articles_types=np.array([lifestyle_articles,entertainment_articles,business_articles,socialmedia_articles,technical_articles,world_articles],dtype=np.int64)\nfig= plt.figure(figsize=(10,10))\nax=fig.gca()\nax.set_title('TOTAL SHARED ARTICLES OF EACH GENRE')\nax.set_ylabel('Number of Articles')\nplt.bar(x=['lifestyle','entertainment','business','socialmedia','technical','world'],height=articles_types,color='rgbkymc')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_types","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n1) Technical genre articles are the highest ones in sharing order.\n\n2) lifestyle articles are shared the least."},{"metadata":{"trusted":true},"cell_type":"code","source":"monday_articles=data[data[' weekday_is_monday'] == 1][' shares'].sum()\ntuesday_articles=data[data[' weekday_is_tuesday'] == 1][' shares'].sum()\nwednesday_articles=data[data[' weekday_is_wednesday'] == 1][' shares'].sum()\nthursday_articles=data[data[' weekday_is_thursday'] == 1][' shares'].sum()\nfriday_articles=data[data[' weekday_is_friday'] == 1][' shares'].sum()\nsaturday_articles=data[data[' weekday_is_saturday'] == 1][' shares'].sum()\nsunday_articles=data[data[' weekday_is_sunday'] == 1][' shares'].sum()\nweekend_articles=data[data[' is_weekend'] == 1][' shares'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_publishing_days= np.array([monday_articles,tuesday_articles,wednesday_articles,thursday_articles,friday_articles,\n                                    saturday_articles,sunday_articles,weekend_articles])\nfig= plt.figure(figsize=(10,10))\nax=fig.gca()\nax.set_title('Total sharing of articles day-wise')\nax.set_ylabel('Number of Articles')\nplt.bar(x=['monday','tuesday','wednesday','thursday','friday','saturday','sunday','weekend'],height=articles_publishing_days\n        ,color='rgbkymc')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=[]\ndays=[' weekday_is_monday',' weekday_is_tuesday',' weekday_is_wednesday',' weekday_is_thursday',' weekday_is_friday',\n     ' weekday_is_saturday',' weekday_is_sunday',' is_weekend']\ngenre=[' data_channel_is_lifestyle',' data_channel_is_entertainment',' data_channel_is_bus',' data_channel_is_socmed',\n       ' data_channel_is_tech',' data_channel_is_world']\nfor i in days:\n    list1=[]\n    for j in genre:\n        list1.append(data.groupby([i,j])[' shares'].sum()[1][1])\n    print('Best channel on {} has articles {} and channel is {}'.format(i,max(list1),genre[list1.index(max(list1))]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Worst_min_shares=pd.DataFrame(data.groupby([' kw_min_min'],sort=True)[' shares'].sum())\nWorst_max_shares=pd.DataFrame(data.groupby([' kw_max_min'],sort=True)[' shares'].sum())\nWorst_avg_shares=pd.DataFrame(data.groupby([' kw_avg_min'],sort=True)[' shares'].sum())\nBest_min_shares=pd.DataFrame(data.groupby([' kw_min_max'],sort=True)[' shares'].sum())\nBest_max_shares=pd.DataFrame(data.groupby([' kw_max_max'],sort=True)[' shares'].sum())\nBest_avg_shares=pd.DataFrame(data.groupby([' kw_avg_max'],sort=True)[' shares'].sum())\nNormal_min_shares=pd.DataFrame(data.groupby([' kw_min_avg'],sort=True)[' shares'].sum())\nNormal_max_shares=pd.DataFrame(data.groupby([' kw_max_avg'],sort=True)[' shares'].sum())\nNormal_avg_shares=pd.DataFrame(data.groupby([' kw_avg_avg'],sort=True)[' shares'].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Worst_min_shares.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lda_00=pd.DataFrame(data.groupby(by=[' LDA_00'])[' shares'].sum().sort_values(ascending=False)).reset_index()\nLda_01=pd.DataFrame(data.groupby(by=[' LDA_01'])[' shares'].sum().sort_values(ascending=False)).reset_index()\nLda_02=pd.DataFrame(data.groupby(by=[' LDA_02'])[' shares'].sum().sort_values(ascending=False)).reset_index()\nLda_03=pd.DataFrame(data.groupby(by=[' LDA_03'])[' shares'].sum().sort_values(ascending=False)).reset_index()\nLda_04=pd.DataFrame(data.groupby(by=[' LDA_04'])[' shares'].sum().sort_values(ascending=False)).reset_index()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## mean respective lda for > 50 shares\nmean_lda_00=np.mean(Lda_00[Lda_00[' shares'] > 50])[0]\nmean_lda_01=np.mean(Lda_01[Lda_01[' shares'] > 50])[0]\nmean_lda_02=np.mean(Lda_02[Lda_02[' shares'] > 50])[0]\nmean_lda_03=np.mean(Lda_03[Lda_03[' shares'] > 50])[0]\nmean_lda_04=np.mean(Lda_04[Lda_04[' shares'] > 50])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(8,8))\nax=fig.gca()\nplt.bar(x=['mean_lda_00','mean_lda_01','mean_lda_02','mean_lda_03','mean_lda_04'],\n        height=[mean_lda_00,mean_lda_01,mean_lda_02,mean_lda_03,mean_lda_04])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=data[' global_subjectivity'],y=data[' shares']) # Subjectivity from 0.0-1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_group_3=[' global_sentiment_polarity', ' global_rate_positive_words',\n       ' global_rate_negative_words', ' rate_positive_words',\n       ' rate_negative_words', ' avg_positive_polarity',\n       ' min_positive_polarity', ' max_positive_polarity',\n       ' avg_negative_polarity', ' min_negative_polarity',\n       ' max_negative_polarity', ' title_subjectivity',\n       ' title_sentiment_polarity', ' abs_title_subjectivity',\n       ' abs_title_sentiment_polarity', ' shares']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,20))\n\nsns.heatmap(data[columns_group_3].corr(),linewidth=1.0,ax=ax,square=True,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Principal Component Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= data[' shares']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_data=data.drop(labels=['url',' shares','date'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\ndata_transformed= scaler.fit_transform(pca_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca=PCA()\nprincipal_comp=pd.DataFrame(pca.fit_transform(pca_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principal_comp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Model Selection:"},{"metadata":{},"cell_type":"markdown","source":"# 3(a).Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size=0.2\nX_train, X_test, y_train, y_test = train_test_split(pca_data, y,  \n    test_size=test_size,random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparam_grid= {'n_estimators':[20,40],\n            'max_depth':[10,20],\n             'max_features':['auto',10,20],\n             'bootstrap':[True,False],             \n            }\n\n## Initial result gave both extreme values as best parameters so run again by increasing limit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search= RandomizedSearchCV(RandomForestRegressor(),param_distributions=param_grid,\n                                  cv=5,scoring='neg_mean_absolute_error',\n                         verbose=1,n_jobs=-1)\nrandomsearch_result=random_search.fit(X_train,y_train)\nbest_paramters= randomsearch_result.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(randomsearch_result.cv_results_).sort_values('mean_test_score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_paramters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nrf=RandomForestRegressor(n_estimators=40,max_depth=10,max_features=10)\nscores=cross_val_score(rf,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absolute_scores= -scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## def display_scores(score):\n   ## print(\"Mean:\", score.mean())\n   ## print(\"Standard deviation:\", score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\ny_pred=rf.predict(X_test)\ntest_score=mean_absolute_error(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'actual_train_mae_score':absolute_scores,\n             'actual_test_mae_score':test_score},index=['Mean'])\n\n### reversing the normalizing of target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(rf.feature_importances_,pca_data.columns).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns=['variables','score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_df=df.sort_values('score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_variables=sorted_df.iloc[1:15,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(15,15))\nax=fig.gca()\nplt.bar(x=important_variables.variables,height=important_variables.score,color='r')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3(b).   RandomForest.PCA__"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_final_data=principal_comp[[0,1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pca,X_test_pca,y_train_pca,y_test_pca=train_test_split(pca_final_data,y,test_size=0.2,random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid_pca= {'n_estimators':[20,40],\n            'max_depth':[10,20],\n             'bootstrap':[True,False],             \n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_pca= RandomizedSearchCV(RandomForestRegressor(),param_distributions=param_grid_pca,\n                                  cv=5,scoring='neg_mean_absolute_error',\n                                  verbose=1,n_jobs=-1)\nrandomsearch_result_pca=random_search_pca.fit(X_train_pca,y_train_pca)\nbest_paramters_pca= randomsearch_result_pca.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(randomsearch_result_pca.cv_results_).sort_values('mean_test_score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_paramters_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pca=RandomForestRegressor(n_estimators=20,max_depth=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_1=cross_val_score(rf_pca,X_train_pca,y_train_pca,scoring='neg_mean_absolute_error',cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absolute_scores_1=-scores_1.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf_pca.fit(X_train_pca,y_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_pca= rf_pca.predict(X_test_pca)\ntest_score_pca= mean_absolute_error(y_test_pca,y_predict_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'train_mse_score':[absolute_scores_1],\n             'test_mse_score':[test_score_pca]},index=['Mean'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__1(d). Learning Curves__"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes=[500,800,1000,1250,2500,5000,10000,12000,16000,18000,20000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ntrain_sizes,train_scores,validation_scores= learning_curve(rf,X=X_train,y=y_train,train_sizes=train_sizes,\n                                             cv=3,scoring='neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean= -train_scores.mean(axis=1)\nvalidation_scores_mean=-validation_scores.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\nplt.ylabel('MAE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a random forest regression model', fontsize = 18, y = 1.03)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes,train_scores_pca,validation_scores_pca= learning_curve(rf_pca,X=X_train_pca,y=y_train_pca,train_sizes=train_sizes,\n                                             cv=3,scoring='neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean_pca= -train_scores_pca.mean(axis=1)\nvalidation_scores_mean_pca=-validation_scores_pca.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_sizes, train_scores_mean_pca, label = 'Training error PCA')\nplt.plot(train_sizes, validation_scores_mean_pca, label = 'Validation error PCA')\nplt.ylabel('MAE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a random forest regression model', fontsize = 18, y = 1.03)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One solution at this point is to change to a more complex learning algorithm. This should decrease the bias and increase the variance. A mistake would be to try to increase the number of training instances. Generally, these other two fixes also work when dealing with a high bias and low variance problem:\n\n__1. Training the current learning algorithm on more features (to avoid collecting new data, you can generate easily polynomial features). This should lower the bias by increasing the model’s complexity.__\n\n__2. Decreasing the regularization of the current learning algorithm, if that’s the case. In a nutshell, regularization prevents the algorithm from fitting the training data too well. If we decrease regularization, the model will fit training data better, and, as a consequence, the variance will increase and the bias will decrease.__"},{"metadata":{},"cell_type":"markdown","source":"### Comparison:\n\n1. PCA Model has less variance but more bias------Solution: Train on more features increasing the complexity of model and decreasing the regularization of model.Meaning allowing it to overfit.\n\n\n\n2. Normal Model has less bias (as compared to PCA but more than a random forest should have) and more variance."},{"metadata":{},"cell_type":"markdown","source":"#### Since it is a bagging method it has less variance. Now we can use boosting to get less bias."},{"metadata":{},"cell_type":"markdown","source":"#  3(C). Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_gradboost={'n_estimators':[100,150],\n                'max_depth':[5,10],\n                'learning_rate':[0.1,0.2]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_data_gbr= principal_comp[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]\nX_train_gbr,X_test_gbr,y_train_gbr,y_test_gbr=train_test_split(pca_data_gbr,y,test_size=0.2,random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad_randomsearch= RandomizedSearchCV(GradientBoostingRegressor(),param_distributions=param_gradboost,cv=3,\n                                      scoring='neg_mean_absolute_error',n_jobs=-1,verbose=1)\ngrad_fit=grad_randomsearch.fit(X_train_gbr,y_train_gbr)\nbest_param_grad= grad_fit.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(grad_fit.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_param_grad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr= GradientBoostingRegressor(n_estimators=50,max_depth=5,learning_rate=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad_result= gbr.fit(X_train_gbr,y_train_gbr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_boosting= cross_val_score(gbr,X_train_gbr,y_train_gbr,scoring='neg_mean_absolute_error',cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absolute_scores_boosting= - scores_boosting.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gbr=gbr.predict(X_test_gbr)\ntest_score_gbr= mean_absolute_error(y_test_gbr,y_pred_gbr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'train_mae_score':[absolute_scores_boosting],\n             'test_mae_score':[test_score_gbr]},index=['Mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes,train_scores_gbr,validation_scores_gbr= learning_curve(gbr,X=X_train_pca,y=y_train_pca,train_sizes=train_sizes,\n                                             cv=5,scoring='neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean_gbr= -train_scores_gbr.mean(axis=1)\nvalidation_scores_mean_gbr=-validation_scores_gbr.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_sizes, train_scores_mean_gbr, label = 'Training error PCA')\nplt.plot(train_sizes, validation_scores_mean_gbr, label = 'Validation error PCA')\nplt.ylabel('MAE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a gradient boosting regression model', fontsize = 18, y = 1.03)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This model has much is better than above two in bias and variance."},{"metadata":{},"cell_type":"markdown","source":"# 3(d) Gradient Boosting with feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_original=gbr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=pd.DataFrame(X_train.columns,gbr_original.feature_importances_).reset_index()\ndf_1.columns=['score','variables']\nselect_columns=df_1.sort_values('score',ascending=False)['variables']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df= pd.concat(objs=[df,df_1],axis=1)\nfeature_importance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_df.columns=['Variables_rf','Score_rf','score_gb','Variables_gb']\nfeature_importance_df=feature_importance_df.sort_values('Score_rf',ascending=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(feature_importance_df['score_gb'][0:25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_columns=df_1.sort_values('score',ascending=False)['variables'][0:25]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_select,X_test_select,y_train_select,y_test_select=train_test_split(data[select_columns.reset_index()['variables']],\n                                                                           y,test_size=0.2,random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad_randomsearch_select= RandomizedSearchCV(GradientBoostingRegressor(),param_distributions=param_gradboost,cv=3,\n                                      scoring='neg_mean_absolute_error',n_jobs=-1,verbose=1)\ngrad_fit_select=grad_randomsearch_select.fit(X_train_select,y_train_select)\nbest_param_grad_select= grad_fit_select.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(grad_fit_select.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_param_grad_select","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_select= GradientBoostingRegressor(n_estimators=100,max_depth=5,learning_rate=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad_result_select= gbr_select.fit(X_train_select,y_train_select)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_boosting_select= cross_val_score(gbr_select,X_train_select,y_train_select,scoring='neg_mean_absolute_error',cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absolute_scores_boosting_select= - scores_boosting_select.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gbr_select=gbr_select.predict(X_test_select)\ntest_score_gbr_select= mean_absolute_error(y_test_select,y_pred_gbr_select)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'train_mae_score':[absolute_scores_boosting_select],\n             'test_mae_score':[test_score_gbr_select]},index=['Mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes,train_scores_select,validation_scores_select= learning_curve(gbr_select,X=X_train_select,\n                                                                         y=y_train_select,train_sizes=train_sizes,\n                                                                           cv=5,scoring='neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean_select= -train_scores_select.mean(axis=1)\nvalidation_scores_mean_select=-validation_scores_select.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_sizes, train_scores_mean_select, label = 'Training error PCA')\nplt.plot(train_sizes, validation_scores_mean_select, label = 'Validation error PCA')\nplt.ylabel('MAE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a gradient boosting regression model', fontsize = 18, y = 1.03)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison of all Scores and Models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Comparison_df= pd.DataFrame({'Training_Scores':[absolute_scores,absolute_scores_1,\n                                                absolute_scores_boosting,absolute_scores_boosting_select],\n                            'Test_Scores':[test_score,test_score_pca,test_score_gbr,test_score_gbr_select]},\n                            index=['Rf','Rf_PCA','gbr','gbr_select'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Comparison_df['Variance']=np.subtract(Comparison_df['Training_Scores'],Comparison_df['Test_Scores'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Comparison_df=Comparison_df.sort_values('Training_Scores')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Comparison_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3(e). Stacking"},{"metadata":{},"cell_type":"markdown","source":"From above comaprison, we are going to create a new dataset through predictions of the four models."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1= rf.predict(pca_data)\nX2=rf_pca.predict(pca_final_data)\nX3=gbr.predict(pca_data_gbr)\nX4=gbr_select.predict(data[select_columns.reset_index()['variables']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### __To Train the final model on large datasets we had to use whole datasets for prediction here___"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_stacking= pd.DataFrame({'Random_Forest':X1,\n                            'Random_Forest_PCA':X2,\n                            'GBR':X3,\n                            \"GBR_select\":X4,\n                            \"Target\":y})\ndata_stacking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_stack,X_test_stack,y_train_stack,y_test_stack= train_test_split(\n    data_stacking[['Random_Forest','Random_Forest_PCA','GBR','GBR_select']],y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LinearRegression()\nlr.fit(X_train_stack,y_train_stack)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_score_lr= cross_val_score(lr,X_train_stack,y_train_stack,scoring='neg_mean_absolute_error',cv=20)\nabsolute_training_lr= -training_score_lr.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_lr= lr.predict(X_test_stack)\ntest_score_lr= mean_absolute_error(y_test_stack,y_predict_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes,train_scores_lr,validation_scores_lr= learning_curve(lr,X=X_train_stack,\n                                                                         y=y_train_stack,train_sizes=train_sizes,\n                                                                           cv=10,scoring='neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean_lr= -train_scores_lr.mean(axis=1)\nvalidation_scores_mean_lr=-validation_scores_lr.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_sizes, train_scores_mean_lr, label = 'Training error PCA')\nplt.plot(train_sizes, validation_scores_mean_lr, label = 'Validation error PCA')\nplt.ylabel('MAE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary:\n\n1. First data is explored using data manipulation techniques of Pandas using visualization techniques by matplotlib package\n\n2. After that, since there are 52 features, Principal Component Analysis(PCA) was done for dimensionality reduction.\n\n3. The first algorithm used is Random Forest Regressor of Sklearn library. It's feature importance was also collected.\n\n4. Secondly same algorithm was used on PCA data which resulted in more bias but less variance.\n\n5. Gradient Boosting Regressor was then used to decrease bias. Here PCA dataset was used as it has less variance originally.\n\n6. Then important features of randomforest were put into Gradient Boosting which proved to be best model of all the four models.\n\n7. During all models, parameters were chosen using Randomized CV and training scores were gathered using cross validation.\n\n8. Finally Stacking of all four models was done. The upper most algorithm was linear regression which proved to be the most effective modelm"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}