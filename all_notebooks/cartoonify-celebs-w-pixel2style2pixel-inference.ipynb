{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n### The notebook is a demo of [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/pdf/2008.00951.pdf) obtained from the authors' original [pixel2style2pixel implementation](https://github.com/eladrich/pixel2style2pixel)."},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Cartoonify results using Pixel2Style2Pixel Model</center></h3>\n<img src=\"https://github.com/eladrich/pixel2style2pixel/raw/master/docs/toonify_input.jpg\" width=\"900\" height=\"750\"/>\n<img src=\"https://github.com/eladrich/pixel2style2pixel/raw/master/docs/toonify_output.jpg\" width=\"900\" height=\"750\"/>\n<h4></h4>\n<h4><center><a href=\"https://github.com/eladrich/pixel2style2pixel\">Source: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation [Elad Richardson et. al.]</a></center></h4>"},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\n\n### This work was inspired by and derives codes from the official [Pixel2Style2Pixel implementation](https://github.com/eladrich/pixel2style2pixel). If you use this work, you should cite the research work [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951) and cite / star ðŸŒŸ the [official implementation](https://github.com/eladrich/pixel2style2pixel)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nCODE_DIR = 'pixel2style2pixel'\n\n!git clone https://github.com/eladrich/pixel2style2pixel.git $CODE_DIR\n    \n!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n!unzip ninja-linux.zip -d /usr/local/bin/\n!update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force \n\n\nos.makedirs('toonify_results')\nos.chdir(f'./{CODE_DIR}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from argparse import Namespace\nimport time\nimport os\nimport cv2\nimport sys\nimport glob\nimport pprint\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\nsys.path.append(\".\")\nsys.path.append(\"..\")\n\nfrom datasets import augmentations\nfrom utils.common import tensor2im, log_input_image\nfrom models.psp import pSp\n\n%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select Experiment Type\n> Select which experiment you wish to perform inference on:\n> 1. ffhq_encode\n> 2. ffhq_frontalize\n> 3. celebs_sketch_to_face\n> 4. celebs_seg_to_face\n> 5. celebs_super_resolution\n> 6. toonify"},{"metadata":{"trusted":true},"cell_type":"code","source":"# experiment_type = 'ffhq_encode'\n# experiment_type = 'ffhq_frontalize'\n# experiment_type = 'celebs_sketch_to_face'\n# experiment_type = 'celebs_seg_to_face'\n# experiment_type = 'celebs_super_resolution'\nexperiment_type = 'toonify'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Inference Parameters"},{"metadata":{},"cell_type":"markdown","source":"> Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on. While we provide default values to run this script, feel free to change as needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"EXPERIMENT_DATA_ARGS = {\n    \"toonify\": {\n        \"model_path\": \"../../input/pixel2style2pixel-pretrained-checkpoints-pytorch/psp_ffhq_toonify.pt\",\n        \"image_path\": \"../../input/celeba-dataset/img_align_celeba/img_align_celeba/000020.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Pretrained Model\n> We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = EXPERIMENT_ARGS['model_path']\nckpt = torch.load(model_path, map_location='cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opts = ckpt['opts']\npprint.pprint(opts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# update the training options\nopts['checkpoint_path'] = model_path\nif 'learn_in_w' not in opts:\n    opts['learn_in_w'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opts= Namespace(**opts)\nnet = pSp(opts)\nnet.eval()\nnet.cuda()\nprint('Model successfully loaded!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Input"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\noriginal_image = Image.open(image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_image = original_image.resize((256, 256))\ninput_image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perform Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_transforms = EXPERIMENT_ARGS['transform']\ntransformed_image = img_transforms(input_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_mask = None\n\ndef run_on_batch(inputs, net, latent_mask=None):\n    result_batch = []\n    for image_idx, input_image in enumerate(inputs):\n        # get latent vector to inject into our input image\n        vec_to_inject = np.random.randn(1, 512).astype('float32')\n        _, latent_to_inject = net(torch.from_numpy(vec_to_inject).to(\"cuda\"),\n                                  input_code=True,\n                                  return_latents=True)\n        # get output image with injected style vector\n        res = net(input_image.unsqueeze(0).to(\"cuda\").float(),\n                  latent_mask=latent_mask,\n                  inject_latent=latent_to_inject)\n        result_batch.append(res)\n    result_batch = torch.cat(result_batch, dim=0)\n    return result_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    tic = time.time()\n    result_image = run_on_batch(transformed_image.unsqueeze(0), net, latent_mask)[0]\n    toc = time.time()\n    print('Inference took {:.4f} seconds.'.format(toc - tic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_vis_image = log_input_image(transformed_image, opts)\noutput_image = tensor2im(result_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = np.concatenate([np.array(input_vis_image.resize((256, 256))),\n                      np.array(output_image.resize((256, 256)))], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_image = Image.fromarray(res)\n\nres_image.save('../toonify_results/sample.jpg')\nres_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_image = Image.fromarray(res)\nres_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_paths = random.sample(glob.glob(os.path.join('../../input/celeba-dataset/img_align_celeba/img_align_celeba/', '*.jpg')), 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_paths[0].split('/')[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_path in image_paths:\n    input_image = Image.open(image_path)\n    input_image = input_image.resize((256, 256))\n    img_transforms = EXPERIMENT_ARGS['transform']\n    transformed_image = img_transforms(input_image)\n    \n    with torch.no_grad():\n        result_image = run_on_batch(transformed_image.unsqueeze(0), net, latent_mask)[0]\n        \n    input_vis_image = log_input_image(transformed_image, opts)\n    output_image = tensor2im(result_image)\n    res = np.concatenate([np.array(input_vis_image.resize((256, 256))), np.array(output_image.resize((256, 256)))], axis=1)\n    res_image = Image.fromarray(res)\n    res_image.save(f\"../toonify_results/{image_path.split('/')[-1]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(f'../')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf pixel2stylepixel\n!rm ninja-linux.zip","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}