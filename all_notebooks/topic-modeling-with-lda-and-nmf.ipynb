{"cells":[{"metadata":{"_cell_guid":"f3566790-44bc-47be-a57b-cd2d89fd6694","_uuid":"5ee816ba302841e36c938857a18f87324038dc73"},"cell_type":"markdown","source":"# Topic Modeling \n\nTopic modeling is a statistical model to discover the abstract \"topics\" that occur in a collection of documents.  \nI will be focusing on two medhods seen as follows. \n* LDA \n* Non-negative matrix factorization  "},{"metadata":{"_cell_guid":"6f9e5711-68f6-472c-afe7-93be77b6f5da","_uuid":"b5be934ed735cb1549429d9975a0a77a53aab4b9"},"cell_type":"markdown","source":"# Import libraries\n\nI used LDA model from sklearn. Other option is using gensim."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69068869-6fbd-456c-8a59-d12b9c7ae974","_uuid":"b2097d7b1bdb4cb258147bf0ecc9bc408e32dcd5"},"cell_type":"markdown","source":"## Read the data"},{"metadata":{"_cell_guid":"b5d768e8-3b74-4b58-be2a-3a30f04f373f","_uuid":"9bb4a18c3ea87a27bf7ee2181a082ceab3cf1cd2","trusted":true},"cell_type":"code","source":"# Input from csv\ndf = pd.read_csv('../input/voted-kaggle-dataset.csv')\n\n# sample data\nprint(df['Description'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Title'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of data frame\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# is there any NaN values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nan value in Description\ndf.Description.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Tags[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#REMOVE NaN VALUES\ndf['Description'].dropna(inplace=True,axis=0)\n\n# check if there is any NaN values\ndf.Description.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# REMOVE EMPTY STRINGS:\nblanks = []  # start with an empty list\n\nfor rv in df['Description']:  # iterate over the DataFrame\n    if type(rv)==str:            # avoid NaN values\n        if rv.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\nprint(blanks)\ndf['Description'].drop(blanks, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"_cell_guid":"d31af256-269b-43b6-8c25-dfaa8ecbd80b","_uuid":"de6657b4355030de74b1a44f0058035b60757b98"},"cell_type":"markdown","source":"# 1.Initiating Tokenizer and Lemmatizer\n\nInitiate the tokenizer, stop words, and lemmatizer from the libraries.\n\n* Tokenizer is used to split the sentences into words.  \n* Lemmatizer (a quite similar term to Stemmer) is used to reduce words to its base form.   \nThe simple difference is that Lemmatizer considers the meaning while Stemmer does not. \n"},{"metadata":{"_cell_guid":"f78d4f89-d08f-45db-8c5c-5dc42fa22a51","_uuid":"efce42d9747224f7dbed4321ad6de166c1e062a6","trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords\n\npattern = r'\\b[^\\d\\W]+\\b'\n# \\b is word boundry\n# [^] is neget\n# \\d is digit and \\W is not word\n\n#tokenize from nltk\ntokenizer = RegexpTokenizer(pattern)\n#I created by myself\ndef tokenizer_man(doc,remove_stopwords=False):\n    doc_rem_puct = re.sub(r'[^a-zA-Z]',' ',doc)\n    words = doc_rem_puct.lower().split()    \n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))     \n        words = [w for w in words if not w in stops]\n    return words\n\nen_stop = get_stop_words('en')\nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NTLK stopwords\n\n#check how many stopwords you have\nstops1=set(stopwords.words('english'))\nprint(stops1)\n#lenght of stopwords\nlen(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding new element to the set\nstops1.add('newWords') #newWord added into the stopwords\nprint(len(stops1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw = str(df['Description'][0]).lower()\ntokens = tokenizer.tokenize(raw)\n\" \".join(tokens)\nlen(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test manual \nstring=df['Description'][0]\nvocab = tokenizer_man(string)\n\" \".join(vocab)\nlen(vocab)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"25f76acb8d57aa80478688fe7d3c1b57570402b4"},"cell_type":"code","source":"remove_words = ['data','dataset','datasets','content','context','acknowledgement','inspiration']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77bc2203-9b96-4f78-a0d2-b18a521c03c1","_uuid":"52763dfe1cf630120f5b5c485d394a23cb1aec4f"},"cell_type":"markdown","source":"## Perform Tokenization, Words removal, and Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list for tokenized documents in loop\ntexts = []\n\n# loop through document list\nfor i in df['Description'].iteritems():\n    # clean and tokenize document string\n    raw = str(i[1]).lower()\n    tokens = tokenizer.tokenize(raw)\n\n    # remove stop words from tokens\n    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n    \n    # remove stop words from tokens\n    stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n    \n    # lemmatize tokens\n    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens_new]\n    \n    # remove word containing only single char\n    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n    \n    # add tokens to list\n    texts.append(new_lemma_tokens)\n\n# sample data\nprint(texts[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['desc_preprocessed'] = \"\"\nfor i in range(len(texts)):\n    df['desc_preprocessed'][i] = ' '.join(map(str, texts[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['desc_preprocessed'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_df=0.9,min_df=2,stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm = tfidf.fit_transform(df['desc_preprocessed'])\n\ndtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF,LatentDirichletAllocation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Non-negative Matrix Factorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"nmf_model = NMF(n_components=7,random_state=42)\nnmf_model.fit(dtm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LDA modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"LDA = LatentDirichletAllocation(n_components=7,random_state=42)\nLDA.fit(dtm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying Topics "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words for NMF modeling\nfor index,topic in enumerate(nmf_model.components_):\n    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words for LDA modeling\nfor index,topic in enumerate(LDA.components_):\n    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_results = nmf_model.transform(dtm)\ndf['NMF_Topic'] = topic_results.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LDA_topic_results = LDA.transform(dtm)\ndf['LDA_Topic'] = LDA_topic_results.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mytopic_dict = {0:'public',\n                1:'sports',\n                2:'machine_learning',\n                3:'neuron_network',\n                4:'politic',\n                5:'economy',\n                6:'text analysis'\n               }\n\ndf['topic_label_NMF']=df['NMF_Topic'].map(mytopic_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['LDA_Topic'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nfirst_topic = nmf_model.components_[0]\nfirst_topic_words = [tfidf.get_feature_names()[i] for i in first_topic.argsort()[:-15 - 1 :-1]]\n\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4000,\n                          height=2500\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}