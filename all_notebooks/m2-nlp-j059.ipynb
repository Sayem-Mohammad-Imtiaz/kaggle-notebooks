{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport numpy as np;\nimport scipy as sp;\nimport sklearn;\nimport sys;\nfrom nltk.corpus import stopwords;\nimport nltk;\nfrom gensim.models import ldamodel\nimport gensim.corpora;\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\nfrom sklearn.decomposition import NMF;\nfrom sklearn.preprocessing import normalize;\nimport pickle;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From sklearn\nimport scipy as sp;\nimport sklearn;\nimport sys;\nfrom gensim.models import ldamodel\nimport gensim.corpora;\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\nfrom sklearn.decomposition import NMF;\nfrom sklearn.preprocessing import normalize;\nimport pickle;\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n#From nltk\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('words')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Annotations.csv\")\nb=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Posts.csv\")\nc=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Categories.csv\")\nd=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Newspaper_Staff.csv\")\ne=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Articles.csv\")\nf=pd.read_csv(\"/kaggle/input/10k-german-news-articles/CrossValSplit.csv\")\ng=pd.read_csv(\"/kaggle/input/10k-german-news-articles/Annotations_consolidated.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ag = a.merge(g, on='ID_Post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"be=b.merge(e, on='ID_Article')\nbe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"be.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"be.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"be.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#taking only 5000 values\nbe=be.iloc[0:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing HTML tags\nimport re\ntags = re.compile(r'<[^>]+>')\ndef html_del(sent):\n    t1 = tags.sub('', sent)\n    return t1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipelining\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\ndef pipeline_ger(text):\n    text=re.sub('[^a-zA-Z]', \" \", str(text))\n    text=text.lower()\n    stop_words_ge = set(stopwords.words('german'))\n    word_tokens = word_tokenize(text)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words_ge]\n    text = TreebankWordDetokenizer().detokenize(filtered_sentence)\n    return text\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying pipline\ndf = pd.DataFrame(be, columns = ['ID_Post', 'ID_Article','Headline','Body_x','Title','Body_y'])\ndf[\"Title\"] = df['Title'].apply(pipeline_ger)\ndf[\"Headline\"] = df['Headline'].apply(pipeline_ger)\ndf[\"Body_x\"] = df['Body_x'].apply(pipeline_ger)\ndf['Body_y'] = df['Body_y'].apply(html_del)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_content = pd.DataFrame(df, columns = ['Body_x','Body_y','Title'])\ndf_content#body_x=posts,body_y=articles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_content.info()\n#completed till here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmat = WordNetLemmatizer()\nvect = TfidfVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_contentt =  vect.fit_transform(df_content['Title'])\nmodel_contentt = pd.DataFrame(model_contentt.toarray(), columns=vect.get_feature_names())\nnmf_model_contentt = NMF(n_components=10, init='random', random_state=0)\nW_contentt = nmf_model_contentt.fit_transform(model_contentt)\nH_contentt = nmf_model_contentt.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_contentx =  vect.fit_transform(df_content['Body_x'])\nmodel_contentx = pd.DataFrame(model_contentx.toarray(), columns=vect.get_feature_names())\nnmf_model_contentx = NMF(n_components=10, init='random', random_state=0)\nW_contentx = nmf_model_contentx.fit_transform(model_contentx)\nH_contentx = nmf_model_contentx.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_contenty =  vect.fit_transform(df_content['Body_y'])\nmodel_contenty = pd.DataFrame(model_contenty.toarray(), columns=vect.get_feature_names())\nnmf_model_contenty = NMF(n_components=10, init='random', random_state=0)\nW_contenty = nmf_model_contenty.fit_transform(model_contenty)\nH_contenty = nmf_model_contenty.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda=LatentDirichletAllocation(n_components=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel_contentt=lda.fit_transform(model_contentt)\nldacomp_contentt=lda.components_\nldacomp_contentt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel_contentx=lda.fit_transform(model_contentx)\nldacomp_contentx=lda.components_\nldacomp_contentx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel_contenty=lda.fit_transform(model_contenty)\nldacomp_contenty=lda.components_\nldacomp_contenty","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_topics(H, W, feature_names, no_top_words, no_top_documents):\n   for topic_idx, topic in enumerate(H):\n       print(\"Topic %d:\" % (topic_idx))\n       print(\" \".join([feature_names[i]\n                       for i in topic.argsort()[:-no_top_words - 1:-1]]))\n       top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_topics(H_contentt, W_contentt, vect.get_feature_names(), 7,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_topics(H_contentx, W_contentx, vect.get_feature_names(), 7,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_topics(H_contenty, W_contenty, vect.get_feature_names(), 7,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_topics(model, vect, n_top_words):\n    words = vect.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_topics(lda,vect,10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}