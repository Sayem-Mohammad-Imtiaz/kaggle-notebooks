{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multiple Linear Regression\n\n\n\n### Housing Case Study\n\n\n\n#### Problem Statement:\n\n\nConsider that a real estate company has the data of real-estate prices in Delhi. The company wants to optimise the selling price of the properties, based on important factors such as area, bedrooms, parking, etc.\n \n\nEssentially, the company wants:\n\n- To identify the variables affecting house prices, e.g., area, number of rooms, bathrooms, etc.\n- To create a linear model that quantitatively relates house prices with variables, such as the number of rooms, area, number of bathrooms, etc.\n- To know the accuracy of the model, i.e. how well do these variables predict the house prices\n\n\n**So the the interpretation of the data is important**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step the are perfomred in the multiple linear regression are as follows\n1. Reading , understanding and visualize the data\n2. Preparing Training and Testing data split\n3. Training the model\n4. Residual Analysis\n5. Prediction and Evalution of the test dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Reading , understanding and visualize the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#standard import for importing the dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignoring all the warning the we get\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the data set\ndata_url = \"/kaggle/input//housing-simple-regression/Housing.csv\"\n\nhousing = pd.read_csv(data_url)\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ploting the dataset\n# numerical variable\nsns.pairplot(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizng the categorical variable\n# make a box plot between continous varaible and categorical variable\nplt.figure(figsize=(15,10))\nplt.subplot(2,3,1)\nsns.boxplot(x=\"mainroad\",y=\"price\",data=housing)\n\nplt.subplot(2,3,2)\nsns.boxplot(x=\"airconditioning\",y=\"price\",data=housing)\n\nplt.subplot(2,3,3)\nsns.boxplot(x=\"furnishingstatus\",y=\"price\",data=housing)\n\nplt.subplot(2,3,4)\nsns.boxplot(x=\"guestroom\",y=\"price\",data=housing)\n\nplt.subplot(2,3,5)\nsns.boxplot(x=\"basement\",y=\"price\",data=housing)\n\nplt.subplot(2,3,6)\nsns.boxplot(x=\"prefarea\",y=\"price\",data=housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it can be clearly seen that how a categorical variable explains about the variance in the price value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.boxplot(x=\"furnishingstatus\",hue=\"airconditioning\", y=\"price\",data=housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can infer that usually having an airconditioned increases the price of house as compared to not having it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Preparing the data for modeling\n\n- Encoding\n    - binary categorical to 0 and 1\n    - other categorical variable to dummy variable or one hot encoding\n- Spliting in test train data set\n- Rescaling of some variable\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the binary categorical variable to 1 or 0 or we can have one hot encoding\nhousing.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['mainroad','guestroom', 'basement',\n            'hotwaterheating', 'airconditioning', 'prefarea']\n\n# one way doing it is mentioned below\n# for var in var_list:\n#     housing[var] = housing[var].apply(lambda x: 1 if x==\"yes\" else 0)\n\n# can also be done by subseting the dataset\nhousing[var_list] = housing[var_list].apply(lambda x: x.map({\"yes\":1,\"no\":0}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting furnishing status to one hot encoding or dummy variable\nstatus = pd.get_dummies(housing['furnishingstatus'])\nstatus.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can drop furnished columns as instead of three columns it can be represent using the following structure\n\nIdentification -\n- 00 Furnished \n- 10 Semi -furnished   \n- 01 unfurnished       ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the redundent columns\nstatus = pd.get_dummies(housing['furnishingstatus'],drop_first=True)\nstatus.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat the dummy data to housing\n\nhousing = pd.concat([housing,status],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = housing.drop(['furnishingstatus'],axis=1)\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing the train test split\ndf_train, df_test = train_test_split(housing,train_size=0.7,random_state=100)\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rescaling the Features\n\n1. Min-Max scaling (normalization) convert between 0 and 1\n2. Standardization (convert the data such that mean = 0 and std = 1)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sklearn geneally comes with 3 types of methods for preprocessing MinMaxScaler\n# fit() learn, will just calculate min and max values\n# transform() x - xmin/(xmax - xmin)\n# fit_tranform() does above two in just one step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create class object\nscaler = MinMaxScaler()\n\n#create a list of only numeric variable\nscaler_list = ['area','bedrooms','bathrooms','stories','parking','price']\n\n# fit the scaler in training data set\ndf_train[scaler_list] = scaler.fit_transform(df_train[scaler_list])\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Modelling or Training the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many of the features show we choose for optimum model training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting a heat map to understand the correlation among feature\nplt.figure(figsize=(14,10))\nsns.heatmap(df_train.corr(), annot= True, cmap='YlGnBu')\nb, t = plt.ylim()\nb += 0.5\nt -= 0.5\nplt.ylim(b, t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('price')\nX_train = df_train\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for every new feature varaible we add we see the following\n# - signification of the variable \n# - if varaible is correlated we'll look at VIF\n\n#only using area for now\nX_train_sm = sm.add_constant(X_train['area'])\n\nlr = sm.OLS(y_train, X_train_sm)\n\nlr_model = lr.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summay Evaluation\n\n- p value of the coef are low mean the coef are significant\n- r sqaured is 28.3 percent means the model explains 28 percent variance in the price with the given variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we add another variable and see the result\n\nX_train_sm = X_train[['area','bathrooms']]\nX_train_sm = sm.add_constant(X_train_sm)\n\nlr = sm.OLS(y_train,X_train_sm)\n\nlr_model = lr.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summay Evaluation\nwe have 2 dependent variable now\n\n- p values of the coefs are low which mean the coefs are significant\n- r sqaured is 48 percent means the model explains 48 percent variance in the price with the given variables\n- this is a relatively good model as compared to the previous one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we add another variable bedrooms\n\n\nX_train_sm = X_train[['area','bathrooms','bedrooms']]\nX_train_sm = sm.add_constant(X_train_sm)\n\nlr = sm.OLS(y_train,X_train_sm)\n\nlr_model = lr.fit()\nlr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now this is a top down approach\n### we can also build the model using all the feature and then try look at the VIF and p value to drop out the unnecessary variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we ada all the varaibles\n\nX_train_sm = sm.add_constant(X_train)\n\nlr = sm.OLS(y_train,X_train_sm)\n\nlr_model = lr.fit()\nlr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vif_calculate(X_df):\n    vif = pd.DataFrame()\n    vif['Features'] = X_df.columns\n    vif['vif'] = [variance_inflation_factor(X_df.values,i) for i in range(X_df.shape[1])]\n    vif['vif'] = round(vif['vif'],2)\n    vif = vif.sort_values(by=\"vif\",ascending=False)\n    return vif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(y_train,X_train):\n    X_train_sm = sm.add_constant(X_train)\n\n    lr = sm.OLS(y_train,X_train_sm)\n\n    lr_model = lr.fit()\n    return lr_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_calculate(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we usually stick with vif less than 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we could have \n- high p-value, high vip = drop it\n- high-low\n    - high p-value, low vif (reomve these first)\n    - low p-value, high vif (after removing above recompute the vif and check it )\n- low p-value, low vip = Keep it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate everything once again and eliminate the feature base on the above rules","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop('semi-furnished',axis=1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(y_train,X_train).summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_calculate(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# again doing the eleminating step\n# bedrooms have high p-value so we eliminat it\nX_train = X_train.drop('bedrooms',axis=1)\nX_train_sm = sm.add_constant(X_train)\nlr_model = train_model(y_train,X_train)\nlr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_calculate(X_train) # now almost most of the every feature is below 5 so this could be our final model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Residual Analysis\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = lr_model.predict(X_train_sm)\n\nres = y_train - y_train_pred\nres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of the error terms - it should have a normal distribution\nsns.distplot(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step :5 Prediction and Evaluation of our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# same transformation needs to be on the training set also\n# we never perform fit() operation on the test set\n# we only transform() on the dataset\n\ndf_test[scaler_list] = scaler.transform(df_test[scaler_list])\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('price')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)\nX_test_sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = X_test_sm.drop(['semi-furnished','bedrooms'],axis=1)\nX_test_sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = lr_model.predict(X_test_sm)\ny_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(y_true=y_test,y_pred=y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### what ever the models  has learned on train set able to generalize well for test because the r2 score is kinda same for both\n\n# We can also use RFE - Recursive feature elimination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test =train_test_split(housing,train_size=0.7,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_list = ['area','bedrooms','bathrooms','stories','parking','price']\n\n# fit the scaler in training data set\ndf_train[scaler_list] = scaler.fit_transform(df_train[scaler_list])\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('price')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting dimension of y varriable\ny_train = y_train.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(y_train, X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RFE(lr,10) # we choose have the best 10 variable ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = rfe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# our 10 best features would be\nX_train.columns[rfe.support_].tolist() # RFE tells that these variable are really significant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = X_train.columns[~rfe.support_]\ncolumns_to_drop ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(columns_to_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can again use our statsmodel to check weather these feature are good or not\n- we can also use the vif values of the feature to understand the multicollinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_calculate(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the statsmodel\n\nX_train_sm = sm.add_constant(X_train)\n\nlr = sm.OLS(y_train,X_train)\n\nlr_model = lr.fit()\nlr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary Evalution of the OLS model fitting\n\n- we can see that vif value of bedroom variable is high and it's p value is also high\n- drop bedrooms and generate the summary and vif values again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop('bedrooms',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif_calculate(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = X_train_sm.drop('bedrooms',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = sm.OLS(y_train,X_train)\nlr_model = lr.fit()\nlr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insights and inferences\n\n- Clearly even after dropping bedrooms variable we are still able to get a high value of r square\n- p-values of all the coefs are very smalls means the coefs are significant\n- VIF values of the variables are below 5\n- This model can explain almost 91 percent variance in the dataset\n\n#### Question\nhow come dropping the bedrooms make the model better when it the variable of higher significance ?\n- bedrooms variable must be high co related to some other variable i.e why we can eliminate it\n\nSuppose we have 3 variable X,Y,Z and all of them have the same values (means highly correlated)\n\n10X = 9X + 1Y = 8X + 1Y + 1Z = .... etc\n\nso it doesn't matter which variables you choose if they are so similar in nature\n\nit could be possible the bedrooms be Y and Area be X. So we can remove Y and have a higher coef value for X which normalize the effect\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}