{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RealTrafic - anomaly detection\n\nIn this notebook, data analysis and anomaly detection was performed for the RealTraffic data from the [Numenta Anomaly Benchmark](https://github.com/numenta/NAB)\n\nThe dataset consists of data from 5 sensors that measure traffic on different road sections. The sensors measure traffic in various metrics:\n- occupancy - the average number of vehicles\n- speed - average speed\n- travel time - average travel time\n\nNote: not all metrics are available at each metrics.\n\n## Data load and analysis\nThe work was started by reading and inspecting the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd ../input/nab/realTraffic/realTraffic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame(columns=['sensor', 'metric', 'timestamp', 'value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for file in os.listdir():\n    if file[-4:] == '.csv':\n        temp = pd.read_csv(file, engine='python')\n        sensor_name = re.split(r\"[_.]\",file)\n        temp['metric'] = sensor_name[0]\n        temp['sensor'] = sensor_name[1]\n        data = data.append(temp, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing a summary for the readings","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sensors_summary = data.groupby(['sensor', 'metric']).agg(['min', 'max', 'count'], on='value')\nsensors_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions:\n- Only two sensors have two metrics. They can be combined to provide more accurate insights.\n- Rest of the sensors measure only one metric.\n- The timestamp is a key between multiple metrics at the same sensor.\n- But the range for the timestamp is varying for different metrics for the same sensor. It has to be taken into account while merging data.\n- Ranges for the same metrics between different sensors look similar.\n- The data doesn't have labels indicating anomalies. This dataset represents unsupervised case.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The distribution of features is important while using most of typical anomaly detection algorithms.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for sensor, metric in sensors_summary.index:\n    plt.figure(figsize=(7, 5))\n    sns.distplot(data[(data['sensor']==sensor) & (data['metric']==metric)]['value']).set_title('Distribution for the {} metric for the {} sensor'.format(metric, sensor))\n    plt.xlabel('Value')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data distribution is far from normal. This would hurt the performance of most of the anomaly detection algorithms.\nThe log1p function was used to fix the issue.  It's commonly used to \"normalize\" the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['transformed'] = np.log1p(data['value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_stats = pd.DataFrame(columns=['sensor', 'metric', 'old_mu', 'new_mu', 'old_sigma', 'new_sigma'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for sensor, metric in sensors_summary.index:\n    #Fetching data\n    sub_data = data[(data['sensor']==sensor) & (data['metric']==metric)]\n    \n    #Plotting distirbutions before and after \"side by side\"\n    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n    sns.distplot(sub_data['value'], ax=ax[0]).set_title('{}: {} - Original distribution'.format(sensor, metric))\n    plt.xlabel('Original value')\n    sns.distplot(sub_data['transformed'], ax=ax[1], color='g').set_title('{}: {} - Transformed distribution'.format(sensor, metric))\n    plt.xlabel('Transformed value')\n    plt.show()\n    \n    # Saving distribution info into the stats df\n    data_stats.loc[len(data_stats)] = {'sensor': sensor, 'metric':metric, 'old_mu': norm.fit(sub_data['value'])[0], 'new_mu':norm.fit(sub_data['transformed'])[0], 'old_sigma': norm.fit(sub_data['value'])[1], 'new_sigma': norm.fit(sub_data['transformed'])[1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The comparison before and after log1p transormation show high decrease in mean and variance values which proves the transformation function is properly selected.\n\nThe highest values are for the TravelTime metric. If some improvement is needed in the future, it can be done by finding exclusive function for the metric.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Anomaly detection\n\nThe plan for this chapter:\n- 2D sensors (sensors with two metrics)\n    - Plot the data on a 2D plot\n    - For one of the sensors find the best algorithm for AD\n    - Plot decision boundary\n    - Perform anomaly detection on the second sensor\n- 1D sensors\n    - Perform AD for 1D\n    - Plot boundary \n\n\nThe work started by plotting the data for the t4013 sensor in the original and transformed scale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetching the data\ndata_t4013 = pd.merge(data[(data['sensor']=='t4013') & (data['metric']=='speed')], data[(data['sensor']=='t4013') & (data['metric']=='occupancy')], on='timestamp').drop_duplicates().filter(['value_x', 'value_y'])\ndata_t4013.columns = ['speed','occupancy']\ndata_t4013.reset_index(drop=True, inplace=True)\n\ndata_6005 = pd.merge(data[(data['sensor']=='6005') & (data['metric']=='speed')], data[(data['sensor']=='6005') & (data['metric']=='occupancy')], on='timestamp').drop_duplicates().filter(['value_x', 'value_y'])\ndata_6005.columns = ['speed','occupancy']\ndata_6005.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_2d(data, title):\n    plt.figure(figsize=(10,8))\n    ax = sns.scatterplot(data['occupancy'], data['speed'])\n    ax.set_title(title)\n    plt.xlabel('Average occupancy')\n    plt.ylabel('Average speed')\n    plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_2d(data_t4013, 'Sensor readings for the t4013 sensor in the original scale')\nplot_2d(data_6005, 'Sensor readings for the 6005 sensor in the original scale')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Readings from the sensors have the same pattern for the main cluster. The t4013 has more outliners. Thus the sensor will be used for finding the best model for anomaly detection.\n\nThe data in the transformed scale is visualized below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_t4013 = pd.merge(data[(data['sensor']=='t4013') & (data['metric']=='speed')], data[(data['sensor']=='t4013') & (data['metric']=='occupancy')], on='timestamp').drop_duplicates().filter(['transformed_x', 'transformed_y'])\ndata_t4013.columns = ['speed','occupancy']\ndata_t4013.reset_index(drop=True, inplace=True)\n\ndata_6005 = pd.merge(data[(data['sensor']=='6005') & (data['metric']=='speed')], data[(data['sensor']=='6005') & (data['metric']=='occupancy')], on='timestamp').drop_duplicates().filter(['transformed_x', 'transformed_y'])\ndata_6005.columns = ['speed','occupancy']\ndata_6005.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot_2d(data_t4013, 'Sensor readings for the t4013 sensor in the transformed scale')\nplot_2d(data_6005, 'Sensor readings for the 6005 sensor in the transformed scale')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the transformed scale plots decision boundary is easier to imagine and has simpler boundary which might be fitted by an ellipse. So it was decided to try the EllipticEvelope first.\n\nThe model fits an ellipse to the dataset. The position and size of the ellipse is determined to maintain selected contamination - the percentage of abnormal observations in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.covariance import EllipticEnvelope","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_pred_labels(pred):\n    for i in range(len(pred)):\n        if pred[i] == -1:\n            pred[i] = 'abnormal'\n        else:\n            pred[i] = 'normal'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t4013_elliptic = EllipticEnvelope(contamination=0.05)\nt4013_elliptic.fit(data_t4013)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.Series(t4013_elliptic.predict(data_t4013))\nupdate_pred_labels(pred)\nprint(pred.value_counts())\nsns.scatterplot(x=data_t4013['occupancy'], y=data_t4013['speed'], hue=pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting the model for various contamination values:","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for cutoff in np.arange(0.03, 0.12, 0.01):\n    t4013_elliptic = EllipticEnvelope(contamination=cutoff)\n    t4013_elliptic.fit(data_t4013)\n    pred = pd.Series(t4013_elliptic.predict(data_t4013))\n    update_pred_labels(pred)\n    print(pred.value_counts())\n    ax = sns.scatterplot(x=data_t4013['occupancy'], y=data_t4013['speed'], hue=pred)\n    ax.set_title('Elliptic for contamination = {:.2f}'.format(cutoff))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best value for cutoff is 0.06","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.patches as patches\n\n#Retrain the model\ncutoff = 0.06\nt4013_elliptic = EllipticEnvelope(contamination=cutoff)\nt4013_elliptic.fit(data_t4013)\npred = pd.Series(t4013_elliptic.predict(data_t4013))\nupdate_pred_labels(pred)\n\n#Plot the result\nplt.figure(figsize=(8, 6))\nax = sns.scatterplot(x=data_t4013['occupancy'], y=data_t4013['speed'], hue=pred)\nax.set_title('Elliptic for contamination = {:.2f}'.format(cutoff))\n\n#Create a rectangle\nax.add_patch(patches.Rectangle(\n        xy=(0.38, 3.85),\n        width=0.4,\n        height=0.55,\n        linewidth=1,\n        color = 'r',\n        fill = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model fails to fit the dataset in multiple places. Especially in the region marked in the red rectangle. This is caused by the data being too far from an ideal normal distribution - even after the transformation.\n\nThe IsolationForest is known for its ability to adapt to complex shapes. So it was selected for the next model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for contamination in [0.015, 0.05]:\n    \n    #Training the model\n    t4013_isolation = IsolationForest(n_estimators=1010, max_features=2, contamination=contamination, bootstrap=False)\n    t4013_isolation.fit(data_t4013)\n\n    #Predicting the result\n    pred = pd.Series(t4013_isolation.predict(data_t4013))\n    update_pred_labels(pred)\n\n    #Plotting\n    print(pred.value_counts())\n    plt.figure(figsize=(10, 7))\n    ax = sns.scatterplot(x=data_t4013['occupancy'], y=data_t4013['speed'], hue=pred)\n    ax.set_title('IsolationForest for the t4013 sensor with contamination {}'.format(contamination))\n    \n    #Create left rectangle\n    ax.add_patch(patches.Rectangle(\n        xy=(0.38, 3.85),\n        width=0.4,\n        height=0.55,\n        linewidth=1,\n        color = 'r',\n        fill = False))\n    \n    #Create right rectangle\n    ax.add_patch(patches.Rectangle(\n        xy=(0.76, 4.275),\n        width=0.25,\n        height=0.1,\n        linewidth=1,\n        color = 'k',\n        fill = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite trying various hyperparameters, it was impossible to properly match most of the items in the red rectangle as normal and items in the black rectangle as abnormal. \n\nTaking into consideration some other anomalies on the results, it was decided to try another model - OneClassSVM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import OneClassSVM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the SVM model is based on distance calculations, it was decided to rescale the data to improve the SVM model performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\ndata_t4013_sc = sc.fit_transform(data_t4013)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for cutoff in np.arange(0.02, 0.06, 0.01):\n    t4013_SVM = OneClassSVM(nu=cutoff)\n    t4013_SVM.fit(data_t4013_sc)\n    pred = pd.Series(t4013_SVM.predict(data_t4013_sc))\n    update_pred_labels(pred)\n    print(pred.value_counts())\n    plt.figure(figsize=(9,6))\n    ax = sns.scatterplot(x=data_t4013['occupancy'], y=data_t4013['speed'], hue=pred)\n    ax.set_title('One-Class SVM for contamination = {:.3f}'.format(cutoff))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SVM model seems to fit the dataset the best of all models tested. The contamination equal to 0.03 seems to fit the issue the best.\n\nThe next step was to plot the decision boundary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_t4013_sc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t4013_SVM = OneClassSVM(nu=0.03, kernel='rbf', tol=1e-10, gamma='auto')\nt4013_SVM.fit(data_t4013_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.Series(t4013_SVM.predict(data_t4013_sc))\nupdate_pred_labels(pred)\nprint(pred.value_counts())\n\nplt.figure(figsize=(10,8))\nax = sns.scatterplot(x=data_t4013_sc[:, 1], y=data_t4013_sc[:, 0], hue=pred)\nax.set_title('The decision boundary for the SingleClassSVM model on scaled data for the t4013 sensor')\nax.set(xlabel='occupancy', ylabel='speed')\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nxx, yy = np.meshgrid(np.linspace(-3, 3, 300),\n                     np.linspace(-3, 3, 300))\nZ = t4013_SVM.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contour(yy, xx, Z, levels=[0], linewidths=1, colors='black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Single abnormal spot can be found in the boundary of the main cluster. It's a downside of an SVM model. Modifying the hyperparameters to avoid the issue leads to deterioration of the decision boundary. The spot can be removed by adding data into the spot location.\n\nPlotting the decision boundary on the original values:","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"xx_yy = np.array([])\nfor i in range(len(xx.ravel())):\n    pair = np.array([xx.ravel()[i], yy.ravel()[i]])\n    pair = sc.inverse_transform(pair)\n    xx_yy = np.concatenate([xx_yy, pair])\n\nxx_yy = xx_yy.reshape(-1, 2)\nxx_yy = np.expm1(xx_yy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.scatterplot(x=np.expm1(data_t4013['occupancy']), y=np.expm1(data_t4013['speed']), hue=pred)\nax.set_title('The decision boundary for the SingleClassSVM model on the original data for t4013')\nax.set(xlabel='occupancy', ylabel='speed')\nplt.xlim(-5, 50)\nplt.ylim(10, 85)\n\nplt.contour(xx_yy[:,1].reshape(300, 300), xx_yy[:,0].reshape(300, 300), Z, levels=[0], linewidths=1, colors='black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The steps were recreated for the second sensor with two metrics: 6005.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\ndata_6005_sc = sc.fit_transform(data_6005)\n\ns6005_SimpleSVM = OneClassSVM(nu=0.03, tol=1e-8)\ns6005_SimpleSVM.fit(data_6005_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.Series(s6005_SimpleSVM.predict(data_6005_sc))\nupdate_pred_labels(pred)\nprint(pred.value_counts())\n\nplt.figure(figsize=(8,6))\nax = sns.scatterplot(x=data_6005_sc[:, 1], y=data_6005_sc[:, 0], hue=pred)\nax.set_title('The decision boundary for the SingleClassSVM model on the transformed data for the 6005 sensor')\nax.set(xlabel='occupancy', ylabel='speed')\nplt.xlim(-3, 3)\nplt.ylim(-4, 3)\nxx, yy = np.meshgrid(np.linspace(-4, 3, 300),\n                     np.linspace(-4, 3, 300))\nZ = s6005_SimpleSVM.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contour(yy, xx, Z, levels=[0], linewidths=1, colors='black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx_yy = np.array([])\nfor i in range(len(xx.ravel())):\n    pair = np.array([xx.ravel()[i], yy.ravel()[i]])\n    pair = sc.inverse_transform(pair)\n    xx_yy = np.concatenate([xx_yy, pair])\n\nxx_yy = xx_yy.reshape(-1, 2)\nxx_yy = np.expm1(xx_yy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nax = sns.scatterplot(x=np.expm1(data_6005['occupancy']), y=np.expm1(data_6005['speed']), hue=pred)\nax.set_title('The decision boundary for  the SingleClassSVM model on the original data for the 6005 sensor')\nax.set(xlabel='occupancy', ylabel='speed')\nplt.xlim(-5, 25)\nplt.ylim(15, 115)\n\nplt.contour(xx_yy[:,1].reshape(300, 300), xx_yy[:,0].reshape(300, 300), Z, levels=[0], linewidths=1, colors='black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The abnormal spots in the main normal cluster are noticeable, but in general, the decision boundary is marked correctly.  \n\nThe rest of the sensors have only one metric. In this case, model selection is less crucial. ElipticEvelope was selected as it allows to easily read decision boundary, which was important for the illustation purposes. ","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"all_single_sensors = ['387', '451', '7578']\nsingle_sensors_boundaries = pd.DataFrame(columns=['sensor', 'metric', 'low', 'high'])\n\nfor single_sensor in all_single_sensors:\n    #Fetching data and fitting the model\n    single_data = np.array(data[data['sensor']==single_sensor]['transformed']).reshape(-1, 1)\n    single_metric = data[data['sensor']==single_sensor]['metric'].iloc[0]\n    single_model = EllipticEnvelope(contamination=0.05)\n    single_model.fit(single_data)\n    pred = single_model.predict(single_data)\n    \n    #Finding the decision boundary, in this 1D case low and high values are enough\n    xx = np.linspace(min(single_data), max(single_data))\n    pred_xx = single_model.predict(xx)\n    for i in range(len(xx)-1):\n        if pred_xx[i] < pred_xx[i+1]:\n            low = xx[i][0]\n        if pred_xx[i] > pred_xx[i+1]:\n            high = xx[i][0]\n            break\n    \n    #Saving boundary values into table\n    single_sensors_boundaries = single_sensors_boundaries.append( {'sensor':single_sensor, 'metric':single_metric, 'low':np.expm1(low), 'high':np.expm1(high)}, ignore_index=True)\n    \n    #Plotting boundary using transformed data\n    plt.figure(figsize=(8,6))\n    ax = sns.distplot(single_data)\n    ax.axvline(low, 0, 0.3, color='#FF6600', linewidth=3)\n    ax.axvline(high, 0, 0.3, color='#FF6600', linewidth=3)\n    ax.set_title('Anomaly boundaries on the tranformed data for the {} sensor'.format(single_sensor))\n    ax.set(xlabel=single_metric)\n\n    #Plotting boundary using original scale\n    plt.figure(figsize=(8,6))\n    ax = sns.distplot(np.expm1(single_data))\n    ax.axvline(np.expm1(low), 0, 0.4, color='#FF6600', linewidth=3)\n    ax.axvline(np.expm1(high), 0, 0.4, color='#FF6600', linewidth=3)\n    ax.set_title('Anomaly boundaries on original data for the {} sensor'.format(single_sensor))\n    ax.set(xlabel=data[data['sensor']==single_sensor]['metric'].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The way the boundaries are determined is easier to understand while looking on the transformed data. However, the boundaries on the original data may be more useful in real case scenarios.\n\nPlease note that detecting anomalies on a single measurement should not be considered as a highly reliable method. More metrics on these sensors would increase confidence in the detection mechanism.\n\nThe table with the low and high boundaries for normal samples:","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"single_sensors_boundaries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The speed measurement is the only metric with is intersecting with 2D sensors. The boundary values for this metric looks comparable between 1D and 2D analysis. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}