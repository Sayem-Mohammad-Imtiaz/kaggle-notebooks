{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['species'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sepal_length = data['sepal_length']\npetal_length = data['petal_length']\nsepal_width=data['sepal_width']\npetal_width=data['petal_width']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#density distribution of petal length, petal width, sepal length, sepal width of Iris-setosa\n\niris_setosa=data[data['species'].str.contains('Iris-setosa')]\niris_versicolor=data[data['species'].str.contains('Iris-versicolor')]\niris_virginica=data[data['species'].str.contains('Iris-virginica')]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Density Plot for Sepal Length and Sepal Width","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(iris_setosa['sepal_length'],iris_setosa['sepal_width'], \n            color='r', shade=True, Label='Iris_Setosa', \n            cmap=\"Reds\", shade_lowest=False).set_title('Density distribution of Sepal Length and Sepal Width of Iris Setosa')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Density Plot for Petal Length and Petal Width","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.kdeplot(iris_setosa['petal_length'],iris_setosa['petal_width'], \n            color='g', shade=True, Label='Iris_Setosa', \n            cmap=\"Greens\", shade_lowest=False)\nax = sns.kdeplot(iris_versicolor['petal_length'],iris_versicolor['petal_width'], \n            color='b', shade=True, Label='Iris_Versicolor', \n            cmap=\"Reds\", shade_lowest=False).set_title('Density distribution of Petal length and Petal Width of Iris-Versicolor and Iris-Setosa')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Density distribution of Sepal Length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.kdeplot(data['sepal_length'],shade=True,color=\"g\").set_title('Density distribution of Sepal Length')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Density distribution of Iris Setosa","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_setosa.plot.density(title = 'Density distribution plot of Iris Setosa')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Density distribution of all flowers with all 4 attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot.density(title='Density distribution of all the flowers',grid='true')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Histogram for Petal Length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['petal_length'],kde = False).set_title('Histogram for petal length')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Petal length of all flowers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x=\"petal_length\",data=data).set_title('Petal length distribution of all flowers')\nrcParams['figure.figsize'] = 20,8.27\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sepal Length of 3 species","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"species\",y=\"sepal_length\",data=data).set_title(\"Sepal Length of three species\")\nrcParams['figure.figsize'] = 10,8.27","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sepal Length and width, Petal length and width of 3 flowers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=data).set_title(\"Distribution of Sepal_length, Sepal_width, petal_length and petal_width of 3 flowers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sepal width of 3 flowers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=data,x=\"species\",y=\"sepal_width\").set_title(\"sepal_width distribution of three flowers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sepal Length and width","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=data.sepal_length,y=data.sepal_width,hue=data.species).set_title(\"Sepal length and Sepal width distribution of three flowers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Petal length and width","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\nax = sns.scatterplot(x=\"petal_length\", y=\"petal_width\",hue=\"species\",size=\"species\",sizes=(20,200),legend=\"full\",data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sepal width of 3 species","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=\"species\", y = \"sepal_width\",data=data, palette=\"muted\").set_title(\"sepal width of 3 species\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Petal width of 3 species","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=\"species\", y = \"petal_width\",data=data, palette=\"muted\").set_title(\"petal width of 3 species\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Petal length and width of 3 species","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=\"petal_length\", y=\"petal_width\", hue=\"species\",\n                  data=data).set_title(\"Distribution of petal length and petal width of the 3 species\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pairplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(data, hue=\"species\", palette=\"husl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pairplot - Sepal Length and width","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, vars=[\"sepal_width\", \"sepal_length\"],diag_kind=\"kde\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data,x_vars=[\"sepal_width\", \"sepal_length\"],y_vars=[\"petal_width\", \"petal_length\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Corelation Matrix of all attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr()\nsns.heatmap(data.corr(),center=0).set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Corelation of attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(), annot=True, fmt=\"f\").set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(), cmap=\"YlGnBu\").set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#corr = np.corrcoef(np.random.randn(10, 200))\nmask = np.zeros_like(data.corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.heatmap(data.corr(), mask=mask, vmax=.3, square=True).set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data.iloc[:,0:4].values\ny=data.iloc[:,4].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train and Test split\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_score\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML Algorithms - Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlg_class=LogisticRegression(random_state=0)\nlg_class.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_logit=lg_class.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_logit.flatten()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_test.flatten(),y_pred_logit.flatten())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot decision boundary for Logistic Regression classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nlogreg = LogisticRegression(C=1e5)\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix - Metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_logit)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(lg_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score - Metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_logistic=accuracy_score(y_test, y_pred_logit)\nacc_logistic","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Report - Metrics Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_logit, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nimport sklearn.metrics as metrics\na_index = list(range(1,11))\na = pd.Series()\nx = [1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    k_class=KNeighborsClassifier(n_neighbors=5) \n    k_class.fit(X_train,y_train)\n    y_pred_neigh=k_class.predict(X_test)\n    a=a.append(pd.Series(metrics.accuracy_score(y_pred_neigh,y_test)))\nplt.plot(a_index, a)\nplt.title(\"KNN Prediction\")\nplt.xticks(x)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Boundary of Neighbour classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN \nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_neigh.flatten()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_neigh)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(k_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_knn=metrics.accuracy_score(y_test, y_pred_neigh)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_neigh, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_class=SVC(kernel='linear',random_state=0)\nsvm_class.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svc=svm_class.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_svc.flatten()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_svc)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(svm_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy Score - metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_svm=metrics.accuracy_score(y_test, y_pred_svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Report metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_svc, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision boundary plot of SVM Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm, datasets\n\n\ndef make_meshgrid(x, y, h=.02):\n    \"\"\"Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    \"\"\"\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    \"\"\"Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    \"\"\"\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n# import some data to play with\niris = datasets.load_iris()\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (svm.SVC(kernel='linear', C=C),\n          svm.LinearSVC(C=C, max_iter=10000),\n          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = ('SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=plt.cm.coolwarm, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel('Sepal length')\n    ax.set_ylabel('Sepal width')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nn_class=GaussianNB()\nn_class.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_bayes=n_class.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_bayes.flatten()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_test.flatten(),y_pred_bayes.flatten())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix - metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_bayes)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(n_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score - metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_bayes=metrics.accuracy_score(y_test, y_pred_bayes)\nacc_bayes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Report - Metrics Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_bayes, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nd_class=DecisionTreeClassifier(criterion='entropy')\nmodel_all_params = d_class.fit(X_train,y_train)\nplt.figure(figsize = (20,10))\nplot_tree(model_all_params,filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_tree=d_class.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot decision surface boundary for decision tree in Iris dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# import the needed dataset.\nfrom sklearn.datasets import load_iris\n# Import the model and an additional visualization tool.\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n# Define a variable to establish three classes/species.\nclass_count = 3\n# Define standard RGB color scheme for visualizing ternary classification in order to match the color map used later.\nplot_colors = 'brg'\n# Define marker options for plotting class assignments of training data.\nmarkers = 'ovs'\n# We also need to establish a resolution for plotting.  I favor clean powers of ten, but this is not by any means a hard and fast rule.\nplot_res = 0.01\n\n# Load the iris dataset from scikit-learn (note the use of from [library] import [function] above)\niris = load_iris()\n\n# Set the size of the figure used to contain the subplots to be generated.\nplt.figure(figsize=(20,10))\n\n# Create an empty list of models to store the results of each pairwise model fit.\nmodels = []\n\n# Use enumerate() to define the possible pairs of features available and iterate over each pair.\nfor pair_index, pair in enumerate([[0, 1], [0, 2], [0, 3], \n                                           [1, 2], [1, 3], \n                                                   [2, 3] ]):\n\n    # We only take the two features corresponding to the pair in question...\n    X, y = iris.data[:, pair] , iris.target\n    \n    # ... to fit the decision tree classifier model.\n    model = DecisionTreeClassifier().fit(X, y)\n    \n    # Append the results to the models list\n    models.append(model)\n    \n    # Establish a two row by three column subplot array for plotting.\n    plt.subplot(2, 3, pair_index + 1)\n    \n    # Define appropriate x and y ranges for each plot...\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    # ... and use each range to define a meshgrid to use as the plotting area.\n    xx, yy = np.meshgrid(np.arange(x_min, \n                                   x_max, \n                                   plot_res),\n                         np.arange(y_min, \n                                   y_max, \n                                   plot_res) )\n    # Use plt.tight_layout() to establish spacing of the subplots.\n    plt.tight_layout(h_pad = 0.5, \n                     w_pad = 0.5, \n                       pad = 4.0 )\n    \n    # Predict the classification of each point in the meshgrid based on the calculated model above.\n    # The numpy methods .c_() and .ravel() reshape our meshgrid values into a format compatible with our model.predict() method,\n    Z = model.predict(np.c_[xx.ravel(), \n                            yy.ravel() ])\n    # Reshape the predictions to match xx...\n    Z = Z.reshape(xx.shape)\n    # ... and prepare a contour plot that reflects the predictions .\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.brg)\n    \n    # Define the subplot axis labels after title casing while preserving case on the unit of measure \n    plt.xlabel(iris.feature_names[pair[0]].title()[0:-4] + iris.feature_names[pair[0]][-4:])\n    plt.ylabel(iris.feature_names[pair[1]].title()[0:-4] + iris.feature_names[pair[1]][-4:])\n    \n    # Plot the training points for each species in turn\n    for i, color, marker in zip(range(class_count), plot_colors, markers):\n        # Subset the data to the class in question with the np.where() method\n        index = np.where(y == i)\n        # Plot the class in question on the subplot\n        plt.scatter(X[index, 0], \n                    X[index, 1], \n                    c = color,\n                    marker = marker,\n                    label = iris.target_names[i],\n                    cmap = plt.cm.brg, \n                    edgecolor = 'black', \n                    s = 15                       )\n\n# Define a title for the overall collection of subplots after each subplot is fully defined\nplt.suptitle('Decision Surface of a Decision Tree Using Paired Features',\n             size = 24                                                   )\n\n# Define the legend for the subplot collection\nplt.legend(loc = 'lower right',\n           fontsize = 16,\n           borderpad = 0.1, \n           handletextpad = 0.1 )\n\n# Set limits just large enough to show everything cleanly\nplt.axis(\"tight\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decdf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_tree.flatten()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix - metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_tree)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(d_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score - metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_dec=accuracy_score(y_test, y_pred_tree)\nacc_dec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Report - Metrics evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_tree, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nran_class=RandomForestClassifier(n_estimators=10,criterion='entropy')\nran_class.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_forest=ran_class.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nclass_names = ['Iris-setosa','Iris-versicolor','Iris-virginica']\nca=confusion_matrix(y_test,y_pred_forest)\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp=plot_confusion_matrix(ran_class,X_train, y_train,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)\n    disp.ax_.set_title(title)\n    print(title)\n    print(disp.confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy score metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy_score\nfrom sklearn.metrics import accuracy_score\nacc_ran=metrics.accuracy_score(y_test, y_pred_forest)\nacc_ran","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classification Report metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification_report\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred_forest, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randf=pd.DataFrame({\"Actual\": y_test.flatten(), \"Predicted\": y_pred_forest.flatten()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot decision surface boundary for Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nn_estimators = 30\ncmap = plt.cm.RdYlBu\nplot_step = 0.02  # fine step width for decision surface contours\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\nRANDOM_SEED = 13  # fix the seed on each iteration\n\n# Load data\niris = load_iris()\n\nplot_idx = 1\n\nmodels = [DecisionTreeClassifier(max_depth=None),\n          RandomForestClassifier(n_estimators=n_estimators),\n          ExtraTreesClassifier(n_estimators=n_estimators),\n          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n                             n_estimators=n_estimators)]\n\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model in models:\n        # We only take the two corresponding features\n        X = iris.data[:, pair]\n        y = iris.target\n\n        # Shuffle\n        idx = np.arange(X.shape[0])\n        np.random.seed(RANDOM_SEED)\n        np.random.shuffle(idx)\n        X = X[idx]\n        y = y[idx]\n\n        # Standardize\n        mean = X.mean(axis=0)\n        std = X.std(axis=0)\n        X = (X - mean) / std\n\n        # Train\n        model.fit(X, y)\n\n        scores = model.score(X, y)\n        # Create a title for each column and the console by using str() and\n        # slicing away useless parts of the string\n        model_title = str(type(model)).split(\n            \".\")[-1][:-2][:-len(\"Classifier\")]\n\n        model_details = model_title\n        if hasattr(model, \"estimators_\"):\n            model_details += \" with {} estimators\".format(\n                len(model.estimators_))\n        print(model_details + \" with features\", pair,\n              \"has a score of\", scores)\n\n        plt.subplot(3, 4, plot_idx)\n        if plot_idx <= len(models):\n            # Add a title at the top of each column\n            plt.title(model_title, fontsize=9)\n\n        # Now plot the decision boundary using a fine mesh as input to a\n        # filled contour plot\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                             np.arange(y_min, y_max, plot_step))\n\n        # Plot either a single DecisionTreeClassifier or alpha blend the\n        # decision surfaces of the ensemble of classifiers\n        if isinstance(model, DecisionTreeClassifier):\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n        else:\n            # Choose alpha blend level with respect to the number\n            # of estimators\n            # that are in use (noting that AdaBoost can use fewer estimators\n            # than its maximum if it achieves a good enough fit early on)\n            estimator_alpha = 1.0 / len(model.estimators_)\n            for tree in model.estimators_:\n                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n\n        # Build a coarser grid to plot a set of ensemble classifications\n        # to show how these are different to what we see in the decision\n        # surfaces. These points are regularly space and do not have a\n        # black outline\n        xx_coarser, yy_coarser = np.meshgrid(\n            np.arange(x_min, x_max, plot_step_coarser),\n            np.arange(y_min, y_max, plot_step_coarser))\n        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n                                         yy_coarser.ravel()]\n                                         ).reshape(xx_coarser.shape)\n        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n                                c=Z_points_coarser, cmap=cmap,\n                                edgecolors=\"none\")\n\n        # Plot the training points, these are clustered together and have a\n        # black outline\n        plt.scatter(X[:, 0], X[:, 1], c=y,\n                    cmap=ListedColormap(['r', 'y', 'b']),\n                    edgecolor='k', s=20)\n        plot_idx += 1  # move on to the next plot in sequence\n\nplt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\nplt.axis(\"tight\")\nplt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy score output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dict={'Logistic_Regression' : [acc_logistic],\n     'KNN' : [acc_knn],\n     'SVM' : [acc_svm],\n     'Naive_Bayes' : [acc_bayes],\n     'Decision_Tree' : [acc_dec],\n     'Random_Forest' : [acc_ran]\n     }\nmodels = pd.DataFrame.from_dict(dict,orient='index')\nmodels.transpose()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.to_csv('mycsvfile.csv',index=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification report of all the ML Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Classification_Report of Logistic Regression : \\n\",classification_report(y_test, y_pred_forest, target_names=target_names))\nprint(\"Classification_Report of SVM : \\n\",classification_report(y_test, y_pred_svc, target_names=target_names))\nprint(\"Classification_Report of KNN : \\n\",classification_report(y_test, y_pred_neigh, target_names=target_names))\nprint(\"Classification_Report of Naive Bayes : \\n\",classification_report(y_test, y_pred_bayes, target_names=target_names))\nprint(\"Classification_Report of Decision Tree : \\n\",classification_report(y_test, y_pred_tree, target_names=target_names))\nprint(\"Classification_Report of Random Forest : \\n\",classification_report(y_test, y_pred_forest, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross_val_score of ML Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Logistic Regression: \")\nprint(cross_val_score(lg_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(lg_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression train set is: \" , accuracy)\nprint(cross_val_score(lg_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(lg_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression test set is: \" , accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"KNN: \")\nprint(cross_val_score(k_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(k_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of KNN train set is: \" , accuracy)\nprint(cross_val_score(k_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(k_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression test set is: \" , accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"SVM: \")\nprint(cross_val_score(svm_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(svm_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of SVM train set is: \" , accuracy)\nprint(cross_val_score(svm_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(svm_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of SVM test set is: \" , accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Naive Bayes: \")\nprint(cross_val_score(n_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(n_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Naive Bayes train set is: \" , accuracy)\nprint(cross_val_score(n_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(n_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Naive Bayes test set is: \" , accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Decision Tree: \")\nprint(cross_val_score(d_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(d_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Decision Tree train set is: \" , accuracy)\nprint(cross_val_score(d_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(d_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Decision Tree test set is: \" , accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Random Forest Classification: \")\nprint(cross_val_score(ran_class, X_train, y_train, scoring='accuracy', cv = 10))\naccuracy = cross_val_score(ran_class, X_train, y_train, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression train set is: \" , accuracy)\nprint(cross_val_score(ran_class, X_test, y_test, scoring='accuracy', cv = 10))\naccuracy_test = cross_val_score(ran_class, X_test, y_test, scoring='accuracy', cv = 10).mean() * 100\nprint(\"Accuracy of Logistic regression test set is: \" , accuracy_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection - KFold for SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\nscores = []\nbest_svr = SVR(kernel='rbf')\ncv = KFold(n_splits=10, random_state=42, shuffle=False)\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n    best_svr.fit(X_train, y_train)\n    scores.append(best_svr.score(X_test, y_test))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"d = data.iloc[:,0:4].values\nd\n#X=data.iloc[:,0:4].values\n#y=data.iloc[:,4].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss={}\nfor i in range(1,10):\n    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=1000).fit(d)\n    wcss[i]=kmeans.inertia_\nplt.figure()\nplt.plot(list(wcss.keys()),list(wcss.values()))\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3).fit(d)\ncentroids = kmeans.cluster_centers_\ny_kmeans=kmeans.fit_predict(d)\nprint(centroids)\nplt.scatter(d[y_kmeans==0,0], d[y_kmeans==0,1], c= 'red',s=50, label='Cluster1')\nplt.scatter(d[y_kmeans==1,0], d[y_kmeans==1,1], c= 'blue',s=50, label='Cluster2')\nplt.scatter(d[y_kmeans==2,0], d[y_kmeans==2,1], c= 'green',s=50, label='Cluster3')\nplt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=50,label='centroids')\nplt.title('Cluster of flowers')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hierarchical Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#from scipy.cluster.hierarchy import dendrogram, linkage\n\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize=(30, 17))  \nplt.title(\"Dendrograms\")\ndend = shc.dendrogram(shc.linkage(d, method='ward'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \ny_hc=cluster.fit_predict(d)\nplt.scatter(d[y_hc==0,0], d[y_hc==0,1], c= 'red',s=100, label='Cluster1')\nplt.scatter(d[y_hc==1,0], d[y_hc==1,1], c= 'blue',s=100, label='Cluster2')\nplt.scatter(d[y_hc==2,0], d[y_hc==2,1], c= 'green',s=100, label='Cluster3')\nplt.title('Cluster of flowers')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}