{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset contains all bitcoin exchanges for the time period of Jan 2012 to September 2020, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields filled with Nan's."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping empty rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(axis=0,how='any',inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting 'Timestamp' to date-time format and setting it as the index of our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Timestamp'] = [datetime.fromtimestamp(x) for x in df['Timestamp']]\ndf = df.set_index(['Timestamp'])\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I am selecting only those rows from where the 'Weighted_Price' is greater than 2500 because before the price 2500 the data was almost stationary and it would effect the model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[df['Weighted_Price'] >2500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df['Weighted_Price']\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(data)\nplt.title('Visualising Weighted Prices over time',fontsize=18)\nplt.xlabel('Time',fontsize=15)\nplt.ylabel('Price value',fontsize=15)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalising values using Standard Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\ndata = sc.fit_transform(np.array(data).reshape(-1,1))\n\ndata[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting our data into training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_size = int(len(data)*0.65)\n\ntraining_data = data[0:training_size,:]\ntesting_data = data[training_size:,:]\n\nprint(training_data.shape)\nprint(testing_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Arranging the data in such a way that the value of current day depends on the previous value.\n\nHere two lists are created for X and Y.The first value of the dataset is added in X and the second value is added in Y because our current prediction depends on the  previous value i.e the timestamp here is 1.Now as the loop continues the second values is then appended in X and third value in Y and so on.Once we reach at the end of the loop the last value of X will be the second last value of our dataset and last value of Y will be the last value of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(data,timestamp=1):\n    X=[]\n    Y=[]\n    \n    for i in range(len(data)-timestamp-1):\n        a = data[i:(i+timestamp)]\n        X.append(a)\n        \n        Y.append(data[i+timestamp])\n        \n    return np.array(X),np.array(Y)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timestamp=1\nX_train,Y_train = create_dataset(training_data,timestamp)\nx_test,y_test = create_dataset(testing_data,timestamp)\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[:6])\n\nprint(Y_train[:6])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating LSTM model for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import models\nfrom tensorflow.keras.layers import LSTM,Dense,Dropout\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\n\nmodel.add(LSTM(50,return_sequences = True,input_shape = (5,1),activation='relu'))\nmodel.add(LSTM(25,return_sequences = True))\nmodel.add(LSTM(16,return_sequences = True))\nmodel.add(LSTM(10,return_sequences = False))\n\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mean_squared_error')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(X_train,Y_train,epochs=10,batch_size=64,validation_split=0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist.history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualising training and validation loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nplt.plot(training_loss,label='training_loss')\nplt.plot(val_loss,label='val_loss')\nplt.legend()\nplt.title('Visualising loss',fontsize=18)\nplt.xlabel('Epochs',fontsize=15)\nplt.ylabel('Loss',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the predicted and original test values into their original form"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = sc.inverse_transform(y_test)\ny_pred = sc.inverse_transform(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_test,label='Original Values')\nplt.plot(y_pred,label='Predicted Values')\nplt.legend()\nplt.title('Original vs Predicted values curve',fontsize=18)\nplt.xlabel('Number of test points',fontsize=15)\nplt.ylabel('Weighted_Price',fontsize=15)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nscore = r2_score(y_test,y_pred)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see both the curves have overlapped and the r2_score is almost 1 which tells us that the model is able to generate good results.\n\nPlease give an upvote if you found this notebook useful and any other type of suggestions are most welcome."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}