{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!cd /kaggle/input\n!pip install pylint\n!pip install autopep8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, _ in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\nTERM_WIDTH = 100\nTOTAL_BAR_LENGTH = 20.\nlast_time = time.time()\nbegin_time = last_time\n\n\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(TERM_WIDTH-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(TERM_WIDTH-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nY_vals = []\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        Y_vals.append(out)\n        return out\n\n\ndef le_net():\n    return LeNet()\n\n\ndef test():\n    net = le_net()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''DenseNet in PyTorch.'''\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate,\n                               kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate,\n                               kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out, x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n\n        num_planes = 2*growth_rate\n        self.conv1 = nn.Conv2d(\n            3, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3]*growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n        self.linear = nn.Linear(num_planes, num_classes)\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef dense_net121():\n    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=32)\n\n\ndef dense_net169():\n    return DenseNet(Bottleneck, [6, 12, 32, 32], growth_rate=32)\n\n\ndef dense_net201():\n    return DenseNet(Bottleneck, [6, 12, 48, 32], growth_rate=32)\n\n\ndef dense_net161():\n    return DenseNet(Bottleneck, [6, 12, 36, 24], growth_rate=48)\n\n\ndef dense_net_cifar():\n    return DenseNet(Bottleneck, [6, 12, 24, 16], growth_rate=12)\n\n\ndef test():\n    net = dense_net_cifar()\n    x = torch.randn(1, 3, 32, 32)\n    y = net(x)\n    print(y)\n\n# test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Train CIFAR10 with PyTorch.'''\nfrom __future__ import print_function\n\nimport argparse\nimport os\nfrom datetime import datetime\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\n\n# pylint: disable=invalid-name,redefined-outer-name,global-statement\n\nbest_acc = 0 # best test accuracy\n\nparser = argparse.ArgumentParser(description='PyTorch CINIC10 Training')\nparser.add_argument('--data', metavar='DIR', default='cinic10',\n                    help='path to dataset (default: cinic-10)')\nparser.add_argument('-a', '--arch', metavar='ARCH', default='lenet',\n                    )\nparser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n                    help='number of data loading workers (default: 2)')\nparser.add_argument('--epochs', default=12, type=int, metavar='N',\n                    help='number of total epochs to run')\nparser.add_argument('-b', '--batch-size', default=64, type=int,\n                    metavar='N',\n                    help='mini-batch size (default: 64), this is the total '\n                         'batch size of all GPUs on the current node when '\n                         'using Data Parallel or Distributed Data Parallel')\nparser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n                    metavar='LR', help='initial learning rate', dest='lr')\nparser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                    help='momentum')\nparser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n                    metavar='W', help='weight decay (default: 1e-4)',\n                    dest='weight_decay')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\n\nargs = parser.parse_args([\"--data\", \"cinic10\"])\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\n# Data loading code\nprint('==> Preparing data..')\n\n\ntraindir = os.path.join(args.data, 'train')\nvalidatedir = os.path.join(args.data, 'valid')\ntestdir = os.path.join(args.data, 'test')\ncinic_mean = [0.47889522, 0.47227842, 0.43047404]\ncinic_std = [0.24205776, 0.23828046, 0.25874835]\nnormalize = transforms.Normalize(mean=cinic_mean, std=cinic_std)\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n])\n\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=cinic_mean, std=cinic_std)\n])\n\ntrainset = datasets.ImageFolder(root=traindir, transform=train_transform)\ntrainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=args.batch_size,\n                                          shuffle=True,\n                                          num_workers=args.workers)\n\nvalidateset = datasets.ImageFolder(root=validatedir, transform=transform)\nvalidateloader = torch.utils.data.DataLoader(validateset,\n                                             batch_size=100,\n                                             shuffle=True,\n                                             num_workers=args.workers)\n\ntestset = datasets.ImageFolder(root=testdir, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset,\n                                         batch_size=100,\n                                         shuffle=True,\n                                         num_workers=args.workers)\n\nclasses = ('airplane', 'automobile', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n# Create Model\nprint('==> Creating model {}...'.format(args.arch))\nmodel = dense_net169().cuda()\n\n# Define loss function (criterion), optimizer and learning rate scheduler\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(),\n                            lr=args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\nscheduler = CosineAnnealingLR(optimizer=optimizer, T_max=args.epochs, eta_min=0)\n\n\ndef train(epoch):\n    ''' Trains the model on the train dataset for one entire iteration '''\n    print('\\nEpoch: %d' % epoch)\n    cudnn.benchmark = True\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        if args.cuda:\n            inputs, targets = inputs.cuda(), targets.cuda()\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef validate(epoch):\n    ''' Validates the model's accuracy on validation dataset and saves if better\n        accuracy than previously seen. '''\n    cudnn.benchmark = False\n    global best_acc\n    model.eval()\n    valid_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(validateloader):\n            if args.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            valid_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(validateloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                         % (valid_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print('Saving..')\n        state = {\n            'model': model.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, './checkpoint/ckpt.t7')\n        best_acc = acc\n\n\ndef test():\n    ''' Final test of the best performing model on the testing dataset. '''\n    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n    checkpoint = torch.load('./checkpoint/ckpt.t7')\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    print('Test best performing model from epoch {} with accuracy {:.3f}%'.format(\n        checkpoint['epoch'], checkpoint['acc']))\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            if args.cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\nstart_time = datetime.now()\nprint('Runnning training and test for {} epochs'.format(args.epochs))\n\n# Run training for specified number of epochs\nfor epoch in range(0, args.epochs):\n    scheduler.step()\n    train(epoch)\n    validate(epoch)\n\ntime_elapsed = datetime.now() - start_time\nprint('Training time elapsed (hh:mm:ss.ms) {}\\n'.format(time_elapsed))\n\n# Run final test on never before seen data\ntest()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}