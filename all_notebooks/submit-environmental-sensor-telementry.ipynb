{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/environmental-sensor-data-132k/iot_telemetry_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hour'] = pd.to_datetime(df['ts'],unit='s').dt.hour\ndf['minute'] = pd.to_datetime(df['ts'],unit='s').dt.minute\ndf['second'] = pd.to_datetime(df['ts'],unit='s').dt.second\ndf['microsecond'] = pd.to_datetime(df['ts'],unit='s').dt.microsecond","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting timestamp to hours, minutes, seconds and microseconds."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('ts', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need 'ts' column any more."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.device.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We have 3 devices."},{"metadata":{"trusted":true},"cell_type":"code","source":"codes, uniques = df.device.factorize()\nprint(codes)\nprint(uniques)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['deviceFactor'] = codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('device', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.motion == True].motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.motion == False].motion.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see set isn't balanced. Let's make more balanced set. We need near 480 samples in motion_false set."},{"metadata":{},"cell_type":"markdown","source":"Our model must predict motion. Recall(for negative samples) is more important than Precission."},{"metadata":{},"cell_type":"markdown","source":"Now we get 50 - 50 balanced sets. Recall(for negative samples) will be near 0.95."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true_train, df_true_test = train_test_split(df[df.motion == 1], test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true_train.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_big, df_false_lit = train_test_split(df[df.motion == 0], test_size = 0.00119, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_lit.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_train, df_false_test = train_test_split(df_false_lit, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_train.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_test.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train = pd.concat([df_true_train, df_false_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train[result_train.motion == True].motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_test = pd.concat([df_true_test, df_false_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_test.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_test[result_test.motion == True].motion.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay. Our set is balanced. Let's make X_train, y_train, X_test, y_test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = result_train.drop('motion', axis = 1)\ny_train = result_train.motion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = result_test.drop('motion', axis = 1)\ny_test = result_test.motion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('motion', axis = 1)\ny = df.motion","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's full set with all 400000 samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's how looking our set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree Classifier:')\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state = 42)\ntree.fit(X_train, y_train)\nprint('train score: ' + str(tree.score(X_train, y_train)))\nprint('test score: ' + str(tree.score(X_test, y_test)))\nfrom sklearn.metrics import confusion_matrix\ny_pred = tree.predict(X_test)\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y_test, y_pred)))\nmatrx = confusion_matrix(y_test, y_pred)\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))\nprint()\nfrom sklearn.metrics import confusion_matrix\ny_pred = tree.predict(X)\nprint('On full dataframe:')\nfrom sklearn.metrics import accuracy_score\nprint('accuracy: ' + str(accuracy_score(y, y_pred)))\nmatrx = confusion_matrix(y, y_pred)\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y, y_pred)))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest Classifier:')\nfrom  sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 42, n_estimators = 250)\nforest.fit(X_train, y_train)\nprint('train score: ' + str(forest.score(X_train, y_train)))\nprint('test score: ' + str(forest.score(X_test, y_test)))\nfrom sklearn.metrics import confusion_matrix\ny_pred = forest.predict(X_test)\nprint('confusion matrix:\\n' + str(confusion_matrix(y_test, y_pred)))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y_test, y_pred)))\nmatrx = confusion_matrix(y_test, y_pred)\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))\nprint()\nfrom sklearn.metrics import confusion_matrix\ny_pred = forest.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('On full dataframe:')\nfrom sklearn.metrics import accuracy_score\nprint('accuracy: ' + str(accuracy_score(y, y_pred)))\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y, y_pred)))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Gradient Boosting Classifier:')\nfrom sklearn.ensemble import GradientBoostingClassifier\nboost = GradientBoostingClassifier(random_state = 42, n_estimators = 500, learning_rate = 0.1)\nboost.fit(X_train, y_train)\nprint('train score: ' + str(boost.score(X_train, y_train)))\nprint('test score: ' + str(boost.score(X_test, y_test)))\nfrom sklearn.metrics import confusion_matrix\ny_pred = boost.predict(X_test)\nprint('confusion matrix:\\n' + str(confusion_matrix(y_test, y_pred)))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y_test, y_pred)))\nmatrx = confusion_matrix(y_test, y_pred)\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))\nprint()\nfrom sklearn.metrics import confusion_matrix\ny_pred = boost.predict(X)\nprint('On full dataframe:')\nfrom sklearn.metrics import accuracy_score\nprint('accuracy: ' + str(accuracy_score(y, y_pred)))\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y, y_pred)))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Time spended by tree')\ny_pred = tree.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y, y_pred))\nprint(matrx)\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Time spended by forest')\ny_pred = forest.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y, y_pred))\nprint(matrx)\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Time spended by gradient boosting classifier')\ny_pred = boost.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y, y_pred))\nprint(matrx)\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best classifier is RandomForest with recall = 0.9543568464730291.\nSecond place - GradientBoosting.\nBut forest spend the most time, gradient_boosting spend less, tree spend the least.\n"},{"metadata":{},"cell_type":"markdown","source":"Let's look for more simple classifiers. We will see this classifiers have more accuracy, but the worst roc_auc_score and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LogisticRegression:')\nfrom sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nlg.fit(X_train, y_train)\nprint('train set score: ', end='')\nprint(lg.score(X_train, y_train))\nprint('test set score: ', end='')\nprint(lg.score(X_test, y_test))\nprint('full dataframe score: ', end='')\nprint(lg.score(X, y))\nfrom sklearn.metrics import confusion_matrix\ny_pred = lg.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('confusion matrix:')\nprint(matrx)\nfrom sklearn.metrics import roc_auc_score\nprint('roc auc score: ', end='')\nprint(roc_auc_score(y, y_pred))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model doesn't \"see\" motion."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('SGDClassifier:')\nfrom sklearn.linear_model import SGDClassifier\nSGD = SGDClassifier()\nSGD.fit(X_train, y_train)\nprint('train set score: ', end='')\nprint(SGD.score(X_train, y_train))\nprint('test set score: ', end='')\nprint(SGD.score(X_test, y_test))\nprint('full dataframe score: ', end='')\nprint(SGD.score(X, y))\nfrom sklearn.metrics import confusion_matrix\ny_pred = SGD.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('confusion matrix:')\nprint(matrx)\nfrom sklearn.metrics import roc_auc_score\nprint('roc auc score: ', end='')\nprint(roc_auc_score(y, y_pred))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This too."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GaussianNB:')\nfrom sklearn.naive_bayes import GaussianNB\nGNB = GaussianNB()\nGNB.fit(X_train, y_train)\nprint('train set score: ', end='')\nprint(GNB.score(X_train, y_train))\nprint('test set score: ', end='')\nprint(GNB.score(X_test, y_test))\nprint('full dataframe score: ', end='')\nprint(GNB.score(X, y))\nfrom sklearn.metrics import confusion_matrix\ny_pred = GNB.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('confusion matrix:')\nprint(matrx)\nfrom sklearn.metrics import roc_auc_score\nprint('roc auc score: ', end='')\nprint(roc_auc_score(y, y_pred))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model has better roc_auc score and recall, but also worse than tree or forest."},{"metadata":{},"cell_type":"markdown","source":"Okay, now we will try increase roc_auc score and accuracy. Let's play with balance of our sets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true_train, df_true_test = train_test_split(df[df.motion == 1], test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_big, df_false_lit = train_test_split(df[df.motion == 0], test_size = 0.01, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_lit.motion.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_false_train, df_false_test = train_test_split(df_false_lit, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_train = pd.concat([df_true_train, df_false_train])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_test = pd.concat([df_true_test, df_false_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = result_train.drop('motion', axis = 1)\ny_train = result_train.motion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = result_test.drop('motion', axis = 1)\ny_test = result_test.motion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('motion', axis = 1)\ny = df.motion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Decision Tree Classifier:')\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state = 42)\ntree.fit(X_train, y_train)\nprint('train score: ' + str(tree.score(X_train, y_train)))\nprint('test score: ' + str(tree.score(X_test, y_test)))\nfrom sklearn.metrics import confusion_matrix\ny_pred = tree.predict(X_test)\nprint('confusion matrix:\\n' + str(confusion_matrix(y_test, y_pred)))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y_test, y_pred)))\nmatrx = confusion_matrix(y_test, y_pred)\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))\nprint()\nfrom sklearn.metrics import confusion_matrix\ny_pred = tree.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('On full dataframe:')\nfrom sklearn.metrics import accuracy_score\nprint('accuracy: ' + str(accuracy_score(y, y_pred)))\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y, y_pred)))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest Classifier:')\nfrom  sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 42, n_estimators = 750)\nforest.fit(X_train, y_train)\nprint('train score: ' + str(forest.score(X_train, y_train)))\nprint('test score: ' + str(forest.score(X_test, y_test)))\nfrom sklearn.metrics import confusion_matrix\ny_pred = forest.predict(X_test)\nprint('confusion matrix:\\n' + str(confusion_matrix(y_test, y_pred)))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y_test, y_pred)))\nmatrx = confusion_matrix(y_test, y_pred)\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))\nprint()\nfrom sklearn.metrics import confusion_matrix\ny_pred = forest.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('On full dataframe:')\nfrom sklearn.metrics import accuracy_score\nprint('accuracy: ' + str(accuracy_score(y, y_pred)))\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y, y_pred)))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How can we see forest and tree have better roc_auc score and accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Gradient Boosting Classifier:')\nfrom sklearn.ensemble import GradientBoostingClassifier\nboost = GradientBoostingClassifier(random_state = 42, n_estimators = 750, learning_rate = 0.2)\nboost.fit(X_train, y_train)\nprint('train score: ' + str(boost.score(X_train, y_train)))\nprint('test score: ' + str(boost.score(X_test, y_test)))\nfrom sklearn.metrics import confusion_matrix\ny_pred = boost.predict(X_test)\nprint('confusion matrix:\\n' + str(confusion_matrix(y_test, y_pred)))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y_test, y_pred)))\nmatrx = confusion_matrix(y_test, y_pred)\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))\nprint()\nfrom sklearn.metrics import confusion_matrix\ny_pred = boost.predict(X)\nmatrx = confusion_matrix(y, y_pred)\nprint('On full dataframe:')\nfrom sklearn.metrics import accuracy_score\nprint('accuracy: ' + str(accuracy_score(y, y_pred)))\nprint('confusion matrix:\\n' + str(matrx))\nfrom sklearn.metrics import roc_auc_score\nprint('roc_auc: ' + str(roc_auc_score(y, y_pred)))\nprint('recall for negative samples(motion = False): ' + str(matrx[1][1] / (matrx[1][0] + matrx[1][1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting Classifier has worse roc_auc but better accuracy."},{"metadata":{},"cell_type":"markdown","source":"So we can change balance of our sets to change metrics depending of our task. We also can find better params for our models. It will change our metrics too."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}