{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Permutation Importance\n\nOne of the most basic questions we might ask of a model is **What features have the biggest impact on predictions?** This concept is called **feature importance**.\nHere, we'll focus on *permutation importance*.  Compared to most other approaches, permutation importance is:\n- Fast to calculate\n- Widely used and understood\n- Consistent with properties we would want a feature importance measure to have\n\n# How it Works\n\nPermutation importance uses models differently than anything you've seen so far, and many people find it confusing at first. So we'll start with an example to make it more concrete. Consider data with the following format:\n\n![Data](https://i.imgur.com/wjMAysV.png)\n\nWe want to predict a person's height when they become 20 years old, using data that is available at age 10. Our data includes useful features (*height at age 10*), features with little predictive power (*socks owned*), as well as some other features we won't focus on in this explanation.\n\n**Permutation importance is calculated after a model has been fitted.** we won't change the model or change what predictions we'd get for a given value of height, sock-count, etc.\n\nInstead we will ask the following question:  If I randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?\n\n![Shuffle](https://i.imgur.com/h17tMUU.png)\n\nRandomly re-ordering a single column should cause less accurate predictions, since the resulting data no longer corresponds to anything observed in the real world.  Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions.  In this case, shuffling `height at age 10` would cause terrible predictions. If we shuffled `socks owned` instead, the resulting predictions wouldn't suffer nearly as much.\n\nWith this insight, the process is as follows:\n\n1. Get a trained model\n2. Shuffle the values in a single column, make predictions using the resulting dataset.  Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. That performance deterioration measures the importance of the variable you just shuffled.\n3. Return the data to the original order (undoing the shuffle from step 2.) Now repeat step 2 with the next column in the dataset, until you have calculated the importance of each column.\n\n# Code Example\n\nOur example will use a model that predicts whether a football team will have the \"Man of the Game\" winner based on the team's statistics.  The \"Man of the Game\" award is given to the best player in the game.  Model-building isn't our current focus, so the cell below loads the data and builds a rudimentary model.","metadata":{"_uuid":"7a4a7bcceb1f146dd1fe4f01b1bc76916eeffa68"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n","metadata":{"_uuid":"e19079e0f8d1f5d37db25b0420ba8acaa77eb3be","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is how to calculate and show importances with the [eli5](https://eli5.readthedocs.io/en/latest/) library:","metadata":{"_uuid":"bb6b0f0c332036e334ae19f55815950b9ea9e97f"}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","metadata":{"_uuid":"0580c683e35565aeb2a77bec5fba247f4d374a5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpreting Permutation Importances\n\nThe values towards the top are the most important features, and those towards the bottom are least important. The first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric). \n\nLike most things in data science, there is some randomness to the exact performance change from shuffling a column.  We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles.  The number after the **Â±** measures how performance varied from one-reshuffling to the next.\n\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n\nIn our example, the most important feature was **Goals scored**. That seems sensible. Soccer fans may have some intuition about whether the orderings of other variables are surprising or not.","metadata":{"_uuid":"3b69ee905f4b893486e6d06c01f2571161b5c40d"}},{"cell_type":"markdown","source":"# Exercises\n\n## Intro\n\nWe will think about and calculate permutation importance with a sample of data from the [Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction) competition.\n\nWe won't focus on data exploration or model building for now. We can just run the cell below to \n- Load the data\n- Divide the data into training and validation\n- Build a model that predicts taxi fares\n- Print a few rows for you to review","metadata":{"_uuid":"16504b78ece01a2745475a5b9e2001cd4d8e7ca2"}},{"cell_type":"code","source":"# Loading data, dividing, modeling and EDA below\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)\n\n# Remove data with extreme outlier coordinates or negative fares\ndata = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +\n                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +\n                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +\n                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +\n                  'fare_amount > 0'\n                  )\n\ny = data.fare_amount\n\nbase_features = ['pickup_longitude',\n                 'pickup_latitude',\n                 'dropoff_longitude',\n                 'dropoff_latitude',\n                 'passenger_count']\n\nX = data[base_features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)\n\n# Environment Set-Up for feedback system.\nimport sys\nsys.path.append('../input/ml-insights-tools')\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom ex2 import *\nprint(\"Setup Complete\")\n\n# show data\nprint(\"Data sample:\")\ndata.head()","metadata":{"_uuid":"29bf87a1a591b3d507c813147f57f801957b7a8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following two cells may also be useful to understand the values in the training data:","metadata":{"_uuid":"0e074f0d8e9bdad81434f98ca5048965e0ed8325"}},{"cell_type":"code","source":"train_X.describe()","metadata":{"_uuid":"2639a61fa87b71ab9442d762d3704572a0eb018f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y.describe()","metadata":{"_uuid":"3fc46ecad845d340d9dbc2bcf3e34d3d39340477","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 1\n\nThe first model uses the following features\n- pickup_longitude\n- pickup_latitude\n- dropoff_longitude\n- dropoff_latitude\n- passenger_count\n\nBefore running any code... which variables seem potentially useful for predicting taxi fares? Do you think permutation importance will necessarily identify these features as important?\n\nOnce you've thought about it, run `q_1.solution()` below to see how you might think about this before running the code.","metadata":{"_uuid":"17dc2a24bbe200e29d5ed354570714204ad9b0be"}},{"cell_type":"code","source":"q_1.solution()","metadata":{"_uuid":"ac7c33142439b148df26751b29ac45dabdf79987","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 2\n\nCreate a `PermutationImportance` object called `perm` to show the importances from `first_model`.  Fit it with the appropriate data and show the weights.\n\nFor your convenience, the code from the tutorial has been copied into a comment in this code cell.","metadata":{"_uuid":"842edb6243a3f88de17db6a9b0a6e4d4184cb861"}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Make a small change to the code below to use in this problem. \nperm = PermutationImportance(first_model, random_state=1).fit(val_X, val_y)\n\nq_2.check()\n\n# uncomment the following line to visualize your results\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","metadata":{"_uuid":"cc2e9b3f50a03f4369ec6bb312d0abab9e82a193","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment the lines below for a hint or to see the solution.","metadata":{"_uuid":"6cf0858472a16d3e506fe3c68fd7d31619075154"}},{"cell_type":"code","source":"# q_2.hint()\n# q_2.solution()","metadata":{"_uuid":"8a6b950ebd154b456830bc2d73e67c887c6bcdc0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 3\nBefore seeing these results, we might have expected each of the 4 directional features to be equally important. But, on average, the latitude features matter more than the longititude features. Can you come up with any hypotheses for this? After you've thought about it, check here for some possible explanations:","metadata":{"_uuid":"5f8f2151b8cbad30616ffc7f972717c8e14eddfd"}},{"cell_type":"code","source":"q_3.solution()","metadata":{"_uuid":"d6883f61ac1bb7e94b30034f71975528d87abf40","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 4\n\nWithout detailed knowledge of New York City, it's difficult to rule out most hypotheses about why latitude features matter more than longitude. A good next step is to disentangle the effect of being in certain parts of the city from the effect of total distance traveled. The code below creates new features for longitudinal and latitudinal distance. It then builds a model that adds these new features to those you already had. Fill in two lines of code to calculate and show the importance weights with this new set of features. As usual, you can uncomment lines below to check your code, see a hint or get the solution.","metadata":{"_uuid":"01c8b5da3d3ba53d06d0d073f0c1f6abe2e84a86"}},{"cell_type":"code","source":"# create new features\ndata['abs_lon_change'] = abs(data.dropoff_longitude - data.pickup_longitude)\ndata['abs_lat_change'] = abs(data.dropoff_latitude - data.pickup_latitude)\n\nfeatures_2  = ['pickup_longitude',\n               'pickup_latitude',\n               'dropoff_longitude',\n               'dropoff_latitude',\n               'abs_lat_change',\n               'abs_lon_change']\n\nX = data[features_2]\nnew_train_X, new_val_X, new_train_y, new_val_y = train_test_split(X, y, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_train_X, new_train_y)\n\n# Create a PermutationImportance object on second_model and fit it to new_val_X and new_val_y\n# Use a random_state of 1 for reproducible results that match the expected solution.\nperm2 = PermutationImportance(second_model, random_state=1).fit(new_val_X, new_val_y)\n\n# show the weights for the permutation importance you just calculated\neli5.show_weights(perm2, feature_names = features_2)","metadata":{"_uuid":"3b5e49debb11ae334f2f3c581dffd3220e779578","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q_4.check()","metadata":{"_uuid":"7e289678093d719e0994626bd061022fd7fd5c5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How would you interpret these importance scores? Distance traveled seems far more important than any location effects. \n\nBut the location still affects model predictions, and dropoff location now matters slightly more than pickup location. Do you have any hypotheses for why this might be? The techniques used later will help us dive into this more.","metadata":{"_uuid":"5a3154ff832bf4ed59b372df1fead464fa0d8bc5"}},{"cell_type":"code","source":"#q_4.solution()","metadata":{"_uuid":"27d953feb9eac0c834c7de286325788524fd88e1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 5\n\nAs you can see the values for `abs_lon_change` and `abs_lat_change` are pretty small (all values are between -0.1 and 0.1), whereas other variables have larger values.  Do you think this could explain why those coordinates had larger permutation importance values in this case?  \n\nConsider an alternative where you created and used a feature that was 100X as large for these features, and used that larger feature for training and importance calculations. Would this change the outputted permutaiton importance values?\n\nWhy or why not?\n\nAfter you have thought about your answer, either try this experiment or look up the answer in the cell below","metadata":{"_uuid":"b0bc500a13d94f755989b51d4bb9e6aaf762491f"}},{"cell_type":"code","source":"q_5.solution()","metadata":{"_uuid":"499d92e99f74866af5f6d916752621ee93110fe3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 6\n\nYou've seen that the feature importance for latitudinal distance is greater than the importance of longitudinal distance. From this, can we conclude whether travelling a fixed latitudinal distance tends to be more expensive than traveling the same longitudinal distance?\n\nWhy or why not? Check your answer below.","metadata":{"_uuid":"63bc02b9ed9a2f26c1a3e75b4a63946be3134517"}},{"cell_type":"code","source":"q_6.solution()","metadata":{"_uuid":"d6250d21dd4da206be91fb5c9f0ef353bf1a9be3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Permutation importance is useful useful for debugging, understanding your model, and communicating a high-level overview from your model. ","metadata":{"_uuid":"095012b5098603eccd557791209cbd642429eea9"}}]}