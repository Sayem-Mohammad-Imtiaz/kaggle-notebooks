{"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py"}},"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.sparse import csc_matrix\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"_kg_hide-input":false,"_uuid":"8389610bce332ffad350b370f7e9a98ccd426366","_cell_guid":"a6e57aa3-6882-47d2-bb2a-edcdce74abe6","_kg_hide-output":false}},{"cell_type":"markdown","source":"Let's just read our dataset and see how it looks.","metadata":{}},{"source":"data = pd.read_csv('../input/spam.csv',encoding='latin-1')\ndata.head()","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"We can change it a bit by droppig 'bad' columns and by giving columns approptiate name.","metadata":{}},{"source":"#data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":'label', \"v2\":'text'})\ndata.head()","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"It is always useful to see the proportion of spam vs ham messages.","metadata":{}},{"source":"num = pd.value_counts(data['label'],sort=True).sort_index()\nnum.plot(kind='bar')\nplt.title('histogram')\nplt.xlabel('spam or not')\nplt.ylabel('number')","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"source":"print(data.shape)\n\ndata.replace(('spam', 'ham'), (1, 0), inplace=True)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Splitting the dataset into train and test.","metadata":{}},{"source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(data[\"text\"],data[\"label\"], test_size = 0.3)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"We will use CountVectorizer to transform data into matrix with numbers. This function returns sparse matrix so we will need to edit it later so it can be used for our implementation of logistic regression.","metadata":{}},{"source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\nvect.fit(x_train)\nx_train_df = vect.transform(x_train)\nx_test_df = vect.transform(x_test)\n\nham_words = ''\nspam_words = ''\nspam = data[data.label == 1]\nham = data[data.label == 0]","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Analyzing the dataset.","metadata":{}},{"source":"import nltk\n\nfor val in spam.text:\n    text = val.lower()\n    tokens = nltk.word_tokenize(text)\n    for words in tokens:\n        spam_words = spam_words + words + ' '\n        \nfor val in ham.text:\n    text = val.lower()\n    tokens = nltk.word_tokenize(text)\n    for words in tokens:\n        ham_words = ham_words + words + ' '","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Let's visualize the most frequent spam and ham words.\nWe can see that if we find word 'free' or 'text' in the message it is probabbly spam, and that there are some 'safe words' like 'will', 'lt, 'gt', 'ok'.","metadata":{}},{"source":"from wordcloud import WordCloud\n\nspam_wordcloud = WordCloud(width=600, height=400).generate(spam_words)\nham_wordcloud = WordCloud(width=600, height=400).generate(ham_words)\n\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()\n\nplt.figure(figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"Just to compare some already existing functions with our implemented logistc regression.","metadata":{}},{"source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l1')\nlr.fit(x_train_df,y_train)\nlrscore = lr.score(x_test_df,y_test)\nprint('Logistic regression score ',lrscore)\nprint(' ')\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(max_depth=60)\nrfc.fit(x_train_df,y_train)\nrfcscore = rfc.score(x_test_df,y_test)\nprint('Random Forest score ',rfcscore)\nprint(' ')\n\nfrom sklearn.naive_bayes import MultinomialNB\nmn = MultinomialNB()\nmn.fit(x_train_df,y_train)\nmnscore = mn.score(x_test_df,y_test)\nprint('Multinomial score ',mnscore)\nprint(' ')","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"To get 'normal' matrix from sparse matrix we will use todense() function.","metadata":{}},{"source":"print('Implementation of logistic regression ')\n\nx_train_df = x_train_df.todense()\nx_test_df = x_test_df.todense()\n","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"**Sigmoid**","metadata":{}},{"source":"def sigmoid(z):\n    return float(1.0 / float((1.0 + math.exp(-1.0*z))))","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def hypothesis(theta,x):\n    z = 0\n    for i in range(len(theta)):\n        xi = x[i]\n        z += np.dot(xi,theta[i].transpose())\n    return sigmoid(z)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def cost_function(theta,m):\n    sumOfErrors = 0\n    for i in range(m):\n        hi = hypothesis(theta,x_train_df[i])\n        if y_train.iloc[i] == 1:\n            error = y_train.iloc[i] * math.log(hi)\n        elif y_train.iloc[i] == 0:\n            error = (1-y_train.iloc[i]) * math.log(1-hi)\n        sumOfErrors += error\n    const = -1/m\n    j = const * sumOfErrors\n    return j\n","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def cost_function_derivative(theta,j,m,alpha):\n    sumErrors = 0\n    for i in range(m):\n        xi = x_train_df[i]\n        hi = hypothesis(theta,xi)\n        yt = y_train.iloc[i]\n        hit = np.subtract((float)(hi),float(yt))\n        error = np.dot(hit,x_train_df[i][j])\n        sumErrors += error\n    m = len(y_test)\n    constant = float(alpha)/float(m)\n    j = constant * sumErrors\n    return j","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def gradient_descent(theta,m,alpha):\n    new_theta = []\n    for j in range(len(theta)):\n        new_theta_value = theta[j] - cost_function_derivative(theta,j,m,alpha)\n        new_theta.append(new_theta_value)\n    return new_theta","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def logistic_regression(alpha,theta,num_iters):\n    m = len(y_train)\n    for x in range(num_iters):\n        new_theta = gradient_descent(theta,m,alpha)\n        theta = new_theta\n        if x % 100 == 0:\n            cost_function(theta,m)\n            print ('theta ', theta)\t\n            print ('cost is ', cost_function(theta,m))\n    score = 0\n    length = len(x_test)\n    for i in range(length):\n        prediction = round(hypothesis(theta,x_test_df[i]))\n        answer = y_test.iloc[i]\n        if prediction == answer:\n            score += 1\n    my_score = float(score) / float(length)\n    print ('Your score: ', my_score)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"**ReLu**\n\nActually this is Leaky ReLu here.","metadata":{}},{"source":"def ReLU(z):\n    if z > 0:\n        return float(z)\n    return float(0.01*z)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def hypothesisrelu(theta,x):\n    z = 0\n    for i in range(len(theta)):\n        xi = x[i]\n        z += np.dot(xi,theta[i].transpose())\n    return ReLU(z)\n    \ndef cost_functionrelu(theta,m):\n    sumOfErrors = 0\n    for i in range(m):\n        hi = hypothesisrelu(theta,x_train_df[i])\n        if y_train.iloc[i] == 1:\n            error = y_train.iloc[i] * abs(hi-1)\n        elif y_train.iloc[i] == 0:\n            error = (1-y_train.iloc[i]) * abs(hi)\n        sumOfErrors += error\n    const = 1/m\n    j = const * sumOfErrors\n    return j\n\ndef cost_function_derivativerelu(theta,j,m,alpha):\n    sumErrors = 0\n    for i in range(m):\n        xi = x_train_df[i]\n        hi = hypothesisrelu(theta,xi)\n        yt = y_train.iloc[i]\n        hit = np.subtract((float)(hi),float(yt))\n        error = np.dot(hit,x_train_df[i][j])\n        sumErrors += error\n    m = len(y_test)\n    constant = float(alpha)/float(m)\n    j = constant * sumErrors\n    return j\n    \ndef gradient_descentrelu(theta,m,alpha):\n    new_theta = []\n    for j in range(len(theta)):\n        new_theta_value = theta[j] - cost_function_derivativerelu(theta,j,m,alpha)\n        new_theta.append(new_theta_value)\n    return new_theta\n\ndef logistic_regression_relu(alpha,theta,num_iters):\n    m = len(y_train)\n    for x in range(num_iters):\n        new_theta = gradient_descentrelu(theta,m,alpha)\n        theta = new_theta\n        if x % 100 == 0:\n            cost_functionrelu(theta,m)\n            print ('theta ', theta)\t\n            print ('cost is ', cost_functionrelu(theta,m))\n    score = 0\n    length = len(x_test)\n    for i in range(length):\n        prediction = round(hypothesisrelu(theta,x_test_df[i]))\n        answer = y_test.iloc[i]\n        if prediction == answer:\n            score += 1\n    my_score = float(score) / float(length)\n    print ('Your score with ReLu: ', my_score)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"**Tanh**","metadata":{}},{"source":"def Tanh(z):\n    return float(math.tanh(z))","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"source":"def hypothesistanh(theta,x):\n    z = 0\n    for i in range(len(theta)):\n        xi = x[i]\n        z += np.dot(xi,theta[i].transpose())\n    return Tanh(z)\n    \ndef cost_functiontanh(theta,m):\n    sumOfErrors = 0\n    for i in range(m):\n        hi = hypothesistanh(theta,x_train_df[i])\n        if y_train.iloc[i] == 1:\n            error = y_train.iloc[i] * (1-hi*hi)\n        elif y_train.iloc[i] == 0:\n            error = (1-y_train.iloc[i]) * (1-hi*hi)\n        sumOfErrors += error\n    const = 1/m\n    j = const * sumOfErrors\n    return j\n\ndef cost_function_derivativetanh(theta,j,m,alpha):\n    sumErrors = 0\n    for i in range(m):\n        xi = x_train_df[i]\n        hi = hypothesistanh(theta,xi)\n        yt = y_train.iloc[i]\n        hit = np.subtract((float)(hi),float(yt))\n        error = np.dot(hit,x_train_df[i][j])\n        sumErrors += error\n    m = len(y_test)\n    constant = float(alpha)/float(m)\n    j = constant * sumErrors\n    return j\n    \ndef gradient_descenttanh(theta,m,alpha):\n    new_theta = []\n    for j in range(len(theta)):\n        new_theta_value = theta[j] - cost_function_derivativetanh(theta,j,m,alpha)\n        new_theta.append(new_theta_value)\n    return new_theta\n\ndef logistic_regression_tanh(alpha,theta,num_iters):\n    m = len(y_train)\n    for x in range(num_iters):\n        new_theta = gradient_descenttanh(theta,m,alpha)\n        theta = new_theta\n        if x % 100 == 0:\n            cost_functiontanh(theta,m)\n            print ('theta ', theta)\t\n            print ('cost is ', cost_functiontanh(theta,m))\n    score = 0\n    length = len(x_test)\n    for i in range(length):\n        prediction = round(hypothesistanh(theta,x_test_df[i]))\n        answer = y_test.iloc[i]\n        if prediction == answer:\n            score += 1\n    my_score = float(score) / float(length)\n    print ('Your score with tanh: ', my_score)","outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"markdown","source":" Now we sholud just run our versions of logistic regression.","metadata":{}},{"source":"initial_theta = np.zeros(x_test_df[1].shape)\nalpha = 0.3\niterations = 1000\nlogistic_regression(alpha,initial_theta,iterations)\nprint(' ')\ninitial_theta = np.zeros(x_test_df[1].shape)\nlogistic_regression_relu(alpha,initial_theta,iterations)\nprint(' ')\ninitial_theta = np.zeros(x_test_df[1].shape)\nlogistic_regression_tanh(alpha,initial_theta,iterations)\nprint(' ')","outputs":[],"cell_type":"code","execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"At the end, we can see that our implemented logistic regression works better than logistic regression implemented in python and that it is certainly better to avoid sigmoid and use tanh in this case instead.","metadata":{}}]}