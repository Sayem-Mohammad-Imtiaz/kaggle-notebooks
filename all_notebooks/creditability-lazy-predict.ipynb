{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRdcrJ2bVyZOkBidJleqA74wYZdhcxrl1QkkBGBM4RK9GlNpfl0si3bup6Yw9UtYxBK6fo&usqp=CAU)pysmash.wordpress.com","metadata":{}},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('../input/cusersmarildownloadsgermancsv/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'german.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head().style.set_properties(**{'background-color':'bisque',\n                                     'color': 'purple'})\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#That's the 18th Notebook in this Dataset. No missing values.","metadata":{}},{"cell_type":"markdown","source":"#https://lazypredict.readthedocs.io/en/latest/\n\n#https://pypi.org/project/lazypredict/\n\n#Code by Srikar Dornala https://www.kaggle.com/srikardornala/standard-way-to-approach-regression-problems/notebook","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\ncorr = df.corr()\ngraph = go.Figure()\ngraph.add_trace(go.Heatmap(z=corr.values, x=corr.index.values, y=corr.columns.values))\ngraph.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Check for imbalance in the dataset","metadata":{}},{"cell_type":"code","source":"!pip install smogn","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Below I chose Creditability, since all values are 1 (phi). I changed to Duration_of_Credit_monthly that have many different values.\n\nValue Error: redefine phi relevance function: all points are 1","metadata":{}},{"cell_type":"code","source":"import smogn\nconcrete_smogn = smogn.smoter(\n    data = df,       \n    y = 'Duration_of_Credit_monthly'  \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(df['Duration_of_Credit_monthly'], label = \"Original\")\nsns.kdeplot(concrete_smogn['Duration_of_Credit_monthly'], label = \"Modified\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#The step above is not compulsory for regression. However the accuracy of classification problems increases by using SMOTE.","metadata":{}},{"cell_type":"code","source":"#splitting the data\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Since we are using a decision tree model , we don't need to use StandardScaler","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\ndtr.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtr1 = DecisionTreeRegressor()\ndtr1.fit(X_train, y_train)\ndtr1.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#As we can see there's no major improvement in the performance of the model, now to go for hyperparameter optimization","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : [ 'mae' ,'mse', 'friedman_mse','poisson'],\n    'splitter' : ['best', 'random'],\n    'max_depth' : [3, 5, 7, 9, 10,12,],\n    'min_samples_split' : [ 2, 3, 4, 5,7],\n    'min_samples_leaf' : [ 2, 3, 4, 5,7]\n}\n\ngrid_search = GridSearchCV(dtr1, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#An alternative approach to select a base model is to use the lazyregressor from lazypredict\n\nhttps://pypi.org/project/lazypredict/","metadata":{}},{"cell_type":"code","source":"!pip install lazypredict","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.4.1 requires numpy~=1.19.2, but you have numpy 1.19.1 which is incompatible.\nsklearn-pandas 2.1.0 requires pandas>=1.1.4, but you have pandas 1.0.5 which is incompatible.\npyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.1 which is incompatible.\npyldavis 3.3.1 requires pandas>=1.2.0, but you have pandas 1.0.5 which is incompatible.\nplotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 1.0.5 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.1 which is incompatible.\nosmnx 1.0.1 requires pandas>=1.1, but you have pandas 1.0.5 which is incompatible.\nmizani 0.7.3 requires pandas>=1.1.0, but you have pandas 1.0.5 which is incompatible.\nmatrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.15.8 which is incompatible.\nimbalanced-learn 0.8.0 requires scikit-learn>=0.24, but you have scikit-learn 0.23.1 which is incompatible.\nautogluon-core 0.1.0 requires numpy==1.19.5, but you have numpy 1.19.1 which is incompatible.","metadata":{}},{"cell_type":"code","source":"import lazypredict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#After installing lazypredict you need to REFRESH PAGE of your Notebook otherwise it will not work!","metadata":{}},{"cell_type":"code","source":"from lazypredict.Supervised import LazyRegressor\nreg = LazyRegressor(ignore_warnings=False, custom_metric=None)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Let's see how XGB regressor (highest Adjusted R-Squared and R - Squared metric in the original code)\n\n#I've just copied the code!","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(verbosity=0) \nxgbr.fit(X_train,y_train)\n#bellow is the Adjusted R-Squared for the model\n1 - (1-xgbr.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Srikar Dornala @srikdornala for the script.' )","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#I'm not Lazy though I think I ruined one more script. \n\nhttps://pypi.org/project/lazypredict/\n\nhttps://lazypredict.readthedocs.io/en/latest/","metadata":{}}]}