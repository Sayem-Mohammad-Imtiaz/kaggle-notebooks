{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Performing Regression with Neural Networks\nThe majority of my projects that involve neural networks are classification based. This notebook is an attempt at using a network in a regression problem. The dataset itself is simple enough: there are about 1300 rows and 6 features, including some categorical variables. Though it's apparent from the title, the target variable \"charges\" is numerical."},{"metadata":{},"cell_type":"markdown","source":"### Imports\nThe model I'll be using makes use of some very essential PyTorch features like Autograd and the almost indespensible **nn** module. We'll therefore import optim for our backpropagation, pandas for some simple ground-up pre-processing and matplotlib.pyplot for plotting some nice visualizations."},{"metadata":{"id":"zgHh4Y5sS-h4","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Data"},{"metadata":{"id":"Z_Eo5PcRT-Wn","colab_type":"code","outputId":"517b8293-de63-4762-e1ef-fbedced69a12","colab":{"base_uri":"https://localhost:8080/","height":272},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/insurance/insurance.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets first get a feel for the dataset. We'll use DataFrame's head(), shape and info() here."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape: \", df.shape)\nprint('--------')\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing the heatmap of the correlation matrix of all the features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['charges']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='age', y='charges',data=df, )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The biggest correlation, which is still not that significant, is between age and charges. There are three 'levels' in the scatterplot above, so the regression is not that straightforward."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its clear that there is no missing data (NaN values). But we'll have to do something about the categorical variables -- specifically, sex, smoker and region. Let's just add some dummy variables. If a categorical variable has n possible values (or classes), it would require $log_{2}(n)$ dummy variables. \n\nThe function below can be easily generalized by checking the type of each column: we are targeting the columns of type 'object'. To keep it simple, lets just hardcode the columns to encode."},{"metadata":{"id":"yZzMqF_Qoixx","colab_type":"code","outputId":"8dbdfb5f-9172-4bf5-83c4-912c77ad3405","colab":{"base_uri":"https://localhost:8080/","height":252},"trusted":true},"cell_type":"code","source":"# smoker encoding\ndf['smoker'] = [1 if a == 'yes' else 0 for a in df['smoker']]\n\n# region encoding\nregions = {\n    'northeast': [0, 0],\n    'northwest': [0, 1],\n    'southeast': [1, 0],\n    'southwest': [1, 1]\n    }\n\ndf['dir1'] = 0\ndf['dir2'] = 0\ndf[['dir1', 'dir2']] = [regions[dir] for dir in df['region']]\ndf = df.drop('region', axis=1)\n\n# sex encoding\ndf['sex'] = [1 if sex == 'male' else 0 for sex in df['sex']]\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After our categorical encoding, we can see the data above. Something is still missing though: normalization. Though its not a strict necessity, it improves the network's training time and often gives better results. "},{"metadata":{"id":"MLcWUAqZlS8K","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# normalization\ntarget = df['charges'].copy()\n\nfor i in df.columns:\n    df[i] = (df[i] - min(df[i])) / (max(df[i] - min(df[i])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\nprint(target.head())","execution_count":null,"outputs":[]},{"metadata":{"id":"rhq73qiv4aVJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# training data\nX_train = torch.tensor((df.drop('charges', axis=1).iloc[:1100]).values.astype(pd.np.float32))\ny_train = torch.tensor((df['charges'].iloc[:1100]).values.astype(pd.np.float32))\n\ny_train = y_train.reshape(-1, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model (using pytorch abstractions like Autograd)\n\ntrain_tensor = torch.utils.data.TensorDataset(X_train, y_train)\ntrainloader = torch.utils.data.DataLoader(train_tensor, batch_size=10)\nmodel = nn.Sequential(nn.Linear(7, 1))\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nlosses = []\n\nepochs = 1000\nfor e in range(epochs):\n    running_loss = 0\n    main_outs = []\n    for data, targets in trainloader:\n   \n        optimizer.zero_grad()\n        outs = model.forward(data)\n\n        main_outs.extend(outs)\n        loss = criterion(outs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    else:\n        print(f\"Training loss: {running_loss/len(trainloader)}\")\n        losses.append(running_loss/len(trainloader))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.plot(losses);","execution_count":null,"outputs":[]},{"metadata":{"id":"zz2VO23qUkuc","colab_type":"code","outputId":"0b10b5d5-f07d-42ec-fc0a-37683470b7e6","colab":{"base_uri":"https://localhost:8080/","height":318},"trusted":true},"cell_type":"code","source":"preds = model(X_train) * (max(target) - min(target)) + min(target)\nsns.regplot(y_test.squeeze(),denorm.squeeze());","execution_count":null,"outputs":[]},{"metadata":{"id":"Il0Db6fhpeq7","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# testing\nX_test = torch.tensor((df.drop('charges', axis=1).iloc[1100:]).values.astype(pd.np.float32))\ny_test = torch.tensor((df['charges'].iloc[1100:]).values.astype(pd.np.float32))\n\ny_test = y_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# denormalize test predictions\npreds = model(X_test).clone().detach()\n\ndenorm = preds * (max(target) - min(target)) + min(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(y_test.squeeze(),denorm.squeeze());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets calculate the coefficient of determination: $R^2 = 1 - \\frac{SSE}{SST}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_tensor = torch.tensor(target.iloc[1100:].values.astype(pd.np.float32))\ntarget_tensor = target_tensor.reshape(-1, 1)\nSSE = pd.np.sqrt(sum((target_tensor - denorm)**2))\nSST = pd.np.sqrt(sum((denorm - denorm.mean())**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(1- SSE/SST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Network from scratch\n\nThe model above makes use of a lot of PyTorch's abstractions, and though is an important part of improving the framework's writability, this is not the best way to go in my opinion when it comes to learning and understanding the concepts and the theory in depth. To facilitate said learning, I've made an attempt to achieve similar results with a network made from scratch in PyTorch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(25):\n#   nn_.losses.append(torch.mean((y - nn_(X))**2).detach().item())\n#   print(\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - nn_(X))**2).detach().item()))\n\n#   nn_.train(X, y)\n#   print(nn_.W1, nn_.W2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\n\n# class NN (nn.Module):\n#   def __init__(self):\n#     super(NN, self).__init__()\n\n#     self.inpsize = 7\n#     self.otpsize = 1\n#     self.hdnsize = 5\n\n#     self.losses = []\n#     self.outputs = []\n\n#     self.W1 = torch.Tensor(self.inpsize, self.hdnsize) # 7 x 5\n#     self.W2 = torch.Tensor(self.hdnsize, self.otpsize) # 5 x 1\n#     self.W1.fill_(1000)\n#     self.W2.fill_(1000)\n#     self.b1 = torch.Tensor()\n\n#   def train(self, X, y):\n#     # forward + backward pass for training\n#     o = self.forward(X)\n#     self.outputs = o\n#     print(\"o: \", o)\n#     self.backward(X, y, o)\n\n#   def forward(self, X):\n#     self.z = torch.matmul(X, self.W1)\n#     self.z2 = torch.Tensor(self.z)\n\n#     # relu on hidden layer\n#     p, q = self.z.shape\n#     for i in range(p):\n#       for j in range(q):\n#         self.z2[i][j] = self.relu(self.z[i][j])\n\n#     self.z3 = torch.matmul(self.z2, self.W2)\n\n#     # o = torch.Tensor(self.z3)\n#     # r, s = self.z3.shape\n#     # for i in range(r):\n#     #   for j in range(s):\n#     #     o[i][j] = self.relu(self.z3[i][j])\n\n#     # print(o, y)\n#     return self.z3\n\n#   def sigmoid(self, s):\n#     return 1 / (1 + torch.exp(-s))\n\n#   def dsigmoid_dx(self, s):\n#     return s * (1 - s)\n\n#   def backward(self, X, y, o):\n#       self.o_error = 0.5*(y - o)**2 # error in output\n#       # 30000 x 1\n\n#       # print(self.o_error.shape, o.shape, y.shape)\n#       self.o_delta = torch.t(self.o_error) @ self.dsigmoid_dx(o) \n#       self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n\n    \n#       # .... -> 30000 x 3\n#       self.z2_delta = self.z2_error * self.dsigmoid_dx(self.z2)\n\n#       # 9 x 30000 * 30000 x 3 -> 9 x 3\n#       self.W1 += ((torch.matmul(torch.t(X), self.z2_delta)) * 0.00001)\n#       self.W2 += (torch.matmul(torch.t(self.z2), self.o_delta) * 0.00001)\n\n#   def relu(self, x):\n#     return max(torch.tensor(0), x)","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"MedicalCostPredict.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}