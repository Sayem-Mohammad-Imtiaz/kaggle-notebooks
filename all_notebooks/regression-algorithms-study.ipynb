{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# data description https://www.kaggle.com/sohier/calcofi#bottle.csv \n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport re\nimport os\nimport warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pandas import DataFrame, read_csv\nfrom scipy.stats.stats import pearsonr ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840658fe4564ba9cf7939bb124013c08839895a8"},"cell_type":"markdown","source":"# Read Data\nOver 60 years of oceanographic data\n\nhttps://www.kaggle.com/sohier/calcofi \n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"bottle = read_csv('../input/bottle.csv', usecols=['Depth_ID', 'Depthm', 'T_degC', 'Salnty', 'STheta'])\ntemp = [(int(i[:2]), int(i[3:5]), int(i[5:7]), i[10:12]) for i in bottle['Depth_ID']]\nbottle['Century'], bottle['Year'], bottle['Month'], bottle['CastType'] = list(zip(*temp))\nbottle = bottle.drop(columns=\"Depth_ID\") \nbottle.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4edb1b480c4c8accc07e931e254c1eb7678b03c"},"cell_type":"code","source":"bottle = bottle[bottle['CastType']=='HY'][bottle['Century']==19][bottle['Year']==49][bottle['Month']==3]\nbottle = bottle.drop(columns='CastType')\nbottle = bottle.drop(columns='Century')\nbottle = bottle.drop(columns='Year')\nbottle = bottle.drop(columns='Month')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"156e663e1913fc6fec08bc795ab7dafeb1ff2155"},"cell_type":"markdown","source":"# Data description"},{"metadata":{"trusted":true,"_uuid":"f3c48c40d767eb3cf85250be3450c84089fe9150"},"cell_type":"code","source":"parameters = ['T_degC', 'Salnty']\nobjective = 'Depthm'\nbottle.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5760c48112eb161cad5636b43d5fc9a49ef1ad7"},"cell_type":"code","source":"x_real = bottle[parameters]\ny_real = bottle[objective]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53bf92ee68fc87f894b4ad72ee37cd7d976c9ddf"},"cell_type":"code","source":"plt.scatter(x_real[parameters[0]], x_real[parameters[1]])\nplt.xlabel(parameters[0])\nplt.ylabel(parameters[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"742fc79873177887a6bcee75ae6e2277ca1208a8"},"cell_type":"code","source":"plt.figure()\nplt.scatter(x_real[parameters[0]], y_real)\nplt.xlabel(parameters[0])\nplt.ylabel(objective)\n\nplt.figure()\nplt.scatter(x_real[parameters[1]], y_real)\nplt.xlabel(parameters[1])\nplt.ylabel(objective)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bbabc599c5e2937cb607ae5f76d16714e9993a0"},"cell_type":"markdown","source":"# Syntethic data\nAnscombe's quartet comprises four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed.\n\nEach dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties."},{"metadata":{"trusted":true,"_uuid":"f18bba68d921143b4e6d6b1a2c9c819c2963dd16"},"cell_type":"code","source":"x1 = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\ny1 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\nx2 = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\ny2 = np.array([9.14, 8.14 ,8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74])\nx3 = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\ny3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])\nx4 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\ny4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9304a3c32cb36742a78ab2da058d6f704ac4d69e"},"cell_type":"code","source":"plt.figure()\nfig, axes = plt.subplots(2, 2)\naxes[0, 0].scatter(x1, y1)\naxes[0, 1].scatter(x2, y2)\naxes[1, 0].scatter(x3, y3)\naxes[1, 1].scatter(x4, y4)\nfor ax in axes.reshape(1, -1)[0]:\n    ax.set_xlim(3, 20)\n    ax.set_ylim(2, 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4a8af578dc0256aae95573990221dd7fc49020b"},"cell_type":"code","source":"DataFrame({'mean of x': [np.mean(x1), np.mean(x2), np.mean(x3), np.mean(x4)], \n          'variance of x': [np.std(x1), np.std(x2), np.std(x3), np.std(x4)], \n          'mean of y': [np.mean(y1), np.mean(y2), np.mean(y3), np.mean(y4)], \n          'variance of y': [np.std(y1), np.std(y2), np.std(y3), np.std(y4)], \n          'correlation between x and y': [pearsonr(x1, y1)[0], pearsonr(x2, y2)[0], \n                                          pearsonr(x3, y3)[0], pearsonr(x4, y4)[0]]\n          },\n         index=[1, 2, 3, 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f18e2ebd19f1b84205db771d7bb1601b4765c66"},"cell_type":"markdown","source":"# Regressions\n[Wikipedia] In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. \n\nHere we will see the implementation of the following regression technics:\n1. Linear Regression (LR) with Ordinary Least Squares / Метод Наименьших Квадратов (OLS / МНК)\n2. Linear Regression (LR) with Weighted Least Squares (WLS) /  Взвешенный МНК\n3. Stepwise Regression\n4. Ridge/Lasso/ElasticNet  Regressions\n5. Polynomial Regression"},{"metadata":{"trusted":true,"_uuid":"55ca4a301b07d1235ef5c0a120e808b3741d344c"},"cell_type":"markdown","source":"# 1. Linear Regression\n$y=ax+b$ in a linear form, where\n\n$y$ - objective value,\n\n$x$ -  parameter value,\n\n$a, b$ - model coefficients\n\nor\n\n$Y=XW$ in matrix form, where\n\n$Y = \\{y_1, y_2, \\dots, y_k\\}$, $y_i$ - objective column vector,\n\n$X = \\{1, x_1, x_2, \\dots, y_m\\}$, $x_j$ -  columns vector of $j$-th parameter (NB! free term is included)\n\n$n$ - number of points, the length of vectors $y_i$ and $x_j$.\n\n$Y_{n \\times k}=X_{n \\times m}W_{m \\times k}$ \n\n## Algorithm\n(analytical approach)\n\n$Y$ - true objective values\n\n$\\hat{Y} = XW$ - predicted objective values \n\nTask: $||Y-\\hat{Y} ||_2 \\to min$ (OLS formulation)\n\nTo find the minimum value, let's take the derivative with respect to $W$:\n\n$\\left(||Y-\\hat{Y}||_2\\right)' = 0$\n\n$\\left((Y-\\hat{Y})^T(Y-\\hat{Y})\\right)' = 0$\n\n$\\left((Y-XW)^T(Y-XW)\\right)' =0$\n\n$(Y-XW)^TY + X^T(Y-XW)=0$\n\n$(Y-XW)^TY$ and $X^T(Y-XW)$ are equal to transparancy operation. So, summa is zero when each one is zero\n\n$X^T(Y-XW) =0$\n\n$X^TY-X^TXW = 0$\n\n$X^TY = X^TXW$\n\n$W = ( X^TX)^{-1}X^TY$\n"},{"metadata":{"trusted":true,"_uuid":"1a4c371800b1630654ef200098e32759e8217963"},"cell_type":"code","source":"x1 = x1.reshape(-1, 1)\nx2 = x2.reshape(-1, 1)\nx3 = x3.reshape(-1, 1)\nx4 = x4.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b834c9deea960b68545ef50632f430d27593092"},"cell_type":"markdown","source":"### Step by step"},{"metadata":{"trusted":true,"_uuid":"d909376af9d9f27791cbdb84bcef6a794133e88d","scrolled":true},"cell_type":"code","source":"x = x1.copy()\ny = y1.copy()\nx = DataFrame([[1] * x.shape[0], list(x[:, 0])]).values.T\ny = DataFrame(y).values\ntemp1 = np.dot(x.T, x)\ntemp2 = np.linalg.inv(temp1)\nprint(np.dot(temp1, temp2)) # check inverse of a matrix\ntemp3 = np.dot(temp2, x.T)\nprint(np.dot(temp3, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2922c38b894c5f4f5c148e97d5d7693603ce5c2d"},"cell_type":"code","source":"class linear_regression:\n    def __init__(self):\n        self.weights = None\n        \n    def fit(self, x, y):\n        x = DataFrame([[1] * x.shape[0], list(x[:, 0])]).values.T\n        y = DataFrame(y).values\n        temp1 = np.dot(x.T, x)\n        try:\n            temp2 = np.linalg.inv(temp1)\n        except:\n            temp1 += np.diag([np.random.uniform() for i in range(temp1.shape[0])])\n            temp2 = np.linalg.inv(temp1)\n        temp3 = np.dot(temp2, x.T)\n        self.weights = np.dot(temp3, y)\n        return self\n    def predict(self, x):\n        x = DataFrame([[1] * x.shape[0], list(x[:, 0])]).values.T\n        y = np.dot(x, self.weights)\n        return y[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e31df0acc27b8036968a0ef2c15b659474ed6418"},"cell_type":"code","source":"mse_error = lambda true, prediction: ((true - prediction)**2).mean()\nmae_error = lambda true, prediction: (abs(true - prediction)).mean()\nvae_error = lambda true, prediction: (abs(true - prediction)).std()\nqae_error = lambda true, prediction: np.percentile(abs(true - prediction), 75) - np.percentile(abs(true - prediction), 25)\nr2_error = lambda true, prediction: 1 - ((true - prediction) ** 2).sum() / ((true - true.mean()) ** 2).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af21a85ebf200fcddb27e46887371ea0b34d36bf"},"cell_type":"code","source":"model_mine = linear_regression()\nmodel_mine.fit(x1, y1)\nmodel_mine.predict(x1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65142ad2647f066814e43eb5b2926d4a331d03ff"},"cell_type":"code","source":"def plot_synthetic(model):\n    plt.figure()\n    fig, axes = plt.subplots(2, 2)\n    axes[0, 0].scatter(x1, y1)\n    axes[0, 0].plot(x1, model.predict(x1), 'r')\n    print(1, \n          ' r2=', r2_error(y1, model.predict(x1)), \n          ', mae=', mae_error(y1, model.predict(x1)))\n    axes[0, 1].scatter(x2, y2)\n    axes[0, 1].plot(x2, model.predict(x2), 'r')\n    print(2, \n          ' r2=', r2_error(y2, model.predict(x2)), \n          ', mae=', mae_error(y2, model.predict(x2)))\n    axes[1, 0].scatter(x3, y3)\n    axes[1, 0].plot(x3, model.predict(x3), 'r')\n    print(3, \n          ' r2=', r2_error(y3, model.predict(x3)), \n          ', mae=', mae_error(y3, model.predict(x3)))\n    axes[1, 1].scatter(x4, y4)\n    axes[1, 1].plot(x4, model.predict(x4), 'r')\n    print(4, \n          ' r2=', r2_error(y4, model.predict(x4)), \n          ', mae=', mae_error(y4, model.predict(x4)))\n    for ax in axes.reshape(1, -1)[0]:\n        ax.set_xlim(3, 20)\n        ax.set_ylim(2, 14)\n# plot_synthetic(model_mine)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcf8c639752a6875f6afee9c9c919fab8d042357"},"cell_type":"markdown","source":" ### Sklearn\n https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html \n \n fit_intercept - whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered)."},{"metadata":{"trusted":true,"_uuid":"ff307c0d41fc10618d43ae6df1ef769e3f55618c"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel_sklearn = LinearRegression(fit_intercept=True)\nmodel_sklearn.fit(x1, y1)\nplot_synthetic(model_sklearn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1488393be9deb0b7cc8db17cd5a75826ac1036a9"},"cell_type":"code","source":"model_sklearn.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa3c28e26b10d83de66707f2c5196bc087101ec"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel_sklearn = LinearRegression(fit_intercept=False)\nmodel_sklearn.fit(x1, y1)\nplot_synthetic(model_sklearn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7df41fa95af82c12462149684fbc659da0d371e7"},"cell_type":"code","source":"model_sklearn.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9223dbfb54e2171c5819d556f9e61e7e9de6b03"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nplt.figure()\nfig, axes = plt.subplots(2, 2)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x1_ = StandardScaler().fit_transform(x1)\n    y1_ = StandardScaler().fit_transform(y1.reshape(-1, 1))\nmodel_sklearn = LinearRegression(fit_intercept=False)\nmodel_sklearn.fit(x1_, y1_)\naxes[0, 0].scatter(x1_, y1_)\naxes[0, 0].plot(x1_, model_sklearn.predict(x1_), 'r')\nprint(1, \n      ' r2=', r2_error(y1_, model_sklearn.predict(x1_)), \n      ', mae=', mae_error(y1_, model_sklearn.predict(x1_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x2_ = StandardScaler().fit_transform(x2)\n    y2_ = StandardScaler().fit_transform(y2.reshape(-1, 1))\nmodel_sklearn = LinearRegression(fit_intercept=False)\nmodel_sklearn.fit(x2_, y2_)\naxes[0, 1].scatter(x2_, y2_)\naxes[0, 1].plot(x2_, model_sklearn.predict(x2_), 'r')\nprint(2, \n      ' r2=', r2_error(y2_, model_sklearn.predict(x2_)), \n      ', mae=', mae_error(y2_, model_sklearn.predict(x2_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x3_ = StandardScaler().fit_transform(x3)\n    y3_ = StandardScaler().fit_transform(y3.reshape(-1, 1))\nmodel_sklearn = LinearRegression(fit_intercept=False)\nmodel_sklearn.fit(x3_, y3_)\naxes[1, 0].scatter(x3_, y3_)\naxes[1, 0].plot(x3_, model_sklearn.predict(x3_), 'r')\nprint(3, \n      ' r2=', r2_error(y3_, model_sklearn.predict(x3_)), \n      ', mae=', mae_error(y3_, model_sklearn.predict(x3_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x4_ = StandardScaler().fit_transform(x4)\n    y4_ = StandardScaler().fit_transform(y4.reshape(-1, 1))\nmodel_sklearn = LinearRegression(fit_intercept=False)\nmodel_sklearn.fit(x4_, y4_)\naxes[1, 1].scatter(x4_, y4_)\naxes[1, 1].plot(x4_, model_sklearn.predict(x4_), 'r')\nprint(4, \n      ' r2=', r2_error(y4_, model_sklearn.predict(x4_)), \n      ', mae=', mae_error(y4_, model_sklearn.predict(x4_)))\nfor ax in axes.reshape(1, -1)[0]:\n    ax.set_xlim((3 - np.mean(x1))/ np.std(x1), (20 - np.mean(x1))/ np.std(x1))\n    ax.set_ylim((2 - np.mean(y1))/ np.std(y1), (14 - np.mean(y1))/ np.std(y1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a46acf2af5e724f0619765aea7fddddee2f6b92"},"cell_type":"markdown","source":" ### Statsmodels\n\nhttps://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html "},{"metadata":{"trusted":true,"_uuid":"3831c4c0bcee8485a07862d14812299345ca9115"},"cell_type":"code","source":"import statsmodels.api as sm\n\nplt.figure()\nfig, axes = plt.subplots(2, 2)\naxes[0, 0].scatter(x1, y1)\nmodel_statsmodels = sm.OLS(y1, x1).fit()\npredictions = model_statsmodels.predict(x1)\naxes[0, 0].plot(x1, predictions, 'r')\nprint(1, \n      ' r2=', r2_error(y1, predictions), \n      ', mae=', mae_error(y1, predictions))\naxes[0, 1].scatter(x2, y2)\nmodel_statsmodels = sm.OLS(y2, x2).fit()\npredictions = model_statsmodels.predict(x2)\naxes[0, 1].plot(x2, predictions, 'r')\nprint(2, \n      ' r2=', r2_error(y2, predictions), \n      ', mae=', mae_error(y2, predictions))\naxes[1, 0].scatter(x3, y3)\nmodel_statsmodels = sm.OLS(y3, x3).fit()\npredictions = model_statsmodels.predict(x3)\naxes[1, 0].plot(x3, predictions, 'r')\nprint(3, \n      ' r2=', r2_error(y3, predictions), \n      ', mae=', mae_error(y3, predictions))\naxes[1, 1].scatter(x4, y4)\nmodel_statsmodels = sm.OLS(y4, x4).fit()\npredictions = model_statsmodels.predict(x4)\naxes[1, 1].plot(x4, predictions, 'r')\nprint(4, \n      ' r2=', r2_error(y4, predictions), \n      ', mae=', mae_error(y4, predictions))\nfor ax in axes.reshape(1, -1)[0]:\n    ax.set_xlim(3, 20)\n    ax.set_ylim(2, 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b6fa3277c554a6bdf5b8d7c259a3c64c8c94d02"},"cell_type":"markdown","source":"### Scipy\n"},{"metadata":{"trusted":true,"_uuid":"3dce1cfe8ae67f5933a71d4892fc085d4cb34a5a"},"cell_type":"code","source":"import scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20b41463c55092e96c159a511639f11084a2f713","_kg_hide-input":false},"cell_type":"code","source":"plt.figure()\nfig, axes = plt.subplots(2, 2)\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x1.T[0], y1)\nprediction = intercept + slope*x1.T[0]\naxes[0, 0].scatter(x1, y1)\naxes[0, 0].plot(x1, prediction, 'r')\nprint(1, \n      ' r2=', r2_error(y1, prediction), \n      ', mae=', mae_error(y1, prediction))\n\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x2.T[0], y2)\nprediction = intercept + slope*x2.T[0]\naxes[0, 1].scatter(x2, y2)\naxes[0, 1].plot(x2, prediction, 'r')\nprint(2, \n      ' r2=', r2_error(y2, prediction), \n      ', mae=', mae_error(y2, prediction))\n\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x3.T[0], y3)\nprediction = intercept + slope*x3.T[0]\naxes[1, 0].scatter(x3, y3)\naxes[1, 0].plot(x3, prediction, 'r')\nprint(3, \n      ' r2=', r2_error(y3, prediction), \n      ', mae=', mae_error(y3, prediction))\n\nslope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x4.T[0], y4)\nprediction = intercept + slope*x4.T[0]\naxes[1, 1].scatter(x4, y4)\naxes[1, 1].plot(x4, prediction, 'r')\nprint(4, \n      ' r2=', r2_error(y4, prediction), \n      ', mae=', mae_error(y4, prediction))\nfor ax in axes.reshape(1, -1)[0]:\n    ax.set_xlim(3, 20)\n    ax.set_ylim(2, 14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b0246a772debf28af1373ae359a51745af41c72"},"cell_type":"markdown","source":"## Comparison of all methods"},{"metadata":{"trusted":true,"_uuid":"7191ccd9f5eef75b2f8ee53b90d43736597d0700"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a57b3ab0f3bff5930691866f82d104fac3bb6c3b"},"cell_type":"markdown","source":"## Analysis of execution time"},{"metadata":{"trusted":true,"_uuid":"0326136a00c0aead6d97ea255c4dd2437416c0fa"},"cell_type":"code","source":"import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72af2ffc87783e6ee7f6525aed99a53e0a3a48b0"},"cell_type":"code","source":"def data(size):\n    xt = np.linspace(0, 1, size)\n    yt = xt + [np.random.uniform(0, 0.5) for i in xt]\n    return xt, yt\nx, y = data(1000)\nplt.scatter(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2960489609fb238609da57da4387d6a516194eec"},"cell_type":"code","source":"def linear_regression_time_check(sizes_range):\n    time_series = []\n    for size in sizes_range:\n        x, y = data(size)\n        x = DataFrame([[1] * len(x), list(x)]).values.T\n        \n        now = datetime.datetime.now()\n        model_mine = linear_regression()\n        model_mine.fit(x, y)\n        model_mine.predict(x)\n\n        then = datetime.datetime.now()\n        delta = then - now\n        time_series.append(delta.microseconds)\n    return time_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef4d9b3c2fd65706ccb0f5921b4eb59cb2d6a75c"},"cell_type":"code","source":"def sklearn_time_check(sizes_range):\n    time_series = []\n    for size in sizes_range:\n        x, y = data(size)\n        x = DataFrame([[1] * len(x), list(x)]).values.T\n        \n        now = datetime.datetime.now()\n        model_mine = LinearRegression()\n        model_mine.fit(x, y)\n        model_mine.predict(x)\n\n        then = datetime.datetime.now()\n        delta = then - now\n        time_series.append(delta.microseconds)\n    return time_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa9569866cd9255d4e64df9a1a5b69441aca768f"},"cell_type":"code","source":"# sizes = [int(i) for i in np.logspace(0, 7, 100)]\n# print(sizes)\n# linear_time = linear_regression_time_check(sizes)\n# sklearn_time = sklearn_time_check(sizes)\n# plt.plot(sizes, linear_time, label='simple hand regression')\n# plt.plot(sizes, sklearn_time, label='sklearn regression')\n# plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6f38d927d8381b7122707a4dbf32f26d6c39d1c"},"cell_type":"markdown","source":"# Polynomial Regresssion"},{"metadata":{"_uuid":"78ab9116d991d7867741bb0f381a56acff52ab28"},"cell_type":"markdown","source":"$y=a_nx^n + \\dots + a_1x+b$ in a linear form, where\n\n$y$ - objective value,\n\n$x$ -  parameter value,\n\n$a_i, b$ - model coefficients\n\nor\n\n$Y=XW$ in matrix form, where\n\n$Y = \\{y_1, y_2, \\dots, y_k\\}$, $y_i$ - objective column vector,\n\n$X = \\{1, x_1, x_2, \\dots, y_m, \\dots, x_1^n, x_2^n, \\dots, y_m^n, Пx_j\\}$, $x_j$ -  columns vector of $j$-th parameter (NB! free term is included)\n\n$n$ - number of points, the length of vectors $y_i$ and $x_j$.\n\n$Y_{n \\times k}=X_{n \\times m}W_{m \\times k}$ \n\n## Algorithm\n(analytical approach)\n\n$Y$ - true objective values\n\n$\\hat{Y} = XW$ - predicted objective values \n\nTask: $||Y-\\hat{Y} ||_2 \\to min$ (OLS formulation)"},{"metadata":{"trusted":true,"_uuid":"241bf7908ce0c98ec715f444811ebc6ff4f38507"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nmodel_degree = lambda d: Pipeline([('PF', PolynomialFeatures(d)), ('LR', LinearRegression(fit_intercept=False))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"486e7cf42a8c08ae1eda933f14dc95fe08220e45"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom pandas import Series, DataFrame\n\nplt.figure()\nfig, axes = plt.subplots(2, 2)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x1_ = StandardScaler().fit_transform(x1)\n    y1_ = StandardScaler().fit_transform(y1.reshape(-1, 1))\nmodel_sklearn = model_degree(2)\nmodel_sklearn.fit(x1_, y1_)\nprediction = DataFrame(model_sklearn.predict(x1_), index = x1_[:, 0]).loc[np.sort(list((x1_[:, 0])))]\naxes[0, 0].scatter(x1_, y1_)\naxes[0, 0].plot(prediction.index, prediction, 'r')\nprint(1, \n      ' r2=', r2_error(y1_, model_sklearn.predict(x1_)), \n      ', mae=', mae_error(y1_, model_sklearn.predict(x1_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x2_ = StandardScaler().fit_transform(x2)\n    y2_ = StandardScaler().fit_transform(y2.reshape(-1, 1))\nmodel_sklearn =  model_degree(2)\nmodel_sklearn.fit(x2_, y2_)\nprediction = DataFrame(model_sklearn.predict(x2_), index = x2_[:, 0]).loc[np.sort(list((x2_[:, 0])))]\naxes[0, 1].scatter(x2_, y2_)\naxes[0, 1].plot(prediction.index, prediction, 'r')\nprint(2, \n      ' r2=', r2_error(y2_, model_sklearn.predict(x2_)), \n      ', mae=', mae_error(y2_, model_sklearn.predict(x2_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x3_ = StandardScaler().fit_transform(x3)\n    y3_ = StandardScaler().fit_transform(y3.reshape(-1, 1))\nmodel_sklearn =  model_degree(2)\nmodel_sklearn.fit(x3_, y3_)\nprediction = DataFrame(model_sklearn.predict(x3_), index = x3_[:, 0]).loc[np.sort(list((x3_[:, 0])))]\naxes[1, 0].scatter(x3_, y3_)\naxes[1, 0].plot(prediction.index, prediction, 'r')\nprint(3, \n      ' r2=', r2_error(y3_, model_sklearn.predict(x3_)), \n      ', mae=', mae_error(y3_, model_sklearn.predict(x3_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x4_ = StandardScaler().fit_transform(x4)\n    y4_ = StandardScaler().fit_transform(y4.reshape(-1, 1))\nmodel_sklearn =  model_degree(2)\nmodel_sklearn.fit(x4_, y4_)\nprediction = DataFrame(model_sklearn.predict(x4_), index = x4_[:, 0]).loc[np.sort(list((x4_[:, 0])))]\naxes[1, 1].scatter(x4_, y4_)\naxes[1, 1].plot(prediction.index, prediction, 'r')\nprint(4, \n      ' r2=', r2_error(y4_, model_sklearn.predict(x4_)), \n      ', mae=', mae_error(y4_, model_sklearn.predict(x4_)))\nfor ax in axes.reshape(1, -1)[0]:\n    ax.set_xlim((3 - np.mean(x1))/ np.std(x1), (20 - np.mean(x1))/ np.std(x1))\n    ax.set_ylim((2 - np.mean(y1))/ np.std(y1), (14 - np.mean(y1))/ np.std(y1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"335031fbe3cdd1bb35442ce473e2e857b95f1dd0"},"cell_type":"markdown","source":"# Weighted Accurasy Function"},{"metadata":{"trusted":true,"_uuid":"3dc9abf0b353f89e6439944a33a5f1858fc9d68a"},"cell_type":"markdown","source":"\n\\begin{equation}\n    Y = XW + e\n\\end{equation}\n\n\\begin{equation}\n    MSE: e = (Y - \\hat Y)^2 = {(Y - XW)}^T(Y - XW)\n\\end{equation}\n\n\\begin{equation}\n    WMSE: e = {(Y - XW)}^T diag(k) (Y - XW)\n\\end{equation}\n"},{"metadata":{"trusted":true,"_uuid":"311cf601060979188b5c80aaea09c8792375447a"},"cell_type":"code","source":"from sklearn.neighbors import DistanceMetric\n# print((max(x1)-min(x1))/N)\ndef density_coefficients(x, epsilon=2):\n    x = x.reshape(-1, 1)\n    metric = DistanceMetric.get_metric('euclidean')\n    dist_matrix = metric.pairwise(x)\n    N_in_epsilon = (dist_matrix < epsilon).sum(axis=0)\n    density = 1 - 1 / np.array(N_in_epsilon)\n    return density\n\nimport matplotlib.colors as colors\nd = density_coefficients(y4)\nax = plt.scatter(x4, y4, c=d, norm=colors.Normalize(vmin=0, vmax=1))\nplt.colorbar(ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b6e9876047f4d2f58fcf8503d38d82bd3578a07"},"cell_type":"code","source":"class polynomial_weighted_regression:\n    def __init__(self, degree):\n        self.weights = None\n        self.degree = degree\n        \n    def fit(self, x, y):\n        k = density_coefficients(y)\n        k = k * 2\n        print(k)\n        x = DataFrame(np.array([list(x[:, 0]**i) for i in range(self.degree + 1)]).T).values\n        y = DataFrame(y).values\n        temp1 = np.dot(np.dot(x.T, np.diag(k)), x)\n        try:\n            temp2 = np.linalg.inv(temp1)\n        except:\n            temp1 += np.diag([np.random.uniform()] * temp1.shape[0])\n            temp2 = np.linalg.inv(temp1)\n        temp3 = np.dot(np.dot(temp2,  x.T), np.diag(k))\n        self.weights = np.dot(temp3, y)\n        return self\n    \n    def predict(self, x):\n        x = DataFrame(np.array([list(x[:, 0]**i) for i in range(self.degree + 1)]).T).values\n        y = np.dot(x, self.weights)\n        return y[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55a64e3a4c366686941c0ea0b4876ce4ef9d25e9"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom pandas import Series, DataFrame\ndegree = 4\n\nplt.figure(figsize=(20, 20))\nfig, axes = plt.subplots(2, 2)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x1_ = StandardScaler().fit_transform(x1)\n    y1_ = StandardScaler().fit_transform(y1.reshape(-1, 1))\naxes[0, 0].scatter(x1_, y1_)\nmodel_sklearn = model_degree(degree)\nmodel_sklearn.fit(x1_, y1_)\nprediction = DataFrame(model_sklearn.predict(x1_), index = x1_[:, 0]).loc[np.sort(list((x1_[:, 0])))]\naxes[0, 0].plot(prediction.index, prediction, 'r')\nmodel_sklearn = polynomial_weighted_regression(degree=degree)\nmodel_sklearn.fit(x1_, y1_)\nprediction = DataFrame(model_sklearn.predict(x1_), index = x1_[:, 0]).loc[np.sort(list((x1_[:, 0])))]\naxes[0, 0].plot(prediction.index, prediction, 'g')\nprint(1, 'r2=', r2_error(y1_, prediction.values), ', mae=', mae_error(y1_, prediction.values))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x2_ = StandardScaler().fit_transform(x2)\n    y2_ = StandardScaler().fit_transform(y2.reshape(-1, 1))\naxes[0, 1].scatter(x2_, y2_)\nmodel_sklearn = model_degree(degree)\nmodel_sklearn.fit(x2_, y2_)\nprediction = DataFrame(model_sklearn.predict(x2_), index = x2_[:, 0]).loc[np.sort(list((x2_[:, 0])))]\naxes[0, 1].plot(prediction.index, prediction, 'r')\nmodel_sklearn =  polynomial_weighted_regression(degree=degree)\nmodel_sklearn.fit(x2_, y2_)\nprediction = DataFrame(model_sklearn.predict(x2_), index = x2_[:, 0]).loc[np.sort(list((x2_[:, 0])))]\naxes[0, 1].plot(prediction.index, prediction, 'g')\nprint(2, ' r2=', r2_error(y2_, model_sklearn.predict(x2_)), ', mae=', mae_error(y2_, model_sklearn.predict(x2_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x3_ = StandardScaler().fit_transform(x3)\n    y3_ = StandardScaler().fit_transform(y3.reshape(-1, 1))\naxes[1, 0].scatter(x3_, y3_)\nmodel_sklearn = model_degree(degree)\nmodel_sklearn.fit(x3_, y3_)\nprediction = DataFrame(model_sklearn.predict(x3_), index = x3_[:, 0]).loc[np.sort(list((x3_[:, 0])))]\naxes[1, 0].plot(prediction.index, prediction, 'r')\nmodel_sklearn =  polynomial_weighted_regression(degree=degree)\nmodel_sklearn.fit(x3_, y3_)\nprediction = DataFrame(model_sklearn.predict(x3_), index = x3_[:, 0]).loc[np.sort(list((x3_[:, 0])))]\naxes[1, 0].plot(prediction.index, prediction, 'g')\nprint(3, \n      ' r2=', r2_error(y3_, model_sklearn.predict(x3_)), \n      ', mae=', mae_error(y3_, model_sklearn.predict(x3_)))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    x4_ = StandardScaler().fit_transform(x4)\n    y4_ = StandardScaler().fit_transform(y4.reshape(-1, 1))\naxes[1, 1].scatter(x4_, y4_)\nmodel_sklearn = model_degree(degree)\nmodel_sklearn.fit(x4_, y4_)\nprediction = DataFrame(model_sklearn.predict(x4_), index = x4_[:, 0]).loc[np.sort(list((x4_[:, 0])))]\naxes[1, 1].plot(prediction.index, prediction, 'r')\nmodel_sklearn =  polynomial_weighted_regression(degree=degree)\nmodel_sklearn.fit(x4_, y4_)\nprediction = DataFrame(model_sklearn.predict(x4_), index = x4_[:, 0]).loc[np.sort(list((x4_[:, 0])))]\naxes[1, 1].plot(prediction.index, prediction, 'g')\nprint(4, \n      ' r2=', r2_error(y4_, model_sklearn.predict(x4_)), \n      ', mae=', mae_error(y4_, model_sklearn.predict(x4_)))\nfor ax in axes.reshape(1, -1)[0]:\n    ax.set_xlim((3 - np.mean(x1))/ np.std(x1), (20 - np.mean(x1))/ np.std(x1))\n    ax.set_ylim((2 - np.mean(y1))/ np.std(y1), (14 - np.mean(y1))/ np.std(y1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4844c6b62eb28992673c72981fa4af1a871de3e"},"cell_type":"code","source":"class weighted_polynomial_regression:\n    def __init__(self, degree=2):\n        self.coef_ = ()\n        self.degree = degree\n    \n    def fit(self, x, y):\n        k = density_coefficients(x)\n        k = k / sum(k)\n        x = np.array([list(x**i) for i in range(self.degree + 1)]).T\n        temp = np.dot(np.dot(x.T, np.diag(k)), x)\n        temp += np.diag([1e-4 * np.random.random() for i in range(self.degree + 1)])\n        temp2 = np.dot(np.linalg.inv(temp), x.T)\n        self.coef_ = tuple(np.dot(np.dot(temp2, np.diag(k)), y.reshape(-1, 1)))\n        print(self.coef_)\n        return self\n    \n    def predict(self, x):\n        return np.array([a * x**i for i, a in enumerate(self.coef_)]).sum(axis=0)\n    \n    def r2_score(self, y, y_):\n        return 1 - ((y  - y_) ** 2).sum() / ((y - y.mean()) ** 2).sum()\n    \n    def mae(self, y, y_):\n        return (abs(y - y_)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4077c4866376d6514a3c93a30f5fea513238520"},"cell_type":"code","source":"model1_ = weighted_polynomial_regression(degree=10).fit(x3, y3)\npred1_ = model1_.predict(test_x)\nplt.scatter(x3, y3)\nplt.scatter(test_x, test_y)\nplt.plot(test_x, pred1_)\nprint(model1_.r2_score(test_y, pred1_))\nprint(model1_.mae(test_y, pred1_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff694e7c52b3a018a6bf33ce6a500c4b016ccca5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e23d5687e9c343d924063cf6848b7dacd342a6c7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd406d6112bfeee6c56e0680d9a6590c862fcb0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"368ba40bce2e19b8605de257c6e92f42c904ab87"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcbfc100fd9c64065ab3ca2b81daa98f67929193"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}