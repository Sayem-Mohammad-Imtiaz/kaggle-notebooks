{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nimport io\n\n# this is needed because misc.imread is deprecated\nimport imageio\n\n# below needs this to run on terminal:  brew install graphviz\nfrom sklearn.tree import export_graphviz\n\n# weird! this needs to run both: conda install graphviz AND pip install graphviz\nimport graphviz\n\n# requires:  conda install pydotplus\n#import pydotplus as pydot","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# use encoding to avoid error \"UnicodeDecodeError: 'utf-8' codec can't decode byte\"\ndiabetes_data = pd.read_csv( '../input/diabetes.csv', encoding='latin-1' )\n\ndiabetes_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdbc21ff-04c9-4348-9eed-8eb480557cd8","_uuid":"64cc8d12afab8c9554f325bc226b8ce8f931c494","collapsed":true,"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\n# plot histograms to reveal how the data is skewed\n\nfig, axes = plt.subplots( nrows=3, ncols=3 )\n( ageHist, pregHist, glucoseHist, bldPressHist, triThickHist\n , insulHist, bmiHist, pedigrHist, placeholder ) = axes.flatten()\n\nfig.set_size_inches( 10, 5 )\n\nageHist.hist( diabetes_data[ \"Age\" ] )\nageHist.set_title( 'Age freq' )\n\npregHist.hist( diabetes_data[ \"Pregnancies\" ] )\npregHist.set_title( 'Pregnancy freq' )\n\nglucoseHist.hist( diabetes_data[ \"PlasmaGlucose\" ] )\nglucoseHist.set_title( 'Glucose freq' )\n\nbldPressHist.hist( diabetes_data[ \"DiastolicBloodPressure\" ] )\nbldPressHist.set_title( 'Blood pressure freq' )\n\ntriThickHist.hist( diabetes_data[ \"TricepsThickness\" ] )\ntriThickHist.set_title( 'Triceps freq' )\n\ninsulHist.hist( diabetes_data[ \"SerumInsulin\" ] )\ninsulHist.set_title( 'Insulin freq' )\n\nbmiHist.hist( diabetes_data[ \"BMI\" ] )\nbmiHist.set_title( 'BMI freq' )\n\npedigrHist.hist( diabetes_data[ \"DiabetesPedigree\" ] )\npedigrHist.set_title( 'Pedigree freq' )\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07469562-a153-4414-8290-1459416e0b37","_uuid":"ce4538c992f08b80322143a8a97b7c486168927b","collapsed":true,"trusted":true},"cell_type":"code","source":"# data has too many young people\n# use log to flatten Age out a bit\nimport numpy as np\ndiabetes_data = diabetes_data.assign( log_Age = lambda x: \n                                 np.log( x[ 'Age' ] ) )\n\n# apply zscore for other features:  glucose, blood pressure\n# triceps thickness, insulin, BMI\nfrom scipy.stats import zscore\n\ndiabetes_data = diabetes_data.assign( zscore_glucose = zscore( diabetes_data[ 'PlasmaGlucose' ] ) )\ndiabetes_data = diabetes_data.assign( zscore_pressure = zscore( diabetes_data[ 'DiastolicBloodPressure' ] ) )\ndiabetes_data = diabetes_data.assign( zscore_thick = zscore( diabetes_data[ 'TricepsThickness' ] ) )\ndiabetes_data = diabetes_data.assign( zscore_insulin = zscore( diabetes_data[ 'SerumInsulin' ] ) )\ndiabetes_data = diabetes_data.assign( zscore_bmi = zscore( diabetes_data[ 'BMI' ] ) )\n\n# apply min-max for other features:  pregnancy, diabetes pedigree\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nminMaxData = pd.DataFrame( scaler.fit_transform( diabetes_data.loc[ :, [ 'Pregnancies','DiabetesPedigree' ] ] )\n                         , columns = [ 'minMaxPreg', 'minMaxPedigree' ] )\ndiabetes_data = pd.concat( [ diabetes_data, minMaxData ], axis = 1, join = 'inner' )\n\ndiabetes_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f64f678-1f06-43c6-918e-e313e1190076","_uuid":"18f139f1d82ad08080cf312216baf9c5bdc12588","collapsed":true,"trusted":true},"cell_type":"code","source":"# remove unneeded features\ndiabetes_copy = diabetes_data.copy(deep=True)\ndel diabetes_copy[ 'Age' ]\ndel diabetes_copy[ 'PlasmaGlucose' ]\ndel diabetes_copy[ 'DiastolicBloodPressure' ]\ndel diabetes_copy[ 'TricepsThickness' ]\ndel diabetes_copy[ 'SerumInsulin' ]\ndel diabetes_copy[ 'BMI' ]\ndel diabetes_copy[ 'DiabetesPedigree' ]\ndel diabetes_copy[ 'Pregnancies' ]\ndel diabetes_copy[ 'PatientID' ]\n\ndiabetes_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98b6517b-9563-4c5f-adab-292ec252a2a6","_uuid":"f6631abf69b85ba2949239ed5793751c7d38715a","collapsed":true,"trusted":true},"cell_type":"code","source":"# split data into 70% training 30% testing\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split( diabetes_copy, test_size = 0.3 )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92c64210-34bc-4580-b292-3ada6ce11cf0","_uuid":"ab19414f63dc2d79597d0c449f063eddaa13f6df","collapsed":true,"trusted":true},"cell_type":"code","source":"# select features to train and test\nfeatures = [ \"log_Age\", \"zscore_glucose\", \"zscore_pressure\", \"zscore_thick\"\n            , \"zscore_insulin\", \"zscore_bmi\", \"minMaxPreg\", \"minMaxPedigree\" ]\nX_train = train[ features ]\nY_train = train[ \"Diabetic\" ]\nX_test = test[ features ]\nY_test = test[ \"Diabetic\" ]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"50c34f72-2b87-4b7a-9b67-b3c7be0eb324","_uuid":"ed885ff4b82d3071237f3997bc5edd07f701fa7f","collapsed":true,"trusted":true},"cell_type":"code","source":"# train a Boosted Decision tree model to predict Diabetic (0 or 1)\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# this will create 200 weak learner decision trees and use them to build a strong classifier\nbdt = AdaBoostClassifier( DecisionTreeClassifier( max_depth = 4 )\n                         , algorithm=\"SAMME\"\n                         , n_estimators=200 )\ndt = bdt.fit( X_train, Y_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d95f7f6-b9f1-4be8-bd9b-55781b8cf844","_uuid":"888441364632566615efdc84796a3308c2d629c6","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz \n%matplotlib inline\n\ndef show_tree( theTree, features, width, height ):\n    the_data = tree.export_graphviz( theTree, out_file = None, \n                             feature_names = features )\n    \n    # find first { in string\n    indexDot = the_data.find( '{' )\n    # hack to resize graph because kaggle doesn't have a way to change size on graphviz dot string\n    the_data = the_data[ : indexDot + 1 ] + '\\n graph [size=\"%d,%d\"]; ' % (width, height) + the_data[ indexDot + 1 : ]\n    \n    graph = graphviz.Source( the_data )\n    return graph\n\n# modified function to handle ensemble instead of a single decision tree\ndef show_tree2( theTree, index, features, path ):\n    f = io.StringIO() \n    export_graphviz( theTree[ index ], out_file=f, feature_names=features ) \n    pydotplus.graph_from_dot_data( f.getvalue() ).write_png( path )\n    #img = misc.imread( path ) \n    img = imageio.imread( path )\n    plt.rcParams[ \"figure.figsize\" ] = ( 20, 20 ) \n    plt.imshow( img )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0be5b5e4-7263-4734-bdbd-ca8df6efc405","_uuid":"520a44a5cfde2f5e192512437fd4c82f12519ab2","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"show_tree( dt[ 0 ], features, 13, 13 )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9e67be3-f1b8-43eb-a061-cdca118f030e","_uuid":"5fd8dff0e17d2aa87954cef183ac240374ac1f95","collapsed":true,"trusted":true},"cell_type":"code","source":"show_tree( dt[ 199 ], features, 13, 13 )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fedc7b4a-dd3f-4b38-8182-2f445193e106","_uuid":"dabade5a7dacd5d65b63beb1df7b57d353c28894","collapsed":true,"trusted":true},"cell_type":"code","source":"for name, importance in zip( X_train.columns, dt.feature_importances_ ):\n    print( name, importance )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"681c9993-2799-41fd-9a53-f413a133c878","_uuid":"c8b6fabd2b9b542040fcae04fb62304eab2e1511","collapsed":true,"trusted":true},"cell_type":"code","source":"# adapted from a function by paultimothymooney\ndef plot_feature_importances( decTree, trainingFeatures ):\n    featureList = trainingFeatures.columns.values\n    \n    # sort both arrays by importance but get only the sorted feature names\n    featureList = [ x for _, x in sorted( zip( decTree.feature_importances_, featureList ) ) ]\n    \n    featureSize = len( featureList )\n    plt.barh( range( featureSize ), sorted( decTree.feature_importances_ ) )\n    plt.ylabel( \"Feature\" )\n    plt.yticks( np.arange( featureSize ), featureList )\n    plt.xlabel( \"Importance\" )\n    \nfrom sklearn import model_selection\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    plt.figure().set_size_inches( 5, 5 )\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = model_selection.learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\ndef plot_ROC( falsePositiveRate, truePositiveRate, areaUnderCurve ):\n    fig = plt.figure()\n    fig.set_size_inches( 15, 5 )\n    rocCurve = fig.add_subplot( 1, 2, 1 )\n\n    rocCurve.plot( falsePositiveRate, truePositiveRate, color = 'darkgreen',\n             lw = 2, label = 'ROC curve (area = %0.2f)' % areaUnderCurve )\n    rocCurve.plot( [0, 1], [0, 1], color = 'navy', lw = 1, linestyle = '--' )\n    rocCurve.grid()\n    plt.xlim( [0.0, 1.0] )\n    rocCurve.set_xticks( np.arange( -0.1, 1.0, 0.1 ) )\n    plt.ylim( [0.0, 1.05] )\n    rocCurve.set_yticks( np.arange( 0, 1.05, 0.1 ) )\n    plt.xlabel( 'False Positive Rate' )\n    plt.ylabel( 'True Positive Rate' )\n    plt.title( 'ROC' )\n    rocCurve.legend( loc = \"lower right\" )\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8e2aa2661d16c1d8cdfc41b2695e7f5c6746915c"},"cell_type":"code","source":"\nplot_feature_importances( dt, X_train )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5bab58ee-2c74-44af-b1f3-6189173322da","_uuid":"54a1d809bcea4632a556172d821c4e3cf7cb314c","collapsed":true,"trusted":true},"cell_type":"code","source":"Y_pred = dt.predict( X_test )\nY_probas = dt.predict_proba( X_test )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ab5691c-df48-4146-a9e7-5db7a7dfad90","_uuid":"e1bc5968d6bef463c442d299d64f1049896fe357","collapsed":true,"trusted":true},"cell_type":"code","source":"# see evaluation metrics\n\nimport numpy as np\nfrom sklearn import metrics\n\n# Receiver Operating Characteristic - precision/recall lift\n# Y_probas[:,1] is the array of probabilities for Diabetic = 1 at the leaf level\nfpr, tpr, thresholds = metrics.roc_curve( Y_test.values, Y_probas[:,1] )\nauc = metrics.auc( fpr, tpr )\n\n# plot ROC curve\nplot_ROC( fpr, tpr, auc )\nplt.show()\n\n# alternative using scikitplot:  conda install -c conda-forge scikit-plot\n\n#import scikitplot as skplt\n#skplt.metrics.plot_roc_curve( Y_test, Y_probas )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"037ba3e0-e8cd-420e-8a0b-47c8cfab0be6","_uuid":"5f36379e67236396fc60d0c0a5701db28b8729d1","collapsed":true,"trusted":true},"cell_type":"code","source":"# score the model on the test data\n\ndef scoreModel( Y_test, Y_pred ):\n    # show accuracy, precision and recall\n    from sklearn.metrics import accuracy_score\n    score = accuracy_score( Y_test, Y_pred )\n    print( \"Accuracy: %.3f \" % round( score, 3 ) )\n    \n    from sklearn.metrics import precision_score\n    precScore = precision_score( Y_test, Y_pred, average = 'binary' )\n    print( \"Precision: %.3f \" % round( precScore, 3 ) )\n    \n    from sklearn.metrics import recall_score\n    recScore = recall_score( Y_test, Y_pred, average = 'binary' ) \n    print( \"Recall: %.3f \" % round( recScore, 3 ) )\n    # confusion matrix\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix( Y_test, Y_pred )\n    print( \"True positives: %d  False negatives: %d\" % ( cm[ 1, 1 ], cm[ 1, 0 ] ) )\n    print( \"False positives: %d  True negatives: %d\" % ( cm[ 0, 1 ], cm[ 0, 0 ] ) )\n    \n    # AUC (area under the curve)\n    auc = metrics.auc( fpr, tpr )\n    print( \"AUC: %.3f\" % auc )\n\nscoreModel( Y_test, Y_pred )","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ac80185e72e5925ac9c0694d5e2f92f5309bcf31"},"cell_type":"code","source":"\n# this is to prevent 100s of DeprecationWarnings for something that is scheduled to\n#  be fixed on the scikit-learn release of August 2018\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ntheLearningCurve = plot_learning_curve( dt, 'Learning Curve For Adaboost Decision Tree', X_train, Y_train, ( 0.92, 1.02 ), 10 )\ntheLearningCurve.figure().set_size_inches( 5, 5 )\ntheLearningCurve.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad23be00-33a1-47cf-a02b-f39e698141d5","_uuid":"e005053d1082dee71d6dc6da8193739fd554c987","collapsed":true,"trusted":true},"cell_type":"code","source":"# copied score of LightGBM using dart for comparison:\n#      Accuracy: 0.945 \n#      Precision: 0.918  <- lower (less of the reported positives are true positives, that is, more false positives) \n#      Recall: 0.913     <- lower (less true positives are reported as positives, that is, more false negatives) \n#      True positives: 1339  False negatives: 128\n#      False positives: 119  True negatives: 2914\n#      AUC: 0.990\n\n# score of LightGBM using gbdt:\n#      Accuracy: 0.946 \n#      Precision: 0.918 \n#      Recall: 0.915 \n#      True positives: 1342  False negatives: 125\n#      False positives: 120  True negatives: 2913\n#      AUC: 0.990","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3541bd41-1943-4438-b1a3-841bdfda8b03","_uuid":"76cb65183b37d7e56bfb6d3201e4aab522654c7e","collapsed":true,"trusted":true},"cell_type":"code","source":"# TODO:  find a MART gradient boosting library\n# install LightGBM:\n\n# run brew install cmake\n# brew install gcc\n# (wait about 50 minutes for the above to complete)\n# git clone --recursive https://github.com/Microsoft/LightGBM ; cd LightGBM\n# export CXX=g++-7 CC=gcc-7\n# mkdir build ; cd build\n# cmake ..\n# make -j4\n\n# line below fails:  No module named 'lightgbm' unless fixed with:  \n# conda config --add chanells conda-forge\n# conda install lightgbm\nimport lightgbm as lgb\n\n# create dataset for LightGBM\nlgb_train = lgb.Dataset( X_train, Y_train )\nlgb_eval = lgb.Dataset( X_test, Y_test, reference = lgb_train )\n\n# configuration:  http://lightgbm.readthedocs.io/en/latest/Parameters.html\nparams = {\n    'task': 'train',\n    'boosting_type': 'dart',\n    # gbdt = Gradient Boosting Decision Tree\n    # dart = Dropouts meet Multiple Additive Regression Trees \n    'objective': 'binary',\n    'metric': { 'auc', 'root_mean_squared_error' },\n    'max_depth': 4,\n    'learning_rate': .5,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 1,\n    'bagging_freq': 0,  # no bagging\n    'verbose': 0,\n    'min_data_in_leaf': 20,  # adjust this to handle overfitting\n    \n    'num_machines': 1,   # can do parallel processing via network\n    \n    'device': 'cpu',   # can use GPUs via OpenCL\n    'gpu_platform_id': -1,\n    'gpu_device_id': -1\n}\n\n# http://lightgbm.readthedocs.io/en/latest/Python-API.html\ngbm = lgb.train( params,\n                lgb_train,\n                num_boost_round = 200,\n                valid_sets = lgb_eval,\n                early_stopping_rounds = 5 )\n\nY_pred = gbm.predict( X_test, num_iteration = gbm.best_iteration )\n\n#for name, importance in zip( X_train.columns, gbm.feature_importances_ ):\n#    print( name, importance )\n\n#from sklearn.metrics import mean_squared_error\n#print( 'The rmse of prediction is:', mean_squared_error( Y_test, Y_pred ) ** 0.5 )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ecfba6f-837e-48e9-8a72-b14288ba692b","_uuid":"d73407fc93acbef14c50669608816b9eb1ca1c4c","collapsed":true,"trusted":true},"cell_type":"code","source":"scoreModel( Y_test, Y_pred.round() )","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"90de8d32626387005e5e1059dffe0ee714034f8c"},"cell_type":"code","source":"# another way of using LightGBM\ngbm = lgb.LGBMClassifier(boosting_type = 'gbdt', \n                         max_depth = 4, \n                         learning_rate = 0.5, \n                         n_estimators = 200, \n                         objective = 'binary',\n                         min_child_samples = 20 )\n\ndt = gbm.fit( X_train, Y_train )\n\nY_pred = dt.predict( X_test )\n\nY_probas = gbm.predict_proba( X_test )\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d3187a13b21f462372dd42bf26dbadd16e08e599"},"cell_type":"code","source":"\n# Receiver Operating Characteristic - precision/recall lift\n# Y_probas[:,1] is the array of probabilities for Diabetic = 1 at the leaf level\nfpr, tpr, thresholds = metrics.roc_curve( Y_test, Y_probas[:,1] )\nauc = metrics.auc( fpr, tpr )\n\n# plot ROC curve\nplot_ROC( fpr, tpr, auc )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a72b070960a4bf6a199d4d2e00f107582817eb50"},"cell_type":"code","source":"scoreModel( Y_test, Y_pred.round() )","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cc13c1dd9f8bb619f7e82d9f8ca07616c308007a"},"cell_type":"code","source":"\n# this is to prevent 100s of DeprecationWarnings for something that is scheduled to\n#  be fixed on the scikit-learn release of August 2018\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ntheLearningCurve = plot_learning_curve( dt, 'Learning Curve For LightGBM Decision Tree', X_train, Y_train, ( 0.92, 1.02 ), 10 )\ntheLearningCurve.figure().set_size_inches( 5, 5 )\ntheLearningCurve.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}