{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Please <B> UPVOTE </B> if you like my notebook","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport imageio\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom tqdm import tqdm_notebook as tqdm\nimport torch.optim as optim\nfrom IPython.display import Image\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autoencoders : \n#### An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”.\n\n#### The simplest form of an autoencoder is a feedforward, non-recurrent neural network similar to single layer perceptrons that participate in multilayer perceptrons (MLP) – having an input layer, an output layer and one or more hidden layers connecting them – where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename = '/kaggle/input/imagesforkernel/Autoencoder_fig.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename='/kaggle/input/imagesforkernel/autoenc.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"IMG_DIR = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our Images ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    choose_img = np.random.choice(os.listdir(IMG_DIR))\n    image_path = os.path.join(IMG_DIR,choose_img)\n    image = imageio.imread(image_path)\n    plt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autoencoder Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Autoencoders(nn.Module):\n    def __init__(self):\n        super().__init__()\n        ### encoder\n        self.conv1 = nn.Conv2d(3,64,5)\n        self.maxpool = nn.MaxPool2d(2,return_indices=True)\n        self.conv2 = nn.Conv2d(64,64,5)\n        self.conv3 = nn.Conv2d(64,128,5)\n        ### decoder\n        self.deconv1 = nn.ConvTranspose2d(128,64,5)\n        self.unpool = nn.MaxUnpool2d(2)\n        self.deconv2 = nn.ConvTranspose2d(64,64,5)\n        self.deconv3 = nn.ConvTranspose2d(64,3,5)\n    \n    def forward(self,x):\n        x = self.conv1(x)\n        x,ind1 = self.maxpool(x)\n        x = self.conv2(x)\n        x,ind2 = self.maxpool(x)\n        x = self.conv3(x)\n        \n        x = self.deconv1(x)\n        x = self.unpool(x,ind2)\n        x = self.deconv2(x)\n        x = self.unpool(x,ind1)\n        x = self.deconv3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading and transforming Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\ntrain_transform = torchvision.transforms.Compose([\n    transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize\n])\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset=torchvision.datasets.ImageFolder('/kaggle/input/celeba-dataset/img_align_celeba/',\n                                                                                       transform=train_transform),\n                                              shuffle=True,batch_size=32,num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Autoencoders().to(device)\n\ncriterian = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training AutoEncoders\n\nNote: It will take approx 30 min for each epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 2 ### increase number of epochs for better result\nfor epoch in tqdm(range(n_epochs)):\n    model.train()\n    iteration = 0\n    for data,_ in tqdm(train_dataloader):\n        optimizer.zero_grad()\n        data = data.to(device)\n        output = model.forward(data)\n        loss = criterian(output,data)\n        loss.backward()\n        optimizer.step()\n        if iteration%1000 == 0:\n            print(f'iteration: {iteration} , loss : {loss.item()}')\n    print(f'epoch: {epoch} loss: {loss.item()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(),'autoencoder.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Autoencoders()\nmodel1.load_state_dict(torch.load('autoencoder.h5'))\nmodel1.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's test our model on 1 image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data,_ in train_dataloader:\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_img = model1(data)\npred_img = pred_img.detach().numpy()\npred_img = pred_img.reshape(32,224,224,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicted Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(pred_img[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Original Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = data.reshape(32,224,224,3)\n\nplt.imshow(new_data[0])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}