{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import h2o\n#connecting to cluster\nh2o.init(strict_version_check=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_csv = \"/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\"\ndata = h2o.import_file(data_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rename(columns={\"PAY_0\": \"PAY_1\"}) #for consistency\ndata.rename(columns={'default.payment.next.month': \"DEFAULT\"}) #easier\n\ncols_names = data.columns #because we know the data type for all the columns (they are all ints)\ncols_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_categorical = ['ID',\n 'LIMIT_BAL',\n  'AGE',\n 'BILL_AMT1',\n 'BILL_AMT2',\n 'BILL_AMT3',\n 'BILL_AMT4',\n 'BILL_AMT5',\n 'BILL_AMT6',\n 'PAY_AMT1',\n 'PAY_AMT2',\n 'PAY_AMT3',\n 'PAY_AMT4',\n 'PAY_AMT5',\n 'PAY_AMT6']\n\ntarget = \"DEFAULT\"\n\ncategorical = [item for item in cols_names if item not in not_categorical and item != target]\ncategorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Onehot encoding (as labels are already encoded as numbers)\n\ndata_onehot = pd.get_dummies(data.as_data_frame(), columns=categorical)\ndata_onehot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the ID column\n\ndata_onehot = data_onehot.drop(columns=['ID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating equally sized bins for age - 5 categories\n\nprint(data_onehot['AGE'].describe())\n\n#add age bins to make it all-inclusive - in case new data may come\n\ndata_onehot['AGE_BINS'] = pd.qcut(data_onehot['AGE'], 5)\n\n#Add age bins for ages (0, 20.999] and (79.0, ) - even though there may be no data for this in the present dataset, it is important to do this in case we have future data\n\ndata_onehot['AGE_BINS_(0, 20.999]'] = 0 #in the same format as after one hot encoding (doing this two cells later)\ndata_onehot['AGE_BINS_(79.0, )'] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_onehot.head() #it works!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we use one hot encoding for these categories\n\ndata_age = pd.get_dummies(data_onehot, columns=['AGE_BINS'])\ndata_age = data_age.drop(columns=['AGE'])\ndata_age.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#some statistical featurs\n\nbill_amt_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\npay_amt_cols = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n\n#mean of Bill_amt and Pay_amt, max, min, std, var\n\ndata_age['BILL_AMT_MEAN'] = data_age[bill_amt_cols].mean(axis=1)\ndata_age['PAY_AMT_MEAN'] = data_age[pay_amt_cols].mean(axis=1)\n\ndata_age['BILL_AMT_MAX'] = data_age[bill_amt_cols].max(axis=1)\ndata_age['PAY_AMT_MAX'] = data_age[pay_amt_cols].max(axis=1)\n\ndata_age['BILL_AMT_MIN'] = data_age[bill_amt_cols].min(axis=1)\ndata_age['PAY_AMT_MIN'] = data_age[pay_amt_cols].min(axis=1)\n\ndata_age['BILL_AMT_MED'] = data_age[bill_amt_cols].median(axis=1)\ndata_age['PAY_AMT_MED'] = data_age[pay_amt_cols].median(axis=1)\n\ndata_age['BILL_AMT_STD'] = data_age[bill_amt_cols].std(axis=1)\ndata_age['PAY_AMT_STD'] = data_age[pay_amt_cols].std(axis=1)\n\ndata_age['BILL_AMT_VAR'] = data_age[bill_amt_cols].var(axis=1)\ndata_age['PAY_AMT_VAR'] = data_age[pay_amt_cols].var(axis=1)\n\n\ndata_age.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#some new variables\n\n#payment fraction of bill statement\nfor i in range(1, 7):        \n    data_age['PAY_FRAC_' + str(i)] = data_age[pay_amt_cols[i-1]] / data_age[bill_amt_cols[i-1]]\ndata_age = data_age.fillna(0)\n\n\n#fraction of credit limit used (bill_amt / limit_bal)\nfor i in range(1, 7):        \n    data_age['USED_CREDIT' + str(i)] = data_age[bill_amt_cols[i-1]] / data_age['LIMIT_BAL']\ndata_age = data_age.fillna(0)\n\n\ndata_age.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_age['PAY_FRAC_1'].max()\n\n\n\n#There are 540. Three simple ways to deal: delete feature, delete rows, set to zero. Have to test.\n\n#Setting to zero\n\nfor i in range (1, 7):\n    #print(len(data_age[data_age['PAY_FRAC_' + str(i)] == np.inf])) #0 of them are -np.inf\n    data_age['PAY_FRAC_' + str(i)] = data_age['PAY_FRAC_' + str(i)].replace({np.inf: 0})\n    #print(len(data_age[data_age['PAY_FRAC_' + str(i)] == np.inf]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling\n\n#Using standard scalar scaling\n#Multiple methods such as min-max scaling, standard scaling, etc. All have different advantages and depend on the distribution of data.\n#Can always change this in the next iterations of the ML pipeline. Trial and error process.\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nscaled_features = data_age.copy()\n\ncol_names = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4' ,'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4' ,'PAY_AMT5', 'PAY_AMT6', 'PAY_FRAC_1', 'PAY_FRAC_2', 'PAY_FRAC_3', 'PAY_FRAC_4', 'PAY_FRAC_5', 'PAY_FRAC_6', 'USED_CREDIT1', 'USED_CREDIT2', 'USED_CREDIT3', 'USED_CREDIT4', 'USED_CREDIT5', 'USED_CREDIT6']\nfeatures = scaled_features[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n\nscaled_features[col_names] = features\nscaled_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_df = pd.DataFrame(scaled_features, columns=['LIMIT_BAL', 'BILL_AMT1', 'PAY_AMT1', 'USED_CREDIT1'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n\nax1.set_title('Before Scaling')\nsns.kdeplot(data_age['LIMIT_BAL'], ax=ax1) #kernel density estimate plot (non-parametric way to estimate the probability density function of a random variable.)\nsns.kdeplot(data_age['BILL_AMT1'], ax=ax1)\nsns.kdeplot(data_age['PAY_AMT1'], ax=ax1)\nsns.kdeplot(data_age['USED_CREDIT1'], ax=ax1)\nax2.set_title('After Standard Scaler')\nsns.kdeplot(scaled_df['LIMIT_BAL'], ax=ax2)\nsns.kdeplot(scaled_df['BILL_AMT1'], ax=ax2)\nsns.kdeplot(scaled_df['PAY_AMT1'], ax=ax2)\nsns.kdeplot(scaled_df['USED_CREDIT1'], ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here how the data is scaled. Now, we have the dataframe *scaled_features.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_features.columns","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}