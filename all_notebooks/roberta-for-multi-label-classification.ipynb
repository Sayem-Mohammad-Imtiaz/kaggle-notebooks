{"cells":[{"metadata":{"extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{},"report_default":{}}}}},"cell_type":"markdown","source":"# # Independence Day: Topic Modelling Hackathon\n\nThis notebook is borrowed from following 2 sources:\n1. https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta\n2. https://www.kaggle.com/sohamxi/juntahack-nlp-final <br>\n\nPlease feel free to cite these if you find it useful for any of your research/project related works.\nThanks!!","execution_count":null},{"metadata":{"extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{},"report_default":{}}}},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/janatahack-independence-day-2020-ml-hackathon/'","execution_count":null,"outputs":[]},{"metadata":{"extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{},"report_default":{}}}},"trusted":true},"cell_type":"code","source":"train = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')\nsubmit = pd.read_csv(path+'sample_submission_UVKGLZE.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{},"report_default":{}}}},"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape, submit.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(np.intersect1d(train.TITLE, test.TITLE)))\nprint(len(np.intersect1d(train.ID, test.ID)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Straight Into Transfer Learning - Roberta + FastAI","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.text import *\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Installing HuggingFace Transformers**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install -q transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\nfrom transformers import AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting Model Classes to try out","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig),\n    'albert':(AlbertForSequenceClassification,AlbertTokenizer, AlbertConfig)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nseed = 10\nuse_fp16 = True\nbs = 4\n\nmodel_type = 'roberta'\npretrained_model_name = 'roberta-large'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setting Seed**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Tokenizer Class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Converting Text to Features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Transformer Model with attention mask (Need to understand More)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        # attention_mask\n        # Mask to avoid performing attention on padding token indices.\n        # Mask values selected in ``[0, 1]``:\n        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tweaking the Configs for this use case","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\nconfig.use_bfloat16 = use_fp16\n#print(config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining The Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining Padding Sequences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Data Bunch ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#col = 'Computer Science'\ndef prep_data_bunch(col):\n    data_transclas = (TextList.from_df(train, cols=['ABSTRACT','TITLE'], processor=transformer_processor)\n                      .split_by_rand_pct(0.1,seed=seed)\n                      .label_from_df(cols= col)\n                      .add_test(test)\n                      .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))\n    \n    return data_transclas\n#data_transclas = prep_data_bunch(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the Tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('[CLS] token :', transformer_tokenizer.cls_token)\n#print('[SEP] token :', transformer_tokenizer.sep_token)\n#print('[PAD] token :', transformer_tokenizer.pad_token)\n#data_transclas.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('[CLS] id :', transformer_tokenizer.cls_token_id)\n#print('[SEP] id :', transformer_tokenizer.sep_token_id)\n#print('[PAD] id :', pad_idx)\n#test_one_batch = data_transclas.one_batch()[0]\n#print('Batch shape : ',test_one_batch.shape)\n#print(test_one_batch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Learner Instance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_learner_instance(data_transclas):\n    learner = Learner(data_transclas, \n                      custom_transformer_model, \n                      opt_func = CustomAdamW, \n                      metrics=[accuracy, error_rate])\n\n    # Show graph of learner stats and metrics after each epoch.\n    learner.callbacks.append(ShowGraph(learner))\n\n    # Put learn in FP16 precision mode. --> Seems to not working\n    if use_fp16: learner = learner.to_fp16()\n    \n    list_layers = [learner.model.transformer.roberta.embeddings,\n              learner.model.transformer.roberta.encoder.layer[0],\n              learner.model.transformer.roberta.encoder.layer[1],\n              learner.model.transformer.roberta.encoder.layer[2],\n              learner.model.transformer.roberta.encoder.layer[3],\n              learner.model.transformer.roberta.encoder.layer[4],\n              learner.model.transformer.roberta.encoder.layer[5],\n              learner.model.transformer.roberta.encoder.layer[6],\n              learner.model.transformer.roberta.encoder.layer[7],\n              learner.model.transformer.roberta.encoder.layer[8],\n              learner.model.transformer.roberta.encoder.layer[9],\n              learner.model.transformer.roberta.encoder.layer[10],\n              learner.model.transformer.roberta.encoder.layer[11],\n              learner.model.transformer.roberta.pooler]\n    \n    learner.split(list_layers)\n    num_groups = len(learner.layer_groups)\n    #print('Learner split in',num_groups,'groups')\n    #print(learner.layer_groups)\n    #print(learner.model)\n    return learner, num_groups\n\n#learner = create_learner_instance(data_transclas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(learner, num_groups, lr, epochs, unfreeze_all):\n    \n    if unfreeze_all == True:\n        learner.unfreeze()\n        learner.fit_one_cycle(epochs, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))\n    else:\n        learner.freeze_to(-2)\n        learner.fit_one_cycle(epochs,max_lr=slice(lr*0.9**num_groups, lr),moms=(0.8,0.9))\n    \n    return learner\n#learner = train_model(learner)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_as_nparray(ds_type, learner, data_transclas):\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in data_transclas.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_sub_file(col, learner, data_transclas):\n    print('Predicting..')\n    test_preds = get_preds_as_nparray(DatasetType.Test, learner, data_transclas)\n    print('Predictions done!')\n    submit[col] = np.argmax(test_preds,axis=1)\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(col, lr, epochs, unfreeze_all):\n    print('Creating DataBunch!')\n    data_transclas = prep_data_bunch(col)\n    print('Prepared DataBunch!')\n    \n    print('Creating Learner instance!')\n    learner, num_groups = create_learner_instance(data_transclas)\n    print('Prepared Learner instance!')\n    \n    print('Training Model!')\n    learner = train_model(learner, num_groups, lr, epochs, unfreeze_all)\n    print('Trained Complete!')\n    \n    print('Creating Submission File!')\n    create_sub_file(col, learner, data_transclas)\n    print('Submission file created!')\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncols = ['Computer Science', 'Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']\n\nlr_dict = {'Computer Science':1e-5\n          , 'Physics': 1e-5\n          , 'Mathematics': 1e-5\n          , 'Statistics': 1e-5\n          , 'Quantitative Biology': 1e-4\n          , 'Quantitative Finance': 1e-4}\nepoch_dict = {'Computer Science':1\n          , 'Physics': 1\n          , 'Mathematics': 1\n          , 'Statistics': 1\n          , 'Quantitative Biology': 1\n          , 'Quantitative Finance': 1}\n\ntuning = {'Computer Science':False\n          , 'Physics': False\n          , 'Mathematics': False\n          , 'Statistics': False\n          , 'Quantitative Biology': False\n          , 'Quantitative Finance': False}\n\nfor col in cols:\n    print('--------------------------')\n    print(f'Executing wrapper for {col}!')\n    main(col, lr = lr_dict[col], epochs = epoch_dict[col], unfreeze_all = tuning[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv(\"predictions_roberta_large_v2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cols:\n    print(submit[col].sum()/submit.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cols:\n    print(train[col].sum()/train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}