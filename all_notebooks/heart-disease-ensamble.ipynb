{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pandas_profiling as pp\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\n#ensembling\nfrom mlxtend.classifier import StackingCVClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/heart-disease-prediction-using-logistic-regression/framingham.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data.dropna(inplace = True) # ignore na\ndata = data.apply(lambda x: x.fillna(x.mean())) #replace na with mean\ntarget = data[\"TenYearCHD\"]\nfeatures = data.drop('TenYearCHD',axis=1)\nsns.histplot(target)\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp.ProfileReport(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote = SMOTE()\nx_smote ,y_smote = smote .fit_resample(features, target)\nsns.histplot(y_smote)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42, shuffle = True)\nxs_train ,xs_test ,ys_train ,ys_test = train_test_split(x_smote ,y_smote , test_size = 0.2 , random_state = 42 ,shuffle = True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM \n## Original Data","metadata":{}},{"cell_type":"code","source":"kernels = ['rbf', 'poly', 'sigmoid']\nbest_svm = None\nbest_score = 0\nbest_pred = None\nall_results = []\nfor kernel in kernels:\n    print(\"Using kernel =\", kernel)\n    for c in range(1, 11):\n        print(\"\\tusing C={}\".format(c), end = \" \")\n        svc =  SVC(kernel=kernel, C=c, probability = True)\n        svc.fit(x_train, y_train)\n        y_pred = svc.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([c, score, kernel])\n        print(\"score = \", score)\n        if score > best_score:\n            best_score = score\n            best_svm = svc\n            best_pred = y_pred\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best score using kernel={} with c = {}\".format(best_svm.kernel, best_svm.C))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using data after oversampling","metadata":{}},{"cell_type":"code","source":"kernels = ['rbf', 'poly', 'sigmoid']\nbest_svm_smote = None\nbest_score_smote = 0\nbest_pred_smote = None\nall_results_smote = []\nfor kernel in kernels:\n    print(\"Using kernel =\", kernel)\n    for c in range(1, 11):\n        print(\"\\tusing C={}\".format(c), end = \" \")\n        svc =  SVC(kernel=kernel, C=c, probability = True)\n        svc.fit(xs_train, ys_train)\n        ys_pred = svc.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([c, score, kernel])\n        print(\"score = \", score)\n        if score > best_score_smote:\n            best_score_smote = score\n            best_svm_smote = svc\n            best_pred_smote = y_pred\nall_results_smote = np.array(all_results_smote, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best score using kernel={} with c = {}\".format(best_svm_smote.kernel, best_svm_smote.C))\ncm = confusion_matrix(y_test,best_pred_smote)\nprint(classification_report(y_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred_smote))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_results[:10][:, 0], all_results[:10][:, 1], label = \"kernel(RBF)\")\nplt.plot(all_results[10:20][:, 0], all_results[10:20][:, 1], label = \"kernel(Poly)\")\nplt.plot(all_results[20:][:, 0], all_results[20:][:, 1], label = \"kernel(sigmoid)\")\nplt.plot(all_results_smote[:10][:, 0], all_results_smote[:10][:, 1], label = \"kernel(RBF)-SMOTE\")\nplt.plot(all_results_smote[10:20][:, 0], all_results_smote[10:20][:, 1], label = \"kernel(Poly)-SMOTE\")\nplt.plot(all_results_smote[20:][:, 0], all_results_smote[20:][:, 1], label = \"kernel(sigmoid)-SMOTE\")\nplt.title(\"Scores on SVM by C value\")\nplt.ylabel(\"score\")\nplt.xticks(range(1, 11))\nplt.xlabel(\"C\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression\n\n## Original Data","metadata":{}},{"cell_type":"code","source":"all_results = []\nbest_lr = None\nbest_score = 0\nbest_pred = None\nworst_param = None\nfor param in range((x_train.shape[1]) + 1):\n    if param != x_train.shape[1]:\n        print(\"Without using param\",features.columns[param], end = \" \")\n        train = x_train.drop(x_train.columns[param], axis=1)\n        test = x_test.drop(x_test.columns[param], axis=1)\n    else:\n        print(\"Using all parameters\", end = \" \")\n    lr_model = LogisticRegression(random_state = 42, max_iter = 10000)\n    lr_model.fit(train , y_train)\n    y_pred = lr_model.predict(test)\n    score = accuracy_score(y_test, y_pred)\n    all_results.append([param, score])\n    print(\"score\", score)\n    if score > best_score:\n        best_lr = lr_model\n        best_score = score\n        best_pred = y_pred\n        worst_param = param\nprint(\"Best without using\", features.columns[worst_param])\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using data after oversampling","metadata":{}},{"cell_type":"code","source":"all_results_smote = []\nbest_lr_smote = None\nbest_score_smote = 0\nbest_pred_smote = None\nworst_param = None\nfor param in range((xs_train.shape[1]) + 1):\n    if param != xs_train.shape[1]:\n        print(\"Without using param\",xs_train.columns[param], end = \" \")\n        train = xs_train.drop(xs_train.columns[param], axis=1)\n        test = xs_test.drop(xs_test.columns[param], axis=1)\n    else:\n        print(\"Using all parameters\", end = \" \")\n    lr_model = LogisticRegression(random_state = 42, max_iter = 10000)\n    lr_model.fit(train , ys_train)\n    ys_pred = lr_model.predict(test)\n    score = accuracy_score(ys_test, ys_pred)\n    all_results_smote.append([param, score])\n    print(\"score\", score)\n    if score > best_score_smote:\n        best_lr_smote = lr_model\n        best_score_smote = score\n        best_pred_smote = ys_pred\n        worst_param = param\nprint(\"Best without using\", features.columns[worst_param])\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')\nall_results_smote = np.array(all_results_smote, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.insert(x_train.columns, len(x_train.columns), \"None\")\nplt.plot(all_results[:, 0], all_results[:, 1], label = \"Original data\")\nplt.plot(all_results_smote[:, 0], all_results_smote[:, 1], label = \"after SMOTE\")\nplt.title(\"Scores on SVM by C value\")\nplt.ylabel(\"score\")\nplt.xticks(range(len(labels)), labels = labels, rotation=90)\nplt.xlabel(\"parameter removed\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest, different criterions and #estimatorsÂ¶\n## Using the original dataset","metadata":{}},{"cell_type":"code","source":"criterions = ['gini', 'entropy']\nbest_score = -1\nbest_pred = []\nbest_forest = None\nall_results = []\nfor criterion in criterions:\n    print(\"Using\", criterion)\n    for estimators in range(10, 201, 10):\n        print(\"\\t{} estimators\".format(estimators), end = \" \")\n        forest = RandomForestClassifier(n_estimators=estimators, criterion = criterion, random_state = 42)\n        forest.fit(x_train, y_train)\n        y_pred = forest.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([forest, y_pred, score, estimators])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_forest = forest\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using data after oversampling","metadata":{}},{"cell_type":"code","source":"criterions = ['gini', 'entropy']\nbest_score_smote = -1\nbest_pred_smote = []\nbest_forest_smote = None\nall_results_smote = []\nfor criterion in criterions:\n    print(\"Using\", criterion)\n    for estimators in range(10, 201, 10):\n        print(\"\\t{} estimators\".format(estimators), end = \" \")\n        forest = RandomForestClassifier(n_estimators=estimators, criterion = criterion, random_state = 42)\n        forest.fit(xs_train, ys_train)\n        ys_pred = forest.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([forest, ys_pred, score, estimators])\n\n        print(\"score = {}\".format(score))\n        if score > best_score_smote:\n            best_pred_smote = ys_pred\n            best_score_smote = score\n            best_forest_smote = forest\nall_results_smote = np.array(all_results_smote, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_results[:20][:, 3], all_results[:20][:, 2], label = \"Gini\")\nplt.plot(all_results[20:][:, 3], all_results[20:][:, 2], label = 'Entropy')\nplt.plot(all_results_smote[:20][:, 3], all_results_smote[:20][:, 2], label = \"Gini - smote\")\nplt.plot(all_results_smote[20:][:, 3], all_results_smote[20:][:, 2], label = 'Entropy - smote')\nplt.title(\"Scores on random forest by number of estimators(using smote)\")\nplt.ylabel(\"score\")\nplt.xlabel(\"number of estimators\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tOriginal data\")\nprint(\"Best score using criterion={} with {} estimators\".format(best_forest.criterion, len(best_forest\n                                                                                         .estimators_)))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tafter smote\")\n\nprint(\"Best score using criterion={} with {} estimators\".format(best_forest_smote.criterion, len(best_forest_smote\n                                                                                         .estimators_)))\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(ys_test, best_pred_smote))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN - different K's\n## Original dataset","metadata":{}},{"cell_type":"code","source":"best_score = -1\nbest_pred = []\nbest_knn = None\nall_results = []\nfor p in [1, 2]:\n    print(\"Using l{}\".format(p))\n    for k in range(2, 31):\n        print(\"\\tUsing k={}\".format(k), end = ' ')\n        knn = KNeighborsClassifier(n_neighbors=k, p = p)\n        knn.fit(x_train, y_train)\n        y_pred = knn.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        all_results.append([k, score])\n        print(\"score = {}\".format(score))\n        if score > best_score:\n            best_pred = y_pred\n            best_score = score\n            best_knn = knn\nall_results = np.array(all_results, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_smote = -1\nbest_pred_smote = []\nbest_knn_smote = None\nall_results_smote = []\nfor p in [1, 2]:\n    print(\"Using l{}\".format(p))\n    for k in range(2, 31):\n        print(\"\\tUsing k={}\".format(k), end = ' ')\n        knn = KNeighborsClassifier(n_neighbors=k, p = p)\n        knn.fit(xs_train, ys_train)\n        ys_pred = knn.predict(xs_test)\n        score = accuracy_score(ys_test, ys_pred)\n        all_results_smote.append([k, score])\n        print(\"score = {}\".format(score))\n        if score > best_score_smote:\n            best_pred_smote = ys_pred\n            best_score_smote = score\n            best_knn_smote = knn\nall_results_smote = np.array(all_results_smote, dtype=object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_results[:29][:, 0], all_results[:29][:, 1], label = \"l1\")\nplt.plot(all_results[29:][:, 0], all_results[29:][:, 1], label = 'l2')\nplt.plot(all_results_smote[:29][:, 0], all_results_smote[:29][:, 1], label = 'l1 - smote')\nplt.plot(all_results_smote[29:][:, 0], all_results_smote[29:][:, 1], label = 'l2 - smote')\nplt.title(\"Scores on KNN by k value\")\nplt.ylabel(\"score\")\nplt.xticks(range(2, 31), rotation=90)\nplt.xlabel(\"k\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tOriginal data\")\nprint(\"Best score using l{} with {} neighbors\".format(best_knn.p, best_knn.n_neighbors))\ncm = confusion_matrix(y_test,best_pred)\nprint(classification_report(y_test,best_pred))\nprint(\"Accuracy score\", accuracy_score(y_test, best_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\t\\tAfter smote\")\nprint(\"Best score using l{} with {} neighbors\".format(best_knn_smote.p, best_knn_smote.n_neighbors))\ncm = confusion_matrix(ys_test,best_pred_smote)\nprint(classification_report(ys_test,best_pred_smote))\nprint(\"Accuracy score\", accuracy_score(ys_test, best_pred_smote))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = [best_forest_smote, best_knn, best_lr, best_svm]\npred = []\nfor model in ensemble:\n    print(model)\n    if model is best_lr and worst_param is not features.shape[1]:\n        pred.append(model.predict_proba(x_test.drop(x_test.columns[worst_param], axis=1)))\n    else:\n        pred.append(model.predict_proba(x_test))\nprobs = sum(pred)/len(ensemble)\nfinal_pred = [0 if p[0] > p[1] else 1 for p in probs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test,final_pred)\nprint(classification_report(y_test,final_pred))\nprint(accuracy_score(y_test, final_pred))\nsns.heatmap(cm/np.sum(cm), annot=True, fmt = '.2%', cmap = 'Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}