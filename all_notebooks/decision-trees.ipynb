{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is a decision tree?\n\nDecision tree learns a set of rules from data which agree with most of the data. For example, today when you woke up, you had to make breakfast, *if you had eggs, you could make omelette, otherwise, you will make simple toast*. This decision which you took is exactly the one like decision trees takes after looking at your data.\n\nBased on the number of your variables, the decision tree finds the most important variable (using [Gini Index](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) or [Information Gain](https://medium.com/geekculture/do-you-know-what-information-gain-is-5ac15d9cf7f9?source=friends_link&sk=61698a055a19e0d95ddcbcb13a2ef226)) and based on this creates a condition. In the next step, in the new data, it does the same until there are no variables left and we finally get a decision.\n\nIn short, decision tree is deriving a set of rules from your data that give you the right output.\n\nLet's see a minimal example now.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first read the data ([Source](https://www.kaggle.com/abhishekvermasg1/decision-tree)). We are [Pandas](https://pandas.pydata.org/) library for reading  the CSV file.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/decision-tree/minimal2var.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the data.","metadata":{}},{"cell_type":"code","source":"df.head(5)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have two independent variables (x1 and x2) in the data and we need to predict y. Since, the output is binary (0 or 1), this becomes a classification problem.\n\nFor solving this classification task, we will be using decision tree from [scikit-learn](https://scikit-learn.org/).","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\nclf = tree.DecisionTreeClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The object 'clf' corresponds to the DecisionTreeClassifier. This object will take in data and train decision tree for us.","metadata":{}},{"cell_type":"code","source":"clf = clf.fit(df[['x1', 'x2']], df['y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we train the decision tree using *fit()* function. The first argument is x (input features) and the second argument is y (output). \n\nIn case, you are confused how the indexing is done in Pandas, read [this](https://towardsdatascience.com/essential-pandas-every-data-scientist-should-know-in-2021-c642719a78bb?sk=a12a92ba455434140092871a7cbb1943).","metadata":{}},{"cell_type":"code","source":"clf.predict([[1, 0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above, we predict data using our classifier. We see that the output is 1.\n\nLet's also see the probability of the above example being either in class 0 or class 1.","metadata":{}},{"cell_type":"code","source":"clf.predict_proba([[1, 0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that probability for class 0 is 0.48 while class 1 is 0.51. The predict function returns the class with highest probability, hence, class 1 was predicted.\n\nLet's plot the tree to understand what are the decisions it is taking inside.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport graphviz\nplt.figure(figsize=(10,10))\n\n# tree.plot_tree(clf, filled=True) \n\ndot_data = tree.export_graphviz(clf, out_file=None, \n                    node_ids=True,\n                    class_names=['0', '1'], feature_names=['x1', 'x2'],\n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see the structure of decision tree.\n\nOur input was (1, 0). So, x1 = 1 and x2 = 0. Looking at the node 0, we see the first rule x1 <= 0.5. It is not the case (x1 = 1) in our input, so, we go to the right i.e. node 4. We see the second rule, x2 <= 0.5. In our case, it is true (x2 = 0). So, we finally settle at node 5. We finally see class 1 written there, which was what *predict()* function returned.\n\n# Why are decision trees important?\n* They give you rules for data which helps you understand the algorithm's inner process. Thus, decision trees are highly sought after in areas like banking and finance where you cannot trust the algorithm blindly.\n\n# What kind of data I should decision tree?\n* Tabular data!\n\n# What next?\n* Look at libraries like [XGBoost](https://xgboost.readthedocs.io/en/latest/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/) and [CatBoost](https://catboost.ai/) which are used to create decision trees that run the world. The decision trees inside these libraries are derived in more complex way then above. If you are interested, the concepts you need to see is [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting). Also, before feeding data to these libraries, you need to pre-process your data i.e. convert it into numbers, normalize it and handle missing values.","metadata":{}}]}