{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Mall Customers Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Importing Modules**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The objective of the proyect is to use different clustering algorithms to come up with conclusions about the dataset, as well as comparing the different used models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, DBSCAN\nimport scipy.cluster.hierarchy as sch\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objects as go\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading and getting to know the dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The dataset has:**\n- Numerical columns: CustomerID, Age, Annual Income and Spending Score.\n- Categorical column: Gender    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are 200 observations in total.\n- The customers range from 18 to 70 years old.\n- The annual income ranges from 15000 USD to 137000 USD.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Checking for missing values*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(dataset).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**EDA**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First I'll drop the ID column, as it doesn't give any useful information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset2 = dataset.copy()\ndataset2 = dataset2.drop(['CustomerID'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_matrix(dataset2,\n    dimensions=[\"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"],\n    color=\"Gender\", symbol=\"Gender\",\n    title=\"Scatter matrix\",\n    labels={col:col.replace('_', ' ') for col in dataset2.columns}) # remove underscore\n\nfig.update_traces(diagonal_visible=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although many relations could be analyzed, I'll focus on the Annual Income vs. Spending Score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Correlations**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [\"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"]\nheat = go.Heatmap(z =dataset2.corr(),\n                  x = x,\n                  y=x,\n                  xgap=1, ygap=1,\n                  colorbar_thickness=20,\n                  colorbar_ticklen=3,\n                  hovertext = dataset2.corr(),\n                  hoverinfo='text'\n                   )\n\ntitle = 'Correlation Matrix'               \n\nlayout = go.Layout(title_text=title, title_x=0.5, \n                   width=600, height=600,\n                   xaxis_showgrid=False,\n                   yaxis_showgrid=False,\n                   yaxis_autorange='reversed')\n   \nfig=go.Figure(data=[heat], layout=layout)        \nfig.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to only be a slight correlation between 'Age' and 'Spending score' for the set. People tend to spend less as they get older.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's see how numerical columns are distributed**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_data = [dataset2['Age'], dataset2['Annual Income (k$)'], dataset2['Spending Score (1-100)']]\ngroup_labels = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=[5, 10, 8])\nfig.update_layout(title_text='Age, Income and Score distribution')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(dataset2, x=\"Annual Income (k$)\", y = \"Spending Score (1-100)\",size='Age', color=\"Gender\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no clear correlation between Annual income and Spending score, let's see later what clustering analysis can tell us","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What about age feature?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Genre = pd.DataFrame(dataset2['Gender'].value_counts()).reset_index()\nGenre.columns = ['Gender','Total']\nfig = px.pie(Genre, values = 'Total', names = 'Gender', title='Gender', hole=.4, color = 'Gender',width=800, height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(Genre, x = 'Gender', y='Total', color='Gender',width=600, height=500)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Male = dataset2[dataset2[\"Gender\"] == 'Male'][['Gender','Age']]\ntemp = pd.DataFrame(Male['Age'].value_counts().reset_index())\ntemp.columns = ['Age','Total']\n\nFemale = dataset2[dataset2[\"Gender\"] == 'Female'][['Gender','Age']]\ntemp2 = pd.DataFrame(Female['Age'].value_counts().reset_index())\ntemp2.columns = ['Age','Total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x = temp['Age'],\n    y = temp['Total'],\n    name='Male',\n    marker_color='rgba(94, 144, 175, 0.8)'\n))\nfig.add_trace(go.Bar(\n    x = temp2['Age'],\n    y = temp2['Total'],\n    name='Female',\n    marker_color='rgba(249, 70, 10, 0.9)'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(title = 'Age per genre', barmode = 'group', xaxis_tickangle=-45)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusions:**\n- There are more women than men evaluated, and both average age is around 33 years.\n- There are more older men than women in the dataset.\n- There is no correlation between age with income or spending score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset2.iloc[:,2:4].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clustering analysis**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'll do some comparison with different clustering algorithms:\n- K-Means\n- Hierarchical clustering\n- Affinity propagation\n- DBSCAN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## K-Means","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"K-means starts with allocating cluster centers randomly and then looks for \"better\" solutions. One thing about this algorithm is that I have to give the number of clusters beforehand, so I'll be using the WCSS (elbow method) to come up with a more accurate idea.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**WCSS**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = \"k-means++\", max_iter = 500, n_init = 10, random_state = 123)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    \nfig = go.Figure(data = go.Scatter(x = [1,2,3,4,5,6,7,8,9,10], y = wcss))\n\n\nfig.update_layout(title='WCSS vs. Cluster number',\n                   xaxis_title='Clusters',\n                   yaxis_title='WCSS')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 5, init=\"k-means++\", max_iter = 500, n_init = 10, random_state = 123)\nidentified_clusters = kmeans.fit_predict(X)\n\n\ndata_with_clusters = dataset2.copy()\ndata_with_clusters['Cluster'] = identified_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_3d(data_with_clusters, x = 'Age', y='Annual Income (k$)', z='Spending Score (1-100)',\n              color='Cluster', opacity = 0.8, size='Age', size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the clusters could be labeled as:\n- Low income and low spending score (blue)\n- Low income and hig spending score (yellow)\n- Mid income and mid spending score (pink): Seems to be the most populated one\n- High annual income and low spending score (purple)\n- High annual income and high spending score (orange)\n\nAs the mall marketing department, we would like to move every observation upward so people spend more money. We should focus on the pink and purple clusters as they represent either a whole lot of people or high annual income to be spent. We may offer some discounts studying what pople in the pink cluster mostly buy, and offer some premium items for people in the purple one.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In the hierarchichal agglomerative clustering, each point starts being an individual cluster, and they group taking into account the distance between each one (first the closer ones). I can set the distance I want to evaluate.\nTo get a better idea of the number of clusters, I'll make use of a dendrogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = ff.create_dendrogram(X,\n                           linkagefun = lambda x: sch.linkage(x, \"ward\"),)\n\n# Ward minimizes the variance of the points inside a cluster.\n\nfig.update_layout(title = 'Hierarchical Clustering', xaxis_title='Customers',\n                   yaxis_title='Euclidean Distance', width=700, height=700)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hc = AgglomerativeClustering(n_clusters = 5, affinity = \"euclidean\", linkage = \"ward\")\nidentified_clusters = hc.fit_predict(X)\n\ndata_with_clusters = dataset2.copy()\ndata_with_clusters['Cluster'] = identified_clusters\n\nfig = px.scatter_3d(data_with_clusters, x = 'Age', y='Annual Income (k$)', z='Spending Score (1-100)',\n              color='Cluster', opacity = 0.8, size='Age', size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Makes the same clusters as K-Means, having just slight","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** \n- By using the Elbow method, the most accurate cluster number may be 3 or 5\n- Taking a look at the Dendrogram, we see that cutting horizontally the largest vertical line (the second blue from the left), 5 clusters seem to be the best option.\n- In both cases as we made the clusters, the same conclusions could be made. These two algorithms seem to have worked really well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Affinity propagation**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This algorithm doesn't require a preset cluster number. It takes as input measures of similarity between pair of data points. As they have similarities, they can belong to the same cluster. I'll use default settings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ap = AffinityPropagation(random_state = 0)\nidentified_clusters = ap.fit_predict(X)\n\ndata_with_clusters = dataset2.copy()\ndata_with_clusters['Cluster'] = identified_clusters\n\nfig = px.scatter_3d(data_with_clusters, x = 'Age', y='Annual Income (k$)', z='Spending Score (1-100)',\n              color='Cluster', opacity = 0.8, size='Age', size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of clusters is 10. It doesn't seem like a good result, but could be an useful algorithm with others datasets, or maybe studying how Age relates to the other variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**DBSCAN**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is a density based clustering algorithm. For each observation, the algorithm will form a shape around it and count how many fata points are within this shape (cluster). After there are no more nearby points, it will procede to make another cluster.\nI'll define the minimum number of data points to determine a cluster and the max distance for points to be part of the same cluster. I don't have to set the number of clusters beforehand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DBS = DBSCAN(eps = 9, min_samples = 5)\n\nidentified_clusters = DBS.fit_predict(X)\n\ndata_with_clusters = dataset2.copy()\ndata_with_clusters['Cluster'] = identified_clusters\n\nfig = px.scatter_3d(data_with_clusters, x = 'Age', y='Annual Income (k$)', z='Spending Score (1-100)',\n              color='Cluster', opacity = 0.8, size='Age', size_max=30)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the result isn't as accurate as K-Means, DBSCAN is a great algorithm for tuning an try to come up with different conclusions. In this case we achieve the same amount of clusters but they are not as informative or representative (might get better with some more tuning).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thanks for reaching the end!! Upvote if you liked it!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}