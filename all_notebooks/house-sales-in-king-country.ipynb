{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\n**Problem Statement**:\nIn this dataset we have to predict the sales price of  houses in King County, Seattle. \n\nBefore doing anything we should first know about the dataset what it contains what are its features and what is the structure of data.It includes homes sold between May 2014 and May 2015.\n\nThe dataset cantains **20** house features plus the price, along with **21613** observations.\n\nThe description for the 20 features is given below: <br>\n\n**1. id** :- It is the unique numeric  number assigned to each house being sold. <br>\n**2. date** :- It is the date on which the house was sold out. <br>\n**3. price**:- It is the price of house which we have to predict so this is our target variable and aprat from it are our features. <br>\n**4. bedrooms** :- It determines number of bedrooms in a house. <br>\n**5. bathrooms** :- It determines number of bathrooms in a bedroom of a house. <br>\n**6.  sqft_living** :- It is the measurement variable which determines the measurement of house in square foot. <br>\n**7. sqft_lot** : It is also the measurement variable which determines  square foot of the lot. <br>\n**8. floors**: It determines total floors means levels of house. <br>\n**9. waterfront** : This feature determines whether a house has a view to waterfront 0 means no 1 means yes. <br>\n**10. view** : This feature determines whether a house has been viewed or not  0 means no 1 means yes. <br>\n**11. condition** : It determines the overall condition of a house on a scale of 1 to 5. <br>\n**12. grade** : It determines the overall grade given to the housing unit, based on King County grading system on a scale of 1 to 11. <br>\n**13. sqft_above** : It determines square footage of house apart from basement. <br>\n**14. sqft_basement** : It determines square footage of the basement of the house. <br>\n**15. yr_built** : It detrmines the date of building of the house. <br>\n**16. yr_renovated** : It detrmines year of renovation of house. <br>\n**17. zipcode**  : It determines the zipcode of the location of the house. <br>\n**18. lat** : It determines the latitude of the location of the house. <br>\n**19.  long** : It determines the longitude of the location of the house. <br>\n**20. sqft_living15** : Living room area in 2015(implies-- some renovations)  <br>\n**21. sqft_lot15** : lotSize area in 2015(implies-- some renovations) <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport xgboost\nimport math\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom time import time\nfrom sklearn.metrics import r2_score\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import norm \nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/kc-house-data/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copying data to another dataframe df_train for our convinience so that original dataframe remain intact.\n\ndf_train=data.copy()\ndf_train.rename(columns ={'price': 'SalePrice'}, inplace =True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets see the first five rows of the data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA CLEANING AND PREPROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"In this step we check whether data contain null or missing values. What is the size of the data. What is the datatype of each column. What are unique values of categorical variables etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape) # shape \n# Check the data types of each column\nprint(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check any number of columns with NaN or missing values \nprint(data.isnull().any().sum(), ' / ', len(data.columns))\n# Check any number of data points with NaN\nprint(data.isnull().any(axis=1).sum(), ' / ', len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FINDING CORRELATION**"},{"metadata":{},"cell_type":"markdown","source":"In this step we check by finding correlation of all the features wrt target variable i.e., price to see whether they are positively correlated or negatively correlated to find if they help in prediction process in model building process or not. But this is also one of the most important step as it also involves domain knowledge of the field of the data means you cannot simply remove the feature from your prediction process just because it is negatively correlated because it may contribute in future prediction for this you should take help of some domain knowledge personnel."},{"metadata":{"trusted":true},"cell_type":"code","source":"# As id and date columns are not important to predict price so we are discarding it for finding correlation\nfeatures = data.iloc[:,3:].columns.tolist()\ntarget = data.iloc[:,2].name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Correlation of price woth other variables to see how many variables are strongly correlated with price\ncorrelations = {}\nfor f in features:\n    data_temp = data[[f,target]]\n    x1 = data_temp[f].values\n    x2 = data_temp[target].values\n    key = f + ' vs ' + target\n    correlations[key] = pearsonr(x1,x2)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing all the correlated features value with respect to price which is target variable\ndata_correlations = pd.DataFrame(correlations, index=['Value']).T\ndata_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As zipcode is negatively correlated with sales price , so we can discard it for sales price prediction."},{"metadata":{},"cell_type":"markdown","source":"**EDA or DATA VISUALIZATION**"},{"metadata":{},"cell_type":"markdown","source":"This is also a very important step in your prediction process as it help you to get aware you about existing patterns in the data how it is relating to your target variables etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nplot_p = sns.lmplot(x=\"sqft_living\", y=\"price\", aspect=1.8,data=data, hue=\"floors\", fit_reg=False)\nplot_p.set_titles(\"Floors by sqft_living and price\", fontsize=15)\nplot_p.set_xlabels(\"sqft Living\")\nplot_p.set_ylabels(\"Price(US)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pairplots to visualize strong correlation\nsns.set()\ncols = [ 'price','sqft_living', 'grade', 'sqft_above', 'view', 'bathrooms','bedrooms','sqft_basement']\nsns.pairplot(data[cols], height = 3.5)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saleprice = data[\"price\"].values\nplt.scatter(x = range(saleprice.shape[0]), y = np.sort(saleprice))\nplt.title('saleprice Vs observations')\nplt.xlabel('Observations')\nplt.ylabel('saleprice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"bedrooms\"].unique().shape[0])\ndata[\"bedrooms\"].value_counts(sort = False).plot.bar()\nplt.title(\"bedrooms\")\nplt.xlabel(\"Number Of Bedrooms\")\nplt.ylabel(\"Houses Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"grade\"].unique().shape[0])\ndata[\"grade\"].value_counts(sort = False).plot.bar()\nplt.title(\"Grades\")\nplt.xlabel(\"Number Of Grades\")\nplt.ylabel(\"Houses Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(data[\"condition\"].unique().shape[0])\ndata[\"condition\"].value_counts(sort = False).plot.bar()\nplt.title(\"Condition\")\nplt.xlabel(\"Condition\")\nplt.ylabel(\"Houses Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = df_train[['sqft_living','grade', 'sqft_above', \n                     'sqft_living15','bathrooms','view','sqft_basement','waterfront','yr_built','lat','bedrooms','long']]\nX = new_data.values\ny = df_train.SalePrice.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SPLITTING DATA INTO TRAINING AND TESTING SET**"},{"metadata":{},"cell_type":"markdown","source":"The training dataset and test dataset must be similar, usually have the same predictors or variables. They differ on the observations and specific values in the variables. If you fit the model on the training dataset, then you implicitly minimize error or find correct responses. The fitted model provides a good prediction on the training dataset. Then you test the model on the test dataset. If the model predicts good also on the test dataset, you have more confidence. You have more confidence since the test dataset is similar to the training dataset, but not the same nor seen by the model. It means the model transfers prediction or learning in real sense.So,by splitting dataset into training and testing subset, we can efficiently measure our trained model since it never sees testing data before.Thus it's possible to prevent overfitting.\n\nI am just splitting dataset into 20% of test data and remaining 80% will used for training the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=568)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_regr = RandomForestRegressor(n_estimators=400,random_state=0)\nrand_regr.fit(X_train, y_train)\nprint(rand_regr.score(X_test,y_test))\npredictions = rand_regr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import explained_variance_score\nexplained_variance_score(predictions,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision=DecisionTreeRegressor()\ndecision.fit(X_train, y_train)\nprint(decision.score(X_test,y_test))\nprediction2 = decision.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance_score(prediction2,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**AdaBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada=AdaBoostRegressor(n_estimators=50, learning_rate=0.2,loss='exponential').fit(X_train, y_train)\nprint(ada.score(X_test,y_test))\nprediction3=ada.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance_score(prediction3,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GradientBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Gdt=GradientBoostingRegressor(n_estimators=400, max_depth=5, loss='ls',min_samples_split=2,\n                              learning_rate=0.1).fit(X_train, y_train)\nprint(Gdt.score(X_test,y_test))\n\nprediction4 = Gdt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance_score(prediction4,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBRegressor(n_estimators=100, learning_rate=0.09, gamma=0).fit(X_train, y_train)\nprint(XGB.score(X_test,y_test))\nprediction5 = XGB.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explained_variance_score(prediction5,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**"},{"metadata":{},"cell_type":"markdown","source":"So, we have seen that accuracy of gradient boosting is around 88.7% and also achieved decent variance score of 0.87 which is very close to 1 . Therefore, it is inferred that Gradient Boosting is the suitable model for this dataset.we can further try using RandomizedSearchCV and GridSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}