{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom skimage import io , transform\nfrom torch.utils.data import Dataset , DataLoader\nfrom torchvision import transforms , utils\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir '../../kaggle/working/resnet18'\n!mkdir '../../kaggle/working/resnet18/models'\n!mkdir '../../kaggle/working/resnet18/models/check'\n!mkdir '../../kaggle/working/resnet18/models/best'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparameters\nn_epochs = 3\n#batch_train = 64\n#batch_test  = 1000\n\nbatch_size=16\n\nlearning_rate = 1e-4 \nmomentum = 0.9\nlog_interval = 10            #printing logs after an interval\n\nrandom_seed = 32\n#torch.backends.cudnn.enabled = False\ntorch.manual_seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neptune\n! pip install neptune-client==0.4.132","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import neptune\n\nneptune.init(project_qualified_name='manickavela/computer-vision', # change this to your `workspace_name/project_name`\n             api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiN2JhMDRjODUtZjk1Ni00M2JjLWI3ZTctYTg5NmFlOGNlOTNmIn0=', # change this to your api token\n            )\n\n\n#project = neptune.init('manickavela/computer-vision',\n#                       api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiN2JhMDRjODUtZjk1Ni00M2JjLWI3ZTctYTg5NmFlOGNlOTNmIn0=')\n#my_neptune = project.get_experiments(id='COM-4')[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../../kaggle/input/mura-v11/MURA-v1.1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_path  = pd.read_csv(\"../../kaggle/input/mura-v11/MURA-v1.1/train_image_paths.csv\",names=[\"path\"])\nvalid_image_path  = pd.read_csv(\"../../kaggle/input/mura-v11/MURA-v1.1/valid_image_paths.csv\",names=[\"path\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_list = []\n\nfor i in range(len(train_image_path)) :\n    my_list.append(re.search('/train/(.*)/patient',train_image_path['path'].iloc[i]).group(1))\n\nmy_list = list(set(my_list))\nmy_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = my_list\n\nenum = enumerate(classes)\nClassToLabel = dict((i,j) for i,j in enum)\nClassToLabel\n#LabelToClass = dict((b,a) for a,b in ClassToLabel)\n#LabelToClass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = my_list\n\nenum = enumerate(classes)\nLabelToClass = dict((b,a) for a,b in enum)\nLabelToClass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ScannedDataset(Dataset) :\n    def __init__(self,csv_file,root_dir,train=True,transform=None) :\n        self.paths = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.train = train\n    def __len__(self) :\n        return len(self.paths)\n\n    def __getitem__(self,idx) :\n        if torch.is_tensor(idx) :\n            idx = idx.tolist()\n        #print('Path feeded : ',self.paths.iloc[idx,0])\n        img_name = os.path.join(self.root_dir,self.paths.iloc[idx,0])\n\n        image = io.imread(img_name)\n        if self.train == True :\n            label = re.search('/train/(.*)/patient',img_name).group(1)\n        elif self.train == False :\n            label = re.search('/valid/(.*)/patient',img_name).group(1)\n        #print(label)\n        label = LabelToClass[label]\n        #print(\"sample : \",label,' ',image.shape)\n        if len(image.shape) > 2 :\n            if image.shape[2] == 3 :\n                image = (image[:,:,0] + image[:,:,1] + image[:,:,2])/3\n                #print('Made into grayscale ')\n        sample = {'image':image , 'labels' : label}\n\n        if self.transform :\n            sample = self.transform(sample)\n\n        \n        return sample\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Rescale(object) :\n    def __init__(self,output_size) :\n        self.output_size = output_size\n\n    def __call__(self,sample) :\n        image , labels = sample['image'] , sample['labels']\n\n        h , w = image.shape[:2] \n        new_h , new_w = self.output_size\n        new_h , new_w = int(new_h) , int(new_w)\n\n        #if len(image.shape) > 2 :                                               #Ignoring 3rd channel\n        #    return {'image': None , 'labels' : None}\n\n        img = transform.resize(image , (new_h , new_w))\n        #print('Befroe sending : ',labels,' ',img)\n        return {'image' :img , 'labels' : labels}\n\nclass ToTensor(object) :\n    def __call__(self,sample) :\n        #print('I came here')\n        image , labels = sample['image'] , sample['labels']\n\n        #print('Length : ',len(image.shape))\n        if len(image.shape) < 3 :\n            image = image[:,:,np.newaxis]\n        # swap color axis because\n\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        #print('Imgae shape before reshaping : ',image.shape)\n        image = image.transpose((2,0,1))\n        return {'image':torch.from_numpy(image).float() , 'labels' : labels}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_dataset = ScannedDataset(csv_file='../../kaggle/input/mura-v11/MURA-v1.1/train_image_paths.csv',\n                            root_dir='../../kaggle/input/mura-v11/',\n                            train=True,\n                            transform=transforms.Compose([Rescale((512,512)) , ToTensor()])) \n\nmy_test = ScannedDataset(csv_file='../../kaggle/input/mura-v11//MURA-v1.1/valid_image_paths.csv',\n                            root_dir='../../kaggle/input/mura-v11/',\n                            train=False,\n                            transform=transforms.Compose([Rescale((512,512)) , ToTensor()])) \n'''\nfor i in range(len(my_dataset)) :\n    sample = my_dataset[i]\n    #print(i,'', sample['image'].shape,'', sample['labels'])\n    \n    if i == 3:\n        break\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(my_dataset,batch_size = batch_size,shuffle=True,num_workers=4)\ntest_loader  = torch.utils.data.DataLoader(my_test,batch_size = batch_size,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv3x3(in_planes,out_planes,stride=1,groups=1,dilation=1) :\n    #in_planes = 1\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=stride,\n                     padding=dilation,groups=groups,bias=False,dilation=dilation)\n    \ndef conv1x1(in_planes,out_planes,stride=1) :\n    return nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=stride,bias=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasicBlock(nn.Module) :\n    expansion = 1\n    def __init__(self,inplanes,planes,stride=1,downsample=None,groups=1,\n                 base_width=64,dilation=1,norm_layer=None) :\n        super(BasicBlock,self).__init__()\n        if norm_layer is None :\n            norm_layer = nn.BatchNorm2d\n        \n        self.conv1 = conv3x3(inplanes,planes,stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes,planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self,x) :\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None :\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Bottleneck(nn.Module) :\n    expansion = 4\n\n    def __init__(self,inplanes,planes,stride=1,downsample=None,groups=1,\n                 base_width=64,dilation=1,norm_layer=None) :\n                super(Bottleneck,self).__init__()\n\n                if norm_layer is None :\n                     norm_layer = nn.BotchNorm2d\n\n                width = int(planes * (base_width/64.))\n\n                self.conv1 = conv1x1(inplanes,width)\n                self.bn1 = norm_layer(width)\n                self.conv2 = conv3x3(width, width, stride, groups, dilation)\n                self.bn2 = norm_layer(width)\n                self.conv3 = conv1x1(width, planes * self.expansion)\n                self.bn3 = norm_layer(planes * self.expansion)\n                self.relu = nn.ReLU(inplace=True)\n                self.downsample = downsample\n                self.stride = stride\n    def forward(self,x) :\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module) :\n    def __init__(self,block,layers,num_classes=1000,zero_init_residual=False,\n                 groups=1,width_per_group=6,replace_stride_with_dilation=None,norm_layer=None) :\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        \n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        \n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)  \n                    nn.init.constant_(m.bn2.weight, 0)  \n\n    def _make_layer(self, block, planes, blocks,\n                    stride= 1, dilate = False) :\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x) :\n        #print('ResNet : ',x.shape)\n        #print('Mess from here\\n')\n        x = self.conv1(x)\n        #print('Mess over\\n')\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x) :\n        #print('Herere too\\n',x)\n        return self._forward_impl(x)\n    \ndef _resnet(arch: str,block,layers,pretrained: bool, progress: bool,**kwargs) :            \n    model = ResNet(block, layers, **kwargs)\n    #if pretrained:\n    #    state_dict = load_state_dict_from_url(model_urls[arch],\n    #                                          progress=progress)\n    #    model.load_state_dict(state_dict)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef resnet18(pretrained: bool = False, progress: bool = True, **kwargs) :\n\n    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating ResNet\n'''\nnet_args = {\n    \"block\" : ResidualBlock,\n    \"layers\" : [2,2,2,2]\n}\n'''\n#network = ResNet(**net_args)\nnetwork = resnet18().to(device)\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(network.parameters(),lr = learning_rate,momentum=momentum)\n\nneptune.create_experiment('MURA Resnet18',params={'Learning Rate':learning_rate,'Momentum':momentum,'Model architecture ':'Resnet18','Epoch':50,\n                                                 'Optimizer':'SGD','Criterion':'Cross Entropy Loss','Batch Size':batch_size})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\n\ndef save_ckp(state, is_best, checkpoint_path, best_model_path):\n    f_path = checkpoint_path\n    torch.save(state, f_path)\n    if is_best:\n        best_fpath = best_model_path\n        shutil.copyfile(f_path, best_fpath)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#../../kaggle/input/mura-v11/\ncheckpoint_path = \"../../kaggle/working/resnet18/models/check/check_point_\"\nbestmodel_path  = \"../../kaggle/working/resnet18/models/best/best_model_\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_count = 0\ntest_count = 0\n\ntrain_losses = []\nvalid_losses = []\naccuracy_train = []\naccuracy_valid = []\n\ncorrect_train = 0\ncorrect_test = 0\n\n#neptune.log_metric('Epoch',20)\nvalid_loss_min = 100\n\nfor epoch in range(1,50) :\n    train_loss = 0.0\n    valid_loss = 0.0\n\n    train_acc = 0.0\n    valid_acc = 0.0\n\n    correct_train = 0\n    correct_test = 0\n\n    #Training\n    network.train()\n    print('Training...')\n    size = 0\n    for i,data in enumerate(train_loader) :\n\n        print('Batches : ',i) \n        inputs, labels = data['image'] , data['labels']\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad() \n        output = network(inputs)\n\n        loss = criterion(output,labels)\n\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()*inputs.size(0)\n\n        pred = output.data.max(1,keepdim=True)[1]\n        correct_train += pred.eq(labels.data.view_as(pred)).sum() \n        size += len(pred)\n        print('corr : ',correct_train)\n        train_acc = correct_train/size          #len(pred) has \n\n        #print(len(pred) )\n        print('Train accuracy ',train_acc)\n        print('Train Loss ',loss.item())\n\n        accuracy_train.append(train_acc)\n        train_losses.append(loss.item())\n\n        neptune.log_metric('train_loss',loss.item())\n        neptune.log_metric('train_acc',train_acc)\n\n        train_count = i\n    #train_acc = 100. * correct_train/len(train_loader.dataset)\n    #print('Accuracy : ',100. * correct_train/len(train_loader.dataset))\n    #accuracy_train.append(100. * correct_train/len(train_loader.dataset))\n    \n    \n    #Validataion\n    print('Validating...')\n    size = 0\n    network.eval()\n    with torch.no_grad() : \n        for i,data in enumerate(test_loader) :\n            print('Batches : ',i) \n       \n            target = data['labels']\n            data = data['image']\n\n            data = data.to(device)\n            target = target.to(device)\n\n            output = network(data)\n\n            loss = criterion(output,target)\n\n            valid_loss += loss.item()*data.size(0)\n\n            pred = output.data.max(1,keepdim=True)[1]\n            correct_test += pred.eq(target.data.view_as(pred)).sum() \n            size += len(pred)\n            valid_acc = correct_test/size\n\n            accuracy_valid.append(valid_acc)\n            valid_losses.append(loss.item())\n        \n            neptune.log_metric('valid_loss',loss.item())\n            neptune.log_metric('valid_acc',valid_acc)\n\n            test_count = i\n        \n        #print('Valid acc  : ',valid_acc)\n        #print('Valid loss : ',loss.item())\n    #valid_acc = 100. * correct_test/len(test_loader.dataset)\n    #print('Accuracy : ',100. * correct_test/len(test_loader.dataset))\n    #accuracy_test.append(100. * correct_test/len(test_loader.dataset))\n    \n\n    checkpoint = {\n            'epoch': epoch + 1,\n            'valid_loss_min': valid_loss,\n            'state_dict': network.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n\n    #save_ckp(checkpoint, False, checkpoint_path +'.pt', bestmodel_path+str(epoch+1))\n\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(test_loader.sampler)\n    #train_losses.append(train_loss)\n    #valid_losses.append(valid_loss)\n\n    neptune.log_metric('train_loss_epoch',train_loss)\n    neptune.log_metric('valid_loss_epoch',valid_loss)\n\n    neptune.log_metric('train_acc_epoch',train_acc)\n    neptune.log_metric('valid_acc_epoch',valid_acc)\n\n    #Saving the best model till now \n    if valid_loss <= valid_loss_min:\n            #print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n            save_ckp(checkpoint, True, checkpoint_path+str(epoch+1)+'.pt', bestmodel_path +str(epoch+1)+'.pt')\n            valid_loss_min = valid_loss\n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mkdir '../../kaggle/working/resnet18'\nmkdir '../../kaggle/working/resnet18/models'\nmkdir '../../kaggle/working/resnet18/models/check'\nmkdir '../../kaggle/working/resnet18/models/best'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls  g","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}