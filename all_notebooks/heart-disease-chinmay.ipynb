{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Plotting Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import cufflinks as cf\n%matplotlib inline\n\n# Metrics for Classification technique\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# Scaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import  RandomizedSearchCV, train_test_split\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Data\n\ndata = pd.read_csv(\"../input/heart-disease-prediction-in-advance/heart.csv\")\ndata.head(6) # Mention no of rows to be displayed from the top in the argument","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Size of the dataset\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have a dataset with 303 rows which indicates a smaller set of data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Out of 14 features, we have 13 int type and only one with float data type.\n* Woah! We have no missing values in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check correleation between various features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nsns.set_context('notebook',font_scale = 1.3)\nsns.heatmap(data.corr(),annot=True,linewidth =2)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check the correlation of various features with the target feature.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(20,12))\nsns.set_context('notebook',font_scale = 2.3)\ndata.drop('target', axis=1).corrwith(data.target).plot(kind='bar', grid=True, figsize=(20, 10), \n                                                   title=\"Correlation with the target feature\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Four feature( \"cp\", \"restecg\", \"thalach\", \"slope\" ) are positively correlated with the target feature.\n* Other features are negatively correlated with the target feature."},{"metadata":{},"cell_type":"markdown","source":"**Individual Feature Analysis**"},{"metadata":{},"cell_type":"markdown","source":"## Age(\"age\") Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check 10 ages and their count\n\nplt.figure(figsize=(25,12))\nsns.set_context('notebook',font_scale = 1.5)\nsns.barplot(x=data.age.value_counts()[:10].index,y=data.age.value_counts()[:10].values)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check the range of age in the dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"minAge=min(data.age)\nmaxAge=max(data.age)\nmeanAge=data.age.mean()\nprint('Min Age :',minAge)\nprint('Max Age :',maxAge)\nprint('Mean Age :',meanAge)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We should divide the Age feature into three parts - \"Young\", \"Middle\" and \"Elder\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Young = data[(data.age>=29)&(data.age<40)]\nMiddle = data[(data.age>=40)&(data.age<55)]\nElder = data[(data.age>55)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,10))\nsns.set_context('notebook',font_scale = 1.5)\nsns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(Young),len(Middle),len(Elder)])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A large proportion of dataset contains Elder people.**"},{"metadata":{},"cell_type":"markdown","source":"**Elderly people are more likely to suffer from heart disease.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['blue','green','yellow']\nexplode = [0,0,0.1]\nplt.figure(figsize=(10,10))\nsns.set_context('notebook',font_scale = 1.2)\nplt.pie([len(Young),len(Middle),len(Elder)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sex(\"sex\") Feature Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['sex'])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ratio of Male to Female is approx 2:1**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot the relation between sex and slope.\n\nplt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['sex'],hue=data[\"slope\"])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot the relation between sex and target.\n\nplt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['sex'],hue=data[\"target\"])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Males are more likely to have heart disease than Female.**"},{"metadata":{},"cell_type":"markdown","source":"## Chest Pain Type(\"cp\") Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['cp'])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As seen, there are 4 types of chest pain**\n\n1. status at least\n2. condition slightly distressed\n3. condition medium problem\n4. condition too bad"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['cp'],hue=data[\"sex\"])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['cp'],hue=data[\"target\"])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* People having least chest pain are not likely to heart disease.\n* People having severe chest pain are  likely to heart disease."},{"metadata":{},"cell_type":"markdown","source":"**Elderly people are more likely to have chest pain.**"},{"metadata":{},"cell_type":"markdown","source":"# Thal Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['thal'])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 3 = normal\n2. 6 = fixed defect\n3. 7 = reversable defect"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['thal'],hue=data[\"target\"])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**People with fixed defect are more likely to have heart disease.**"},{"metadata":{},"cell_type":"markdown","source":"## Target "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nsns.set_context('notebook',font_scale = 1.5)\nsns.countplot(data['target'])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The ratio between 1 and 0 is much less than 1.5 which indicates that target feature is not imbalanced. So for a balanced dataset, we can use accuracy_score as evaluation metrics for our model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Enginnering"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_val = []\ncontinous_val = []\nfor column in data.columns:\n    print(\"--------------------\")\n    print(f\"{column} : {data[column].unique()}\")\n    if len(data[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_val.remove('target')\ndfs = pd.get_dummies(data, columns = categorical_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\ncol_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndfs[col_to_scale] = sc.fit_transform(dfs[col_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"**Splitting our dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfs.drop('target', axis=1)\ny = dfs.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will work on following algo - \n\n* KNN\n* Random Forest Classifier\n* XGBoost\n* CatBoost"},{"metadata":{},"cell_type":"markdown","source":"## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1 = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter Optimization\n\ntest_score = []\nneighbors = range(1, 25)\n\nfor k in neighbors:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    test_score.append(accuracy_score(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 8))\nplt.plot(neighbors, test_score, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**At K=19, we are getting highest test accuracy.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1 = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We achieved accuracy 89% with KNN Model after Hyperparameter Optimization.**"},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)\ny_pred2 = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyperparameter Optimization\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n\nparams2 ={\n    \n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)], \n    'max_features': ['auto', 'sqrt'],\n    'max_depth': max_depth, \n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4], \n    'bootstrap': [True, False]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(random_state=42)\n\nrfcs = RandomizedSearchCV(estimator=rfc, param_distributions=params2, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfcs.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfcs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = rfcs.predict(X_test)\nprint(accuracy_score(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We achieved accuracy 83% approx with Random Forest Classifier Model. There is no improvement after Hyperparameter Optimization.**"},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(random_state = 42)\nxgb.fit(X_train,y_train)\ny_pred3 = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We achieved accuracy 83% approx with XGBoost Classifier Model.**"},{"metadata":{},"cell_type":"markdown","source":"## CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = CatBoostClassifier(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.fit(X_train,y_train)\ny_pred4 = model4.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We achieved accuracy 81% approx with CatBoost Classifier Model.**"},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"**From the above models KNN is giving us the best accuracy which is 89%.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}