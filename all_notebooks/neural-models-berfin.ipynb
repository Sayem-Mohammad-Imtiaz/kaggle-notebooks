{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport time\nimport requests\nimport os\nimport numpy as np\nimport io\nimport imageio\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy\nimport h5py\nimport pickle\nimport pandas as pd\nfrom urllib.request import urlopen\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras import Input, layers\nfrom array import array\nfrom keras.layers.merge import add\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, GRU, Embedding, Multiply,  Concatenate, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n\nWORKING_DIRECTORY = os.path.dirname(os.getcwd())\nTRAIN_DATA_FILENAME = \"../input/project-data/eee443_project_dataset_train.h5\"\nTEST_DATA_FILENAME = \"../input/project-data/eee443_project_dataset_test.h5\"\nTRAIN_IMAGES_DIR = WORKING_DIRECTORY + \"/data/train_images/\"\nTEST_IMAGES_DIR = WORKING_DIRECTORY + \"/data/test_images/\"\nTID = \"../input/images-for-train/train_images/\"\nCAPS_DICT_DIR = \"../input/caps-dict/caps_dict\"\nPICKLE_FEATURE_DIR = \"../input/features/feature_data\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = h5py.File(TRAIN_DATA_FILENAME, \"r\")\n\nfor key in list(f.keys()):\n    print(key, \":\", f[key][()].shape)\n\ntrain_cap = f[\"train_cap\"][()]\ntrain_imid = f[\"train_imid\"][()]\ntrain_ims = f[\"train_ims\"][()]\ntrain_url = f[\"train_url\"][()]\nword_code = f[\"word_code\"][()]\n\n\ndf = pd.DataFrame(word_code)\ndf = df.sort_values(0, axis=1)\nwords = np.asarray(df.columns)\n\nwordtoix = {}\nfor i in range(len(words)):\n  word = words[i]\n  wordtoix[word] = i\n\nprint(\"Vocab Size =\", len(words))\n\n\ndef caption_array_to_str(caption_array):\n    \n    caption = \"\"\n    \n    for word in caption_array:\n\n        if (word == 'x_NULL_') or (word == 'x_START_') or (word == 'x_END_'):\n            continue\n            \n        caption += word + \" \"\n            \n    return caption\n\ndef tidy_caps(url, imid, cap, image_directory=None):\n\n    caps_dict = {}\n\n    if image_directory is not None:\n        \n        url_list = [u.split(\"/\")[-1].strip() for u in np.char.decode(url).tolist()]\n        \n        for f in tqdm(os.listdir(image_directory)):\n            \n            if f.endswith(\".jpg\") or f.endswith(\".jpeg\"):\n                \n                ind = url_list.index(f) + 1\n                caps_dict[f] = cap[np.where(imid == ind)]\n\n    else:\n        \n        for i in range(len(url)):\n            \n            name = url[i].decode().split(\"/\")[-1]\n            caps_dict[name] = cap[np.where(imid == (i+1))]\n            \n    return caps_dict\n\n\n\n\ndef create_pre_processed_set(image_directory, shuffle=False):\n    \n    file_data = tf.data.Dataset.list_files(str(image_directory) + \"*.jpg\", shuffle=shuffle)\n\n    def process_files(path):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.image.resize(img, (299, 299))\n        img = tf.keras.applications.inception_v3.preprocess_input(img)\n        return img\n\n    def process_name(path):\n        name = path.numpy().decode().split(\"/\")[-1]\n        return name\n\n    def trial(path):\n        img = tf.py_function(process_files, [path], tf.float32)\n        name = tf.py_function(process_name, [path], tf.string)\n        d = (img, name)\n        return d\n\n    pre_processed_set = file_data.map(lambda x: trial(x))\n    \n    return pre_processed_set\n\n\n\n\n\ndef create_features_pickled(pickle_file_dir, dataset_images):\n\n    model = tf.keras.applications.InceptionV3(weights='imagenet')\n    inception = tf.keras.Model(model.input, model.layers[-2].output)\n\n    with open(pickle_file_dir, \"wb\") as outfile:\n\n        for data in tqdm(dataset_images.batch(1)):\n\n            image = data[0]\n            key = data[1].numpy().astype(str)[0]\n\n            feature = inception(image)\n\n            feature = tf.reshape(feature, -1).numpy()\n\n            super_tuple = (feature, caps_dict[key], tf.constant(key))\n            pickle.dump(super_tuple, outfile)\n\n    outfile.close()\n    \n    \n    \ndef loadpickle(filename):\n\n    with open(filename, \"rb\") as f:\n\n        while True:\n\n            try:\n                yield pickle.load(f)\n\n            except EOFError:\n                break\n                \n\n                \ndef create_dataset(image_directory, feature_pickle_directory):\n    \n    if not os.path.isfile(feature_pickle_directory):\n        dataset_images = create_pre_processed_set(image_directory)\n        create_features_pickled(feature_pickle_directory, dataset_images)\n        \n    dataset = tf.data.Dataset.from_generator(loadpickle, args=[feature_pickle_directory], output_types=(np.float32,np.int32, tf.string))\n    \n    return dataset\n\n\ndef create_caption_dictionary(train_url, train_imid, train_cap, image_directory, caption_pickle_directory):\n    \n    if not os.path.isfile(caption_pickle_directory):\n        caps_dict, data_length = tidy_caps(train_url, train_imid, train_cap, image_directory)\n\n        with open(caption_pickle_directory, \"wb\") as outfile:\n            pickle.dump(caps_dict, outfile)\n        outfile.close()\n\n    else:\n        with open(caption_pickle_directory, \"rb\") as infile:\n            caps_dict = pickle.load(infile)\n        infile.close()\n    \n    return caps_dict, len(caps_dict)\n\n\n\ncaps_dict, data_length = create_caption_dictionary(train_url, train_imid, train_cap, TID, CAPS_DICT_DIR)\n\nprint( \"{} of {} retrieved. {:.1f}% of data is clean.\".format(data_length, len(train_url), 100 * data_length/len(train_url) ) )\n\n\n\nfeature_dataset = create_dataset(TID, PICKLE_FEATURE_DIR)\n\nfor d in feature_dataset.take(2):\n    features = d[0]\n    captions = d[1].numpy()\n    image_name= d[2].numpy().decode()\n    \n    im = cv2.imread(TID + image_name)\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    plt.imshow(im)\n    plt.show()\n    cap = words[captions]\n    \n    for c in cap:\n        c = caption_array_to_str(c)\n        print(c)\n        \n    print(features.shape, captions.shape, TID + image_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(dataset, max_length, num_photos_per_batch, vocab_size):\n\n  X1, X2, y = list(), list(), list()\n\n  n = 0\n  \n  while 1:\n        \n    for data in dataset:\n        \n        n += 1\n        feature = data[0].numpy()\n        caps = data[1].numpy()\n        \n        for i in range(caps.shape[0]):\n        \n            seq = caps[i]\n            \n#             print(n, i, seq, feature)\n            \n            for m in range(1, seq.shape[0]):\n                # split into input and output pair\n                in_seq, out_seq = seq[:m], seq[m]\n                # pad input sequence\n                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                # encode output sequence\n                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                # store\n                # print('photo size', photo.shape)\n                # print('X2 size', in_seq.shape)\n                # print('y size', out_seq.shape)\n                X1.append(feature)\n                X2.append(in_seq)\n                y.append(out_seq)\n\n    \n        if n == num_photos_per_batch:\n#             print(\"BATCH END\\n\", np.array(y))\n            yield [np.array(X1), np.array(X2)], np.array(y)\n#             print(\"BATCH START\")\n            X1, X2, y = list(), list(), list()\n            n = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embedding(wordtoix):\n    # Load Glove vectors\n    glove_dir = '../input/glove6b200d/glove.6B.200d.txt'\n    embeddings_index = {} # empty dictionary\n    f = open(glove_dir, encoding=\"utf-8\")\n\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        # if (word == 'startseq' or word == 'unk' ):\n        #   print(word)\n\n        embeddings_index[word] = coefs\n    f.close()\n    print('Found %s word vectors.' % len(embeddings_index))\n\n\n    embedding_dim = 200\n\n    # Get 200-dim dense vector for each of the 10000 words in out vocabulary\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    for word, i in wordtoix.items():\n\n        if (word == 'xUNK'):\n          word = 'unk'\n\n        embedding_vector = embeddings_index.get(word)\n        if  embedding_vector is None:\n          print(word)\n\n        if embedding_vector is not None:\n            # Words not found in the embedding index will be all zeros\n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 17\nvocab_size = 1004\nembedding_dim = 200\nembedding_matrix = create_embedding(wordtoix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_merge_model(embedding_matrix):\n    \n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    \n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    \n    #add, not concatenate! wrong \n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    \n    #set embedding layer's weight matrix \n    model.layers[2].set_weights([embedding_matrix])\n    model.layers[2].trainable = True\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_merge_model(embedding_matrix)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_init_inject_model(embedding_matrix):\n    \n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)  \n    \n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n\n    #image is set as state \n    se3,state = GRU(256,return_state = True)(se2,initial_state = fe2)  \n    \n    decoder2 = Dense(256, activation='relu')(se3)    \n    outputs = Dense(vocab_size, activation='softmax')(decoder2)   \n    \n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)    \n    \n    #set embedding layer's weight matrix \n    model.layers[2].set_weights([embedding_matrix])\n    model.layers[2].trainable = True\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_pre_inject_model(embedding_matrix):\n    \n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(embedding_dim, activation='relu')(fe1)    \n    fe2_reshaped = Reshape((1, embedding_dim), input_shape=(embedding_dim,))(fe2)    \n\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)        \n    se3,state3 = GRU(256,return_state = True)(fe2_reshaped)   \n    se4,state4 = GRU(256,return_state = True)(se2, initial_state = state3)      \n    decoder2 = Dense(256, activation='relu')(se4)    \n    outputs = Dense(vocab_size, activation='softmax')(decoder2)    \n\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)    \n\n    model.layers[4].set_weights([embedding_matrix])\n    model.layers[4].trainable = True\n   \n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#concatenate trial!\ninputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(200, activation='relu')(fe1)\nfe2 = Reshape((1, embedding_dim), input_shape=(embedding_dim,))(fe2)   \nx = Model(inputs=inputs1, outputs=fe2)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)        \ny = Model(inputs=inputs2, outputs=se2)\n\ncombined = Concatenate(axis = 1)([x.output, y.output])\ndecoder2 = Dense(256, activation='relu')(combined)    \noutputs = Dense(vocab_size, activation='softmax')(decoder2)    \n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)    \n\nmodel.layers[4].set_weights([embedding_matrix])\nmodel.layers[4].trainable = True"},{"metadata":{},"cell_type":"markdown","source":"\ninputA = Input(shape=(32,))\ninputB = Input(shape=(128,))\n\nx = Dense(8, activation=\"relu\")(inputA)\nx = Dense(4, activation=\"relu\")(x)\nx = Model(inputs=inputA, outputs=x)\n\ny = Dense(64, activation=\"relu\")(inputB)\ny = Dense(32, activation=\"relu\")(y)\ny = Dense(4, activation=\"relu\")(y)\ny = Model(inputs=inputB, outputs=y)\n\ncombined = concatenate([x.output, y.output])\n\nz = Dense(2, activation=\"relu\")(combined)\nz = Dense(1, activation=\"linear\")(z)\n\nmodel = Model(inputs=[x.input, y.input], outputs=z)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_par_inject_model(embedding_matrix):\n    \n    max_length = 17\n    vocab_size = 1004\n    embedding_dim = 200\n    \n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(200, activation='relu')(fe1)\n\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n\n    de = Multiply()([fe2,se2])\n\n    se3 = LSTM(256)(de)\n    decoder2 = Dense(256, activation='relu')(se3)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n\n    model.layers[3].set_weights([embedding_matrix])\n    model.layers[3].trainable = True\n    \n    return model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Creating embedding matrix...\")\nembedding_matrix = create_embedding(wordtoix)\nprint('Embedding matrix is ready!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 17\nvocab_size = 1004\nembedding_dim = 200\n\nprint(\"Creating par-inject model...\")\nmodel = create_par_inject_model(embedding_matrix) \n\n#print(\"Creating pre-inject model...\")\n#model = create_pre_inject_model(embedding_matrix)\n\n#print(\"Creating init-inject model...\")\n#model = create_init_inject_model()\n\n#print(\"Creating merge model...\")\n#model = create_merge_model()\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nprint(\"Model compiled...\")\n\n# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=1e-2,\n#     decay_steps=10000,\n#     decay_rate=0.9)\n# opt = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule, clipvalue=5)\n# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"mae\", \"acc\"])\n\nepochs = 1\nnum_photos_per_batch = 8\n\ndata_length = 1000\nfeature_dataset = feature_dataset.take(data_length)\n\nval_length = round(data_length * 0.15)\ntrain_length = data_length - val_length\n\nprint(train_length)\n\ntrain_step = train_length//num_photos_per_batch\nval_step = val_length//num_photos_per_batch\n\nval_dataset = feature_dataset.take(val_length) \ntrain_dataset = feature_dataset.skip(val_length)\ntrain_dataset = feature_dataset\n\nprint(\"Starting training...\")\n\nfor i in range(epochs):\n\n    train_generator = data_generator(train_dataset, max_length, num_photos_per_batch, vocab_size)\n    val_generator = data_generator(val_dataset, max_length, num_photos_per_batch, vocab_size)\n    \n    history=model.fit_generator(train_generator,\n                                steps_per_epoch = train_step,\n                                epochs = 1,\n                                validation_data = val_generator,\n                                validation_steps = val_step,\n                                shuffle=False, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_formatted(ls):\n    caption = \"\"\n    for word in ls:\n        if word is not (\"x_NULL_\" or \"x_START_\" or \"x_END_\"):\n            caption += word + \" \"\n    print(caption)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from nlgeval import compute_individual_metrics\nreferences = [\"i am running fast\", \"cow is running\"]\nhypothesis = \"i am running\" \nmetrics_dict = compute_individual_metrics(references, hypothesis)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_caption(model, feature_model, img_path):\n        \n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    img = np.expand_dims(img, axis=0)\n    img = feature_model(img)\n    img = img.numpy().reshape(1, -1)\n    print(img.shape)\n    seq = np.array([0]*17).reshape(1, -1)\n    seq[:, 0] = 1\n    \n    \n    for i in range(16):\n        pred = model.predict([img,seq])\n        seq[:, i+1] = np.argmax(pred)\n    \n    seq = seq.reshape(-1)\n    \n    im = cv2.imread(img_path)\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    plt.imshow(im)\n    plt.show()\n    return seq\n\nimage_name = \"../input/images-for-train/train_images/10004450986_d3c8220117_z.jpg\"\n\ninception_model = tf.keras.applications.InceptionV3(weights='imagenet')\ninception = tf.keras.Model(inception_model.input, inception_model.layers[-2].output)\n\nseq = predict_caption(model, inception, image_name)\n    \npred_seq = words[seq]\nactual = words[caps_dict[\"10004450986_d3c8220117_z.jpg\"]]\nfrom nltk.translate.bleu_score import sentence_bleu\nscore = sentence_bleu(actual, pred_seq)\nprint(score)\nprint_formatted(pred_seq)\nprint(actual)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = ['pos','lkl','lkd','aaa']\nreferences = [['aaa','aaa','aaa'],['ooo', 'ooo', 'ooo'],['ooo', 'ooo', 'ooo']]\n\nfrom nltk.translate.bleu_score import sentence_bleu\nscore = sentence_bleu(references, prediction)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"!pip install git+https://github.com/Maluuba/nlg-eval.git@master\n!nlg-eval --setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}