{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Identifying Political Post-Bots on Twitter\n\nDuring the 2016 USA election, certain candidates were accused of marketing their party's campaigns on social media using post bots sourced from Russia, namely Twitter and Facebook.\n\nIn this kernel, we inspect a dataset consisting of 3 million politically-related tweets around the time of the 2016 election, some of which were posted by the aforementioned post bots. We use Sci-Kit Learn and Tensorflow-Keras to build a classifier capable of identifying right-wing-leaning post bots based on account data and tweet contents.\n\n---\n\n1. Environment setup\n2. Data pre-processing\n3. Decision tree classification using Sci-Kit Learn\n4. NLP clssification using Keras\n\n---\n\n### 1 | Environment Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport os\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nimport re\nimport keras.preprocessing\nfrom collections import Counter\nimport keras\nimport tensorflow as tf\n\n# Read input data\ndf = pd.read_csv(\"../input/3-million-russian-troll-tweets-538/IRAhandle_tweets_1.csv\")\ndf2 = pd.read_csv(\"../input/3-million-russian-troll-tweets-538/IRAhandle_tweets_2.csv\")\ndf3 = pd.read_csv(\"../input/3-million-russian-troll-tweets-538/IRAhandle_tweets_3.csv\")\ndf4 = pd.read_csv(\"../input/3-million-russian-troll-tweets-538/IRAhandle_tweets_4.csv\")\n\n# Build a cumulative dataframe\ndf = df.append(df2)\ndf = df.append(df3)\ndf = df.append(df4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### 2 | Data Preprocessing\n\nFirst, we extract the columns we wish to leave in the framework. Next, we reformat qualitative data into quantitative forms.\n\nIn order to train the model, we want to use the following fields:\n* ```content``` - text content of the tweet\n* ```region``` - world region from which the tweet was posted\n* ```language``` - language in which the tweet was written\n* ```following``` - number people followed by the account of the tweet's poster\n* ```followers``` - number people following the account of the tweet's poster\n* ```updates``` - number of tweets created by the poster's account\n* ```retweet``` - retweet count for the tweet\n* ```account_type``` - type of account **(response variable of interest)**\n* ```post_type``` - type of post"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Select columns to leave\ncolumns_to_leave = ['content', 'region', 'language', 'following',\n                    'followers', 'updates', 'retweet',\n                    'account_type', 'post_type']\ncolumns_as_label = ['account_category']\n\n# Drop all languages besides English\ndf = df.loc[df['language'] == 'English']\ndf.drop(columns=['language'])\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------\n\n#### 2.1 | Quantify Qualitative Data\n\nThe columns will be quantified by simple categorization, with each unique textual/qualitative field mapping to a unique number.\n\nColumns to be quantified:\n* ```account_type```\n* ```region```\n* ```content```"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Convert account_type\naccount_type_map = {}\nval_id = 0\nfor item in df.account_type.unique():\n    account_type_map[item] = val_id\n    val_id += 1\ndf['account_type'] = df['account_type'].apply(lambda x: account_type_map[x])\n\n# Convert region\nregion_type_map = {}\nval_id = 0\nfor item in df.region.unique():\n    region_type_map[item] = val_id\n    val_id += 1\ndf['region'] = df['region'].apply(lambda x: region_type_map[x])\n\n# Convert language\nlanguage_type_map = {}\nval_id = 0\nfor item in df.language.unique():\n    language_type_map[item] = val_id\n    val_id += 1\ndf['language'] = df['language'].apply(lambda x: language_type_map[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### 2.2 | Train & Test Splitting\n\nWe split the dataset into training and testing sets."},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Splitting data into training and test sets\ncolumns_to_leave_in_overall_data = columns_to_leave + columns_as_label\ntrain, test = train_test_split(df[columns_to_leave_in_overall_data], test_size=0.2)\n\n# Filter out unnecessary columns\ntrain_X = train[columns_to_leave]\ntrain_y = train[columns_as_label]\ntest_X = test[columns_to_leave]\ntest_y = test[columns_as_label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### 3 | Training a Model with Sci-Kit Learn\n\nFor the classification using decision trees, we attempt to use the fields ```region```, ```following```, ```followers```, ```updates```, and ```retweet``` to predict ```account_type```.\n\n#### 3.1 | Trying a Model\n\nBelow, we train Sci-Kit Learn's basic ```DecisionTreeClassifier```."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_columns = ['region', 'following', 'followers', 'updates', 'retweet']\n\nclf = DecisionTreeClassifier()\nclf.fit(train_X[train_columns], train_y)\n\npredicted_array = clf.predict(test_X[train_columns])\ntest_y_list = list(test_y.account_category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, we validate the accuracy results from the ```DecisionTreeClassifier``` used above."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"correct_values = 0\nincorrect_map = {}\n\n# Manually score tweet accuracy (without Sci-Kit Learn automatic scoring methods)\n# This gives a breakdown of the misclassified data\nfor (predicted, correct) in zip(predicted_array, test_y_list):\n    if predicted == correct:\n        correct_values += 1\n    else:\n        if correct in incorrect_map:\n            incorrect_map[correct] += 1\n        else:\n            incorrect_map[correct] = 1\n\n# Output accuracy values\nprint(\"Overall accuracy: {}%\".format(np.round(100 * correct_values / len(predicted_array), decimals=2)))\nprint(\"---\")\nfor k in incorrect_map.keys():\n    print(\"Misclassified \\\"{}\\\": {}% | count = {}\".format(k, np.round(\n        100 * incorrect_map[k] / test_y.account_category.value_counts()[k], decimals=2\n    ), incorrect_map[k]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n#### 3.2 | Alternative Decision Tree Models\n\nClearly, the ```DecisionTreeClassifier``` does not perform as well as we had hoped. Below, we try to use other models: ```RandomForestClassifier``` and ```AdaBoostClassifier```."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"# Apply a one-hot-style encoding to account_category\ndf[['account_category']] = df.account_category.apply(lambda x: 1 if x == 'RightTroll' else 0)\n\n# Split the dataset once again for consistency\ncolumns_to_leave_in_overall_data = columns_to_leave + columns_as_label\ntrain, test = train_test_split(df[columns_to_leave_in_overall_data], test_size=0.2)\ntrain_X = train[columns_to_leave]\ntrain_y = train[columns_as_label]\ntest_X = test[columns_to_leave]\ntest_y = test[columns_as_label]\n\n# Function for prediction of data with various parameters, for easy testing\ndef pred(classifier, lab, test_X, test_y, print_output):\n    predicted_array = classifier.predict(test_X[train_columns])\n    test_y_list = list(test_y.account_category)\n    correct_values = 0\n\n    for (predicted, correct) in zip(predicted_array, test_y_list):\n        if predicted == correct:\n            correct_values += 1\n    \n    # Output the results\n    if print_output:\n        print(\"---\" + lab + \"---\")\n        print(\"Accuracy: {}%\".format(\n            np.round(100 * correct_values / len(predicted_array), decimals=2)))\n        \n# Test set validation of decision tree models\ndef test_df(path_num, clfc, lab, print_output):\n    df_test = pd.read_csv(\"../input/3-million-russian-troll-tweets-538/IRAhandle_tweets_\" + str(path_num) + \".csv\")\n    region_type_map = {}\n    val_id = 0\n    for item in df_test.region.unique():\n        region_type_map[item] = val_id\n        val_id += 1\n\n    # Update the test data file to match the style of the training data\n    df_test['region'] = df_test['region'].apply(lambda x: region_type_map[x])\n    df_test[['account_category']] = df_test.account_category.apply(lambda x: 1 if x == 'RightTroll' else 0)\n    train_columns = ['region', 'following', 'followers', 'updates', 'retweet']\n    \n    # Predict values\n    pred(clfc, lab, df_test[train_columns], df_test[['account_category']], print_output)\n    \n# Columns for model training\ntrain_columns = ['region', 'following', 'followers', 'updates', 'retweet']\n\n# Fit the various models\nclf = DecisionTreeClassifier()\nclf.fit(train_X[train_columns], train_y)\nclf2 = RandomForestClassifier()\nclf2.fit(train_X[train_columns], train_y)\nclf3 = AdaBoostClassifier()\nclf3.fit(train_X[train_columns], train_y)\n\n# Output tests results on a never-before-seen test file\nfor (cl, lab) in [(clf, \"DecisionTreeClassifier-Test\"),\n                  (clf2, \"RandomForestClassifier-Test\"),\n                  (clf3, \"AdaBoostClassifier-Test\")]:\n    test_df(5, cl, lab, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### 4 | Training a Model with Keras\n\nWe attempt to build a Keras model, utilizing basic NLP techniques to extract insights from the tweet text contents.\n\n#### 4.1 | Data & Text Cleaning\n\nFirst, text data was pre-processed and cleaned, so that later it could be fed into a Keras neural network."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"russian_bot_tweets = df.copy()\n\n# One-hot-style encoding of all russian trolls once mor \nrussian_bot_tweets[\"russian_bot\"] = russian_bot_tweets[\"account_category\"]\n\n# Clean remove null texts\nrussian_bot_tweets = russian_bot_tweets[pd.notnull(russian_bot_tweets[\"content\"])]\n\n# Clean the text of \"RT\"\nrussian_bot_tweets[\"text\"] = russian_bot_tweets[\"content\"].apply(lambda x: x.replace(\"RT\", \"\"))\n\n# Combine the dataframes\ntweets = russian_bot_tweets[[\"text\", \"russian_bot\"]]\n\n# Clean text\ntweets[\"text\"] = list(map(lambda x: nltk.word_tokenize(re.sub(\"[^a-zA-Z\\s]\", \"\", re.sub(r\"http.?://[^\\s]+[\\s]?\", \"\", (re.sub(r\"@\\w+\", \"\", x)))).lstrip().rstrip().lower()), tweets[\"text\"]))\n\n# Select only the 50000 most used words, or else the input layer will be way too big\nvocabulary = list(dict(Counter(list([i for l in tweets[\"text\"] for i in l])).most_common(50000)).keys())\n\n# Converting the word list to a dictionary (makes the next step faster)\nvocabulary_dict = dict(zip(vocabulary, range(len(vocabulary))))\n\n# Add a padding keyword in to the dictionary for future use\nvocabulary_dict[\"<PAD>\"] = len(vocabulary) + 1\n\n# Convert tweet text to numeric, dictionary-specific form\ntweets[\"text\"] = [[vocabulary_dict[s] for s in i  if s in vocabulary_dict] for i in tweets[\"text\"]]\n\n# Split data to train and test\ntweets_train, tweets_test, russian_bot_train, russian_bot_test = train_test_split(tweets[\"text\"], tweets[\"russian_bot\"], test_size=0.25, random_state=100)\n\n# Pad all tweets so they are all the same size\nmax_size = len(max(tweets[\"text\"], key=lambda x: len(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Building the Keras model\n\nBelow, we bulid a simple neural network Keras model with 16 nodes."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Convert the training and test sets to usable datasets\ntweets_train = keras.preprocessing.sequence.pad_sequences(tweets_train,\n                                                        value=vocabulary_dict[\"<PAD>\"],\n                                                        padding=\"post\",\n                                                        maxlen=max_size)\n\ntweets_test = keras.preprocessing.sequence.pad_sequences(tweets_test,\n                                                       value=vocabulary_dict[\"<PAD>\"],\n                                                       padding=\"post\",\n                                                       maxlen=max_size)\n\n# Build a Keras model\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(len(vocabulary_dict) + 1, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\nmodel.summary()\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\n# Split the tweets\ntweets_val = tweets_train[:20000]\npartial_tweets_train = tweets_train[20000:]\nrussian_bot_val = russian_bot_train[:20000]\npartial_russian_bot_train = russian_bot_train[20000:]\n\n# Train the model\nhistory = model.fit(partial_tweets_train, partial_russian_bot_train, epochs=40, batch_size=1000, validation_data=(tweets_val, russian_bot_val), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 | Keras Model Validation\n\nBelow, we obtain the accuracy statistics for our Keras model."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Validate the model\nresults = model.evaluate(tweets_test, russian_bot_test)\n\nprint(\"Model loss: {}\\nModel accuracy: {}%\".format(np.round(results[0], decimals=4), np.round(100 * results[1], decimals=2)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}