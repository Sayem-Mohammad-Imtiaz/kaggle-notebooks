{"cells":[{"metadata":{"_uuid":"eb716f15d31773f7d5e09fd52077fe4d57573c8d"},"cell_type":"markdown","source":"# Outliers: Assignment 6\n    Dweepa Honnavalli 01FB16ECS138\n    Ishita Bhandari 01FB16ECS143\n    Kavya Varma 01FB16ECS162"},{"metadata":{"_uuid":"62b56c1a5a89be156573231d4599a2647f4fd1e3"},"cell_type":"markdown","source":"### Question\nUse the given dataset which describesâ€‹records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil. Split the dataset into Training and Testing data. Use any 2 classification or clustering techniques to predict 'Absenteeism time in hours'. Justify any parameter (such as k (for k-NN or k-means), etc.) if pre-determined. Compare the techniques used and state which of the two is better for this data. Clearly interpret the results and support your interpretations with computations of suitable performance measures."},{"metadata":{"_kg_hide-input":false,"_uuid":"398a8b39433ab5817f50e387759a6f7e6cfadbe1","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nnRowsRead = 1000 # specify 'None' if want to read whole file\ndf1 = pd.read_csv('../input/Absenteeism_at_work.csv', delimiter=',', nrows = None)\ndf1.dataframeName = 'Absenteeism_at_work.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2fac697e858077373fa5d837a44ab01430f559","trusted":true},"cell_type":"markdown","source":"# Techniques Used\n\n> Two classification techniques have been used to approach this question\n1.  Decision Trees\n1.  Logistic Regression\n\n## Classification vs Clustering\n\nClassification is a supervised learning technique in which we know the classes before processing, and we attempt to train the model based on the training data set and use that to predict future observations.\nClustering is an unsupervised technique in which the number of classes is not known prior to processing. It is mainly a way to explore the data and infer some information from it.\n\nIn our dataset of 'Absenteeism At Work', there are various features along with the real value of no of hours absent. In such a case where the target is known and we want to train a model to predict based on the given set of observations and values, a classification technique would be the preferred approach.\n\nIn the case of clustering, we may yeild clusters which may be algorithmically similar but not in terms of the ''Absenteeism time in hours'."},{"metadata":{"_uuid":"71c0f2b0b43dfeb3e39ceeb9ae7207e38cb2ca43"},"cell_type":"markdown","source":"### Separation of Training and Testing Data\n    The ratio of train to test chosen is 4:1"},{"metadata":{"_uuid":"5036d513a04fefe5e2ad30d711b8fcbfe67370c0","trusted":true},"cell_type":"code","source":"X= df1.iloc[:,0:14]\ny=df1['Absenteeism time in hours']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(\"Unique labels in 'Abenteeism Time in Hours'\\n\", y.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a3eb06cddca335d0663445b39fd2e9d157cb0d6"},"cell_type":"markdown","source":"### Creating new y labels\n>     Each class is assigned a value based on one hot encoding"},{"metadata":{"_uuid":"5218602a6daab78a8e6fdd0324911c2fa9996d38","trusted":true},"cell_type":"code","source":"# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(y)\nencoded_Y = encoder.transform(y)\n# convert integers to dummy variables (i.e. one hot encoded)\nnewy = np_utils.to_categorical(encoded_Y)\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, newy, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53372c7a264f0d4122b1295dddc2f951355e33ca"},"cell_type":"markdown","source":"### Comparison of different classification techniques\nTo get an idea of how different classification techniques perform, here is a comparitive analysis of the same.\n\nThe methods that have been tested are:\n1. Logistic Regression\n2. Linear Discriminant Analysis\n3. K Nearest Neighbours\n4. Decision Trees\n5. Gaussian NB\n6. Support Vector Machine\n7. Neural Networks"},{"metadata":{"_uuid":"84ccab3c741fab52998df9266de6f8be45e3aac4","trusted":true},"cell_type":"code","source":"seed = 7\n# prepare models\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\nmodels.append(('K Nearest Neighbours', KNeighborsClassifier()))\nmodels.append(('Decision Trees', DecisionTreeClassifier()))\nmodels.append(('Naive Bayes', GaussianNB()))\nmodels.append(('Support Vector Machine', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d5c1b81a09f7642c31ea1ec676402bdcfaba1a0","trusted":true},"cell_type":"code","source":"classifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", input_dim=X_train1.shape[1], units=6, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=10, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=len(y_train1[0]), kernel_initializer=\"uniform\"))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.fit(X_train1, y_train1, batch_size = 100, epochs = 30,verbose=0)\nscore= classifier.evaluate(X_test1,y_test1)\ny_pred=classifier.predict(X_test1)\ndef accuracy2(y_pred,y_test):\n    l=len(y_pred)\n    count=0\n    for i in range(l):\n        m=max(y_pred[i])\n       # print(m)\n        index= np.where(y_pred[i]==m)\n        #print(index)\n        one = np.where(y_test[i]==1)\n        #print(one)\n        if(one[0]==index[0]):\n            count+=1\n       # elif(one[0]==(index[0]-1) or one[0]==(index[0]+1)):\n        #    count+=0.5\n    return count/l\nprint(\"Neural Network:\",accuracy2(y_pred,y_test1))\n      \nfor name, model in models:\n    model.fit(X_train,y_train)\n    cv_results = model.score(X_test,y_test)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f \" % (name, cv_results)\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69dd6a115eac5a6835fe4ebd7a750ff40febfa2b"},"cell_type":"markdown","source":"## Techniques Finalised "},{"metadata":{"_uuid":"118ce7471b8e73059c72d3662768d232520bd4bb"},"cell_type":"markdown","source":"## Decision trees\n\n### Why Decision Trees?\nThe general motive of using Decision Tree is to create a training model which can use to predict class or value of target variables by learning decision rules inferred from prior data(training data). This perfectly fits our requirement where we need to predict the 'hours absent' from the given observations. Our observations also include the class to which each set belongs. By splitting into training and testing, our problem statement can be solved using decision trees. Decision trees are intuitive in the sense that they can explain non linearlity nicely. \n\nHigh-performing with regard to searching down a built tree, because the tree traversal algorithm is efficient even for massive data sets\n\n### Choices relating to decision trees:\nThe function chosen to measure the quality of split is the Gini Index. A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. "},{"metadata":{"_uuid":"3f99f96546898462646c82dbddd832356a5888c9","trusted":true},"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\nclf.predict(X_test)\nclf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"31b74f3a31bb602b494f32f0e564c4474bfd7b49","trusted":true},"cell_type":"code","source":"import graphviz \nyu=[str(i) for i in y.unique()]\ndot_data = tree.export_graphviz(clf,feature_names=list(X_train),class_names=yu, filled=True, rounded=True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d71ecbfca6d56f7464f627dc6d9ab3cd9b9ffb83"},"cell_type":"markdown","source":"## Logistic Regression\n\n### Why Logistic Regression?\nThe independent variables are not normally distributed and the variances across different features differ. Logistic Regression does not require any of the above. It is quite robust and handles cases that entail wide ranges of variance. It yeilds the best accuracy among the other classification techniques.\n\n### Choices relating to Logistic Regression:\nMaximum iteration *(max_iter)* is 1000000 to provide for enough iterations to lead to convergence."},{"metadata":{"_uuid":"a1b975bbea11e2c23254d38970daadba3a9ce195","trusted":true},"cell_type":"code","source":"clf1 = LogisticRegression(random_state=0, solver='lbfgs',max_iter=10000000,multi_class='multinomial').fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c86627a470bfd9d92db491b30b2b1976ac91667","trusted":true},"cell_type":"code","source":"y_pred=clf1.predict(X_test)\nclf1.score(X_test, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}