{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"data_path = \"../input/pima-indians-diabetes-database/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Library Imports"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader as DL\nfrom torch.nn.utils import weight_norm as WN\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, precision_recall_curve\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom time import time\nimport random as r","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def breaker():\n    print(\"\\n\" + 30*\"-\" + \"\\n\")\n    \ndef head(x, no_of_ele=5):\n    breaker()\n    print(x[:no_of_ele])\n    breaker()\n    \ndef getCol(x):\n    return [col for col in x.columns]\n\ndef getObj(x):\n    s = (x.dtypes == \"object\")\n    return list(s[s].index)\n\nsc_X = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Handling"},{"metadata":{},"cell_type":"markdown","source":"**Inputs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(data_path + \"diabetes.csv\")\n\nbreaker()\nprint(\"Dataset Shape :\", repr(data.shape))\nbreaker()\n\nfor name in getCol(data):\n    print(name)\nbreaker()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:, :-1].copy().values\ny = data.iloc[:, -1].copy().values\n\n#X = sc_X.fit_transform(X)\n\nX, y = X.astype(float), y.astype(float)\n\nnum_features = X.shape[1]\ndel data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dataset Template**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DS(Dataset):\n    def __init__(this, X=None, y=None, mode=\"train\"):\n        this.mode = mode\n        this.X = X\n        if mode == \"train\":\n            this.y = y\n            \n    def __len__(this):\n        return this.X.shape[0]\n    \n    def __getitem__(this, idx):\n        if this.mode == \"train\":\n            return torch.FloatTensor(this.X[idx]), torch.FloatTensor(this.y[idx])\n        else:\n            return torch.FloatTensor(this.X[idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ANN"},{"metadata":{},"cell_type":"markdown","source":"**Config**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CFG():\n    tr_batch_size = 128\n    ts_batch_size = 128\n    va_batch_size = 128\n    \n    epochs  = 50\n    n_folds = 5\n    \n    IL = num_features\n    HL_1 = [256]\n    HL_2 = [256, 128]\n    OL = 1\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \ncfg = CFG()\n\nsim_ts_data_setup = DS(X, None, \"test\")\nsim_ts_data = DL(sim_ts_data_setup, batch_size=cfg.ts_batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setup**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ANN(nn.Module):\n    def __init__(this, IL=None, HL=None, OL=None):\n        super(ANN, this).__init__()\n        \n        this.HL = HL\n        this.DP1 = nn.Dropout(p=0.2)\n        this.DP2 = nn.Dropout(p=0.5)\n        \n        if len(HL) == 1:\n            this.BN1 = nn.BatchNorm1d(IL)\n            this.FC1 = WN(nn.Linear(IL, HL[0]))\n            \n            this.BN2 = nn.BatchNorm1d(HL[0])\n            this.FC2 = WN(nn.Linear(HL[0], OL))\n        \n        elif len(HL) == 2:\n            this.BN1 = nn.BatchNorm1d(IL)\n            this.FC1 = WN(nn.Linear(IL, HL[0]))\n            \n            this.BN2 = nn.BatchNorm1d(HL[0])\n            this.FC2 = WN(nn.Linear(HL[0], HL[1]))\n            \n            this.BN3 = nn.BatchNorm1d(HL[1])\n            this.FC3 = WN(nn.Linear(HL[1], OL))\n            \n        else:\n            raise NotImplementedError(\"Only Supports Networks of Depth 1 and 2\")\n            \n    def getOptimizer(this, lr=1e-3, wd=0):\n        return optim.Adam(this.parameters(), lr=lr, weight_decay=wd)\n    \n    def forward(this, x):\n        if len(this.HL) == 1:\n            x = this.BN1(x)\n            #x = this.DP1(x)\n            x = F.relu(this.FC1(x))\n            x = this.BN2(x)\n            #x = this.DP2(x)\n            x = torch.sigmoid(this.FC2(x))\n            return x\n        else:\n            x = this.BN1(x)\n            #x = this.DP1(x)\n            x = F.relu(this.FC1(x))\n            x = this.BN2(x)\n            #x = this.DP2(x)\n            x = F.relu(this.FC2(x))\n            x = this.BN3(x)\n            #x = this.DP2(x)\n            x = torch.sigmoid(this.FC3(x))\n            return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(X=None, y=None, n_folds=None, HL_Used=None):\n    breaker()\n    print(\"Training ...\")\n    breaker()\n    \n    fold = 0\n    LP = []\n    names = []\n    bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n    \n    start_time = time()\n    for tr_idx, va_idx in KFold(n_splits=n_folds, shuffle=True, random_state=0).split(X, y):\n        print(\"Processing Fold {fold} ...\".format(fold=fold+1))\n        \n        X_train, X_valid, y_train, y_valid = X[tr_idx], X[va_idx], y[tr_idx], y[va_idx]\n        \n        tr_data_setup = DS(X_train, y_train.reshape(-1, 1))\n        va_data_setup = DS(X_valid, y_valid.reshape(-1, 1))\n        \n        DLS = {\"train\" : DL(tr_data_setup, batch_size=cfg.tr_batch_size, shuffle=True, generator=torch.manual_seed(0)),\n               \"valid\" : DL(va_data_setup, batch_size=cfg.va_batch_size, shuffle=False)}\n        \n        torch.manual_seed(0)\n        model = ANN(cfg.IL, HL_Used, cfg.OL)\n        model.to(cfg.device)\n        \n        optimizer = model.getOptimizer()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, eps=1e-6, verbose=True)\n        \n        for e in range(cfg.epochs):\n            epochLoss = {\"train\" : 0, \"valid\" : 0}\n            for phase in [\"train\", \"valid\"]:\n                if phase == \"train\":\n                    model.train()\n                else:\n                    model.eval()\n                lossPerPass = 0\n                \n                for feat, label in DLS[phase]:\n                    feat, label = feat.to(cfg.device), label.to(cfg.device)\n                    \n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(phase == \"train\"):\n                        output = model(feat)\n                        loss   = nn.BCELoss()(output, label)\n                        if phase == \"train\":\n                            loss.backward()\n                            optimizer.step()\n                    lossPerPass += (loss.item() / label.shape[0])\n                epochLoss[phase] = lossPerPass\n            LP.append(epochLoss)\n            scheduler.step(epochLoss[\"valid\"])\n            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n                bestLoss = epochLoss\n                name = \"./Model_Fold_{fold}.pt\".format(fold=fold)\n                names.append(name)\n                torch.save(model.state_dict(), name)\n        fold += 1\n        \n    breaker()\n    print(\"Time Taken to Train {f} folds for {e} epochs : {:.2f} minutes\".format((time() - start_time)/60, f=n_folds, e=cfg.epochs))\n    breaker()\n    print(\"Best Loss :\", repr(bestLoss))\n    breaker()\n    \n    return LP, names, model\n\ndef eval_fn(model=None, names=None, dataloader=None, num_obs_test=None):\n    y_pred = np.zeros((num_obs_test, 1))\n        \n    for name in names:\n        Pred = torch.zeros(cfg.ts_batch_size, 1).to(cfg.device)\n        \n        model.load_state_dict(torch.load(name))\n        model.eval()\n        \n        for X in dataloader:\n            X = X.to(cfg.device)\n            with torch.no_grad():\n                op = model(X)\n            Pred = torch.cat((Pred, op), dim=0)\n        Pred = Pred[cfg.ts_batch_size:].cpu().numpy()\n        y_pred = np.add(y_pred, Pred)\n    y_pred = np.divide(y_pred, len(names))\n    \n    y_pred[np.argwhere(y_pred > 0.5)]  = 1\n    y_pred[np.argwhere(y_pred <= 0.5)] = 0\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"LP_1, Names_1, Network_1 = train_fn(X=X, y=y, n_folds=cfg.n_folds, HL_Used=cfg.HL_1)\n\ny_pred = eval_fn(Network_1, set(Names_1), sim_ts_data, sim_ts_data_setup.__len__())\n\nprint(\"Configuration 2 Accuracy : {:.5f} %\".format(accuracy_score(y, y_pred) * 100))\nbreaker()\n\nLPV = []\nLPT = []\nfor i in range(len(LP_1)):\n  LPT.append(LP_1[i][\"train\"])\n  LPV.append(LP_1[i][\"valid\"])\n\nxAxis = [i+1 for i in range(cfg.epochs)]\nplt.figure(figsize=(15, 30))\nfor fold in range(cfg.n_folds):\n    plt.subplot(cfg.n_folds, 1, fold+1)\n    plt.plot(xAxis, LPT[fold*cfg.epochs:(fold+1)*cfg.epochs], \"b\", label=\"Training Loss\")\n    plt.plot(xAxis, LPV[fold*cfg.epochs:(fold+1)*cfg.epochs], \"r--\", label=\"Validation Loss\")\n    plt.legend()\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Fold {fold}\".format(fold=fold+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"LP_2, Names_2, Network_2 = train_fn(X=X, y=y, n_folds=cfg.n_folds, HL_Used=cfg.HL_2)\n\ny_pred = eval_fn(Network_2, set(Names_2), sim_ts_data, sim_ts_data_setup.__len__())\n\nprint(\"Configuration 2 Accuracy : {:.5f} %\".format(accuracy_score(y, y_pred) * 100))\nbreaker()\n\nLPV = []\nLPT = []\nfor i in range(len(LP_2)):\n  LPT.append(LP_2[i][\"train\"])\n  LPV.append(LP_2[i][\"valid\"])\n\nxAxis = [i+1 for i in range(cfg.epochs)]\nplt.figure(figsize=(15, 30))\nfor fold in range(cfg.n_folds):\n    plt.subplot(cfg.n_folds, 1, fold+1)\n    plt.plot(xAxis, LPT[fold*cfg.epochs:(fold+1)*cfg.epochs], \"b\", label=\"Training Loss\")\n    plt.plot(xAxis, LPV[fold*cfg.epochs:(fold+1)*cfg.epochs], \"r--\", label=\"Validation Loss\")\n    plt.legend()\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Fold {fold}\".format(fold=fold+1))\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}