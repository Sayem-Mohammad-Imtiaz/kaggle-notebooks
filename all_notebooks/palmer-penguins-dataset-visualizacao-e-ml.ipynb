{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Explorando Análise de Dados no Palmer Penguins Dataset - Alternative Iris Dataset","metadata":{"_uuid":"3ee777ef-fbc0-448f-8343-756688ee7e87","_cell_guid":"b211c04b-3fce-47f9-81c3-b462ac95f5e7","trusted":true}},{"cell_type":"markdown","source":"A proposta deste dabook é explorar ferramentas de visualização de dados e datasets dos Kaggle.","metadata":{"_uuid":"5b0fbccc-8916-406d-acb3-4c8a128bcb3e","_cell_guid":"8c24da69-fae8-4958-aa2b-66bf1fbc1833","trusted":true}},{"cell_type":"markdown","source":"Dataset referência: [palmer-penguins-datasetalternative-iris-dataset](https://www.kaggle.com/ashkhagan/palmer-penguins-datasetalternative-iris-dataset)","metadata":{}},{"cell_type":"markdown","source":"Para adaptar a esse estudo foram feitos alguns ajustes no dataset palmer penguins:\n\n1. Remoção colunas 'island' e 'sex'\n1. Criação coluna 'id'\n1. Mudança de posição coluna 'species'\n1. Remoção linhas com dados \"NA\"","metadata":{}},{"cell_type":"markdown","source":"Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n   1. [Univariate Variable Analysis](#4)\n      1. [Categorical Variables](#5)\n      1. [Numerical Variables](#6)\n1. [Correlation](#7)\n1. [Data Visualization](#8)\n1. [Pandas Profiling](#9)\n1. [Train-Test Split](#10)\n1. [Scores of Models](#11)\n1. [Best Features Selection](#12)\n1. [Dimensionality Reduction](#13)\n   1. [Principle Component Analysis (PCA)](#14)\n   1. [Linear Discriminant Analysis (LDA)](#15)\n1. [Conclusion](#16)","metadata":{"_uuid":"a53e1c7f-cc20-4081-9bf1-af2be0494688","_cell_guid":"dc3876b1-1714-4711-bc15-da7f7df18bc5","trusted":true}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b006cd78-8e06-4a05-8560-685b91da7d30","_cell_guid":"742a3082-342d-4046-a028-d2b4713c3247","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a> \n# Importando Bibliotecas Necessárias","metadata":{"_uuid":"247b3964-8c70-4de9-adf5-857aa40b9062","_cell_guid":"784fd3cb-653f-4aab-a94b-d004dc4270ec","trusted":true}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nimport numpy as np \nimport pandas as pd \nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"72250b06-0aa4-4fa3-872d-fcb6a7a2393d","_cell_guid":"52cbd5b6-0034-4c39-8621-624ba6a899cd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> \n# Lê e apresenta informações sobre o Dataset","metadata":{"_uuid":"0b0b4282-7995-4f7b-bb68-58e22491790d","_cell_guid":"3cd4063f-96b8-4316-8218-e323f7923fa4","trusted":true}},{"cell_type":"code","source":"dataset = pandas.read_csv('../input/newpenguins10/newpenguins.csv')\ndataset.sample(10)","metadata":{"_uuid":"0b1bbc3d-946c-46d4-9e4e-b10b245f6e10","_cell_guid":"261380fa-235c-4401-aedc-03a3a15f0381","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tirando fora a coluna com as informações de ID","metadata":{}},{"cell_type":"code","source":"dataset.drop(\"id\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> \n## Descrição das Variáveis","metadata":{"_uuid":"9758311a-f240-4e27-8eeb-d4abf492323a","_cell_guid":"8911d928-8f3c-4122-b396-3c68435b429e","trusted":true}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"df493259-357a-4f0e-b5dc-8c1685d74992","_cell_guid":"896bfc12-2443-4bed-81f7-95ebf8c0348a","trusted":true}},{"cell_type":"code","source":"dataset.info()","metadata":{"_uuid":"6d62886a-3840-408e-96e3-0bba5acbcd09","_cell_guid":"75614860-c8fc-42cc-bc69-5a598e510a31","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> \n## Análise das Variáveis","metadata":{"_uuid":"1b1438a0-78b5-47ec-886a-4431447603eb","_cell_guid":"8be64b37-37eb-4c56-acfc-05f141857a6f","trusted":true}},{"cell_type":"markdown","source":"*** Categorical Variables:** ['species']\n\n*** Numerical Variables:** ['bill_lenght_mm', 'bill_depth_mm', 'flipper_lenght_mm', 'body_mass_g']","metadata":{"_uuid":"819710c5-56e5-46fb-925b-946851d9cff2","_cell_guid":"632050d5-23a0-4f93-a19b-ddd0e2200799","trusted":true}},{"cell_type":"markdown","source":"<a id=\"5\"></a> \n### Variáveis Categóricas","metadata":{"_uuid":"f183879b-5a8c-4787-893c-23c62348818f","_cell_guid":"770444fd-b79f-491f-bf7e-68b40be4fb11","trusted":true}},{"cell_type":"code","source":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","metadata":{"_uuid":"2d88a113-92a7-4317-9897-cf440d7dafc9","_cell_guid":"86a1e510-13e7-4940-b4ad-61f65ec3b3c3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","metadata":{"_uuid":"ec4b7e92-8f84-414e-83ca-6f3043cb1536","_cell_guid":"18b662a6-3153-45e6-9121-78de698cde29","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('darkgrid')\nfor c in categorical_list:\n    bar_plot(c)","metadata":{"_uuid":"f89f51c2-7cf6-41a4-a9c5-d27518e37a67","_cell_guid":"1ed29c71-bfdb-4f09-a6a7-1b171dd441ca","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> \n### Variáveis Numéricas","metadata":{"_uuid":"8d51f890-45a3-44b0-acd6-e40661880191","_cell_guid":"9a8da691-4028-4a90-85aa-70b0340437f8","trusted":true}},{"cell_type":"code","source":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","metadata":{"_uuid":"0bc8d394-5688-4489-84a1-020d1ded0961","_cell_guid":"825d7f1d-50ce-4101-b8ac-8b4908548a15","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","metadata":{"_uuid":"370ff7b1-6942-4fbd-9fb8-ddb6b743250e","_cell_guid":"8ed99fb4-540e-409e-8b59-baf873796d6f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in numerical_float64_list:\n    plot_hist(n)","metadata":{"_uuid":"1d6d49ac-18d5-4950-b7c9-ece2595ef7c3","_cell_guid":"c1e396df-57d3-4cf3-874d-a59d09bf0fec","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,15))\n\nplt.subplot(2,2,1)\nsns.histplot(dataset['bill_length_mm'], color = 'red', kde = True).set_title('bill_length_mm Interval and Counts')\n\nplt.subplot(2,2,2)\nsns.histplot(dataset['bill_depth_mm'], color = 'green', kde = True).set_title('bill_depth_mm Interval and Counts')\n\nplt.subplot(2,2,3)\nsns.histplot(dataset['flipper_length_mm'], kde = True, color = 'blue').set_title('flipper_length_mm Interval and Counts')\n\nplt.subplot(2,2,4)\nsns.histplot(dataset['body_mass_g'], kde = True, color = 'black').set_title('body_mass_g Interval and Counts')","metadata":{"_uuid":"aba58dd9-f234-45e7-bbc7-e43eae26633e","_cell_guid":"9cdfa491-8d6d-4fc3-b56c-0eb9b478f2c8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a> \n# Correlação","metadata":{"_uuid":"04524a9b-103a-44f7-b35b-b638c84e8faf","_cell_guid":"572d8d43-8240-4307-ae04-29d23eaa6085","trusted":true}},{"cell_type":"code","source":"features = dataset.columns\nsns.set_style('darkgrid')\nsns.pairplot(dataset[features])","metadata":{"_uuid":"2a99f602-10fe-4a30-9ac6-82892015f338","_cell_guid":"db8ea422-7535-4ab6-940d-20c8217c8062","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(dataset, hue = 'species')","metadata":{"_uuid":"652694c3-d8ac-423d-a4c4-edb353e9d38d","_cell_guid":"216c0692-bd0e-44dd-8b90-d1afde56ec64","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.corr()","metadata":{"_uuid":"3402048a-403e-4844-b5a9-349686e746c9","_cell_guid":"b47298f7-08ed-4f67-bf0a-4253678ade38","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","metadata":{"_uuid":"249ac0ea-b025-48c1-b1d3-24d5e40c968b","_cell_guid":"66c6ce9d-40fd-4d52-a083-8a62cd2cb9b6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('darkgrid')\naxes = pandas.plotting.scatter_matrix(dataset, alpha = 0.3, figsize = (10,7), diagonal = 'kde' ,s=80)\ncorr = dataset.corr().values\n\nplt.xticks(fontsize =10,rotation =0)\nplt.yticks(fontsize =10)\nfor ax in axes.ravel():\n    ax.set_xlabel(ax.get_xlabel(),fontsize = 15, rotation = 60)\n    ax.set_ylabel(ax.get_ylabel(),fontsize = 15, rotation = 60)\n# put the correlation between each pair of variables on each graph\nfor i, j in zip(*np.triu_indices_from(axes, k=1)):\n    axes[i, j].annotate(\"%.3f\" %corr[i, j], (0.8, 0.8), xycoords=\"axes fraction\", ha=\"center\", va=\"center\")","metadata":{"_uuid":"ca39d66a-38e7-47d4-b722-500401b11645","_cell_guid":"b8846dbb-d63b-4979-9eff-f19554c8c741","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a> \n# Visualização","metadata":{"_uuid":"f217d3e8-3f32-46f4-92f6-949a349dbd83","_cell_guid":"2af0f41c-f553-4ca2-b03c-c658718f6a25","trusted":true}},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.barplot(x = 'species', y = 'bill_length_mm', data = dataset, palette=\"cubehelix\")\nplt.subplot(2,2,2)\nsns.barplot(x = 'species', y = 'bill_depth_mm', data = dataset, palette=\"Oranges\")\nplt.subplot(2,2,3)\nsns.barplot(x = 'species', y = 'flipper_length_mm', data = dataset, palette=\"Oranges\")\nplt.subplot(2,2,4)\nsns.barplot(x = 'species', y = 'body_mass_g', data = dataset, palette=\"cubehelix\")","metadata":{"_uuid":"f7ac66dd-1b25-47b7-b37e-8c5c82416d0f","_cell_guid":"c6f6c50a-0ccb-40d7-8027-67992a50a8bb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.violinplot(x = 'species', y = 'bill_length_mm', data = dataset, palette=\"rocket_r\")\nplt.subplot(2,2,2)\nsns.violinplot(x = 'species', y = 'bill_depth_mm', data = dataset, palette=\"rocket_r\")\nplt.subplot(2,2,3)\nsns.violinplot(x = 'species', y = 'flipper_length_mm', data = dataset, palette=\"rocket_r\")\nplt.subplot(2,2,4)\nsns.violinplot(x = 'species', y = 'body_mass_g', data = dataset, palette=\"rocket_r\")","metadata":{"_uuid":"6df84847-6278-487c-9212-79ed16d26c51","_cell_guid":"46ab55b0-983f-4c08-ba05-43bce2a425a5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.boxplot(x = 'species', y = 'bill_length_mm', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(2,2,2)\nsns.boxplot(x = 'species', y = 'bill_depth_mm', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(2,2,3)\nsns.boxplot(x = 'species', y = 'flipper_length_mm', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(2,2,4)\nsns.boxplot(x = 'species', y = 'body_mass_g', data = dataset, palette=\"gist_ncar_r\")","metadata":{"_uuid":"4fad95e3-7af7-4cf8-a755-680c2ffa3e9b","_cell_guid":"d7eb815a-8c51-4ddd-a6ae-04316082e339","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nsns.distplot(dataset['bill_length_mm'], color=\"red\").set_title('bill_length_mm Interval')\nplt.subplot(2,2,2)\nsns.distplot(dataset['bill_depth_mm'], color=\"green\").set_title('bill_depth_mm Interval')\nplt.subplot(2,2,3)\nsns.distplot(dataset['flipper_length_mm'], color=\"blue\").set_title('flipper_length_mm Interval')\nplt.subplot(2,2,4)\nsns.distplot(dataset['body_mass_g'], color=\"black\").set_title('body_mass_g Interval')","metadata":{"_uuid":"d8251af6-b866-4d2f-ac82-2beff93eec77","_cell_guid":"9ffc50f0-3d3e-48de-a249-a27f117e526f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1, figsize=(5,5))\nplt.title(\"Distribution of Species\")\ndataset['species'].value_counts().plot.pie(autopct=\"%1.1f%%\")","metadata":{"_uuid":"ec6202a8-691d-4176-9256-77a27bc24289","_cell_guid":"14646138-2da2-4aa5-a009-8f1b57561726","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a> \n# PERFIL Pandas","metadata":{"_uuid":"b26ceee0-efac-434b-bd4d-afcbb7e9c2b5","_cell_guid":"fa93c0ef-8621-427e-96a2-273f94ea12b0","trusted":true}},{"cell_type":"markdown","source":"Pandas profiling é uma biblioteca bem útil que gera relatórios sobre os dados. Com ele pode-se recuperar os tipos de dados, sua  distribuição e várias informações estatísticas. A ferramenta tem muitas técnicas para preapração dos dados. Bibliotecas gráficas envolvendo mapas de características e correlação. Mais detalhes em: https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/","metadata":{"_uuid":"873255df-3809-4350-ba3e-3af990a0bab6","_cell_guid":"43b42977-33d7-47c1-ad0e-b2e013bfae74","trusted":true}},{"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(dataset)","metadata":{"_uuid":"846cdd8a-8283-4a68-ab1e-46e5094259ad","_cell_guid":"4cca7875-dc66-4957-a40a-acdd498dd5b9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a> \n# Divisão entre Treinamento e Teste","metadata":{"_uuid":"e735a4bf-b8cf-4113-8721-7238f4d3ff28","_cell_guid":"0442c792-b564-4de2-b643-ff806859071e","trusted":true}},{"cell_type":"code","source":"X = dataset.iloc[:,0:4].values \ny = dataset.iloc[:,4:].values","metadata":{"_uuid":"9e6d41ca-f7f1-4011-ba2c-71aed675b6ee","_cell_guid":"fb262453-285c-4731-b95c-e3b9fca38de7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","metadata":{"_uuid":"556555f0-4cd4-4f11-aa80-74644ff13fa0","_cell_guid":"dedc3141-ed0c-4f22-aebc-76b394468598","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardization é um método que transforma os dados de forma a se ter média zero e desvio padrão de 1 e a distribuição tende a ser normal. A fórmula envolve a subtração do valor médio seguida pela divisão pela variança.","metadata":{"_uuid":"a6b4b23e-6dff-4204-a0fb-3b41af805dfc","_cell_guid":"600c1214-b26a-4748-b065-895a341159a6","trusted":true}},{"cell_type":"code","source":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"_uuid":"3a38cd04-b7c9-4abe-a2b6-e7021437087b","_cell_guid":"94362cd5-7806-4759-b875-aea8192e28ae","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a> \n# Scores dos Modelos","metadata":{"_uuid":"4a7e826f-0fa6-4d93-ba44-1f5587df3350","_cell_guid":"0851a3a2-c49d-4c58-944a-993997990867","trusted":true}},{"cell_type":"markdown","source":"Aplicam-se algoritmos de ML ao dataset. Os resultados conterão scores de treinamento, teste e validação, matriz de confusão, informações estatísticas e relatórios de classificação para cada algoritmo.","metadata":{"_uuid":"9c291d8c-5a2a-4f05-a243-5a97db3e42ec","_cell_guid":"e59f8161-45db-4dd3-8d96-3c060cbe44d0","trusted":true}},{"cell_type":"code","source":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  score = model.score(X_valid, y_valid)\n  #print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) / (tp + fp + tn + fn)\n  precision = tp / (tp + fp)\n  recall    = tp / (tp + fn)\n  f1score  = 2 * precision * recall / (precision + recall)\n  specificity = tn / (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n  print(\"\")\n   \n  for m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","metadata":{"_uuid":"a5343dca-183c-4c55-bf45-6d6216c355b8","_cell_guid":"d72150a2-cdb9-4525-9290-f64a1d8dd66e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","metadata":{"_uuid":"974f093e-5e18-4cd9-9522-b759c749c5ba","_cell_guid":"9196e23c-cf49-4858-945b-fcd040fd2a76","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","metadata":{"_uuid":"d87e4209-82e4-4b90-a0c2-4511001c65a4","_cell_guid":"76fa878f-39f0-428b-b34b-73d145f10070","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"12\"></a>\n# Seleção das Melhores Features","metadata":{"_uuid":"55501435-52e4-4243-bbad-bfea16f834d9","_cell_guid":"fd3f68ba-66cb-4821-9178-fc45397d1bb0","trusted":true}},{"cell_type":"code","source":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","metadata":{"_uuid":"16ecfa1a-65a9-4216-bfeb-d701f34d1284","_cell_guid":"f9c8ff04-52d4-4282-b5db-99cd806c03aa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a>\n# Redução Dimensional","metadata":{"_uuid":"5a7a03ef-c3dc-47f2-ad3a-faba65792943","_cell_guid":"2d47584f-5ae1-4c82-a990-450ec44d6611","trusted":true}},{"cell_type":"markdown","source":"Em ciência de dados, redução dimensional é uma transformação nos dados que diminui o número de dimensões que os representam sem perda do seu significado. Esta redução conduz a menor necessidade de processamento, sendo frequentemente usada em processamento de sinais, recionhecimento de fala, neuroinformática, bioinformática, onde um grande conjunto de observações e variáveis são examinadas.","metadata":{"_uuid":"9a47d1a9-89f9-47f3-bcee-a8bd41e94b5d","_cell_guid":"49d05d89-78eb-4217-8256-8f07aafac486","trusted":true}},{"cell_type":"markdown","source":"<a id=\"14\"></a>\n## Análise de Componentes Principais (Principle Component Analysis - PCA)","metadata":{"_uuid":"cc2fbce9-000c-4686-b731-971b635c3dca","_cell_guid":"a1907a9f-4101-4a84-9928-9ebf4ab3693c","trusted":true}},{"cell_type":"markdown","source":"PCA é m atécnica estaística muito útil usada na área de reconhecimento e classificação e compreensão de imagens, por exemplo. O princiapal objetivo é manter o conjunto de dados com alta variança, reduzindo as suas dimensões. As dimensões suprimidas contem pouca informação sobre a população. O método combina variáveis altamente correlacionadaspara criar um menor conjunto de variáveis artificiais chamadas de componente principais que representamm as maiores variações dos dados. \n\nPCA é um método muito efetivo para revelar as informações importantes nos dados. O método busca mostrar dados multidimensionais com menores veriáveis as quais capturam as características básicas das amostras.","metadata":{"_uuid":"0c68ea00-188d-4fcf-b367-a5e6ddc577ff","_cell_guid":"2c614308-0cd8-4cdb-bbe7-2ef40b073e13","trusted":true}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nselectedX = X[:,]\n\npcaX = PCA(n_components=2)\npcaX = pcaX.fit(selectedX)\npcaX = pcaX.transform(X)\nprint(pcaX.shape)\n\nplt.scatter(pcaX[:,0], pcaX[:,1])\nplt.show()","metadata":{"_uuid":"c32e93da-fdf8-446f-b193-cf639faf5fd3","_cell_guid":"f42b7d36-c4ea-45d8-aec8-ac4e8e2b6fde","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a>\n## Análise Discriminatória Linear (Linear Discriminant Analysis LDA)","metadata":{"_uuid":"cd1e60a3-a994-458d-b3c2-1e359a545ec2","_cell_guid":"623c48a6-9aef-4a73-a9dc-642c733a3698","trusted":true}},{"cell_type":"markdown","source":"Análise Discriminatória Linear é usada como técnica de redução dimensional durante o estágio de pré-processamento em aplicações de ML. O objetivo é impedir overfitting enquanto reduz custo computacional. Mesmo sendo similar ao PCA, o LDA tem como objetivo maximizar a distância entre as classes, enquanto o PCA tentar maximizar a distância entre os pontos do dataset.\n\nEm resumo o LDA reduz o tamanho do data set através da maximização da diferença entre as classes.","metadata":{"_uuid":"52a6407f-cee9-48a0-99bb-4a8a005760c8","_cell_guid":"e9ede61d-1e1f-4242-bc74-df6b9416dbdd","trusted":true}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nldaX = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nprint(ldaX.shape)\n\nplt.scatter(ldaX[:,0], ldaX[:,1])\nplt.show()","metadata":{"_uuid":"4caeae27-29ee-4c2a-a775-ce68cc7f069c","_cell_guid":"1556fdf0-de73-4879-89ec-fec710174bf9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ldaX = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\npcaX = PCA(n_components=2).fit_transform(X, y)\n\n\nplt.figure(figsize=(25,8))\n\nplt.subplot(1,2,1)\nplt.title('PCA')\nplt.scatter(pcaX[:,0], pcaX[:, 1])\n\nplt.subplot(1,2,2)\nplt.title('LDA')\nplt.scatter(ldaX[:,0], ldaX[:, 1])\n\n\nplt.show()","metadata":{"_uuid":"938f0e57-c741-4802-82be-d13db0435d11","_cell_guid":"5cc4a80a-054e-41b3-91fc-7a5f72c200e9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"16\"></a> \n# Conclusion","metadata":{"_uuid":"eb05dfd5-624a-40ab-8b07-fac365aeec1c","_cell_guid":"2b3b1da4-9172-49fc-ae53-0562c3936276","trusted":true}},{"cell_type":"markdown","source":"Neste databook foi examinado o DataSet Palmern Penguins: análise exploratória dos dados, visualização, técnicas de ML e métricas de avaliação, bem como redução dimensional.","metadata":{"_uuid":"1dee3ccf-acd8-437a-a8cd-e29b7e35c59a","_cell_guid":"59f0d185-3858-4d78-8208-0b1a382fa00a","trusted":true}}]}