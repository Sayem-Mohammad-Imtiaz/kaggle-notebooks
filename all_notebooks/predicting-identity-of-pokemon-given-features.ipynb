{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cc728702-2254-197a-fd71-ddd91074fd75"},"source":"\n\nTodo:\nGet visualizations of semi-processed data\nUnderstand why some of the models take so long to run\nUnderstand how Google S2 cells work so that I can keep them as features\nHave a better way of showing other appearances close by."},{"cell_type":"markdown","metadata":{"_cell_guid":"5e24ea5c-f57f-4d9b-aeaf-f2c24067e211"},"source":"**Importing and loading the Data**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37fc7d7b-fd64-561a-d032-2021b1ac63a7"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.basemap import Basemap\n#from matplotlib import animation\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('../input/300k.csv', low_memory=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d24aec0b-90b3-9a48-a8af-b0767c91b579"},"source":"**Visualizing the Data**\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b3daef4-cc30-eb64-9d52-ecaf3179050b"},"outputs":[],"source":"train[['city','latitude', 'longitude', 'appearedLocalTime']].head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b99a0d1-ca5a-68b6-723f-d92a353d683a"},"outputs":[],"source":"#Used code from Kostya Bahshetsyan's data visualization. Todo: learn how basemap functions.\nplt.figure(1, figsize=(20,10))\nm1 = Basemap(projection='merc',\n             llcrnrlat=-60,\n             urcrnrlat=65,\n             llcrnrlon=-180,\n             urcrnrlon=180,\n             lat_ts=0,\n             resolution='c')\n\n\nm1.fillcontinents(color='#191919',lake_color='#000000') # dark grey land, black lakes\nm1.drawmapboundary(fill_color='#000000')                # black background\nm1.drawcountries(linewidth=0.1, color=\"w\")              # thin white line for country borders\n\n# Plot the data\nx, y = m1(train.longitude.tolist(),train.latitude.tolist())\nm1.scatter(x,y, s=3, c=\"#1292db\", lw=0, alpha=1, zorder=5)\nplt.title(\"Pokemon activity\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a13d82f-dd27-9095-37f6-377418bfb899"},"source":"**Checking for missing data**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"927eaa3e-40a2-41c6-d937-04fecf6adc36"},"outputs":[],"source":"NAs = pd.concat([train.isnull().sum()], axis=1)\nNAs[NAs.sum(axis=1) > 0]"},{"cell_type":"markdown","metadata":{"_cell_guid":"730a6c0c-3cb6-37fb-594c-a066725f9912"},"source":" **Dropping features that are encoded as numeric values**\n\nDropping booleans regarding if there is a gym or pokestop within the closest X meters in favor of using gym/pokestop distance (Km)\nDropping appearedDayofWeek since PokemonGO doesn't have bonuses based on what day of the week it is.\nDropping S2 cells, as I don't understand how they work and terminology yet. Will read up."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eab14a2a-b62c-c12f-a408-8ec8b62cf895"},"outputs":[],"source":"train = train.drop(['_id', 'cellId_90m', 'cellId_180m', 'cellId_370m', 'cellId_730m', 'cellId_1460m', 'cellId_2920m', 'cellId_5850m'],1)\ntrain = train.drop(['gymIn100m', 'gymIn250m', 'gymIn500m', 'gymIn1000m', 'gymIn2500m', 'gymIn5000m', 'pokestopIn100m', 'pokestopIn250m', 'pokestopIn500m', 'pokestopIn1000m', 'pokestopIn2500m', 'pokestopIn5000m'],1)\ntrain = train.drop(['appearedDayOfWeek'],1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ffae7fac-e47d-0a29-f7a4-f14ab59fd440"},"source":"**Relying on appearedLocalTime for pokemon appearance time**\n\nNoticed that appearedMinute/Hour/Day/Month/Year wasn't consistent with appearedLocalTime. Dropping them all in favor of pulling DateTime objects from appearedLocalTime."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc16f1f9-c1cd-f265-b094-2b1fe1981923"},"outputs":[],"source":"#Noticed that the appeared Hour/Minute/Day/Month/Year weren't consisted with appearedLocalTime. Removed them all in favor of appearedLocalTime\ntrain = train.drop(['appearedHour', 'appearedMinute', 'appearedDay', 'appearedMonth', 'appearedYear'],1)\n#Convert appearedLocalTime string to DateTime\ntrain['appearedLocalTime'] =  pd.to_datetime(train['appearedLocalTime'], format='%Y-%m-%dT%H:%M:%S')        #Note that %y is a 2digit, while %Y is 4digits for the year\n#Now reinstate the appeared Hour/Minute/Day/Month/Year, then drop appearedLocalTime\ntrain['appearedHour'] = train['appearedLocalTime'].dt.hour\ntrain['appearedMinute'] = train['appearedLocalTime'].dt.minute\ntrain['appearedDay'] = train['appearedLocalTime'].dt.day\ntrain['appearedMonth'] = train['appearedLocalTime'].dt.month\ntrain['appearedYear'] = train['appearedLocalTime'].dt.year\ntrain = train.drop(['appearedLocalTime'],1)\n#Now use 1-of-K encoding using pd.get_dummies()\nHour = pd.get_dummies(train.appearedHour, drop_first=True, prefix='hour')\nMinute = pd.get_dummies(train.appearedMinute, drop_first=True, prefix='minute')\nDay = pd.get_dummies(train.appearedDay, drop_first=True, prefix='day')\nMonth = pd.get_dummies(train.appearedMonth, drop_first=True, prefix='month')\nYear = pd.get_dummies(train.appearedYear, drop_first=True, prefix='year')\ntrain = train.join(Hour)         #To avoid dummy variable trap\ntrain = train.join(Minute)\ntrain = train.join(Day)\ntrain = train.join(Month)\ntrain = train.join(Year)\n#Now we drop the appearedTimeX feature\ntrain = train.drop(['appearedHour', 'appearedMinute', 'appearedDay', 'appearedMonth', 'appearedYear'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"465a7370-3e97-a94e-b033-bef6d16228d2"},"outputs":[],"source":"#Converting appearedTimeofDay into ordinal\ntime_mapping = {\"morning\": 0, \"afternoon\": 1, \"evening\": 2, \"night\": 3}\ntrain['appearedTimeOfDay'] = train['appearedTimeOfDay'].map(time_mapping)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9d93e95-e648-2d50-517c-1b784095d81c"},"outputs":[],"source":"#Same for terrainType\nTerr = pd.get_dummies(train.terrainType, drop_first=True, prefix='terr')\n#train = train.join(Terr)         #To avoid dummy variable trap\n#Now we drop the terrain feature\n#train = train.drop(['terrainType'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3afd17db-e5d8-f459-aefd-b6d3a2121518"},"outputs":[],"source":"#Get dummies on cities\nCity = pd.get_dummies(train.city, drop_first=True, prefix='city')\ntrain = train.join(City)         #To avoid dummy variable trap\n#Now we drop the city feature\ntrain = train.drop(['city'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c685bda6-3d06-9c23-9f74-e96c26b116c8"},"outputs":[],"source":"#redefining continents such that they correspond to the main 7 continents (no Antartica, yes Indian)\ntrain.continent[train['continent']=='America/Indiana']='America'\ntrain.continent[train['continent']=='America/Kentucky']='America'\ntrain.continent[train['continent']=='Pacific']='Australia'\ntrain.continent[train['continent']=='Atlantic']='Europe'\ntrain.continent[train['continent']=='America/Argentina']='CentralAmerica'\n#Then change them to dummies\nContinent = pd.get_dummies(train.continent, drop_first=True, prefix='continent')\ntrain = train.join(Continent)         #To avoid dummy variable trap\n#Now we drop the continent feature\ntrain = train.drop(['continent'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6eb843a8-9ac4-fab9-41ad-23b41596528d"},"outputs":[],"source":"#Comparing weather columns and choosing to drop weatherIcon. Then use dummies for weather\ntrain['weather'].value_counts()\ntrain['weatherIcon'].value_counts()             #These weather icons are based on time of day as well, making me inclined to not use them.\nWeather = pd.get_dummies(train.weather, drop_first=True, prefix='weather')\ntrain = train.join(Weather)         #To avoid dummy variable trap\n#Now we drop both weather features\ntrain = train.drop(['weatherIcon', 'weather'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60af2b6f-5975-69e4-ffa0-a9c32dfe728a"},"outputs":[],"source":"#Want to band windBearing into the 8 cardinal directions. (Probably used azimuth degrees where blowing north is 0 degrees and blowing west is 90 degrees)\n#We define North as 0, NW as 1, W as 2, etc...\ntrain.loc[(train['windBearing'] >= 337.5), 'windBearing'] = 0\ntrain.loc[(train['windBearing'] < 22.5), 'windBearing'] = 0\ntrain.loc[(train['windBearing'] >= 22.5) & (train['windBearing'] < 67.5), 'windBearing'] = 1\ntrain.loc[(train['windBearing'] >= 67.5) & (train['windBearing'] < 112.5), 'windBearing'] = 2\ntrain.loc[(train['windBearing'] >= 112.5) & (train['windBearing'] < 157.5), 'windBearing'] = 3\ntrain.loc[(train['windBearing'] >= 157.5) & (train['windBearing'] < 202.5), 'windBearing'] = 4\ntrain.loc[(train['windBearing'] >= 202.5) & (train['windBearing'] < 247.5), 'windBearing'] = 5\ntrain.loc[(train['windBearing'] >= 247.5) & (train['windBearing'] < 292.5), 'windBearing'] = 6\ntrain.loc[(train['windBearing'] >= 292.5) & (train['windBearing'] < 337.5), 'windBearing'] = 7\n#Now make them into dummies\nWindBearing = pd.get_dummies(train.windBearing, drop_first=True, prefix='windBearing')\ntrain = train.join(WindBearing)         #To avoid dummy variable trap\n#Now we drop the wind direction feature\ntrain = train.drop(['windBearing'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df788614-e7f0-e62d-1949-0da06fb94a31"},"outputs":[],"source":"#Some quick functions for converting minutes for sunrise/sunset minute standardization\ndef OnlyPositiveTime(x):\n    if x<0:\n        return x+1440                   #Where 1440 = minutes per day\n    else:\n        return x\n    \ndef OnlyNegativeTime(x):\n    if x>0:\n        return x-1440                   #Where 1440 = minutes per day\n    else:\n        return x"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3bc5e2d-d7f3-fbb1-2c5e-81b072ec2f5f"},"outputs":[],"source":"#Turned Sunrise/set Hour & Minute into dummies. Made sure that minutes since midnight for sunrise/set is positive (no negative minutes)\nSunriseHour = pd.get_dummies(train.sunriseHour, drop_first=True, prefix='sunriseHour')\nSunriseMinute = pd.get_dummies(train.sunriseMinute, drop_first=True, prefix='sunriseMinute')\nSunsetHour = pd.get_dummies(train.sunsetHour, drop_first=True, prefix='sunsetHour')\nSunsetMinute = pd.get_dummies(train.sunsetMinute, drop_first=True, prefix='sunsetMinute')\ntrain = train.join(SunriseHour)         #To avoid dummy variable trap\ntrain = train.join(SunriseMinute)\ntrain = train.join(SunsetHour)\ntrain = train.join(SunsetMinute)\n#Now we drop the sunrise/set time features\ntrain = train.drop(['sunriseHour', 'sunriseMinute', 'sunsetHour', 'sunsetMinute'],1)\ntrain['sunriseMinutesMidnight'].apply(OnlyPositiveTime)\ntrain['sunsetMinutesMidnight'].apply(OnlyPositiveTime)\n#Make sure that each sighting's minutes since sunrise (sunriseMinutesSince) is positive & that sunsetMinutesBefore is negative\ntrain['sunriseMinutesSince'].apply(OnlyPositiveTime)\ntrain['sunsetMinutesBefore'].apply(OnlyNegativeTime)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e59089b-509f-837d-5433-2f1ef4c973fa"},"outputs":[],"source":"#Change urban-suburban-urban into numeric values. 0=urban, 1=midurban, 2=suburban, 3=rural\n#Dropping suburban and midurban columns, since they dont seem to be accurate. A sighting can't be both urban, suburban, and midurban if they are partitioned bands of population density\n#Instead banding to get the urban, suburban, midurban, rural categorization, then changing to ordinal\ntrain = train.drop(['urban', 'suburban', 'midurban', 'rural'],1)\ntrain.loc[train['population_density'] < 200, 'population_density'] = 0\ntrain.loc[(train['population_density'] >= 200) & (train['population_density'] < 400), 'population_density'] = 1\ntrain.loc[(train['population_density'] >= 400) & (train['population_density'] < 800), 'population_density'] = 2\ntrain.loc[train['population_density'] > 800, 'population_density'] = 3\n#Just changing the name to show that I processed\ntrain.rename(columns={'population_density' : 'Urbanity'}, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ca9da9a-2985-e834-6fc2-8340c1efeaba"},"outputs":[],"source":"#Changing pokestopDistanceKm from a str to a float\nPokestopDistance = pd.to_numeric(train['pokestopDistanceKm'], errors='coerce')\ntemporary = pd.concat([train, PokestopDistance], axis=1)\n#This ends up dropping 39 instances. I'll find out what is causing the NaN's later (Note: errors='coerce' made them NaN's)\ntrain = temporary.dropna()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ed9d8fb-3d99-f8fa-0a98-d91493bd9fad"},"outputs":[],"source":"#Making sure that pokemonID (the first column)) and class (the last column) are the same\nrow_ids = train[train['class'] != train.pokemonId].index        #This yields an empty set --> identical columns\n#So now drop one of them and keep the other (for now) to use as the labels\ntrain.drop(['class'],1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f5564fb-e9cb-52a9-a056-a716eb5dce7f"},"source":"**Splitting data set (train) into training and validation sets**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c26c06dd-8512-1bea-5e19-2c5c5af6208f"},"outputs":[],"source":"train_features = train.drop(['pokemonId'],1)\ntrain_labels = train['pokemonId']\nX_train, X_test, Y_train, Y_test = train_test_split(train_features, train_labels, train_size = 0.7, random_state = 46)\nX_train.shape, Y_train.shape, X_test.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73af61c2-2fcc-bfbb-d67a-5bb04cad919a"},"outputs":[],"source":"model = BernoulliNB()\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\nacc_1 = round(accuracy_score(Y_test, Y_pred)*100, 2)\nacc_1\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67f94ceb-cef6-61f5-84e9-fd8455424432"},"outputs":[],"source":"model = KNeighborsClassifier(n_neighbors = 3)\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\nacc_3 = round(accuracy_score(Y_test, Y_pred)*100, 2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45f55cd3-fe8b-b4b2-9e56-4fc5fa967e77"},"outputs":[],"source":"model = GaussianNB()\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\nacc_5 = round(accuracy_score(Y_test, Y_pred)*100, 2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8359b16e-1fa7-64bc-3a65-ca5f6217acd3"},"outputs":[],"source":"model = Perceptron()\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\nacc_6 = round(accuracy_score(Y_test, Y_pred)*100, 2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31f07940-95fd-dd41-5b96-023b1d43431c"},"outputs":[],"source":"model = SGDClassifier()\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\nacc_8 = round(accuracy_score(Y_test, Y_pred)*100, 2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb6675a5-3a3d-458d-ce7a-a0ec0c5b295c"},"outputs":[],"source":"model = DecisionTreeClassifier()\nmodel.fit(X_train, Y_train)\nY_pred = model.predict(X_test)\nacc_9 = round(accuracy_score(Y_test, Y_pred)*100, 2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da9c939b-c08b-499f-7bdd-e439f83b0de2"},"outputs":[],"source":"models = pd.DataFrame({\n    'Model' : ['BernoulliNB', 'KNeighbors', 'Gaussian', 'Perceptron',  'Stochastic Gradient Decent', 'Decision Tree'],\n    'Accuracy Score' : [acc_1, acc_3, acc_5, acc_6, acc_8, acc_9]\n    })\nmodels.sort_values(by='Accuracy Score', ascending=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}