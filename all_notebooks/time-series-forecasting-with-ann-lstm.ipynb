{"cells":[{"metadata":{},"cell_type":"markdown","source":"## __A Study on Stock Price of Crude Oil Time Series Forecasting with Simple Neural Networks and LSTM__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Forecasting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Forecasting is the estimation of the future values of a variable.\n* We make forecasting about future events by drawing meaning from the models we obtained by using information from earlier periods.\n* Strategies, plans and targets for the future are determined by forecasting. \n* Forecasting studies were initially realized with simple modeling experiments and then more complex models were established by using the advantages of techonology, and better results were obtanied.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Forecasting Method: Artificial Neural Network (ANN)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A neural network can be thought of as a network of “neurons” which are organised in layers. The predictors (or inputs) form the bottom layer and the forecasts (or outputs) form the top layer. There may also be intermediate layers containing “hidden neurons”.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The simplest networks contain no hidden layers and are equivalent to linear regressions. This figure shows the neural network version of a linear regression with four predictors. The coefficients attached to these predictors are called “weights”. The forecasts are obtained by a linear combination of the inputs. The weights are selected in the neural network framework using a “learning algorithm” that minimises a “cost function” such as the MSE. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://otexts.com/fpp2/nnet2.png\" style=\"width: 600px;\"/> <br>\n_A neural network with four inputs and one hidden layer with three hidden neurons._ <br>","execution_count":null},{"metadata":{"tags":["LSTM","Forecasting"]},"cell_type":"markdown","source":"### Forecasting Method: Long Short Term Memory (LSTM)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LSTM is a special type of RNN that can learn long-term dependencies. It was first introduced in 1997 by Hochreiter and Schmindhuber. LSTMs are designed to prevent the problem of long-term dependence and are widely used. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![Rnn](https://camo.githubusercontent.com/a44d19bc6f7d2d8ef95452127f3d7e61dfd6379b/68747470733a2f2f692e6962622e636f2f7938546d3837312f526573696d312e706e67) \n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ A standart RNN has a single layer \n![LSTM](https://camo.githubusercontent.com/70fec97625524a11d1b7296ec18c13ac4b16dd37/68747470733a2f2f692e6962622e636f2f7a354d485368502f526573696d322e706e67)\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ LSTM has four specially placed layers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The line passing horizontally from the top along the diagram in the LSTM represents cell state. Data processed in layers contained in repeating modules has a certain effect on the values of cell state.\n<br> ![cellstate](https://camo.githubusercontent.com/3cc728a04a43b4970bf862473b5680ed10453ef4/68747470733a2f2f692e6962622e636f2f564c5058364d772f526573696d332e706e67)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LSTM has ability to add or remove information to the cell state through structures called gates. The gate in the figure consists of a sigmoid network layer and point multiplication. \n<br> ![gate](https://camo.githubusercontent.com/0ed236e6916c452367c01cf8a1d1a6db47911b88/68747470733a2f2f692e6962622e636f2f784d665a6851772f526573696d332d352e706e67)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://camo.githubusercontent.com/623dc99da4b11773ac862bc61844994da77b34ce/68747470733a2f2f692e6962622e636f2f3452354a53714b2f526573696d342e706e67\" style=\"width: 700px;\"/> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<br>\n<br>\nLSTM firstly determines which information passes through the cell state. This decision is made by sigmoid layer which is called forget gate layer. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://camo.githubusercontent.com/8ba9e82854ade2c8646edd3b3589c8fd4f54ec29/68747470733a2f2f692e6962622e636f2f4d7044527162772f526573696d352e706e67) ![](https://camo.githubusercontent.com/27d1a141e78dc5e90ccb54e4215c30cf53d14447/68747470733a2f2f692e6962622e636f2f39477a337674422f526573696d362e706e67)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The next step is to decide which new information is stored in cell state. This happens in two steps. Firstly, the sigmoid layer called input gate layer (update gate) decides which value to refresh.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The tanh layer creates new candidate values. Cell state is refreshed by combining these two generated values. This newly obtained values is the value stored for each state value to be transferred to next block. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://camo.githubusercontent.com/49bc070ed5fc3183fe134fa9ccbda98a5c465656/68747470733a2f2f692e6962622e636f2f4c3534744a72442f526573696d372e706e67) ![](https://camo.githubusercontent.com/7b094617e63e11993b628e3fdd641676362b18ef/68747470733a2f2f692e6962622e636f2f355474785671332f526573696d382e706e67)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At the last step, it  should be decided that what the output will be. Cell state for ouput is applied to tanh function in the sigmoid layer and multiplication with output value of sigmoid gate.(the value is set to -1 to 1) Thus, the value for decided part of the output is obtained. \n<br> ![](https://camo.githubusercontent.com/aca69a19f65aa774e2986e10b900f4fc4e067a6d/68747470733a2f2f692e6962622e636f2f7658787a5671742f526573696d392e706e67)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.layers import LSTM\nprint(\"All libraries have been imported\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Crude Oil WTI CL=F Stock Price\n<br> I get the data from \"finance.yahoo.com\". Dataset can be downloaded from [here](https://ca.finance.yahoo.com/quote/CL%3DF/history?period1=1262304000&period2=1577836800&interval=1d&filter=history&frequency=1d). I set the date range from Jan 01, 2010 to Jan 01, 2020 . ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Load the data into a Pandas dataframe and have a quick peek of the head rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/crude-oil-stock-price/CrudeOil.csv')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have NaN values and we drop these values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\nWe will drop the columns we don't need then convert \"Date\" column to datatime data type and set \"Date\" column to index.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date'])\ndf_chg= df.set_index(['Date'], drop=True)\ndf_chg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Plotting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We plot a time series line plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\ndf_chg['Adj Close'].plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, the \"Adj Close\" data are quite erratic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Split The Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We split the data by nearly %80 and %20 to train and test. Split the data to train and test set by date \"2017-12-27\". That is, the data prior to this date is the traning data and the data from this data onward is the test data and we plotting it again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"split_date = pd.Timestamp('2017-12-27')\ndf1 = df_chg['Adj Close']\ntrain = df1.loc[:split_date]\ntest = df1.loc[split_date:]\nplt.figure(figsize=(15,8))\nax = train.plot()\ntest.plot(ax=ax)\nplt.legend(['train', 'test']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scale The Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We scale train and test data to [-1, 1]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We splited train and test with Date values. We have to convert datas.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We have\", len(train), \"train values\")\nprint(\"We have\", len(test), \"test values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\nWe split train and test again ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_processed = df_chg.iloc[:, 0:1].values\ntrain_processed = train_processed[0:1996:1]\ntest_processed = df_chg.iloc[:, 0:1].values\ntest_processed = test_processed[1995:2495:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and scale datas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_sc = scaler.fit_transform(train_processed)\ntest_sc = scaler.transform(test_processed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get traning and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_sc[:-1]\ny_train = train_sc[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_sc[:-1]\ny_test = test_sc[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple ANN - Traning Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We create a Sequantial model\n* add layers via the .add() method\n* Pass an input_dim argument to the first layer.\n* The activation function is the Rectified Linear Unit- Relu.\n* Configure the learning process, which is done via the compile method.\n* A loss function is mean_squared_error , and An optimizer is adam.\n* Stop training when a monitored loss has stopped improving.\n* patience=2, indicate number of epochs with no improvement after which training will be stopped.\n* The ANN is trained for 100 epochs and a batch size of 1 is used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model = Sequential()\nnn_model.add(Dense(12, input_dim=1, activation='relu'))\nnn_model.add(Dense(1))\nnn_model.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\nhistory = nn_model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\nIt had an early stopping at Epoch 19/100","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We predict our model and get R2 scores for train and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test_nn = nn_model.predict(X_test)\ny_train_pred_nn = nn_model.predict(X_train)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_nn)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_nn)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM - Traning Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use shift function that shifts the entire column by 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sc_df = pd.DataFrame(train_sc, columns=['Y'], index=train.index)\ntest_sc_df = pd.DataFrame(test_sc, columns=['Y'], index=test.index)\n\n\nfor s in range(1,2):\n    train_sc_df['X_{}'.format(s)] = train_sc_df['Y'].shift(s)\n    test_sc_df['X_{}'.format(s)] = test_sc_df['Y'].shift(s)\n\nX_train = train_sc_df.dropna().drop('Y', axis=1)\ny_train = train_sc_df.dropna().drop('X_1', axis=1)\n\nX_test = test_sc_df.dropna().drop('Y', axis=1)\ny_test = test_sc_df.dropna().drop('X_1', axis=1)\n\nX_train = X_train['X_1']\ny_train = y_train['Y']\nX_test = X_test['X_1']\ny_test = y_test['Y']\n\nX_train = X_train.values\ny_train = y_train.values\n\nX_test = X_test.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will need to convert all our input variables in a 3D vector form.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lmse = X_train.reshape(X_train.shape[0], 1, 1)\nX_test_lmse = X_test.reshape(X_test.shape[0], 1, 1)\n\nprint('Train shape: ', X_train_lmse.shape)\nprint('Test shape: ', X_test_lmse.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* LSTM has a visible layer with 1 input.\n* A hidden layer with LSTM neurons.\n* We used relu activation function for the LSTM neurons. \n* A loss function is mean_squared_error , and An optimizer is adam.\n* Stop training when a monitored loss has stopped improving.\n* The LSTM is trained for 100 epoch and a batch size of 1 is used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model = Sequential()\nlstm_model.add(LSTM(7, input_shape=(1, X_train_lmse.shape[1]), activation='relu', kernel_initializer='lecun_uniform', return_sequences=False))\nlstm_model.add(Dense(1))\nlstm_model.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\nhistory_lstm_model = lstm_model.fit(X_train_lmse, y_train, epochs=100, batch_size=1, verbose=1, shuffle=False, callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\nIt had an early stopping at Epoch 35/100","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We predict our LSTM model and get R2 scores for train and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test_lstm = lstm_model.predict(X_test_lmse)\ny_train_pred_lstm = lstm_model.predict(X_train_lmse)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare test MSE of both models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_test_mse = nn_model.evaluate(X_test, y_test, batch_size=1)\nlstm_test_mse = lstm_model.evaluate(X_test_lmse, y_test, batch_size=1)\nprint('NN: %f'%nn_test_mse)\nprint('LSTM: %f'%lstm_test_mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forecasting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_y_pred_test = nn_model.predict(X_test)\nlstm_y_pred_test = lstm_model.predict(X_test_lmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nplt.plot(y_test, label='True')\nplt.plot(y_pred_test_nn, label='NN')\nplt.title(\"ANN's Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Adj Close Scaled')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nplt.plot(y_test, label='True')\nplt.plot(lstm_y_pred_test, label='LSTM')\nplt.title(\"LSTM's Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Adj Close scaled')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* https://mc.ai/an-introduction-on-time-series-forecasting-with-simple-neura-networks-lstm/\n* https://otexts.com/fpp2/nnetar.html\n* J. Schmidhuber, 2015\n* Olah, Understanding LSTM Networks, 2015\n* S. Hochreiter & J. Schmidhuber, Long Short-Term Memory, Neural Computation 9(8):1735-1780, 1997\n* https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/#:~:text=the%20validation%20dataset.-,Early%20Stopping%20in%20Keras,configured%20when%20instantiated%20via%20arguments","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}