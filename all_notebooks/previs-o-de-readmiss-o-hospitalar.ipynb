{"cells":[{"metadata":{"slideshow":{"slide_type":"slide"},"_uuid":"c9454b655ceee3b40da76c929828b6d92835185d"},"cell_type":"markdown","source":"## Previsão de readmissão hospitalar utilizando modelo de Deep Learning"},{"metadata":{"_uuid":"c6c9c24cb46a9b8f6cb075d10a4759dda70f2561"},"cell_type":"markdown","source":"### por Antonildo Santos "},{"metadata":{"_uuid":"5103e1b6de8fb80161a9ffe4dca6e2709aae2b58"},"cell_type":"markdown","source":"### Introdução\nO objetivo desta análise é criar um modelo de Rede Neural Profunda (Deep Learning) capaz prever, com o mais alto grau de precisão possível, os atendimentos propícios a ocorrência de Readmissão Hospitalar."},{"metadata":{"_uuid":"87d67b942328e9c68dbed57f908d77f5730023ec"},"cell_type":"markdown","source":"### Apresentação dos dados\n\nPara realizar este trabalho utilizei um conjunto de dados disponível publicamente no repositório da UCI [Link]( https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008# ). Este conjunto de dados representa 10 anos (1999-2008) de atendimento clínico em 130 hospitais dos EUA, contendo 101.766 observações em 10 anos. Inclui mais de 50 atributos, que representam características do paciente, diagnósticos, exames, etc. As informações foram extraídas do banco de dados contendo registros que satisfizeram os seguintes critérios. \n \n* 1) Cada atendimento representa uma internação hospitalar. \n* 2) Contempla apenas atendimentos de pacientes diabéticos, ou seja, aquele em que qualquer tipo de diabetes foi introduzido no sistema como um diagnóstico. \n* 3) O tempo de permanência foi de no mínimo 1 dia e no máximo 14 dias. \n* 4) Testes laboratoriais foram realizados durante o atendimento. \n* 5) Medicamentos foram administrados durante o atendimento.\n \nOs dados contêm atributos como número do paciente, raça, gênero, idade, tipo de internação, tempo no hospital, número de exames laboratoriais realizados, resultado do exame de HbA1c, diagnósticos, número de medicamentos utilizados, se usa medicamentos para diabético e quais, número de pacientes ambulatoriais , internação e visitas de emergência no ano anterior à hospitalização, etc. Alguns desses atributos serão desconsiderados nesta análise pois não terão relevância para o objetivo do trabalho.\n"},{"metadata":{"_uuid":"57b6d216f4fad9c7c7aec77c57baf4a9573b84eb"},"cell_type":"markdown","source":"###  Importando bibliotecas"},{"metadata":{"trusted":true,"_uuid":"4e1cbab189dc2b635f2d4d70a52cb48eef62b353"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nimport matplotlib as mpl\nfrom IPython.display import Image\nfrom sklearn.metrics import mean_squared_error\n#import seaborn as sns   \nimport warnings\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac5e5e97aff71c22387f261cd744f2cfc3270904"},"cell_type":"markdown","source":"### Carga de dados"},{"metadata":{"trusted":true,"_uuid":"0458e3a72fe4a702ee9afef07c6572bbe43cdd56","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Loading the database\ndf_uci_diabetic = pd.read_csv('../input/diabetic-data-cleaning/diabetic_data.csv', decimal=b',')\n\n# Criando um novo dataframe a partir do df_uci_diabetic\ndf = df_uci_diabetic.copy (deep = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90831ea9fad99e7e1f5020cabccd18cc1be984cd"},"cell_type":"markdown","source":"### Análise Exploratória"},{"metadata":{"trusted":true,"_uuid":"256522d90fcee7242b1d1d0e389d6b738f4add96"},"cell_type":"code","source":"print('O Dataframe diabetic_data possui ' + str(df.shape[0]) + ' linhas e ' + str(df.shape[1]) + ' colunas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be495b2e082ca9bea4c541ef1ec6896298041d93"},"cell_type":"code","source":"# Checking Data Types and Descriptive Statistics\nprint (df.info ()) \nprint (df.describe ())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28a6709357449e1e5b89993fd4e566757b5136fb"},"cell_type":"code","source":"# Viewing the first 10 rows of the dataframe\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac4f7f363717bb04d6314cee663ab0533771f487"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80e64a01c6e24414fa991e8b957c82959a866638"},"cell_type":"code","source":"# Checking for missing data\nfor col in df.columns:\n    if df[col].dtype == object:\n        if df[col][df[col] == '?'].count() > 0:\n            print(col,df[col][df[col] == '?'].count(),' Correspondendo a ',np.around((df[col][df[col] == '?'].count()/df[col].count())*100,2), '% das observações')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39f141ec66a69c2664b20ee218dc24a3b14d201f"},"cell_type":"code","source":"# Evaluating the distribution of data in each attribute (has missing data)\nfor col in df.columns:\n    if df[col].dtype == object:\n        if df[col][df[col] == '?'].count() != 0:       \n            print(df.groupby([col])[col].count())\n            print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e718de26f128849230423fc3106fa4e926867b3"},"cell_type":"code","source":"# Evaluating the distribution of data in each attribute (no missing data)\nfor col in df.columns:\n    if df[col].dtype == object:\n        if df[col][df[col] == '?'].count() == 0:       \n            print(df.groupby([col])[col].count())\n            print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e89809cf1695d6fb7b14c36a7381c7f8aadc894"},"cell_type":"code","source":"# Checking the median\nfor col in df.columns:\n    if df[col].dtype != object:\n        print(col, df[col].median())\n        print('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e0ef94c5547c9a9343973adfd20f9e85aec794"},"cell_type":"markdown","source":"### Limpeza e Transformação dos dados\n\nOs critérios de eliminação de atributos e observações depende muito da interpretação dos dados feita pelo Cientista de Dados na fase de exploração. Estes critérios passam pela avaliação de cada atributo do conjunto de dados, verificação da distribuição de frequência, analise de correlações entre variáveis, modelo preditivo que será aplicado, além de um certo conhecimento do negócio em estudo, para então decidir quais atributos e/ou observações devem ser descartados. Considerando o conjunto de dados em questão, decidi eliminar alguns atributos nos quais avalio que não impactará no resultado das análises preditivas. Descartarei os atributos \"encounter_id\", \"patient_nbr\", \"weight\", \"payer_code\", \"examide\", \"citoglipton\" e \"medical_specialty\". Por exemplo o atributo \"weight\", que corresponde ao peso do paciente, seria um atributo muito importante a ser considerado na análise, porém em 97% das observação este atributo está sem valor, tornando-se um dado insuficientemente consistente para o modelo aplicar algum tipo de generalização.\n\nCom base na consulta a documentação disponibilizada pelo repositório dos dados e entendimento de cada atributo do conjunto de dados, decidi também eliminar algumas observações que acredito não impactar no objetivo proposta neste trabalho. Precisarei também transformar alguns dados, com o objetivo de prepará-los para serem entregue ao modelo preditivo proposto."},{"metadata":{"trusted":true,"_uuid":"87f9c1b7da8a4cf61ed48043de422d9f52b7433b"},"cell_type":"code","source":"# Deleting columns that will not be used\ndf.drop(['encounter_id', 'patient_nbr', 'weight', 'payer_code', 'examide', 'citoglipton', 'medical_specialty'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45ed5df669a5493f5e60f0f39433f6b583cd4d63"},"cell_type":"markdown","source":"#### Categorização de diagnósticos\nNo conjunto de dados existem três diagnosticos, um principal e dois secundários, contendo em média 752 codigos distintos em cada um, por isso resolvi realizar um reagrupamento com base numa análise realizada por Strack et al. em 2014, sobre o mesmo tema e utilizando o mesmo conjunto de dados, publicado em ( https://www.hindawi.com/journals/bmri/2014/781670/abs/ ). "},{"metadata":{"trusted":true,"_uuid":"62724a94cc017dd2c6fc2b97387ec564ef0deca3"},"cell_type":"code","source":"Image('../input/imagem-1/Agrupamento_CID_9.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a49909508c879a1c1a0bd27ad00ba52770aab717"},"cell_type":"code","source":"# Creating new columns to assign transformed values\ndf['d1'] = df['diag_1']\ndf['d2'] = df['diag_2']\ndf['d3'] = df['diag_3']\ndf['classe'] = -1\ndf['change_t'] = -1\ndf['gender_t'] = -1\ndf['diabetesMed_t'] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8e5b65130c66baa765ccc401d4b26853ab3beee"},"cell_type":"code","source":"# Regrouping the main diagnosis\ndf['d1'] = df.apply(lambda row: 1 if (row['diag_1'][0:3].zfill(3) >= '390') and (row['diag_1'][0:3].zfill(3) <= '459' ) or  (row['diag_1'][0:3].zfill(3) == '785' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 2 if (row['diag_1'][0:3].zfill(3) >= '460') and (row['diag_1'][0:3].zfill(3) <= '519' ) or  (row['diag_1'][0:3].zfill(3) == '786' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 3 if (row['diag_1'][0:3].zfill(3) >= '520') and (row['diag_1'][0:3].zfill(3) <= '579' ) or  (row['diag_1'][0:3].zfill(3) == '787' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 4 if (row['diag_1'][0:3].zfill(3) == '250') else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 5 if (row['diag_1'][0:3].zfill(3) >= '800') and (row['diag_1'][0:3].zfill(3) <= '999' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 6 if (row['diag_1'][0:3].zfill(3) >= '710') and (row['diag_1'][0:3].zfill(3) <= '739' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 7 if (row['diag_1'][0:3].zfill(3) >= '580') and (row['diag_1'][0:3].zfill(3) <= '629' ) or  (row['diag_1'][0:3].zfill(3) == '788' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 8 if (row['diag_1'][0:3].zfill(3) >= '140') and (row['diag_1'][0:3].zfill(3) <= '239' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 9 if (row['diag_1'][0:3].zfill(3) >= '790') and (row['diag_1'][0:3].zfill(3) <= '799' ) or  (row['diag_1'][0:3].zfill(3) == '780' ) or  (row['diag_1'][0:3].zfill(3) == '781' ) or  (row['diag_1'][0:3].zfill(3) == '784' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 10 if (row['diag_1'][0:3].zfill(3) >= '240') and (row['diag_1'][0:3].zfill(3) <= '249' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 10 if (row['diag_1'][0:3].zfill(3) >= '251') and (row['diag_1'][0:3].zfill(3) <= '279' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 11 if (row['diag_1'][0:3].zfill(3) >= '680') and (row['diag_1'][0:3].zfill(3) <= '709' ) or  (row['diag_1'][0:3].zfill(3) == '782' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 12 if (row['diag_1'][0:3].zfill(3) >= '001') and (row['diag_1'][0:3].zfill(3) <= '139' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '290') and (row['diag_1'][0:3].zfill(3) <= '319' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:1] >= 'E') and (row['diag_1'][0:1] <= 'V' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '280') and (row['diag_1'][0:3].zfill(3) <= '289' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '320') and (row['diag_1'][0:3].zfill(3) <= '359' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '630') and (row['diag_1'][0:3].zfill(3) <= '679' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '360') and (row['diag_1'][0:3].zfill(3) <= '389' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '740') and (row['diag_1'][0:3].zfill(3) <= '759' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 0 if (row['diag_1'][0:3].zfill(3)  == '783' or row['diag_1'][0:3].zfill(3)  == '789') else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: -1 if (row['diag_1'][0:1] == '?') else row['d1'], axis=1)                           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dadfa255aaff9d99a5610ba4d5bef0c5c9bb0b7"},"cell_type":"code","source":"# Regrouping of the first secondary diagnosis\ndf['d2'] = df.apply(lambda row: 1 if (row['diag_2'][0:3].zfill(3) >= '390') and (row['diag_2'][0:3].zfill(3) <= '459' ) or  (row['diag_2'][0:3].zfill(3) == '785' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 2 if (row['diag_2'][0:3].zfill(3) >= '460') and (row['diag_2'][0:3].zfill(3) <= '519' ) or  (row['diag_2'][0:3].zfill(3) == '786' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 3 if (row['diag_2'][0:3].zfill(3) >= '520') and (row['diag_2'][0:3].zfill(3) <= '579' ) or  (row['diag_2'][0:3].zfill(3) == '787' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 4 if (row['diag_2'][0:3].zfill(3) == '250') else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 5 if (row['diag_2'][0:3].zfill(3) >= '800') and (row['diag_2'][0:3].zfill(3) <= '999' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 6 if (row['diag_2'][0:3].zfill(3) >= '710') and (row['diag_2'][0:3].zfill(3) <= '739' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 7 if (row['diag_2'][0:3].zfill(3) >= '580') and (row['diag_2'][0:3].zfill(3) <= '629' ) or  (row['diag_2'][0:3].zfill(3) == '788' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 8 if (row['diag_2'][0:3].zfill(3) >= '140') and (row['diag_2'][0:3].zfill(3) <= '239' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 9 if (row['diag_2'][0:3].zfill(3) >= '790') and (row['diag_2'][0:3].zfill(3) <= '799' ) or  (row['diag_2'][0:3].zfill(3) == '780' ) or  (row['diag_2'][0:3].zfill(3) == '781' ) or  (row['diag_2'][0:3].zfill(3) == '784' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 10 if (row['diag_2'][0:3].zfill(3) >= '240') and (row['diag_2'][0:3].zfill(3) <= '249' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 10 if (row['diag_2'][0:3].zfill(3) >= '251') and (row['diag_2'][0:3].zfill(3) <= '279' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 11 if (row['diag_2'][0:3].zfill(3) >= '680') and (row['diag_2'][0:3].zfill(3) <= '709' ) or  (row['diag_2'][0:3].zfill(3) == '782' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 12 if (row['diag_2'][0:3].zfill(3) >= '001') and (row['diag_2'][0:3].zfill(3) <= '139' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '290') and (row['diag_2'][0:3].zfill(3) <= '319' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:1] >= 'E') and (row['diag_2'][0:1] <= 'V' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '280') and (row['diag_2'][0:3].zfill(3) <= '289' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '320') and (row['diag_2'][0:3].zfill(3) <= '359' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '630') and (row['diag_2'][0:3].zfill(3) <= '679' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '360') and (row['diag_2'][0:3].zfill(3) <= '389' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '740') and (row['diag_2'][0:3].zfill(3) <= '759' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 0 if (row['diag_2'][0:3].zfill(3)  == '783' or row['diag_2'][0:3].zfill(3)  == '789') else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: -1 if (row['diag_2'][0:1] == '?') else row['d2'], axis=1)                           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33a908102ad9f8511f10889abbf76fd0e6f60ff1"},"cell_type":"code","source":"# Regrouping the second secondary diagnosis\ndf['d3'] = df.apply(lambda row: 1 if (row['diag_3'][0:3].zfill(3) >= '390') and (row['diag_3'][0:3].zfill(3) <= '459' ) or  (row['diag_3'][0:3].zfill(3) == '785' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 2 if (row['diag_3'][0:3].zfill(3) >= '460') and (row['diag_3'][0:3].zfill(3) <= '519' ) or  (row['diag_3'][0:3].zfill(3) == '786' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 3 if (row['diag_3'][0:3].zfill(3) >= '520') and (row['diag_3'][0:3].zfill(3) <= '579' ) or  (row['diag_3'][0:3].zfill(3) == '787' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 4 if (row['diag_3'][0:3].zfill(3) == '250') else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 5 if (row['diag_3'][0:3].zfill(3) >= '800') and (row['diag_3'][0:3].zfill(3) <= '999' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 6 if (row['diag_3'][0:3].zfill(3) >= '710') and (row['diag_3'][0:3].zfill(3) <= '739' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 7 if (row['diag_3'][0:3].zfill(3) .zfill(3)>= '580') and (row['diag_3'][0:3].zfill(3) <= '629' ) or  (row['diag_3'][0:3].zfill(3) == '788' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 8 if (row['diag_3'][0:3].zfill(3) >= '140') and (row['diag_3'][0:3].zfill(3) <= '239' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 9 if (row['diag_3'][0:3].zfill(3) >= '790') and (row['diag_3'][0:3].zfill(3) <= '799' ) or  (row['diag_3'][0:3].zfill(3) == '780' ) or  (row['diag_3'][0:3].zfill(3) == '781' ) or  (row['diag_3'][0:3].zfill(3) == '784' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 10 if (row['diag_3'][0:3].zfill(3) >= '240') and (row['diag_3'][0:3].zfill(3) <= '249' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 10 if (row['diag_3'][0:3].zfill(3) >= '251') and (row['diag_3'][0:3].zfill(3) <= '279' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 11 if (row['diag_3'][0:3].zfill(3) >= '680') and (row['diag_3'][0:3].zfill(3) <= '709' ) or  (row['diag_3'][0:3].zfill(3) == '782' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 12 if (row['diag_3'][0:3].zfill(3) >= '001') and (row['diag_3'][0:3].zfill(3) <= '139' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '290') and (row['diag_3'][0:3].zfill(3) <= '319' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:1] >= 'E') and (row['diag_3'][0:1] <= 'V' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '280') and (row['diag_3'][0:3].zfill(3) <= '289' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '320') and (row['diag_3'][0:3].zfill(3) <= '359' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '630') and (row['diag_3'][0:3].zfill(3) <= '679' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '360') and (row['diag_3'][0:3].zfill(3) <= '389' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '740') and (row['diag_3'][0:3].zfill(3) <= '759' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 0 if (row['diag_3'][0:3].zfill(3)  == '783' or row['diag_3'][0:3].zfill(3)  == '789') else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: -1 if (row['diag_3'][0:1] == '?') else row['d3'], axis=1)                           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d7123e75ab9215ad2efcf78c7b9e38cb554a10"},"cell_type":"code","source":"print(df.groupby(['d1', 'diag_1']).d2.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d06705cf2982e85a39fd2346ea451418aa4ccf6"},"cell_type":"code","source":"print(df.groupby(['d2', 'diag_2']).d2.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"945381576328b16acbf572be772b20c9e7d3087a"},"cell_type":"code","source":"print(df.groupby(['d3', 'diag_3']).d3.count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"229283f99df377bbbdced90351cadf602c082814"},"cell_type":"markdown","source":"Conforme consta na documentação, trata-se de um do conjunto de dados contendo atendimentos onde qualquer tipo de diabetes foi introduzido no sistema como um diagnóstico, então eliminarei as observações onde não existe nenhum diagnóstico registrado."},{"metadata":{"trusted":true,"_uuid":"dfc2420575b848bf5aa47caaef0d2e742f9d981f"},"cell_type":"code","source":"df = df[(df.d1 > -1) | (df.d2 > -1) | (df.d3 > -1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c8087c832a5490c56aa5b9b2ddf67ed10476e9b"},"cell_type":"code","source":"# Deleting the original columns from diagnostics\ndf.drop(['diag_1'], axis = 1, inplace = True)\ndf.drop(['diag_2'], axis = 1, inplace = True)\ndf.drop(['diag_3'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5376425391572a92f85d4bb45ffb6ef48d258e0e"},"cell_type":"markdown","source":"### Aplicando a transformação dos dados"},{"metadata":{"trusted":true,"_uuid":"95929b917695d8a855882f732538044d6bac849a"},"cell_type":"code","source":"# Assigns the class the values 1 or 0, 1 corresponding to readmission occurrences in less than 30 days\ndf['classe'] = df.apply(lambda row: 0 if (row['readmitted'][0:3] == '>30' or row['readmitted'][0:2] == 'NO') else row['classe'], axis=1) \ndf['classe'] = df.apply(lambda row: 1 if (row['readmitted'][0:3] == '<30') else row['classe'], axis=1)\ndf.drop(['readmitted'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b431b0adfce5367b229bb3c6933772e582bf1ba0"},"cell_type":"code","source":"df['change_t'] = df.apply(lambda row: 1 if (row['change'] == 'Ch') else -1, axis=1)\ndf['change_t'] = df.apply(lambda row: 0 if (row['change'] == 'No') else row['change_t'], axis=1)\ndf.drop(['change'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d961ed30b804dc18bc3218e313199f9d8f3b84c0"},"cell_type":"code","source":"df['gender_t'] = df.apply(lambda row: 1 if (row['gender'] == 'Male') else -1, axis=1)\ndf['gender_t'] = df.apply(lambda row: 0 if (row['gender'] == 'Female') else row['gender_t'], axis=1)\ndf.drop(['gender'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e91e2ac387a0ec18a5dd8aff3f8c28f24e3a008"},"cell_type":"code","source":"df['diabetesMed_t'] = df.apply(lambda row: 1 if (row['diabetesMed'] == 'Yes') else -1, axis=1)\ndf['diabetesMed_t'] = df.apply(lambda row: 0 if (row['diabetesMed'] == 'No') else row['diabetesMed_t'], axis=1)\ndf.drop(['diabetesMed'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cca14f2c8c01b0a70e965697088e5d1676c26a8d"},"cell_type":"code","source":"m = 0\nmedicacoes = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', \n              'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide', \n              'metformin-pioglitazone','metformin-rosiglitazone', 'glimepiride-pioglitazone', \n              'glipizide-metformin', 'troglitazone', 'tolbutamide', 'acetohexamide']\nfor col in df.columns:\n    if col in medicacoes:       \n        colname = 'Med' + str(m) + '_t'\n        df[colname] = df.apply(lambda row: 0 if (row[col] == 'No') else 1, axis=1)\n        df.drop([col], axis = 1, inplace = True)\n        m = m + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6fb5aa00ff9108855c583495a410629735d24b9"},"cell_type":"code","source":"df['A1Cresult_t'] = df.apply(lambda row: 0 if (row['A1Cresult'][0:4] == 'Norm') else -1, axis=1) \ndf['A1Cresult_t'] = df.apply(lambda row: 1 if (row['A1Cresult'][0:2] == '>7' or row['A1Cresult'][0:2] == '>8') else row['A1Cresult_t'], axis=1) \ndf.drop(['A1Cresult'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e909ac9d84f9d8d780b9ef7f7187554114abe4cd"},"cell_type":"code","source":"df['max_glu_serum_t'] = df.apply(lambda row: 0 if (row['max_glu_serum'][0:4] == 'Norm') else -1, axis=1) \ndf['max_glu_serum_t'] = df.apply(lambda row: 1 if (row['max_glu_serum'][0:2] == '>7' or row['max_glu_serum'][0:2] == '>8') else row['max_glu_serum_t'], axis=1) \ndf.drop(['max_glu_serum'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"008bc26d011990881662ef12bbd5db1aa9815e8b"},"cell_type":"code","source":"df['age_faixa'] = df.apply(lambda row: 0 if (row['age'] == '[0-10)') else -1, axis=1) \ndf['age_faixa'] = df.apply(lambda row: 1 if (row['age'] == '[10-20)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 2 if (row['age'] == '[20-30)') else row['age_faixa'], axis=1) \ndf['age_faixa'] = df.apply(lambda row: 3 if (row['age'] == '[30-40)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 4 if (row['age'] == '[40-50)') else row['age_faixa'], axis=1) \ndf['age_faixa'] = df.apply(lambda row: 5 if (row['age'] == '[50-60)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 6 if (row['age'] == '[70-80)') else row['age_faixa'], axis=1) \ndf['age_faixa'] = df.apply(lambda row: 7 if (row['age'] == '[80-90)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 8 if (row['age'] == '[90-100)') else row['age_faixa'], axis=1)\ndf.drop(['age'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92515192a659a48433ba50411e1d9148e372a691"},"cell_type":"code","source":"df['race_t'] = df.apply(lambda row: 0 if (row['race'] == '?') else -1, axis=1) \ndf['race_t'] = df.apply(lambda row: 1 if (row['race'] == 'AfricanAmerican') else row['race_t'], axis=1)\ndf['race_t'] = df.apply(lambda row: 2 if (row['race'] == 'Asian') else row['race_t'], axis=1) \ndf['race_t'] = df.apply(lambda row: 3 if (row['race'] == 'Caucasian') else row['race_t'], axis=1)\ndf['race_t'] = df.apply(lambda row: 4 if (row['race'] == 'Hispanic') else row['race_t'], axis=1) \ndf['race_t'] = df.apply(lambda row: 5 if (row['race'] == 'Other') else row['race_t'], axis=1)\ndf.drop(['race'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51e5e59dd32f706c82f450e0f962556463758967"},"cell_type":"code","source":"# Saving the dataset with the transformations\ndf.to_csv('./diabetes_data_modificado.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28ff6620c3a3bfe5bff0101f4b6f11d5a1ea29f2"},"cell_type":"code","source":"# Loading the transformed database\ndf = pd.read_csv('diabetes_data_modificado.csv', decimal=b',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44fdc6b7a2e1ed9a80b158b00ceca937208076ff"},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2b70adfd31e8eb526c40d7ef6f1ad5609010626"},"cell_type":"code","source":"print (df.info ()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c23b1979289884909e133dca64cd03b7f7cb615"},"cell_type":"code","source":"print(df.groupby(['classe']).classe.count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171f1c60c6e62623194c3c585b7a0181950106f4"},"cell_type":"markdown","source":"### Construção do Modelo Preditivo"},{"metadata":{"_uuid":"a90adb5c4db0999b2b576497b40bf62aeac29546"},"cell_type":"markdown","source":"O indicador de Readmissão Hospitalar  mede a taxa de pacientes que retornaram ao hospital em até 30 dias desde a última alta hospitalar correspondente a primeira admissão. Este indicador avalia a capacidade progressiva do prestador de serviço em ajudar na recuperação do paciente. Nos Estados unidos, várias iniciativas já foram tomadas para garantir o sucesso da recuperação da saúde de seus pacientes, usando técnicas de trabalho em equipe a tecnologia para diminuir a readmissão hospitalar.\n\nA taxa de readmissão hospitalar é frequentemente usada como uma medida da qualidade assistencial de um hospital, segundo determina a ANS, o indicador de Readmissão Hospitalar é um dos critérios para o estabelecimento alcançar o reajuste de 100% do IPCA, consequentemente uma alta taxa de readmissão pode afetar o índice de reajuste dos contratos firmados entre operadoras de planos de saúde e prestadores de serviço. Segundo a diretora-adjunta de Desenvolvimento Setorial da ANS, Michelle Mello “Esse é um indicador internacional clássico para avaliação da qualidade de atendimento e cuidado prestados ao paciente nos hospitais. Quanto menor for a reincidência de internação, ou seja, quanto menor for a readmissão potencialmente evitável, melhor é considerado o atendimento prestado pela unidade hospitalar”.\n\nUm dos grandes desafios dos hospitais é identificar as readmissões que poderiam ser evitadas. Ser capaz de prever quais pacientes serão readmitidos pode ajudar os hospitais e operadoras de plano de saúde a economizar milhões de reais e melhorar a qualidade dos cuidados e recuperação dos pacientes\n\nO objetivo deste trabalhe é implementar um modelo de Deep Learning, capaz de classificar os pacientes que serão readmitidos, com o mais alto grau de precisão possível. Um dos desafios ao analisar este conjunto de dados é o enorme desequilíbrio da variável target: as readmissões com menos de 30 dias correspondem apenas 11,16% dos atendimentos. Nesse caso, é muito pior ter falsos negativos do que falsos positivos em nossas previsões, pois falsos negativos significam que algum paciente foi readmitido, porém o modelo não foi capaz de prever, isso poderá comprometer os idicadores de qualidade da instituição. "},{"metadata":{"trusted":true,"_uuid":"d541ffd25d4866d39dd5cf2d4406a1c1ea38f7e2"},"cell_type":"code","source":"# Data Manipulation Packages\nimport sklearn \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA, RandomizedPCA\nfrom sklearn.preprocessing import scale, MinMaxScaler, MultiLabelBinarizer, QuantileTransformer, Normalizer, StandardScaler, MaxAbsScaler, RobustScaler\n\n# Keras e TensorFlow\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.models import load_model\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport tensorflow as tf\n\n# Pacotes para Confusion Matrix e Balanceamento de Classes\n#from pandas_ml import ConfusionMatrix\n#import pandas_ml as pdml\nimport imblearn\n\nLABELS = [\"Normal\", \"Readmissão\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"265d16f97396b621814265a9b2801e8c26bf9bf2"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.metrics import classification_report\n\ndef pretty_print_conf_matrix(y_true, y_pred, \n                             classes,\n                             normalize=False,\n                             title='Confusion matrix',\n                             cmap=plt.cm.Blues):\n    \"\"\"\n    referência: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\n    \"\"\"\n\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Configure Confusion Matrix Plot Aesthetics (no text yet) \n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n    fig.colorbar(cax)\n    plt.title(title, fontsize=14)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)\n\n    # Calculate normalized values (so all cells sum to 1) if desired\n    if normalize:\n        cm = np.round(cm.astype('float') / cm.sum(),2) #(axis=1)[:, np.newaxis]\n\n    # Place Numbers as Text on Confusion Matrix Plot\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\",\n                 fontsize=12)\n\n\n    # Add Precision, Recall, F-1 Score as Captions Below Plot\n    rpt = classification_report(y_true, y_pred)\n    rpt = rpt.replace('avg / total', '      avg')\n    rpt = rpt.replace('support', 'N Obs')\n\n    plt.annotate(rpt, \n                 xy = (0,0), \n                 xytext = (-50, -140), \n                 xycoords='axes fraction', textcoords='offset points',\n                 fontsize=12, ha='left')    \n\n    # Plot\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e897f70ea3b31bdf63550f621dfe2ac06dc1f1a"},"cell_type":"code","source":"# Function for the statistics of accuracy, inaccuracy, false negative and false positive rates\ndef estatisticas(y_true, y_pred):\n    false_neg = 0\n    false_pos = 0\n    incorrect = 0\n    y2_true = np.array(y_true)\n    total = len(y_true)\n    for i in range(len(y_true)):        \n        if y_pred[i] != y2_true[i]:\n            incorrect += 1\n            if y2_true[i] == 1 and y_pred[i] == 0:\n                false_neg += 1\n            else:\n                false_pos += 1\n\n    inaccuracy = incorrect / total\n\n    print('Inacurácia:', inaccuracy)\n    print('Acurácia:', 1 - inaccuracy)\n    if incorrect > 0:\n        print('Taxa de Falsos Negativos:', false_neg/incorrect)\n        print('Taxa de Falsos Positivos:', false_pos / incorrect )    \n    print('Falsos Negativos/total:', false_neg/total)\n    return inaccuracy, incorrect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"513f6244803de184f09405d8bdc8e224585ea0a0"},"cell_type":"code","source":"#df['classe'].hist()\n#plt.show()\ncount_classes = pd.value_counts(df['classe'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Distribuição\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Frequência\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5764ded848579bc21c1707e528c53f85c74be09b"},"cell_type":"code","source":"print('O Dataframe diabetic_data_modificado possui ' + str(df.shape[0]) + ' linhas e ' + str(df.shape[1]) + ' colunas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e7ce2dc7a926c05148c4b64b87b751aa7a8aac"},"cell_type":"code","source":"readmissoes = df.loc[df['classe'] == 1]\nnao_readmissoes = df.loc[df['classe'] == 0]\nprint(\"Temos\", len(readmissoes), \"pontos de dados como readmissões e\", len(nao_readmissoes), \"pontos de dados considerados normais.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38f31f9bad68fcef78ef206c392a36c3bb7e8665"},"cell_type":"code","source":"# Assigning Values to the X and Y Variables of the Model\nX = df.iloc[:,:-1]\ny = df['classe']\n\n# Aplicando Scala e Redução de dimensionalidade com PCA\nX = scale(X)\npca = PCA(n_components = 10, random_state=38)\nX = pca.fit_transform(X)\n\n# Gerando dados de treino, teste e validação\nX1, X_valid, y1, y_valid = train_test_split(X, y, test_size = 0.10, random_state = 0)\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.26, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c1fbfcb6b52476599681b7ba4fa119eeffa64f7"},"cell_type":"code","source":"print(\"Tamanho do Dataset de Treino: \", X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c765da012ec9feba99d6656c1d9da777ce0b70c"},"cell_type":"code","source":"print(\"Tamanho do Dataset de Validaçao: \", X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a9c5b6f09649330243db6b4469f14b89b498ff5"},"cell_type":"code","source":"print(\"Tamanho do Dataset de Test: \", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c672717177ac1f5aa997597c1ca4d23361f96afa"},"cell_type":"markdown","source":"### Começando com uma Rede Neural Simples"},{"metadata":{"trusted":true,"_uuid":"9a0b4f02f6aeb94747982a02b93e51af9e61a2e6"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(10, input_dim = 10, activation = 'relu'))     \nmodel.add(Dense(1, activation = 'sigmoid'))                \nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b66bb0de4a671a021bd0894e18ef7a4a2b23f65d"},"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 1, validation_data=(X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b09a368a8f66f6220c4648a0079be18112c9ca"},"cell_type":"code","source":"print(\"Erro/Acurácia: \", model.evaluate(X_valid, y_valid, verbose = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d551d5222b7249bceb09e18e21050910a8e6779d"},"cell_type":"code","source":"y_predicted = model.predict(X_valid).T[0].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"635cede994a4c398328d45609b471f24071f64bd"},"cell_type":"code","source":"# Plot Confusion Matrix\nwarnings.filterwarnings('ignore')\npretty_print_conf_matrix(y_valid, y_predicted, \n                         classes= ['0', '1'],\n                         normalize=False, \n                         title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"166524d870ead6f9e1f4de7c3887ef3178037a0c"},"cell_type":"code","source":"estatisticas(y_valid, y_predicted)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d983c3876345642728458607725f0fbae927509"},"cell_type":"markdown","source":"Podemos observar que apesar do modelo está demonstrando uma acurácia de aproximadamente 88%, analisando a ConfusionMatrix percebemos que os resultados não foram satisfatório, consequencia do desbalanciamento dos dados. Então vamos aplicar um oversampling para corrigir um viés no conjunto de dados original, empregando Synthetic Minority Over-sampling Technique para balancear os dados. "},{"metadata":{"_uuid":"0b20fc3b8161f4c4d0f6c7a6463e94887a4c25d1"},"cell_type":"markdown","source":"### Aplicando Oversampling"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"486c00d96bc24bdc057cc5d107de740e4825382e"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=42, ratio='minority')\nX2, y2 = smote.fit_sample(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cba9089c2e1cbc12d32f59f04dcb337a0185145d"},"cell_type":"code","source":"count_classes = pd.value_counts(y2, sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Distribuição\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Frequência\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67b76a5edb038413dd39f96e25c481e425053d02"},"cell_type":"markdown","source":"### Gerando dados de Treino\n\nOs dados balanceados servirão para gerar apenas o conjunto de dados treino, para que os dados sintéticos gerados não vazem para os conjuntos de teste e validação."},{"metadata":{"trusted":true,"_uuid":"96941e43195418bfac6672ac8565347e57dea5fe"},"cell_type":"code","source":"# Generating training data based on balanced data\nX2_train, X_test_, y2_train, y_test_ = train_test_split(X2, y2, test_size = 0.33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55780a430a4988bceb182fc7fa2b365d2d6deebc"},"cell_type":"markdown","source":"### Aumentando o Número de Camadas na Rede Neural"},{"metadata":{"trusted":true,"_uuid":"64866ef224ddff0417cc0f8b99a8d3d4114e8b8e"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom tensorflow import set_random_seed\nimport keras as keras\nfrom sklearn.metrics import precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5384b31f16dfd3df65bea167d3e28d09d8de29e5"},"cell_type":"code","source":"#OPTIMIZER = Adam(lr=0.01, beta_1=0.99, beta_2=0.999, amsgrad=True) # otimizador\nOPTIMIZER = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f8579fc7e28334d148566ae30731689711f2324"},"cell_type":"code","source":"# Class to calculate metric of accuracy based on recall\nclass Metrics(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = np.round(model2.predict(X_val)).T[0]\n    \n        self._data.append({\n            'val_recall': recall_score(y_val, np.round(model2.predict(X_val)).T[0]),\n            'val_precision': precision_score(y_val, np.round(model2.predict(X_val)).T[0]),\n        })\n        return\n\n    def get_data(self):\n        return self._data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a17d3d0c523b2b7765a98b648c5535886008322"},"cell_type":"code","source":"batch_size = 8790\nseed = 100\nset_random_seed(seed)\nmetrics = Metrics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35298ff8676c1f70f225d11eb26c878b6bf6d0fc"},"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Dense(10, input_dim = 10,   kernel_initializer='ones', activation = 'tanh')) \nmodel2.add(Dense(1024, activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(512, activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(16,  activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(8,  activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(4,  activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(1,  activation = 'sigmoid'))\nmonitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')   \nmodel2.compile(loss = 'binary_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfed9176369306962ca43e4a4c6e1510f4ee2818"},"cell_type":"code","source":"history = model2.fit(X2_train, y2_train, epochs = 100, batch_size = batch_size, validation_data=(X_valid, y_valid), callbacks = [monitor, metrics], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"26347fa05349cd8e92de651b525895faad4c01f8"},"cell_type":"code","source":"# Perform the training until you achieve the best recall accuracy, mandating the balance of total accuracy\nLastrecall = 0\nMaxrecall = 0\nMaxprecision = 0\nfor i in range(5980,9790,10):\n    batch_size = i\n    print(i)\n    metrics = Metrics()\n    history = model2.fit(X2_train, y2_train, epochs = 100,  batch_size = batch_size, validation_data=(X_valid, y_valid), callbacks = [monitor, metrics], shuffle=False)\n    if recall_score(y_test,np.round(model2.predict(X_test)).T[0]) > Maxrecall and precision_score(y_test,np.round(model2.predict(X_test)).T[0]) > Maxprecision:\n        print(recall_score(y_test,np.round(model2.predict(X_test)).T[0]), i)\n        Maxrecall = recall_score(y_test,np.round(model2.predict(X_test)).T[0])\n        Maxprecision = precision_score(y_test,np.round(model2.predict(X_test)).T[0])\n        if Maxrecall > Lastrecall:\n            Lastrecall = Maxrecall\n            model2.save('./best_model.h5')   \n#    metrics.get_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21ef91eb8c4e58b286942277a915cb21bf720d66"},"cell_type":"code","source":"# load model from single file\nmodel2 = load_model('best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d280dfb9972bd0e627baff73df7fed71b4954cc"},"cell_type":"code","source":"# Evaluating the Model\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss'), \nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2af809fe721142f53b1bf085b393a4a516d3ffc5"},"cell_type":"code","source":"print(\"Loss: \", model2.evaluate(X_valid, y_valid, verbose=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6166db26ab3941b540ddc63ef684e72df3840d13"},"cell_type":"code","source":"from sklearn.metrics import recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa7dd48cb2ebc397e974395ae00da7271103e3ad"},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68c9bafa42d1b95a70bd55f6efb7d54214158234"},"cell_type":"code","source":"probs = model2.predict_proba(X_valid)\npreds = probs[:,0]\nfpr, tpr, threshold = metrics.roc_curve(y_valid, preds)\nroc_auc = metrics.auc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6617f12e5b41f5ddc10d08a17fcdf8fa6df698d1"},"cell_type":"code","source":"plt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True  Positive rate')\nplt.xlabel('False Positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d26118bd1117e2b62c77358287ca669dc41afab"},"cell_type":"code","source":"y2_predicted = np.round(model2.predict(X_test)).T[0]\ny2_correct = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f8602acd297cd085dd3a1473aca7aaf6dcfaa1b"},"cell_type":"code","source":"np.setdiff1d(y2_predicted, y2_correct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"315984bfeb801a93732b765454dc21b33dcb74b5"},"cell_type":"code","source":"inaccuracy, incorrect = estatisticas(y2_correct, y2_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9320e01dc7a8c53904fd024d0f391d6001a6295e"},"cell_type":"code","source":"print('Validation Results')\nprint(recall_score(y_valid,np.round(model2.predict(X_valid)).T[0]))\nprint('\\nTest Results')\nprint(1 - inaccuracy)\nprint(recall_score(y_test,np.round(model2.predict(X_test)).T[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c2567f86ffc4c400ba22b6d57f55045aba20a3d"},"cell_type":"code","source":"print(incorrect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"132cc0f7d7a4a37bc0cbfbf1ea3eb1bb32ee8e15"},"cell_type":"code","source":"# Plot Confusion Matrix\nwarnings.filterwarnings('ignore')\n#plt.style.use('classic')\n#plt.figure(figsize=(5,5))\npretty_print_conf_matrix(y2_correct, y2_predicted, \n                         classes= ['0', '1'],\n                         normalize=False, \n                         title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d061e48542394668dac2d47f9a1b02cb07954031"},"cell_type":"markdown","source":"### Conclusão\nO modelo de Deep Learning conseguiu atingir um excelente índice de acurácia no recall da classe minoritária, demonstrando ser uma ferramenta eficaz na identificação antecipada de pacientes que necessitarão de uma maior atenção da equipe assistencial, por possuir uma alta probabilidade de ocorrência de readmissão.\n"},{"metadata":{"_uuid":"612993999d5bcf083bcb213c704dfc058817154f"},"cell_type":"markdown","source":"\n#### Referencias\n\nData Science Acabemy: Formação Inteligencia Artificial           \nhttps://www.datascienceacademy.com.br/pages/formacao-inteligencia-artificial\n\nFator de Qualidade: dados de readmissão hospitalar devem ser informados à ANS   \nhttp://www.ans.gov.br/aans/noticias-ans/qualidade-da-saude/3167-fator-de-qualidade-dados-de-readmissao-hospitalar-devem-ser-informados-a-ans\n\n3 formas únicas de diminuir a readmissão hospitalar       \nhttps://saudebusiness.com/noticias/3-formas-diminuir-readmissao-hospitalar/\n\nResampling strategies for imbalanced datasets       \nhttps://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n\nThe Right Way to Oversample in Predictive Modeling       \nhttps://beckernick.github.io/oversampling-modeling/\n\nScikit-learn - Confusion Matrix   \nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n"},{"metadata":{"trusted":true,"_uuid":"ef20c87cad9e401d972000c98ea336f3ea27c1c4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}