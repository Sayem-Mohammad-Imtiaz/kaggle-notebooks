{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Decision tree in sklearn - analisi parametri (min_samples_leaf and max_depth )\nAttraverso questo kernel farò il mio primo approccio agli alberi di decisione in sklearn; in particolare, voglio vedere come i parametri *min_samples_leaf* e *max_depth* influiscano sull'accuratezza e sul problema dell'*overfitting*, confrontando poi anche con la tecnica del **reduced error pruning**."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Operazioni preliminari sui dati \n\ndataset in: /kaggle/input/winequality-red.csv"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = 500 # prendo le prime 501 righe\n# winequality-red.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndata = pd.read_csv('/kaggle/input/winequality-red.csv', delimiter=',', nrows = nRowsRead)\ndata.dataframeName = 'winequality-red.csv'\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Controllo quali valori può assumere la y target\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.quality.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modifico i possibili valori target raggruppandoli in 3 classi: bad, average, good\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ranges = (2, 5, 6, 8) # da 3 a 5 bad, 6 average e da 7 a 8 good\nclasses_names = ['bad', 'average', 'good']\ndata['quality'] = pd.cut(data['quality'], bins = ranges, labels = classes_names)\n\nprint(data.quality.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preparo i dati per il training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, test_size = 0.3) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creo l'albero di decisione\n\nNota: il metodo *DecistionTreeClassifier* offerto da scikit mette a disposizione alcuni parametri da settare in maniera appropriata per evitare overfitting, tramite il controllo della crescita dell'albero di decisione.\nIn particolare mostrerò come cambia l'accuratezza al variare di *min_samples_leaf* e *max_depth*."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\ndepths = [1, 3, 5, 8, 10, None]\nminsamples = [1, 5, 10, 20, 40, None]\n\nfor d in depths:\n    for m in minsamples:\n        t = tree.DecisionTreeClassifier(max_depth = d, min_samples_leaf = m)\n#        t.fit(train.drop('quality', axis = 1), train['quality']) ","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}