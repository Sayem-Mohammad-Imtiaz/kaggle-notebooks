{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spotify Top Tracks of the Decade"},{"metadata":{},"cell_type":"markdown","source":"##  In this exercise I decided to use a data set from Spotify that contained the top 50 songs for each year from 2010-2019.  As a musician I was curious to see what a hit song in 2020 might look like (and how to write one!), and also to test my hypothesis that popular songs are getting more sad sounding.  Then for fun I decided to see if I could use Machine Learning to make playlists based off of their feature characteristics."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data set can be found here!\n\n#                           https://www.kaggle.com/leonardopena/top-spotify-songs-from-20102019-by-year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import packages and read data into a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#first import of packages \n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\n\nimport sklearn\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv(\"../input/top-spotify-songs-from-20102019-by-year/top10s.csv\", encoding='ISO-8859-1') \n\n\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming the columns\n\ndf.rename(columns={'title':'Track Name','artist':'Artist Name','bpm':'Beats Per Minute','top genre':'Genre','nrgy':'Energy','dnce':'Danceability', 'dB':'Loudness dB','spch':'Speechiness','live':'Liveness','val':'Valence','dur':'Length','acous':'Acousticness','pop':'Popularity'},inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 1 - EDA and Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The datatypes of the different columns\n\nprint(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get initial descriptive statistics on the columns\n\npd.set_option('precision', 3)\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical EDA with bar graphs to see most popular Artists, Genres, and Tracks"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the number of songs of each genre\n\nprint(type(df['Genre']))\n\npopular_genre = df.groupby('Genre').size()\n\npopular_genre = popular_genre.sort_values(ascending=False)\n\npopular_genre\n\ngenre_list = df['Genre'].values.tolist()\n\ngenre_top20 = popular_genre[0:20,]\n\ngenre_top20 = genre_top20.sort_values(ascending=True)\n\ngenre_top20 = pd.DataFrame(genre_top20, columns = [ 'Number of Songs'])\n\ngenre_top20.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\n\n\nax = sns.barplot(x = 'Number of Songs' , y = genre_top20.index , data = genre_top20, orient = 'h', palette = sns.color_palette(\"muted\", 20), saturation = 0.8)\n\nplt.title(\"Top 20 Genres of the Decade... That's a lot of Pop!\",fontsize=30)\nplt.xlabel('Number of Songs', fontsize=25)\nplt.ylabel('Genre', fontsize=10)\n\nxticks = [0, 10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340]\n\nplt.xticks(xticks, size=20,rotation=45)\nplt.yticks(size=20)\nsns.despine(bottom=True, left=True)\n\n\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pie chart to show top 20 genres\n\nlabels = genre_top20.index\nsizes = genre_top20.values\n\nexplode = (  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.1, 0.1)\n\nplt.figure(figsize = (10,10))\n\nplt.pie(sizes, labels = labels, explode = explode)\n\nplt.title(\"Top 20 Genres on the List\", fontsize=16)\n\nautopct=('%1.1f%%')\nplt.axis('equal')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the least popular genres\n\n\ngenre_bot29 = popular_genre[21:,]\n\ngenre_bot29 = genre_bot29.sort_values(ascending=True)\n\ngenre_bot29 = pd.DataFrame(genre_bot29, columns = [ 'Number of Songs'])\n\ngenre_bot29.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pie chart to show bottom 35 genres\n\nlabels = genre_bot29.index\nsizes = genre_bot29.values\n\nexplode = ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)\n\nplt.figure(figsize = (10,10))\n\nplt.pie(sizes, labels = labels, explode = explode)\n\nplt.title(\"Least Popular Genres on the Top 50 List\", fontsize=16)\n\nautopct=('%1.1f%%')\nplt.axis('equal')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the number of songs by each of the artists\n\n\npopular_artist = df.groupby('Artist Name').size()\n\npopular_artist = popular_artist.sort_values(ascending=False)\n\npopular_artist\n\nartist_list=df['Artist Name'].values.tolist()\n\nartist_top25 = popular_artist[0:25,]\n\nartist_top25 = artist_top25.sort_values(ascending=True)\n\nartist_top25 = pd.DataFrame(artist_top25, columns = [ 'Number of Songs'])\n\nartist_top25.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\n\n\nax = sns.barplot(x = 'Number of Songs' , y = artist_top25.index , data = artist_top25, orient = 'h', palette = sns.color_palette(\"muted\", 25), saturation = 0.8)\n\nplt.title(\"Top 25 Artists of the Decade\",fontsize=30)\nplt.xlabel('Number of Songs on Top 50 List', fontsize=25)\nplt.ylabel('Artist', fontsize=10)\n\nxticks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n\nplt.xticks(xticks, size=20,rotation=45)\nplt.yticks(size=20)\nsns.despine(bottom=True, left=True)\n\n\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Songs that made the top 50 list twice\n\nplt.figure(figsize=(16,8))\n\ntracks = pd.value_counts(df['Track Name']).iloc[:18].index\n\nsns.countplot(df['Track Name'], order = tracks, orient = 'h', palette = sns.color_palette(\"magma\", 25), saturation =0.7)\n\nplt.title('Songs That Made the Top 50 List on Two Different Years',fontsize=30)\nplt.xlabel('Track', fontsize=25)\nplt.ylabel('Number of Years in Top 50 List', fontsize=25)\n\nplt.xticks(size=20,rotation=90)\nplt.yticks( [0, 1, 2]  , size=20)\nsns.despine(bottom=True, left=True)\n\n\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Investigating  bad data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a histogram to show the spread of Popularity since we notice some strange stats worth investigating\n\nplt.hist(df['Popularity'],bins=100)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigating low popularity\n\nlow_pop = df[df['Popularity'] <= 20]\n\nlow_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspect bad data...  How can the popularity be 0 if these are top 50 songs?\n\ndf.loc[df['Popularity']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop bad data\n\ndf = df.drop(df.index[[50, 138, 267, 362, 442]])\n\ndf = df.reset_index()\n\n# check it's gone\n\ndf.iloc[[50, 138, 267, 362, 442]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean up index\n\ndf = df.drop('index', axis=1)\ndf = df.drop('Unnamed: 0', axis=1)\n\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2 - Statistical EDA, Normalization, and PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get descriptive statistics on the columns to see the change\n\npd.set_option('precision', 3)\n\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get df ready for scatter matrix\n\ndf_features = df.drop(df.columns[[0, 1, 2, 3]], axis =1)\n\ndf_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalization of the data to get the values between 0 and 1 in order to help with  PCA and regression analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the data with Min/Max\n\ndf_norm = df_features\n\nscaler = MinMaxScaler() \n\ndf_norm = scaler.fit_transform(df_norm)\n\ndf_norm = pd.DataFrame(df_norm, columns = df_features.columns)\n\ndf_norm.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a histogram to show the difference (note the x-axis)\n\nplt.hist(df_features['Loudness dB'], bins=10)     #original data\nplt.show()\n\n\nplt.hist(df_norm['Loudness dB'], bins=10)          #standardized data\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA (Principle Component Analysis) to reduce the feature columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the PCA algorithm with our Data\n\npca = PCA().fit(df_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Cumulative Summation of the Explained Variance\n\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Spotify Data Explained Variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the explained variance for each component\n\nexplained_variance = pca.explained_variance_ratio_\n\nprint(explained_variance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how much variance can be explained for 8 components\n\nprint('The explained variance for this many components is:  ',explained_variance[0:8].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visually inspect pca\n\nmap = pd.DataFrame(pca.components_, columns=df_norm.columns)\nplt.figure(figsize=(12,6))\nsns.heatmap(map, cmap='gist_earth_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose number of components\n\npca = PCA(n_components = 8)\n\ndata_pca = pca.fit_transform(df_norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing the features and their interdependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the relationship between all features\n\nscatter_matrix(df_norm)\n\nplt.gcf().set_size_inches(30, 30)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use a spearman correlation to measure the relationship between features\n\npd.set_option('display.width', 100)\npd.set_option('precision', 3)\n\ncorrelation = df_norm.corr(method='spearman')\n\nprint(correlation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap of the correlation to visualize the relationships between features\n\nplt.figure(figsize=(10,10))\nplt.title('Correlation heatmap')\n\nsns.heatmap(correlation, annot = True, vmin=-1, vmax=1, cmap=\"YlGnBu\", center=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing the relationship between Danceablity and Valence\n\nfig = plt.subplots(figsize = (10,10))\n\nsns.regplot(x = 'Valence', y = 'Danceability', data = df_norm, color = 'olive')\n\nsns.kdeplot(df_norm['Valence'], df_norm['Danceability'])\n\nprint('The spearman correlation is:  ',correlation['Danceability']['Valence'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  _**Happier songs are more danceable!**_"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing the relationship between valence vs popularity\n\n\nf, ax = plt.subplots(figsize=(6, 6))\n\ncmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True, start=2.8, rot=.1)\n\nsns.kdeplot(df['Valence'], df['Popularity'], cmap=cmap, n_levels=16, shade=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  _**My hypothesis that popular songs are more depressing was wrong!**_"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing the relationship between valence vs energy\n\nsns.jointplot(x=df['Valence'], y=df['Energy'], data=df, kind=\"kde\", color='lightblue');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  _**Sadder songs do have less energy though!**_"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing the relationship between valence vs length\n\n\nsns.jointplot(df['Valence'], df['Length'], kind=\"hex\", color=\"#4CB391\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing the relationship between loudness vs danceability\n\nsns.catplot(y=\"Danceability\", x=\"Loudness dB\", kind = \"swarm\", data = df_features, palette = 'rocket_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing the spread of popularity throught the years\n\nsns.catplot(y = \"Popularity\", x = \"year\", kind = \"box\", data = df, palette = 'seismic')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  _**The most recent songs are the most popular!  ...this makes me question the Popularity score**_"},{"metadata":{},"cell_type":"markdown","source":"## Analyzing the trends over the years"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PairGrid to analyze trends over the years\n\nsns.set()\n\ng = sns.PairGrid(df, y_vars = ['Beats Per Minute', 'Energy', 'Danceability', 'Loudness dB', \n                               'Liveness', 'Valence', 'Length', 'Acousticness', 'Speechiness', 'Popularity'] , x_vars = ['year'], aspect = 4)\n\ng = g.map(sns.lineplot, color=\"blue\")\n\n# Adjust the tick positions and labels\n\ng.set(xticks=[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])\n\n\n# Adjust the arrangement of the plots\n\ng.fig.subplots_adjust(wspace=.02, hspace=.02);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  _**In general hit songs are getting slower, less energy, way more danceable, a little louder, slighly happier, way shorter, way more acoustic, with less lyrics!**_"},{"metadata":{},"cell_type":"markdown","source":"# Part 3 - Multiple Linear Regression to make a prediction for 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"# insert year cloumn into features df\n\ndf_features.insert(0, 'year', df['year'])\n\n\ndf_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choose the Best Multiple Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup features and target\n\n\nX = df_features[['year']]\ny = data_pca\n\n# can switch variable z to y to see effect of all features on predicition (also change y to z)\nz = df_features[['Beats Per Minute', 'Energy', 'Danceability', 'Loudness dB', 'Liveness', 'Valence', 'Length', 'Acousticness', 'Speechiness', 'Popularity']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train, Test, Split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n\n# Instantiate model\nmlr = LinearRegression()\n\n# Fit Model\nmlr.fit(X_train, y_train)\n\n# Predict\ny_pred = mlr.predict(X_test)\n\n\n# RMSE\nprint('The Root Mean Squared Error for this model is:  ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Fold Cross Val \n\nmlr = LinearRegression()\n\n\nmlr.fit(X, y)\n\n\nmse = cross_val_score(mlr, X, y, scoring='neg_mean_squared_error', cv=10)\n\n\n# fix the sign of MSE scores\nmse_scores = -mse\n\n\n# convert from MSE to RMSE\nrmse_scores = np.sqrt(mse_scores)\n\n\n# calculate the average RMSE\nprint('The Root Mean Squared Error for this model is:  ', rmse_scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression and GridSearchCV\n\nridge = Ridge()\n\nparams = { 'alpha' : [ 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 40, 80, 100, 1000, 10000 ]  }\n\nrr = GridSearchCV(ridge, params, scoring = 'neg_mean_squared_error', cv=10)\n\nrr.fit(X, y)\n\nprint(rr.best_params_)\nprint(rr.best_score_)\n\nrr_mse = -(rr.best_score_)\n\nrr_rmse = np.sqrt(rr_mse)\n\nprint('The Root Mean Squared Error for this model is:  ', rr_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression and GridSearchCV\n\nlasso = Lasso()\n\nparams = { 'alpha' : [ 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 40, 80, 100, 1000, 10000 ]  }\n\nlr = GridSearchCV(lasso, params, scoring = 'neg_mean_squared_error', cv=10)\n\nlr.fit(X, y)\n\nprint(lr.best_params_)\nprint(lr.best_score_)\n\nlr_mse = -(lr.best_score_)\n\nlr_rmse = np.sqrt(lr_mse)\n\nprint('The Root Mean Squared Error for this model is:  ', lr_rmse)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### _**It looks like the K-Fold Cross Validation MLR model is the best!**_"},{"metadata":{},"cell_type":"markdown","source":"## Predicition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict a hit song in 2020's features\n\nhit = mlr.predict([[2020]])\n\nhit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reverse pca\n\nhit = pca.inverse_transform(hit)\n\nhit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reverse normalization\n\nhit = scaler.inverse_transform(hit)\n    \nhit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding a song from our data to serve as an exemplar for the predicted values by using the machine learning technique, K-Nearest Neighbor Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the features of the prediciton into a dataframe\n\nhit = pd.DataFrame(hit)\n\nhit = hit.drop(columns = 9, axis=1)\n\nhit\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction for 2020 using the machine learning classifier KNN\n\n\nknn = KNeighborsClassifier(n_neighbors = 1)\n\nknn.fit(df_features[['Beats Per Minute','Energy','Danceability','Loudness dB','Liveness','Valence', 'Length', 'Acousticness', 'Speechiness']], df_features.index)\n\ny_pred = knn.predict(hit)\n\ny_pred = pd.DataFrame(y_pred)\n\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look up the index\n\nwinner = df.iloc[[388]]\n\nwinner","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  _**There you have it... If you wish to write a hit song in 2020 you can write one just like Close by Nick Jonas !**_  "},{"metadata":{},"cell_type":"markdown","source":"##  _**And again, in 2020 hit songs are getting slower, less energy, way more danceable, a little louder, slightly happier, way shorter, way more acoustic, with less lyrics!**_"},{"metadata":{},"cell_type":"markdown","source":"# Part 4 - Make Playlists based off of the feature characteristics using Machine Learning technique K-Means Clustering"},{"metadata":{},"cell_type":"markdown","source":"### Standardization of the data to fix skew and get mean=0 and std=1 in order to help with clustering\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding out the skew for each feature\n\nskew = df_features.skew()\n\nprint(skew)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale the data\n\nscaler = StandardScaler()\n\ndf_scaled = scaler.fit_transform(df_features)\n\ndf_scaled = pd.DataFrame(df_scaled)\n\ndf_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot to show the difference\n\nplt.hist(df_features['Speechiness'], bins=10)                    #original data\nplt.show()\n\nplt.hist(df_scaled.iloc[8], bins=10)                            #standardized data\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find the appropriate amount of clusters\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose the best number of clusters using elbow method and inertia\n\nk = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\ninertias = []\n\nfor i in k:\n    km = KMeans(n_clusters=i, max_iter=1000, random_state=42)\n    km.fit(df_scaled)\n    inertias.append(km.inertia_)\n\nplt.plot(k, inertias)\nplt.xlabel(\"Value for k\")\nplt.ylabel(\"Inertias\")\nplt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\nscore=[]\n\nfor n_cluster in k:\n    kmeans = KMeans(n_clusters=n_cluster).fit(df_scaled)\n    silhouette_avg = silhouette_score(df_scaled, kmeans.labels_)\n    score.append(silhouette_score(df_scaled, kmeans.labels_))\n    \n    print('Silhouette Score for %i Clusters: %0.4f' % (n_cluster, silhouette_avg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot cluster options\n\nplt.plot(k, score, 'o-')\nplt.xlabel(\"Value for k\")\nplt.ylabel(\"Silhouette score\")\nplt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set number of clusters\n\nkclusters = 8\n\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, init='k-means++', random_state=42).fit(df_scaled)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add clustering labels to dataframe\n\ndf.insert(0, 'Playlist Number', kmeans.labels_)\n\ndf.head()    # check out the Cluster Labels column!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Playlist #1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 0, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Playlist #2"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 1, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Playlist #3"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 2, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Playlist #4"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 3, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Playlist #5"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 4, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Playlist #6"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 5, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Playlist #7"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 6, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Playlist #8"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Playlist Number'] == 7, df.columns[[1, 2]]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now all we have to do is name these playlists... I'll leave that up to you!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}