{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df =pd.read_csv(\"/kaggle/input/bank-loan-classification/UniversalBank.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls=df.isnull().sum()\nnulls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls_percentage = nulls[nulls!=0]/df.shape[0]*100\nprint('the percentages of null values per feature:\\n')\nprint(round(nulls_percentage,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is it have no null features so it printed nothing,otherwise it would have printed the percentage of null values per feature.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now deleting the columns which are not required","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['ID']\ndel df[\"ZIP Code\"]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('CreditCard').CreditCard.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It is an imbalanced dataset will see if accuracy decreases.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={\"Personal Loan\": \"Personal_Loan\",\"Securities Account\":\"Sec_Acc\",\"CD Account\":\"CD_Acc\"},inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Personal_Loan'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#it is also imbalanced we have to balance the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reordering Columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df=df[['Age','Experience','Income',\"Family\",\"CCAvg\",\"Education\",\"Mortgage\",\"Sec_Acc\",\"CD_Acc\",\"Online\",\"CreditCard\",\"Personal_Loan\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df,x=\"Personal_Loan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df,x=\"CreditCard\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install bubbly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from bubbly.bubbly import bubbleplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# figure = bubbleplot(dataset = df, x_column = 'Experience', y_column = 'Income', \n#     bubble_column = 'Personal_Loan', time_column = 'Age', size_column = 'Mortgage', color_column = 'Personal_Loan', \n#     x_title = \"Experience\", y_title = \"Income\", title = 'Experience vs Income. vs Age vs Mortgage vs Personal Loan',\n#     x_logscale = False, scale_bubble = 3, height = 650)\n\n# plt.iplot(figure, config={'scrollzoom': True})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 5)\nsns.distplot(df['Age'], color = 'cyan')\nplt.title('Distribution of Age', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age is well managed","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 5)\nsns.distplot(df['Income'], color = 'red',norm_hist=True)\nplt.title('Distribution of Income', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So income is not well managed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 5)\nsns.distplot(df['Experience'], color = 'green')\nplt.title('Distribution of Experience', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So Experience is welll balanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)\nsns.boxplot(df['Personal_Loan'], df['Income'], palette = 'viridis')\nplt.title('Relation of Income with Personal Loan', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURE SELECTION"},{"metadata":{},"cell_type":"markdown","source":"## CHECKING FOR CORRELATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20, 10)\nplt.style.use('ggplot')\nsns.heatmap(df.corr(), annot = True, cmap = 'Wistia')\nplt.title('Heatmap for the Dataset', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=df.corr()\ncorr[\"Personal_Loan\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df.drop('Personal_Loan',axis=1)\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df['Personal_Loan']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Extra Trees Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using ANOVA TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_classif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select=f_classif(x,y)\nselect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_values=pd.Series(select[1])\np_values.index=x.columns\np_values.sort_values(ascending=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_values=p_values[p_values<0.05]\np_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From all the three feature selection techniques we have got the top features i.e Income,CCAvg,CC_Acc,Mortgage,Education","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hence droping less coreleated features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Online','CreditCard'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Personal_Loan'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It seems to be a imbalanced dataset still trying different classification algorithms.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAIN TEST AND SPLIT"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.20,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.bincount(Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL FITTING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install imblearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOGISTIC REGRESSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr1 = LogisticRegression()\n\nlr1.fit(X_train,Y_train)\n\nY_pred_lr1 = lr1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lr1 = round(accuracy_score(Y_pred_lr1,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr1)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(Y_test, Y_pred_lr1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_lr1,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_lr1)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRYING K FOLD CROSS VALIDATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=cross_val_score(lr1,x,y,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So by k fold cross validation we get that our model's accuracy will lie from 93.4% to 96.6% with a mean of 94.7%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No we have to oversample the data by using SMOTE ALgorithm as our True negatives are less.Here you can also use Nearmiss(undersampling) and check for the accuracy.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smt=SMOTE()\nx_new,y_new= smt.fit_sample(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(x_new,y_new,test_size=0.20,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train,Y_train)\n\nY_pred_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.bincount(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(Y_test, Y_pred_lr) \nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_lr,Y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **HYPER PARAMETER TUNING FOR LOGISTIC REGRESSION MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=30)\nlogreg_cv.fit(X_train,Y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg2=LogisticRegression(C=1000,penalty=\"l2\")\nlogreg2.fit(X_train,Y_train)\nprint(\"score\",logreg2.score(X_test,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_lr_2 = logreg2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lr = round(accuracy_score(Y_pred_lr_2,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_lr)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now our true negatives are also more so it now predicting well.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NAIVE BAYES"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(X_train,Y_train)\n\nY_pred_nb = nb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_nb = round(accuracy_score(Y_pred_nb,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Naive Bayes is: \"+str(score_nb)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(Y_test, Y_pred_nb) \nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_nb,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_nb)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K NEAREST NEIGHBORS "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train,Y_train)\nY_pred_knn=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using KNN is: \"+str(score_knn)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(Y_test, Y_pred_knn) \nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_knn,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_knn)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(200):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train,Y_train)\n    Y_pred_dt = dt.predict(X_test)\n    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \n\n\ndt = DecisionTreeClassifier(random_state=best_x)\ndt.fit(X_train,Y_train)\nY_pred_dt = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = confusion_matrix(Y_test, Y_pred_dt) \nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_dt,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_dt)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_rf=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Random Forest is: \"+str(score_rf)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(Y_test,Y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_rf,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_rf)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train, Y_train)\n\nY_pred_xgb = xgb_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_xgb = round(accuracy_score(Y_pred_xgb,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(Y_test,Y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_xgb,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test,Y_pred_xgb)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (5, 5)\nplt.title('ROC curve for classifier', fontweight = 30)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = auc(fpr, tpr)\nprint(\"AUC Score :\", auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NEURAL NETWORK"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(11,activation='relu',input_dim=11))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,Y_train,epochs=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_nn = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_nn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rounded = [round(x[0]) for x in Y_pred_nn]\n\nY_pred_nn = rounded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_nn = round(accuracy_score(Y_pred_nn,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Neural Network is: \"+str(score_nn)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(Y_test,Y_pred_nn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y_pred_nn,Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = [score_lr,score_nb,score_knn,score_dt,score_rf,score_xgb,score_nn]\n\nalgorithms = [\"Logistic Regression\",\"Naive Bayes\",\"K-Nearest Neighbors\",\"Decision Tree\",\"Random Forest\",\"XGBoost\",\"Neural Network\"]    \n\nfor i in range(len(algorithms)):\n    print(\"The accuracy score achieved using \"+algorithms[i]+\" is: \"+str(scores[i])+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=algorithms,y=scores)\nplt.rcParams['figure.figsize'] = (15, 7)\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict \ndict = {'Logistic Regression':score_lr,'Naive Bayes':score_nb,'K Nearest Neighbors':score_knn,'Decision Tree':score_dt,'Random Forest':score_rf,'XGBoost':score_xgb,'Neural Network':score_nn} \ndict1 = OrderedDict(sorted(dict.items())) \nprint(dict1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SO XGBOOST MODEL GAVE US THE HIGHEST ACCURACY.."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}