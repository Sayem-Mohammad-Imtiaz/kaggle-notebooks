{"cells":[{"metadata":{},"cell_type":"markdown","source":"# UrbanSound8K Challenge at Kaggle\n**Challenge:** Classification problem of 10 different ambient noise\n8732 audio files split into 10 folds by the author. Author specifically requests 10-fold cross-validation to be performed. \n\n**Summary of Progress:**\n1.\tInput data is Mel-frequency cepstrum coefficients of the audio data, produced by librosa package.\n2.\tTensorflow and Keras packages are used to construct, compile, train and test model.\n3.\tLatest model layer construction: 2 2D convolutional layers both with L2 regularizer and RelU activation, maxpooling by 2X2 kernel and 0.5 dropout after every conv layer, 1 fully connected layer before final classification layer with softmax activation.\n4.\tSingle split performed (1:9 training-validation ratio). No 10-fold cross-validation performed yet.\n\n**Current Findings:**\n1.\tValidation accuracy of around 60% reached, where it fluctuates around 5% at each epoch. Time taken to obtain mfcc is 9 minutes. \n2.\tChanging n_mfcc parameter fin librosa didnâ€™t significantly lead to improvement in accuracy\n3.\tThe use of SGD optimizer instead of adam led to a decrease in accuracy.\n\n**Next Steps:**\n1.\tUse of deep ensemble model to boost accuracy as suggested by Ricky\n2.  Use of Tensorflow to calculate MFCC, exploting GPU acceleration\n\n","execution_count":null},{"metadata":{"_uuid":"03df8787-e052-4a6b-8527-2e81d5509067","_cell_guid":"d4895fce-14df-4545-8c17-2aa5fe8a7935","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10e9821d-d34b-43d9-b535-3f2d4f2288f4","_cell_guid":"b3f44fd7-3979-4ebc-b3cb-22d10a7b328e","trusted":true},"cell_type":"code","source":"pip install librosa","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5f8236a-7e49-4673-bd5a-cff6f38fedfc","_cell_guid":"8c4ebf4f-77c4-4c00-adb0-c40adfcd8dae","trusted":true},"cell_type":"code","source":"# Load Imports\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport torch\nimport tensorflow as tf\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical, np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom keras.optimizers import Adam\nfrom sklearn import metrics\nfrom scipy.io import wavfile","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed1f7fd3-6558-407e-9502-4f0e6b342683","_cell_guid":"d10b10fa-065e-4bcb-8298-d23c3a92ded9","trusted":true},"cell_type":"code","source":"def feature_extraction(data):\n    #sample rate conversion, bit depth and audio channel modification\n    try:\n        audio, sample_rate = librosa.load(data, res_type = 'kaiser_fast')\n        max_pad = 174\n        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=64, n_mels = 40)\n        pad_width = max_pad - mfccs.shape[1]\n        mfccspadded = np.pad(mfccs, pad_width = ((0,0),(0, pad_width)), mode='constant')\n    except Exception as e:\n        print(\"Error encountered while parsing file: \",e)\n        return None\n    return mfccspadded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_extraction1(data):\n    audio_binary = tf.io.read_file(data)\n    sampling_rate = 44100\n    audio, sample_rate = tf.audio.decode_wav(audio_binary, desired_channels = 1)\n    #sample_rate, audio = wavfile.read(data)\n    signals = tf.cast(tf.reshape(audio, [1, -1]), tf.float32)\n    stfts = tf.signal.stft(signals, frame_length = 1024, frame_step=512, fft_length=1024, pad_end = True)\n    magnitude_spectrograms = tf.abs(stfts)\n    num_spectrogram_bins = magnitude_spectrograms.shape[-1]\n    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 20, 20000, 64\n    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n                                                                        num_mel_bins, \n                                                                        num_spectrogram_bins, \n                                                                        sample_rate, \n                                                                        lower_edge_hertz,\n                                                                        upper_edge_hertz\n                                                                        )\n    mel_spectrograms = tf.tensordot(magnitude_spectrograms, linear_to_mel_weight_matrix, 1)\n    log_offset = 1e-6\n    \n    log_mel_spectrograms = tf.math.log(mel_spectrograms + log_offset)\n    num_mfccs = 30\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(\n    log_mel_spectrograms)[..., :num_mfccs]\n    return mfccs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"807ac5eb-fed6-42d7-b80e-91a136d364ba","_cell_guid":"d6adf61f-63aa-4325-82dd-021f27ae7a08","trusted":true},"cell_type":"code","source":"datasetpath = \"../input/urbansound8k\"\nmetadata = pd.read_csv(datasetpath + \"/UrbanSound8K.csv\")\nprint(metadata.head())\nprint(metadata['class'].value_counts())\nprint(metadata['classID'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"560b07ce-7502-47ce-8904-79a564dc2e03","_cell_guid":"51b5d3b6-bfc8-451f-bddb-a1bc796f67a3","trusted":true},"cell_type":"markdown","source":"Next, we extract the class and the data in the form of dataframes. Data needs preprocessing in terms of audio channels, sample rate and bit depth. Mel-frequency cepstrum coefficients are also calculated.","execution_count":null},{"metadata":{"_uuid":"bac6668a-9b20-468b-aaa2-9cb46b268362","_cell_guid":"c0a1b956-b98b-46d0-b470-fd2576fa7453","trusted":true},"cell_type":"code","source":"\naudiodata = []\nfor index, row in metadata.iterrows():\n    file_name = os.path.join(os.path.abspath(datasetpath), 'fold' + str(row['fold']) + '/', str(row['slice_file_name']))\n    class_id = row['classID']\n    fold = row['fold']\n    audio = feature_extraction(file_name)\n    audiodata.append([audio, class_id, fold])\nfeatures = pd.DataFrame(audiodata, columns=['feature', 'class_label', 'fold'])\nprint('Finished extraction from', len(features), 'files')\n\n#encoder for classification labels into model-understandable numerical data (onehotencoder)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afterwards, we split the dataset into training and testing sets. The documentation proposed splitting the testing set size to be 20%. However, since the data is already divided into folds, we can implement 10-fold crossvalidation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loader(fold):    \n    traindf = features[features['fold'] != fold]\n    testdf = features[features['fold'] == fold]\n    \n    train_x = np.array(traindf.feature.tolist())\n    train_x = train_x.reshape(len(traindf), 80, -1, 1)\n    train_x = tf.convert_to_tensor(train_x)\n\n    train_y = torch.tensor(traindf['class_label'].values)\n    le = LabelEncoder()\n    train_y = torch.tensor(to_categorical(le.fit_transform(train_y)))\n    train_y = train_y.numpy()\n    train_y = tf.convert_to_tensor(train_y)\n#train_data = TensorDataset(train_x, train_y)\n#train = DataLoader(train_data, batch_size = BATCH_S, shuffle = True)\n\n    print(train_x.shape)\n    print(train_y.shape)\n\n    test_x = np.array(testdf.feature.tolist())\n    test_x = test_x.reshape(len(testdf), 80, -1, 1)\n    test_x = tf.convert_to_tensor(test_x)\n    \n    test_y = torch.tensor(testdf['class_label'].values)\n    le = LabelEncoder()\n    test_y = torch.tensor(to_categorical(le.fit_transform(test_y))) \n    test_y = test_y.numpy()\n    test_y = tf.convert_to_tensor(test_y)\n\n    print(test_x.shape)\n    print(test_y.shape)\n    \n    return train_x, train_y, test_x, test_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda:0')\nelse:\n    device = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvNet():\n    model = Sequential()\n\n    model.add(Conv2D(12, (3, 3), padding = \"same\", activation = 'relu', input_shape = (80, 87, 1)))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    model.add(Conv2D(12, (3, 3), padding = \"same\", activation = 'relu'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    model.add(Conv2D(12, (3, 3), padding = \"same\", activation = 'relu'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    model.add(Conv2D(64, (3, 3), padding = \"same\", activation = 'relu'))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    model.add(Conv2D(64, (3, 3), padding = \"same\", activation = 'relu'))\n    model.add(MaxPool2D(pool_size = (2,2)))\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(Dense(1024, activation = \"relu\"),)\n    model.add(Dense(10, activation = 'softmax'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_S = 5\nMAX_EPOCHS = 20\nopt = tf.keras.optimizers.SGD(\n    learning_rate=0.01, momentum=0.5, nesterov=False, name='SGD')\nacc = []\nfor fold in range(1, 11):\n    train_x, train_y, test_x, test_y = loader(fold)\n    model = ConvNet()\n    model.compile(optimizer = opt, loss ='categorical_crossentropy', metrics = ['accuracy'])\n\n    model.fit(train_x, train_y, epochs = MAX_EPOCHS, batch_size = BATCH_S, validation_data = (test_x, test_y));\n    predictions = model.predict(test_x);\n    score = model.evaluate(test_x, test_y)\n    print(score)\n    acc.append(score[1])\n\nprint(\"10-fold crossvalidation accuracy score: {}\".format(np.mean(acc)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.argmax(predictions, axis = 1)\nresult = pd.DataFrame(preds)\nresult.to_csv(\"UrbanSound8kResults.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}