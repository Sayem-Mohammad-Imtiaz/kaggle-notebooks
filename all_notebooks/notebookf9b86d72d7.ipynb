{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context = 'notebook', palette='deep')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/titanic-machine-learning-from-disaster/train.csv')\ntest = pd.read_csv('../input/titanic-machine-learning-from-disaster/test.csv')\nIDtest = test['PassengerId']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.fillna(np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Fare'] = dataset['Fare'].map(lambda i : np.log(i) if i > 0 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Embarked'] = dataset['Embarked'].fillna('S')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_NaN_age = list(dataset['Age'][dataset['Age'].isnull()].index)\n\nfor i in index_NaN_age:\n    age_med = dataset['Age'].median()\n    age_pred = dataset['Age'][((dataset['SibSp'] == dataset.iloc[i]['SibSp']) & (dataset['Parch'] == dataset.iloc[i]['Parch']) & (dataset['Pclass'] == dataset.iloc[i]['Pclass']))].median()\n    if not np.isnan(age_pred) : \n        dataset['Age'].iloc[i] = age_pred\n    else:\n        dataset['Age'].iloc[i] = age_med","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.drop(labels = ['Name'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Fsize'] = dataset['SibSp']+dataset['Parch'] +1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.get_dummies(dataset, columns = ['Title'])\ndataset = pd.get_dummies(dataset, columns = ['Embarked'], prefix='Em')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ticket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.get_dummies(dataset, columns = ['Ticket'], prefix = 'T')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Pclass'] = dataset['Pclass'].astype('category')\ndataset = pd.get_dummies(dataset, columns = ['Pclass'], prefix = 'Pc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.drop(labels = ['PassengerId'], axis = 1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Survived\"] = train[\"Survived\"].astype(int)\nY_train = train[\"Survived\"]\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y=Y_train, scoring = 'accuracy', cv = kfold, n_jobs = 4))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\", 'Algorithm', data=cv_res, palette='Set3', orient='h', **{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC, param_grid=ada_param_grid, cv=kfold, scoring = 'accuracy', n_jobs = 4, verbose = 1)\n\ngsadaDTC.fit(X_train, Y_train)\n\nada_best = gsadaDTC.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gsadaDTC.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ExtC = ExtraTreesClassifier()\n\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\ngsExtC.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFC = RandomForestClassifier()\n\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\ngsRFC.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GBC = GradientBoostingClassifier()\n\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\ngsGBC.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVMC = SVC(probability=True)\n\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\ngsSVMC.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}