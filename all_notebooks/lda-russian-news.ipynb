{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pymorphy2[fast]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom gensim import corpora, models\nimport pymorphy2\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"morph = pymorphy2.MorphAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(token):\n    return morph.parse(token)[0].normal_form","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df = pd.read_csv('/kaggle/input/russian-news-2020/news.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.loc[news_df['source'] == 'ria.ru', 'publication_date'] = (news_df.loc[news_df['source'] == 'ria.ru', 'publication_date'].str\n                                                              .extract(r'(?P<date>\\d{2}\\.\\d{2}\\.\\d{4})', expand=False)\n                                                              .apply(lambda x: '-'.join(reversed(x.split('.'))) if type(x) is str else x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.loc[news_df['source'] == 'lenta.ru', 'publication_date'] = news_df.loc[news_df['source'] == 'lenta.ru', 'publication_date'].str.split('T').str.get(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_mapper = {\n    'января': '01',\n    'февраля': '02',\n    'марта': '03',\n    'апреля': '04',\n    'мая': '05',\n    'июня': '06',\n    'июля': '07',\n    'августа': '08',\n    'сентября': '09',\n    'октября': '10',\n    'ноября': '11',\n    'декабря': '12'\n}\nnews_df.loc[news_df['source'] == 'meduza.io', 'publication_date'] = (news_df.loc[news_df['source'] == 'meduza.io', 'publication_date']\n                                                                     .apply(lambda x: f'{x.split()[3]}-{month_mapper[x.split()[2]]}-{x.split()[1].zfill(2)}' if type(x) is str else x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.loc[news_df['source'] == 'tjournal.ru', 'publication_date'] = pd.to_datetime(news_df.loc[news_df['source'] == 'tjournal.ru', 'publication_date'], unit='s').dt.strftime('%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.loc[news_df['source'] == 'tjournal.ru', 'text'] = news_df.loc[news_df['source'] == 'tjournal.ru', 'text'].str.replace('\\n', '').str.replace(r'\\s+', ' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.loc[news_df['source'] == 'tjournal.ru', 'tags'] = news_df.loc[news_df['source'] == 'tjournal.ru', 'text'].str.findall(r'#\\w+').str.join(', ').str.replace('#', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df.loc[news_df['source'] == 'tjournal.ru', 'text'] = news_df.loc[news_df['source'] == 'tjournal.ru', 'text'].apply(lambda x: x[:x.find('#')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents = news_df.text.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing\nSplit the text into tokens, bring the tokens to normal form and take only tokens longer than two characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = [\n    [lemmatize(word) for word in re.findall(r'\\w+', document.lower()) if len(word) > 2]\n    for document in documents\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a dictionary of words from our texts. Let's leave only words that occur at least 5 times and no more than 25% of documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = corpora.Dictionary(texts)\ndictionary.filter_extremes(no_below=5, no_above=0.25, keep_n=25000)\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training model\n![topic_modeling](https://miro.medium.com/max/1200/1*IJw8N-HSEzLpwJDS6JVs-w.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel = models.ldamulticore.LdaMulticore(corpus, id2word=dictionary, num_topics=100, passes=50, alpha='symmetric', eta=None, decay=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perplexity\n\n\n![perplexity](https://wikimedia.org/api/rest_v1/media/math/render/svg/fc7974a9bf394db8698fb76c0fa060c6c21068ed)"},{"metadata":{"trusted":true},"cell_type":"code","source":"perplexity = ldamodel.log_perplexity(corpus)\nprint(2**(-perplexity))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of the resulting topics\nLet's see the resulting topics and their most frequent words."},{"metadata":{"trusted":true},"cell_type":"code","source":"for t, top_words in ldamodel.print_topics(num_topics=-1, num_words=10):\n    print(\"Topic\", t, \":\", top_words)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the distribution of rubrics, subrubrics and tags by topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df['topic'] = [max(i, key=lambda x: x[1])[0] for i in ldamodel[corpus]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    counts = news_df[news_df.topic == i].rubric.value_counts()\n    print(counts[counts > 5])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    counts = news_df[news_df.topic == i].subrubric.value_counts()\n    print(counts[counts > 5])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    tags = []\n    for i in news_df[news_df.topic == i].tags.dropna():\n        tags += i.split(', ')\n    counts = Counter(tags)\n    print('\\n'.join(map(str, counts.most_common()[:5])))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wordcloud\n\n\nVisualizing each topic with a word cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    frequencies = dict(ldamodel.show_topic(i, topn=100))\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate_from_frequencies(frequencies)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of topics over time"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure()\nf, ax = plt.subplots(100, 1, figsize=(75, 900))\n\nfor i, topic_name in enumerate(range(news_df.topic.max())):\n    counts = news_df[news_df.topic == topic_name]['publication_date'].dropna().value_counts().to_dict()\n    ax[i].bar(news_df['publication_date'].dropna().drop_duplicates().sort_values(), news_df['publication_date'].dropna().drop_duplicates().sort_values().map(counts))\n    ax[i].set_title(topic_name)\n    ax[i].tick_params(labelrotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}