{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><font size=\"6\">Chinese MNIST Classification</font></h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis notebook challenges the task of image classification using an interesting datasets published by [Gabriel Preda].\n\nWe will proceed with a simple approach without using image data augmentation or pretrained models.\n\n[Gabriel Preda]: https://www.kaggle.com/gpreda","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Configurations and read metadata.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nEPOCHS = 50\nBATCH_SIZE = 32\nIMG_SIZE = 64\nIMG_ROOT = '../input/chinese-mnist/data/data/'\n\ntrain_df = pd.read_csv('../input/chinese-mnist/chinese_mnist.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking metadata.\nCheck for missing values and the balance of the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['character'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_file_name(x):\n    file_name = f'input_{x[0]}_{x[1]}_{x[2]}.jpg'\n    return file_name\n\n\ndef add_filenames(df, img_root):\n    filenames = list(os.listdir(img_root))\n    df['filenames'] = df.apply(create_file_name, axis=1)\n    return df \n    \ntrain_df = add_filenames(train_df, IMG_ROOT)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(train_df, \n                                     test_size=0.2,\n                                     random_state=SEED,\n                                     stratify=train_df['character'].values) \ntrain_df, val_df = train_test_split(train_df,\n                                    test_size=0.1,\n                                    random_state=SEED,\n                                    stratify=train_df['character'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_datasets(df, img_root, img_size, n):\n    imgs = []\n    for filename in tqdm(df['filenames']):\n        img = io.imread(img_root+filename)\n        img = transform.resize(img, (img_size,img_size,n))\n        imgs.append(img)\n        \n    imgs = np.array(imgs)\n    df = pd.get_dummies(df['character'])\n    return imgs, df\n\n\ntrain_imgs, train_df = create_datasets(train_df, IMG_ROOT, IMG_SIZE, 1)\nval_imgs, val_df = create_datasets(val_df, IMG_ROOT, IMG_SIZE, 1)\ntest_imgs, test_df = create_datasets(test_df, IMG_ROOT, IMG_SIZE, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model (1)\nFirst, build a simple neural network and try it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (IMG_SIZE, IMG_SIZE, 1)\n\nmodel = Sequential()\nmodel.add(Conv2D(16, kernel_size=3, padding='same', input_shape=input_shape, activation='relu'))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPool2D(3))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(15, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es_callback = tf.keras.callbacks.EarlyStopping(patience=20, \n                                               verbose=1, \n                                               restore_best_weights=True)\n\n\nhistory = model.fit(train_imgs, \n                    train_df, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=[es_callback],\n                    validation_data=(val_imgs, val_df))\n\npd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot()\npd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_imgs, test_df) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model (2)\nMake some changes and try again.\n1. Add some layers.\n2. Change the optimization function.\n3. Apply label smoothing.\n4. Add a learning rate scheduler.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(16, kernel_size=3, padding='same', input_shape=input_shape, activation='relu'))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(3))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(Conv2D(16, kernel_size=3, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(3))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(15, activation='softmax'))\nopt = tfa.optimizers.LazyAdam()\nloss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.025)\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr_callback(batch_size=32, plot=False):\n    lr_start   = 0.003\n    lr_max     = 0.00125 * batch_size\n    lr_min     = 0.001\n    lr_ramp_ep = 20\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    if plot == True:\n        rng = [i for i in range(EPOCHS)]\n        y = [lrfn(x) for x in rng]\n        plt.plot(rng, y)\n        plt.xlabel('epoch', size=14); plt.ylabel('learning_rate', size=14)\n        plt.title('Training Schedule', size=16)\n        plt.show()\n    return lr_callback\n\nget_lr_callback(plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es_callback = tf.keras.callbacks.EarlyStopping(patience=20, \n                                               verbose=1, \n                                               restore_best_weights=True)\n\n\nhistory = model.fit(train_imgs, \n                    train_df, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=[es_callback, get_lr_callback(BATCH_SIZE)],\n                    validation_data=(val_imgs, val_df))\n\npd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot()\npd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_imgs, test_df) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}