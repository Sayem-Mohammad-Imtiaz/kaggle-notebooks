{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ## Deep Learning for Text and Sequence Notes\n\n*This is my notebook to record important terms, codes, and concepts of applying deep learning to work with text data.*\n\n**Reference**: *Francois, C. (2017). Deep learning with Python.*\n\n#### The fundamental algorithms: \n- RNN\n- Conv1D\n\nVectorizing text is the process of transform text into numeric tensors.\n\nTokens: words, characters, n-grams: are the units that text is broken down to.\n\nTokenization: breaking text into tokens.\n\n#### Methods of associating a vector with a token:\n- One-hot encoding\n- Word embedding\n\nN-grams: groups of N consecutive words extracted from a sentence. Extracting N-grams is a form of feature engineering. Useful for shallow language-processing rather than in deep learning.\n\n#### One-hot encoding:\n\nThe vector is all zeros, except the n-th entry (encoded as 1).\n\nOne-hot hashing trick: used when the number of unique tokens in the vocabulary is too large to handle explicitly.\n\n- Binary\n- Sparse\n- High-dimensional","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# word-level one-hot encoding\n\nfrom keras.preprocessing.text import Tokenizer \n\nsamples = [\"I love learning deep learning.\",\n           \"Machine learning is the future of humanity.\"]\n\ntokenizer = Tokenizer(num_words=1000) # only tokenize 1000 common words\ntokenizer.fit_on_texts(samples)\n\nsequences = tokenizer.texts_to_sequences(samples)\n\none_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n\nword_index = tokenizer.word_index\nprint(\"Found %s unique tokens.\" % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word embeddings\n\n- Low-dimensional floating-point vectors (dense vectors)\n- Learned from data.\n- Pack more information into far fewer dimensions.\n\nWords meaning different things are embedded at points far away from each other.\n\nThe Embedding layer ~ a dictionary that maps integer indices to dense vectors.\n\nAll sequences in a batch must have the same length. Shorter sequence is **padded with zeros**, longer sequence is **truncated**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(1000, 64) \n# max 1000 tokens/sequences, 64 dimensions/length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMDB movie-review sentiment-prediction\n\nfrom keras.datasets import imdb\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\n\nmax_features = 10000\nmaxlen = 20\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(10000, 8, input_length=maxlen))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n\nhistory = model.fit(x_train, y_train,\n                   epochs=10,\n                   batch_size=32,\n                   validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using GloVe embeddings (https://nlp.stanford.edu/projects/glove)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Recurrent Neural Networks\n\nDensely connected networks, convnet = feedforward networks.\n\nRNN:\n- Processes information incrementally while maintaining an internal model of what it's processing.\n- Built from past infor and constantly updated as new infor comes in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# RNN in Keras\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32, return_sequences=True))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\nmax_features = 10000\nmaxlen = 500\nbatch_size = 32\n\nprint(\"Loading data...\")\n(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(input_train), \"train sequences\")\nprint(len(input_test), \"test sequences\")\n\n\nprint(\"Pad sequences (samples x time)\")\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint(\"input_train_shape: \", input_train.shape)\nprint(\"input_test_shape: \", input_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n\nhistory = model.fit(input_train, y_train,\n                   epochs = 10,\n                   batch_size = 128,\n                   validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \"b\")\nplt.plot(epochs, val_acc, \"bo\")\nplt.title(\"Training and validation accuracy.\")\nplt.show()\n\nplt.plot(epochs, loss, \"r\")\nplt.plot(epochs, val_loss, \"ro\")\nplt.title(\"Training and validation loss.\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM and GRU layers \n\nSimpleRNN is too simple. Vanishing gradients problem.\n\nLSTM and GRU can solve the problem.\n\nLSTM saves information for later (allows past information to be reinjected at a later time), thus preventing older signals from gradually vanishing.\n\nLSTM solves difficult NLP problems: Q&A, machine translation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n\nhistory = model.fit(input_train, y_train,\n                   epochs=10,\n                   batch_size=128,\n                   validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recurrent dropout: prevents overfitting in recurrent layers.\n\nStacking recurrent layers; increases the representational power of the network.\n\nBidirectional recurrent layers: increases accuracy and mitigates forgetting issues.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = \"../input/weather-archive-jena/jena_climate_2009_2016.csv\"\nf = open(fname)\n\ndata = f.read()\nf.close()\n\nlines = data.split(\"\\n\")\nheader = lines[0].split(\",\")\nlines = lines[1:]\n\nprint(header)\nprint(len(lines))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert into a NumPy array\n\nimport numpy as np\n\nfloat_data = np.zeros((len(lines), len(header) - 1))\nfor i, line in enumerate(lines):\n    values = [float(x) for x in line.split(\",\")[1:]]\n    float_data[i, :] = values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = float_data[:, 1]\nplt.plot(range(len(temp)), temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(1440), temp[:1440])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to be updated","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}