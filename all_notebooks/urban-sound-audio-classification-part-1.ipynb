{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Overview\n\n- Objective : \n  - **To classifiy urban sound**\n  - used **librosa** library for music and audio analysis\n- Classification Problem\n- Data cleaning\n- Exploratory Data Analysis\n- Data Preprocessing : Feature Extraction using MFCC\n- **Artificial Neural Network (ANN)** Training and Prediction","metadata":{}},{"cell_type":"markdown","source":"# About Project\n\nAutomatic environmental sound classification is a growing area of research with numerous real world applications. Whilst there is a large body of research in related audio fields such as speech and music, work on the classification of environmental sounds is comparatively scarce.\n\nThere is a plethora of real world applications for this research, such as:\n\n• Content-based multimedia indexing and retrieval\n• Assisting deaf individuals in their daily activities\n• Smart home use cases such as 360-degree safety and security capabilities\n• Industrial uses such as predictive maintenance","metadata":{}},{"cell_type":"markdown","source":"![](https://cdn.lucidsamples.com/c/15-category_default/sound-effects-packs.jpg)","metadata":{}},{"cell_type":"markdown","source":"## Code and Resources used\n\n- Python version: 3.7.6\n- Packages: Pandas, Numpy, Seaborn, Matplotlib, Scikit, Keras, Tensorflow, Librosa, Ipython\n- Resources used:\n\n  * Medium : https://mikesmales.medium.com/sound-classification-using-deep-learning-8bc2aa1990b7\n  * Heartbeat : https://heartbeat.fritz.ai/working-with-audio-signals-in-python-6c2bd63b2daf\n","metadata":{}},{"cell_type":"markdown","source":"# Web Scraping\n\nDataset URL: https://urbansounddataset.weebly.com/urbansound8k.html\n\nFor this we will use a dataset called Urbansound8K. The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, which are:\n\n• Air Conditioner\n• Car Horn\n• Children Playing\n• Dog bark\n• Drilling\n• Engine Idling\n• Gun Shot\n• Jackhammer\n• Siren\n• Street Music\n","metadata":{}},{"cell_type":"markdown","source":"# Audio file overview\n\nSound are pressure waves, and these waves can be represented by numbers over a time period. These air pressure differences communicates with the brain.\n\nThese sound excerpts are digital audio files in .wav format. Sound waves are digitised by sampling them at discrete intervals known as the sampling rate (typically 44.1kHz for CD quality audio meaning samples are taken 44,100 times per second).\n\nEach sample is the amplitude of the wave at a particular time interval, where the bit depth determines how detailed the sample will be also known as the dynamic range of the signal (typically 16bit which means a sample can range from 65,536 amplitude values).","metadata":{}},{"cell_type":"markdown","source":"## Sampling Frequency\nThe sampling frequency (or sample rate) is the number of samples (data points) per second in a sound. For example: if the sampling frequency is 44 khz, a recording with a duration of 60 seconds will contain 2,646,000 samples. In practice, sampling even higher than 10x helps measure the amplitude correctly in the time domain.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"# important packages\n\t\nimport pandas as pd\t\t\t\t\t# data manipulation using dataframes\nimport numpy as np\t\t\t\t\t# data statistical analysis\n\nimport seaborn as sns\t\t\t\t# Statistical data visualization\nimport matplotlib.pyplot as plt\t\t# data visualisation\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Librosa library\nLibrosa is a Python package for music and audio processing by Brian McFee and will allow us to load audio in our notebook as a numpy array for analysis and manipulation.\n\nFor much of the preprocessing we will be able to use Librosa’s load() function, which by default converts the sampling rate to 22.05 KHz, normalise the data so the bit-depth values range between -1 and 1 and flattens the audio channels into mono.","metadata":{}},{"cell_type":"code","source":"import librosa\t\t\t\t\t\t\t# package for music and audio analysis\nimport librosa.display","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython.display as ipd\t\t\t# public api for display tool in ipython","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Visualizing an audio file","metadata":{}},{"cell_type":"code","source":"audio_file = \"../input/urbansound8k/fold3/102105-3-0-0.wav\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading file","metadata":{}},{"cell_type":"code","source":"data,sample_rate = librosa.load(audio_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Playing audio","metadata":{}},{"cell_type":"code","source":"ipd.Audio(audio_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is dog sound. Let us visualize in waveform using librosa library","metadata":{}},{"cell_type":"markdown","source":"## Waveform visualization\ndepicts the waveform visualization of the amplitude vs the time representation of the signal.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,5))\nlibrosa.display.waveplot(data, sr = sample_rate)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spectogram :  \nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. They are time-frequency portraits of signals. Using a spectrogram, we can see how energy levels (dB) vary over time.","metadata":{}},{"cell_type":"code","source":"X = librosa.stft(data)\n\n#converting into energy levels(dB)\nXdb = librosa.amplitude_to_db(abs(X))\n\nplt.figure(figsize=(20, 5))\nlibrosa.display.specshow(Xdb, sr=sample_rate, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Log-frequency axis: \nFeatures can be obtained from a spectrogram by converting the linear frequency axis, as shown above, into a logarithmic axis. The resulting representation is also called a log-frequency spectrogram","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 5))\nlibrosa.display.specshow(Xdb, sr=sample_rate, x_axis='time', y_axis='log')\nplt.colorbar()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Dataset","metadata":{}},{"cell_type":"code","source":"raw_df = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\ndf = raw_df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Inspection","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\t\t\t# for concise summary of dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"### MISSING DATA ###\n\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explanatory Data Analysis\n\nI looked at the distributions of the data and the value counts for the various categorical variables. Below are a few highlights :","metadata":{}},{"cell_type":"markdown","source":"## Target Variable","metadata":{}},{"cell_type":"code","source":"df['class'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y=\"class\", data=df, order = df['class'].value_counts().index)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is almost balanced","metadata":{}},{"cell_type":"markdown","source":"## Other variables : Predictors","metadata":{}},{"cell_type":"code","source":"df.hist( bins = 10, figsize = (10,10), color = 'r')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate correlations\ncorr = df.corr()\n\n# Heatmap\nsns.heatmap(corr,  annot=True, fmt=\".2f\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obervations\n\n- start and end timings are correlated to each other\n- most of the voice are foreground as backgruond voices\n- data in fold folders are almost equal","metadata":{}},{"cell_type":"markdown","source":"## Sounds of different classes","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classID = list(df['classID'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_list = []\n\nfor ID in classID:\n    for i in range(len(df)):\n        if(df.classID[i] == ID):\n            \n            file = df['slice_file_name'][i]\n            folder = str(df['fold'][i])\n            class_id = ID\n            class_ = df['class'][i]\n            audio_file = \"../input/urbansound8k/\" + \"fold\" + folder + \"/\" + file\n            audio_list.append(audio_file)\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"random sound file of different class","metadata":{}},{"cell_type":"code","source":"import random\naudio_file = random.choice(audio_list)\ndata,sample_rate = librosa.load(audio_file)\nipd.Audio(audio_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Waveforms of different classes","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ID in classID:\n    for i in range(len(df)):\n        if(df.classID[i] == ID):\n            \n            file = df['slice_file_name'][i]\n            folder = str(df['fold'][i])\n            class_id = ID\n            class_ = df['class'][i]\n            audio_file = \"../input/urbansound8k/\" + \"fold\" + folder + \"/\" + file\n            print(audio_file)\n            \n            data,sample_rate = librosa.load(audio_file)\n\n            plt.figure(figsize=(14,5))\n            librosa.display.waveplot(data, sr = sample_rate)\n            plt.title(class_)\n            plt.show()\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}