{"cells":[{"metadata":{"_cell_guid":"f3566790-44bc-47be-a57b-cd2d89fd6694","_uuid":"5ee816ba302841e36c938857a18f87324038dc73"},"cell_type":"markdown","source":"# Synopsis Visualization \n\nI modified this code from https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html by adding visualization to the resulting topics.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6f9e5711-68f6-472c-afe7-93be77b6f5da","_uuid":"b5be934ed735cb1549429d9975a0a77a53aab4b9"},"cell_type":"markdown","source":"## Import libraries\n\nHere I used LDA model from gensim. Personally I find it simpler to implement than using sklearn.","outputs":[],"execution_count":null},{"metadata":{"_kg_hide-input":false,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora, models\nimport pandas as pd\nimport gensim\nimport pyLDAvis.gensim","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d31af256-269b-43b6-8c25-dfaa8ecbd80b","_uuid":"de6657b4355030de74b1a44f0058035b60757b98"},"cell_type":"markdown","source":"## Initiating Tokenizer and Stemmer\n\nIn addition, I create a list of words which are common in this context to be removed before performing topic modeling.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f78d4f89-d08f-45db-8c5c-5dc42fa22a51","collapsed":true,"_uuid":"efce42d9747224f7dbed4321ad6de166c1e062a6","trusted":false},"cell_type":"code","source":"pattern = r'\\b[^\\d\\W]+\\b'\ntokenizer = RegexpTokenizer(pattern)\nen_stop = get_stop_words('en')\nlemmatizer = WordNetLemmatizer()\n\n# remove certain words\ntobe_removed = []","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69068869-6fbd-456c-8a59-d12b9c7ae974","_uuid":"b2097d7b1bdb4cb258147bf0ecc9bc408e32dcd5"},"cell_type":"markdown","source":"## Read the data and make it into a list","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b5d768e8-3b74-4b58-be2a-3a30f04f373f","_uuid":"9bb4a18c3ea87a27bf7ee2181a082ceab3cf1cd2","trusted":false,"collapsed":true},"cell_type":"code","source":"# Input from csv\ndf = pd.read_csv('../input/datasynopsis-all-share-new.csv',sep='|')\n\n# sample data\nprint(df['Synopsis'].head(2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77bc2203-9b96-4f78-a0d2-b18a521c03c1","_uuid":"52763dfe1cf630120f5b5c485d394a23cb1aec4f"},"cell_type":"markdown","source":"## Perform Tokenization, words removal, and Lemmatization","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7675db3c-9ce6-4e48-872b-dd873ed4cff4","_uuid":"18b72d95b337a5b0a8a705e8a3c944b3d4b38c7f","_kg_hide-output":false,"trusted":false,"collapsed":true},"cell_type":"code","source":"# list for tokenized documents in loop\ntexts = []\n\n# loop through document list\nfor i in df['Synopsis'].iteritems():\n    # clean and tokenize document string\n    raw = str(i[1]).lower()\n    tokens = tokenizer.tokenize(raw)\n\n    # remove stop words from tokens\n    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n    \n    # remove some words from tokens\n    new_stopped_tokens = [raw for raw in stopped_tokens if not raw in tobe_removed]\n    \n    # lemmatize tokens\n    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in new_stopped_tokens]\n    \n    # remove\n    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n    \n    # add tokens to list\n    texts.append(new_lemma_tokens)\n\n# sample data\nprint(texts[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"029a375c-0eef-47d8-8ca4-6b1a89e0f4a8","_uuid":"093f6591e74cf0ff1a7aa87c8a7e3929fdb7a895"},"cell_type":"markdown","source":"## Create term dictionary and document-term matrix","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"08446e5a-dbd6-4c51-af59-8e3308028c7e","collapsed":true,"_uuid":"2b327e6fb315449b5c64fd725ec48a7214bdcce3","trusted":false},"cell_type":"code","source":"# turn our tokenized documents into a id <-> term dictionary\ndictionary = corpora.Dictionary(texts)\n# convert tokenized documents into a document-term matrix\ncorpus = [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80fb3583-f49f-45dd-b109-a15969717a28","_uuid":"8f1dd137b2f125d4205611652ec1b4c3d554d979"},"cell_type":"markdown","source":"## Generate LDA model\n\nHere I used pre-determined number of topics. It will better calculating perplexity to find the optimum number of topics.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"514e1e40-8f81-4646-92cd-086cc936cec7","_uuid":"bb1fd5c50ae79ccc3fea90a50f3b66d9ee74006c","trusted":false,"collapsed":true},"cell_type":"code","source":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\nprint(ldamodel.print_topics(num_topics=10, num_words=5))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d966f1b-d673-4eea-95c5-9a5ba42ed6fb","_uuid":"8cb2e8ae7ee90b985dfb1d534906be8ec19f32a2"},"cell_type":"markdown","source":"## Visualize the topic model\n\nUsing pyLDAvis, we can create an interactive visualization.","outputs":[],"execution_count":null},{"metadata":{"scrolled":true,"_cell_guid":"c8e9ca71-8b5a-4936-99e5-300541cbef9a","_uuid":"27acee57413d8fbe647a593d70ca9d0bac19d0b0","trusted":false,"collapsed":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}