{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Goal : Clusterize animes.\n\nWhat I want is try to clusters anime by content, in this case I will only consider the description of the series as the only information that I have.\nThe purpouse of this notebook is to sharpen my text analisys and clustering skills.\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Imports\n\n#Computing\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#Clustering\nfrom sklearn.cluster import KMeans,SpectralClustering\nfrom yellowbrick.cluster import KElbowVisualizer\n\n#Decomposition\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n#Text\nimport unicodedata, re, string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\n\n#Default\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read the dataset\ndf=pd.read_csv('/kaggle/input/anime-dataset/anime.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some basic information\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As seen by the info method, a lot of animes have a null description. For the purpouse of this notebook, I will discard all the element that have a null description and I will focus on the not-null description animes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"animes=df[['title','description']].dropna()\nanimes.head()\n#animes.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are left with 8173 records in the dataset.\nSince this is a text clustering notebook, the first thing I'm going to do is to clean text. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Function to clean the text, Taken  by : https://www.kaggle.com/oragula/sentiment-analysis-rotten-tomato-movie-reviews","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef remove_len_2(words):\n    \"\"\"Remove all the words with len <= 2\"\"\"\n    new_words = []\n    for word in words:\n        if len(word)<=2:\n            pass\n        else:\n            new_words.append(word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_numbers(words):\n    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(\"\\d+\", \"\", word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = remove_len_2(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_numbers(words)\n    words = remove_stopwords(words)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenize text\nanimes['Tokenized']=animes['description'].apply(nltk.word_tokenize)\nanimes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize text\nanimes['Clean_text']=animes['Tokenized'].apply(lambda y: normalize(y))\nanimes['Clean_text'][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Function to reconvert a tokenization into a single string\nThis is needed for the TfIdfVectorizer method \"\"\"\ndef conv2str(y):  \n     \n    str1 = \" \"   \n    return (str1.join(y)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"animes['Clean_text1']=animes['Clean_text'].apply(lambda y: conv2str(y))\nanimes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Apply the TF_idf vectorizer to get the sparse matrix of the TF_IDF process\"\"\"\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(animes['Clean_text1'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"X is a sparse matrix, that means it can be used as training and it can be used as a matrix.\nBut the representation as a sparse matrix is really optimal because a lot of elements of the matrix are 0.\n\n* (0,20871) means that in position (0,20871) we have the value 0.09422377859075083\n* And so on...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#First row and the element extracted are the same.\n\nprint(X)\nprint(X.toarray()[0][20871])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"A simple view of the feature names\"\"\"\nprint(vectorizer.get_feature_names()[:10])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"First cluster using KMeans and run the elbow visualizer to find the best number of clusters\"\"\"\nmodelKm = KMeans(random_state=12)\nvisualizer = KElbowVisualizer(modelKm, k=(1,12))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"First cluster using Spectral and run the elbow visualizer to find the best number of clusters\"\"\"\nmodelSc = SpectralClustering(random_state=5)\nvisualizer = KElbowVisualizer(modelSc, k=(1,12))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Train the Kmeans with the best n of clusters\"\"\"\nmodelKm = KMeans(n_clusters=4,random_state=12)\nmodelKm.fit(X)\ny_kmeans = modelKm.predict(X)\n\n\"\"\"Dimensionality reduction used to plot in 2d representation\"\"\"\npc=TruncatedSVD(n_components=2)\nX_new=pc.fit_transform(X)\ncentr=pc.transform(modelKm.cluster_centers_)\n\nprint(centr)\nplt.scatter(X_new[:,0],X_new[:,1],c=y_kmeans, cmap='viridis')\nplt.scatter(centr[:,0],centr[:,1],marker='X',alpha=0.5,color='red',s=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelSc = SpectralClustering(n_clusters=4, random_state=5)\ny_spc=modelSc.fit_predict(X)\n\n\npc=TruncatedSVD(n_components=2)\nX_new=pc.fit_transform(X)\n\nplt.scatter(X_new[:,0],X_new[:,1],c=y_spc, cmap='viridis')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For both clustering algorithm the result seems similar from a graphical point of view. But let's inspect with wordcloud the most important words of each cluster.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rebuild the clusters in pandas df.\n\nanimes['ClusterKmeans']=y_kmeans\nanimes['ClusterSpectral']=y_spc\nanimes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract text based on cluster \nclus0_text=animes[animes['ClusterKmeans']==0]\nclus1_text=animes[animes['ClusterKmeans']==1]\nclus2_text=animes[animes['ClusterKmeans']==2]\nclus3_text=animes[animes['ClusterKmeans']==3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus0_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus1_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus2_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus3_text['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the worldcloud view, is it possible to see that we have 4 clusters based on these words:\n(The order of the clusters can be different)\n\n* Cluster 1: Based on episodes, dungeon, recap episode, first season -> This cluster is kinda confusing me.\n* Cluster 2: Clearly a cluster based on school anime. -> School anime.\n* Cluster 3: Family, life, human, ... -> Thematics connected to this.\n* Cluster 4: Based on season? -> Maybe it is a cluster where all the second,third,.. seasons are.\n\nLet's see if with the spectral cluster we are more lucky.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract text based on cluster \nclus0_text_sp=animes[animes['ClusterSpectral']==0]\nclus1_text_sp=animes[animes['ClusterSpectral']==1]\nclus2_text_sp=animes[animes['ClusterSpectral']==2]\nclus3_text_sp=animes[animes['ClusterSpectral']==3]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(221)\nplt.title('Cluster1')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus0_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\nplt.subplot(222)\nplt.title('Cluster2')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus1_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\nplt.subplot(223)\nplt.title('Cluster3')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus2_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\nplt.subplot(224)\nplt.title('Cluster4')\nword_cloud = WordCloud(background_color='black',max_font_size = 80).generate(\" \".join(clus3_text_sp['Clean_text1']))\nplt.imshow(word_cloud)\nplt.axis('off')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## With the spectral clustering more or less we found the same clusters:\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## More or less the result of the two clustering algorithms is the same, based on the tf-idf value of the text contained in 'Description'.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}