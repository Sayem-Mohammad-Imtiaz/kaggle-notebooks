{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is Churn Rate?\n\n**Churn rate** (sometimes called attrition rate), in its broadest sense, is a measure of the number of individuals or items moving out of a collective group over a specific period. It is one of two primary factors that determine the steady-state level of customers a business will support.\n"},{"metadata":{},"cell_type":"markdown","source":"## Part 1 - Data Preprocessing\n\n### Importing the Libraries\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing The Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/bank-churn/Bank_churn_modelling.csv\")\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, After the first look on the dataset we can conclude that the dependent varaiables() for the prediction of bank churning is from the columns 3 to 12.\n\nSo we drop them from X which contains the features Indexes from 3 to 12."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:, 3:13].values\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We store the **Dependent value/predicted value** in y by storing the 13th index in the variable **y**."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset.iloc[:, 13].values\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting Categorical Variable\n\nNow, We need to convert our categorical dependent variables into numeric dependent variables.\n\nCategorical variables are known to hide and mask lots of interesting information in a data set. It’s crucial to learn the methods of dealing with such variables.\n\nThe only 2 values are Gender and Region which need to converted into numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating label encoder object no. 2 to encode Gender name(index 2 in features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"labelencoder_X_2 = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding Gender from string to just 2 numbers i.e. 0,1(male,female) respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Variables\n\nA dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc. Technically, dummy variables are dichotomous, quantitative variables. Their range of values is small; they can take on only two quantitative values.\n\nNow creating Dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\ntransformer = ColumnTransformer(transformers=[(\"OneHot\",OneHotEncoder(),[1])],remainder='passthrough')\nX = transformer.fit_transform(X.tolist())\nX = X.astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Variable Trap\n\nBy including dummy variable in a regression model however, one should be careful of the Dummy Variable Trap. The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others.\n\nWe can avoid the dummy variable trap by using one less varaible from all the variable. \n\nFor example , There are three dummies created for the feature \"Geography\". Now if we remove any one of the dummy then the we will avoid the trap.So, In this case we will remove the first column which has index 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the dataset into the Training set and Test set\n\nIn Machine Learning, we make a model which is nothing but an algorithm where some parameters needs to be modified such that it is able to perform good at the application i.e it is able to predict values of one wants to.\n\nHow can we modify those parameters such that it can do well ?\nWe can train the model using data which we call as training data or training set. The training data is the one which already has the actual value that the model should have predicted and thus the algorithm changes the value of parameters to account for the data in the training set.\n\nBut how do we know after training the model is overall good ?\nFor that, we have test data/test set which is basically a different data for which we know the values but this data was never shown to the model before. Thus if the model after training is performing good on test set as well then we can say that the Machine Learning model is good.\n\nIf the model is not tested and is made such that it just perform good on training data then parameters will be such that they are only good enough to predict the value for data which was in training set. That is not general. This is called overfitting.\n\nSo we don’t land making a useless model which is only good for the training set and not general enough.\n\nThus test set and training set is important to make Machine Learning model better."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling\n\nFeature Scaling or Standardization  is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the part 1 has a lot of work in it and we have not even started our neural network.\n\nNow it is important to know that one of the **big part of being a data scientist is data preprocessing** so that we can have a  usable data to apply to our models or neural network.\n\nSo, now lets start making our Artificial Neural Network"},{"metadata":{},"cell_type":"markdown","source":"You can also have a look at [ANN for Bank Churn Modeling](http://https://towardsdatascience.com/building-your-own-artificial-neural-network-from-scratch-on-churn-modeling-dataset-using-keras-in-690782f7d051)"},{"metadata":{},"cell_type":"markdown","source":"## Part 2 - ANN\n\nListing out the steps involved in training the ANN with Stochastic Gradient Descent:-\n\n1)Randomly initialize the weights to small numbers close to 0(But not 0).\n\n2)Input the 1st observation of your dataset in the Input Layer, each Feature in one Input Node.\n\n3)Forward-Propagation from Left to Right, the neurons are activated in a way that the impact of each neuron’s activation.\nis limited by the weights.Propagate the activations until getting the predicted result y.\n\n4)Compare the predicted result with the actual result. Measure the generated error.\n\n5)Back-Propagation: From Right to Left, Error is back propagated.Update the weights according to how much they are\nresponsible for the error.The Learning Rate tells us by how much such we update the weights.\n\n6)Repeat Steps 1 to 5 and update the weights after each observation(Reinforcement Learning).\nOr: Repeat Steps 1 to 5 but update the weights only after a batch of observations(Batch Learning).\n\n7)When the whole training set is passed through the ANN.That completes an Epoch. Redo more Epochs."},{"metadata":{},"cell_type":"markdown","source":"### Importing the Keras libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Keras Packages\n\nFor building the Neural Network layer by layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To randomly initialize the weights to small numbers close to 0(But not 0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initializing the ANN\n\nSo there are actually 2 ways of initializing a deep learning model\n\n1. Defining each layer one by one\n2. Defining a Graph\n\n\nWe did not put any parameter in the Sequential object as we will be defining the Layers manually"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the input layer and the first hidden layer\n\nThere is no thumb rule but you can set the number of nodes in Hidden Layers as an Average of the number of Nodes in Input and Output Layer Respectively.\n\n* So set Output Dim=6\n* Init will initialize the Hidden Layer weights uniformly\n* Activation Function is Rectifier Activation Function(Relu)\n* Input dim tells us the number of nodes in the Input Layer.This is done only once and wont be specified in further layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(output_dim = 6, init = 'uniform' , activation = 'relu', input_dim = 11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the second hidden layer\n\n* Set Output Dim=6\n* Init will initialize the Hidden Layer weights uniformly\n* Activation Function is Rectifier Activation Function(Relu)\n* No need for Input Dim.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the output layer\n\n* Set Output Dim=1\n* Init will initialize the Hidden Layer weights uniformly\n* Activation Function is Sigmoid Activation Function(sigmoid)\n\n**Sigmoid activation function** is used whenever we need Probabilities of 2 categories or less(Similar to Logistic Regression)\n\n\nSwitch to **Softmax** when the dependent variable has more than 2 categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compiling the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the ANN to the Training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3 — Making the predictions and evaluating the model\n\n### Predicting the Test set results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5) #if y_pred is larger than 0.5 it returns true(1) else false(2)\n\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This Model when trained on the train data and when tested on the test data gives us an accuracy of around 84% in both of the cases**.Which from our point of view is Great!!!\n\n### Making the Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obtained from Confusion Matrix.You may change values as per what is obtained in your confusion matrix.\n\n\n**Congratulations! you just wrote your own Neural Network for theBank which had given you this task.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}