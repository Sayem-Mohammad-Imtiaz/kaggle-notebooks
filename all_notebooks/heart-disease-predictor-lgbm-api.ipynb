{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0.0 Business Contex"},{"metadata":{},"cell_type":"markdown","source":"## 0.1 Business Problem\n\nOur challenge is to help the Cardio Catch Diseases a company that specializes in detecting heart diseases in the early stages.\n\nIts Business model is 'service' it means that the company offers an early heart disease diagnostic for a price.\n\nImportant Details:\n\n    Actually, the precision of the diagnostic is between 55% and 65%.\n    For each 5% of accuracy over 50% the price is R$ 500,00\n\nMain Goal:\n\nCreate a tool that increases diagnostic accuracy and that this accuracy is stable for all diagnostics.\n\nSecondary Goals:\n\nAnswer the CEO questions,\n\n    What is the Accuracy and Precision of the tool?\n    How much profit will Cardio Catch Diseases have with the new tool?\n    How Reliable is the result given by the new tool?\n\nBusiness problem reference: https://sejaumdatascientist.com/projeto-de-data-science-diagnostico-precoce-de-doencas-cardiovasculares/\n\n\nIndex:\n\n0.2 - Helper Functions and Imports\n\n1.0 - Collect Data\n\n2.0 - Data Description\n\n3.0 - Splitting data\n\n4.0 - EDA\n\n5.0 - Hypothesis Test\n\n6.0 - Pre-Processing / Feature Engineering\n\n7.0 - Machine Learning Modelling\n\n8.0 - API"},{"metadata":{},"cell_type":"markdown","source":"\n## 0.2 Conclusions and Results\n\nAfter all procedures and analyses performed the best result was with LGBM Model.\n\nLGBM: with a threshold of 0.4\n\n    Recall of 0.78\n    Accuracy of 0.73\n\nWhat answers the first question of the CEO.\n\nAbout the profit, the price was increased by 4.4 times reaching R$ 2219.0\n\nWhy focus on recall than precision(preferred metric of the business)?\n\nBecause of the nature of the problem, heart diseases, a false diagnosis to an actual sick patient could lead to aggravation of the problem or even death. Bussines wise, it could hurt us as a future legal process. Focusing on recall we make less money but we get away less risk of the legal process.\n\nScenario focusing on precision\n\n    Precision = 0.80\n    Accuracy = 0.75\n    Recall = 0.67\n\nPrice increased by 4.63 times reaching R$ 2316.0"},{"metadata":{},"cell_type":"markdown","source":"\n# 0.2 Helper Functions and Imports"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 0.2.1 Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Propose\nimport pickle\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import shapiro\nfrom scikitplot import metrics as mt\n\n# Hipo Test\nfrom scipy import stats\nfrom scipy.stats import f_oneway\nfrom scipy.stats import ttest_ind\n\n\n# Pre-processing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n\n# Modelling\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cluster import KMeans\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report, accuracy_score, cohen_kappa_score, precision_score, f1_score, recall_score\nfrom yellowbrick.classifier.threshold import discrimination_threshold\n\n# Set options and warnings\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('MAX_ROWS', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.2.2 Helper Functions and Transformers"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Helper Functions\ndef balanced_target(target, dataset, hue=None):\n    \"\"\"\n    Function to check the balancing of the target variable.\n\n    :target:  An pd.Series of the target variable that will be checked.\n    :dataset: An Dataframe object. \n    \"\"\"\n    sns.set(style='darkgrid', palette='Accent')\n    ax = sns.countplot(x=target, hue=hue, data=dataset)\n    ax.figure.set_size_inches(10, 6)\n    ax.set_title('Cardio Distribution', fontsize=18, loc='left')\n    ax.set_xlabel(target, fontsize=14)\n    ax.set_ylabel('Count', fontsize=14)\n    ax=ax\n\n\ndef univariate_analysis(target, df):\n    \"\"\"\n    Function to perform univariate analysis.\n\n    df: DataFrame\n    \"\"\"\n    for col in df.columns.to_list():\n\n        fig = sns.displot(x=col, hue=target, data=df, kind='hist')\n        fig.set_titles(f'{col}\\n distribuition', fontsize=16)\n        fig.set_axis_labels(col, fontsize=14)\n\n\ndef multi_histogram(data: pd.DataFrame, variables: list) -> None:\n\n    # set of initial plot posistion\n    plt.figure(figsize=(18, 10))\n    n = 1\n    for column in data[variables].columns:\n        plt.subplot(3, 3, n)\n        _ = sns.distplot(a=data[column], bins=50, hist=True)\n        n += 1\n\n    plt.subplots_adjust(hspace=0.3)\n\n    plt.show()\n\n\n\ndef multi_boxplot(data: pd.DataFrame, variables: list) -> None:\n\n    \"\"\"\n    Function to check for outliers visually through a boxplot\n\n    data: DataFrame\n\n    variable: list of numerical variables\n    \"\"\"\n\n    # set of initial plot posistion\n    plt.figure(figsize=(18, 10))\n    n = 1\n    for column in data[variables].columns:\n        plt.subplot(3, 3, n)\n        _ = sns.boxplot(x=column, data=data)\n        n += 1\n\n    plt.subplots_adjust(hspace=0.3)\n\n    plt.show()\n\n\ndef hipo_test(*samples):\n\n    samples = samples\n\n    try:\n        if len(samples) == 2:\n            stat, p = ttest_ind(*samples)\n        elif len(samples) > 2:\n            stat, p = f_oneway(*samples)\n    except:\n        raise Exception(\"Deve ser fornecido pelo menos duas samples!!!\")\n\n    if p < 0.05:\n        print(f'O valor de p é: {p}')\n        print('Provável haver diferença')\n    else:\n        print(f'O valor de p é: {p}')\n        print('Provável que não haja diferença')\n\n    return stat, p\n\n\ndef point_bi_corr(a, b):\n\n    \"\"\"\n    Function to calculate point biserial correlation coefficient heatmap function\n    Credits: Bruno Santos - Comunidade DS\n\n    :a: input dataframe with binary variable\n    :b: input dataframe with continous variable\n    \"\"\"\n\n    # Get column name\n    a = a.values.reshape(-1)\n    b = b.columns.reshape(-1)\n\n    # apply scipys point-biserial\n    stats.pointbiserialr(a, b)\n\n    # correlation coefficient array\n    c = np.corrcoef(a, b)\n\n    # dataframe for heatmap\n    df = pd.DataFrame(c, columns=[a, b], index=[a, b])\n\n    # return heatmap\n    return sns.heatmap(df, annot=True).set_title('{} x {} correlation heatmap'.format(a, b));\n\n\ndef change_threshold_lgbm(X, y, model, n_splits, thresh):\n\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n        pred_prob = model.predict_proba(X_valid)\n\n        for i in range(0, len(pred_prob)):\n            if pred_prob[i, 1] >= thresh:\n                pred_prob[i, 1] = 1\n            else:\n                pred_prob[i, 1] = 0\n\n        Acc = accuracy_score(y_valid, pred_prob[:, 1])\n        Kappa =  cohen_kappa_score(y_valid, pred_prob[:, 1])\n        Recall = recall_score(y_valid, pred_prob[:, 1])\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    print('####### Bussines Metrics #######')\n    print('\\n')\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased precision: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round(prc_inc/500,2)}')\n    print('\\n')\n\n    # print classification report\n    print('####### Machine Learning Metrics #######\\n')\n    print(classification_report(y_valid, pred_prob[:,1], digits=2))\n\n    # Confusion Matrix\n    mt.plot_confusion_matrix(y_valid, pred_prob[:,1], normalize=False, figsize=(10,8))\n\n    return pred_prob[:, 1]\n\n\ndef change_threshold_lr(X, y, model, n_splits, thresh):\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n        y_scores_final = model.decision_function(X_valid)\n        y_pred_recall = (y_scores_final > thresh)\n\n        Acc = accuracy_score(y_valid, y_pred_recall)\n        Kappa =  cohen_kappa_score(y_valid, y_pred_recall)\n        Recall = recall_score(y_valid, y_pred_recall)\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    print('####### Bussines Metrics #######\\n')\n\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased precision: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round(prc_inc/500,2)}')\n    print('\\n')\n\n    print('####### Machine Learning Metrics #######\\n')\n    print(f'New kappa: {cohen_kappa_score(y_valid,y_pred_recall)}\\n')\n    print(classification_report(y_valid, y_pred_recall, digits=2))\n\n\n    return y_pred_recall\n\n\n################################################# Custons Transformers ###########################################################\n\nclass PreProcessingTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        Xtemp = X.copy()\n\n        # Height\n        index_height = Xtemp.loc[Xtemp['height'] > 230, ['height']].index\n        Xtemp.drop(index_height, inplace=True)\n        index_height1 = Xtemp.loc[Xtemp['height'] < 112, ['height']].index\n        Xtemp.drop(index_height1, inplace=True)\n\n        # Weight\n        index_weight = Xtemp.loc[Xtemp['weight'] < 40, ['weight']].index\n        Xtemp.drop(index_weight, inplace=True)\n\n        # ap_hi\n        index_ap_hi = Xtemp.loc[Xtemp['ap_hi'] < 10, ['ap_hi']].index\n        Xtemp.drop(index_ap_hi, inplace=True)\n\n        # ap_lo\n        index_ap_lo = Xtemp.loc[Xtemp['ap_lo'] < 5, ['ap_lo']].index\n        Xtemp.drop(index_ap_lo, inplace=True)\n\n        # SMOTE + TOMEKLINK\n        X = Xtemp.drop('cardio', axis=1)\n        y = Xtemp['cardio']\n\n        smt = SMOTETomek(random_state=42)\n        Xres, yres = smt.fit_resample(X, y)\n        Xtemp = pd.concat([Xres, yres], axis=1)\n\n        return Xtemp\n\n\nclass FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        Xtemp = X.copy()\n\n        # Cluster based var\n        kmeans = KMeans(n_clusters=2, init='k-means++',n_init=20, random_state=0).fit(Xtemp)\n        Xtemp['kmeans_cat'] = kmeans.labels_\n\n        # # Cluster GMM\n        # gmm = GaussianMixture(n_components=3).fit(Xtemp)\n        # Xtemp['gauss_cat'] = gmm.predict(Xtemp)\n\n        # Year_age\n        Xtemp['year_age'] = Xtemp['age'] / 365\n\n        # drop 'id' and 'age' 'smoke','alco','gluc', 'ap_lo', 'cholesterol', 'height', 'active', 'weight'\n        Xtemp.drop(['id', 'age'], inplace=True, axis=1)\n\n        # IMC\n        Xtemp['imc'] = Xtemp['weight']/(Xtemp['height']/100)**2\n\n        # cat_dwarfism\n        Xtemp['cat_Dwarfism'] = [1 if value < 145 else 0 for value in Xtemp['height']]\n\n        # ap_hi divide 10\n        Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']] = Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']]/10\n\n        # ap_lo divide 10\n        Xtemp.loc[Xtemp['ap_lo'] > 190, ['ap_lo']] = Xtemp.loc[Xtemp['ap_lo'] > 190, ['ap_lo']]/10\n\n        # ap_hi divide 10\n        Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']] = Xtemp.loc[Xtemp['ap_hi'] > 220,['ap_hi']]/10\n\n        # ap_hi divide 10\n        Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']] = Xtemp.loc[Xtemp['ap_hi'] > 220,['ap_hi']]/10\n\n        # ap_lo divide 10\n        Xtemp.loc[Xtemp['ap_lo'] > 190, ['ap_lo']] = Xtemp.loc[Xtemp['ap_lo'] > 190,['ap_lo']]/10\n\n        return Xtemp\n\n\nclass CatBloodPressureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        Xtemp = X.copy()\n\n        # cat_bloodpressure\n        def cat_bloodpressure(df):\n\n            if df['ap_hi'] < 90 and df['ap_lo'] < 60:\n                return 1 #Hipotensão\n            elif 90 <= df['ap_hi'] < 140 and 60 <= df['ap_lo'] < 90:\n                return 2    # Pré-Hipotensão\n            elif 140 <= df['ap_hi'] < 160 and 90 <= df['ap_lo'] < 100:\n                return 3  # 'Hipertensão estagio1'\n            elif df['ap_hi'] >= 160 and df['ap_lo'] >= 100:\n                return 4 # 'Hipertensão estagio2'\n            else:\n                return 5 # 'no_cat'\n\n        # cat_bloodpressure\n        Xtemp['cat_bloodpressure'] = Xtemp.apply(cat_bloodpressure, axis=1)\n\n        return Xtemp\n\n\nclass TotalPressureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n\n        return self\n\n    def transform(self, X, y=None):\n\n        Xtemp = X.copy()\n\n        # total_preassure\n        Xtemp['total_pressure'] = Xtemp['ap_hi'] + Xtemp['ap_lo']\n\n        return Xtemp\n\n\nclass MyRobustScalerTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n\n        Xtemp = X.copy()\n\n        scaler = RobustScaler()\n        Xscaled = scaler.fit_transform(Xtemp)\n        Xtemp = pd.DataFrame(Xscaled, columns=Xtemp.columns.to_list())\n\n        return Xtemp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.0 Collect Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/cardiovascular-disease-dataset/cardio_train.csv', sep=';')\nprint(f'The number of columns are {train.shape[1]}')\nprint(f'The number of rows are {train.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.0 Data Description\nTakeaways:\n\n-    Variable age is in days. Let's divide it by 365 and check if the values make sense.\n-    We will drop 'id' at some point because doesn't give us much information.\n-    No need to encoding categorical variables.\n-    Binary classification problem.\n-    Let's consider the possibility of OHE the categorical variables as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sneak look\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Details of the data\npd.DataFrame({'missingPerc': train.isna().mean(),\n              'uniques': train.nunique(),\n              '%uniquePerc': round((train.nunique()/train.shape[0])*100, 2),\n              'data_types': train.dtypes,\n              'mean': round(train.mean(), 2),\n              'median': round(train.median(), 2),\n              'std': round(train.std(), 2),\n              'min': round(train.min(), 2),\n              'max': round(train.max(), 2)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.0 Splitting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data\ndfTrain = train[:60000]\n\n# Validação\ndfValid = train[60000:]\nX = dfValid.drop(['cardio'], axis=1)\ny = dfValid['cardio']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.0 EDA"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Univariate Analysis\n\nMain takeaways:\n\n-    Our target is balanced with 35000 observations for each class.\n-   'age' and 'weight'(as they increase) seems to have more influence over 'cardio'\n-    We do have outliers!!!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking all numerical vars\nnumerical_vars = ['age','height', 'weight', 'ap_hi', 'ap_lo']\nmulti_histogram(data=train, variables=numerical_vars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.1'cardio' analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Analysys\nbalanced_target(target='cardio',dataset=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.2 'Age'\n- Looks like the count of heart diseases increases with 'Age'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the variable 'Age' to years instead of days\nage_df = train.copy()\nage_df['age_years'] = age_df['age'] / 365\n\nprint(age_df['age_years'].describe())\nprint('\\n')\n\n# Plot distribution\nfig = sns.distplot(a=age_df['age_years'], hist=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.3 'Height\n\n\n-    The distribution to class 0 and 1 are very similar.\n-    We found a value of 250 each is impossible to happen in the real world, so we will replace its value for 207 (the highest possible value).\n-    There are small values like 55cm. Dwarfism is related to a lower expectancy of life usually due to cardiovascular diseases having it in mind we will create a new binary variable 1 - for cases where height is lower than 1,45 and 0 when it's not the case. We will also drop the cases where the Height is less than 112 because these data points are probably an error. The height of dwarf people usually varies between 112 and 145.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe\nprint(train['height'].describe())\nprint('\\n')\n\n# Plot distribution\nfig = sns.distplot(a=train['height'], hist=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.4 'weight'\n\n-    Looks like higher weight leads to more cases of heart disease.\n-    Drop the weights bellow 40. Because they are too small and are probably due to some error."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe\nprint(train['weight'].describe())\nprint('\\n')\n\n# Plot distribution\nfig = sns.distplot(a=train['weight'], hist=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.4 'ap_hi'\nResearching little be about the subject on the internet unusual readings of Systolic blood pressure below 10 and above 220.\n\n-    We have 130 observations below 10 and 624 observations over 220 which is unusual. Let's drop them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count below 10 values\ntrain.loc[train['ap_hi']<10,:].size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count values over 220\ntrain.loc[train['ap_hi']>220,:].size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping\ndf = train.copy()\nindex = df.loc[(df['ap_hi']<10)|(df['ap_hi']>220),['ap_hi']].index\ndf.drop(index, inplace=True)\n\n# Describe\nprint(df['ap_hi'].describe())\nprint('\\n')\n\n# Plot distribution\nfig = sns.distplot(a=df['ap_hi'], hist=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.5 'ap_lo'\n\nResearching little be about the subject on the internet unusual readings of Diastolic blood pressure below 5 and divide values over 190 by 10.\n\nThe former is because probably these numbers are type-error. For example, 190.00 blood pressure can be typed as 19000.\n\n-    Let's drop the values below 10.\n-    Let's divide the values over 190 by 10.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping\ndf = train.copy()\nindex = df.loc[(df['ap_lo']<5)|(df['ap_lo']>190),['ap_lo']].index\ndf.drop(index, inplace=True)\n\n# Describe\nprint(df['ap_lo'].describe())\nprint('\\n')\n\n# Plot distribution\nfig = sns.distplot(a=df['ap_lo'], hist=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 4.2 Checking Ouliers\nWe do have a lot of outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"out_train = train.drop(['id','gender','gluc','smoke','cholesterol','active','alco','cardio'],axis=1)\nvars = out_train.columns.to_list()\n\nmulti_boxplot(data=train, variables=vars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2 Bivariate Analysis\n\n\n-    Let's work with just numerical variables.\n-    By the 'age' vs 'weight' plot we can see that the incidence of event 1 increases as age and weight increase as well.\n-    The variable 'height' seems to have little influence on our positive event.\n-    By the 'ap_hi' and 'ap_lo' we can see the presence of some outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set of Variables\nbi_train = train.drop(['id','gender','gluc','smoke','cholesterol','active','alco'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2.1 'age' vs 'weight'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'age' vs 'weight'\nsns.scatterplot(x='age',y='weight',hue='cardio',data=bi_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.2 'age' vs 'height'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'age' vs 'height'\nsns.scatterplot(x='age',y='height',hue='cardio',data=bi_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.3 'age' vs 'ap_lo'"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='age',y='ap_lo',hue='cardio',data=bi_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.4 'age' vs 'ap_hi'"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='age',y='ap_hi',hue='cardio',data=bi_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.5 cardio vs categoricals\nIt seems that our target variable distribution is following the distribution of the categorical variable. Ex. 'cholesterol' in our dataset there is a more number of 1-normal that's why there is a concentration of 'cardio' events in this category."},{"metadata":{"trusted":true},"cell_type":"code","source":"# cardio distribution by 'cholesterol'\nbalanced_target(target='cholesterol',hue='cardio', dataset=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cardio distribution by 'gluc'\nbalanced_target(target='gluc',hue='cardio', dataset=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cardio distribution by 'alco'\nbalanced_target(target='alco',hue='cardio', dataset=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cardio distribution by 'active'\nbalanced_target(target='active',hue='cardio', dataset=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.0 Hypothesis Test\n\n1.0 Does the 'age' affect the number of cases of cardiovascular disease?\n\n2.0 Does the 'weight' affect the number of cases of cardiovascular disease?\n\n3.0 Does the 'height' affect the number of cases of cardiovascular disease?"},{"metadata":{},"cell_type":"markdown","source":"**5.1 Does the 'age' affect the number of cases of cardiovascular disease?**\n\nConclusion p-value less than 0.05 there is probably a difference since the mean of class 1 is higher the variable 'age' has an impact over the target.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_class0 = train.loc[train['cardio']==0,'age']\nsample_class1 = train.loc[train['cardio']==1,'age']\n\nprint(f'The Mean to class 0: {np.mean(sample_class0)}')\nprint(f'The Mean to class 1: {np.mean(sample_class1)}')\nprint('\\n')\n\n# Testing the Hypothesis\nhipo_test(sample_class0, sample_class1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**5.2 Does the 'weight' affect the number of cases of cardiovascular disease?¶**\n\nConclusion p-value less than 0.05 there is probably a difference since the mean of class 1 is higher the variable 'weight' has an impact over the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_class0 = train.loc[train['cardio']==0,'weight']\nsample_class1 = train.loc[train['cardio']==1,'weight']\n\nprint(f'The Mean to class 0: {np.mean(sample_class0)}')\nprint(f'The Mean to class 1: {np.mean(sample_class1)}')\nprint('\\n')\n\n# Testing the Hypothesis\nhipo_test(sample_class0, sample_class1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5.3 Does the 'height' affect the number of cases of cardiovascular disease?**\n\nConclusion p-value less than 0.05 there is probably a difference since the mean of class 0 is higher the variable 'height' has a little impact on the predictions of class 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_class0 = train.loc[train['cardio']==0,'height']\nsample_class1 = train.loc[train['cardio']==1,'height']\n\nprint(f'The Mean to class 0: {np.mean(sample_class0)}')\nprint(f'The Mean to class 1: {np.mean(sample_class1)}')\nprint('\\n')\n\n# Testing the Hypothesis\nhipo_test(sample_class0, sample_class1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.0 Pre-Processing / FeatureEngineering\n\n## During the Pre-Processing - Feature Engineering step we perform the actions below\n\n#### obs: The code is in the Helpers Function section of this notebook it was done trough custom transformers\n\n    We created an age variable in years.\n    We dropped all observations of 'height' with more than 230\n    We dropped al observations of 'height' with less than 112\n    We created the cat_dwarfism variable - if the 'height' is less than 145 it's class 1 otherwise it's 0.\n    We created an 'IMC' variable.\n    We dropped the 'weight' below 40.\n    We dropped the 'ap_hi' values that were less than 10.\n    We dropped the 'ap_hi' values that were less than 5.\n    We divided the 'ap_hi' over 220 by 10 three times. It was necessary due to extremely high values\n    We divided the 'ap_lo' over 190 by 10 two times. It was necessary due to extremely high values\n    We scaled all numerical variables with RobustScaler() due to the presence of outliers, the RobustScaler is not sensitive to extreme values.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 6.1 Testing Pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"transf = PreProcessingTransformer()\ndf1 = transf.fit_transform(dfTrain)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the FeatureEng\ntranf = FeatureEngineeringTransformer()\ndf = tranf.fit_transform(dfTrain)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps=[('feateng', FeatureEngineeringTransformer()),\n                           ('totalpress', TotalPressureTransformer()),\n                           ('catpress', CatBloodPressureTransformer())\n                           ])\n\nct = ColumnTransformer([('numerical',MyRobustScalerTransformer(), ['year_age','height','weight', 'ap_hi', 'ap_lo','total_pressure', 'imc']),\n                       ('categorical',OneHotEncoder(drop='first'), ['cholesterol', 'gluc', 'active', 'gender', 'smoke', 'alco','cat_bloodpressure','cat_Dwarfism','kmeans_cat'])])\n\n\npipeline_final = Pipeline(steps= [('geral',pipeline),\n                                  ('num_cat', ct)])\n\nexemp = pipeline_final.fit_transform(dfTrain)\npd.DataFrame(exemp).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.0 Machine Learning Modelling\n\nWe choose to deploy the LightGBM model the main reason is that it had the best performance, especially when we use the threshold was 0.60 we achieve a precision of 84% with a global accuracy of 75%, after the tunning of hyperparameters we retrained the model with all the data and deploy it as an API.\n\nThe Algorithms tested was:\n\nLogisticRegression:\n\nWhat is Logistic Regression?\n\nLogistic regression is a transformation of the linear regression model that allows us to probabilistically model binary variables. It is also known as a generalized linear model that uses a logit-link. Logistic regression is great when we want to model binary data, just like we are doing here, when we want class probability predictions or when we want some interpretability of the model trough its coefficients we can quantify the impact of each feature on your model’s predictions via the odds ratio. On the order hand, Logistic Regression is not that great when our data is not linearly separable.\n\nHow does it work?\n\nLogistic regression works very much like linear regression. Input (x) are combined linearly using weights or coefficients values to predict an output (y). The key difference is that the output is modeled as binary values through the equation below\n$$ y = e**(b0 + b1*x) / (1 + e**(b0 + b1*x)) $$\n\nThe coefficients (Beta values b) of the logistic regression algorithm are estimated by maximum-likelihood estimation.\n\nMaximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms. The best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and value very close to 0 (e.g. female) for the other class.\n\nPros:\n\n-    Very Simple and easy to understand.\n-    Interpretable.\n-    Probabilistically outputs.\n-    Low cost of maintenance\n\nCons:\n\n-    Sensible to highly correlated inputs.\n-    Assumes Gaussian Distribution.\n\nRandomForest:\n\nWhat is Random Forest?\n\nRandom Forest is a set of Decision Trees that are built and can be used for regression or classification tasks (our case). To better understand Random Forest we will quickly understand what Bagging is.\n\nBagging: An isolated decision tree has a high variance, however, if we build several Decision Trees based on random samples taken from our dataset and average the predictions for each sample, we considerably reduce the variance of our forecasts. This process is called Bagging.\n\nThe formula being:\n$$ fbag(x) = \\frac {1}{b}\\sum f^b(x) $$\n\nBy reducing the variance through the average of several Decision Trees we were able to increase the accuracy of the model.\n\nHow does Random Forest work?\n\nThe difference between Random Forest and Bagging is at the time of each split, in the case of Random Forest, the algorithm randomly chooses m predictors from the total p predictors of the dataset, so the algorithm cannot consider all the predictors for each tree built.\n$$ m =\\sqrt[2]{p} $$\n\nWhat is the reason for this? In order to avoid, as in Bagging, if the dataset has a very strong predictor, all trees will be built equally, using this predictor as the main 'split'. Random Forests avoid this problem by forcing the algorithm to work with only part of the predictors.\n\nPros:\n\nFrom decision trees, we can identify more significant movies, as well as the relationship between the variables\n\nEasy to understand, the visualization of the tree is easy to understand, especially when communicating the analysis process to the business areas, since it does not require any statistical knowledge to be read and interpreted.\n\nThey do not presuppose the distribution of space or the structure of the classifier, nor are they restricted to data types, being able to manipulate categorical or numerical types.\n\nCons:\n\nOverfitting is one of the biggest difficulties for decision tree models. This problem is solved by defining restrictions on the model and tunning parameters (pruning).\n\nNot suitable for continuous variables, the decision tree loses information when it categorizes variables in different categories.\n\nLGBM:\n\nWhat is LGBM?\n\nAccording to the proper documentation of LightGBM, it is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n-    Faster training speed and higher efficiency.\n\n-    Lower memory usage.\n\n-    Better accuracy.\n\n-    Support for parallel and GPU learning.\n\n-    Capable of handling large-scale data.\n\nThe main difference of LightGBM compared with other tree-based algorithms is that it grows the tree in a vertical way(leaf-wise) instead of a horizontal way(level-wise). It will choose the leaf with max delta loss to grow and keep growing the same leaf, usually, a Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\nThe diagrams below may give a more visual comprehension:\n\nConstraints: Usually LightGBM does not work well in small datasets, there is no formal limit here but 10.000 instances would be a good number.\n\nMain Parameters:\n\nmax_depth: boosting_type: learning_rate: n_estimators: num_leaves: subsample: min_split_gain: min_split_weight: min_child_samples:\n\nCatBoost:\n\nWhat is Cat Boost?\n\nCatboost is a recently open-sourced ML algorithm. It can be integrated with TensorFlow and Apple’s Core ML. The library works well with multiple Categories of data, such as audio, text, image including historical data.\n\nIt is especially powerful in two ways:\n\n-    It yields state-of-the-art results without extensive data training typically required by other machine learning methods, and\n-    Provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n\nIts name came from Categorical data and Boost. Since the library is based on gradient boosting machine learning\n\nAdvantages\n\n-    Performance: CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front.\n\n-    Handling Categorical features automatically: We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\n-    Robust: It reduces the need for extensive hyper-parameter tuning and lowers the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature, and others.\n\n-    Easy-to-use: You can use CatBoost from the command line, using a user-friendly API.\n"},{"metadata":{},"cell_type":"markdown","source":"# 7.1 LogisticRegression Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model\ndef train_LR(X, n_iter, n_splits=10):\n\n    print('> Iniciando Treinamento...')\n\n    cleaning_pipeline = Pipeline(steps=[('preproc', PreProcessingTransformer())])\n\n    pipeline = Pipeline(steps=[('feateng', FeatureEngineeringTransformer()),\n                               ('totalpress', TotalPressureTransformer()),\n                               ('catpress', CatBloodPressureTransformer())\n                               ])\n\n    ct = ColumnTransformer([('numerical',MyRobustScalerTransformer(),  ['year_age', 'height', 'weight', 'ap_hi', 'ap_lo', 'total_pressure', 'imc']),\n                           ('categorical',OneHotEncoder(drop='first'), ['cholesterol', 'gluc', 'active', 'gender', 'smoke', 'alco', 'cat_bloodpressure', 'cat_Dwarfism', 'kmeans_cat'])])\n\n    pipeline_final = Pipeline(steps= [('geral',pipeline),\n                                      ('num_cat', ct),\n                                      ('model', LogisticRegression(C=0.5, n_jobs=-1))])\n\n    data = cleaning_pipeline.fit_transform(X)\n    X = data.drop(['cardio'], axis=1)\n    y = data['cardio']\n\n    # Fit the random search model\n    print('> Fitting Modelo...')\n\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n\n        pipeline_final.fit(X_treino, y_treino)\n        pred = pipeline_final.predict(X_valid)\n        Acc = accuracy_score(y_valid, pred)\n        Kappa =  cohen_kappa_score(y_valid, pred)\n        Recall = recall_score(y_valid, pred)\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    print('####### Bussines Metrics #######')\n\n    print('\\n')\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased accuracy: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round((prc_inc/500),2)}')\n    print('\\n')\n\n    print('####### Machine Learning Metrics #######')\n    print(f'Accuracy mean: {np.mean(acc)}')\n    print(f'Accuracy std: {np.std(acc)}')\n\n    print('\\n')\n\n    print(f'kappa mean: {np.mean(kappa)}')\n    print(f'kappa std: {np.std(kappa)}')\n\n    print('\\n')\n\n    print(f'Recall mean: {np.mean(recall)}')\n    print(f'Recall std: {np.std(recall)}')\n\n    print('> Treinamento realizado...')\n    return pipeline_final\n\nmodel_lr = train_LR(X=dfTrain, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.1.1 Changing the Threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discrimination_threshold\ndiscrimination_threshold(model_lr, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the Threshold - Increasing Precision\nnew_predictions = change_threshold_lr(X=X, y=y, model=model_lr, n_splits=10, thresh=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the Threshold - Increasing Recall\nnew_predictions = change_threshold_lr(X=X, y=y, model=model_lr, n_splits=10, thresh=-0.58)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.2 RandomForest Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model\ndef train_RF(X, n_iter, n_splits=10):\n\n    print('> Iniciando Treinamento...')\n\n    cleaning_pipeline = Pipeline(steps=[('preproc', PreProcessingTransformer())])\n\n    pipeline = Pipeline(steps=[('feateng', FeatureEngineeringTransformer()),\n                               ('totalpress', TotalPressureTransformer()),\n                               ('catpress', CatBloodPressureTransformer())\n                               ])\n\n    ct = ColumnTransformer([('numerical',MyRobustScalerTransformer(), ['year_age', 'height', 'weight', 'ap_hi', 'ap_lo', 'total_pressure', 'imc']),\n                           ('categorical',OneHotEncoder(drop='first'), ['cholesterol', 'gluc', 'active', 'gender', 'smoke', 'alco', 'cat_bloodpressure', 'cat_Dwarfism', 'kmeans_cat'])])\n\n\n    pipeline_final = Pipeline(steps= [('geral',pipeline),\n                                      ('num_cat', ct),\n                                      ('model', RandomForestClassifier(n_jobs=-1))])\n\n\n    data = cleaning_pipeline.fit_transform(X)\n    X = data.drop(['cardio'], axis=1)\n    y = data['cardio']\n\n\n    # Fit the random search model\n    print('> Fitting Modelo...')\n\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n\n        pipeline_final.fit(X_treino, y_treino)\n        pred = pipeline_final.predict(X_valid)\n        Acc = accuracy_score(y_valid, pred)\n        Kappa =  cohen_kappa_score(y_valid, pred)\n        Recall = recall_score(y_valid, pred)\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    print('####### Bussines Metrics #######')\n\n    print('\\n')\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased accuracy: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round((prc_inc/500),2)}')\n    print('\\n')\n\n    print('####### Machine Learning Metrics #######')\n    print(f'Accuracy mean: {np.mean(acc)}')\n    print(f'Accuracy std: {np.std(acc)}')\n\n    print('\\n')\n\n    print(f'kappa mean: {np.mean(kappa)}')\n    print(f'kappa std: {np.std(kappa)}')\n\n    print('\\n')\n\n    print(f'Recall mean: {np.mean(recall)}')\n    print(f'Recall std: {np.std(recall)}')\n\n    print('> Treinamento realizado...')\n\n    return pipeline_final\n\nmodel_rf = train_RF(X=dfTrain, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.3 LightGBM Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training LGBM\n\ndef train_lightGBM(X, n_iter, n_splits=10):\n\n    print('> Iniciando Treinamento...')\n\n    cleaning_pipeline = Pipeline(steps=[('preproc', PreProcessingTransformer())])\n\n    pipeline = Pipeline(steps=[('feateng', FeatureEngineeringTransformer()),\n                               ('totalpress', TotalPressureTransformer()),\n                               ('catpress', CatBloodPressureTransformer())\n                               ])\n\n    ct = ColumnTransformer([('numerical',MyRobustScalerTransformer(), ['year_age', 'height', 'weight', 'ap_hi', 'ap_lo', 'total_pressure', 'imc']),\n                           ('categorical',OneHotEncoder(drop='first'), ['cholesterol', 'gluc', 'active', 'gender', 'smoke', 'alco', 'cat_bloodpressure', 'cat_Dwarfism', 'kmeans_cat'])])\n\n    pipeline_final = Pipeline(steps= [('geral',pipeline),\n                                      ('num_cat', ct),\n                                      ('feature_selection', VarianceThreshold(threshold=0.1)),\n                                      ('model', lgb.LGBMClassifier())])\n\n    data = cleaning_pipeline.fit_transform(X)\n    X = data.drop(['cardio'], axis=1)\n    y = data['cardio']\n\n    # Fit the model\n    print('> Fitting Modelo...')\n\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n        pipeline_final.fit(X_treino, y_treino)\n        pred = pipeline_final.predict(X_valid)\n        Acc = accuracy_score(y_valid, pred)\n        Kappa =  cohen_kappa_score(y_valid, pred)\n        Recall = recall_score(y_valid, pred)\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    print('####### Bussines Metrics #######')\n\n    print('\\n')\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased accuracy: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round((prc_inc/500),2)}')\n    print('\\n')\n\n    print('####### Machine Learning Metrics #######')\n    print(f'Accuracy mean: {np.mean(acc)}')\n    print(f'Accuracy std: {np.std(acc)}')\n\n    print('\\n')\n\n    print(f'kappa mean: {np.mean(kappa)}')\n    print(f'kappa std: {np.std(kappa)}')\n\n    print('\\n')\n\n    print(f'Recall mean: {np.mean(recall)}')\n    print(f'Recall std: {np.std(recall)}')\n\n    print('> Treinamento realizado...')\n\n    return pipeline_final\n\nmodel_lgbm = train_lightGBM(X=dfTrain, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.3.1 - Tunning the Hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tunnig Parameters - GridSearch\n\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = model_lgbm\n\n\nparam_grid = {'learning_rate': [0.1, 1, 1.2],\n              'n_estimators': [100, 500, 1000],\n              'num_leaves': [10, 31, 50],\n              'min_child_samples' : [10, 20, 30]\n             }\n\n# Function to tune the parameters\ndef tunnig_gridsearch(Xtrain, model, param_grid, cv, scoring, refit):\n\n    \"\"\"\n    Função para tunnig de parâmetros utilizando o GridSearchCV\n\n    \"\"\"\n\n\n    cleaning_pipeline = Pipeline(steps=[('preproc', PreProcessingTransformer())])\n\n    data = cleaning_pipeline.fit_transform(Xtrain)\n    X_ = data.drop(['cardio'], axis=1)\n    y_ = data['cardio']\n\n\n    search = GridSearchCV(estimator=model.named_steps['model'],\n                          param_grid=param_grid,\n                          scoring=scoring,\n                          refit=refit,\n                          cv=cv,\n                          verbose=1,\n                          n_jobs=-1,\n                          return_train_score=True)\n    search.fit(X_, y_)\n\n    return search.best_params_, search.cv_results_\n\n# Function to retrain with the new parameters and over the all data\ndef retrain_lightGBM(X,valid, params, n_iter, n_splits=10):\n\n    print('> Iniciando Treinamento...')\n\n    cleaning_pipeline = Pipeline(steps=[('preproc', PreProcessingTransformer())])\n\n    pipeline = Pipeline(steps=[('feateng', FeatureEngineeringTransformer()),\n                               ('totalpress', TotalPressureTransformer()),\n                               ('catpress', CatBloodPressureTransformer())\n                               ])\n\n    ct = ColumnTransformer([('numerical',MyRobustScalerTransformer(), ['year_age', 'height', 'weight', 'ap_hi', 'ap_lo', 'total_pressure', 'imc']),\n                           ('categorical',OneHotEncoder(drop='first'), ['cholesterol', 'gluc', 'active', 'gender', 'smoke', 'alco', 'cat_bloodpressure', 'cat_Dwarfism', 'kmeans_cat'])])\n\n    pipeline_final = Pipeline(steps= [('geral',pipeline),\n                                      ('num_cat', ct),\n                                      ('feature_selection', VarianceThreshold(threshold=0.1)),\n                                      ('model', lgb.LGBMClassifier(learning_rate=params['learning_rate'],\n                                                                   n_estimators=params['n_estimators'],\n                                                                   num_leaves=params['num_leaves'],\n                                                                   min_child_samples=params['min_child_samples']\n                                                                   ))])\n\n    data = cleaning_pipeline.fit_transform(X)\n    X = data.drop(['cardio'], axis=1)\n    y = data['cardio']\n\n    # Fit the model\n    print('> Fitting Modelo...')\n\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n        pipeline_final.fit(X_treino, y_treino)\n        pred = pipeline_final.predict(X_valid)\n        Acc = accuracy_score(y_valid, pred)\n        Kappa =  cohen_kappa_score(y_valid, pred)\n        Recall = recall_score(y_valid, pred)\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    # modelo treinado somente nas 60000 obs\n\n    Xvalid = valid.drop(['cardio'], axis=1)\n    yvalid = valid['cardio']\n\n    tunned_pipe = pipeline_final.fit(Xvalid, yvalid)\n\n    # retrain over all data\n    tunned_model = pipeline_final.fit(X, y)\n\n\n    print('####### Bussines Metrics #######')\n\n    print('\\n')\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased accuracy: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round((prc_inc/500),2)}')\n    print('\\n')\n\n    print('####### Machine Learning Metrics #######')\n    print(f'Accuracy mean: {np.mean(acc)}')\n    print(f'Accuracy std: {np.std(acc)}')\n\n    print('\\n')\n\n    print(f'kappa mean: {np.mean(kappa)}')\n    print(f'kappa std: {np.std(kappa)}')\n\n    print('\\n')\n\n    print(f'Recall mean: {np.mean(recall)}')\n    print(f'Recall std: {np.std(recall)}')\n\n    print('> Treinamento realizado...')\n\n    return tunned_model, tunned_pipe\n\n\n\n# Calling the Functions\nbest_params, cv_results = tunnig_gridsearch(Xtrain=dfTrain,\n                                            model=model,\n                                            param_grid=param_grid,\n                                            scoring='accuracy',\n                                            refit='accuracy',\n                                            cv=5)\n\ntunned_model, tunned_pipe = retrain_lightGBM(train, dfValid, params=best_params, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.3.2 Interpreting the models\n\nWith a threshold of 0.40, we can get a new recall of 0.78 with an accuracy of 0.73.\n\nThat means that our model will predict class 1 right (positive) with 78% of the timekeeping accuracy of 0.73.\n\nBusiness-wise it represents an increase of 4.4x in the price reaching R$ 2219.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discrimination_threshold\ndiscrimination_threshold(tunned_pipe, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the threshold - Recall\nnew_pred_prob = change_threshold_lgbm(X=X, y=y, model=tunned_pipe, n_splits=10, thresh=0.40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the threshold - Precision\nnew_pred_prob = change_threshold_lgbm(X=X, y=y, model=tunned_pipe, n_splits=10, thresh=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.4 CatBoost Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CatBoost\ndef train_catBoost(X, n_iter, n_splits=10):\n\n    print('> Iniciando Treinamento...')\n    cleaning_pipeline = Pipeline(steps=[('preproc', PreProcessingTransformer())])\n\n    pipeline = Pipeline(steps=[('feateng', FeatureEngineeringTransformer()),\n                               ('totalpress', TotalPressureTransformer()),\n                               ('catpress', CatBloodPressureTransformer())\n                               ])\n\n    ct = ColumnTransformer([('numerical',MyRobustScalerTransformer(), ['year_age', 'height', 'weight', 'ap_hi', 'ap_lo', 'total_pressure', 'imc']),\n                           ('categorical',OneHotEncoder(drop='first'), ['cholesterol', 'gluc', 'active', 'gender', 'smoke', 'alco', 'cat_bloodpressure', 'cat_Dwarfism', 'kmeans_cat'])\n                           ])\n\n\n    pipeline_final = Pipeline(steps= [('geral',pipeline),\n                                      ('num_cat', ct),\n                                      ('model', CatBoostClassifier(\n                                                            n_estimators=100,\n                                                            depth=6,\n                                                            l2_leaf_reg=0.0,\n                                                            bagging_temperature=1,\n                                                            early_stopping_rounds=100,\n                                                            loss_function='Logloss',\n                                                            eval_metric='Accuracy',\n                                                            verbose=False))])\n\n    data = cleaning_pipeline.fit_transform(X)\n    X = data.drop(['cardio'], axis=1)\n    y = data['cardio']\n\n    # Fit the random search model\n    print('> Fitting Modelo...')\n\n    # cross-validação\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    acc = []\n    kappa = []\n    recall = []\n    for linhas_treino, linhas_valid in skf.split(X, y):\n\n        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n\n        pipeline_final.fit(X_treino, y_treino)\n        pred = pipeline_final.predict(X_valid)\n        Acc = accuracy_score(y_valid, pred)\n        Kappa =  cohen_kappa_score(y_valid, pred)\n        Recall = recall_score(y_valid, pred)\n        acc.append(Acc)\n        kappa.append(Kappa)\n        recall.append(Recall)\n\n    print('####### Bussines Metrics #######')\n\n    print('\\n')\n    acc_inc = np.mean(acc) - 0.50\n    prc_inc = round((acc_inc/0.05)*500, 2)\n    print(f'Increased accuracy: {round(acc_inc,2)}')\n    print(f'Price Increased in: {prc_inc}')\n    print(f'Percentual of Price increassing: {round((prc_inc/500),2)}')\n    print('\\n')\n\n    print('####### Machine Learning Metrics #######')\n    print(f'Accuracy mean: {np.mean(acc)}')\n    print(f'Accuracy std: {np.std(acc)}')\n\n    print('\\n')\n\n    print(f'kappa mean: {np.mean(kappa)}')\n    print(f'kappa std: {np.std(kappa)}')\n\n    print('\\n')\n\n    print(f'Recall mean: {np.mean(recall)}')\n    print(f'Recall std: {np.std(recall)}')\n\n    print('> Treinamento realizado...')\n\n    return pipeline_final\n\nmodel_cat = train_catBoost(X=train, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7.4.1 Interpreting the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discrimination_threshold\ndiscrimination_threshold(model_lgbm, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the threshold - Recall\nnew_pred_prob = change_threshold_lgbm(X=X, y=y, model=model_cat , n_splits=10, thresh=0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing the threshold - Precission\nnew_pred_prob = change_threshold_lgbm(X=X, y=y, model=model_cat , n_splits=10, thresh=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.0 API_Predictions\nIn our example the prediction was class 0, therefore no heart diseases in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example Data\ntest_data = X.sample()\n\n# Tunnig into a json\ndf_json = test_data.to_json(orient='records')\ndf_json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.1 API_Predictions - Normal Threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"# url = 'http://0.0.0.0:5000/predict'\nurl = 'https://pa001-app.herokuapp.com/predict'\ndata = df_json\nheader = {'Content-type': 'application/json'}\n\n# Request\nr = requests.post(url=url, data=data, headers=header)\nprint(r.status_code)\nr.json()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.2 API_Predictions - Impossing Threshold (0.4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# url = 'http://0.0.0.0:5000/predict_thresh'\nurl = 'https://pa001-app.herokuapp.com/predict_thresh'\ndata = df_json\nheader = {'Content-type': 'application/json'}\n\n# Request\nr = requests.post(url=url, data=data, headers=header)\nprint(r.status_code)\nr.json()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}