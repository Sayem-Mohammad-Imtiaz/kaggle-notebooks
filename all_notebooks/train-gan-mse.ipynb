{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Training GAN with pretrained models"},{"metadata":{},"cell_type":"markdown","source":"A DCGAN model is trained for image enhancement(superres, decrappify). <br>The dataset used is Flicker Image Dataset, availabe on Kaggle.\nFor training the model synthetic data is generated as in kernel: https://www.kaggle.com/greenahn/crappify-imgs<br>and saved to disk, which in conjunction with high resolution images are used to train the model.<br><br>\nFor more details, find the github repository at: https://github.com/nupam/GANs-for-Image-enhancement"},{"metadata":{},"cell_type":"markdown","source":"Both pretrained generator and discriminator models are loaded from disk, output file of kernel,<br> https://www.kaggle.com/greenahn/pretrain-gan-mse.<br>\nThey are then put together as a GAN, and trained."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\nfrom fastai.vision.gan import *\nimport gc\nfrom torchvision.models import vgg16_bn","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## These folders contain crappy images in different resolution with differnt crappafication logic (randomly selected)\norig_path = Path('../input/flickrproc/hr/hr')\nfnames_df = pd.read_csv('../input/flickrproc/files.csv')\nbs = 16\nFOLDERS = {256:Path('../input/flickrproc/crappy_256/crappy/'), 320:Path('../input/flickrproc/crappy_320/crappy/'), }\nFOLDERS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"##loading training data\n## if dummy=True is provided, then dataset of ony 32 images is retured\ndef get_data(size=None, bs=None, folder=320, split=0.9, dummy=False):\n    if dummy:\n        if bs is None: bs = 1\n            \n        if size is None: \n            data = ImageImageList.from_df(fnames_df.iloc[:32], path = FOLDERS[320], cols='name').split_by_rand_pct(0.2, seed=34).label_from_func(lambda x: orig_path/Path(x).name).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n        else:\n            data = ImageImageList.from_df(fnames_df.iloc[:32], path = FOLDERS[320], cols='name').split_by_rand_pct(0.2, seed=34).label_from_func(lambda x: orig_path/Path(x).name).transform([], size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n        data.c = 3\n        return data\n    \n    if bs is None: \n        raise ValueError('Batchsize is not provided')\n    if size is None:\n        raise ValueError('Size of image is not provided')\n    \n    folder = FOLDERS[folder]\n    src = ImageImageList.from_df(fnames_df, \n                           path = folder, cols='name')\n    src = src.split_by_idx(np.arange(int(src.items.shape[0]*split), src.items.shape[0]))\n    \n    data = src.label_from_func(lambda x: orig_path/Path(x).name).transform(get_transforms(max_zoom=1.2), size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gen = get_data(128,bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = 1e-3\ny_range = (-3.,3.)\nloss_gen = MSELossFlat()\narch = models.resnet34\n\ndef create_gen_learner():\n    return unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,\n                         self_attention=True, y_range=y_range, loss_func=loss_gen, model_dir=\"/kaggle/working\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/pretrain-gan-mse/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen = create_gen_learner()\nlearn_gen.load(Path('/kaggle/input/pretrain-gan-mse/gen_pre'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How good is pretrained model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results(rows=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading critic"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_critic_data(bs, size=256, split=0.9):\n    \n    def labeler(x):\n        ret = 'generated' if Path(x).parent.name == 'crappy' else 'original'\n        return ret\n    \n    df = fnames_df\n    valid_names = list(df['name'].iloc[int(split*len(df)):])\n    \n    src1 = ImageList.from_df(df, path = Path('../input/flickrproc/crappy_320')/'crappy', cols='name')\n    src2 = ImageList.from_df(df, path = orig_path, cols='name')\n    src1.add(items=src2)\n    \n    src = src1.split_by_valid_func(lambda x : Path(x).name in valid_names)\n    data = src.label_from_func(labeler)\n    data = data.transform(get_transforms(), size=size).databunch(bs=bs).normalize(imagenet_stats)\n    \n    data.c = 3\n    return data\n\ndata_critic = get_critic_data(bs, 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\ndef create_critic_learner(data, metrics):\n    return   Learner(data_critic, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd, model_dir=\"/kaggle/working\")\nlearn_critic = create_critic_learner(data_critic, accuracy_thresh_expand)\nlearn_critic.load('../input/pretrain-gan-mse/critic-pre')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GAN\n**Putting both models together as a GAN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training is done by adaptiveliy switching between discriminator and generator.<br>Discriminator is trained whenever discriminator loss drops below 0.65."},{"metadata":{"trusted":true},"cell_type":"code","source":"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\nlearn = GANLearner.from_learners(learn_gen, learn_critic, weights_gen=(1.,50.), show_img=True, switcher=switcher,\n                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd, model_dir=\"/kaggle/working\", gen_first=True)\nlearn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 2e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit(8,lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.show_results(rows=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.save('gen-128')\nlearn_critic.save('critic-128')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del learn.data\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Increasing image size and training again"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.data = get_data(256,bs//2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit(7,lr/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.show_results(rows=10, figsize=(30, 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving models"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.save('gen-256')\nlearn_critic.save('critic-256')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.export(\"/kaggle/working/export.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}