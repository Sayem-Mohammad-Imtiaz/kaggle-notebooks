{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# importing required packages for building classification machine learning model\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.tree import export_graphviz\nimport six\nimport pydot\nfrom sklearn import tree\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import accuracy_score, confusion_matrix #to check the accuracy of the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ca51f1e730edbc3dccc1772ff958b09001f2813"},"cell_type":"code","source":"# reading forest types datasets which is downloaded from UCI\n# https://archive.ics.uci.edu/ml/datasets/Forest+type+mapping\n# download the data and upload it to Kaggle by clicking \"Add Data\"\ndf_train = pd.read_csv('../input/training.csv')\ndf_Test = pd.read_csv('../input/testing.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1abde72afec20a5a6de845025d72c700e93eb563"},"cell_type":"code","source":"# displaying top 5 records to check whether its reading properly from kaggle server\n# here the first column 'class' is the output and rest of the columns are inputs\n# so we need to split the datasets into two\n# 1 dataset contains only output, (i.e) column 'class'\n# 2 dataset will have rest of the columns\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868307faa747c2eec0aa45382f18016241e23054"},"cell_type":"code","source":"df_train['class'].unique()\n# Class: 's' ('Sugi' forest), 'h' ('Hinoki' forest), 'd' ('Mixed deciduous' forest), 'o' ('Other' non-forest land)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7766ff1338c271100bb769ab03bb8b04f6b66015"},"cell_type":"code","source":"X_train = df_train.iloc[:, 1:].values # extracting inputs from training dataset - column 1 to till end\ny_train = df_train.iloc[:, 0].values # extracting output from training dataset - column 0\n\nX_test = df_Test.iloc[:, 1:].values # extracting inputs from testing dataset - column 1 to till end\ny_test = df_Test.iloc[:, 0].values # extracting output from training dataset - column 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f0a3e8509952ac9aad7d90a9484652b4cb350e"},"cell_type":"code","source":"# Feature Scaling - Our dataset values are not scaled, (i.e) columns values are not in specific range\n# By applying feature scaling - the sum of values in all columns will be zero\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c58d883086c1bd2a3bdc597096e11781f71c8bfc"},"cell_type":"markdown","source":"**Implementing through Logistic Regression Algorithm which has highest accuracy - 86%**"},{"metadata":{"trusted":true,"_uuid":"d1ed2110b6b837a010c359e605ee7fb6f96c39f0"},"cell_type":"code","source":"# implementing through logistic regression algorithm with penalty l1 and liblinear solver - 86%\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty ='l1',solver='liblinear',\n                                multi_class='ovr', C=6)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13e23b19c95194efe6541ce8a76ddc943bfc919e"},"cell_type":"code","source":"# displaying confusion matrix for calculating accuracy\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f35870e3df04a01b0ddda3b21b0472d28012122"},"cell_type":"code","source":"# implementing through random forest algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"738bfda10641b3462857b76540382345a126fe19"},"cell_type":"code","source":"# to check feature importance\nfeature_imp = pd.Series(classifier.feature_importances_, index= df_train.columns[1:]).sort_values(ascending=False)\nsns.barplot(x=feature_imp, y =feature_imp.index)\nplt.xlabel('Feature importance Score')\nplt.ylabel('Features')\nplt.title('Visualizing Important Features')\nplt.legend(handles=[])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6698ef1749568e47dce61a00a17fd0f0cdb03c7c"},"cell_type":"code","source":"# Saving inner decision trees generated by random forest algorithm\ncol = df_train.columns[1:]\ndotfile = six.StringIO()\ni_tree = 1\nfor tree_in_forest in classifier.estimators_:\n    export_graphviz(tree_in_forest,out_file='tree.dot',\n    feature_names=col,\n    filled=True,\n    rounded=True)\n    (graph,) = pydot.graph_from_dot_file('tree.dot')\n    name = 'tree' + str(i_tree)\n    graph.write_png(name+  '.png')\n    os.system('dot -Tpng tree.dot -o tree.png')\n    i_tree +=1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"519f748dbaa31078a514b50e7ed4cff985a1f134"},"cell_type":"code","source":"# Showing inner decision trees generated by random forest algorithm\nfig=plt.figure(figsize=(50, 50), dpi=150, facecolor='w', edgecolor='k')\ncolumns = 2\nrows = 5\nfor i in range(1, 2):\n    img = mpimg.imread('tree' + str(i) + '.png')    \n    fig.add_subplot(rows, columns, i)\n    plt.title('Random Forest Decision Tree' + str(i))\n    plt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"626e5401061d399d4e2783a46b13554f8bfa6c92"},"cell_type":"markdown","source":"**Implementing through other algorithm which has accuracy less than random forest**"},{"metadata":{"trusted":true,"_uuid":"ff967a485772bda1860114e116c98926eb3d6f46"},"cell_type":"code","source":"# implementing through naive bayes algorithm - 80%\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26cbee2451ae4399eab988a83ae8911d222fc395"},"cell_type":"code","source":"# implementing through XGB classifier algorithm - 78%\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f00bef3649508ec992244775f6ed6b84c43cf5ef"},"cell_type":"code","source":"# implementing through decision tree algorithm - 76%\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}