{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I. Importing data and required libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install demoji","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.utils as ku\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport re\nimport demoji\nimport emoji\ndemoji.download_codes()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Some EDA(not really)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"virgin_neg, virgin_eq, virgin_pos = (data['airline_sentiment'].loc[data['airline']=='Virgin America'].value_counts())\nunited_neg, united_eq, united_pos = (data['airline_sentiment'].loc[data['airline']=='United'].value_counts())\nsouthwest_neg, southwest_eq, southwest_pos = (data['airline_sentiment'].loc[data['airline']=='Southwest'].value_counts())\ndelta_neg, delta_eq, delta_pos = (data['airline_sentiment'].loc[data['airline']=='Delta'].value_counts())\nUSairways_neg, USairways_eq, USairways_pos = (data['airline_sentiment'].loc[data['airline']=='US Airways'].value_counts())\namerican_neg, american_eq, american_pos = (data['airline_sentiment'].loc[data['airline']=='American'].value_counts())\n\nprint('Sentiment type by airline:\\n')\nprint('Virgin America: positive =',virgin_pos, 'negative =',virgin_neg, 'neutral =', virgin_eq)\nprint('United: positive =',united_pos, 'negative =',united_neg, 'neutral =', united_eq)  \nprint('Southwest: positive =',southwest_pos, 'negative =',southwest_neg, 'neutral =', southwest_eq)\nprint('Delta: positive =',delta_pos, 'negative =',delta_neg, 'neutral =', delta_eq)\nprint('US Airways: positive =',USairways_pos, 'negative =',USairways_neg, 'neutral =', USairways_eq)\nprint('American: positive =',american_pos, 'negative =',american_neg, 'neutral =', american_eq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('Airline')\nplt.ylabel('Sentiment count')\nX = ['Virgin America', 'United', 'Southwest', 'Delta', 'US Airways', 'American']\npositive = [virgin_pos, united_pos, southwest_pos, delta_pos, USairways_pos, american_pos]\nnegative = [virgin_neg, united_neg, southwest_neg, delta_neg, USairways_neg, american_neg]\nneutral = [virgin_eq, united_eq, southwest_eq, delta_eq, USairways_eq, american_eq]\nbr1 = np.arange(6) \nbr2 = [x + 0.25 for x in br1] \nbr3 = [x + 0.25 for x in br2]\n\nplt.bar(br1, positive, color='b', label='positive', width=0.25)\nplt.bar(br2, negative, color='r', label='negative', width=0.25)\nplt.bar(br3, neutral, color='g', label='neutral', width=0.25)\nplt.xticks([r+0.25 for r in range(6)], X)\nplt.title('Types of sentiment count per airline:')\nplt.legend()\nplt.grid()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Text Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"StopWords = set(stopwords.words('english'))\n\ndef text_preprocess(text):\n    #text = ' '.join([word.lower() for word in text.split() if word.lower() not in StopWords])\n    text = ' '.join([word for word in text.split() if ('@' not in word and 'http' not in word and 'https' not in word and '#' not in word)])\n    trans = str.maketrans('','',string.punctuation)\n    text = text.translate(trans)\n    return text\n\ndata['text'] = data['text'].apply(text_preprocess)\ndata.head()        \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV. Tokenization and Lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['text']\nX = X.tolist()\ndef lemmatize(data):\n    lemmatizer = WordNetLemmatizer()\n    lem_data = []\n    for text in data:\n        lem_text = ''\n        for word in text.split():\n            word = lemmatizer.lemmatize(word)\n            word = lemmatizer.lemmatize(word, pos='v')\n            lem_text = lem_text + ' ' + word\n        lem_data.append(lem_text)\n    return lem_data\n        \nX_lem = lemmatize(X)\nprint(X_lem)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling emojis in data, replacing them with them corresponding text words.\nemoji2text = {}\nfor text in X_lem:\n    emoji2text.update(demoji.findall(text))\n    \nemoji2text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_lem_demoji = []\nfor text in X_lem:\n    for emoji, txt in emoji2text.items():\n        if emoji in text:\n            text = re.sub(emoji, txt+' ', text)\n    X_lem_demoji.append(text)\nX_lem_demoji","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(data['airline_sentiment'])\nX_train, X_test, y_train, y_test = train_test_split(X_lem_demoji, y, random_state=0)\ny_train_true = y_train\ny_train = ku.to_categorical(y_train, num_classes=3)\ny_test_true = y_test\ny_test = ku.to_categorical(y_test, num_classes=3)\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nvocab_size = len(tokenizer.word_index)\nmax_len = 100\ntrain_seq = tokenizer.texts_to_sequences(X_train)\ntrain_pad = pad_sequences(train_seq, maxlen=max_len)\ntest_seq = tokenizer.texts_to_sequences(X_test)\ntest_pad = pad_sequences(test_seq, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_dim=300, lr=0.001, nn1=32, nn2=32):\n    model = keras.Sequential()\n    model.add(keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_len))\n    model.add(keras.layers.SpatialDropout1D(0.2))\n    model.add(keras.layers.Bidirectional(keras.layers.LSTM(nn1, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n    #model.add(keras.layers.Bidirectional(keras.layers.LSTM(nn2, dropout=0.3, recurrent_dropout=0.3)))\n    #model.add(keras.layers.Dense(embedding_dim, activation='relu'))\n    model.add(keras.layers.Dense(3, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n    return model\n\ncv_model = KerasClassifier(build_fn=build_model, epochs=10, batch_size=512)\nnn1 = [64,128,256]\n#nn2 = [32,64,128]\nlr = [0.001,0.01,0.1]\nembedding_dim = [300,250,200,150]\nbatch_size = [64,128,256,512]\nparam_grid = dict(embedding_dim=embedding_dim, lr=lr, batch_size=batch_size, nn1=nn1)\ngrid = RandomizedSearchCV(estimator=cv_model, param_distributions=param_grid, n_jobs=1, cv=3)\n\ncv_results = grid.fit(train_pad, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V. Training model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nmodel = keras.Sequential([\n    keras.layers.Embedding(vocab_size+1, 150, input_length=max_len),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n    #keras.layers.Bidirectional(keras.layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3)),\n    #keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dense(3, activation='softmax')\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nmy_callbacks = [EarlyStopping(monitor='val_loss', patience=3, mode='min')]\nhistory = model.fit(train_pad, y_train, epochs=10, batch_size=256, validation_data=(test_pad, y_test), callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(test_pad)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy with Bi-LSTM neural network\nprint(accuracy_score(y_test_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tfidf Vectorization, predictions using Logistic Regression.\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.9)\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n\nclf = LogisticRegression(max_iter=1000).fit(X_train_tfidf, y_train_true)\ny_pred = clf.predict(X_test_tfidf)\nprint('Accuracy with Logistic Regression and tfidf Vectorization',accuracy_score(y_test_true, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}