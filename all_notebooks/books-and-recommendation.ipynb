{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport datetime\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv = pd.read_csv('/kaggle/input/goodreadsbooks/books.csv', error_bad_lines=False)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fn = rsv.authors.unique()\nthresh = 0\nwriters = dict()\nfor f in fn:\n    fc = rsv[rsv.authors == f].bookID.count() \n    if fc >= thresh: \n        writers[f] = fc","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"writers = dict(sorted(writers.items(), key=lambda item: item[1], reverse=True)[:10])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple Counting of writers\nThus obtained graph is shown as follows\nshowing that P.G. Woodhouse and Stephen King are equally dominating","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40, 40))\nplt.barh(range(len(writers)), list(writers.values()), align='center')\nplt.yticks(range(len(writers)), list(writers.keys()), fontsize=30)\nplt.show();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question mark \nON the Authors columns, Clearly multiple authors are seprated by a \"/\" which is \"incorrect\" for direct\ndata analysis. So after figuring out a \"function\" which does the following:->\n* It looks for '/' in the authors\n* Seprates them if normally acessed\n* But if the book is already seen, maximum number of writers are considered\n* Hence we find that actual count was incorrect\n* Also includes single writers\n","metadata":{}},{"cell_type":"code","source":"title_arr = []\nwriters_arr = []\nsingle_writer = []\nfor f in fn:\n    if '/' in f:\n        try:\n            book = rsv[rsv.authors == f].title.values[0]\n            if book in title_arr:\n                if len(writers_arr[title_arr.index(book)]) < len(f.split('/')):\n                    writers_arr[title_arr.index(book)] = f.split('/')\n            else:\n                title_arr.append(book)\n                writers_arr.append(f.split('/'))\n        except IndexError:\n            title_arr.append(rsv[rsv.authors == f].title.values[0])\n            writers_arr.append(f.split('/'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(title_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(title_arr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_writes = set()\ncount = 0\nmax_len = 0\nfor x,y  in zip(title_arr, writers_arr):\n    if len(y) > max_len:\n        max_len = len(y)\n    for z in y:\n        unique_writes.add(z)\n        count+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in writers.keys():\n    if '/' not in k:\n        unique_writes.add(k)\n        count+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing the unique writers\n* We can see there are 7123 unique writers among 10464\n* Thus giving us a way to categorize","metadata":{}},{"cell_type":"code","source":"len(unique_writes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_books = dict.fromkeys(unique_writes, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x,y in zip(title_arr, writers_arr):\n    for z in y:\n        get_books[z]+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in writers.items():\n    if '/' not in x[0]:\n        get_books[x[0]]+=x[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking into Titles\nWe find that twilight has maximum reviews and rating of 3.59\nAlthough max rating among top 10 is of Harry Potter and Half blood Prince","metadata":{}},{"cell_type":"code","source":"rsv.sort_values(by=['ratings_count'],\n                ascending=False).loc[:,(\"title\",\"average_rating\")][:10]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_books = dict(sorted(get_books.items(), key=lambda item: item[1], reverse=True))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The correct visualization\nThe visualization is perfect as everything is taken into account, hence Seeing Stephen King as a dominant","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40, 40));\nplt.barh(range(10), list(get_books.values())[:10], align='center')\nplt.yticks(range(10), list(get_books.keys())[:10], fontsize=30)\nplt.xticks(range(100), range(100), fontsize=25, rotation=90)\nplt.show();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dates = []\nfor x in rsv.publication_date.values:\n    try:\n        dates.append(pd.to_datetime(datetime.datetime.strptime(x,\"%m/%d/%Y\")))\n    except:\n        dates.append(pd.to_datetime(datetime.datetime.now()))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv[\"New dates\"] = dates","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Incorrect dates\nAs you can see only 2 dates are incorrect in the dateset which can effect the anaylsis","metadata":{}},{"cell_type":"code","source":"rsv.sort_values(by='New dates',ascending=False).loc[:,(\"title\",\"New dates\")]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv[\"Rating_Interval\"] = pd.DataFrame(pd.cut(rsv.average_rating, 5, [0.0,1.0,2.0,3.0,4.0,5.0]))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings = dict()\nfor i in range(0,5,1):\n    ratings[str(i)+\" to \"+str(i+1)] = (rsv[\"Rating_Interval\"] == pd.Interval(left = float(i), right = float(i+1))).sum()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del ratings['0 to 1']","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(40, 40));\nplt.barh(range(len(ratings)), list(ratings.values()), align='center')\nplt.yticks(range(len(ratings)), list(ratings.keys()), fontsize=30)\n# plt.yticks(range(50), range(50), fontsize=30)\nplt.show();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ratings\nFrom the above graph we can clearly see that the dominating rating is\n3 to 4, moreover the average lies between 3 to 4.\nThis is skewed data, as well an indication that the \ndataset given has more 3 to 5 star books within the sample","metadata":{}},{"cell_type":"code","source":"lang = dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv.language_code.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lang[\"eng\"] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skip = [\"eng\",\"en-US\",\"en-CA\",\"en-GB\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unq_lang = rsv.language_code.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in unq_lang:\n    if x not in skip:\n        lang[x] = rsv[rsv.language_code == x].bookID.count()\n    if x in skip:\n        lang[\"eng\"]+=rsv[rsv.language_code == x].bookID.count()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(40, 40))\nplt.barh(range(len(lang)), width=sorted(list(lang.values()))[::-1], align='center')\nplt.yticks(range(len(lang)), list(lang.keys()), fontsize=30)\nplt.show();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observing Above graph\nEnglish is most dominating language,\nbut in the next bar graph Down below if\nEnglish is removed we see graph looks normal\nand not totally dominant by a single language","metadata":{}},{"cell_type":"code","source":"del lang[\"eng\"]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(40, 40))\nplt.barh(range(len(lang)), width = sorted(list(lang.values()))[::-1], align='center')\nplt.yticks(range(len(lang)), list(lang.keys()), fontsize=30)\nplt.show();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv = rsv.set_index('bookID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wrong dates\nBookId\n45531\n31373","metadata":{}},{"cell_type":"code","source":"rsv = rsv.drop([45531, 31373])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = rsv.publisher.value_counts()\nl = l[l >= 20]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nplt.barh(range(10), width=l[:10])\nplt.yticks(range(10), l.index.values[:10], fontsize= 25)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate(word):\n    return ord(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\nThis function is for encoding Authors, This function was made after a lot of thought such that to have less than 5 % clashes","metadata":{}},{"cell_type":"code","source":"def encode(strs):\n    if '/' in strs:\n        final_sum = 0\n        intr = strs.split('/')\n        for s in intr:\n            noramlize = len(s)\n            summation = sum([calculate(x) for x in s])\n        final_sum+=(summation/noramlize)\n        return final_sum\n    else:\n        return sum([calculate(x) for x in strs])/len(strs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv.insert(2,'Encoded authors', rsv.authors.apply(encode))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode(rsv.authors.values[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding titles\nThis function was made to stand out with log and bias to length of the title,****Because**** as follows:->\n* A title depends on it's length(talking syntaxically)\n* And moreover it depends upon arrangement of words **such  as(\"a after p\" or \"p after a\") are different things**.","metadata":{}},{"cell_type":"code","source":"def encode_title(tt):\n    total = 0\n    for w in tt.split(' '):\n        if len(w) == 0:\n            continue\n        total+= sum([calculate(x)*math.log2(lg + len(w)) for lg,x in enumerate(w)])/ len(w)\n    return total/len(tt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor x in rsv.title.values:\n    l.append(encode(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(l), len(set(l))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv.insert(1,'Encoded_titles',rsv.title.apply(encode_title)/rsv.average_rating.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rsv.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Constructing x\n* For KMeans Clustering we need only numerical type data\n* And for that we need to eliminate string or textual data","metadata":{}},{"cell_type":"code","source":"rsv.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = rsv[['Encoded_titles', 'Encoded authors', 'average_rating', '  num_pages','ratings_count','text_reviews_count', 'New dates']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['New dates'] = x['New dates'].astype(np.int64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['New dates'].astype(np.float64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = x.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = x.drop('bookID',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Date conversion","metadata":{}},{"cell_type":"code","source":"x['New dates'] = x['New dates'] //  10**12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The dtypes\nAll of them are numeric","metadata":{}},{"cell_type":"code","source":"x.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = x[x['New dates'] > 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adjusting\n* Dates were < 0 in timestamp format\n* The Encoded Titles had an infinity Thanks to [This guy](https://www.kaggle.com/carlosdg) for helping me.","metadata":{}},{"cell_type":"code","source":"x = x[x['Encoded_titles'] != np.inf]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Doing elbow method for n\n* Looking for wcss which is optimal and hence obtain correct amount of categoires.\n* **Weighted Cumlatice shared sum** is the formula which takes the weights of all the features in account\n* This weight is minimized with optimum value by looking at an elbow(shape in graph)","metadata":{}},{"cell_type":"code","source":"wcss = [] \nfor i in range(1,11):\n    clusters = KMeans(n_clusters = i, random_state = 42) \n    clusters.fit(x.values)\n    wcss.append(clusters.inertia_)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1,11),wcss,'b')\nplt.title('This is Clustering')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.plot(4,wcss[3],'.',mew = 4, ms =8,color = 'r')\nplt.annotate(xy = [4,wcss[3]],s='(%.2f , %.1f) Seems to be Optimal'%(4,wcss[3]))\nplt.vlines(4,0,wcss[3],linestyle='dashed')\nplt.hlines(wcss[3],0,4,linestyle='dashed')\nplt.xlim(0,None)\nplt.ylim(0,None)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters = KMeans(n_clusters=4, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x['cluster'] = clusters.fit_predict(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Suggestion Logic","metadata":{}},{"cell_type":"markdown","source":"## Find Mins\n* The function finds 10 minimum from a threshold value\n* and then get the indexes of them from dataframe\n* So the encoded values from a text based entry is matched\n* The logic bieng that the similiar texts are encoded equally","metadata":{}},{"cell_type":"code","source":"def find_mins(df, thresh):\n    indexes = dict()\n    for i in range(df.shape[0]):\n        indexes[df.iloc[i,1]] = i\n    one = []\n    two = []\n    for x,y in indexes.items():\n        if x <= thresh:\n            one.append(x)\n        else:\n            two.append(x)\n    one = sorted(one, reverse=True)[:10]\n    two = sorted(two)[:10]\n    get_one = [df.iloc[indexes[val]] for val in one] \n    get_two = [df.iloc[indexes[val]] for val in two]\n    see = pd.DataFrame((get_one + get_two))\n    return see","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find correct values\n* This function tries to find the colums which have similiarty with Encoded titles\n* Also if n is greater than the total space of the selected items it returns all selected\n* Sorting with **number of Pages** so that we get most fat book","metadata":{}},{"cell_type":"code","source":"def find_inarr(df,n):\n    lol = []\n    for i in range(df.shape[0]):\n        lol.append(rsv[rsv['Encoded_titles'] == df.iloc[i, 0]])\n    lol = pd.concat(lol)\n    if n > lol.shape[0]:\n        n = lol.shape[0]\n    return lol.sort_values([\"  num_pages\"], ascending=False).iloc[:n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main Suggestion Function\n**Before any of below a clustering is done already, so we do this within the same cluster**\n* This function is the actual suggestion function\n* It first finds the same author as the book passed\n* IF yes, returns the minium **diff value** that is matematically the nearest point\n* IF no, we construct a new dataframe for final according to above functions\n* When these are parse the values are sorted with **Titles** **Rating** and **Date Published**\n* And final is to returned according to number of pages","metadata":{}},{"cell_type":"code","source":"def suggest(df,n=1):\n    selector = x[x['cluster'] == df.iloc[-1]]\n    finals = selector[selector['Encoded authors'] == df['Encoded authors']]\n    if finals.shape[0] == 1:\n        finals = find_mins(selector, df['Encoded authors'])\n    middle = finals - df\n    suggest = middle.abs().sort_values([\"Encoded_titles\",\"average_rating\",\"New dates\"]).iloc[:-1,:]\n    suggest = finals.loc[suggest.index]\n#     print(suggest.iloc[:, 0])\n    return find_inarr(suggest,n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Real","metadata":{}},{"cell_type":"code","source":"rsv[rsv['Encoded_titles'] == x.iloc[1311,0]]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Suggested","metadata":{}},{"cell_type":"code","source":"suggest(x.iloc[1311], 2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Real","metadata":{}},{"cell_type":"code","source":"rsv[rsv['Encoded_titles'] == x.iloc[5,0]]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Suggested","metadata":{}},{"cell_type":"code","source":"suggest(x.iloc[5])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}