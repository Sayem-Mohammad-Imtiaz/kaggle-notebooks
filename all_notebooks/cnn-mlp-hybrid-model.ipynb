{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **CNN + MLP Model in PyTorch**","metadata":{}},{"cell_type":"markdown","source":"### **Introduction**\n\nThe following model is intended for predicting house prices based on input images and corresponding numeric data such as beds, baths, etc. There are a few options for achieving this, and here we have chosen to use a hybrid CNN + MLP model. The CNN routinely handles the images, and the numeric data is handled by the MLP. The outputs of the MLP and the fully connected (FC) layers of the CNN are concatenated into another set of FC layers, which have a final output of a price prediction in the continuous range of $[0,1]$. The model is purposely lightweight in order to run efficiently with the goal of squeezing as much performance as possible out of the system, without large computational demands. Due to the small model size, a 2X performance improvement is obtained with GPU, but not much more. Note that 20 epochs takes about 4 minutes on GPU.\n\n![CNN_MLP_Hybrid_Arch.png](attachment:1c2f12b3-d64b-4aa4-9bf2-545e1f562abb.png)","metadata":{},"attachments":{"1c2f12b3-d64b-4aa4-9bf2-545e1f562abb.png":{"image/png":"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"}}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n# Created on Wed Jun 9 2021\n\n# @author: mbadal1996\n\n\n# ====================================================\n# CNN + MLP Model for Image + Numeric Data \n# ====================================================\n\n# Comments:\n# The following Python code is a hybrid CNN + MLP \n# classifier for combined image data + numeric features \n# (meta-data) which further describe the images.The \n# output of the model is a continuous float value in the \n# range [0,1] which is due to normalization of the \n# training label. In that sense it is a regression as \n# opposed to a classification. The original purpose of \n# the code was to make predictions on housing prices \n# (see So-Cal Housing in Kaggle) but this kind of hybrid\n# model is useful for various other problems where \n# both images and numeric features are combined. In the \n# event that a binary or multi-class output is desired \n# (instead of a float value regression), then the final \n# output layer of the CNN+MLP should be modified for the \n# number of classes and then passed through a softmax \n# function.\n\n# As an example, the house features (numeric data) CSV \n# file is also included in the repository so that the \n# user can see the format. House images are not included\n# since they are too many and can be easily downloaded \n# from Kaggle at:\n\n# https://www.kaggle.com/ted8080/house-prices-and-images-socal\n\n# Useful content at PyTorch forum is acknowledged for \n# combining images and numeric data features. \n\n# -----------------------------------------------------\n\n# IMPORTANT NOTE:\n# When organizing data in folders to be input to \n# dataloader, it is important to keep in mind the \n# following for correct loading:\n\n# 1) The train and validation data were separated into \n# their own folders by hand by class (one class: house) c\n# alled 'socal_pics/train' and 'socal_pics/val'. That \n# means the sub-folder 'train' contains one folder: house.\n# The same is true for the val data held in the folder \n# 'socal_pics/val'. So the organization looks like:\n\n# socal_pics > train > house\n# socal_pics > val > house\n\n# Place the metadat CSV file in same folder as Python \n# script\n\n# 2) The test data is organized differently since there \n# are no labels for those images. Instead, the test data\n# are held in the folder 'socal_pics/test' where the \n# sub-folder here 'test' just contains one folder called\n# 'test'. This is instead of the 'house' folder. So the\n# organization looks like:\n\n# socal_pics > test > test\n\n# ======================================================","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:53.867123Z","iopub.execute_input":"2021-08-02T05:33:53.867499Z","iopub.status.idle":"2021-08-02T05:33:53.873815Z","shell.execute_reply.started":"2021-08-02T05:33:53.867418Z","shell.execute_reply":"2021-08-02T05:33:53.872818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Imports and Parameters**\n\nBelow we perform standard imports, choose computing device, define parameters, and set a seed for reproducible results. Note that due to this, the shuffle feature in the data loader is chosen as False to prevent overriding seed. This also ensures that the numeric data loader (defined below) will be in step with the image loader. Approximate timer is also initialized.  ","metadata":{}},{"cell_type":"code","source":"# ====================================================\n\n# Python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\n# Pytorch\nimport torch\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\n# Choose Device (GPU or CPU)\ndev = 'cuda'\n# dev = 'cpu'\n\n# =====================================================\n# Parameters\n\n# Image Parameters\nCH = 3  # number of channels\nratio = 1.5625  # width/height ratio to resize images\nimagewidth = 157  # square dimension (size x size) \nimageheight = int(np.floor(imagewidth/ratio))\ncropsize = imageheight\n#cropsize = imagewidth\n\n\n# Neural Net Parameters\nlearn_rate = 1e-3  \nnum_epochs = 20  # At least 20 epochs for 100x100 images\nbatch_size = 100  \n\n\n# Seed for reproduceable random numbers (eg weights \n# and biases). NOTE: Seed will be overidden by using\n# image transforms like random flip or setting \n# shuffle = True in data loader.\ntorch.manual_seed(1234)\n\n\n# Start Timing Code  (only approximate)\ntic = time.perf_counter()\n\n# ======================================================","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:53.875114Z","iopub.execute_input":"2021-08-02T05:33:53.87535Z","iopub.status.idle":"2021-08-02T05:33:55.164887Z","shell.execute_reply.started":"2021-08-02T05:33:53.875326Z","shell.execute_reply":"2021-08-02T05:33:55.164001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Image Transforms**\n\nBelow we set up transforms for the housing images which may or may not be of equal size (which they must be for the CNN). They also need to be reduced in resolution and cropped. Simply for convenience, a size of 100x100 pixels is chosen according to the parameters above. Note that 157/1.5625 = 100.\n\nThe images are not color normalized but this is a potentially useful addition that can be employed to improve model learning.","metadata":{}},{"cell_type":"code","source":"# ======================================================\n\n# Image Transforms:\n# Create transforms for training data augmentation. \n# In each epoch, random transforms will be applied \n# according to the Compose function. They are random \n# since we are explicitly choosing \"Random\" versions \n# of the transforms. To \"increase the dataset\" one \n# should run more epochs, since each epoch has new \n# random data.\n# NOTE: Augmentation should only be for train data.\n# NOTE: For augmentation transforms, best to use \n# larger batches\n\n# Transform for training data\ntransform_train = transforms.Compose([\n        transforms.Resize([imageheight, imagewidth]),\n        transforms.CenterCrop(cropsize),\n        #transforms.RandomHorizontalFlip(p=0.5),\n        #transforms.RandomRotation(degrees = (-20,20)), \n        #transforms.RandomVerticalFlip(p=0.5),\n        transforms.ToTensor()])\n\n# Transform for validation data\ntransform_val = transforms.Compose([\n        transforms.Resize([imageheight, imagewidth]),\n        transforms.CenterCrop(cropsize),\n        transforms.ToTensor()])\n\n# Transform for test data\n#transform_test = transforms.Compose([\n#        transforms.Resize([imageheight, imagewidth]),\n#        transforms.CenterCrop(cropsize),\n#        transforms.ToTensor()])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:55.166812Z","iopub.execute_input":"2021-08-02T05:33:55.167398Z","iopub.status.idle":"2021-08-02T05:33:55.173782Z","shell.execute_reply.started":"2021-08-02T05:33:55.167359Z","shell.execute_reply":"2021-08-02T05:33:55.172878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Import Numeric Data**\n\nBelow we import the CSV numeric data into a dataframe and then extract the needed values into appropriate tensors. Following this, the data is normalized on a scale of $[0,1]$. One can also standardize the data instead using e.g. the z-norm. Writing a function to handle the repetitive operations below would be a  useful modification to the code.","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# DATA IMPORT\n\n# Import train,val, and test data and set up data \n# loader. Note that ImageFolder will organize data \n# according to class labels of the folders \"house, \n# etc\" as found in the train and val data folder.\n# NOTE: When calling a specific image (such as 135) \n# from train data, the first XXX images are class 0,\n# then the next YYY are class 1, and etc. if more \n# than one class existed (which is not the case here).\n\n# Import CSV of Housing Data\n# Read Data from File; Create Tensors for train,test,val\nrawdata = \\\npd.read_csv('../input/subset-of-house-prices-and-images-socal/socal2_cleaned_mod.csv')\n\n#  Import all columns in CSV\nXraw = np.column_stack((rawdata['image_id'].values,\n                        rawdata['n_citi'].values,\n                        rawdata['bed'].values,\n                        rawdata['bath'].values,\n                        rawdata['sqft'].values,\n                        rawdata['price'].values))\n\n# ====================================================\n# Prepare Training Data\n\n# ====================================================\n# NOTE: Normalization was done after splitting data.\n# ====================================================\n\nXraw_train = Xraw[0:2000,:]  # Get required train data\n#city_data_train = Xraw_train[:,1]  # import city data\nbdrm_data_train = Xraw_train[:,2]  # import bdrm data\nbath_data_train = Xraw_train[:,3]  # import bath data\nsqft_data_train = Xraw_train[:,4]  # import sqft data\nyraw_true_train = Xraw_train[:,5]  # import price\n\n# NORMALIZE DATA (COULD STANDARDIZE INSTEAD)\n# Normalize data based to scale [0,1]. Could also \n# standardize as z = (x - mean)/stddev\n#city_train_norm = city_data_train/np.max(city_data_train)\nbdrm_train_norm = bdrm_data_train/np.max(bdrm_data_train)\nbath_train_norm = bath_data_train/np.max(bath_data_train)\nsqft_train_norm = sqft_data_train/np.max(sqft_data_train)\ny_true_train_norm = yraw_true_train/np.max(yraw_true_train)\n\n# Convert to torch tensor\n#city_train = torch.from_numpy(city_train_norm).float()\nbdrm_train = torch.from_numpy(bdrm_train_norm).float()\nbath_train = torch.from_numpy(bath_train_norm).float()\nsqft_train = torch.from_numpy(sqft_train_norm).float()\ny_train = torch.from_numpy(y_true_train_norm).float()\n\n# Combine sqft, bdrm, etc into one meta_data\nmeta_train = torch.stack((bdrm_train,bath_train,\n                          sqft_train),dim=1)\n\n# ===================================================\n# Prepare Validation Data\n\n# ===================================================\n# NOTE: Normalization was done after splitting data. \n# ===================================================\n\nXraw_val = Xraw[2000:3000,:]  # Get required val data\n#city_data_val = Xraw_val[:,1]  # import city data\nbdrm_data_val = Xraw_val[:,2]  # import bdrm data\nbath_data_val = Xraw_val[:,3]  # import bath data\nsqft_data_val = Xraw_val[:,4]  # import sqft data\nyraw_true_val = Xraw_val[:,5]  # import price data\n\n# NORMALIZE DATA (COULD STANDARDIZE INSTEAD)\n# Normalize data based to scale [0,1]. Could also \n# standardize as z = (x - mean)/stddev\n#city_val_norm = city_data_val/np.max(city_data_val)\nbdrm_val_norm = bdrm_data_val/np.max(bdrm_data_val)\nbath_val_norm = bath_data_val/np.max(bath_data_val)\nsqft_val_norm = sqft_data_val/np.max(sqft_data_val)\ny_true_val_norm = yraw_true_val/np.max(yraw_true_val)\n\n# Convert to torch tensor\n#city_val = torch.from_numpy(city_val_norm).float()\nbdrm_val = torch.from_numpy(bdrm_val_norm).float()\nbath_val = torch.from_numpy(bath_val_norm).float()\nsqft_val = torch.from_numpy(sqft_val_norm).float()\ny_val = torch.from_numpy(y_true_val_norm).float()\n\n# Combine sqft, bdrm, etc into one meta_data\nmeta_val = torch.stack((bdrm_val,bath_val,\n                        sqft_val),dim=1)\n\n# ================================================","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:55.179249Z","iopub.execute_input":"2021-08-02T05:33:55.181802Z","iopub.status.idle":"2021-08-02T05:33:55.260314Z","shell.execute_reply.started":"2021-08-02T05:33:55.181764Z","shell.execute_reply":"2021-08-02T05:33:55.259523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Numeric Data Loaders**\n\nBelow we have created data loader functions for pulling out and storing batches of numeric data for train and validation. These batches will be of the same size as the image batches and correspond to the appropriate houses. It is fairly easy to combine into one function but here, for readability, there are two. ","metadata":{}},{"cell_type":"code","source":"# ================================================\n# Generate batches of meta_data (house features)\n\n# Metadata (House Feaures) Training Batches\ndef get_batch_train(batch_size,which_batch,\n                    array_len=len(y_train)):\n        \n    num_batches = int(np.floor(array_len/batch_size))\n    \n    # Initialize lists\n    batch_y = []\n    batch_meta = []\n    for i in range(num_batches+1):\n        batch_y_train = \\\n            y_train[i*batch_size:(i+1)*batch_size]\n        batch_meta_train = \\\n            meta_train[i*batch_size:(i+1)*batch_size,:]\n        # NOTE: batch_y_train and batch_meta_train should\n        # be enough to pull out batches directly. No need\n        # to append to lists as below, but is convenient.\n        batch_y.append(batch_y_train)\n        batch_meta.append(batch_meta_train)\n        \n    # Call each batch from ydata_train and metadata_train\n    ydata_train = torch.FloatTensor(batch_y[which_batch]) \n    metadata_train = torch.FloatTensor(batch_meta[which_batch])\n    return ydata_train,metadata_train\n\n\n# Metadata Validation Batches\ndef get_batch_val(batch_size,which_batch,\n                  array_len=len(y_val)):\n        \n    num_batches = int(np.floor(array_len/batch_size))\n    \n    # Re-initialize lists\n    batch_y = []\n    batch_meta = []\n    for i in range(num_batches+1):\n        batch_y_val = \\\n            y_val[i*batch_size:(i+1)*batch_size]        \n        batch_meta_val = \\\n            meta_val[i*batch_size:(i+1)*batch_size,:]\n        # NOTE: batch_y_val and batch_meta_val should\n        # be enough to pull out batches directly. No need\n        # to append to lists as below, but is convenient.\n        batch_y.append(batch_y_val)\n        batch_meta.append(batch_meta_val)\n        \n    # Call each batch from ydata_val and metadata_val\n    ydata_val = torch.FloatTensor(batch_y[which_batch]) \n    metadata_val = torch.FloatTensor(batch_meta[which_batch])\n    return ydata_val,metadata_val\n\n\n# ====================================================","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:55.264174Z","iopub.execute_input":"2021-08-02T05:33:55.266152Z","iopub.status.idle":"2021-08-02T05:33:55.280177Z","shell.execute_reply.started":"2021-08-02T05:33:55.266115Z","shell.execute_reply":"2021-08-02T05:33:55.279171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Image Data Loaders**\n\nBelow we create data loaders for the images with shuffle = False, to maintain order with the numeric data loader above.","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Create Image Data Loader for Train,Validation,Test\n\n# Training Data\nimages_train = \\\ndatasets.ImageFolder('../input/subset-of-house-prices-and-images-socal/socal_pics/train',\n                     transform=transform_train)\nloader_train = \\\ntorch.utils.data.DataLoader(images_train, shuffle=False, \n                            batch_size=batch_size)\n\n# Validation Data\nimages_val = \\\ndatasets.ImageFolder('../input/subset-of-house-prices-and-images-socal/socal_pics/val',\n                     transform=transform_val)\nloader_val = \\\ntorch.utils.data.DataLoader(images_val, shuffle=False, \n                            batch_size=batch_size)\n\n# Testing Data\n# Can add testing data loader as well if desired","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:55.28519Z","iopub.execute_input":"2021-08-02T05:33:55.288035Z","iopub.status.idle":"2021-08-02T05:33:57.485157Z","shell.execute_reply.started":"2021-08-02T05:33:55.28799Z","shell.execute_reply":"2021-08-02T05:33:57.484258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CNN + MLP Hybrid Model**\n\nBelow we define the CNN+MLP model architecture. This particular model is set up for images of size 100x100. It was found that larger images were not necessarily helpful in improving performance.","metadata":{}},{"cell_type":"code","source":"# ==================================================\n# CNN + MLP Model Architecture\n# ==================================================\n\n# Here we have used a combined CNN + MLP. The CNN \n# processes image data and the MLP is employed for \n# input/learning of numeric data/features. The \n# outputs of each are concatenated to form one \n# stream of data.\n\n# NOTE NOTE NOTE: The CNN used in this problem \n# takes images of 100x100 pixels if linear input\n# layer is X * 22 * 22 or 200x200 pixels \n# with X * 47 * 47.\n\n# Two convolution network    \nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        # Image CNN\n        self.pool = torch.nn.MaxPool2d(2, 2)\n        self.conv1 = torch.nn.Conv2d(3, 10, 5)\n        self.conv2 = torch.nn.Conv2d(10, 10, 5)\n        self.fc1 = torch.nn.Linear(10 * 22 * 22, 120)\n        self.fc2 = torch.nn.Linear(120, 60)\n        \n        # Data MLP\n        # 3 inputs (eg bdrm,bath,sqft) to MLP\n        self.fc3 = torch.nn.Linear(3, 120)  \n        self.fc4 = torch.nn.Linear(120, 60)\n\n        # Cat outputs from CNN + MLP\n        self.fc5 = torch.nn.Linear(60 + 60, 120)\n        # 1 output (price) from CNN+MLP\n        self.fc6 = torch.nn.Linear(120, 1)  \n        # NOTE: output is trained as a regression value\n        # (continuous), in the range [0,1].\n        \n    def forward(self, x1, x2):\n        # Image CNN\n        x1 = self.pool(F.relu(self.conv1(x1)))\n        x1 = self.pool(F.relu(self.conv2(x1)))\n        x1 = x1.view(-1, 10 * 22 * 22)\n        x1 = F.relu(self.fc1(x1))\n        x1 = F.relu(self.fc2(x1))\n        \n        # Data MLP\n        x2 = x2.view(-1, 3) \n        x2 = F.relu(self.fc3(x2))\n        x2 = F.relu(self.fc4(x2))\n        \n        # Cat outputs from CNN + MLP\n        x3 = torch.cat((x1, x2), dim=1)\n        x3 = F.relu(self.fc5(x3))\n        x3 = self.fc6(x3)\n        \n        return x3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:57.488882Z","iopub.execute_input":"2021-08-02T05:33:57.48919Z","iopub.status.idle":"2021-08-02T05:33:57.50159Z","shell.execute_reply.started":"2021-08-02T05:33:57.489163Z","shell.execute_reply":"2021-08-02T05:33:57.500793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Loss Function and Optimizer**\n\nBelow we define an instance of the model class 'Net', choose the loss function, and the optimizer. The particular choice of MSE loss is due to the requirement of a regression (continuous) output from the model, which is the price prediction. ","metadata":{}},{"cell_type":"code","source":"    \n# ==============================================\n# ==============================================\n\n# Call instance of CNN+MLP NN class\nmodel = Net().to(dev)\n\n# MSE loss func since NN output is contin. in [0,1]\nloss_fn = torch.nn.MSELoss(reduction='mean')\n\n# Optimizer used to train parameters\noptimizer = torch.optim.Adam(model.parameters(), \n                             lr=learn_rate)\n\n# ===============================================","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:33:57.503206Z","iopub.execute_input":"2021-08-02T05:33:57.503612Z","iopub.status.idle":"2021-08-02T05:34:01.367543Z","shell.execute_reply.started":"2021-08-02T05:33:57.503577Z","shell.execute_reply":"2021-08-02T05:34:01.366701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Training Loop**\n\nBelow is the model training loop as well as the validation loop in each epoch. The average loss values and percent error are computed per epoch and stored in tensors for later plotting after training. ","metadata":{}},{"cell_type":"code","source":"# ===============================================\n\n# Initialize tensor to store loss values\nresult_vals = torch.zeros(num_epochs,4)\ncount_train = 0  # Initialize Counter\ncount_val = 0\n#X_test,y_true_test = loader_test\n\nprint(' ')\nprint('epoch  | loss_train  | loss_val  | err_train  | err_val')\nprint('-------------------------------------------------------')\n\nerror_train = torch.zeros(batch_size)\nerror_val = torch.zeros(batch_size)\n\nfor epoch in range(num_epochs):\n    # New epoch begins\n    running_loss_train = 0\n    running_loss_val = 0\n    running_error_train = 0\n    running_error_val = 0\n    num_batches_train = 0\n    num_batches_val = 0\n    count_train = 0\n    count_val = 0\n    j = 0  # Initialize batch counter\n    k = 0  # Initialize batch counter\n    \n    model.train() # Set torch to train\n    for X_train,_ in loader_train:\n        X_train = X_train.to(dev) \n        # (X,y) is a mini-batch:\n        # X size Nx3xHxW (N: batch_size, 3: three ch )\n        # y size N\n        \n        # Get metadata in batches from function \n        ydata_train,metadata_train = \\\n            get_batch_train(batch_size,j)\n        ydata_train,metadata_train = \\\n            ydata_train.to(dev),metadata_train.to(dev) \n        \n        # reset gradients to zero\n        optimizer.zero_grad()\n        \n        # run model and compute loss\n        N,C,nX,nY = X_train.size()\n        y_pred_train = model(X_train.view(N,C,nX,nY),\n                             metadata_train)\n        loss_train = loss_fn(y_pred_train.squeeze(),\n                             ydata_train)\n        \n        # Back propagation\n        loss_train.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n        # Compute and update loss for entire training set\n        running_loss_train += \\\n            loss_train.cpu().detach().numpy()\n        num_batches_train += 1 \n       \n        for i in range(len(ydata_train)):\n            error_train[i] = \\\n            abs(ydata_train[i].item() - \\\n            y_pred_train.squeeze()[i].item())/ydata_train[i].item()\n        \n        error_train_sum = sum(error_train)\n        running_error_train = running_error_train + \\\n            error_train_sum\n        \n        j = j+1 # Step batch counter\n    \n        \n    k = 0  # Re-initialize batch counter for validation\n    model.eval()  # Set torch for evaluation\n    for X_val,_ in loader_val:\n        X_val = X_val.to(dev) \n        # (X,y) is a mini-batch:\n        # X size Nx3xHxW (N: batch_size, 3: three ch )\n        # y size N\n        \n        # Get metadata in batches from function\n        ydata_val,metadata_val = \\\n            get_batch_val(batch_size,k)\n        ydata_val,metadata_val = \\\n            ydata_val.to(dev),metadata_val.to(dev) \n        \n        # run model and compute loss\n        N,C,nX,nY = X_val.size()\n        y_pred_val = model(X_val.view(N,C,nX,nY),\n                           metadata_val)\n        loss_val = loss_fn(y_pred_val.squeeze(),\n                           ydata_val)\n        \n        # Compute and update loss for entire val set\n        running_loss_val += \\\n            loss_val.cpu().detach().numpy()\n        num_batches_val += 1 \n        \n        for i in range(len(ydata_val)):\n            error_val[i] = \\\n            abs(ydata_val[i].item() - \\\n            y_pred_val.squeeze()[i].item())/ydata_val[i].item()\n        \n        \n        error_val_sum = sum(error_val)\n        running_error_val = running_error_val + \\\n            error_val_sum\n        \n        k = k+1  # Step batch counter\n\n    ave_loss_train = running_loss_train/num_batches_train\n    ave_loss_val = running_loss_val/num_batches_val\n    ave_error_train = \\\n        (running_error_train.item()/len(y_train))*100\n    ave_error_val = \\\n        (running_error_val.item()/len(y_val))*100\n\n\n# ============================================\n\n    \n    # Store loss to tensor for plotting\n    result_vals[epoch, 0] = ave_loss_train\n    result_vals[epoch, 1] = ave_loss_val\n    result_vals[epoch, 2] = ave_error_train\n    result_vals[epoch, 3] = ave_error_val\n    \n    # Print loss every N epochs\n    #if epoch % 2 == 1:\n    print(epoch, '      ', round(ave_loss_train.item(),5), \n          '     ', round(ave_loss_val.item(),5), \n          '   ', round(ave_error_train,5),'  ', \n          round(ave_error_val,5))\n\n# =================================================    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:34:01.36888Z","iopub.execute_input":"2021-08-02T05:34:01.369242Z","iopub.status.idle":"2021-08-02T05:38:13.231975Z","shell.execute_reply.started":"2021-08-02T05:34:01.369207Z","shell.execute_reply":"2021-08-02T05:38:13.231032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Conclusion and Plotting Results**\n\nBelow, the computing time is obtained, followed by plotting the loss and percent error in house prediction. Note that with the given (relatively small) data set and limited features (beds, baths, and rooms), this model reaches about 49 percent error (train) and 40 percent error (validation). Obviously this result can be improved, but it is a reasonable starting point. To interpret, if a house costs 200K dollars, the error is about +/- 80K dollars. A better estimate (and better agreement between train and val) can be achieved with k-fold cross validation due to the small size of data set. More images/data can improve the overall accuracy as well if overfitting are kept under control. The CNN+MLP model may also benefit from employing batch-norm and drop-out. Finally, one important data feature which was not included in the CSV file is the house addresses. If these are converted to numeric GPS coordinates and used for training, it should have a significant impact on prediction accuracy since house price is well-known to be influenced by location. This update to the data is forthcoming.","metadata":{}},{"cell_type":"code","source":"    \n# =================================================    \n# End Timing Code\ntoc = time.perf_counter()\n\n# Measure Time\nruntime = toc - tic\n\nprint(' ')\nprint('Computing Time')\nprint(runtime)\n\n\n# ==============================================\n# Plot Loss and Accuracy for train and val sets\n# ==============================================\n\nxvals = torch.linspace(0, num_epochs, num_epochs+1)\nplt.plot(xvals[0:num_epochs].cpu().numpy(), \n         result_vals[:,0].cpu().detach().numpy())\nplt.plot(xvals[0:num_epochs].cpu().numpy(), \n         result_vals[:,1].cpu().detach().numpy())\nplt.legend(['loss_train', 'loss_val'], \n           loc='upper right')\n#plt.xticks(xvals[0:num_epochs])\nplt.title('Loss (CNN + MLP Model)')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.tick_params(right=True, labelright=True)\n# plt.savefig('loss.pdf', bbox_inches='tight', dpi=2400)\nplt.show()\n#\n#\n# For plotting percent error (which needs to be added above)\nplt.plot(xvals[0:num_epochs].cpu().numpy(), \n         result_vals[:,2].cpu().detach().numpy())\nplt.plot(xvals[0:num_epochs].cpu().numpy(), \n         result_vals[:,3].cpu().detach().numpy())\nplt.legend(['error_train', 'error_val'], \n           loc='upper right')\n#plt.xticks(xvals[0:num_epochs])\nplt.title('Percent Error (CNN + MLP Model)')\nplt.xlabel('epochs')\nplt.ylabel('ave_error')\nplt.tick_params(right=True, labelright=True)\n# plt.ylim(-0.15, 1.0)\nplt.show()\n\n## ==============================================","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T05:38:13.233256Z","iopub.execute_input":"2021-08-02T05:38:13.233582Z","iopub.status.idle":"2021-08-02T05:38:13.702511Z","shell.execute_reply.started":"2021-08-02T05:38:13.233547Z","shell.execute_reply":"2021-08-02T05:38:13.701645Z"},"trusted":true},"execution_count":null,"outputs":[]}]}