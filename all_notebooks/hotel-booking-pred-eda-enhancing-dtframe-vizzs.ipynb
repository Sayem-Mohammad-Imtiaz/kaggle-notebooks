{"cells":[{"metadata":{},"cell_type":"markdown","source":" ## Hotel Booking Demand. EDA + Predictive Model + Step to Step Data Science Project. "},{"metadata":{},"cell_type":"markdown","source":"![](https://img.nh-hotels.net/nh_alicante-069-hotel_facilities.jpg)"},{"metadata":{},"cell_type":"markdown","source":"\n* > #### Is there any other way to render tables in a jupyter notebook with quantitative analysis results in a more beautiful fashion?. \n\n* > #### Data Processing : Does Splitting Data into multiple dataframes, each containing data of a particular type (discrete, continuous, nominal, ordinal, interval, interval ratio, etc. ) help?.\n\n\n* > #### See how to programmatically give answers to questions like : \n    * Do customer type influence the number of cancelations?\n    * Do desposit type shows a higher cancelation rate?.\n    * Do customer of a hotel cancel more than other customer form other hotels?.\n    * Do we see booking changes occuring at a particular time slot throughout the year,do happen more changes in the summer, what other than time patterns can be found?.\n    * Which subset of days since a customer is in the waiting list shows a higher probability of cancelation?. \n    * Which market_segment does better(lower percentage cancelation). \n    * Whats the distribution of children, adults, required_car_parking_spaces and other discrete data. \n        \n        \n* > #### Amongst other methods for feature selection the estimation of how strong the relationships between the variables are stands out as a useful way of indiciating where there might a strong dependancy between variables. \n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n*** \nTo give a solid ground from where to answer these and other questions I decided to take on this project with the idea of further improving the data preparation steps, sorting chronologically the steps that are to be followed on similar data science projecs.\n\n"},{"metadata":{},"cell_type":"markdown","source":"> ### Data Ingestion "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        if \".csv\" in filename:\n            df=pd.read_csv(path)\n        \n        \n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Function to Beautify Renderization of HTML Tables in Jupyter Notebooks"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfrom IPython.display import display_html\ndef display_side_by_side(*args, is_styler=(True, True), title=None):\n    html_str=''\n    \n    for i, container in enumerate(args): \n        if is_styler[i]:\n            html_str+=container.render()\n        else: \n            html_str+=container.to_html()\n            \n            \n    if title:\n        \n        import re \n\n        pattern = \"<thead>(.*)</thead>\"\n        pattern = re.compile(pattern)\n        extracted_block = re.search(pattern, html_str).group(1) \n        new_block =  '<caption>{}</caption> \\n'.format(title) + extracted_block\n        \n        block_text = new_block.split(\"\\n\")[0]\n        html_str = html_str.replace(extracted_block, new_block)\n    \n    \n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#### Styling Configuration / \nimport random\nstyle_props = [dict(selector=\"caption\",  props=[(\"text-align\", \"center\"),\n                                                  (\"font-size\", \"110%\"),\n                                                  (\"color\", \"black\"),\n                                                  ('text-decoration', 'underline')]),\n              \n              dict(selector='th', props=[(\"text-align\", \"left\")]),\n              dict(selector=\"td\", props=[(\"text-align\", \"left\"), (\"border\", \"1px solid black\")])]\n\n\nr = lambda : random.randint(150,256)\n\nget_hex_color = lambda : \"#%02X%02X%02X\" % ((r(), r(), r()))\nget_hex_prop = lambda: \"background-color : {}\".format(get_color())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef set_background_colors(series):\n    \n    color = get_hex_color()\n    background_vector = [ \"background-color : {}\".format(color) if value != \"\"\n                           else \"background-color : turquoise\" for value in series ]\n    \n    return background_vector\n\n    \ndef BeautifiedFrame(df, hide_index=True, callback=True, subset=None):\n   \n    \n    ### We dont want to show the index when rendering the layout on the window. \n    if hide_index:\n        view = df.style.hide_index()\n    else: \n        view = df.style\n    \n    ### Apply color background to all the dataframe\n    if not callback:\n        properties[\"background-color\"] = get_hex_color()\n        view = view.set_properties(**properties)\n\n    ### Apply color mapping with certain conditions on column or row wise. \n    if callback:\n        view = view.apply(set_background_colors, axis=0, subset=subset)        \n    \n    \n   \n    return view","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation "},{"metadata":{},"cell_type":"markdown","source":"A few aspects must be tackled during Data Preparation : \n\n\n    - Categorical, Nummerical, or a mixture of both?.\n    - Are there missing values, None, nan values?.\n    - If so, whats the best strategy or method to fill those instances. \n    - Do we need to drop a column or columns?. \n    "},{"metadata":{},"cell_type":"markdown","source":"> ### Exploring Data and its Types"},{"metadata":{},"cell_type":"markdown","source":"* By splitting data into its multiple subtypes, **Categorical[Nominal, Ordinal] and Nummerical [Discrete, Continuous, Interval, Interval Ratio] **we often facilitate the visualization of the data, because we simply gor from one single original df with > 10 columns to small dataframes with hardly more than 10 columns. Thus, visualization and an early grasp of whats in the data is facilitated.\n\n* This approach also helps to break down data preparation into multiple steps, without missing important ones, like **inputing missing data or finding outliers**, perform univariate analysis, bivariate analysis and multivariate analysis, and find relationships between variables, both, between and within groups. From the probabilities that emerge from these various statistical test we are more prepared to take on modeling and to build a model to predict a number \"Regression\" or a category \"Classification\". \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\ndfExploration = df.dtypes.reset_index().rename(columns={\"index\": \"columns\", 0: \"type\"}).sort_values(by=\"type\")\nviewExploration = dfExploration[dfExploration.columns[::-1]]\n\n### View Break Down of variables based on their data type classes. \ncat_columns = dfExploration.where(dfExploration[\"type\"] == \"object\")[\"columns\"].dropna().to_list()\nint_columns = dfExploration.where(dfExploration[\"type\"] == \"int64\")[\"columns\"].dropna().to_list()\nfloat_columns = dfExploration.where(dfExploration[\"type\"] == \"float\")[\"columns\"].dropna().to_list() \n\nview_variables = pd.DataFrame([int_columns, cat_columns,  float_columns]).T\nview_variables.columns = [\"\\'int64\\' Variables\",\"\\'object\\' Variables\",  \"\\'float\\' Variables\"]\nview_variables = view_variables.fillna(\"\")\n\n### Apply Styling to the View of our DataFrame.\nview_variables = BeautifiedFrame(view_variables, callback=True)\nview_variables.set_caption(\"Pandas Infered Data Types \").set_table_styles(style_props)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* > #### The way pandas handles data ingestion is not always as straight as one might think. Particularly pandas own data types, those that might be inferred when reading a csv or xlsx file were at first misleading. So if you are giving your first steps with Data Wrangling in pandas be aware that although pandas dtype(\"object) is often associated with categorical data, that is not always the case.\n\n* > #### That is the case for columns \"arrival_date_month\" and \"reservation_status_date\" where data is stored as object, which turns out to be a string. A first glimpse to the names will direct your future decissions as for which columns to keep as categorical and which not, based on your requirements and the type of analysis you want to carry out.\n\n* > #### Given that time is an important factor to analyze in this projec where we aim to find patterns like trends, periodicty, frequencies, correlations and relationships between variables, amongst others. \n\n* >  #### Its equally important to keep an eye on the columns that are not treated as \"object\" in pandas. That is the case for \"agent\", \"company\", that have been stored as \"float64\". Instead, these fall in the group of categorical variables, specifically of the type ordinal."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def explore_na(df):\n    na_columns = df.isna().sum()\n    summary_na = (na_columns[na_columns != 0]/df.shape[0]).sort_values(ascending=False)\n    summary_na = summary_na.round(1)\n    \n    if len(summary_na) == 0:\n        print(\"Non NaN values have been found\")\n    return summary_na.reset_index().rename(columns = {\"index\": \"Variables with \", 0:\"% NaN\"}).style.hide_index().set_properties(**{\"text-align\": \"left\"})\n    \ndf_na = explore_na(df)\ndf_na","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Splitting Data into its multiple types and sub-types: \n    > * Categorical : Nominal, Ordinal, Dichotomous, \n    > * Nummerical : Discrete, Continuous, Ratio,Interval Ratio, "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ncolumns_discrete = [\"required_car_parking_spaces\", \n           \"stays_in_weekend_nights\",\n           \"stays_in_week_nights\",\n           \"adults\",\n           \"children\",\n           \"babies\",\n           \"previous_cancellations\",\n           \"previous_bookings_not_canceled\",\n           \"booking_changes\",\n           \"total_of_special_requests\",\n           \"days_in_waiting_list\"]\n\n\ncolumns_ordinal = [\"agent\", \"company\"]\ncolumns_dichotomous = [\"is_canceled\", \"is_repeated_guest\", \"hotel\"]\ncolumns_continuous = [\"adr\", \"lead_time\"]\n\ncolumns_nominal =  [ col for col in cat_columns if \"date\" not in col and col not in columns_dichotomous]\ncolumns_time = [col for col in df.columns.values if \"date\" in col]\n\n\n##### DataFrames ######\n\n### Categorical Data\ndfOrdinal = df[columns_ordinal]\ndfNominal = df[columns_nominal]\ndfDichotomous = df[columns_dichotomous]\n\n### Nummerical Data \ndfDiscrete = df[columns_discrete]\ndfContinuous = df[columns_continuous]\ndfIntervalRatio = df[columns_time]\n\n\n### View Break Down of variables based on their data type classes. \nordinal_columns = dfOrdinal.columns.to_list()\nnominal_columns = dfNominal.columns.to_list()\ndic_columns = dfDichotomous.columns.to_list()\ndiscrete_columns = dfDiscrete.columns.to_list()\ncontinuous_columns = dfContinuous.columns.to_list()\ninterval_ratio_columns = dfIntervalRatio.columns.to_list()\n\n\n### Sorting the dataframe on the horizontal axis to fit the descending order : from types with more variables to less. \ncolumns = [discrete_columns,nominal_columns, interval_ratio_columns, \n           dic_columns, continuous_columns, ordinal_columns]\n\n### Passing all the columns = series to a new dataframe and transposing to get - surely exists a better workaround to get this done. \nalldata = pd.DataFrame(columns).T\nallcolumns = [\"dfDiscrete\", \"dfNominal\",\"dfIntervalRatio\",\n              \"dfDichotomous\", \"dfContinuous\",\"dfOrdinal\"]\n\nalldata.columns = allcolumns\n\n### Inputing \"\" for None Values that are by default by pandas assigned to the cells when joining data. \nalldata = alldata.fillna(\"\")\n\n\n### Apply Styling to the View of our DataFrame.\nview_variables = BeautifiedFrame(alldata)\nview_variables.set_caption(\"Break Down of Data - DataFrames and their Columns \").set_table_styles(style_props)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Exploring Ordinal Data"},{"metadata":{},"cell_type":"markdown","source":" ####     As seen in the Kernel the two columns that we find in our dataframe containing ordinal data are described as follows: \n       \n       * \"company\" : \"ID of the company/entity that made the booking or responsible for paying the booking. ID is                              presented instead of designation. \n       \n       * \"agent\" : ID of the Travel Agency that made the booking. "},{"metadata":{},"cell_type":"markdown","source":"> ### Ranking the best N Travel Agencies and Companies based on Cancelation Rates.  \n  \n#### Note : An important feature of this section is the leveraging of the Pandas Style Api which provides a fine application of styling to the html rendered in our jupyter notebook. Some of the code is hardcoded, but a good bulk of it its on the way to be a more production ready code to be used to gain insights during the first steps of any kind of exploratio. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def GetTopPerformances(df, column_target,\n                       renamed_column,\n                       filter_column,\n                       volume,\n                       top_n, \n                       sort_column=(None, None), \n                       th=None, \n                       captions = None):\n    \n    ## df, agent, Agent, \"TotalBookings\", 500, 25. \n    \n    ################################################\n    performance_df = df[[column_target, \"is_canceled\"]]\n    performance_df = performance_df[performance_df[column_target].notna()]\n\n    Performance = performance_df.groupby(performance_df.columns.to_list()).size().reset_index().rename(columns = {0:\"Totals\"})\n    Performance = pd.pivot_table(Performance, columns = [\"is_canceled\"], index = [column_target]).reset_index()\n    new_columns = [renamed_column, \"Not Cancelations\", \"Cancelations\"]\n\n    Performance.columns = new_columns \n\n    Performance[\"TotalBookings\"] = Performance[renamed_column].apply(lambda x : performance_df[performance_df[column_target] == x].shape[0])\n    Performance[\"PctCancelations\"] = (Performance[\"Cancelations\"]/Performance[\"TotalBookings\"])*100\n    Performance[\"PctNotCancelations\"] = (Performance[\"Not Cancelations\"]/Performance[\"TotalBookings\"])*100\n    Performance[\"PctCancelations\"] = Performance[\"PctCancelations\"].round(2)\n    Performance[\"PctNotCancelations\"] = Performance[\"PctNotCancelations\"].round(2)\n\n\n    AllPerformance = Performance[[\"Cancelations\", \"PctCancelations\", \"Not Cancelations\", \"PctNotCancelations\",\n                                  \"TotalBookings\", renamed_column]]\n    FinalPerformance = AllPerformance[AllPerformance[filter_column] > volume].fillna(0)\n\n\n    ################################################\n    if sort_column is None:\n        sort_column = (\"PctCancelations\", \"PctNotCancelations\")\n        \n    TopCanc = FinalPerformance[[renamed_column, \"Cancelations\",\"TotalBookings\",\"PctCancelations\"]]\n    TopCanc = TopCanc.sort_values(by=sort_column[0], ascending=False)[:top_n]\n    \n    TopNoCanc = FinalPerformance[[renamed_column, \"Not Cancelations\",\"TotalBookings\",\"PctNotCancelations\"]]\n    TopNoCanc = TopNoCanc.sort_values(by=sort_column[1], ascending=False)[:top_n]\n  \n    \n    color1 = get_hex_color()\n    color2 = get_hex_color()\n\n    stylerCancelations = BeautifiedFrame(TopCanc,\n                                         callback=False,\n                                         subset=[\"PctCancelations\",\n                                         renamed_column])\n    \n    if captions: \n        stylerCancelations = stylerCancelations.set_caption(captions[0]).set_table_styles(style_props)\n\n    stylerNoCancelations = BeautifiedFrame(TopNoCanc,\n                                           callback=False,\n                                           subset=[\"PctNotCancelations\",\n                                           renamed_column])\n    if captions: \n        stylerNoCancelations = stylerNoCancelations.set_caption(captions[1]).set_table_styles(style_props)\n    \n    \n    def highlight_over(row, threshold): \n        \n        if row[3] < threshold:\n            \n            return  [ \"background-color: yellow\",\n                       \"background-color: white\",\n                       \"background-color: white\",\n                       \"background-color: yellow\" ]\n                     \n        else: \n            \n            return  [ \"background-color: {}\".format(color1), \n                      \"background-color: white\",\n                      \"background-color: white\",\n                      \"background-color: {}\".format(color2)]\n        \n    \n    def highlight_under(row, threshold): \n        \n        if row[3] > threshold:\n            return  [ \"background-color: yellow\",\n                       \"background-color: white\",\n                       \"background-color: white\",\n                       \"background-color: yellow\" ]\n                     \n        else: \n            return  [ \"background-color: {}\".format(color1), \n                      \"background-color: white\",\n                      \"background-color: white\",\n                      \"background-color: {}\".format(color2)]\n        \n        \n    \n\n    if th:\n        stylerCancelations = stylerCancelations.apply(highlight_over,threshold=100-th, axis=1)  \n        stylerNoCancelations = stylerNoCancelations.apply(highlight_under,threshold=th, axis=1)\n    \n    \n    return stylerCancelations, stylerNoCancelations\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dfOrdinal.head()\n\nn = 25 ## top N per Volume\ndf1 = dfOrdinal[\"agent\"].where(dfOrdinal[\"agent\"].notna()).value_counts().reset_index()\ndf1.columns = [\"Agent\", \" TotalBookings\"]\ndf1 = df1[df1.columns[-1::-1]]\n\ndf2 = dfOrdinal[\"company\"].where(dfOrdinal[\"company\"].notna()).value_counts().reset_index()\ndf2.columns = [\"Company\", \"TotalBookings\"]\ndf2 = df2[df2.columns[-1::-1]]\n\n\nproperties = {\"text-align\": \"center\",\n              \"border-color\": \"black\", \n              \"border\":\"1px solid black\"}\n\nTopVolumeAgents = df1[:n]\nTopVolumeCompanies = df2[:n]\n\nStylerdf1 = BeautifiedFrame(TopVolumeAgents[:], callback=True, subset=\"Agent\").set_caption(\"Top 25 Agencies [Id] Volume\").set_properties(**properties)\nStylerdf2 = BeautifiedFrame(TopVolumeCompanies[:], callback=True, subset=\"Company\").set_caption(\"Top 25 Companies [Id] Volume\").set_properties(**properties)\n\nStylerdf1 = Stylerdf1.set_table_styles(style_props)\nStylerdf2 = Stylerdf2.set_table_styles(style_props)\n\ndisplay_side_by_side(Stylerdf1, Stylerdf2, is_styler=(True,True))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Viewing Top Travel Agencies per Volume(Total Bookings) and % Cancelation\n\n    - Highlighted in yellow those Travel Agencies with a better ratio of Not Cancelations (lowest cancelation rates)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### Performance here is understood, as agents companies or entities with a low percentage of cancelations.\n### Additional information of fees, contracts, payments associated with those companies or agents, etc are out of the scope\n### of this rather rough ranking of the best \"agents\", \"companies\", etc. \n\n### The aim with this notebook was to explore in deep the functionalities that emerge from utilizing panda's styling API. \n### In the process of creating more actionable data is where can see where and when steps in EDA or Data Science Projects could\n### be pulled off so optimization of time and resources are leveraged at best. \n\n### In the moment highlighting of background in rows, columns, cells are conditioned to the input parameters \n### that are passed to GetTopPerformances as well as for the logic that can be seen in\n\ntarget_column = \"agent\" ### Column to estimate the performance with. \ntop_n = 25  ### Number of Top Agents, Companies, etc to be shown in the table. \n\n\n\ncaption1 = \"Top {} Agencies [Id] based on % Cancelations\".format(top_n)  \ncaption2 = \"Top {} Agencies [Id]  based on % Not Cancelations\".format(top_n)  \n    \nstyler1, styler2 = GetTopPerformances(df, column_target=target_column,\n                                          renamed_column = target_column.capitalize(),\n                                          filter_column=\"TotalBookings\", \n                                          volume=100,\n                                          top_n=25, \n                                          sort_column=(\"TotalBookings\", \"TotalBookings\"), \n                                          th = 80, captions=[caption1, caption2])\n\n\ntarget_column = \"company\"\ncaption3 = \"Top {} Companies [Id] based on \\n % Cancelations\".format(top_n)  \ncaption4 = \"Top {} Companies [Id] based on % Cancelations\".format(top_n) \n\nstyler3, styler4 = GetTopPerformances(df, column_target=target_column,\n                                          renamed_column = target_column.capitalize(),\n                                          filter_column=\"TotalBookings\", \n                                          volume=100,\n                                          top_n=25, \n                                          sort_column=(\"TotalBookings\", \"TotalBookings\"),\n                                          th=90, captions=[caption3, caption4])\n    \ndisplay_side_by_side(styler1, styler2, styler3, styler4, is_styler=(True, True, True, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * > #### We focused on looking at how well or bad agencies and companies perform, by calculating cancelation rates we obtain a good marker of how they are doing and we can drive our business decissions to improve the rate of no cancelations by inspecting at how worth is to invest at a particular entity. \n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"reversed_columns = ordinal_columns[-1::-1]\nsubset_orig = df[reversed_columns + [\"is_canceled\"]]\nsubset_orig = subset_orig.fillna(-1)\n\ncondition = (subset_orig.is_canceled == 1) & (subset_orig.agent != -1) & (subset_orig.company != -1)\naggregated_df = subset_orig.where(condition).groupby(reversed_columns).size().reset_index().rename(columns = {0: \"Total\"})\naggregated_df = aggregated_df.sort_values(by=\"Total\", ascending=False)\naggregated_df = aggregated_df.rename(columns = {\"Total\": \"Cancelations\"})[:10]\n\ncaption1 = \"Count Travel Agencies - Companies Pair instances based on Total Cancelations\"  \nCancelations = BeautifiedFrame(aggregated_df, callback=True).set_caption(caption1).set_table_styles(style_props)\n\n\n\ncondition = (subset_orig.is_canceled == 0) & (subset_orig.agent != -1) & (subset_orig.company != -1)\naggregated_df = subset_orig.where(condition).groupby(reversed_columns).size().reset_index().rename(columns = {0: \"Total\"})\naggregated_df = aggregated_df.sort_values(by=\"Total\", ascending=False)\naggregated_df = aggregated_df.rename(columns = {\"Total\": \"Not Cancelations\"})[:10]\n\ncaption2 = \"Count Travel Agencies - Companies Pair instances based on Total Bookings(Not Cancelations)\"\n\nNotCancelations = BeautifiedFrame(aggregated_df, callback=True).set_caption(caption2).set_table_styles(style_props)\n    \ndisplay_side_by_side(Cancelations, NotCancelations, is_styler=(True, True))\n\n#print(\"The total number of cancelations where agent and company is :  \", )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   >* In a similar fashion than the previous computation we now look at unique pairs of agencies and companies, to see if there is any pair that shows a high or low cancelation.  "},{"metadata":{},"cell_type":"markdown","source":"> ### Exploring Nummerical Variables\n"},{"metadata":{},"cell_type":"markdown","source":"> #### Continuous Variables : These show"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt, seaborn as sns\n\nsns.set()\n\nfig = plt.figure(figsize=(25,10))\n\nax1 = fig.add_subplot(2, 1, 1)\nsns.distplot(dfContinuous[\"adr\"],ax=ax1)\n\nax2 = fig.add_subplot(2, 1, 2)\nsns.distplot(dfContinuous[\"lead_time\"],ax=ax2)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Computing Probability of No Cancelations based on total days_in_waiting_list. \n- How likely is a customer to not cancel in a given period of time?.\n    \n    Time Periods are broken down as follows: \n    \n    **\"3 Month Probs\"**\n       - Probability of No Cancelation in the first 3 months : Interval [0,90] (days)\n       - Probability \"  \"  \"   between month 3 and month 6: Interval [90,180] \n    \n    **\"2 Month Probs\"**\n       - Probability of No Cancelation in the first 2 months : Interval [0,60] (days)\n       - Probability \"  \"  \"   between month 2 and month 4: Interval [60,120] (days)\n     \n    **\"1 Month Probs\"**\n       - Probability of No  Cancelation in the first 2 months : Interval [0,60] (days)\n       - Probability \"  \"  \"   between month 2 and month 4: Interval [60,120] (days)\n     \n    **\"15 Days Probs\"**\n   \n       - Probability of No Cancelation in the first 15 days: Interval [0,15] (days)\n       - Probability of No Cancelation between day 15 and day 30: Interval [15,30] (days)\n    \n    **\"Weekly Probs\"**\n       - Probability of No Cancelation in the first 7 days: Interval [0,7] (days)\n       - Probability of No Cancelation between day 15 and day 30: Interval [7,15] (days)\n   \n    **\"3Days Probs\"**\n       - Probability of No Cancelation in the first 3 days: Interval [0,3] (days)\n       - Probability of No Cancelation between day 3 and day 6: Interval [3,6] (days)\n   \n    \n    "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_df(df, condition_value):\n    \n    df_selection = df[[\"days_in_waiting_list\", \"is_canceled\"]]\n    condition = df_selection.is_canceled == condition_value\n    \n    if condition_value == 0: \n        new_column_name = \"NoCancelations\"\n    else: \n        new_column_name = \"Cancelations\"\n\n    df_selection = df_selection.where(condition)[\"days_in_waiting_list\"].value_counts().to_frame().reset_index()\n    df_selection.columns = [\"days_in_waiting_list\", new_column_name]\n\n    df_selection = df_selection[df_selection[\"days_in_waiting_list\"] != 0].sort_values(by=\"days_in_waiting_list\", ascending=True)\n    df_selection = df_selection.fillna(0)\n    df_selection[\"days_in_waiting_list\"].fillna(0, inplace=True)\n    return df_selection\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def find_best_time(orig_df, step, is_cancelation=False):\n    \n    step = step\n    min_x = 0 \n    max_x = step\n    max_threshold = orig_df[\"days_in_waiting_list\"].max()\n    \n    mapping = {}\n    interval_data = {}\n    it = 1 \n    \n\n    if is_cancelation:\n        columns =  [\"Cancelations\", \"ProbCancelations\"]\n    else: \n        columns = [\"NoCancelations\", \"ProbNoCancelations\"]\n    \n    while(max_x < max_threshold): \n        \n        condition = (orig_df[\"days_in_waiting_list\"] >= min_x) & (orig_df[\"days_in_waiting_list\"] <= max_x)\n        values  = orig_df[condition][columns[0]]\n        if len(values) == 0:\n            rolled_mean = 0\n        else: \n            rolled_mean = values.mean()\n        \n        mapping[it] = \"[{}-{}]\".format(min_x, max_x)\n        interval_data[it] = rolled_mean\n        min_x += step\n        max_x += step\n        it+=1\n\n    total_cancelations = sum(interval_data.values())\n    probs = [ value/total_cancelations*100 for value in list(interval_data.values())]\n    \n    \n    ### Composing the new dataframe with the probabilities and the Interval-Ratio Data. \n    final_df = pd.DataFrame(probs, index=interval_data.keys())\n    final_df[\"Interval\"] =  [ mapping[value] for value in final_df.index ]\n    \n    final_df.fillna(0, inplace=True)\n    \n    final_df.columns = [columns[1], \"Interval\"]\n    final_df = final_df[[\"Interval\", columns[1]]]\n    final_df.sort_values(by=columns[1], ascending=False, inplace=True)\n   \n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef show_best_days(orig_df, is_cancelation=False):\n\n    periods = [1,2,7,15,30,60,90]\n    stylers = []\n    captions = [\"2Days Probs\", \"3Days Probs\", \"Weekly Probs\",\n                \"15 Days Probs\", \"Month Probs\", \"2 Month Probs\", \"3 Month Probs\"]\n\n    for i, period in enumerate(periods): \n        best_times = find_best_time(orig_df, period, is_cancelation)\n        styler = BeautifiedFrame(best_times, hide_index=False).set_table_styles(style_props).set_caption(captions[i])\n        stylers.append(styler)\n\n\n    display_side_by_side(stylers[6], stylers[5], stylers[4],  is_styler=(True, True, True))\n    display_side_by_side(stylers[3], stylers[2], stylers[1], is_styler=(True, True, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"WaitingListNoCancelation = get_df(df, 0)\nWaitingListCancelation = get_df(df, 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"show_best_days(WaitingListNoCancelation, is_cancelation= False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see the probabilities equally distributed across time periods. Thus, we can say there is no special time of the year when customers do keep their booking status unchanged."},{"metadata":{},"cell_type":"markdown","source":"* Computing Probability of Cancelations based on total days_in_waiting_list. \n    \n    **\"3 Month Probs\"**\n       - Probability of Cancelation in the first 3 months : Interval [0,90] (days)\n       - Probability \"  \"  \"   between month 3 and month 6: Interval [90,180] \n    \n    **\"2 Month Probs\"**\n       - Probability of Cancelation in the first 2 months : Interval [0,60] (days)\n       - Probability \"  \"  \"   between month 2 and month 4: Interval [60,120] (days)\n     \n    **\"1 Month Probs\"**\n       - Probability of Cancelation in the first 2 months : Interval [0,60] (days)\n       - Probability \"  \"  \"   between month 2 and month 4: Interval [60,120] (days)\n     \n    **\"15 Days Probs\"**\n   \n       - Probability of Cancelation in the first 15 days: Interval [0,15] (days)\n       - Probability of Cancelation between day 15 and day 30: Interval [15,30] (days)\n    \n    **\"Weekly Probs\"**\n       - Probability of Cancelation in the first 7 days: Interval [0,7] (days)\n       - Probability of Cancelation between day 15 and day 30: Interval [7,15] (days)\n   \n    **\"3Days Probs\"**\n       - Probability of Cancelation in the first 3 days: Interval [0,3] (days)\n       - Probability of Cancelation between day 3 and day 6: Interval [3,6] (days)\n   \n    \n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"show_best_days(WaitingListCancelation, is_cancelation= True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As expected cancelation with respect of time period show a different pattern, that is, we can say as we see in the tables that customers cancel more frequently in the following cases : \n\n    * The first three months - **P=0.4979**.\n    * The first two months - **P=0.4012**.\n    * The second month of the first two months - **P=0.2721**.\n    * Between days 30-45 - **P=0.20**.\n    * Betwen days 35-42 - **P=0.1345**. \n    * Between days 38-40 - **P=0.59958**.\n    \n\n* These probabilities tell us how certain is that cancelations will occur at certain times while a customer is in the waiting list. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndata = dfDiscrete[\"stays_in_weekend_nights\"].value_counts()\nnew_indices = data.index.to_list()\nnew_indices.pop(0)\n#data = data.loc[data.index.to_list().remove(0)]\n#data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Visualization - Discrete and Continous Data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt, seaborn as sns\nfig = plt.figure(figsize=(25,10))\n\nax = fig.add_subplot(2,1,1)\ndata = dfDiscrete[\"stays_in_weekend_nights\"].value_counts()\n\nnew_indices = data.index.to_list()\nnew_indices.pop(0)\n\ndata = data.loc[new_indices]\n\nx = data.index.values\ny = data.values\nsns.barplot(x,y)\nax.set_title(\"Stays in Weekend Nights\")\n\nax = fig.add_subplot(2,1,2)\ndata = dfDiscrete[\"stays_in_week_nights\"].value_counts()\n\n\nnew_indices = data.index.to_list()\nnew_indices.pop(0)\ndata = data.loc[new_indices]\n\nx = data.index.values\ny = data.values\nsns.barplot(x,y)\nax.set_title(\"Stays in Week Nights\")\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25,20))\n\n####### Pie Plot for Adults\n\nfig.add_subplot(4,2,1)\ntotal_count = dfDiscrete[\"adults\"].count()\norig_data = dfDiscrete[\"adults\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.10 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"adults\", fontsize=25)\n\n\n\n####### Pie Plot for children\n\nfig.add_subplot(4,2,2)\ntotal_count = dfDiscrete[\"children\"].count()\norig_data = dfDiscrete[\"children\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.10 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"Children\", fontsize=25)\n\n####### Pie Plot for Babies\n\nfig.add_subplot(4,2,3)\ntotal_count = dfDiscrete[\"babies\"].count()\norig_data = dfDiscrete[\"babies\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.3 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"Babies\", fontsize=25)\n\n####### Pie Plot for required Parking Spaces\n\nfig.add_subplot(4,2,4)\ntotal_count = dfDiscrete[\"required_car_parking_spaces\"].count()\norig_data = dfDiscrete[\"required_car_parking_spaces\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.3 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"Required Car Parking Spaces\", fontsize=25)\n\n\n\n####### Pie Plot for required Parking Spaces\n\nfig.add_subplot(4,2,5)\ntotal_count = dfDiscrete[\"previous_bookings_not_canceled\"].count()\norig_data = dfDiscrete[\"previous_bookings_not_canceled\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.5 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"Previous Bookings Not Canceled\", fontsize=25)\n\n\n\n####### Pie Plot for required Parking Spaces\n\nfig.add_subplot(4,2,6)\ntotal_count = dfDiscrete[\"total_of_special_requests\"].count()\norig_data = dfDiscrete[\"total_of_special_requests\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.15 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"Total of Special Requests\", fontsize=25)\n\n\n\n####### Pie Plot for previous cancelations\n\nfig.add_subplot(4,2,7)\ntotal_count = dfDiscrete[\"previous_cancellations\"].count()\norig_data = dfDiscrete[\"previous_cancellations\"].value_counts()\n\ncondition_small = (orig_data/total_count*100 < 3)\ncondition_big = (orig_data/total_count*100 > 3)\n\ndata = orig_data[condition_big]\ndata[\"others\"] = orig_data[condition_small].sum()\n\n\nlabels = data.index.values\ntotals = data.values\n\nexplode_values = [ 0.5 for i  in range(1, len(totals)+1) ]\nexplode_values.reverse()\nplt.pie(totals, labels = labels, explode=explode_values,  autopct='%1.1f%%', shadow=True, startangle=180 )\nplt.title(\"Previous Cancelations\", fontsize=25)\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Measure Strongness of Relationship between Continuous and Discrete Data and Dichotomous Variables. "},{"metadata":{},"cell_type":"markdown","source":"    \n    \"is_canceled\", \"is_repeated_guest\", \"hotel\". \n"},{"metadata":{},"cell_type":"markdown","source":"> \"The Point-Biserial Correlation Coefficient is a correlation measure of the strength of association between a continuous-level variable (ratio or interval data) and a binary variable. Binary variables are variables of nominal scale with only two values.\""},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfrom scipy.stats import pointbiserialr\ndef CorrDichNum(dfDichotomous, dfDiscrete, dfContinuous,  \n                dic_columns, discrete_columns, continuous_columns):\n    \n    corr = {}\n\n    for dic_column in dic_columns: \n\n        \n        corr[dic_column] = {}\n        mapping = { v:i for i,v in enumerate(dfDichotomous[dic_column].unique()) }\n\n        if \"object\" not in dfDichotomous[dic_column].dtypes.name :\n            dic_array = np.array(dfDichotomous[dic_column].values)\n            \n        else:\n            dic_array = [ mapping[value] for value in dfDichotomous[dic_column]  ]\n\n\n        for column_disc in discrete_columns:   \n            pbc = pointbiserialr(dfDiscrete[column_disc].fillna(method=\"ffill\"), dic_array)\n            if pbc.pvalue < 0.05:\n                corr[dic_column][column_disc] = abs(pbc.correlation)\n\n        for column_cont in continuous_columns:   \n                pbc = pointbiserialr(dfContinuous[column_cont].fillna(method=\"ffill\"), dic_array)\n                if pbc.pvalue < 0.05:\n                    corr[dic_column][column_cont] = abs(pbc.correlation)\n\n\n    import operator\n    for key in corr.keys():\n        corr[key] = dict( sorted(corr[key].items(), key=operator.itemgetter(1),reverse=True) )\n                    \n                         \n    return corr\n\n                     \n    \ndic_columns = dfDichotomous.columns.values\ndiscrete_columns = dfDiscrete.columns.values\ncontinuous_columns = dfContinuous.columns.values\n\ncorr = CorrDichNum(dfDichotomous, dfDiscrete, dfContinuous,\n                   dic_columns, discrete_columns, continuous_columns)\n\n##### Correlation between dichotomous variables and countinous or discrete variable(nummerical) \ncorr_canceled = pd.DataFrame.from_dict(corr[\"is_canceled\"], orient=\"index\").reset_index().rename(columns={\"index\": \"variable_name\",\n                                                                                               0: \"correlation_value\"})\n\ncorr_hotel = pd.DataFrame.from_dict(corr[\"hotel\"], orient=\"index\").reset_index().rename(columns={\"index\": \"variable_name\",\n                                                                                               0: \"correlation_value\"})\n\ncorr_repeated_guest = pd.DataFrame.from_dict(corr[\"is_repeated_guest\"], orient=\"index\").reset_index().rename(columns={\"index\": \"variable_name\",\n                                                                                               0: \"correlation_value\"})\n\n##### Tidying columns and editing style. \n\ncorr_canceled = corr_canceled.round(3)\ncorr_hotel = corr_hotel.round(3)\ncorr_repeated_guest = corr_repeated_guest.round(3)\n\ncorr_canceled.columns = [\" \\\"is_canceled\\\" correlations\", \"\"]\ncorr_hotel.columns = [\" \\\"hotel\\\" correlations\", \"\"]\ncorr_repeated_guest.columns = [\" \\\"is_repeated_guest\\\" correlations\", \"\"]\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ViewCorrCanceled = BeautifiedFrame(corr_canceled, callback=True)\nViewCorrHotel = BeautifiedFrame(corr_hotel, callback=True)\nViewCorrRepeatedGuest = BeautifiedFrame(corr_repeated_guest, callback=True)\n\ndisplay_side_by_side(ViewCorrCanceled, ViewCorrHotel, ViewCorrRepeatedGuest, is_styler=(True, True, True))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Exploring Interval Ratio (Time) Variables"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pylab as pylab\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 5),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large'}\npylab.rcParams.update(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from datetime import datetime\nfrom time import strptime\n\n\ncolumns = [\"arrival_date_year\", \"arrival_date_month\", \"arrival_date_day_of_month\"]\nAuxDf = dfIntervalRatio.rename(columns={\"arrival_date_year\": \"year\",\n                                        \"arrival_date_month\": \"month\", \n                                        \"arrival_date_day_of_month\": \"day\"})\n\nAuxDf[\"month\"] = AuxDf[\"month\"].apply(lambda x : strptime(x, '%B').tm_mon ) \ndfIntervalRatio[\"Arrival\"] = pd.to_datetime(AuxDf[[\"year\", \"month\", \"day\"]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\ndfIntervalRatio[\"reservation_status_date\"] = pd.to_datetime(dfIntervalRatio[\"reservation_status_date\"],\n                              format='%Y-%m-%d')\n\ndiff = np.mean([datetime.days for datetime in (dfIntervalRatio[\"reservation_status_date\"] - dfIntervalRatio[\"Arrival\"])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dfIntervalRatio.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\",\n         \"November\", \"December\"]\n\n\nCancellationsByMonth = df.groupby([\"arrival_date_month\"])[\"is_canceled\"].count().reset_index()\nCancellationsByMonth[\"arrival_date_month\"] = pd.Categorical(CancellationsByMonth[\"arrival_date_month\"], \n                                                            categories = months,\n                                                            ordered=True)\n\n\nn_cancelations = df[df[\"is_canceled\"] == 1].count()[0]\nCancellationsByMonth[\"Total_Cancelations\"] = [n_cancelations]*CancellationsByMonth.shape[0]\nCancellationsByMonth[\"Pct_cancelations\"] = CancellationsByMonth[\"is_canceled\"]/CancellationsByMonth[\"Total_Cancelations\"]*100\n\n\ndf_month = CancellationsByMonth.sort_values(by=\"Pct_cancelations\", ascending=False)\ndf_month[\"Pct_cancelations\"] = df_month[\"Pct_cancelations\"].round(2).astype(str) + \" %\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nCancellationsByDay = df.groupby([\"arrival_date_day_of_month\"])[\"is_canceled\"].count().reset_index()\n\n\nn_cancelations = df[df[\"is_canceled\"] == 1].count()[0]\nCancellationsByDay[\"Total_Cancelations\"] = [n_cancelations]*CancellationsByDay.shape[0]\nCancellationsByDay[\"Pct_cancelations\"] = CancellationsByDay[\"is_canceled\"]/CancellationsByDay[\"Total_Cancelations\"]*100\n\n\ndf_day = CancellationsByDay.sort_values(by=\"Pct_cancelations\", ascending=False)\ndf_day[\"Pct_cancelations\"] = df_day[\"Pct_cancelations\"].round(2).astype(str) + \" %\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Use seaborn style defaults and set the default figure size\nsns.set(rc={'figure.figsize':(25,10)})\n\nsorted_days = CancellationsByDay.sort_values(by=\"arrival_date_day_of_month\")\nplt.plot(sorted_days[\"arrival_date_day_of_month\"],sorted_days[\"is_canceled\"],linewidth=3, color=\"orange\")\n\n\nplt.title(\"Cancelations By Day between 2015-2017\", fontsize=25)\nplt.xlabel(\"Day\", fontsize=20)\nplt.ylabel(\"Total Number of Cancelations\", fontsize=20)\nplt.xticks(list(range(1,32)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n\n# Use seaborn style defaults and set the default figure size\nsns.set(rc={'figure.figsize':(25,10)})\n\nsorted_months = CancellationsByMonth.sort_values(by=\"arrival_date_month\").set_index(\"arrival_date_month\")\n\nsorted_months[\"is_canceled\"].plot(linewidth=3, color=\"blue\")\n\nplt.title(\"Cancelations By Month between 2015-2017\", fontsize=25, pad=20)\nplt.xlabel(\"Month\", fontsize=20)\nplt.ylabel(\"Cancelations\", fontsize=20)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * We don't see a particular pattern in the time data other than there seems to be a higher number of cancelations in the summer period. \n"},{"metadata":{},"cell_type":"markdown","source":"> ### Stats that lead to Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"* > ####  Its now that the previous fragmentation of the dataset into various dataframes makes sense. \n* > #### Our goal here is to measure the relationship between a combination of variables, their types will define what type of test will help us to come out with certain probabilities that a given relationship exists between these variables.   \n* > #### In general we could divide the type of relationships in Categorical to Categorical, Categorical to Nummerical, Nummerical to Nummerical."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport scipy.stats as ss\nfrom statsmodels.multivariate.manova import MANOVA\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n\n\n\nmanova = MANOVA(dfDiscrete.fillna(method=\"ffill\"), dfDichotomous[\"is_canceled\"])\nprint(manova.mv_test())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def categorical_relationship(cat_x, cat_y):\n\n    #### Split data target value = y and non_target values = X\n    y = cat_y.values\n    dfdummies = pd.get_dummies(cat_x)\n\n    x = np.array(dfdummies.values)\n    chi2Values, pvalues = chi2(x, y)\n\n    for i, chi2pvalue in enumerate(zip(chi2Values, pvalues)):\n        if chi2pvalue[1]  < 0.05:\n            print(\"predictor [{}]\".format(dfdummies.columns.values[i]))\n            print(\"Chi Value: \", chi2pvalue[0])\n            print(\"p value: \", chi2pvalue[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cat_dict = {}    \nfor column in dfNominal.columns.values:\n        print(\"################################################################\")\n        print(\"Estimating Relationship between [{}] and Target Variable [is_canceled]\".format(column))\n        categorical_relationship(dfNominal[column], dfDichotomous[\"is_canceled\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\ndef label_encoder(data):\n\n    from sklearn.preprocessing import LabelEncoder\n    label_encoder = LabelEncoder()\n    data = label_encoder.fit_transform(data)\n    classes = label_encoder.classes_\n    mappings = { i : classes[i] for i in range(0, len(classes))}\n    return data, mappings\n    \n\ndef onehot_encoder(data, list_features=None):\n    \n    \n    data = np.array(data).reshape(1,)\n    print(\"data after reshaped: \", data)\n    from sklearn.preprocessing import OneHotEncoder    \n   \n    if list_features is None or len(list_features) is 0:\n        onehot_encoder = OneHotEncoder(categories=\"auto\")\n    else:\n        onehot_encoder = OneHotEncoder(categorical_features=list_features)\n   \n\n    data = onehot_encoder.fit_transform(data).toarray()\n    data = np.delete(data, 0, 1) #Remove one variable to avoid dummy variable trap.\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\n\ndef heatmap(self, corr):\n\n    import seaborn as sns\n    sns.heatmap(corr,\n                xticklabels=corr.columns.values, \n                yticklabels=corr.columns.values, \n                annot=True)\n\n\ndef most_highly_correlated(self, corr_mat, numtoreport):\n\n    try:\n        # find the correlations\n        cormatrix = corr_mat\n        # set the correlations on the diagonal or lower triangle to zero,\n        # so they will not be reported as the highest ones:\n        cormatrix *= np.tri(*cormatrix.values.shape, k=-1).T\n        # find the top n correlations\n        cormatrix = cormatrix.stack()\n        cormatrix = cormatrix.reindex(cormatrix.abs().sort_values(ascending=False).index).reset_index()\n        # assign human-friendly names\n        cormatrix.columns = [\"FirstVariable\", \"SecondVariable\", \"Correlation\"]\n\n        report = cormatrix.head(numtoreport)\n        self.results.append(report)\n        return report, cormatrix\n\n    except Exception as Ex:\n        return None\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### More Visualizations - Grid Visuals to Spot Linear Non Linear Relationships between a selection of candidate variables. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(4,2, figsize=(15,10))\n\n\nfrom random import randint\nr = lambda: randint(0,256)\nhex_color = lambda: \"#%02X%02X%02X\" % (r(), r(), r())\n\n    \n    \ndf1 = df[[\"lead_time\", \"adr\", \"is_canceled\"]]\ndf2 = df[[\"adr\", \"previous_bookings_not_canceled\", \"is_canceled\"]]\ndf3 = df[[\"lead_time\", \"previous_bookings_not_canceled\", \"is_canceled\"]]\ndf4 = df[[\"arrival_date_month\", \"previous_bookings_not_canceled\", \"is_canceled\"]]\ndf4 = df4.groupby([\"arrival_date_month\", \"previous_bookings_not_canceled\"])[\"is_canceled\"].sum().reset_index()\nlevels = list(df[\"is_canceled\"].unique())\n\ndfs = [df1, df2, df3, df4]\nlevel_title =  { 0 : \"Not Cancelation\",\n                 1 :  \"Cancelation\"}\n\nfor i, dfplot in enumerate(dfs):\n    for j, level in enumerate(levels):\n       \n        x_column = dfplot.columns.to_list()[0]\n        y_column = dfplot.columns.to_list()[1]\n        \n        df_filtered = dfplot.loc[dfplot.is_canceled == level, :]\n\n        if i>0:\n            max_value = max(df_filtered[y_column])\n            ylim = (0, int(max_value*1.2))\n        else:\n            ylim = (0,1000)\n\n        ax[i][j].scatter(x_column, y_column, data=df_filtered, s=4, c=hex_color(), label=str(level) )\n        ax[i][j].set_title(\"{} - {} / {} \".format(level_title[level], x_column, y_column), fontsize=15)\n        ax[i][j].set(ylim=ylim)\n        if i == 3:\n            ax[i][j].set_xticklabels(df_filtered[x_column].unique(), **{\"rotation\":-65})\n        \n        \nfig.tight_layout(pad=2.5)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def highlight_selection(s, threshold, column, ncolumns):\n    value = list(s[column].values[0].split(\" %\"))[0]\n    if float(value) >=threshold[0] and float(value) <=threshold[1] :\n        arg = \"background-color: turquoise\"\n    else: \n        arg = \"\"\n    return [arg for i in range(0, ncolumns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def beautify_frame(df, hide_index=True):\n    \n    if not hide_index:\n        return df.style.set_properties(**{\"text-align\":\"left\"})\n    return df.style.hide_index().set_properties(**{\"text-align\":\"left\"})\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## EDA - Many Variables to \"is_canceled\""},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def pct_aggregator(df, column_target, column_grouping, condition_value):\n    \n    condition = df[column_target] == condition_value \n    ### Total number of observations where there is a cancelation = bookings where is_canceled ==1\n    total_bookings = df.where(condition).count()[1]\n\n    ### Cancelations : Grouping + Aggregation\n    group_cancelations = df.where(condition).groupby(column_grouping)[column_target].count()\n    cancelations_df = group_cancelations.reset_index().rename(columns={column_target: \"TotalCancelations\"})\n    \n    ### Bookings : Grouping + Aggregation\n    group_bookings = df.groupby(column_grouping)[column_target].count()\n    bookings_df = group_bookings.reset_index().rename(columns={column_target: \"TotalBookings\"})\n    \n    result_df = cancelations_df.merge(bookings_df, on = column_grouping[0])\n    \n    n_cancelations = result_df[\"TotalCancelations\"].sum()\n    print(\"Total cancelations : \", n_cancelations)\n    \n    ###  Computing the Percentage of Cancelations for each of the groups to which unique MultiIndeces have been generated. \n    result_df[\"PCT_Cancelations\"] = (result_df[\"TotalCancelations\"]/result_df[\"TotalBookings\"])*100\n    \n\n    ### Rearranging the columns in the result_df to match the following order: \n    columns = [ column_grouping[0],  \"PCT_Cancelations\", \"TotalCancelations\", \"TotalBookings\"]\n    result_df = result_df[columns]\n\n    result_df.sort_values(by=\"PCT_Cancelations\", ascending=False, inplace=True)\n    result_df[\"PCT_Cancelations\"] = result_df[\"PCT_Cancelations\"].round(2).astype(str) + \" %\"\n    result_df.sort_values(by=[\"PCT_Cancelations\", \"TotalCancelations\", \"TotalBookings\"],\n                          ascending=(False, False, False), inplace=True)\n\n    return result_df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def pct_multiple_aggregator(df, target_column, group_columns, condition, ascending_order=False): \n    \n    clause = df[target_column] == condition\n    result_df = df.groupby(group_columns)[target_column].count().reset_index().rename(columns={\"is_canceled\": \"TotalBookings\"})\n    \n    ### Composing new names for the future columns holding pct values and totals. \n    f = lambda: \"NoCancelations (Totals)\" if condition == 0  else \"Cancelations (Totals)\"\n    name_totals = f()\n    name_pct = \"Pct\" + name_totals\n\n    ### \n    cancelations = df.where(clause).groupby(group_columns)[target_column].count()\n    cancelations = cancelations.reset_index().rename(columns={\"is_canceled\" : name_totals})\n\n    \n    result_df = pd.merge(result_df, cancelations, on = group_columns, suffixes=(False, False))\n    result_df[name_pct] = result_df[name_totals]/result_df[\"TotalBookings\"]*100\n    result_df = result_df[[ group_columns[0], group_columns[1], name_pct, name_totals, \"TotalBookings\" ]]\n\n    result_df.sort_values(by=name_pct, ascending=ascending_order, inplace=True)\n    result_df[name_pct] = result_df[name_pct].round(2).astype(str) + \" %\"\n    return result_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Country vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"country_cancelations = pct_aggregator(df, \"is_canceled\", [\"country\"], 1)\ncancelations1000 = country_cancelations[country_cancelations[\"TotalBookings\"] > 1000]\ncancelations100 = country_cancelations[country_cancelations[\"TotalBookings\"] > 100]\n\ncaption1 = \"Pct Cancelations per Country - Bookings > 100\" \ncaption2 = \"Pct Cancelations per Country - Bookings > 1000\" \n\nViewCancelations_100 = BeautifiedFrame(cancelations100, callback=True, subset=[\"country\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(cancelations1000, callback=True,  subset=[\"country\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_100, ViewCancelations_1000, is_styler=(True, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Meal vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"meal_cancelations = pct_aggregator(df, \"is_canceled\",[\"meal\"], 1)\nmeal_cancelations100 = meal_cancelations[meal_cancelations[\"TotalBookings\"] > 100]\nmeal_cancelations1000 = meal_cancelations[meal_cancelations[\"TotalBookings\"] > 1000]\n\ncaption1 = \"Pct Cancelations per Meal - Bookings > 100\" \ncaption2 = \"Pct Cancelations per Meal - Bookings > 1000\" \n\nViewCancelations_100 = BeautifiedFrame(meal_cancelations100, callback=True, subset=[\"meal\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(meal_cancelations1000, callback=True,  subset=[\"meal\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_100, ViewCancelations_1000, is_styler=(True, True))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Reserved Room vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"reserved_room_cancelations =  pct_aggregator(df, \"is_canceled\",[\"reserved_room_type\"], 1)\n\nroom_cancelations100 = reserved_room_cancelations[reserved_room_cancelations[\"TotalBookings\"] > 100]\nroom_cancelations1000 = reserved_room_cancelations[reserved_room_cancelations[\"TotalBookings\"] > 1000]\n\n\ncaption1 = \"Pct Cancelations per Reserved Room - Bookings > 100\" \ncaption2 = \"Pct Cancelations per Reserved Room - Bookings > 1000\" \n\n\nViewCancelations_100 = BeautifiedFrame(room_cancelations100, callback=True, subset=[\"meal\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(room_cancelations1000, callback=True,  subset=[\"meal\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_100, ViewCancelations_1000, is_styler=(True, True))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Deposit Type vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"deposit_cancelations = pct_aggregator(df, \"is_canceled\",[\"deposit_type\"], 1)\ndeposit_cancelations100 = deposit_cancelations[deposit_cancelations[\"TotalBookings\"] > 100]\ndeposit_cancelations1000 = deposit_cancelations[deposit_cancelations[\"TotalBookings\"] > 1000]\n\n\ncaption1 = \"Pct Cancelations per Deposit Type - Bookings > 100\" \ncaption2 = \"Pct Cancelations per Deposit Type - Bookings > 1000\" \n\n\nViewCancelations_100 = BeautifiedFrame(deposit_cancelations100, callback=True, subset=[\"deposit_type\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(deposit_cancelations1000, callback=True,  subset=[\"deposit_type\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_100, ViewCancelations_1000, is_styler=(True, True))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Customer Type vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"customer_cancelations = pct_aggregator(df, \"is_canceled\",[\"customer_type\"], 1)\n\n\ncustomer_cancelations100 = customer_cancelations[customer_cancelations[\"TotalBookings\"] > 100]\ncustomer_cancelations1000 = customer_cancelations[customer_cancelations[\"TotalBookings\"] > 1000]\n\n\ncaption1 = \"Pct Cancelations per Customer Type - Number Bookings > 100\" \ncaption2 = \"Pct Cancelations per Customer Type - Number Bookings > 1000\" \n\n\nViewCancelations_100 = BeautifiedFrame(customer_cancelations100, callback=True, subset=[\"customer_type\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(customer_cancelations1000, callback=True,  subset=[\"customer_type\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_1000, ViewCancelations_100, is_styler=(True, True))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Market Segment vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"segment_cancelations = pct_aggregator(df, \"is_canceled\",[\"market_segment\"], 1)\n\nsegment_cancelations100 = segment_cancelations[segment_cancelations[\"TotalBookings\"] > 100]\nsegment_cancelations1000 = segment_cancelations[segment_cancelations[\"TotalBookings\"] > 1000]\n\n\ncaption1 = \"Pct Cancelations per Segment Type - Bookings > 100\" \ncaption2 = \"Pct Cancelations per Segment Type - Bookings > 1000\" \n\n\nViewCancelations_100 = BeautifiedFrame(segment_cancelations100, callback=True, subset=[\"market_segment\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(segment_cancelations1000, callback=True,  subset=[\"market_segment\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_1000, ViewCancelations_100, is_styler=(True, True))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Distribution Channel vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"distrib_cancelations = pct_aggregator(df, \"is_canceled\",[\"distribution_channel\"], 1)\n\ndistrib_cancelations100 = distrib_cancelations[distrib_cancelations[\"TotalBookings\"] > 100]\ndistrib_cancelations1000 = distrib_cancelations[distrib_cancelations[\"TotalBookings\"] > 1000]\n\ncaption1 = \"Pct Cancelations per Distribution Channel - Bookings > 100\" \ncaption2 = \"Pct Cancelations per Distribution Channel - Bookings > 1000\" \n\nViewCancelations_100 = BeautifiedFrame(distrib_cancelations100, callback=True, subset=[\"distribution_channel\", \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCancelations_1000 = BeautifiedFrame(distrib_cancelations1000, callback=True,  subset=[\"distribution_channel\", \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCancelations_100, ViewCancelations_1000, is_styler=(True, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Unique [Market Segment + Distribution Channel] vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"campaign_cancelations = pct_multiple_aggregator(df, \"is_canceled\", \n                                                [\"market_segment\", \"distribution_channel\"],\n                                                1, \n                                                ascending_order=True)\n\n## Selectig only those campaigns with a significant number of bookings. \ncampaign_cancelations100 = campaign_cancelations[campaign_cancelations[\"TotalBookings\"] > 100]\ncampaign_cancelations1000= campaign_cancelations[campaign_cancelations[\"TotalBookings\"] > 1000]\n\n## Setting the caption title\ncaption1 = \"Campaign Figures - Cancelations with TotalBookings > 100\"\ncaption2 = \"Campaign Figures - Cancelations with TotalBookings > 1000\"\n\n## Beautifying the Frame. \nViewCampaigns100 = BeautifiedFrame(campaign_cancelations100, callback=True, subset=[\"market_segment\",\n                                                                              \"distribution_channel\",\n                                                                              \"TotalBookings\"]).set_caption(caption1).set_table_styles(style_props)\nViewCampaigns1000 = BeautifiedFrame(campaign_cancelations1000, callback=True, subset=[\"market_segment\",\n                                                                              \"distribution_channel\",\n                                                                              \"TotalBookings\"]).set_caption(caption2).set_table_styles(style_props)\n\n\n\ndisplay_side_by_side(ViewCampaigns100, ViewCampaigns1000, is_styler=(True, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Hotels vs Cancelations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hotel1_name = \"City Hotel\"\nhotel2_name = \"Resort Hotel\"\n\nhotel1 = df[df[\"hotel\"] == hotel1_name]\nhotel2 = df[df[\"hotel\"] == hotel2_name]\n\ngroupby_columns = [\"market_segment\", \"distribution_channel\"]\n\ngroupby_columns = [\"hotel\"]  + groupby_columns\n\nhotel1 = hotel1.groupby(groupby_columns)[\"is_canceled\"].count().reset_index().rename(columns={\"is_canceled\": \"TotalBookings\"})\nhotel2 = hotel2.groupby(groupby_columns)[\"is_canceled\"].count().reset_index().rename(columns={\"is_canceled\": \"TotalBookings\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ncondition1 = (df[\"is_canceled\"] == 1) & (df[\"hotel\"] == hotel1_name)\ncondition2 = (df[\"is_canceled\"] == 1) & (df[\"hotel\"] == hotel2_name)\n\nhotels1_canceled = df.where(condition1).groupby(groupby_columns)[\"is_canceled\"].count().reset_index().rename(columns={\"is_canceled\": \"TotCancelations\"})\nhotels2_canceled = df.where(condition2).groupby(groupby_columns)[\"is_canceled\"].count().reset_index().rename(columns={\"is_canceled\": \"TotCancelations\"})\n\nhotel1 = pd.merge(hotel1, hotels1_canceled, on=groupby_columns, suffixes=(False, False))\nhotel2 = pd.merge(hotel2, hotels2_canceled, on=groupby_columns, suffixes=(False, False))\n\nhotel1[\"PctCancelations\"] = hotel1[\"TotCancelations\"]/hotel1[\"TotalBookings\"]*100\nhotel2[\"PctCancelations\"] = hotel2[\"TotCancelations\"]/hotel2[\"TotalBookings\"]*100\n\nhotel1 = hotel1[[\"hotel\", \"market_segment\", \"distribution_channel\", \"PctCancelations\", \"TotCancelations\", \"TotalBookings\"]]\nhotel2 = hotel2[[\"hotel\", \"market_segment\", \"distribution_channel\", \"PctCancelations\", \"TotCancelations\", \"TotalBookings\"]]\n\nhotel1.sort_values(by=\"PctCancelations\", ascending=True, inplace=True)\nhotel2.sort_values(by=\"PctCancelations\", ascending=True, inplace=True)\n\nhotel1[\"PctCancelations\"] = hotel1[\"PctCancelations\"].round(2).astype(str) + \"  %\"\nhotel2[\"PctCancelations\"] = hotel2[\"PctCancelations\"].round(2).astype(str) + \"  %\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hotel1_cancelations100 = hotel1[hotel1[\"TotalBookings\"] > 100]\nhotel1_cancelations1000 = hotel1[hotel1[\"TotalBookings\"] > 1000]\n\nhotel2_cancelations100 = hotel2[hotel2[\"TotalBookings\"] > 100]\nhotel2_cancelations1000 = hotel2[hotel2[\"TotalBookings\"] > 1000]\n\n## Setting the caption title\ncaption11 = \"Campaign Figures ({}) - Cancelations with TotalBookings > 100\".format(hotel1_name)\ncaption12 = \"Campaign Figures ({}) - Cancelations with TotalBookings > 1000\".format(hotel1_name)\ncaption21 = \"Campaign Figures ({}) - Cancelations with TotalBookings > 100\".format(hotel2_name)\ncaption22 = \"Campaign Figures ({}) - Cancelations with TotalBookings > 1000\".format(hotel2_name)\n\n\n## Beautifying the Frame. \n\nViewCampaignsHotel1_100 = BeautifiedFrame(hotel1_cancelations100 , callback=True, subset=[\"market_segment\",\n                                                                              \"distribution_channel\",\n                                                                              \"TotalBookings\"]).set_caption(caption11).set_table_styles(style_props)\n\nViewCampaignsHotel1_1000 = BeautifiedFrame(hotel1_cancelations1000 , callback=True, subset=[\"market_segment\",\n                                                                              \"distribution_channel\",\n                                                                              \"TotalBookings\"]).set_caption(caption12).set_table_styles(style_props)\n\nViewCampaignsHotel2_100 =BeautifiedFrame(hotel2_cancelations100 , callback=True, subset=[\"market_segment\",\n                                                                              \"distribution_channel\",\n                                                                              \"TotalBookings\"]).set_caption(caption21).set_table_styles(style_props)\n\nViewCampaignsHotel2_1000 = BeautifiedFrame(hotel2_cancelations100 , callback=True, subset=[\"market_segment\",\n                                                                              \"distribution_channel\",\n                                                                              \"TotalBookings\"]).set_caption(caption22).set_table_styles(style_props)\n\ndisplay_side_by_side(ViewCampaignsHotel1_100, ViewCampaignsHotel1_1000, \n                     ViewCampaignsHotel2_100, ViewCampaignsHotel2_1000,\n                     is_styler=(True,True, True, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### EDA - Other Questions(Ongoing)"},{"metadata":{},"cell_type":"markdown","source":"*  #### When do operation changes occur?.\n*  #### There is any connection between days_in_waiting_list and the rest of the variables?.\n*  #### Avg Time between reservation date an arrival?.\n*  #### What variables show linear relationship with booking_changes?.\n"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"df[ dfDiscrete.columns.to_list() + dfContinuous.columns.to_list()].corr()[\"booking_changes\"].abs().sort_values(ascending=False)[1:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf_anova = dfNominal.nunique()\nanova_columns = df_anova[df_anova < 10].index.to_list()\n\ndf_encoded = df[[\"booking_changes\"] + anova_columns]\ndf_encoded[anova_columns] = df_encoded[anova_columns].apply(lambda col: le.fit_transform(col))\ndf_encoded.head(10)\n\nfor column in anova_columns:\n    print(\"###########################################################\")\n    print(\"######### One-Way Anova for column {} ##########\".format(column))\n    print(\"###########################################################\")\n\n    \n    mod = ols(\"{}  ~ booking_changes\".format(column), data=df_encoded).fit()\n    aov_table = sm.stats.anova_lm(mod, typ=2)\n    esq_sm = aov_table['sum_sq'][0]/(aov_table['sum_sq'][0] +  aov_table['sum_sq'][1])\n    aov_table['EtaSq'] = [esq_sm, 'NaN']\n    print(aov_table)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" > ### EDA - Booking Changes vs Customer Type"},{"metadata":{},"cell_type":"markdown","source":"* #### What can we say about booking_changes?. Lets look at variation per group for all variables from the group of Nominal Variables. In other words :\n\n   *  Do number of booking changes show a relationship with customer type?.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"BookingChangesCustomer = df.groupby([\"customer_type\"])[\"booking_changes\"].sum().reset_index()\nn_changes = df[\"booking_changes\"].sum()\nBookingChangesCustomer[\"Total_Booking_Changes\"] = [n_changes]*BookingChangesCustomer.shape[0]\nBookingChangesCustomer[\"PctChanges\"] = (BookingChangesCustomer[\"booking_changes\"]/n_changes)*100\nBookingChangesCustomer.sort_values(by=\"PctChanges\", ascending=False, inplace=True)\nBookingChangesCustomer[\"PctChanges\"] = BookingChangesCustomer[\"PctChanges\"].round(2).astype(str) + \" %\"\nBookingChangesCustomer.style.hide_index().set_properties(**{\"text-align\":\"left\"})\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" > ### EDA - Booking Changes vs Deposit Type"},{"metadata":{},"cell_type":"markdown","source":"   *  Do number of booking changes show a relationship with ****deposit type?****.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"BookingChangesDeposit = df.groupby([\"deposit_type\"])[\"booking_changes\"].sum().reset_index()\n\nn_changes = df[\"booking_changes\"].sum()\nBookingChangesDeposit[\"Total_Booking_Changes\"] = [n_changes]*BookingChangesDeposit.shape[0]\n\nBookingChangesDeposit[\"PctChanges\"] = (BookingChangesDeposit[\"booking_changes\"]/n_changes)*100\nBookingChangesDeposit.sort_values(by=\"PctChanges\", ascending=False, inplace=True)\nBookingChangesDeposit[\"PctChanges\"] = BookingChangesDeposit[\"PctChanges\"].round(2).astype(str) + \" %\"\nBookingChangesDeposit.style.hide_index().set_properties(**{\"text-align\":\"left\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" > ### EDA - Booking Changes - Time Series (Month/Day)"},{"metadata":{},"cell_type":"markdown","source":" * #### Do number of booking changes show a trend when looking at ****time intervals?****"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nmonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\",\n         \"November\", \"December\"]\n\nBookingChanges = df.groupby([\"arrival_date_year\", \"arrival_date_month\", \"arrival_date_day_of_month\"])[\"booking_changes\"].sum().reset_index()\n\n\nBookingChanges[\"arrival_date_month\"] = pd.Categorical(BookingChanges[\"arrival_date_month\"], \n                                                            categories = months,\n                                                            ordered=True)\n\n\ncolumns = [\"arrival_date_year\", \"arrival_date_month\", \"arrival_date_day_of_month\"]\nAuxDf = BookingChanges.rename(columns={\"arrival_date_year\": \"year\",\n                                        \"arrival_date_month\": \"month\", \n                                        \"arrival_date_day_of_month\": \"day\"})\n\nAuxDf[\"month\"] = AuxDf[\"month\"].apply(lambda x : strptime(x, '%B').tm_mon ) \nBookingChanges[\"Arrival\"] = pd.to_datetime(AuxDf[[\"year\", \"month\", \"day\"]], format=\"%Y-%m-%d\", errors=\"coerce\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ntime_selection = BookingChanges.where(BookingChanges[\"arrival_date_year\"] == 2015).set_index(\"Arrival\")\ntime_selection[\"booking_changes\"].plot(linewidth=2)\n\nplt.title(\"Booking Changes 2015 - Month vs Total Changes\", fontsize=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"time_selection = BookingChanges.where(BookingChanges[\"arrival_date_year\"] == 2016).set_index(\"Arrival\")\ntime_selection[\"booking_changes\"].plot(linewidth=2)\nplt.title(\"Booking Changes 2016 - Month vs Total Changes\", fontsize=25)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"time_selection = BookingChanges.where(BookingChanges[\"arrival_date_year\"] == 2017).set_index(\"Arrival\")\ntime_selection[\"booking_changes\"].plot(linewidth=2)\nplt.title(\"Booking Changes 2017 - Month vs Total Changes\", fontsize=25)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nBookingChangesByDay = df.groupby([\"arrival_date_day_of_month\"])[\"booking_changes\"].sum().reset_index()\nBookingChangesByDay[\"booking_changes\"].plot(linewidth=2, color=\"orange\")\nplt.title(\"Booking Changes 2015-2017 Daily Totals\", fontsize=25)\nplt.xticks(list(range(1,31))[::2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Building a Predictive Model"},{"metadata":{},"cell_type":"markdown","source":"* Initial set of features to add to our model :"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfeatures = corr_canceled[corr_canceled.columns.values[0]].values\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"models = [\"Random_Forest_Classification\",\n          \"Decission_Tree_Classification\",\n          \"Logistic_Regression\", \n          \"Support Vector Machine\", \n          \"Gradient Boosting Tree\",\n          \"Naive Bayes\"]\n\nprint(\"Building a Predictive Model : There will be any cancelation?.\\n\" +\n      \"We attempt to build a robust predictive model based off of these methods: \")\nprint(\"Suggested Models :\\n\")\nprint(\"\\n\".join( f\"{i}. {model}\"  for i,model in enumerate(models)) )\nprint(\"\\n\\nSuggested Features - Independent Variables:\\n\")\n\n\nfeatures = corr_canceled[corr_canceled.columns.values[0]].to_list()\n\n## As seen in another Kernel booking_changes and days_in_waiting_list may incur in data leakage, that is, the absence of these values while\n# while predicting the class of new instance, the values of these two variable might not be there. \nfeatures.remove(\"booking_changes\")\nfeatures.remove(\"days_in_waiting_list\")\nprint(\"Nummerical Variables : \")\nprint(\"\\n\".join( f\"\\t> {model}\"  for  model in features) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Random Forest & Decission Tree Classification & Logistic Regression + More."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def summary_classification(clf):\n    print(\"Number of iterations: \", clf.n_iter_)\n    import operator\n    coef_weights = { variable:weight for variable, weight in zip(independent_df.columns.values, clf.coef_[0])}\n    sorted_coef = dict(sorted(coef_weights.items(), key=operator.itemgetter(1), reverse=False))\n    df_coef = pd.DataFrame.from_dict(data=sorted_coef, orient=\"index\")\n    print(\"Intercept: \", clf.intercept_)\n    print(\"Classes: \", clf.classes_ )\n    df_coef = df_coef.rename(columns={0: \"Weights\"})\n    return df_coef\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_samples = df.shape[0]\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nX = df[features].values\ny = df[\"is_canceled\"]\n\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n\nscores = cross_val_score(clf, X, y, cv=5)\nprint(\"Decission Tree Classifier\", scores.mean())\n\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\nscores = cross_val_score(clf, X, y, cv=5)\nprint(\"Random Forest Classifier: \", scores.mean())\n\n\nclf = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\nscores = cross_val_score(clf, X, y, cv=5)\nprint(\"Extra Trees Classifiesr: \" , scores.mean())\n\n\n#clf = GradientBoostingClassifier(n_estimators=120, learning_rate=0.01, random_state=0).fit(X_train, y_train)\n#print(\"Gradient Boosting Tree Score \\n with learning rate={} and n_estimators={} : \".format(0.01, 100), clf.score(X_test, y_test))\n\n################### Logistic Regression ##################\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0, class_weight=\"balanced\", solver=\"sag\").fit(X,y)\n\nclf.predict(X[:2, :])\nclf.predict_proba(X[:2, :])\nclf.score(X, y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_coef = summary_classification(clf)\nViewCoefficients = BeautifiedFrame(df_coef, callback=True, hide_index=False).set_caption(\"Logistic Regression Coeff. Importance\")\nViewCoefficients = ViewCoefficients.set_table_styles(style_props)\nViewCoefficients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* #### Improving the Model with new variables (Feature Selection based on the EDA) we encode the groups of the independent variables that show a higher similarity in variance with cancelations, that is a lowest percentage of cancelations. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nencoded_columns = [\"deposit_type\", \"customer_type\", \"market_segment\", \n                  \"distribution_channel\", \"country\", \"reserved_room_type\", \"meal\"]\n\nmap_encoding = {\"deposit_type\" : \"No Deposit\", \n                \"customer_type\": \"Transient\",\n                \"market_segment\": \"Direct\", \n                \"distribution_channel\": \"Direct\",\n                \"country\" : \"PRT\",\n                \"reserved_room_type\" : \"A\",\n                \"assigned_room_type\" : \"D\",\n                \"meal\": \"FB\"} \n\nadditional_encodings =  {\"assigned_room_type\" : \"A\", \"reserved_room_type\" : \"D\"} \n        \nall_dummies = [ pd.get_dummies(df[K])[V] for K,V in map_encoding.items() ] \nall_dummies += [ pd.get_dummies(df[K])[V] for K,V in additional_encodings.items() ]\n\nencoded_features = pd.concat(all_dummies, axis=1)\n\n### Add in the horizontal axis the columns of all the independent variables that will be part of our next model. \ndic_columns = [\"hotel\", \"is_repeated_guest\"]\ndummies_hotel = pd.get_dummies(df[\"hotel\"])\nindependent_df = pd.concat([df[features], encoded_features, dummies_hotel, df[\"is_repeated_guest\"]], axis=1)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = independent_df.values\ny = df[\"is_canceled\"]\n\n##### KFolds #####\nkfolds = 4\nsplit = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n\n\n### Decission Tree Classifier\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n\nscores = cross_val_score(clf, X, y, cv=5)\nprint(\"Decission Tree Classifier : \", scores.mean())\n\n\n### Decission Tree Classifier hyperparameters tuned. \n\nclf = DecisionTreeClassifier(random_state = 0, max_features=\"auto\", min_impurity_decrease=1e-6)\ncv_results = cross_val_score(clf, X, y, cv=split, scoring=\"accuracy\", n_jobs=-1)\nprint(\"Decission Tree Classifier Tuned [Score]: {}\".format(cv_results.mean()))\n\n\n### Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\nscores = cross_val_score(clf, X, y, cv=5)\nprint(\"Random Forest Classifier: \", scores.mean())\n\n### Random Forest Classifier Tuned\nclf = RandomForestClassifier(n_estimators=120, max_depth=None, min_samples_split=2, random_state=42)\nscores = cross_val_score(clf, X, y, cv=split, scoring=\"accuracy\", n_jobs=-1)\nprint(\"Random Forest Classifier Tuned: \", scores.mean())\n\n### Extra Trees Classifier\nclf = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\nscores = cross_val_score(clf, X, y, cv=5)\nprint(\"Extra Trees Classifiesr: \" , scores.mean())\n\n\n### Gradient Boosting Classifier\n#clf = GradientBoostingClassifier(n_estimators=120, learning_rate=0.01, random_state=0).fit(X_train, y_train)\n#print(\"Gradient Boosting Tree Score : {} \\n with learning rate={} and n_estimators={} : \".format(clf.score(X_test, y_test),\n                                                                                                 #0.01, 100))\n\n\n### Logistic Regression Classifier\nclf = LogisticRegression(random_state=42, n_jobs=-1).fit(X,y)\n\nkfolds = 4\nsplit = KFold(n_splits=kfolds, random_state=42, shuffle=True)\n\nscores = cross_val_score(clf, X, y, cv=split, scoring=\"accuracy\", n_jobs=-1)\nprint(\"Logistic Regression accuracy:\" , scores.mean())\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## CONCLUSIONS AND TAKEAWAYS"},{"metadata":{},"cell_type":"markdown","source":"* Random Forest Classification with a number of estimators around 110~130 shows good accuracy numbers 86%. \n\n* Artificial Neural Network will surely notch up the prediction by learning other relationships than we are not able to spot or learn using machine learning or statistics. \n\n* Focusing on designing useful pipelines that do the always tedious task of preparing, cleaning data is key. \n\n* By adding hand picked features we can incur into high bias, that is that for new examples, that is the model may fail to generalize for new examples. \n\n* By structuring Data Science Projects together with the honing of knowledge in pandas methods, we not only shorten our times in Wrangling, Cleaning and Processing Data, we are able to see as well the growing need of the utilization of higher levels of abstraction. The more structured and neatly planned the processes are the easier to spot anomalies and patterns in later stages throughout the work, to facilitate an orderly univariate, bivariate or multivariate analysis. \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}